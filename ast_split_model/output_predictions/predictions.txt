STEP 1: Retrieving the Data
STEP 1: Train the labels, STEP 2: We will align the training and test dummies, STEP 3: Train the target variable, STEP 4: Lets look at the features
STEP 1: Adding the Features
STEP 1: import impute as data
STEP 1: checking missing data
STEP 1: Which columns have duplicated
STEP 1: import seaborn as sns, STEP 2: Distribution of Amount
STEP 1: Implement boxcox, STEP 2: Distribution of Incomes Total
STEP 1: import seaborn as sns, STEP 2: Distribution of Amount Credit
STEP 1: Original training data, STEP 2: Contract types of Loan
STEP 1: Understanding the data
STEP 1: Age of customer sales
STEP 1: Number of bureau entries per bureau, STEP 2: add the features to the application, STEP 3: add the features to the test set
STEP 1: Number of Loans per Application
STEP 1: Find one row per column, STEP 2: One to another application
STEP 1: merge the previous columns with previous values
STEP 1: Moved columns with negative values
STEP 1: Bas Cash Categorical Data
STEP 1: Loading the data
STEP 1: This is the important part, STEP 2: Preprocess the polygons
STEP 1: we combine test and hold
STEP 1: Classification and prediction
STEP 1: Create submission file
STEP 1: Run a simple classifier

STEP 1: Train the best model



STEP 1: Training the model
STEP 1: Standard Scaling, STEP 2: normalizing and flip
STEP 1: Categorical to label for each day, STEP 2: CALCULATE MEANS AND STANDARD DEVIATIONS
STEP 1: SMOOTH A DISCRETE FUNCTION
STEP 1: STORE PROBABILITIES IN PR, STEP 2: show some plots




STEP 1: Linear linear model, STEP 2:  Ridge pretrained model










STEP 1: data pass a submission, STEP 2: load the data, STEP 3: Load csv file, STEP 4: load the sell prices csv file
STEP 1: Create a list of dummy features
STEP 1: We will need different scaling factors the w columns in dataframe, STEP 2: load original weights, STEP 3: difference in original dataframe, STEP 4: For the Original Weights
STEP 1: difference in original dataframe
STEP 1: pass to filepath, STEP 2: Load filter weights, STEP 3: Convert to csv
STEP 1: Load filter weights
STEP 1: Convert to csv
STEP 1: Getting the submission file, STEP 2: Number of days, STEP 3: Month , Days
STEP 1: Function to create centroids for image detection
STEP 1: Function to create centroids for image detection
STEP 1: Create the centroids
STEP 1: Function for creating centroids in pixel data

STEP 1: group by standardization

STEP 1: We will concentrate on time series
STEP 1: Plot the autocorrelation between variables
STEP 1: Lets plot the lag plot
STEP 1: Lets sort the distributions that look like
STEP 1: plot the distribution of norm
STEP 1: Load the data, STEP 2: Read the Data
STEP 1: Categorical to label for each day, STEP 2: CALCULATE MEANS AND STANDARD DEVIATIONS
STEP 1: SMOOTH A DISCRETE FUNCTION
STEP 1: STORE PROBABILITIES IN PR, STEP 2: show one loop


STEP 1: Generate data for regression, STEP 2: Randomly select a list of items from the grid, STEP 3: For each item
STEP 1: the edges in the image
STEP 1: Null Values chart
STEP 1: count missing values, STEP 2: Create a new index for the column
STEP 1: Null Values in Test Dataset
STEP 1: Filter by sex
STEP 1: Looking at the distribution of age
STEP 1: Loading the Data
STEP 1: Function to plot images
STEP 1: Plot random images
STEP 1: Generate random images
STEP 1: Taking a look at the test data, STEP 2: Add image name
STEP 1: Loading the Data
STEP 1: THIS FUNCTION FOR LATCH, STEP 2: This is a moving average
STEP 1: Now , we will freeze our data, STEP 2: Ignore the warnings, STEP 3: Import Train and Test Data

STEP 1: Global Training parameters
STEP 1: Balance the price
STEP 1: UpVote if this was helpful
STEP 1: DISPLAY FEATURE EXTRACTION
STEP 1: Compile the model
STEP 1: Early Stopping, STEP 2: Early stopping to prevent overfitting
STEP 1: Make a prediction

STEP 1: Load the data, STEP 2: Creating the variable names
STEP 1: Distribution of peaks
STEP 1: Convert data type to list of columns, STEP 2: make histogram for train data
STEP 1: Example of predictions, STEP 2: About the image
STEP 1: import sample submission
STEP 1: Initialize the directory, STEP 2: Load the data
STEP 1: Processing the data
STEP 1: Define the image transformations here
STEP 1: Simple CNN model, STEP 2: Train the model
STEP 1: Open the test images
STEP 1: some config values, STEP 2: resize the image to grayscale
STEP 1: Simple Keras Libraries, STEP 2: Print final result
STEP 1: Compile the loss
STEP 1: save the model in a JSON file
STEP 1: load json and create model
STEP 1: Show some predictions
STEP 1: clean up working directory
STEP 1: function to plot the image and mask
STEP 1: idea from this kernel
STEP 1: Creating tf.data objects
STEP 1: Read in the libraries, STEP 2: Importing the libraries
STEP 1: read in train data

STEP 1: to reduce memory usage
STEP 1: Extracting the column type
STEP 1: Train the model
STEP 1: Plot the distribution of trip duration
STEP 1: Plot the distribution of trip duration

STEP 1: Add new Features
STEP 1: Engineer Numerical Features
STEP 1: plot the important features
STEP 1: Pickup Hour Distribution
STEP 1: Plot the trip duration vs week
STEP 1: Plot the countplot
STEP 1: BUILD BASELINE CNN
STEP 1: Global Training set, STEP 2: The training data, STEP 3: The label is the last feature
STEP 1: find the training inputs
STEP 1: Define the training method
STEP 1: generate validation inputs
STEP 1: Summary of results
STEP 1: For placeholder labels

STEP 1: load and shuffle filenames
STEP 1: split into train and validation filenames




STEP 1: define iou or jaccard loss function
STEP 1: create network and compiler
STEP 1: cosine learning rate annealing, STEP 2: set the learning rate annealing
STEP 1: create train and validation generators
STEP 1: load and shuffle filenames

STEP 1: save dictionary as csv file
STEP 1: Feature importance for the forest



STEP 1: Load Train and Test Data, STEP 2: Prepare the test data

STEP 1: converting these object type columns to integers
STEP 1: drop useless columns
STEP 1: Training the dataset

STEP 1: Split into train and test





STEP 1: Load Train and Test Data, STEP 2: Prepare the test data

STEP 1: converting these object type columns to integers
STEP 1: drop useless columns

STEP 1: Split into train and test


STEP 1: Random Forest Pipeline, STEP 2: Go to top features, STEP 3: Drop useless features
STEP 1: Prepare for Data, STEP 2: Add RandomForestClassifiers to the list, STEP 3: Sentiment Classifier LGBM
STEP 1: Random Forest Pipeline, STEP 2: Sklearn feature importances, STEP 3: Drop useless features

STEP 1: function to clean special words from text, STEP 2: Clean the data

STEP 1: Just importing the necessary libraries




STEP 1: Any results you write to the current directory are saved as output, STEP 2: Setting up Auto Seed, STEP 3: Let it run, STEP 4: GPU is set
STEP 1: You can tune your attention, STEP 2: Training the Neural Network, STEP 3: Any results you write to the current directory are saved as output, STEP 4: Any results you write to the current directory are saved as output
STEP 1: Create a LightGBM RandomForest Classifier, STEP 2: Train the model



STEP 1: the matplotlib way
STEP 1: Baseline CountVectorizer
STEP 1: Plot the results
STEP 1: Building the model
STEP 1: Load the data
STEP 1: Word Cloud for Mixed Assets, STEP 2: the matplotlib way
STEP 1: the matplotlib way
STEP 1: Numbers of records
STEP 1: the matplotlib way
STEP 1: Create a word cloud from a piece of text, STEP 2: the matplotlib way
STEP 1: the matplotlib way
STEP 1: the matplotlib way
STEP 1: the matplotlib way
STEP 1: the matplotlib way
STEP 1: Prepare for data analysis
STEP 1: Convert data type to list of devices, STEP 2: Loop through all devices
STEP 1: Revenue and revenue
STEP 1: Daily Revenue by date
STEP 1: Let us plot this now
STEP 1: Adding target variable

STEP 1: Prepare for Training and Validation, STEP 2: Remove Type Columns
STEP 1: Is there any toxicity
STEP 1: Numerical hemorrhage Type, STEP 2: Iterate over each whale type
STEP 1: Specify the method, STEP 2: A simple kernel that uses a Keras model trained in my local system, STEP 3: Train Validation Split, STEP 4: Computing dataset splitting
STEP 1: Train the model, STEP 2: Accuracy of the loss, STEP 3: Close the prediction and save the loss
STEP 1: Postprocess for post processing, STEP 2: Postprocess for post processing
STEP 1: Import some libraries, STEP 2: to reduce memory usage
STEP 1: set some global variables




STEP 1: decode test set and add to submission file
STEP 1: Define the project, STEP 2: Training the model, STEP 3: Reading the data
STEP 1: Impute competition table, STEP 2: List all rows for a table

STEP 1: Detect hardware , return appropriate distribution, STEP 2: TPUStrategy for distributed training, STEP 3: Number of replicas in the strategy
STEP 1: Load Model into TPU
STEP 1: TurnOff You can not use the internet in this competition, STEP 2: Folders in input directory , those contain all the necessary files
STEP 1: grid is grid, STEP 2: Turn off the previous row, STEP 3: Iterate over the Test Set
STEP 1: grid is grid, STEP 2: Turn off the previous row, STEP 3: Iterate through indices
STEP 1: grid is grid, STEP 2: Turn off the previous row, STEP 3: Iterate and plot random images
STEP 1: Lets see least frequent landmarks
STEP 1: Lets see least frequent landmarks

STEP 1: mechanism to select attributes and check results against tournaments since, STEP 2: Importing libraries, STEP 3: Listing the available files
STEP 1: from sklearn import time, STEP 2: Function to convert date to datetime, STEP 3: Read all files
STEP 1: Protein Interactions with Disease, STEP 2: More To Come
STEP 1: Visualize video files
STEP 1: print basic information on the dataset
STEP 1: print basic information on the dataset
STEP 1: Nilebird visualization, STEP 2: Brain Development Functional Datasets
STEP 1: Initialize DictLearning object
STEP 1: Show networks using plotting utilities
STEP 1: Mean of all correlations
STEP 1: Then find the center of the regions and plot a connectome
STEP 1: Set color palette, STEP 2: Add as an overlay all the regions of index, STEP 3: show the plot
STEP 1: print basic information on the dataset
STEP 1: plot the processed data
STEP 1: Show the predicted probabilities




STEP 1: Get label descriptions
STEP 1: Look at random sample data
STEP 1: Normalize image and label
STEP 1: accuracy in each category
STEP 1: TurnOff You can not use the internet in this competition, STEP 2: import seaborn as sns, STEP 3: skipz feature importance, STEP 4: Importing important functions, STEP 5: Random Forest Importance
STEP 1: There are no
STEP 1: the train dataframe
STEP 1: plotting benign images, STEP 2: Plot oil price















STEP 1: Import libraries and data , reduce memory usage, STEP 2: import libraries for Bayesian Optimization, STEP 3: Light curves are irregularly spaced on the time axis, STEP 4: Any results you write to the current directory are saved as output, STEP 5: Importing relevant packages

STEP 1: Creating a simple tree metric
STEP 1: Public LB Score





STEP 1: Baseline parameters for LightGBM, STEP 2: LGBM Classifier
STEP 1: Number of Team Members

STEP 1: Create submission file, STEP 2: save the submission .csv file
STEP 1: Importing all the basic python libraries, STEP 2: several prints in one cell
STEP 1: Read the input path, STEP 2: Listing the available files
STEP 1: Spliting the data
STEP 1: Write the output for submission
STEP 1: load the data
STEP 1: Remove constant values, STEP 2: Remove duplicate images
STEP 1: Setting the label, STEP 2: Train Validation Split


STEP 1: building the model and compiling it
STEP 1: load best weights, STEP 2: Evaluate on validation set
STEP 1: Load the model
STEP 1: Test filenames, STEP 2: Save file names for prediction, STEP 3: Extract the id from a string, STEP 4: Apply file name to prediction, STEP 5: Last check the head

STEP 1: Create submission file, STEP 2: save the csv file
STEP 1: Nested Building level
STEP 1: Plot the feature ranking
STEP 1: import numpy modules
STEP 1: make a prediction
STEP 1: Plot number of bedrooms

STEP 1: Save predictions to csv, STEP 2: AUC for y, STEP 3: Fast auc function for TPU
STEP 1: There are no, STEP 2: label encoding, STEP 3: Importing relevant packages, STEP 4: Train the model, STEP 5: Ignore the warnings
STEP 1: Load the Data, STEP 2: Convert dataset shape
STEP 1: Catboost Classifier, STEP 2: Train the model
STEP 1: Replace all the outliers in the Forest, STEP 2: Rename the forest
STEP 1: creating list and empty list, STEP 2: replace all rows with TensorType, STEP 3: Add oof types to the forest
STEP 1: wild area map
STEP 1: Histogram of the forest
STEP 1: Histogram of the forest
STEP 1: plot raw weather data
STEP 1: Histogram of highways
STEP 1: Now we use the median of the forest for the forest, STEP 2: how does it look like
STEP 1: What is the distribution of groundways
STEP 1: Histogram of Death points
STEP 1: Plotting the distribution of the forest
STEP 1: cross validation and metrics, STEP 2: Importing the Binary Analysis, STEP 3: import tqdm as pqdm, STEP 4: Ignore the warnings
STEP 1: Training the model, STEP 2: You can tune our labels, STEP 3: Choose what device to run
STEP 1: Reading the train set, STEP 2: plotting benign images, STEP 3: look at random images
STEP 1: Multiple Data Set, STEP 2: Forecast the class
STEP 1: load the data
STEP 1: Open a slide, STEP 2: Read in the past, STEP 3: display the image, STEP 4:  close the test file
STEP 1: credits to Rohit Singh, STEP 2: Overlay the mask on surface images
STEP 1: Importing all the basic python libraries, STEP 2: Dataset and dataloader, STEP 3: Importing the required libraries, STEP 4: Import some stuff
STEP 1: Reading the train set, STEP 2: plotting benign images, STEP 3: Loop over the images
STEP 1: cross validation and metrics
STEP 1: Ensure determinism in the results, STEP 2: initial seed things
STEP 1: FUNCTIONS TAKEN FROM
STEP 1: LOAD PROCESSED TRAINING DATA FROM DISK


STEP 1: SAVE DATASET TO DISK, STEP 2: SAVE DATASET TO DISK
STEP 1: LOAD DATASET FROM DISK
STEP 1: initial seed things, STEP 2: Embedding using the original word index, STEP 3: Calculate the mean embedding matrix, STEP 4: delete our embedding and paragramming, STEP 5: shape of matrix
STEP 1: missing entries in the embedding are set using np.random.normal

STEP 1: The method for training is borrowed from




STEP 1: load the data
STEP 1: This is how the data looks like


STEP 1: load the index of our dataframe, STEP 2: Add embeddings to our index, STEP 3: Close the file
STEP 1: Convert values to embeddings
STEP 1: Simple LSTM model
STEP 1: Train the model
STEP 1: More To Come, STEP 2: Importing libraries, STEP 3: Merge the data into one dataframe
STEP 1: Clean missing values
STEP 1: Define rating function, STEP 2: converting to zero, STEP 3: Let us define the prime numbers, STEP 4: limit over time
STEP 1: Now we can filter the previous values
STEP 1: delete the merchant id groups
STEP 1: Combine train and test
STEP 1: Create the parameters for LGBM

STEP 1: Predictions for each class
STEP 1: split into train and test
STEP 1: Fit on all features
STEP 1: Example of submission
STEP 1: Preprocessing libraries, STEP 2: Animal Encoding, STEP 3: Encode categorical features
STEP 1: Importing the libraries
STEP 1: Reading the test data, STEP 2: Train the data
STEP 1: Function for getting data from file, STEP 2: Looking at the element data
STEP 1: add averaged properties to DataFrame
STEP 1: converting angles into lattice angles, STEP 2: put all lattice angles in degrees
STEP 1: Distribution for some features
STEP 1: plot distribution of current column
STEP 1: plot distribution of current column
STEP 1: define list of features
STEP 1: Perform Principal Component Analysis
STEP 1: Define RMSL Error Function
STEP 1: Deep Learning Begins ..
STEP 1: Define RMSL loss function, STEP 2: Compile a model

STEP 1: Set up LightGBM models



STEP 1: Import the necessary packages, STEP 2: This was copied from

STEP 1: Training the stacked models
STEP 1: fillna as missing column, STEP 2: Filling the missing values
STEP 1: revenue for Release month
STEP 1: Generate a mask for the upper triangle
STEP 1: columns can be inserted into a dictionary, STEP 2: function to convert text data to dict, STEP 3: convert text into dictionary
STEP 1: We can visualize the movie title in the train set
STEP 1: Train and predict
STEP 1: Here are the skew values
STEP 1: Fitting a lot of values, STEP 2: Revenue by power
STEP 1: Logarithmic transform of the values
STEP 1: Splitting the data into train and test data
STEP 1: Training the model
STEP 1: Load ICN numbers, STEP 2: Convertion numbers into indices
STEP 1: initialize label matrix, STEP 2: converting fnc variables to integers
STEP 1: initialize the confusion matrix, STEP 2: iterate over all features

STEP 1:  discrete features from scores
STEP 1: storing results per domain
STEP 1: Reading the data, STEP 2: Split the data into train and test data

STEP 1: Training the masker, STEP 2: Loading the matlab for participants
STEP 1: Convert to dataframe, STEP 2: Categorical Features, STEP 3: Concatenate all PCA
STEP 1: Test scores are equal
STEP 1: Absolute Normalization function
STEP 1: Prepare for submission
STEP 1: Preparing the model, STEP 2: Run the baseline function
STEP 1: skimage image processing packages
STEP 1: Prepare the data
STEP 1: Function for reading DICOM, STEP 2: Data loading and overview, STEP 3: convert to float, STEP 4: Creation of the image with respect to float, STEP 5: remove small part of image

STEP 1: What about the images

STEP 1: Pad the image to be padded
STEP 1: Importing important packages and data, STEP 2: Importing libraries, STEP 3: Listing the available files
STEP 1: Lets see if these features are highly correlated
STEP 1: Plot some histograms



STEP 1: TRAIN ERROR LIST
STEP 1: We keep only RGB channels as Alpha channel is always empty, STEP 2: Load the images from the data, STEP 3: Show some examples
STEP 1: Any results you write to the current directory are saved as output
STEP 1: Load the Data
STEP 1: Fit and Transform
STEP 1: Merge text into train and test set
STEP 1: Code from Whale Classification Model, STEP 2: mechanism to select attributes and check results against tournaments since, STEP 3: import category encoding, STEP 4: Simple Imputer with sklearn, STEP 5: Importing the Linear model, STEP 6: Light GBM regressor, STEP 7: import lightgbm as lgb, STEP 8: Modified to add option to use float
STEP 1: Merge Datasets and extract Date time features
STEP 1: Logarithmic transform of target values
STEP 1: Encode categorical features, STEP 2: Average the categorical features
STEP 1: Impute missing values
STEP 1: add new features
STEP 1: Prepare the data, STEP 2: Average the categorical features, STEP 3: Preprocess the test data
STEP 1: start to prediction, STEP 2: Accumulate the test data, STEP 3: Delete the test and save our model
STEP 1: Make the submission .csv file
STEP 1: Time Series plotting, STEP 2: Time Series plot, STEP 3: save the csv files
STEP 1: Fit the model, STEP 2: Define the image transformations here


STEP 1: Explore the sample size, STEP 2: Test data size



STEP 1: Split the data into train and test data, STEP 2: Specific basic information, STEP 3: check some types of train labels
STEP 1: Random Forest Importance
STEP 1: Importing all libraries
STEP 1: pip install googletrans
STEP 1: Load train data, STEP 2: Read in the store
STEP 1: Load train data
STEP 1: Read Test Data
STEP 1: RF regressor model
STEP 1: Lets return a list of ranked features identified, STEP 2: Applying to the ranks list
STEP 1: Plot the feature importances of the forest
STEP 1: PLOT Batch Normalization


STEP 1: The notebook is forked from
STEP 1: Deep Learning Essential, STEP 2: Instantiate the estimator, STEP 3: Train the model
STEP 1: If submission is a single prediction
STEP 1: The notebook is forked from
STEP 1: Deep Learning Essential, STEP 2: Instantiate the estimator, STEP 3: Train the model

STEP 1: Set up mask, STEP 2: Do the augmentation



STEP 1: View original image, STEP 2: converting image to grayscale
STEP 1: Converting the mask to numpy array, STEP 2: array to array, STEP 3: Create mask for each label, STEP 4: Visualize some Labels
STEP 1: DISPLAY NN color

STEP 1: Open the second cell
STEP 1: RLE encoding , as suggested by Tadeusz Hupa≈Ço, STEP 2: Define the RLE encoding



STEP 1: Importing all the basic python libraries, STEP 2: load data from csv file, STEP 3: subset expedition data set, STEP 4: sort by time, STEP 5: Frequency of Submission
STEP 1: Load some column names
STEP 1: Distribution of data


STEP 1: Train Validation Split
STEP 1: Transforming the data
STEP 1: Define a Logistic Regression model
STEP 1: Save vectoriser, STEP 2: Save the model weights, STEP 3: Save the model weights





STEP 1: Based on Thanks
STEP 1: get the data fields ready for stacking
STEP 1: We can also display a spectrogram using librosa.display.specshow
STEP 1: Display the log spectrogram
STEP 1: Zero Crossing Rate
STEP 1: Zero Crossing Rate
STEP 1: Plot the spectrum


STEP 1: copy images to the original image, STEP 2: Compute high valores, STEP 3: The Lunging pixel of the pixels
STEP 1: Standardize the pixel values

STEP 1: Group title by title
STEP 1: QUENCY AND Numerical columns
STEP 1: Group by game session
STEP 1: Distribution of Event Text
STEP 1: Frequency of Type, STEP 2: Frequency of Type
STEP 1: Frequency of World, STEP 2: Frequency of World
STEP 1: Frequency of World, STEP 2: Frequency of World
STEP 1: Training and testing data
STEP 1: week of year
STEP 1: Distribution of title
STEP 1: In train data
STEP 1: Plot Game time
STEP 1: world time vs type
STEP 1: What about the World
STEP 1: Frequency of each feature, STEP 2: What about the columns
STEP 1: Frequency of each feature, STEP 2: What about the columns
STEP 1: Frequency of World



STEP 1: Start with the sample submission values

STEP 1: Load the data
STEP 1: We create the classifier


STEP 1: plotting benign images, STEP 2: Binary countplot for feature, STEP 3: show the plot
STEP 1: plotting benign images, STEP 2: Count plot for all other features, STEP 3: show the plot
STEP 1: Read submission file
STEP 1: get the data fields ready for stacking
STEP 1: get the data fields ready for stacking
STEP 1: Loading all submission files
STEP 1: How does this work
STEP 1: some config values
STEP 1: fill up the missing values
STEP 1: Tokenize the sentences
STEP 1: Pad the sentences
STEP 1: plot the important features
STEP 1: Function for getting rewards
STEP 1: Get the train dataframe
STEP 1: Clip the target range

STEP 1: Load data as pandas dataframe
STEP 1: Map Numerical Results
STEP 1: Prepare the Data
STEP 1: Save submission duration
STEP 1: Save test prediction for LGBM baseline
STEP 1: Prepare the Data
STEP 1: Save submission duration
STEP 1: Save test prediction for LGBM
STEP 1: Prepare the Data
STEP 1: Save submission duration
STEP 1: Save test prediction for LGBM
STEP 1: load the data
STEP 1: the train dataframe
STEP 1: plot the distribution of the input data, STEP 2: show the plot
STEP 1: Create the input data, STEP 2: the train data set
STEP 1: Extract activation dates
STEP 1: Deal Probability Distribution for First SVD component
STEP 1: Deal Probability Distribution of the Second SVD component
STEP 1: Deal Probability Distribution of the Second SVD component
STEP 1: Deal Probability of First SVD component
STEP 1: Deal Probability Distribution of the Second SVD component
STEP 1: Deal Probability of second SVD component
STEP 1: Splitting the data for model training
STEP 1: Making a submission file
STEP 1: plot feature importances
STEP 1: Split the train dataset into development and valid based on time
STEP 1: Draw the heatmap using seaborn
STEP 1: Prepare the Data
STEP 1: Create a submission file
STEP 1: Set the target variable, STEP 2: Visualizing the target variable
STEP 1: For each card
STEP 1: plot distribution of weather
STEP 1: Lets plot the cloud coverage
STEP 1: How many bedrooms per bedrooms, STEP 2: Distribution of number of occurrences
STEP 1: Scatter plot of price
STEP 1: Filling it with ulimit, STEP 2: price plot
STEP 1: Wordcloud for DisplayAddress
STEP 1: move the data to ulimit, STEP 2: Plot the distribution of the target value
STEP 1: Train Set Missing Values
STEP 1: Calculate xgb score, STEP 2: import xgb as xgb
STEP 1: plot the important features
STEP 1: custom function for ngram generation
STEP 1: custom function for horizontal bar chart
STEP 1: initialize frequency dictionary for frequency, STEP 2: Create frequency of questions, STEP 3: Get the horizontal bar chart
STEP 1: initialize frequency dictionary for frequency, STEP 2: Create frequency of questions, STEP 3: Get the horizontal bar chart
STEP 1: Creating two subplots
STEP 1: Creating two subplots
STEP 1: Creating two subplots
STEP 1: Get the tfidf vectors
STEP 1: Specific helper functions
STEP 1: Frequency distribution by prior order
STEP 1: Distributions of priors
STEP 1: Departments distribution for each Department
STEP 1: Read the files
STEP 1: Players registered on the training data
STEP 1: Scatter plot of Cats vs Yards
STEP 1: Random Forest Importance
STEP 1: Load Data Set
STEP 1: plot the important features
STEP 1: Floor We will see the count plot of floor variable
STEP 1: Now let us see how the price changes with respect to floors
STEP 1: Now lets separate the month, STEP 2: Are there seasonal patterns to the number of transactions
STEP 1: Lets plot of locations for each location
STEP 1: To reduce memory usage, STEP 2: reset data type
STEP 1: Train Set Missing Values
STEP 1: Draw the heatmap using seaborn
STEP 1: Plot the countplot
STEP 1: We can see that number of bedrooms are very high
STEP 1: Plot the data
STEP 1: position on lat
STEP 1: plot the important features
STEP 1: Setting the text text, STEP 2: Convert text into string, STEP 3: Wordcloud for tags
STEP 1: Load the Data
STEP 1: Number of punctuations by author
STEP 1: Prepare the data for modeling
STEP 1: plot the important features
STEP 1: Get the tfidf vectors
STEP 1: Get the tfidf vectors
STEP 1: Get the tfidf vectors
STEP 1: add the predictions as new features
STEP 1: add the predictions as new features
STEP 1: add the predictions as new features
STEP 1: plot the important features
STEP 1: generate a confusion matrix, STEP 2: Plot the confusion matrix
STEP 1: Kagglegym import .., STEP 2: Resize the environment, STEP 3: Get the training data
STEP 1: Estimate the environment, STEP 2: Train the model
STEP 1: Setting up environment, STEP 2: Train the model
STEP 1: Setting up the environment, STEP 2: Train the model
STEP 1: Detect Duplicates, STEP 2: Target Variable Exploration
STEP 1: Target frequency distribution
STEP 1: Now we will plot the correlation matrix, STEP 2: Draw the heatmap using seaborn
STEP 1: Draw the heatmap using seaborn
STEP 1: Loading the data
STEP 1: Read the image file and convert to int
STEP 1: Get the name of the landmark
STEP 1: plotting the original image
STEP 1: Load the Data
STEP 1: define the vectorsizer
STEP 1: Transforming the comment text into vectors
STEP 1: Identify an individual column
STEP 1: Load the Data
STEP 1: plot distribution of formation energy
STEP 1: We visualize the correlation matrix
STEP 1: extracting radians from degree degree
STEP 1: Removing punctuation from the text
STEP 1: Or just override the analyzer totally with our preprocess text
STEP 1: Make a submission file
STEP 1: Create submission file
STEP 1: Get rid of unwanted features
STEP 1: Create submission file
STEP 1: load the data
STEP 1: Modelling the Missing Values
STEP 1: Load the data
STEP 1: Checking for Missing Values
STEP 1: Filling No Missing Values
STEP 1: defining some useful features, STEP 2: Basic basic details
STEP 1: Drop target variable
STEP 1: Create submission file
STEP 1: Load the data
STEP 1: filling missing values
STEP 1: Prepare the data
STEP 1: Make a submission file
STEP 1: plot the heatmap
STEP 1: Checking Best Feature for Final Model
STEP 1: Split the data
STEP 1: Create submission file


STEP 1: Convert values to embeddings
STEP 1: fill zero values for optimization
STEP 1: Set infinite costs
STEP 1: some config values



STEP 1: Duplicate image identification


STEP 1: set h to all, STEP 2: For each image id , determine the list of pictures



STEP 1: Store the mapping as a dictionary, STEP 2: For each whale , find the unambiguous images ids, STEP 3: Sort each whale into a list





STEP 1: Evaluate the model

STEP 1: format the v

STEP 1: Deleting unnecessary features
STEP 1: Preprocess the dataframe, STEP 2: Pytorch Data Loader, STEP 3: fillna categorical features, STEP 4: First check the head
STEP 1: Adding Cabin features
STEP 1: Reading the input Files from their respective Directory, STEP 2: Merging the datasets
STEP 1: Not fraud results, STEP 2: Downsampling blacklist, STEP 3: Downsampling and fraudpled files, STEP 4: Downsampling Percent

STEP 1: Plot the distribution of residuals
STEP 1:  legend
STEP 1: It means that We were able to make a good model
STEP 1:  legend
STEP 1: It means that We were able to make a good model
STEP 1: Use the forecasting data
STEP 1: Distribution of top countries
STEP 1: Confirmed Cases of Correlation
STEP 1: Plotting Fatalities of Correlation
STEP 1: Number of confirmed cases
STEP 1: Plot the Confirmed cases
STEP 1: Plot the Confirmed cases
STEP 1: Diffirmed Cases during the time series
STEP 1: Calculate the total number of holidays, STEP 2: Create an array of indeces, STEP 3: Drop predicted cases
STEP 1: Checking missing values
STEP 1: Check for Missing Values
STEP 1: What are the value counts for each column
STEP 1: checking number of occurences

STEP 1: Clean NaNs, STEP 2: Filling the missing values with median
STEP 1: Random Forest Importance
STEP 1: use cached rdkit mol object to save memory, STEP 2: Call garbage collector

STEP 1: Linear optimizer, STEP 2: Set the optimizer
STEP 1: Import the required libraries, STEP 2: Set image size, STEP 3: Define the image generator, STEP 4: Create the generator, STEP 5: Create the generator for the validation set
STEP 1: reset the test generator, STEP 2: Apply the model on the test set, STEP 3: Get the indices of the predicted class values
STEP 1: Apply model to test set and output predictions






STEP 1: Lets remove Labels, STEP 2: Clean the question text
STEP 1: Checking missing values
STEP 1: Check for Missing Values
STEP 1: What are the value counts for each column
STEP 1: checking number of occurences

STEP 1: Clean NaNs, STEP 2: Filling the missing values with median

STEP 1: Function for label encoding, STEP 2: Add label in text
STEP 1: Global Training settings
STEP 1: Lets remove Labels
STEP 1: Load the data, STEP 2: Quick check on dimensions






STEP 1: Building the model, STEP 2: Cross entropy loss, STEP 3: Define the optimizer

STEP 1: sample in training and test set
STEP 1: Generate a grid for the predictions
STEP 1: Now we will predict on the test data, STEP 2: Test feature extraction, STEP 3: Create test images, STEP 4: remove test image, STEP 5: Extracting test image
STEP 1: Number of confirmed cases
STEP 1: Distribution of confirmed cases



STEP 1: plot distribution of the transaction
STEP 1: F fraud by product category
STEP 1: F fraud by Card Network
STEP 1: F fraud by Card Type
STEP 1: Number of fraud domain, STEP 2: Reporting of the fraud rate
STEP 1: Get the major features, STEP 2: visualize categorical features

STEP 1: Create arrays and dataframes to store results


STEP 1: drop the relevant features
STEP 1: drop the relevant features
STEP 1: Training the model
STEP 1: Prepare submission dataframe
STEP 1: Exploring the data, STEP 2: Read in the Data
STEP 1: Code from Whale Classification Model
STEP 1: Encode existing ship
STEP 1: Yes , there is no missing ship
STEP 1: Checking for missing ship
STEP 1: Check for missing values
STEP 1: one hot encode the targets
STEP 1: Split data into train and test
STEP 1: Any results you write to the current directory are saved as output
STEP 1: One hot encode
STEP 1: drop column name, STEP 2: Separate data for ordinal feature
STEP 1: XOR with xor
STEP 1: Add HF features, STEP 2: Splitting the data
STEP 1: Any results you write to the current directory are saved as output
STEP 1: load validation data, STEP 2: loop over each image
STEP 1: load test data
STEP 1: reading all submission files
STEP 1: drop column name, STEP 2: Separate data for ordinal feature
STEP 1: XOR with xor
STEP 1: Add HF features, STEP 2: Splitting the data
STEP 1: Any results you write to the current directory are saved as output
STEP 1: Get user credentials
STEP 1: Create the TPU


STEP 1: Any results you write to the current directory are saved as output



STEP 1: Generate a classification model, STEP 2: Split data into train and test data, STEP 3: Split the dataset into training and validation, STEP 4: Checking the shape of the data
STEP 1: Spliting the data
STEP 1: Classifiers to use

STEP 1: Logistic Regression CV, STEP 2: Fit the log regressor
STEP 1: Logistic Regression model

STEP 1: params we will probably want to do some hyperparameter optimization later


STEP 1: Now we can add some parameters to our sample
STEP 1: Preprocess the data


STEP 1: plotly offline imports
STEP 1: load dataframe with train labels
STEP 1: Define some examples, STEP 2: plotting a pie chart, STEP 3: show the plot
STEP 1: Plot the pie chart for the train and test datasets
STEP 1: Analysis on the Data
STEP 1: plotting a pie chart
STEP 1: Function to get dummy values for cloud type
STEP 1: fill dummy columns
STEP 1: plotting a pie chart
STEP 1: Find out correlation between columns and plot
STEP 1: get sizes of images from test and train sets



















STEP 1: Importing the libraries, STEP 2: new pretrained model, STEP 3: Get the model
STEP 1: plot random image
STEP 1: Let us plot this now
STEP 1: Same for test
STEP 1: fill the missing values with
STEP 1: Here I merge the data
STEP 1: flatten list of strings
STEP 1: Prepare array of labels and vector representation of labels, STEP 2: convert to uint
STEP 1: Splitting the data into train and test data, STEP 2: Splitting the data into train and test data
STEP 1: Any results you write to the current directory are saved as output, STEP 2: Resize each hair images


STEP 1: Create hair augmentation
STEP 1: Revenue between species, STEP 2: Time Series Analysis
STEP 1: convert latitude and latitude to integer coordinates
STEP 1: Lets plot some few positive examples
STEP 1: Store numerical values, STEP 2: Save categorical variables, STEP 3: Number of features
STEP 1: Separating the feature names

STEP 1: obtain Feature Selection
STEP 1: Load the data, STEP 2: Adding date column


STEP 1: Log the predicted values
STEP 1: Converting data to test dataframe
STEP 1: Loading the data



STEP 1: This was copied from

STEP 1: Loading the data
STEP 1: Find missing values, STEP 2: find missing values
STEP 1: Check the number of submissions per customer

STEP 1: Load the data
STEP 1: Define evaluation examples, STEP 2: Plotting some examples
STEP 1: Set columns to most suitable type to optimize for memory usage



STEP 1: Things to use, STEP 2: Mask first layer, STEP 3: Combine with mask
STEP 1: Import libraries we need, STEP 2: Things to use



STEP 1: Preprocessing libraries
STEP 1: One hot encoding the words
STEP 1: Setting up a validation strategy, STEP 2: Setting up a validation strategy
STEP 1:  reshape and resizing
STEP 1: Transform training set
STEP 1: Preparing the test data
STEP 1: Train the model, STEP 2: Plot the images
STEP 1: Train the test image, STEP 2: Plot the images
STEP 1: Transform training set
STEP 1: Preparing the test data
STEP 1: Train the model, STEP 2: Plot the images
STEP 1: Train the test image, STEP 2: Plot the images
STEP 1: merge train and test
STEP 1: Target variable, STEP 2: Use the Keras tokenizer, STEP 3: Maximum sequence length, STEP 4: Tokenize the sentences
STEP 1: Maximum sequence length, STEP 2: Tokenize the sentences



STEP 1: Drop columns that are not features

STEP 1: Training the model
STEP 1: Fit the model


STEP 1: Drop columns that are not features

STEP 1: Function to merge the results
STEP 1: merge train and test and test, STEP 2: Generate random sample, STEP 3: Separating the data
STEP 1: Read in the data

STEP 1: Transfer calendar calendar
STEP 1: More To Come
STEP 1: Relationship between popularity and revenue
STEP 1: Converting data into a dictionary, STEP 2: convert json columns into dictionary, STEP 3: Loop over all rows, STEP 4: Save result as a dictionary
STEP 1: Set up a LightGBM model
STEP 1: Light GBM error, STEP 2: Fitting the model, STEP 3: Evaluation of Validation Set
STEP 1: Summary of experiments
STEP 1: Frequency between Male and Female
STEP 1: Get all patients in our dataset, STEP 2: Age per patient
STEP 1: It creates a pie chart
STEP 1: Distribution for Percent


STEP 1: Load train data
STEP 1: Load test data
STEP 1: Importing libraries, STEP 2: Importing the libraries
STEP 1: Only load those columns in order to save space

STEP 1: Number of teams by Date
STEP 1: Top Leaderboard Scores
STEP 1: Create Top Teams List
STEP 1: Count of LB Submissions that improved score
STEP 1: Convert to float format
STEP 1: loop over all the attributes
STEP 1: Training the model
STEP 1: convert to set of predictions
STEP 1: Ensure determinism in the results
STEP 1: select a class
STEP 1: Plot the heatmap
STEP 1: Import necessary libraries, STEP 2: pip install googletrans, STEP 3: feature extractoring and preprocess the model, STEP 4: code takesn from


STEP 1: This augmentation is a wrapper of librosa function
STEP 1: Same as above

STEP 1: for albumentations, STEP 2: Training augmentation
STEP 1: Defining the function for evaluating ROC curve

STEP 1: using keras tokenizer here
STEP 1: zero pad the sequences

STEP 1: Load the data into a dictionary, STEP 2: Intialize the input string, STEP 3: Close the file
STEP 1: create a embedding matrix for the words index, STEP 2: Get the embedding matrix


STEP 1: Implementing the Model, STEP 2: Train the model

STEP 1: Machine Learning Specific, STEP 2: Typical WordPiece tokenizer
STEP 1: Load text data into DataFrame

STEP 1: Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer


STEP 1: Linear network exploration
STEP 1: Setting up some basic configuration


STEP 1: basic training configuration




STEP 1: plotting benign images, STEP 2: show some images, STEP 3: Show some plots
STEP 1: Check actual values
STEP 1: Check actual values
STEP 1: For merging features
STEP 1: Loading the data
STEP 1: Function for cleaning the text
STEP 1: Segregating the sentiment
STEP 1: MosT common positive words
STEP 1: MosT common positive words
STEP 1: Top common words
STEP 1: Exploring the data




STEP 1: Setting positive , negative ,, STEP 2: Train the model
STEP 1: Read the train , test and sub files
STEP 1: Initialize plain dictionary, STEP 2: Put raw text into dict, STEP 3: Print the length of plain dictionary

STEP 1: This variable is used by Bayesian Optimization
STEP 1: Importing the necessary libraries, STEP 2: To display some figures
STEP 1: Full cipher text
STEP 1: Handle missing values, STEP 2: HANDLE MISSING VALUES, STEP 3: Handle missing values
STEP 1: SCALE target variable
STEP 1: EXTRACT DEVELOPTMENT TEST
STEP 1: FITTING THE MODEL
STEP 1: Import the corpus using nltk, STEP 2: Function for cleaning the sentence, STEP 3: clean the sentence
STEP 1: XGBRegressor import, STEP 2: cretrieve the cross validation set
STEP 1: Import libraries , reduce memory usage, STEP 2: Importing important packages
STEP 1: plot distribution on top of the input data
STEP 1: plotting benign images, STEP 2: look at total energy for each type
STEP 1: Function to determine outlier score
STEP 1: Boilerplate Code Essential imports, STEP 2: Any results you write to the current directory are saved as output, STEP 3: Importing the necessary Packages, STEP 4: Simple Pytorch Model
STEP 1: Get list of all test task files, STEP 2: process test tasks, STEP 3: Load all test tasks
STEP 1: the training set, STEP 2: Apply the test tasks
STEP 1: Mean of the matrix
STEP 1: flatten the prediction values
STEP 1: convert list of lists to list of lists, STEP 2: Flatten the test predictions, STEP 3: Prepare submission file
STEP 1: Public LB Score, STEP 2: import general packages, STEP 3: Ignore the warnings, STEP 4: Importing the libraries, STEP 5: Deep Learning Begins ..
STEP 1: Load the data
STEP 1: This variable is NOT listed as categorical , but clearly is
STEP 1: plotting benign images, STEP 2: Finding Fraud domain, STEP 3: Set the colors
STEP 1: plotting benign images, STEP 2: In Fraud domain, STEP 3: Set the color palette
STEP 1: Ploting Card Features
STEP 1: plotting benign images, STEP 2: Understanding Card Features, STEP 3: Set the color palette
STEP 1: Check card balance
STEP 1: plotting benign images, STEP 2: Proportion of Fraud amount, STEP 3: set the colors
STEP 1: Function to prepare the data
STEP 1: Setting the X and y
STEP 1: Create the parameters for the model, STEP 2: Creating the data structures, STEP 3: Train the model with early stopping
STEP 1: To plot pretty figures, STEP 2: plot the important features
STEP 1: summarize history for accuracy
STEP 1: summarize history for loss
STEP 1: Set global parameters
STEP 1: Number of characters in the dataframe
STEP 1: Distribution of Words
STEP 1: Average Word Length
STEP 1: Use the tokenizer
STEP 1: text version of squash , slight different from original one
STEP 1: Save word index as JSON
STEP 1: Import libraries , reduce memory usage, STEP 2: Importing the metric, STEP 3: Light curves are irregularly spaced on the time axis, STEP 4: English is not available, STEP 5: Importing the libraries, STEP 6: import color code

STEP 1: Load the Data
STEP 1: Load the data
STEP 1: Is it toxic, STEP 2: Obtain the score, STEP 3: Print the mean absolute error
STEP 1: Is it toxic, STEP 2: Logistic Error of toxicity, STEP 3: Print the mean squared error
STEP 1: take a dictionary of keys and values
STEP 1: No Penalty Version
STEP 1: Cross entropy loss, STEP 2: The accuracy of the classifier
STEP 1: Turn on the test set, STEP 2: predict on test set
STEP 1: from tqdm import tqdm, STEP 2: Importing the data, STEP 3: import nltk as prenet, STEP 4: Reading the data
STEP 1: Function for removing numbers
STEP 1: Replaces repetitions of exlamation marks, STEP 2: Replaces repetitions of question marks, STEP 3: Replaces repetitions of stop marks


STEP 1: Replacing a word, STEP 2: Function to replace frequent words with regularized words
STEP 1: Training the Neural Network, STEP 2: Evaluating the model
STEP 1: Split the data, STEP 2: Split the data, STEP 3: Split the data
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: Predict on validation set, STEP 2: Predict on train data
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: add the bar mode
STEP 1: Change the bar mode
STEP 1: Predict on validation set, STEP 2: Predict on train data
STEP 1: Import libraries and utils, STEP 2: Import libraries and read in data, STEP 3: import nltk tokenizer, STEP 4: Light curves are irregularly spaced on the time axis, STEP 5: Ignore the warnings
STEP 1: some config values
STEP 1: reset acoustic data
STEP 1: Prepare the data, STEP 2: Generate audio data, STEP 3: Delete unwanted data, STEP 4: convert the list of inputs to numpy array
STEP 1: For the ts
STEP 1: function to prepare the data
STEP 1: Prepare the data, STEP 2: Load all the data, STEP 3: load all the features
STEP 1: Plot the joint entropy
STEP 1: Plot the data

STEP 1: Plot the plot
STEP 1: Plot the data


STEP 1: Plotting a joint plot
STEP 1: Plot the data
STEP 1: Load libraries and data
STEP 1: reset acoustic data
STEP 1: Define the mean value of a dictionary of floats

STEP 1: some config values
STEP 1: reset acoustic data
STEP 1: Prepare the data, STEP 2: Generate audio data, STEP 3: Delete unwanted data, STEP 4: convert the list of inputs to numpy array
STEP 1: For the ts
STEP 1: function to prepare the data
STEP 1: Prepare the data, STEP 2: Load all the data, STEP 3: load all the features

STEP 1: Plot the spectrogram
STEP 1: Plot the spectrogram


STEP 1: Plotting the joint plot
STEP 1: Plot the distribution of the sample entropy

STEP 1: Plotting the detrenduation
STEP 1: Plotting the detrenduation
STEP 1: Load the data
STEP 1: Errors in regression set
STEP 1: There are no
STEP 1: Get train images, STEP 2: Read in each image, STEP 3: Convert the list of input images into array
STEP 1: load label dictionary, STEP 2: from the above kernel
STEP 1: Prepare the Data, STEP 2: Convert data into list of attributes, STEP 3: Convert the list of train targets to array
STEP 1: plotting benign images, STEP 2: plot individual examples
STEP 1: Basics for training, STEP 2: basic training configuration, STEP 3: set some parameters
STEP 1: Load the data
STEP 1: For later use, STEP 2: Process gleason string, STEP 3: Process gleason score
STEP 1: Define the model
STEP 1: Cross entropy loss, STEP 2: Accuracy of the input data
STEP 1: Daily training data
STEP 1: Plotting the Yards
STEP 1: Plot Yards
STEP 1: Plotting the data
STEP 1: Histogram of Sensity Density
STEP 1: one hot encoding, STEP 2: iterate through categorical features and assign values to dict
STEP 1: Converting the categorical features into indices, STEP 2: Convert given index to binary class labels
STEP 1: numeric columns in a list, STEP 2: Function to get numerical features
STEP 1: Build the CNN
STEP 1: Constants and defaults, STEP 2: Statistics per batch, STEP 3: Compute mean and standard deviation
STEP 1: Function for generating nonAN, STEP 2: Common words in comments
STEP 1: Country language by country, STEP 2: Average comment length by country
STEP 1: Heathistogram of Correlation, STEP 2: Change the bar mode
STEP 1: In Trajections, STEP 2: Distribution plot for FVC, STEP 3: Change the bar mode
STEP 1: Frequency EACH READING, STEP 2: Change the bar mode
STEP 1: Take Care of Filtering, STEP 2: Distribution plot for FVC, STEP 3: Change the bar mode
STEP 1: Distribution of Auto Readability, STEP 2: Change the bar mode
STEP 1: Taking care of them, STEP 2: Distribution plot for FVC, STEP 3: Change the bar mode
STEP 1:  Pie chart of labels
STEP 1: Setting the diagnostic variables, STEP 2: Create strategy from tpu, STEP 3: Kaggle Datasets, STEP 4: Training the model
STEP 1: Instantiating the tokenizer from DistilBERT model, STEP 2: Create a save file, STEP 3: create the save path, STEP 4: Save the loaded tokenizer locally, STEP 5: Early stopping tokenizer
STEP 1: Encoding the data, STEP 2: Splitting the train and validation sets
STEP 1: Creating tf.data objects, STEP 2: Converting data into Tensordata for TPU processing, STEP 3: Converting the test data into tensor
STEP 1: Model initialization and fitting on train and valid sets, STEP 2: summarize the VNN model
STEP 1: Callback that streams epoch result
STEP 1: This variable is for NN, STEP 2: Train the model
STEP 1: Start the CNN, STEP 2: summarize the CNN model
STEP 1: Train the model
STEP 1: Model initialization and fitting the LSTM model, STEP 2: summarize the model
STEP 1: Train the model
STEP 1: Start the model, STEP 2: summarize the model
STEP 1: Train the model
STEP 1: Model initialization and fitting on the TPU, STEP 2: summarize the model
STEP 1: Train the model
STEP 1: basic hyperparameters
STEP 1: Set some parameters, STEP 2: Load the data
STEP 1: Loading the image, STEP 2: Load the images
STEP 1: Distribution of channel values
STEP 1: Red Channel Values
STEP 1: Green Channel Values
STEP 1: Blue Channel Values
STEP 1: TPU or GPU detection, STEP 2: Create strategy from tpu, STEP 3: Setup data pipeline
STEP 1: Return the path of the image file, STEP 2: Path for plotting, STEP 3: Train Validation Split
STEP 1: Define learning rate scheduler
STEP 1: basic training configuration, STEP 2: the training and validation set
STEP 1: Cross entropy loss, STEP 2: Define the accuracy of our model
STEP 1: Load the losses

STEP 1: basic training configuration, STEP 2: basic hyperparammeters
STEP 1: Initialize path variables, STEP 2: iterate over each image in a list
STEP 1: Linear Corellation validation loss, STEP 2: Accuracy of the error
STEP 1: Number of targets per target, STEP 2: Set up weights
STEP 1: the train and validation sets, STEP 2: Create Valid Datasets
STEP 1: Whether to shuffle the samples, STEP 2: for shuffling
STEP 1: Define the model
STEP 1: Look at Numpy Data
STEP 1: Setting figure parameters
STEP 1: Setup the training set
STEP 1: Evaluate Cross Validation
STEP 1: How to check
STEP 1: Setting figure parameters
STEP 1: Generating the countplot
STEP 1: Get random index, STEP 2: Loop over raw data
STEP 1: Load the data
STEP 1: Printing all the outputs
STEP 1: filtering out insincere questions
STEP 1: Removing the sincere text
STEP 1: Importing the scoring functions, STEP 2: AUC for TTA
STEP 1: Extracting the hits

STEP 1: Load labels data
STEP 1: Load labels data
STEP 1: Now , lets play around a video
STEP 1: Function for reading video
STEP 1: Exploratory Data Analysis, STEP 2: Importing the library, STEP 3: import seaborn as png, STEP 4: Summarize the data, STEP 5: Importing the plot
STEP 1: Importing the necessary Packages
STEP 1: Filter Binary Features, STEP 2: Number of zeros in each column
STEP 1: Prepare the Data

STEP 1: Distplot for yaw
STEP 1: Frequency of class




















































STEP 1: Load a scene
STEP 1: lets see the sample data
STEP 1: grab the top sample
STEP 1: Setting the test dataset, STEP 2: Test Data Analisys
STEP 1: Remove Drift from Training Data

STEP 1: Prepare for data exploration, STEP 2: to reduce memory usage
STEP 1: The label encoding, STEP 2: Encode categorical features, STEP 3: Final number of the column



STEP 1: Filter out low frequencies, STEP 2: Filter out low frequencies




STEP 1: mechanism to select attributes and check results against tournaments since, STEP 2: Importing libraries, STEP 3: Listing the available files, STEP 4: Any results you write to the current directory are saved as output
STEP 1: Any results you write to the current directory are saved as output




STEP 1: big part of feature engineering
STEP 1: Load all the data as pandas Dataframes
STEP 1: Start by Looking at Historic Tournament Seeds


STEP 1: Define the Gini metric
STEP 1: Understanding the distribution of the transaction data

STEP 1: mechanism to select attributes and check results against tournaments since, STEP 2: Load Training Data
STEP 1: Remove Training and Validation set
STEP 1: Create aggregated dataframe, STEP 2: converting the train data to datetime
STEP 1: Matching function between the ISO code and country names, STEP 2: Continuous Features
STEP 1: Line plot on each country
STEP 1: Line plot on each country
STEP 1: Plot the Fatalities over Country
STEP 1: import geopandas as png, STEP 2: Loading the shapefile, STEP 3: Remove rows from the dataframe
STEP 1: Matching function between the ISO code and country names
STEP 1: import seaborn as sns, STEP 2: Display the distribution of hits
STEP 1: Preparing the submission file
STEP 1: Define val, STEP 2: Fills in the long form
STEP 1: Display Confirmed Cases
STEP 1: Filter Iran data
STEP 1: convert values into numpy array












STEP 1: Load the data
STEP 1: from tqdm import tqdm, STEP 2: We define the Gini metric, STEP 3: We calculate the Gini coefficient for the predictions, STEP 4: Training the Kini Score
STEP 1: Prepare data for classification





STEP 1: mechanism to select attributes and check results against tournaments since, STEP 2: Light curves are irregularly spaced on the time axis, STEP 3: Folders in input directory , those contain all the necessary files, STEP 4: Importing the required libraries, STEP 5: import some stuff
STEP 1: punctuations and other stopwords, STEP 2: Function for spacy tokenizer
STEP 1: Flesch reading data
STEP 1: summarize all text, STEP 2: Consensus of Sincere
STEP 1: Actual vectorization of sincere and insincere questions
STEP 1: Looking at sincere variables
STEP 1: Print Selected Topic
STEP 1: Import the model
STEP 1: Import the data
STEP 1: Loading an Audio Data
STEP 1: Get some samples

STEP 1: function from EDA kernel
STEP 1: more functions from LightGBM baseline
STEP 1: Standard plotly imports
STEP 1: Import the English language class
STEP 1: baseline model with tokenizer
STEP 1: Function to create the tokenizer
STEP 1: Generate a vocabulary
STEP 1: Building Kappa Score
STEP 1: create submission list, STEP 2: Test prediction on test, STEP 3: Train NN model, STEP 4: save the submission csv file



STEP 1: plotting benign images, STEP 2: Benign image viewing





STEP 1: The basic structure of model
STEP 1: for each product, STEP 2: calculate the correlation matrix
STEP 1: Here is the sample submission file
STEP 1: Filter out low frequencies from the signal
STEP 1: pip install googletrans, STEP 2: Light curves are irregularly spaced on the time axis, STEP 3: Setting the color palette, STEP 4: several prints in one cell, STEP 5: Importing libraries, STEP 6: Listing the available files
STEP 1: several prints in one cell
STEP 1: Load the data
STEP 1: Release Count By Year
STEP 1: PLOT PLOT count
STEP 1: Distribution by Day of Month
STEP 1: Distribution of Release
STEP 1: Sieve eratosthenes
STEP 1: Building Vocabulary and calculating coverage
STEP 1: Most of the missing words are punctuation and upper case words
STEP 1: Function for cleaning special mentions
STEP 1: Building Vocabulary and calculating coverage
STEP 1: Function for cleaning special mentions
STEP 1: Preprocess the processed data
STEP 1: Same for wavelet, STEP 2: Plotting forward pass
STEP 1: left seat right seat
STEP 1: Time of the experiment
STEP 1: Galvanic Skin Response


STEP 1: plotting rolling statistics
STEP 1: Removing the residuals
STEP 1: Train a submission, STEP 2: Exploring the target variable
STEP 1: Load prime cities
STEP 1: Train the XGB regressor
STEP 1: Importing the necessary modules, STEP 2: Ignore the warnings
STEP 1: Number of boxes per patient, STEP 2: Which boxes have imbalanced
STEP 1: plot number of cases per image
STEP 1: plot centers on the image
STEP 1: Age distribution by gender and target
STEP 1: What is the area of gender
STEP 1: Pixel spacing by patient
STEP 1: Distribution of bounding box areas distributed by the number of boxes distributed
STEP 1: Is there black pixels
STEP 1: Looking at the bounding ratio
STEP 1: Linear Discriminant Analysis
STEP 1: Linear Discriminant Analysis
STEP 1: Fit the model



























STEP 1: import seaborn as sns, STEP 2: visualize correlation between pairs
STEP 1: Import some libs, STEP 2: from sklearn.preprocessing import Imputer, STEP 3: pip install googletrans






STEP 1: Open the log file
STEP 1: Get feature ranking








STEP 1: Finally train the model
STEP 1: plot max features
STEP 1: Get the ranking
STEP 1: Checking Best Feature for Final Model
STEP 1: Checking Best Feature for Final Model
STEP 1: Running the random search

STEP 1: Here we average all the predictions and provide the final summary
STEP 1: Save the final prediction

STEP 1: Save grid search parameters

STEP 1: Load MTCNN settings, STEP 2: Inception Resnet V




STEP 1: Create fast MTCNN network
STEP 1: Fast MTCNN implementation
STEP 1: Frontal face detector, STEP 2: Function to detect faces, STEP 3: dictionary to store the times
STEP 1: Instantiate MTCNN object, STEP 2: Detect face detector, STEP 3: TIME DICNN with TOCNN

STEP 1: Training the network, STEP 2: Disable gradients for training
STEP 1: Train the model
STEP 1: zip file to zip file, STEP 2: Example of Dogs, STEP 3: Close the file

STEP 1: daily sales per department, STEP 2: Function for getting daily sales data
STEP 1: daily sales data, STEP 2: Convert item name to daily sales dataframe
STEP 1: Visualizing data, STEP 2: Plot the series
STEP 1: Normalize item lookup
STEP 1: Average value per week


STEP 1: Group data per item
STEP 1: Validate item lookup
STEP 1: Let us plot this now

STEP 1: create dataframe for difference
STEP 1: daily sales per cluster
STEP 1: Combine Humpling Age vs

STEP 1: Import required libraries
STEP 1: Set some paths for the audio
STEP 1: initialize some audio, STEP 2: collect all files in subfolder, STEP 3: Print the total
STEP 1: Comparing Spectrograms for different birds
STEP 1: plotting benign images, STEP 2: Loop over waves
STEP 1: plotting benign images, STEP 2: Let us plot this
STEP 1: Function for converting wave file
STEP 1: pip install it, STEP 2: Now , we prepare the data
STEP 1: Importing the required libraries, STEP 2: Logistic Regression without Logistic Regression



STEP 1: Running the model


STEP 1: Running the model

STEP 1: Load the data
STEP 1: Binarize the data
STEP 1: from tqdm import tqdm, STEP 2: Read wav files
STEP 1: Pad the audio


STEP 1: separate the dataframe for plotting, STEP 2: Most common features
STEP 1: separate variable stats, STEP 2: Accumulate the feature count
STEP 1: create the mask, STEP 2: Apply the mean mask, STEP 3: Mean the image
STEP 1: plotting the dataframe
STEP 1: Prepare list of files, STEP 2: Loop over the Training Data

STEP 1: Get importance of feature, STEP 2: Overall loss function
STEP 1: Importing the necessary Packages





STEP 1: create a dictionary to store the impact encoding, STEP 2: Apply the impact encoding for categorical features
STEP 1: Importing all libraries
STEP 1: sklearn part
STEP 1: Deep Learning Begins ..
STEP 1: Read data from the CSV file
STEP 1: Since the labels are textual , so we encode them categorically
STEP 1: Add feature scaling
STEP 1: Train and Validation Split
STEP 1: Fit the model with early stopping callback
STEP 1: summarize history for loss
STEP 1: summarize history for accuracy
STEP 1: load the data, STEP 2: loading in the data

STEP 1: Set GPU Configuration
STEP 1: Load Data for each type


STEP 1: Write predictions to file for submission, STEP 2: submit test prediction, STEP 3: Total training time, STEP 4: Test i.e, STEP 5: Print cv score, STEP 6: check coverage

STEP 1: Wrapper for fast.ai library
STEP 1: Special thanks to
STEP 1: mechanism to select attributes and check results against tournaments since, STEP 2: Import libraries and data, STEP 3: pip install googletrans, STEP 4: Listing the available files, STEP 5: Importing useful libraries
STEP 1: More To Come, STEP 2: to reduce memory usage
STEP 1: About the data
STEP 1: Load the data
STEP 1: Detecting NA values
STEP 1: Lets look at the missing values in the data
STEP 1: Lets check the Missing Values
STEP 1: Lets check the Missing Values
STEP 1: Hour of day
STEP 1: One Hot Encoding






STEP 1: Any results you write to the current directory are saved as output, STEP 2: one hot encode text columns
STEP 1: Import required libraries

STEP 1: Run the grid search
STEP 1: Split the data
STEP 1: Importing libraries, STEP 2: Listing the available files
STEP 1: Importing the library
STEP 1: Now , we will freeze our data, STEP 2: Simple LSTM model, STEP 3: Import the necessary libraries, STEP 4: Choose what device to run



STEP 1: Importing libraries, STEP 2: Listing the available files
STEP 1: Maping the category values in our dict
STEP 1: merge train and test
STEP 1: separate the target variable
STEP 1: Transforming ordinal Features
STEP 1: from sklearn import image
STEP 1: Importing the necessary Packages
STEP 1: Load data and prepare them
STEP 1: Load in Data
STEP 1: difference in Weeks
STEP 1: plot the correlation matrix
STEP 1: Frequency of Sex
STEP 1: Smoking Status Viz
STEP 1: Smoking Status Viz
STEP 1: Training the Data
STEP 1: Now , we will freeze our data, STEP 2: Simple LSTM model, STEP 3: Import the necessary libraries, STEP 4: Choose what device to run



STEP 1: Prepare for data analysis
STEP 1: the func is from
STEP 1: Extracting image data
STEP 1: Calculate number of items in each class, STEP 2: Now aggregated bins, STEP 3: Create the submission file
STEP 1: Write original image to file, STEP 2: Using skimage
STEP 1: Writing original image, STEP 2: Using skimage
STEP 1: Importing the libraries
STEP 1: Instantiate an iterator, STEP 2: Iterate over validation set, STEP 3: Instantiate an iterator
STEP 1: Set up the optimizer
STEP 1: load a sample image
STEP 1: prepare polygons list, STEP 2: load each color as a list
STEP 1: create polygons and draw them, STEP 2: Relimizing the figure
STEP 1: then get the number of polygons per image

STEP 1: Importing the required libraries, STEP 2: import three band files, STEP 3: Loading the data
STEP 1: Run our model, STEP 2: Read sample submission
STEP 1: Code in python
STEP 1: load the data



STEP 1: getting average sales per departments, STEP 2: Average sales per Department
STEP 1: Total Sales by Category
STEP 1: group sales by state id, STEP 2: Total sales by State, STEP 3: Mover Sales by State
STEP 1: group by store, STEP 2: Total sales by Store, STEP 3: Mean Sales per Store
STEP 1: display the scatter plot, STEP 2: Plotting sales per Day, STEP 3: show the plot




STEP 1: save the submission file
STEP 1: Extract the columns, STEP 2: Transfer by mode
STEP 1: Save the data, STEP 2: One hot encode the data
STEP 1: Separating target and ids
STEP 1: Initializeententent vector, STEP 2: Random Forest Objects, STEP 3: Apply entropy on the dataset
STEP 1: Time Series plotting
STEP 1: Plot Time Series
STEP 1: pip install googletrans, STEP 2: libraries for scipy
STEP 1: pip install googletrans
STEP 1: libraries for scipy
STEP 1: open file selection
STEP 1: function to convert a MAT file into a dictionary
STEP 1: calculate the normalized FFT curve
STEP 1: define array of frequencies
STEP 1: Theoretical family of A

STEP 1: Define the loss function







STEP 1: Function to replace zero runs
STEP 1: Preprocessing data, STEP 2: Normalize features, STEP 3: normalize each feature
STEP 1: from sklearn.list import a listdirectory, STEP 2: Function to get the files with extension
STEP 1: Loading the data
STEP 1: write a mean squared error, STEP 2: Mean absolute error, STEP 3: Status of Loss
STEP 1: histogram plot of age field, STEP 2: Change markers to default traces, STEP 3: Change the bar mode, STEP 4: show the image

STEP 1: FVC line
STEP 1: FVC line
STEP 1: Define Validation Function
STEP 1: fill in submission file, STEP 2: Analysis on the sample submission



STEP 1: Data transformation for sklearn, STEP 2: define which attributes shall not be transformed , are numeric or categorical
STEP 1: define which attributes shall not be transformed , are numeric or categorical

STEP 1: APPLY DEFINED TRANSFORMATIONS

STEP 1: individual patient group, STEP 2: Training and Validation


STEP 1: Exponential Discriminant
STEP 1: Define the grid lines
STEP 1: Any results you write to the current directory are saved as output


STEP 1: Create array of ordinal and long, STEP 2: Now let us remove long ordinal features, STEP 3: chinal and long
STEP 1: There are no
STEP 1: Run final cluster classification, STEP 2: merge data with cluster id
STEP 1: Evaluating a model

STEP 1: Read in the training data
STEP 1: Plotting the region


STEP 1: Cleaning the data





STEP 1: Fill NaN values with mean, STEP 2: Spliting the data






STEP 1: Kaggle Datasets

STEP 1: Initialize train and test images
STEP 1: numpy and matplotlib defaults, STEP 2: Function for dataset conversion


STEP 1: Peek at training data
STEP 1: Splitting the test data


STEP 1: CUSTOM LEARNING SCHEUDLE
STEP 1: Set up learning rate schedule, STEP 2: Learning Rate Scheduler
STEP 1: Learning Rate Schedule
STEP 1: Loading Dnet
STEP 1: Load the Xception image
STEP 1: Instantiating the Inception Model
STEP 1: Instantiating the Inception Resnet


STEP 1: Submit to Kaggle
STEP 1: Plot the distribution of the submission
STEP 1: Load the dependancies
STEP 1: Select a random DICOM file, STEP 2: show each axis
STEP 1: Fit the frequency histogram
STEP 1: Plot the number of bins
STEP 1: Things to use, STEP 2: show each axis
STEP 1: load train dataframe, STEP 2: Check DICOM data
STEP 1: Pivot for each image









STEP 1: Handle missing values, STEP 2: HANDLE MISSING VALUES, STEP 3: Handle missing values
STEP 1: SCALE target variable
STEP 1: EXTRACT DEVELOPTMENT TEST
STEP 1: FITTING THE MODEL
STEP 1: Loading the data




