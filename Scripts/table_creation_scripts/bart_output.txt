
Cleaned documentation: So there are four outliers in the datset. Should we drop them? We will see about that
Processed documentation: So there are four outliers in the datset. Should we drop them? We will see.
===========================================================================================
Cleaned documentation: Replace some outliers. Some new features. Categorical features with One-Hot encode. Aggregations for application set. Count installments lines
Processed documentation: Replace some outliers. Categorical features with One-Hot encode. Aggregations for application set. Count installments lines.
===========================================================================================
Cleaned documentation: Replace some outliers. Some new features. Categorical features with One-Hot encode. Aggregations for application set. Count credit card lines
Processed documentation: Replace some outliers. Categorical features with One-Hot encode. Aggregations for application set. Count credit card lines.
===========================================================================================
Cleaned documentation: Kaggle has not ehough memory to clean this dataset. Aggregated dataset has 3411 features. df = aggregate()
Processed documentation: Kaggle has not ehough memory to clean this dataset. Aggregated dataset has 3411 features.
===========================================================================================
Cleaned documentation: step = 'Tilii`s Bayesian optimization'. scores[step] = scor. scores.loc['LB', step] = .797. scores.T
Processed documentation: step = 'Tilii`s Bayesian optimization' scores[step] = scor.loc['LB', step] =.797.
===========================================================================================
Cleaned documentation: step = 'Bayesian optimization for new set'. scores[step] = scor. scores.loc['LB', step] = .797. scores.T
Processed documentation: step = 'Bayesian optimization for new set' scores[step] = scor.loc['LB', step] =.797.
===========================================================================================
Cleaned documentation: The idea is to create a mask with regions we know for sure are blue rooftops. Hold on...
Processed documentation: The idea is to create a mask with regions we know for sure are blue rooftops.
===========================================================================================
Cleaned documentation: Plot multiple images in a grid. Green channel. Red channel. Blue channel. Orange channel
Processed documentation: Plot multiple images in a grid. Red channel. Blue channel. Orange channel.
===========================================================================================
Cleaned documentation: Write the data to file to save it for a new session....
Processed documentation: Write the data to file to save it for a new session.
===========================================================================================
Cleaned documentation: from nltk.tokenize import RegexpTokenizer. from nltk.corpus import stopwords. TOKENIZER = RegexpTokenizer(r'\w+'). STOPWORDS = set(stopwords.words('english'))
Processed documentation: from nltk.tokenize import RegexpTokenizer. TOKENIZER = RegexPTokenizer(r'\w+'). STOPWORDS = set(stopwords.words('english'))
===========================================================================================
Cleaned documentation: train_wordvec = pd.DataFrame(data = train_feats). train_wordvec.columns = [str(col) + '_col' for col in train_wordvec.columns]. print(train_wordvec.shape). train_wordvec.head()
Processed documentation: train_wordvec = pd.DataFrame(data = train_feats). train_ wordvec.columns = [str(col) + '_col' for col in train_word Vec.Columns
===========================================================================================
Cleaned documentation: test_wordvec = pd.DataFrame(data = test_feats). test_wordvec.columns = [str(col) + '_col' for col in test_wordvec.columns]. print(test_wordvec.shape). test_wordvec.head()
Processed documentation: test_wordvec = pd.DataFrame(data = test_feats). test_ wordvec.columns = [str(col) + '_col' for col in test_word Vec.Columns
===========================================================================================
Cleaned documentation: src_lm = ItemLists(path, TextList.from_df(train, path=".", cols = [ 'question_title', "question_body", 'answer']),. TextList.from_df(val, path=".", cols = [ 'question_title', "question_body", 'answer']))
Processed documentation: src_lm = ItemLists(path, TextList.from_df(train, path=".", cols = [ 'question_title', "question_body", 'answer']),. TextList from_
===========================================================================================
Cleaned documentation: val_raw_preds = learn.get_preds(ds_type=DatasetType.Valid). val_preds_fwd = get_ordered_preds(learn, DatasetType.Valid, val_raw_preds). val_raw_preds = learn_bwd.get_preds(ds_type=DatasetType.Valid). val_preds_bwd = get_ordered_preds(learn_bwd, DatasetType.Valid, val_raw_preds)
Processed documentation: val_raw_preds = learn.get_Preds(ds_type=DatasetType.Valid). val_ Preds_fwd = get_ordered_prededs(learn, Dataset
===========================================================================================
Cleaned documentation: test_raw_preds = learn.get_preds(ds_type=DatasetType.Test). test_preds_fwd = get_ordered_preds(learn, DatasetType.Test, test_raw_preds). test_raw_preds = learn_bwd.get_preds(ds_type=DatasetType.Test). test_preds_bwd = get_ordered_preds(learn_bwd, DatasetType.Test, test_raw_preds)
Processed documentation: test_raw_preds = learn.get_Preds(ds_type=DatasetType.Test). test_predS_fwd = get_ordered_prededs(learn, Dataset
===========================================================================================
Cleaned documentation: transforms.Normalize((0.70244707, 0.54624322, 0.69645334), (0.23889325, 0.28209431, 0.21625058)). creating test data. prepare the test loader
Processed documentation: Transforms.transforms.Normalize((0.70244707, 0.54624322,0.69645334), (0.23889325, 0,28209431,0,21625058
===========================================================================================
Cleaned documentation: cases. Active Case = confirmed - deaths - recovered. replacing Mainland china with just China. filling missing values
Processed documentation: Active Case = confirmed - deaths - recovered. replacing Mainland china with just China. filling missing values.
===========================================================================================
Cleaned documentation: First death again in France by Feb 17. Italy follows suite. The rest is history!
Processed documentation: First death again in France by Feb 17. Italy follows suite.
===========================================================================================
Cleaned documentation: Returns the confusion matrix between rater's ratings. The following 3 functions have been taken from Ben Hamner's github repository
Processed documentation: The following 3 functions have been taken from Ben Hamner's GitHub repository.
===========================================================================================
Cleaned documentation: model = xgb.XGBRegressor(n_estimators=500, nthread=-1, max_depth=19, learning_rate=0.01, min_child_weight = 150, colsample_bytree=0.8). model.fit(xtrain, ytrain)
Processed documentation: model = xgb.XGBRegressor(n_estimators=500, nthread=1, max_depth=19, learning_rate=0.01, min_child_weight = 150, colsample
===========================================================================================
Cleaned documentation: This is the inference kernel for BERT pytorch. Check out the amazing Kernel for finetuning BERT by Yuval:
Processed documentation: This is the inference kernel for BERT pytorch. Check out the amazing Kernel for finetuning BERT.
===========================================================================================
Cleaned documentation: Checking the combination of min(wm_yr_wk) and max(wm_yr_wk). first week ID. last week ID
Processed documentation: Checking the combination of min(wm_yr_week) and max(wm-yr-week) first week ID.
===========================================================================================
Cleaned documentation: Points to Note: Data is available till June month. Not much trend to look out for.
Processed documentation: Not much trend to look out for. Data is available till June month.
===========================================================================================
Cleaned documentation: total_market_obs.append(market_obs_df.copy()). if len(total_market_obs) == 1:. history_df = total_market_obs[0]. else:. history_df = pd.concat(total_market_obs[-(14+1):], ignore_index = True)
Processed documentation: total_market_obs.append(market_ob_df.copy()). if len(total_ market_obs) == 1:. history_df = total_market obs[0]. else:.history_
===========================================================================================
Cleaned documentation: Split the signature given as string into a 4-tuple of integers.. cpmp
Processed documentation: Split the signature given as string into a 4-tuple of integers.
===========================================================================================
Cleaned documentation: Time Sereis EDA Kernel by Chris. FEATURE ENGINEER - WEEK. FEATURE ENGINEER - WEEK
Processed documentation: Time Sereis EDA Kernel by Chris. FEATURES ENGINEER - WEEK.
===========================================================================================
Cleaned documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. TODO: Try others encoders
Processed documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. Try others encoders.
===========================================================================================
Cleaned documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. TODO: Try others encoders
Processed documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. Try others encoders.
===========================================================================================
Cleaned documentation: print("Paragram : "). oov_paragram = check_coverage(vocab_low, embed_paragram). print("FastText : "). oov_fasttext = check_coverage(vocab_low, embed_fasttext)
Processed documentation: print("Paragram : "). oov_paragram = check_coverage(vocab_low, embed_Paragram). print("FastText : ") oov fasttext = check coverage
===========================================================================================
Cleaned documentation: print("Paragram : "). oov_paragram = check_coverage(vocab_low, embed_paragram). print("FastText : "). oov_fasttext = check_coverage(vocab_low, embed_fasttext)
Processed documentation: print("Paragram : "). oov_paragram = check_coverage(vocab_low, embed_Paragram). print("FastText : ") oov fasttext = check coverage
===========================================================================================
Cleaned documentation: print("Paragram : "). oov_paragram = check_coverage(vocab_low, embed_paragram). print("FastText : "). oov_fasttext = check_coverage(vocab_low, embed_fasttext)
Processed documentation: print("Paragram : "). oov_paragram = check_coverage(vocab_low, embed_Paragram). print("FastText : ") oov fasttext = check coverage
===========================================================================================
Cleaned documentation: out_layer = KL.BatchNormalization()(out_layer). sub_df_public = sub_df_public.merge(test_df[["Date"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}),. how="left", on=["Date"] + loc_group)
Processed documentation: out_layer = KL.BatchNormalization()(out_ layer). sub_df_public = sub_DF_public.merge(test_df[["Date"] + loc_group + pred_cols
===========================================================================================
Cleaned documentation: x_test = []. for i in tqdm(meta_test.signal_id.values):. idx=i-8712. clear_output(wait=True). x_test.append(abs(feature_extractor(test_set.iloc[:, idx].values, n_part=400)))
Processed documentation: x_test = []. for i in tqdm( meta_test.signal_id.values):. idx=i-8712. clear_output(wait=True). x_ test.append(
===========================================================================================
Cleaned documentation: x_test = np.array(x_test).reshape(-1,x_test[0].shape[0]). x_test_lp = np.array(x_test).reshape(-1,x_test_lp[0].shape[0]). x_test_hp = np.array(x_test).reshape(-1,x_test_hp[0].shape[0]). x_test_dc = np.array(x_test).reshape(-1,x_test_dc[0].shape[0])
Processed documentation: x_test = np.array(x_ test).reshape(-1,x_Test[0].shape[0]). x_test_lp = numpy.array('lp', 'hp', 'dc',
===========================================================================================
Cleaned documentation: Manipulation of columns for both training dataset. Manipulation of columns for both testing dataset
Processed documentation: Manipulation of columns for both training dataset and testing dataset.
===========================================================================================
Cleaned documentation: Replacing the text Not Reported and N/A with numpy missing value cmputation. Viewing the dataset
Processed documentation: Replacing the text Not Reported and N/A with numpy missing value cmputation.
===========================================================================================
Cleaned documentation: Getting the dataset to check the correlation. Converting the dataset to the correlation function
Processed documentation: Getting the dataset to check the correlation. Converting the data to the correlation function.
===========================================================================================
Cleaned documentation: Analysis of Patient Information: The distribution graph show the distribution of patient's age, gender and the smoking status.
Processed documentation: Analysis of Patient Information: The distribution graph show the distribution of patient's age, gender and smoking status.
===========================================================================================
Cleaned documentation: y_holdout = y[test_idx]. cross_score = cross_val_score(clf, X_train, y_train, cv=self.n_splits, scoring='roc_auc'). print(" cross_score: %.5f" % (cross_score.mean())). Log odds transformation
Processed documentation: y_holdout = y[test_idx]. cross_score = cross_val_score(clf, X_train, y_train), cv=self.n_splits, scoring='roc_
===========================================================================================
Cleaned documentation: Creates the Z matrix corresponding to vector X.. Function to process Almon lags. Solve for Z[t][0].
Processed documentation: Creates the Z matrix corresponding to vector X. Function to process Almon lags. Solve for Z[t]
===========================================================================================
Cleaned documentation: Generate test set data. Process properties for 2016. Process properties for 2017
Processed documentation: Generate test set data. Process properties for 2016 and 2017.
===========================================================================================
Cleaned documentation: cats = train["City"].unique().tolist(). fig, ax = plt.subplots(34, 1, figsize=(25, 400)). x = tem.where(train["City"]==variable, inplace = False)
Processed documentation: cats = train["City"].unique().tolist(). fig, ax = plt.subplots(34, 1, figsize=(25, 400), x = tem.where(train["City"]==variable
===========================================================================================
Cleaned documentation: Convert unknown cities in test data to clusters based on known cities using KMeans. data = data.drop(['City'], axis=1)
Processed documentation: Convert unknown cities in test data to clusters based on known cities using KMeans.
===========================================================================================
Cleaned documentation: install tensorflow if not installed. import tensorflow. from sklearn.metrics import mean_absolute_error
Processed documentation: install tensorflow if not installed. from sklearn.metrics import mean_absolute_error.
===========================================================================================
Cleaned documentation: Define the model. Fit the model. predictions_1 = clf.predict(X_valid) Your code here. mae_1 = mean_absolute_error(y_true=y_valid, y_pred=predictions_1) Your code here
Processed documentation: Define the model. Fit the model with the model's predictions.
===========================================================================================
Cleaned documentation: keras regressor. reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=baseline_model,batch_size=5,epochs=100, verbose=1). X_ktrain = StandardScaler().fit_transform(X_train.values). arr = sav_train['rev_save'].values. y_ktrain = StandardScaler().fit_transform(arr[:, np.newaxis]). reg.fit(X_ktrain,y_ktrain). print (reg)
Processed documentation: reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=baseline_model,batch_size=5,epochs=100, verbose=1).
===========================================================================================
Cleaned documentation: deal with null values in test dataset. cols3 = X_test.columns[X_test.isna().any()].tolist(). print(cols3). X_test['Days_from_open'] = X_test['Days_from_open'].fillna(int(round(X_test['Days_from_open'].mean())))
Processed documentation: deal with null values in test dataset. cols3 = X_ test.columns[X_test.isna()].tolist() print(cols3). X_test['Days_from_open']
===========================================================================================
Cleaned documentation: hm_wide = HeatMap( list(zip(drop.dropoff_latitude_round3.values, drop.dropoff_longitude_round3.values, drop.Num_Trips.values)), min_opacity=0.2, radius=5, blur=15, max_zoom=1 )drop_map.add_child(hm_wide). print(pickup.shape). For each pickup point add a circlemarker
Processed documentation: hm_wide = HeatMap( list(zip(drop.dropoff_latitude_round3.values, drop. dropoff_longitude_ round3. values), min_opacity=0.2, radius
===========================================================================================
Cleaned documentation: MedianBlur': A.MedianBlur(blur_limit=5, p=1.0),. CenterCrop': A.CenterCrop(height=256, width=256, p=1.0),. RandomRotate90': A.RandomRotate90(p=1.0),. ShiftScaleRotate': A.ShiftScaleRotate(p=1.0),. Rotate': A.Rotate()
Processed documentation: MedianBlur': A.Medianblur(blur_limit=5, p=1.0),. CenterCrop: A.CenterCrop(height=256, width=256),. RandomRot
===========================================================================================
Cleaned documentation: history = model.fit(. train_inputs, train_labels,. batch_size=64,. epochs=100,. callbacks=[. tf.keras.callbacks.ReduceLROnPlateau(),. tf.keras.callbacks.ModelCheckpoint('model.h5'). validation_split=0.05
Processed documentation: history = model.fit( train_inputs, train_labels,. batch_size=64,. epochs=100,. callbacks=[. tf.keras.callbacks.ReduceLROnPlateau()
===========================================================================================
Cleaned documentation: fig = px.line(. history.history, y=['loss', 'val_loss'],. labels={'index': 'epoch', 'value': 'Mean Squared Error'},. title='Training History'). fig.show()
Processed documentation: fig = px.line(. history, y=['loss', 'val_loss'],. labels={'index': 'epoch', 'value': 'Mean Squared Error'},. title='Training History
===========================================================================================
Cleaned documentation: import efficientnet.keras as efn. from tensorflow import keras. model = efn.EfficientNetB0(weights='imagenet') or weights='noisy-student'
Processed documentation: import efficientnet.keras as efn. model. model = e fn.EfficientNetB0(weights='imagenet') or weights='noisy-student'
===========================================================================================
Cleaned documentation: Competitiveness, includes more game options - overfitting for now in Tournaments. for now
Processed documentation: Competitiveness, includes more game options - overfitting for now in Tournaments.
===========================================================================================
Cleaned documentation: train_datagen=ImageDataGenerator(. rescale=1./255,. validation_split=0.25,. horizontal_flip = True,. zoom_range = 0.3,. width_shift_range = 0.3,. height_shift_range=0.3
Processed documentation: train_datagen=ImageDataGenerator(. rescale=1./255,. validation_split=0.25,. horizontal_flip = True,. zoom_range = 0.3,. width_shift_range
===========================================================================================
Cleaned documentation: Configure the TensorBoard callback and fit your model. tensorboard_callback = TensorBoard("logs"). model.fit_generator(generator=train_generator,. steps_per_epoch=30,. validation_data=valid_generator,. validation_steps=30,. epochs=nb_epochs,. callbacks=[tensorboard_callback])
Processed documentation: Configure the TensorBoard callback and fit your model.
===========================================================================================
Cleaned documentation: Implements large margin arc distance. Reference: blob/master/src/modeling/metric_learning.py. loss function. metrics. initialize accumulators. obtain predictions. obtain confidences. add to accumulator
Processed documentation: Implements large margin arc distance. Reference: blob/master/src/modeling/metric_learning.py.
===========================================================================================
Cleaned documentation: Compute patch coordinates helper functions: pad_image(...) transpose_image(...) get_tissue_parts_indices(...) get_tissue_subparts_coords(...) eval_and_append_xy_coords(...) main function: compute_coords(image, patch_size, ...)
Processed documentation: Compute patch coordinates helper functions: pad_image, transpose, get_tissue_parts_indices, get-tissue-subparts, eval. main function: compute_coords(image, patch_size
===========================================================================================
Cleaned documentation: TensorFlow: Stitch patches together, using tf-operations and tf.data.Dataset compute_coords(..., precompute=False) -> patch_image(..., coords)
Processed documentation: TensorFlow: Stitch patches together, using tf-operations and tf.data.Dataset compute_coords.
===========================================================================================
Cleaned documentation: III. Run it all: model.create() -> dataset.create() -> train(train) -> predict(val).decode() -> predict(test).decode() -> submit
Processed documentation: Model.create() -> dataset. create() -> train(train) -> predict(val).decode() -> predict (test).decodes() -> submit.
===========================================================================================
Cleaned documentation: train/test Ids. generate test DataFrame. generate X, targets. X_train = data.drop(list(target_columns), axis=1).drop('Id', axis=1). y_train = data[list(target_columns)]
Processed documentation: train/test Ids. generate test DataFrame. generate X, targets. X_train = data.drop(list(target_columns), axis=1).drop('Id', axis=2). y_train=
===========================================================================================
Cleaned documentation: plt.figure(figsize=(24,24)). ax = sns.boxplot(x='release_Year', y='revenue', data=train[train['revenue'] in list(mostly_release_year_movies)]). for label in ax.xaxis.get_ticklabels():. label.set_rotation(60)
Processed documentation: Figure.plt.figure(figsize=(24,24). ax = sns.boxplot(x='release_Year', y='revenue', data=train[train['revenue'] in list(mostly
===========================================================================================
Cleaned documentation: KERAS MODEL DEFINITION. params. Inputs. Embeddings layers. rnn layer. main layer. output. model
Processed documentation: KERAS MODEL DEFINITION. Inputs. Embeddings layers. rnn layer. main layer. output. model.
===========================================================================================
Cleaned documentation: Conclusions:__ The __dataset is very imbalanced.__ Data augmentation and resampling techniques will be required to perform the defect detection.
Processed documentation: Conclusions: The __dataset is very imbalanced. Data augmentation and resampling techniques will be required.
===========================================================================================
Cleaned documentation: Read images and masks Load the images and masks into the DataFrame and divide the pixel values by 255.
Processed documentation: Read images and masks into the DataFrame and divide the pixel values by 255.
===========================================================================================
Cleaned documentation: Plotting the depth distributions Separatelty plotting the depth distributions for the training and the testing data.
Processed documentation: Plotting the depth distributions for the training and the testing data.
===========================================================================================
Cleaned documentation: x_test = np.array([upsample(np.array(load_img(ROOT+"/test/images/{}.png".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)
Processed documentation: x_test = np.array([upsample(load_img(ROOT+"/test/images/{}.png".format(idx), grayscale=True), / 255 for idx in tq
===========================================================================================
Cleaned documentation: Define Empty lists. Output with sentiment data for each pet. Output with sentiment data for each pet
Processed documentation: Define Empty lists. Output with sentiment data for each pet.
===========================================================================================
Cleaned documentation: python_trace_param_automata" applies sequence of rules to the input grid. What are these commands are written below.
Processed documentation: python_trace_param_automata" applies sequence of rules to the input grid.
===========================================================================================
Cleaned documentation: Global rule These are grid->grid rules. There are many of them, so this function is pretty big.
Processed documentation: Global rule These are grid->grid rules. There are many of them.
===========================================================================================
Cleaned documentation: Doing granulometry with scipy.ndimage Optical granulometry is the process of measuring the different grain sizes in a granular material.
Processed documentation: Granulometry is the process of measuring the different grain sizes in a granular material. Doing granulometry with scipy.ndimage
===========================================================================================
Cleaned documentation: Get stats of a given embeddings index. Put all embeddings in a numpy matrix. Get embedding stats
Processed documentation: Get stats of a given embeddings index. Put all embeds in a numpy matrix. Get embedding stats.
===========================================================================================
Cleaned documentation: Much better... Quorans is the top word missed now...I wonder if Quora or quora is in the embeddings_index vocabulary..
Processed documentation: Much better... Quorans is the top word missed now. Quora or quora is in the embeddings_index vocabulary.
===========================================================================================
Cleaned documentation: Good...So let us replace all heights of the form \d\'\d such as 5'4 with the "5 foot 4"..
Processed documentation: So let us replace all heights of the form \d\'\d such as 5'4 with the "5 foot 4"
===========================================================================================
Cleaned documentation: There are also frequent mentions of height..I wonder how that will affect the results...
Processed documentation: There are also frequent mentions of height..I wonder how that will affect the results.
===========================================================================================
Cleaned documentation: Better! Quorans seems to be a repeatedly missed word in this embedding as well...
Processed documentation: Better! Quorans seems to be a repeatedly missed word in this embedding as well.
===========================================================================================
Cleaned documentation: Blend (by percentage) These percenages will be tuned more soon. Score: 0.27049
Processed documentation: Blend (by percentage) These percenages will be tuned more soon.
===========================================================================================
Cleaned documentation: Let's take all entries with parent category 'Услуги' (services) and see, what do we have...
Processed documentation: Let's take all entries with parent category 'У   (services) and see, what do we have...
===========================================================================================
Cleaned documentation: hold_out_dataset_1 = ResizedNpyMelanomaDataset(train_npy, hold_out_indices, df=hold_out_df,. transform=transform_fun(RESIZE_SHAPE, key="dev", plot=True)). hold_out_dataset_2 = MelanomaDataset(hold_out_df, transform=transform_fun(RESIZE_SHAPE, key="dev", plot=True))
Processed documentation: hold_out_dataset_1 = ResizedNpyMelanomaDataset(train_npy, hold_out indices, df=hold_ out_df,. transform=transform_fun(
===========================================================================================
Cleaned documentation: Data Generator I highly build upon the [nice data generator presented by Shervine Amidi.]( Thank you! :-)
Processed documentation: Data Generator I highly build upon the [nice data generator presented by Shervine Amidi.]
===========================================================================================
Cleaned documentation: test_pred = model.predict(test_dataloader)[0:testdf.shape[0]]. dev_pred = model.predict(dev_dataloader). test_pred_df = turn_pred_to_dataframe(testdf, test_pred). dev_pred_df = turn_pred_to_dataframe(dev_data, dev_pred). test_pred_df.to_csv("brute_force_test_pred.csv", index=False). dev_pred_df.to_csv("brute_force_dev_pred.csv", index=False)
Processed documentation: test_pred = model.predict(test_dataloader)[0:testdf.shape[0]]. dev_pred means model. predict(dev_daloader). test_pred_df
===========================================================================================
Cleaned documentation: Test data We can find multiple segments of sequences in the test folder. Let's peek at their names:
Processed documentation: We can find multiple segments of sequences in the test folder. Let's peek at their names.
===========================================================================================
Cleaned documentation: These cases are handeled separately:. print ("Total Wrong: " , ct1.0/len(pred_list))
Processed documentation: These cases are handeled separately:. print ("Total Wrong: ", ct1.0/len(pred_list)
===========================================================================================
Cleaned documentation: train_labels_df = train_labels_df["labelId"].apply(lambda x: [int(i) for i in x]). valid_labels_df = valid_labels_df["labelId"].apply(lambda x: [int(i) for i in x])
Processed documentation: train_labels_df = train_Labels_DF["labelId"].apply(lambda x: [int(i) for i in x). valid_lables_DF = valid_Lab labels_df
===========================================================================================
Cleaned documentation: Model complexity. Loop over different values of k. validation accuracy. Plot
Processed documentation: Model complexity. Loop over different values of k. validation accuracy.
===========================================================================================
Cleaned documentation: time. df_train['count_typo'] = df_train.text.apply(count_typo). df_train['typo'] = df_train.text.apply(find_typo). try:. df_train['correct_typo'] = df_train.text.apply(correct_typo). except:. pass. df_train.to_csv('getdata.csv',index=False)
Processed documentation: df_train.text.apply(find_typo). try:. df_train['correct_ty Po'] = df_ train.text(correct_Typo). except:. pass. df-train.to
===========================================================================================
Cleaned documentation: MULTIOUTPUTREGRESSOR FOR MULTILABEL TASK. model= MultiOutputRegressor(GradientBoostingRegressor(random_state=42)).fit(X, b). PREDICT PUBLIC SET. public_preds=model.predict(X2). PREDICT PRIVATE SET. private_preds=model.predict(X3). DATAFRAMES OF PREDS
Processed documentation: MULTIOUTPUTREGRESSOR FOR MULTILABEL TASK. model= MultiOutputRegressor(GradientBoostingRegressor (random_state=42)).fit(X, b). PR
===========================================================================================
Cleaned documentation: Some Feature Engineering. df_train["healthpack"] = df_train["boosts"] + df_train["heals"]. df_test["healthpack"] = df_test["boosts"] + df_test["heals"]
Processed documentation: Some Feature Engineering. df_train["healthpack"] = df_ train["boosts"] + df_training["heals"]. df_test ["healthpack"], df_ test ["boosts"],df_test
===========================================================================================
Cleaned documentation: Loading the data Load the data. Some fields are arrays but are stored as strings containing Python literal arrays.
Processed documentation: Some fields are arrays but are stored as strings containing Python literal arrays.
===========================================================================================
Cleaned documentation: System. Basics. matplotlib inline. SKlearn. PyTorch. Data Augmentation for Image Preprocessing
Processed documentation: System. matplotlib inline. SKlearn. PyTorch. Data Augmentation for Image Preprocessing.
===========================================================================================
Cleaned documentation: Data object and Loader. Get a sample. Outputs. Criterion example. Unsqueeze(1) from shape=[3] to shape=[3, 1]
Processed documentation: Data object and Loader. Unsqueeze(1) from shape=[3] to shape:[3, 1] Get a sample.
===========================================================================================
Cleaned documentation: Data object and Loader. Get a sample. Outputs. Criterion example. Unsqueeze(1) from shape=[3] to shape=[3, 1]
Processed documentation: Data object and Loader. Unsqueeze(1) from shape=[3] to shape:[3, 1] Get a sample.
===========================================================================================
Cleaned documentation: ResNet50 ---. model = ResNet50Network(output_size=output_size, no_columns=no_columns).to(device). Uncomment and Train. train_folds(preds_submission = preds_submission, model = model, version = version)
Processed documentation: ResNet50 ---. model = ResNet50Network( output_size=output_size, no_columns=no_Columns).to(device). Uncomment and Train. train_folds(preds_
===========================================================================================
Cleaned documentation: Submission ---. Import submission file. ss = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv'). ss['target'] = preds_submission. ss.to_csv(f'submission_v7.2_SingleModel.csv', index=False)
Processed documentation: Submission ---. Import submission file. ss = pd.read_csv('/kaggle/ input/siim-isic-melanoma-classification/sample_submission.csv'). ss['target
===========================================================================================
Cleaned documentation: Data📁 extend()`: Extend list by appending elements from the iterable. The code below does the following:
Processed documentation: The code below does the following:. extend()`: Extend list by appending elements from the iterable.
===========================================================================================
Cleaned documentation: ResNet34. eff_net34 = ResNet34Network(output_size = num_classes).to(device). Uncomment and train the model. train(model=eff_net34, epochs=epochs, batch_size=batch_size, num_workers=num_workers,. learning_rate=learning_rate, weight_decay=weight_decay, plot_loss=plot_loss)
Processed documentation: ResNet34.Resnet34.Network(output_size = num_classes).to(device). Uncomment and train the model. train(model=eff_net34, epochs=epochs, batch_
===========================================================================================
Cleaned documentation: sort the dog and cat fractions with respect to nr. of animals in train.csv
Processed documentation: sort the dog and cat fractions with respect to nr. of animals in train.
===========================================================================================
Cleaned documentation: How Many unique patients? 👩‍⚕️👨‍⚕️ Remember: 176 unique patients, with entries between 6 and 10 each.
Processed documentation: Remember: 176 unique patients, with entries between 6 and 10 each.
===========================================================================================
Cleaned documentation: Let us look at the JSON subcolumns in `data1`... Lets first look at the first observation in `customDimensions`:
Processed documentation: Let us look at the JSON subcolumns in `data1`... Lets first look at  `customDimensions`:
===========================================================================================
Cleaned documentation: We will keep these for now, too. Finally we can look at `trafficSource`:
Processed documentation: We will keep these for now, too. Finally we can look at trafficSource.
===========================================================================================
Cleaned documentation: Test Dataset Overview Distributions look ~ the same as in Train Data.
Processed documentation: Test Dataset Overview Distributions look the same as in Train Data.
===========================================================================================
Cleaned documentation: Preprocess .csv files 📐 Add Image Path This will help access the images in the feature.
Processed documentation: Preprocess.csv files. Add Image Path This will help access the images in the feature.
===========================================================================================
Cleaned documentation: RandomGreyscale 🌘 Randomly convert image to grayscale with a probability of p (default 0.1).
Processed documentation: Randomly convert image to grayscale with a probability of p (default 0.1).
===========================================================================================
Cleaned documentation: RandomVerticalFlip 🌍🌎 Vertically flip the given PIL Image randomly with a given probability.
Processed documentation: RandomVerticalFlip is a way to randomly flip the given PIL Image with a given probability.
===========================================================================================
Cleaned documentation: train3 = ((train3) / data[:train2.shape[0]].std(axis=1)[:, np.newaxis]). test3 = ((test3) / data[train2.shape[0]:].std(axis=1)[:, np.newaxis])
Processed documentation: train3 = ((train3) / data[:train2.shape[0]].std(axis=1)[:, np.newaxis]). test3 = (test3) + data[train2].shape(axis
===========================================================================================
Cleaned documentation: mod.fit_generator(datagen.flow(X, y, batch_size=100), steps_per_epoch=len(X) / 100, epochs=30, initial_epoch=0). history = mod.fit(X, y, epochs=2, batch_size=100, verbose=1, shuffle=True). gc.collect()
Processed documentation: mod.fit_generator(datagen.flow(X, y, batch_size=100), steps_per-epoch=len(X) / 100, epochs=30, initial_epoch =0
===========================================================================================
Cleaned documentation: Preparing test data to get predictions. I had to split the data into parts, because of memory constraints.
Processed documentation: I had to split the data into parts, because of memory constraints.
===========================================================================================
Cleaned documentation: def get_learner(data,layers,save_name='best_nn'):. return tabular_learner(data, layers=layers, metrics=[accuracy],. callback_fns=[partial(SaveModelCallback, monitor='val_loss', name=save_name)]. def get_learner_no_cb(data,layers):. return tabular_learner(data, layers=layers, metrics=[accuracy,QuadraticKappaScore()])
Processed documentation: def get_learner(data,layers,save-name='best_nn'):. return tabular_ learner( data, layers, metrics=[accuracy],. callback_fns=[partial(SaveModel
===========================================================================================
Cleaned documentation: predict = model.predict_generator(test_gen, steps=test_gen.samples/batch_size). threshold = 0.5. testdf['category'] = np.where(predict > threshold, 1,0)
Processed documentation: predict = model.predict_generator(test_gen, steps= test_gen.samples/batch_size). threshold = 0.5. testdf['category'] = np.where(predict >
===========================================================================================
Cleaned documentation: Zero Crossing Rate 🚷 Note: the rate at which the signal changes from positive to negative or back.
Processed documentation: Zero Crossing Rate is the rate at which the signal changes from positive to negative or back.
===========================================================================================
Cleaned documentation: For Plotting. Using plotly + cufflinks in offline mode. For time series decomposition. Pandas option
Processed documentation: Using plotly + cufflinks in offline mode. For time series decomposition. Pandas option.
===========================================================================================
Cleaned documentation: We see that 43.6% of sales come from California while Texas and Winscoin have comparable sales 27.6% and 28.8%
Processed documentation: We see that 43.6% of sales come from California while Texas and Winscoin have comparable sales 27.6%.
===========================================================================================
Cleaned documentation: Missing continuous values. Normalizing continuous variables. categorical variables. converting categorical variables to integer codes.
Processed documentation: Missing continuous values. Normalizing continuous variables. converting categorical variables to integer codes.
===========================================================================================
Cleaned documentation: X_l = load_obj_from_sets("X_l"). y_l = load_obj_from_sets("y_l"). X_test = load_obj_from_sets("X_test"). Select validation. Make train_X and train labels
Processed documentation: X_l = load_obj_from_sets("X"), y_l= load_ obj_from sets("y_l"), X_test = load obj_ from sets ("X_test"),
===========================================================================================
Cleaned documentation: pick one of the sites randomly. randomly crop. randomly crop the negative control image. center crop
Processed documentation: pick one of the sites randomly. randomly crop the negative control image. center crop.
===========================================================================================
Cleaned documentation: Now we oversample the negative class. There is likely a much more elegant way to do this...
Processed documentation: Now we oversample the negative class. There is likely a much more elegant way to do this.
===========================================================================================
Cleaned documentation: Libraries Required Go back to the Table of Contents Let's first install all the libraries required for the application.
Processed documentation: Let's first install all the libraries required for the application. Go back to the Table of Contents.
===========================================================================================
Cleaned documentation: Below plots show the analysis of Imaged sites with respect to Sex, Diagnosis and Tumour type.
Processed documentation: plots show the analysis of Imaged sites with respect to Sex, Diagnosis and Tumour type.
===========================================================================================
Cleaned documentation: model.add(Flatten()). model.add(Dense(100, activation='relu')). model.add(Dropout(0.3)). model.add(Conv1D(64,. padding='valid',. activation='relu')). model.add(Dropout(0.3)). model.add(Flatten()). model.add(Dense(100, activation='relu')). model.add(Dropout(0.5))
Processed documentation: model.add(Flatten()). model. add(Conv1D(64,. padding='valid',. activation='relu'), model.add (Dense(100, activation=' relu')), model. Add
===========================================================================================
Cleaned documentation: ds = ds.map(lambda img: image_decode(img)) ds = ds.map(lambda img, label: (tf.py_function(my_test, [img], tf.uint8),label))
Processed documentation: ds = ds.map(lambda img: image_decode(img), label: (tf.py_function(my_test, [ img], tf.uint8),label)) ds = dS.map
===========================================================================================
Cleaned documentation: Visualizing batch effects for cell line HEPG2. Interestingly, again, HEPG2-01~03, HEP-04, and HEPG2-05~07 are clustered toghther.
Processed documentation: HEP-04 and HEPG2-05~07 are clustered toghther. Interestingly, again, HEPG 2-01~03, HEP-03, and HEP -04 are clustered together.
===========================================================================================
Cleaned documentation: I changed the X dimension structure to have (Nsample, Nrows in frame, N columns in frame, 1) in load2d.
Processed documentation: I changed the X dimension structure to have (Nsample, Nrows in frame, N columns in frame) in load2d.
===========================================================================================
Cleaned documentation: Definitely. Onto data cleaning. Data Cleaning Going down the list, start with `age`
Processed documentation: Definitely. Onto data cleaning. Going down the list, start with `age`
===========================================================================================
Cleaned documentation: data_transforms = albumentations.Compose([. albumentations.Resize(224, 224),. albumentations.HorizontalFlip(),. albumentations.RandomBrightness(),. albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),. albumentations.JpegCompression(80),. albumentations.HueSaturationValue(),. albumentations.Normalize(),. AT.ToTensor(). albumentations.Random
Processed documentation: data_transforms = albumentations.Compose(224, 224),. albuments.JpegCompression(80),.Albums.Normalize() AT.ToTensor(). albums
===========================================================================================
Cleaned documentation: y_holdout = y[test_idx]. cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc'). print(" cross_score: %.5f" % (cross_score.mean()))
Processed documentation: y_holdout = y[test_idx]. cross_score = cross_val_score(clf, X_train, y_train,. cv=3, scoring='roc_auc'). print("
===========================================================================================
Cleaned documentation: Join sales time series with calendar dates. calendar_dts.head(). sales_train_l_dt = pd.merge(left=sales_train_l,right=calendar,left_on='d', right_on='d')
Processed documentation: Join sales time series with calendar dates. calendar_dts. head() sales_train_l_dt = pd.merge(left=sales_ train_l,right=calendar,left_on='
===========================================================================================
Cleaned documentation: Define Root Mean Squared Error. moving average. Exponential Series Average. Seasonal Naive. Exponential Average Smoothing
Processed documentation: Define Root Mean Squared Error. moving average. Exponential Series Average. Seasonal Naive.
===========================================================================================
Cleaned documentation: So you have patients that have multiple images. Also apparently the data is imbalanced. Let's verify:
Processed documentation: So you have patients that have multiple images. Also apparently the data is imbalanced.
===========================================================================================
Cleaned documentation: General information This kernel will be dedicated to EDA and other things. Work is in progress.
Processed documentation: This kernel will be dedicated to EDA and other things. Work is in progress.
===========================================================================================
Cleaned documentation: Training model on all data Now, let's train model on all data. I use AdaBound:
Processed documentation: Training model on all data. I use AdaBound:
===========================================================================================
Cleaned documentation: pred = model.predict(X_test, batch_size = 1024, verbose = 1). predictions = np.round(np.argmax(pred, axis=1)).astype(int). sub['prediction'] = predictions. sub.to_csv("submission.csv", index=False)
Processed documentation: pred = model.predict(X_test, batch_size = 1024, verbose = 1). predictions = np.round(np.argmax(pred, axis=1).astype(int). sub['pred
===========================================================================================
Cleaned documentation: model8 = build_model4(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs=5)
Processed documentation: model8 = build_model4(lr = 1e-4, lr_d = 1E-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units
===========================================================================================
Cleaned documentation: model9 = build_model5(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs=10)
Processed documentation: model9 = build_model5(lr = 1e-4, lr_d = 1E-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units
===========================================================================================
Cleaned documentation: Stacking And now let's try stacking :) We can use the same function for it.
Processed documentation: Stacking. We can use the same function for it. Let's try stacking.
===========================================================================================
Cleaned documentation: Functions used in this kernel They are in the hidden cell below.
Processed documentation: Functions used in this kernel are in the hidden cell below.
===========================================================================================
Cleaned documentation: test = test.sort_values('TransactionDT'). test['prediction'] = result_dict_xgb['prediction']. sub['isFraud'] = pd.merge(sub, test, on='TransactionID')['prediction']. sub.to_csv('submission_xgb.csv', index=False)
Processed documentation: test = test.sort_values('TransactionDT'). test['prediction'] = result_dict_xgb['predicted']. sub['isFraud'] = pd.merge(sub, test, on='
===========================================================================================
Cleaned documentation: test = test.sort_values('TransactionDT'). test['prediction'] = result_dict_lgb['prediction'] + result_dict_xgb['prediction']. sub['isFraud'] = pd.merge(sub, test, on='TransactionID')['prediction']. sub.to_csv('blend.csv', index=False)
Processed documentation: test = test.sort_values('TransactionDT'). test['prediction'] = result_dict_lgb('prediction']. sub['isFraud'] = pd.merge(sub, test,
===========================================================================================
Cleaned documentation: user_type There are three main user_types. Let's see prices of their wares, where prices are below 100000.
Processed documentation: There are three main user_types. Let's see prices of their wares, where prices are below 100000.
===========================================================================================
Cleaned documentation: Plotting countplot. Making annotations is a bit more complicated, because we need to iterate over axes.
Processed documentation: Plotting countplot.com is a bit more complicated, because we need to iterate over axes.
===========================================================================================
Cleaned documentation: It seems that almost one thousand pets have mismatch in breeds and fur lengths. Let's see!
Processed documentation: It seems that almost one thousand pets have mismatch in breeds and fur lengths.
===========================================================================================
Cleaned documentation: Some words/phrases seem to be useful, but it seems that different adoption speed classes could have similar important words...
Processed documentation: Some words/phrases seem to be useful, but it seems that different adoption speed classes could have similar important words.
===========================================================================================
Cleaned documentation: print('Fold', fold_n, 'started at', time.ctime()). print(f'Fold {fold_n}. AUC: {score:.4f}.'). print(''). feature importance
Processed documentation: print('Fold', fold_n,'started at', time.ctime()). print(f'Fold {fold_n}. AUC: {score:.4f}.'). print('''). feature
===========================================================================================
Cleaned documentation: SHAP Another interesting tool is SHAP. It also provides explanations for a variety of models.
Processed documentation: SHAP provides explanations for a variety of models. Another interesting tool is SHAP.
===========================================================================================
Cleaned documentation: submission = pd.read_csv('../input/sample_submission.csv'). submission['target'] = (prediction_lr + prediction_svc) / 2. submission.to_csv('submission.csv', index=False). submission.head()
Processed documentation: submission = pd.read_csv('../ input/sample_submission.csv') submission.target = (prediction_lr + prediction_svc) / 2. submission.to_ CSV('submission
===========================================================================================
Cleaned documentation: Sklearn feature selection Sklearn has several methods to do feature selection. Let's try some of them!
Processed documentation: Sklearn has several methods to do feature selection. Let's try some of them!
===========================================================================================
Cleaned documentation: train.loc[train['Census_GenuineStateName'] == 'UNKNOWN', 'Census_GenuineStateName'] = 'OFFLINE'. test.loc[test['Census_GenuineStateName'] == 'UNKNOWN', 'Census_GenuineStateName'] = 'OFFLINE'. train['Census_GenuineStateName'] = train['Census_GenuineStateName'].cat.remove_unused_categories(). test['Census_GenuineStateName'] = test['Census_GenuineStateName'].cat.remove_unused_categories()
Processed documentation: train.train.loc[train['Census_GenuineStateName'] == 'UNKNOWN', 'Census.Genuine stateName'] = 'OFFLINE' test. train.loc [test].cat.remove
===========================================================================================
Cleaned documentation: submission['HasDetections'] = (result_dict['prediction'] + result_dict1['prediction'] + result_dict2['prediction']) / 3. submission['HasDetections'] = (result_dict['prediction'] + result_dict1['prediction']) / 2
Processed documentation: submission['HasDetections'] = (result_dict['prediction'] + result_dict1]'s. "prediction" + "Prediction" s. "HasDetection"s "Predictions" s
===========================================================================================
Cleaned documentation: And here we have information on sell prices for all items in all stores by weeks.submission
Processed documentation: And here we have information on sell prices for all items in all stores by weeks.
===========================================================================================
Cleaned documentation: We have information about sales of 3049 various items, which belong to different categories and departments.
Processed documentation: We have information about sales of 3049 various items, which belong to different categories.
===========================================================================================
Cleaned documentation: The column seem to have a normal distribution. This reminds me of the recent Santander competition...
Processed documentation: The column seem to have a normal distribution. This reminds me of the recent Santander competition.
===========================================================================================
Cleaned documentation: sub = pd.read_csv('../input/sample_submission.csv'). pred = (logreg.predict_proba(test_vectorized) + logreg1.predict_proba(test_vectorized) + logreg2.predict_proba(test_vectorized)) / 3. sub['injection'] = pred. sub.head(). sub.to_csv('sub.csv', index=False)
Processed documentation: sub = pd.read_csv('../ input/sample_submission.csv'). pred = (logreg.predict_proba(test_ vectorized) + logreg1.p predict_proBA(
===========================================================================================
Cleaned documentation: In-depth per sample analysis Let's analyse individual audio files with all the above visualizations to get a better understanding.
Processed documentation: In-depth per sample analysis. Let's analyse individual audio files with all the above visualizations.
===========================================================================================
Cleaned documentation: Categorical transformer. This is a wrapper for categorical encoders. :param cat_cols: :param drop_original: :param encoder:
Processed documentation: Categorical transformer. This is a wrapper for categorical encoders.
===========================================================================================
Cleaned documentation: time. model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.1). oof_lr, prediction_lr, scores = train_model(X, X_test, y, params=None, folds=folds, model_type='sklearn', model=model)
Processed documentation: model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.1), scores = train_model(X, X_test, y, params=None, folds
===========================================================================================
Cleaned documentation: Most of comments aren't toxic. We can also see some spikes in the distribution...
Processed documentation: Most of comments aren't toxic. We can also see some spikes in the distribution.
===========================================================================================
Cleaned documentation: model_conv.cuda(). optimizer = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9). exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
Processed documentation: model_conv.cuda(). optimizer = optim.SGD(model_ Conv.parameters(), lr=0.001, momentum=0,9). exp_lr_scheduler = lr_sc
===========================================================================================
Cleaned documentation: Functions used in this kernel They are in the hidden cell below.
Processed documentation: Functions used in this kernel are in the hidden cell below.
===========================================================================================
Cleaned documentation: Generate Features. All boosters. All kills. All distance. Players in team. Players in match
Processed documentation: Generate Features. All boosters. All kills. All distance. Players in team.
===========================================================================================
Cleaned documentation: Less than 5 players in a team is ideal for an average winPlacePerc of more than 0.5
Processed documentation: Less than 5 players in a team is ideal for an average win place of more than 0.5.
===========================================================================================
Cleaned documentation: resampling imbalanced dataset using SMOTE/ADASYN. sm = SMOTE(random_state=12, ratio = 1.0). X_train_res, y_train_res = sm.fit_sample(X_train_scaled, y_train). type(X_train_res), type(y_train_res)
Processed documentation: resampling imbalanced dataset using SMOTE/ADASYN. sm = SMOTE(random_state=12, ratio = 1.0). X_train_res, y_train _res = sm.fit_
===========================================================================================
Cleaned documentation: Let us now pick a single Pixel cluster, I've picked a rather large one ...
Processed documentation: Let us now pick a single Pixel cluster, I've picked a rather large one.
===========================================================================================
Cleaned documentation: XGB Model XGB Model Training](10.-XGB-Model-Training) XGB Model](10.1-XGB-Model) XGB Feature Importance](10.2.XGB-Feature-Importance) XGB Submission](10.3.XGB-Submission)
Processed documentation: XGB Model XGB Model Training[10.1-XGB-Model] XGB Feature Importance [10.2.XGB - Feature-Importance] X GB Submission [10-3.X GB-Sub
===========================================================================================
Cleaned documentation: Catboost Model Training Catboost Model Training](11.-Catboost-Model-Training) Catboost Model](11.1-Catboost-Model) Catboost Feature Importance](11.2.Catboost-Feature-Importance) Catboost Submission](11.3.Catboost-Submission)
Processed documentation: Catboost Model Training is a training program for Catboost models. Catboost Model training is available in English and Spanish.
===========================================================================================
Cleaned documentation: time. train,target = SMOTE().fit_resample(train,target.ravel()). train = pd.DataFrame(train). display(train.head()). target = pd.Series(target). target.value_counts().plot(kind="barh")
Processed documentation: train,target = SMOTE().fit_resample(train,target.ravel()). train = pd.DataFrame(train). display(train.head()). target = pD.Series(target). target.
===========================================================================================
Cleaned documentation: max_depth': 9,. num_leaves': 39,. feature_fraction': 0.8108472661400657,. bagging_fraction': 0.9837558288375402,. lambda_l1': 0.06,. lambda_l2': 0.1,
Processed documentation: max_depth': 9,. num_leaves': 39,. feature_fraction': 0.8108472661400657,. bagging_fractions':0.9837558288375402,. lambda_
===========================================================================================
Cleaned documentation: rmse. One-hot encoding for categorical columns with get_dummies. Display/plot feature importance
Processed documentation: One-hot encoding for categorical columns with get_dummies. Display/plot feature importance.
===========================================================================================
Cleaned documentation: pred_val_y, pred_test_y = train_pred(model_cnn(embedding_matrix), epochs = 2). outputs.append([pred_val_y, pred_test_y, '2d CNN'])
Processed documentation: pred_val_y, pred_test_y = train_pred(model_cnn(embedding_matrix), epochs = 2). outputs.append('2d CNN'): false.
===========================================================================================
Cleaned documentation: scaler = MinMaxScaler(). train_df = scaler.fit_transform(train_df). train_df = pd.DataFrame(train_df). train_df.columns = tr_col. train_df
Processed documentation: scaler = MinMaxScaler() train_df = scaler.fit_transform(train_df). train_DF = pd.DataFrame(train-df) train-df.columns = tr_col
===========================================================================================
Cleaned documentation: Load Data TOP](Outline-of-the-notebook) The goal here is to predict whether or not status.
Processed documentation: The goal here is to predict whether or not status. will change.
===========================================================================================
Cleaned documentation: We now have a model with a CV score of 0.8032. Nice! Let's submit that. Make submission
Processed documentation: We now have a model with a CV score of 0.8032. Let's submit that.
===========================================================================================
Cleaned documentation: Transpose the input volume CXY to XYC order, which is what matplotlib requires.. plt.imshow(np.hstack((im.transpose(1,2,0)[...,:3], target_as_rgb)))
Processed documentation: Transpose the input volume CXY to XYC order, which is what matplotlib requires. plt.imshow(np.hstack((im.transpose(1,2,0)[...,:3],
===========================================================================================
Cleaned documentation: more conservative since it's an approximated value. more conservative since it's an approximated value
Processed documentation: more conservative since it's an approximated value.
===========================================================================================
Cleaned documentation: librosa.clicks We can add a click at the location of each detected onset
Processed documentation: librosa.clicks can add a click at the location of each detected onset.
===========================================================================================
Cleaned documentation: Listen to the original audio plus the detected onsets. One way is to add the signals together, sample-wise:
Processed documentation: Listen to the original audio plus the detected onsets. One way is to add the signals together.
===========================================================================================
Cleaned documentation: Compute the frame indices for estimated onsets in a signal:. frame numbers of estimated onsets
Processed documentation: Compute the frame indices for estimated onsets in a signal.
===========================================================================================
Cleaned documentation: Estimating Global TempoÂ¶ We will use librosa.beat.tempo to estimate the global tempo in an audio file. Estimate the tempo:
Processed documentation: Estimating Global TempoÂ¶ We will use librosa.beat.tempo to estimate the global tempo in an audio file.
===========================================================================================
Cleaned documentation: Use librosa.beat.beat_track to estimate the beat locations and the global tempo:
Processed documentation: Use librosa.beat.beat to estimate the beat locations and the global tempo.
===========================================================================================
Cleaned documentation: We are dealing with images with different size som RGB some in Grey Scale. I
Processed documentation: We are dealing with images with different size som RGB some in Grey Scale.
===========================================================================================
Cleaned documentation: Plot ROC curves. Plot the roc curve. Plot ROC curves. Plot the roc curve
Processed documentation: Plot ROC curves. Plot the roc curve.
===========================================================================================
Cleaned documentation: Approximated differentiable SMAPE. SMAPE, rounded up to the closest integet. SMAPE as Kaggle calculates it. MAE on log1p
Processed documentation: Approximated differentiable SMAPE. SMAPE, rounded up to the closest integet. MAE on log1p.
===========================================================================================
Cleaned documentation: parameters = {'max_depth' : list(range(5,20))}. base_model = DecisionTreeRegressor(). cv_model = GridSearchCV(base_model, param_grid=parameters, cv=5, return_train_score=True).fit(X_train,y_train)
Processed documentation: parameters = {'max_depth' : list(range(5,20))}. base_model = DecisionTreeRegressor. cv_model means GridSearchCV. return_train_score=True).fit(
===========================================================================================
Cleaned documentation: test_pred = model.predict(test_merged[X_1.columns]). test_pred_inv = np.exp(test_pred) -1. submission_predicted = pd.DataFrame({'Id': test['Id'], 'Sales': test_pred_inv }). submission_predicted.to_csv('submission.csv',index=False)
Processed documentation: test_pred = model.predict(test_merged[X_1.columns]. test_pred_inv = np.exp( test_Pred) -1. submission_predicted = pd.
===========================================================================================
Cleaned documentation: We drop the "Date" column and print all the places once. PS: I do not like asterisks
Processed documentation: We drop the "Date" column and print all the places once.
===========================================================================================
Cleaned documentation: Let's deal with regionidzip first. We could apply the same trick onto regionidzip as well as regionidcity
Processed documentation: Let's deal with regionidzip first. We could apply the same trick to regionidcity as well.
===========================================================================================
Cleaned documentation: sklearn не все гладко, чтобы в colab удобно выводить картинки. warnings
Processed documentation: sklearn.sklearn  чтопреграйте,    “Не  теперпран
===========================================================================================
Cleaned documentation: FC (fully conected) слоев. Накинуть скор можно за счет изменения параметров, конфигурации самой сети, подключения предобученной модели (например, ResNet).
Processed documentation: FC (fully conected)    “Накинуть  скор можно за счет из
===========================================================================================
Cleaned documentation: That's it. Let's make the submission. Last time I checked, the submission file returned 2688.84 (Private) and 2673.97 (Public).
Processed documentation: That's it. Let's make the submission. Last time I checked, the submission file returned 2688.84 (Private)
===========================================================================================
Cleaned documentation: Bringing all the calculated values into the same dataframe i.e values of q0.05 , q0.5 , 0.95 !!
Processed documentation: Bringing all the calculated values into the same dataframe i.e values of q0.05, q00.5, 0.95!!
===========================================================================================
Cleaned documentation: hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, Y_test)). hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,. show_accuracy=True, verbose=1, validation_split=0.2)
Processed documentation: hist = model.fit(X_train, Y_train), show_accuracy=True, verbose=1, validation_data=(X_test, Y-test), validation_split=0.2.
===========================================================================================
Cleaned documentation: if blackout :. dataset = dataset.map(transform_random_blackout, num_parallel_calls = AUTO). dataset = dataset.map(micro, num_parallel_calls = AUTO)
Processed documentation: if blackout :. dataset = dataset.map(transform_random_blackout, num_parallel_calls = AUTO). dataset. map(micro, num parallel calls=AUTO) dataset.
===========================================================================================
Cleaned documentation: Block 1. Block 2. Block 3. Block 4. Block 5. Final Block. Compile. model.fit(X,y,validation_split=0.1,batch_size=batch_size,epochs=epochs)
Processed documentation: Block 1. Block 2. Block 3. Block 4. Block 5. Compile. model.fit(X,y,validation_split=0.1,batch_size=batch_ size,epochs=
===========================================================================================
Cleaned documentation: train_dataset = train_dataset.map(cutmix,num_parallel_calls = AUTO). train_dataset = train_dataset.map(transform_rotation, num_parallel_calls = AUTO). train_dataset = train_dataset.map(gridmask, num_parallel_calls = AUTO)
Processed documentation: train_dataset = train_ dataset.map(cutmix,num_parallel_calls = AUTO). train_dat aset. map(transform_rotation, num_par parallel_c
===========================================================================================
Cleaned documentation: BUILD 512 SEPARATE MODELS. avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader). print(roc_auc_score(train2.loc[test_index]['target'],valid_preds_fold)). print(test_preds_fold). PRINT CV AUC
Processed documentation: BUILD 512 SEPARATE MODELS. avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader). print(roc_auc_score(train2.
===========================================================================================
Cleaned documentation: remove outliers. Also returns data for the outliers. Maye I need to keep them...
Processed documentation: remove outliers. Also returns data for the outliers, which I need to keep.
===========================================================================================
Cleaned documentation: X = tf.placeholder(dtype=tf.float32, shape=[None, (len(group_cols)2 + len(other_cols))], name='X'). X = tf.placeholder(dtype=tf.float32, shape=[None, (len(group_cols) + len(other_cols)), 2], name='X')
Processed documentation: X = tf.placeholder(dtype=tf.float32, shape=[None, (len(group_cols)2 + len(other_Cols)], name='X'). X =tf.place
===========================================================================================
Cleaned documentation: It seems like businesses' sales grow over time, which is good. Group 2:
Processed documentation: It seems like businesses' sales grow over time, which is good.
===========================================================================================
Cleaned documentation: MİMARİNİN OLUŞTURULMASI. adet 5x5 boyutunda filtrelerden oluşan ve relu aktivasyonlu konvolüsyon katmanı. boyutunda filtelerden oluşan max_pool katmanı
Processed documentation: MİMARİNİn OLUŞTURULMASI. adet 5x5 boyutunda filtrelerden oluşan ve relu aktivasyonlu kon
===========================================================================================
Cleaned documentation: OLUŞTURULAN MİMARİ İLE DEEP LEARNING NETWORK (DNN) MODELİ OLUŞTURULMASI. VERİLERLE EĞİTİM YAPILMASI
Processed documentation: OLUŞTURULAN MİMARİ İLE DEEP LEARNING NETWORK (DNN) MODELİ OLUşTURulMASI. VERİLERLE E
===========================================================================================
Cleaned documentation: from sklearn.linear_model import LogisticRegression. clf = LogisticRegression(C=0.1, solver='sag'). scores = cross_val_score(clf, x_train_res,y_train_res, cv=5,scoring='f1_weighted'). scores
Processed documentation: from sklearn.linear_model import LogisticRegression. clf = Logisticregression(C=0.1, solver='sag') scores = cross_val_score(clf, x_
===========================================================================================
Cleaned documentation: from sklearn.linear_model import LogisticRegression. clf1 = LogisticRegression(C=0.1, solver='sag'). scores = cross_val_score(clf1, X_d,y_d, cv=5,scoring='f1_weighted')
Processed documentation: from sklearn.linear_model import LogisticRegression. clf1 = Logisticregression(C=0.1, solver='sag') scores = cross_val_score(clf1, X
===========================================================================================
Cleaned documentation: The format of the submission file is also not good for data science...
Processed documentation: The format of the submission file is also not good for data science.
===========================================================================================
Cleaned documentation: As per the contest notes, this should end at "d_1941," so that's what we should see here...
Processed documentation: As per the contest notes, this should end at "d_1941," so that's what we should see here.
===========================================================================================
Cleaned documentation: Uncomment to load the pickle files.... df_train = pd.read_pickle('df_train_pickle.pkl'). df_test = pd.read_pickle('df_test_pickle.pkl'). df_submission = pd.read_pickle('df_submissions_pickle.pkl')
Processed documentation: Uncomment to load the pickle files. df_train = pd.read_pickle('df_train_Pickle.pkl') df_test = pD.read-pickle ('df_test
===========================================================================================
Cleaned documentation: Examples Here's a dirty `plot_play()` method with some examples including links to youtube videos with the corresponding play.
Processed documentation: Here's a dirty `plot_play()` method with some examples including links to youtube videos.
===========================================================================================
Cleaned documentation: In order to feed the CNN model we need to reshape our $54000$x$784$ flatten image data to $54000$x$28$x$28$x$1$ dimensions.;
Processed documentation: In order to feed the CNN model we need to reshape our $54000$x$784$ flatten image data.
===========================================================================================
Cleaned documentation: Next, we'll load in the metadata along with a single batch of images to work on in this example.
Processed documentation: Next, we'll load in the metadata along with a single batch of images to work on.
===========================================================================================
Cleaned documentation: Part 2 - Quick visualisations Let's start with some basic histograms, showing the distribution of accuracy and time.
Processed documentation: Part 2 - Quick visualisations. Let's start with some basic histograms, showing the distribution of accuracy.
===========================================================================================
Cleaned documentation: Try some more. Calculate the KDE. Plot. KDE only. Overplot the points
Processed documentation: Try some more. Calculate the KDE. Plot. Overplot the points.
===========================================================================================
Cleaned documentation: add lookahead. lookahead = Lookahead(k=5, alpha=0.5) Initialize Lookahead. lookahead.inject(model) add into model
Processed documentation: lookahead = Lookahead(k=5, alpha=0.5) Initialize Lookahead. lookahead.inject(model) add into model.
===========================================================================================
Cleaned documentation: A, .B, .Pronoun { font-weight: bold }.True { color: green }.False { color: red }. Add styles
Processed documentation: A,.B,.Pronoun { font-weight: bold }.True { color: green }.False {color: red }. Add styles
===========================================================================================
Cleaned documentation: I notice that the infinity norm is related to the range of a uniform distribution...
Processed documentation: I notice that the infinity norm is related to the range of a uniform distribution.
===========================================================================================
Cleaned documentation: If you found these fetures are helpful to you, please do upvote. for reading
Processed documentation: If you found these fetures helpful to you, please do upvote for reading.
===========================================================================================
Cleaned documentation: It looks pretty OK to me. What do you think? Let's save it then!!
Processed documentation: It looks pretty OK to me. Let's save it then!!
===========================================================================================
Cleaned documentation: We will also need a function for generating our `transformation` for `train`, `valid` and `test` loaders.
Processed documentation: We will also need a function for generating our `transformation` for `train' and 'valid' loaders.
===========================================================================================
Cleaned documentation: split the data based on quality factor of 75, 90 and 95. print(df_qf_tr['Label'].value_counts()). print(df_qf_val['Label'].value_counts()). print(df_qf_test['Label'].value_counts()). save the splits
Processed documentation: split the data based on quality factor of 75, 90 and 95. print(df_qf_tr['Label'].value_counts()). print(DF_qF_val['Label]].value
===========================================================================================
Cleaned documentation: split the data based on stego scheme of JMiPOD, UERD, JUNIWARD. print(df_stego_tr['Label'].value_counts()). print(df_stego_val['Label'].value_counts()). print(df_stego_test['Label'].value_counts()). save the splits
Processed documentation: split the data based on stego scheme of JMiPOD, UERD, JUNIWARD. print(df_stego_tr['Label'].value_counts()). print(DF_
===========================================================================================
Cleaned documentation: Generate text features:. Initialize decomposition methods:. Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
Processed documentation: Generate text features: Initialize decomposition methods: Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
===========================================================================================
Cleaned documentation: Manually adjusted coefficients:. print("Train Predictions Counts = ", Counter(train_predictions)). print("Test Predictions Counts = ", Counter(test_predictions))
Processed documentation: Manually adjusted coefficients:. print("Train Predictions Counts = ", Counter(train_predictions). print("Test Predictionscounts =!", Counter(test_prediction))
===========================================================================================
Cleaned documentation: Remove chirstmas date. dates = dates[~dates['ds'].isin(ignore_date)]. Remove zero day. dates = dates[dates['y'] > 0]
Processed documentation: Remove chirstmas date. dates = dates[~dates['ds'].isin(ignore_date)]. Remove zero day. dates[dates['y'] > 0]
===========================================================================================
Cleaned documentation: Decision Tree. from sklearn import tree. from sklearn.tree import DecisionTreeClassifier. parameters={'max_depth':range(1,10)}. clf=GridSearchCV(tree.DecisionTreeClassifier(),param_grid=parameters,n_jobs=-1,cv=10). clf.fit(X_train_data,y_train_data). print(clf.best_score_). print(clf.best_params_)
Processed documentation: From sklearn.tree import DecisionTreeClassifier. parameters={'max_depth':range(1,10)}. clf=GridSearchCV(tree.DecisionTreeClassifiers,param_grid=parameters,
===========================================================================================
Cleaned documentation: Random Forest. from sklearn.ensemble import RandomForestClassifier. rfc = RandomForestClassifier(). rfc.fit(X = X_train_data,y = y_train_data)
Processed documentation: Random Forest. classifier. rfc = RandomForestClassifier(). rfc.fit(X = X_train_data,y = y_train-data)
===========================================================================================
Cleaned documentation: Create adaboost-decision tree classifer object. Adaboost_model = AdaBoostClassifier(. DecisionTreeClassifier(max_depth=5),. n_estimators = 600,. learning_rate = 1)
Processed documentation: Adaboost_model = AdaBoostClassifier(max_depth=5),. n_estimators = 600,. learning_rate = 1) Adaboost-decision tree classifer object.
===========================================================================================
Cleaned documentation: submission. finalsubmission_ada = pd.DataFrame({'installation_id': group_ada_pred_2.index,'accuracy_group': group_ada_pred_2['accuracy_group']}). finalsubmission_ada.index = sample_submission.index. finalsubmission_ada.to_csv('submission.csv', index=False)
Processed documentation: submission. finalsubmission_ada = pd.DataFrame({'installation_id': group_ada_pred_2.index,'accuracy_group': group-ada-pred-2.accuracy,
===========================================================================================
Cleaned documentation: Gradient Boost. from sklearn.ensemble import GradientBoostingClassifier. GBM_model = GradientBoostingClassifier(n_estimators=50,. learning_rate=0.3,. subsample=0.8)
Processed documentation: Gradient Boost. GBM_model = GradientBoostingClassifier.(n_estimators=50,. learning_rate=0.3,. subsample= 0.8)
===========================================================================================
Cleaned documentation: submission.... finalsubmission_gb = pd.DataFrame({'installation_id': group_gb_pred.index,'accuracy_group': group_gb_pred['accuracy_group']}). finalsubmission_gb.index = sample_submission.index. finalsubmission_gb.to_csv('submission.csv', index=False)
Processed documentation: submission.... finalsubmission_gb = pd.DataFrame({'installation_id': group_gb_pred.index,'accuracy_group': group-gb-pred['accurate_group']}).
===========================================================================================
Cleaned documentation: Worldmeter Infor update Because data update one time per day. I update lastest information from worldometers for reference only
Processed documentation: Worldmeter Infor update Because data update one time per day.
===========================================================================================
Cleaned documentation: Getting Bounding Box of dog (ROI) in Image. crop image. display crop image
Processed documentation: Getting Bounding Box of dog (ROI) in Image. crop image.
===========================================================================================
Cleaned documentation: Basic exploration Just look at image dimensions, confirm it's 3 band (RGB), byte scaled (0-255).
Processed documentation: Image dimensions are 3 band (RGB), byte scaled (0-255)
===========================================================================================
Cleaned documentation: even subsampling is throwing memory error for me, :p. length = np.shape(pixel_matrix)[0]. rand_ind = np.random.choice(length, size=50000). sns.pairplot(pixel_matrix[rand_ind,:])
Processed documentation: even subsampling is throwing memory error for me, :p. length = np.shape(pixel_matrix)[0]. rand_ind = numpy.random.choice(length, size=50000). sns
===========================================================================================
Cleaned documentation: test detection on one image. test_filename[64]),. output_type = 'array',. show the result
Processed documentation: test detection on one image. output_type = 'array',. show the result.
===========================================================================================
Cleaned documentation: Create the FCN and return a keras model.. Input image: 75x75x3. x32. x64. x128. x128. x2
Processed documentation: Create the FCN and return a keras model. Input image: 75x75x3. x64. x128. x2.
===========================================================================================
Cleaned documentation: From. Thank you!!. If you have preprocessed data, input here and delete process method.. x_train = pd.read_csv('../input//train.csv')
Processed documentation: If you have preprocessed data, input here and delete process method.. x_train = pd.read_csv('../input//train.csv')
===========================================================================================
Cleaned documentation: About 10min in Kernel. Add your feature here. Add your feature here
Processed documentation: About 10min in Kernel. Add your feature here.
===========================================================================================
Cleaned documentation: g = np.argmax(g.cpu().detach().numpy(), axis=1). v = np.argmax(v.cpu().detach().numpy(), axis=1). c = np.argmax(c.cpu().detach().numpy(), axis=1)
Processed documentation: g = np.argmax(g.cpu().detach().numpy(), axis=1). v = np arg max(v.cpu() detach() numpy()() axis=1) c
===========================================================================================
Cleaned documentation: fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values
Processed documentation: fill up the missing values. Pad the sentences. Get the target values.
===========================================================================================
Cleaned documentation: Convert Date (TransactionDT(sec) is a difference from "2017/11/01"). Year, Month, Days, Weekdays, Hour, Minitue. Set Dtypes. Diff Now Time
Processed documentation: Convert Date (DT(sec) is a difference from "2017/11/01"). Year, Month, Days, Weekdays, Hour, Minitue. Set Dtypes. Diff Now Time
===========================================================================================
Cleaned documentation: PhotoInterpretation',. Orientation0', 'Orientation1', 'Orientation2', 'Orientation3', 'Orientation4', 'Orientation5',. PixelSpacing0', 'PixelSpacing1']. ds.PhotometricInterpretation,. ds.ImageOrientationPatient,. ds.PixelSpacing]
Processed documentation: PhotoInterpretation',. Orientation0', 'Orientation1','Orientation2','orientation3', 'orientation4', 'orientation5']. ds.PhotometricInterpret
===========================================================================================
Cleaned documentation: Generate Kaggle submission The code below can be used to generate a Kaggle submission file.
Processed documentation: The code below can be used to generate a Kaggle submission file.
===========================================================================================
Cleaned documentation: TARGET value 0 means loan is repayed, value 1 means loan is not repayed.
Processed documentation: TARGET value 0 means loan is repayed, value 1 means loan not repayed.
===========================================================================================
Cleaned documentation: from sklearn.feature_extraction.text import TfidfVectorizer. from sklearn.linear_model import LogisticRegression. from sklearn.model_selection import cross_val_score. from scipy.sparse import hstack
Processed documentation: from sklearn.feature_extraction.text import TfidfVectorizer. from sklearning.linear_model import LogisticRegression. from scipy.sparse import hstack.
===========================================================================================
Cleaned documentation: I'll update more in coming future so watch this space. Please upvote to encourage me. Thank you all :)
Processed documentation: I'll update more in coming future so watch the space. Please upvote to encourage me.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.. Endzones
Processed documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
===========================================================================================
Cleaned documentation: from fastai.callbacks.tracker import EarlyStoppingCallback. cbs=[EarlyStoppingCallback(learn, monitor='auroc', min_delta=0.001, patience=3, )]. with experiment.train():. learn.fit_one_cycle(30, 1e-4, wd=0.2, callbacks=cbs)
Processed documentation: from fastai.callbacks.tracker import EarlyStoppingCallback. cbs=[EarlyStopping callback(learn, monitor='auroc', min_delta=0.001, patience=3, )]. with experiment
===========================================================================================
Cleaned documentation: Imports We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`.
Processed documentation: Imports We are using a typical data science stack: numpy, pandas, sklearn, matplotlib.
===========================================================================================
Cleaned documentation: Imports We are using a typical data science stack: ``numpy``, ``pandas``, ``sklearn``, ``matplotlib``.
Processed documentation: Imports We are using a typical data science stack: numpy, pandas, sklearn, matplotlib.
===========================================================================================
Cleaned documentation: Nullity Matrix The msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion
Processed documentation: nullity matrix is a data-dense display which lets you quickly visually analyse data.
===========================================================================================
Cleaned documentation: Imports We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`.
Processed documentation: Imports We are using a typical data science stack: numpy, pandas, sklearn, matplotlib.
===========================================================================================
Cleaned documentation: Use can use train_df.pkl, test_df.pkl for FE, FS for your baseline_predict
Processed documentation: Use can use train_df, test_df.pkl for FE, FS for your baseline_predict.
===========================================================================================
Cleaned documentation: Nullity Matrix The msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion.
Processed documentation: nullity matrix is a data-dense display which lets you quickly visually analyse data.
===========================================================================================
Cleaned documentation: We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`.
Processed documentation: We are using a typical data science stack: numpy, pandas, sklearn, matplotlib.
===========================================================================================
Cleaned documentation: Goal: for each building and meter pair, visualize where target is missing and where target is zero VS time.
Processed documentation: Goal: for each building and meter pair, visualize where target is missing.
===========================================================================================
Cleaned documentation: Use can use train_df.pkl, test_df.pkl for FE, FS for your baseline_predict
Processed documentation: Use can use train_df, test_df.pkl for FE, FS for your baseline_predict.
===========================================================================================
Cleaned documentation: Create base models. Parameters. Create parameters to use for Grid Search. Search parameter space
Processed documentation: Create base models. Create parameters to use for Grid Search.
===========================================================================================
Cleaned documentation: Compare distributions of training set and test set (Adoption Speed). Plot 1. Plot 2
Processed documentation: Compare distributions of training set and test set (Adoption Speed)
===========================================================================================
Cleaned documentation: For autoreloading modules. For notebook plotting. Standard libraries. Visualization. Machine Learning. Directories. Info about dataset
Processed documentation: For autoreloading modules. For notebook plotting. Standard libraries. Machine Learning. Directories.
===========================================================================================
Cleaned documentation: Use this code if you want to save the figure. fig = heatmap.get_figure(). fig.savefig("Heatmap(TopFeatures).png")
Processed documentation: Use this code if you want to save the figure.
===========================================================================================
Cleaned documentation: A custom implementation of EfficientNetB5 for the APTOS 2019 competition (Regression). Initialize model
Processed documentation: A custom implementation of EfficientNetB5 for the APTOS 2019 competition (Regression)
===========================================================================================
Cleaned documentation: create shared variables that can be changed later on. Model error. Data likelihood
Processed documentation: create shared variables that can be changed later. Model error. Data likelihood.
===========================================================================================
Cleaned documentation: create shared variables that can be changed later on. Model error. Data likelihood
Processed documentation: create shared variables that can be changed later. Model error. Data likelihood.
===========================================================================================
Cleaned documentation: create shared variables that can be changed later on. Model error. Data likelihood. Fitting the model
Processed documentation: create shared variables that can be changed later. Model error. Data likelihood. Fitting the model.
===========================================================================================
Cleaned documentation: Convert to Hounsfield Units Getting to the code below was really painful. Credits to lots of people.
Processed documentation: Convert to Hounsfield Units.
===========================================================================================
Cleaned documentation: Quant loss No news here vs. baseline notebook. Again, credits to [Ceshine Lee's quantile regression tutorial](
Processed documentation: Quant loss No news here vs. baseline notebook.
===========================================================================================
Cleaned documentation: These functions are used to calculate the performance of our 'predictions' at this competitions metric: the logarithmic loss.
Processed documentation: These functions are used to calculate the performance of our 'predictions' at this metric: the logarithmic loss.
===========================================================================================
Cleaned documentation: y_hat = grid.predict_proba(X)[:, 1]. score = roc_auc_score(y, y_hat). print("Overall AUC: {:.3f}" .format(score))
Processed documentation: y_hat = grid.predict_proba(X), score = roc_auc_score(y, y_hat). print("Overall AUC: {:.3f}".format(score), y_
===========================================================================================
Cleaned documentation: This is straight up just the % difference between the two images in gayscale. Structural Similarity Index (SSIM)
Processed documentation: This is straight up just the % difference between the two images in gayscale.
===========================================================================================
Cleaned documentation: Just splits the image in half, then flips one, then compares the difference between the two. imshow(diff). get_symmetry('../input/recognizing-faces-in-the-wild/train/F0002/MID2/P00017_face2.jpg')
Processed documentation: Just splits the image in half, then flips one, then compares the difference between the two. imshow(diff). get_symmetry('../input/recognizing-faces-in-the-wild/train/F
===========================================================================================
Cleaned documentation: Again, LB is not fooled. No variable can predict target better than 0.54 LB.
Processed documentation: No variable can predict target better than 0.54 LB.
===========================================================================================
Cleaned documentation: SUBMIT 84 IMAGES OF EACH OF 119 BREEDS and 4 of breed 120. if (i%10==0)|(i==119): print(i84)
Processed documentation: SUBMIT 84 IMAGES OF EACH OF 119 BREEDS and 4 of breed 120. if (i%10==0) print(i84)
===========================================================================================
Cleaned documentation: Define Variable Groupings All variables in the same group get "squeezed" together and thus the group's dimension is reduced.
Processed documentation: Define Variable Groupings All variables in the same group get "squeezed" together. The group's dimension is reduced.
===========================================================================================
Cleaned documentation: STRATIFIED K FOLD. print("Current Fold: {}".format(fold_)). BUILD MODEL. CALLBACKS. TRAIN. PREDICT TEST. PREDICT OOF. RECORD AUC
Processed documentation: STRATIFIED K FOLD. print("Current Fold: {}".format(fold_). BUILD MODEL. CALLBACKS. PREDICT TEST. RECORD AUC.
===========================================================================================
Cleaned documentation: Build 512 SVM SVM has been shown to score LB 0.928 [here][1]. We will ensemble this with our NN.
Processed documentation: Build 512 SVM SVM has been shown to score LB 0.928. We will ensemble this with our NN.
===========================================================================================
Cleaned documentation: LIST COLUMNS TO LOAD. USEFUL V COLUMNS. DECLARE COLUMN DTYPES. for c in str_type: dtypes[c] = 'str'
Processed documentation: LIST COLUMNS TO LOAD. DECLARE COLUMN DTYPES. for c in str_type: dtypes[c] ='str'
===========================================================================================
Cleaned documentation: Helper Functions Below are functions to manipulate `rle` masks. Click "code" to the right to see the code.
Processed documentation: helper functions to manipulate `rle` masks. Click "code" to the right to see the code.
===========================================================================================
Cleaned documentation: original_feature_set = ['building_age', 'le_primary_use', 'cloud_coverage',. is_weekend','wind_speed', 'day_of_week',. wind_compass_direction', 'sea_level_pressure', 'air_temperature',. day_of_month', 'dew_temperature', 'hour',. month', 'meter', 'building_id',. site_id', 'square_feet']
Processed documentation: original_feature_set = ['building_age', 'le_primary_use', 'cloud_coverage',. is_weekend','wind_speed', 'day_of_week',. wind_compass_
===========================================================================================
Cleaned documentation: we included meter_reading in the initial feature set. which is not a feature. replace meter_reading with precip_depth_1_hr
Processed documentation: we included meter_reading in the initial feature set. which is not a feature. replace meter_read with precip_depth_1_hr.
===========================================================================================
Cleaned documentation: Categorical Cross Entropy with Label Smoothing. Label Smoothing is done to enhance accuracy
Processed documentation: Categorical Cross Entropy with Label Smoothing.
===========================================================================================
Cleaned documentation: Use Kfold to get robusted score. freeze bert. Start training. Start test. save best model
Processed documentation: Use Kfold to get robusted score. freeze bert. save best model. Start training.
===========================================================================================
Cleaned documentation: fill up the missing values. x_train, x_test, y_train, word_index = load_and_prec(). x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec()
Processed documentation: fill up the missing values. x_train, x_test, y_train,. word_index = load_and_prec()
===========================================================================================
Cleaned documentation: word_vectorizer = TfidfVectorizer(sublinear_tf=True,. strip_accents='unicode',. analyzer='word',. token_pattern=r'\w{1,}',. ngram_range=(1,2),. max_features=30000). word_vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,. min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,. smooth_idf=1, sublinear_tf=1 )
Processed documentation: word_vectorizer = TfidfVectorizer(sublinear_tf=True,. strip_accents='unicode',. analyzer='word',. token_pattern=r'\w{1,}',
===========================================================================================
Cleaned documentation: char_vectorizer = TfidfVectorizer(. sublinear_tf=True,. strip_accents='unicode',. tokenizer=char_analyzer,. analyzer='word',. ngram_range=(1, 3),. max_df=0.9,. max_features=60000) 50k. char_vectorizer.fit(all_text). train_char_features = char_vectorizer.transform(train_text). test_char_features = char_vectorizer.transform(test_text)
Processed documentation: char_vectorizer = TfidfVectorizer( sublinear_tf=True,. strip_accents='unicode',. tokenizer=char_analyzer,. analyzer='word',. ngram_range=(
===========================================================================================
Cleaned documentation: For using rfe selected features. X_train = X_train[cols]. X_test = X_test[cols]
Processed documentation: For using rfe selected features, use X_train and X_test.
===========================================================================================
Cleaned documentation: Function to decribe Categorical data. Decribing categorical variables. def decribe_Categorical(x):. from IPython.display import display, HTML. display(HTML(x[x.columns[x.dtypes =="category"]].describe().to_html)). decribe_Categorical(train_data)
Processed documentation: Function to decribe Categorical data. Decribing categorical variables.
===========================================================================================
Cleaned documentation: creating dummies columns categorical varibles. FUNCTION TO CREATE DUMMIES COLUMNS FOR CATEGORICAL VARIABLES. dropping the original columns
Processed documentation: creating dummies columns categorical varibles. dropping the original columns.
===========================================================================================
Cleaned documentation: FITTING RANDOM MODEL. Obtain class predictions. Obtain probability predictions. CONFUSION MATRIX
Processed documentation: FITTING RANDOM MODEL. Obtain class predictions and probability predictions. CONFUSION MATRIX
===========================================================================================
Cleaned documentation: bagging_freq': 5,. bagging_fraction': 0.8,. reg_lambda': 0.3,. lgbm. mlp. knn. for SEED in np.arange(third_number):
Processed documentation: bagging_freq': 5, bagging_fraction': 0.8, reg_lambda': 0, lgbm.mlp. knn.arange(third_number): 3.
===========================================================================================
Cleaned documentation: Restore some columns. Sort, rank, and assign adjusted ratio. Deal with edge cases. Align with maxPlace. Credit:. Edge case
Processed documentation: Restore some columns. Sort, rank, and assign adjusted ratio. Deal with edge cases. Align with maxPlace.
===========================================================================================
Cleaned documentation: walkDistance : Total distance traveled on foot measured in meters. original picture link](
Processed documentation: walkDistance : Total distance traveled on foot measured in meters.
===========================================================================================
Cleaned documentation: AvSig Version has many values. Length : 8531 And some values is broken. like `1.2&x17;3.1144.0`
Processed documentation: AvSig Version has many values. And some values are broken. like 1.2&x17;3.1144.0.
===========================================================================================
Cleaned documentation: Feature Engineering : 모델이 학습할 수 있도록 데이터를 준비하고 feature를 선별하는 단계. category value-> numeric part(직접해보기)
Processed documentation: Feature Engineering :    ‘ 학’습할   ‘  ’ ‘’ ‘”  “’” ‘ ‘
===========================================================================================
Cleaned documentation: Label encoder를 활용하면 위의 작업을 더 쉽게할 수 있음. NA value를 채우기 위해 더미 변수 생성
Processed documentation: Label encoder    ‘’’   ‘”’ “”  ”” ’‘ ’ ’
===========================================================================================
Cleaned documentation: Extract features for training images. Run following commands if required pacakges have not been installed in osgoenv environment
Processed documentation: Extract features for training images. Run following commands if required pacakges have not been installed.
===========================================================================================
Cleaned documentation: unfreeze and search appropriate learning rate for full training. learn.lr_find(start_lr=slice(1e-6, 1e-5), end_lr=slice(1e-2, 1e-1), wd=1e-3). learn.recorder.plot(suggestion=True)
Processed documentation: unfreeze and search appropriate learning rate for full training. learn.lr_find(start_lr=slice(1e-6, 1e-5), end_lr =slice(2e-1, 2e
===========================================================================================
Cleaned documentation: train all layers. learn.recorder.plot_losses(). schedule of the lr (left) and momentum (right) that the 1cycle policy uses. learn.recorder.plot_lr(show_moms=True)
Processed documentation: train all layers. learn.recorder.plot_losses(). schedule of the lr (left) and momentum (right) that the 1cycle policy uses.
===========================================================================================
Cleaned documentation: train all layers. learn.recorder.plot_losses(). schedule of the lr (left) and momentum (right) that the 1cycle policy uses. learn.recorder.plot_lr(show_moms=True)
Processed documentation: train all layers. learn.recorder.plot_losses(). schedule of the lr (left) and momentum (right) that the 1cycle policy uses.
===========================================================================================
Cleaned documentation: Domain knowledge Some domain knowledge can be gained from watching the following video and from reading [here.](
Processed documentation: Domain knowledge can be gained from watching the following video and from reading [here.]
===========================================================================================
Cleaned documentation: CHECK FOR DUPLICATES & DEAL WITH THEM. keep = False: All duplicates will be shown. dupRows_df.head()
Processed documentation: Keep = False: All duplicates will be shown. dupRows_df.head()
===========================================================================================
Cleaned documentation: Generate text features:. Initialize decomposition methods:. Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
Processed documentation: Generate text features: Initialize decomposition methods: Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
===========================================================================================
Cleaned documentation: Adding '_{quantile}_validation'. Start with a fresh w_df.. Use tao = 0.005. Add 'sub_id' column. Add tao onto sub_id
Processed documentation: Add'sub_id' column. Add tao onto sub_id. Use tao = 0.005. Add 'quantile_validation' to w_df.
===========================================================================================
Cleaned documentation: Creating Dataset Let's raise a glass to create the Dataset! champaign-clipart-champagne-glass-6.png](attachment:champaign-clipart-champagne-glass-6.png) class
Processed documentation: Creating Dataset Let's raise a glass to create the Datasets.
===========================================================================================
Cleaned documentation: Domain knowledge Some domain knowledge can be gained from watching the following video and from reading [here.](
Processed documentation: Domain knowledge can be gained from watching the following video and from reading [here.]
===========================================================================================
Cleaned documentation: some config values. fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values
Processed documentation: some config values. fill up the missing values. Pad the sentences. Get the target values.
===========================================================================================
Cleaned documentation: Hmm seems like numbers also are a problem. Lets check the top 10 embeddings to get a clue.
Processed documentation: Hmm seems like numbers also are a problem. Lets check the top 10 embeds to get a clue.
===========================================================================================
Cleaned documentation: Looks good. No obvious oov words there we could quickly fix. Thank you for reading and happy kaggling
Processed documentation: Looks good. No obvious oov words there we could quickly fix.
===========================================================================================
Cleaned documentation: landmark id = 1の画像を確認 Confirmation of images with landmark id = 1.
Processed documentation: landmark id = 1. Confirmation of images with landmark id =1.
===========================================================================================
Cleaned documentation: print(c) 辞書型で、それぞれのidに対する個数が出ている c is the type of dictionaly and composed of the id and the number of each id.
Processed documentation: print(c) is the type of dictionaly and composed of the id and the number of each id.
===========================================================================================
Cleaned documentation: id = 138982が6272個で一番多いので、どういう画像か見てみる Since id = 138982 is the most with 6272, let's see what kind of image.
Processed documentation: id = 138982 is the most with 6272, let's see what kind of image.
===========================================================================================
Cleaned documentation: idを関連付けてconfidenceを出すのが課題。 The challenge is to associate the id of the training image with this test image to give confidence.
Processed documentation: The challenge is to associate the id of the training image with this test image to give confidence.
===========================================================================================
Cleaned documentation: testデータのid, landmark id, confidence（自信)の順になっている。 The order of submission file is the test data id, landmark id, and confidence.
Processed documentation: The order of submission file is the test data id, landmark id, and confidence.
===========================================================================================
Cleaned documentation: sample data for quick test. train_time = train_time[:, ::100]. train_signal = train_signal[:, ::100]. train_opench = train_opench[:, ::100]
Processed documentation: sample data for quick test. train_time = train.time[:, ::100]. train.signal = train_signal[,. ::100] train.opench = train opench[,.::100]
===========================================================================================
Cleaned documentation: Initial momentum (in GeV/c) along 'X' and 'Y' axis with particle charges(Positive or Negative).
Processed documentation: Initial momentum (in GeV/c) along 'X' and 'Y' axis with particle charges.
===========================================================================================
Cleaned documentation: news_train['subjects'].nunique(). news_train['provider'].nunique(). print(news_train['audiences'].nunique()). print(news_train['time'].nunique()). print(news_train['sourceTimestamp'].nunique()). print(news_train['firstCreated'].nunique()). print(market_train['time'].head(10000)). news_sub = pd.DataFrame(). news_sub[['A','B','C']] = news_train[['assetName','sentimentNegative','sentimentNeutral','sentimentPositive']].groupby('assetName').count(). news_sub[['a','b','c']] = news_train[['assetName','sentimentNegative','sentimentNeutral','sentimentPositive']].groupby('assetName').mean(). market_train = market_train.join(news_sub,on='assetName',how='left')
Processed documentation: news_train = pd.DataFrame() news_sub = news_train[['A','B','C'].groupby('assetName').count() market_train.join(news_sub,on='
===========================================================================================
Cleaned documentation: Data Cleaning TfidfVectorizer` - It Transforms text to feature vectors that can be used as input to estimator.
Processed documentation: Data Cleaning Tfidf Vectorizer. Transforms text to feature vectors that can be used as input to estimator.
===========================================================================================
Cleaned documentation: sample_submission sample_submission` shows the submission format. It seems we need to predict `accuracy_group` for each `installation_id`.
Processed documentation: sample_submission shows the submission format. It seems we need to predict `accuracy_group` for each `installation_id.
===========================================================================================
Cleaned documentation: Read in the data CSV files. train = pd.read_csv(datadir/'train.csv'). test = pd.read_csv(datadir/'test.csv'). sample_submission = pd.read_csv(datadir/'sample_submission.csv'). class_map = pd.read_csv(datadir/'class_map.csv')
Processed documentation: Read in the data CSV files. train = pd.read_csv(datadir/'train.csv') test = pD. read_ CSV( datadir /'test. CSV') sample_submission
===========================================================================================
Cleaned documentation: As written in the [evaluation metric]( page, macro F1 score is defined as: image.png](attachment:image.png) Let's simply implement this equation.
Processed documentation: As written in the [evaluation metric] page, macro F1 score is defined as: image.png. Let's simply implement this equation.
===========================================================================================
Cleaned documentation: Any permutation invariant is okay. calc_pair_feature(h1, h2) == calc_pair_feature(h2, h1). batch, nodenode, ch). edge, ch). edge, 1)
Processed documentation: Any permutation invariant is okay. calc_pair_feature(h1, h2) == calc_ Pair_feature (h2, h1). batch, nodenode, ch). edge, ch) calc_
===========================================================================================
Cleaned documentation: Read in the data CSV files. train = pd.read_csv(datadir/'train.csv'). test = pd.read_csv(datadir/'test.csv'). sample_submission = pd.read_csv(datadir/'sample_submission.csv'). class_map = pd.read_csv(datadir/'class_map.csv')
Processed documentation: Read in the data CSV files. train = pd.read_csv(datadir/'train.csv') test = pD. read_ CSV( datadir /'test. CSV') sample_submission
===========================================================================================
Cleaned documentation: Put everything together with `Transform` class. Update] I added albumentations augmentations introduced in [Bengali: albumentations data augmentation tutorial](
Processed documentation: Put everything together with `Transform` class. Update] I added albumentations augmentations introduced in [Bengali: albuments data augmentation tutorial]
===========================================================================================
Cleaned documentation: Each row contains question and answer information together with the original Q&A page URL of the StackExchange properties.
Processed documentation: Each row contains question and answer information together with the original Q&A page URL.
===========================================================================================
Cleaned documentation: For example, let's visualize CO2. Molecules can be represented by [SMILES]( string notation.
Processed documentation: For example, let's visualize CO2. Molecules can be represented by [SMILES].
===========================================================================================
Cleaned documentation: bst = xgb.train(params, Xtrain, num_boost_round=num_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, evals_result=evals_result, verbose_eval=verbose_eval, feval=mcc_eval, maximize=True,)
Processed documentation: Xgb.bst = xgb.train(params, Xtrain, num_boost_round=num_round, evals=watchlist, early_stopping_rounds=early_st stopping_roundS
===========================================================================================
Cleaned documentation: Is this better than Raddar's kernel? He computed this for new transactions only. Let's see what we get.
Processed documentation: Is this better than Raddar's kernel? He computed this for new transactions only.
===========================================================================================
Cleaned documentation: Now the night starts at around 0.25 and ends around 0.75. We can define the night oindex accordingly.
Processed documentation: Now the night starts at around 0.25 and ends at 0.75. We can define the night oindex accordingly.
===========================================================================================
Cleaned documentation: Let's look at which nights passband 0 is used and which nights the other passbands are used.[](
Processed documentation: Let's look at which nights passband 0 is used and which nights the other passbands are used.
===========================================================================================
Cleaned documentation: Let's see how our standardised direction is distributed, for the offense teams, and for the defense teams.
Processed documentation: Let's see how our standardised direction is distributed, for the offense and defense teams.
===========================================================================================
Cleaned documentation: Now the night starts at around 0.25 and ends around 0.75. We can define the night index accordingly.
Processed documentation: Now the night starts at around 0.25 and ends at 0.75. We can define the night index accordingly.
===========================================================================================
Cleaned documentation: a = \sum_{i ,j \in E} \frac{d(i,j)}{N(N-1)}$$, where d(i,j) is the shortest path.
Processed documentation: a = d(i,j), where d( i,j) is the shortest path.
===========================================================================================
Cleaned documentation: sentences starting with a dot (as below) is my personal opinion. have fun!`enter code here`
Processed documentation: sentences starting with a dot (as below) is my personal opinion. have fun!
===========================================================================================
Cleaned documentation: the func is from. the func is from. filename = images_path + filename
Processed documentation: the func is from. filename = images_path + filenames_path.
===========================================================================================
Cleaned documentation: model = get_model_lstmonly((464,)). model = SqueezeNet((464,)). model = get_model_cnnlstm((464,)). model = get_model_Squeezelstm((464,))
Processed documentation: model = get_model_lstmonly((464,)), model = SqueezeNet((464), model = get-model_cnnlstm ((464,), model= get- model_Squeezel
===========================================================================================
Cleaned documentation: train할 데이터 수는 891개, test(제출해야)할 데이터 수는 418개를 확인해 볼 수 있습니다.
Processed documentation: train할    891개, test(제출   해    ‘’’ ‘”’   ’”
===========================================================================================
Cleaned documentation: count값이 다른 feature가 보입니다. Null)가 있는 것으로 보이는데 한번 알아보도록 하겠습니다. Null Data) 확인
Processed documentation: count.count    ‘’’ ‘”’   “”  ”” “,” ”, ‘,’, �
===========================================================================================
Cleaned documentation: Cabin feature 는 NaN 이 대략 80% 이므로, 생존에 영향을 미칠 중요한 정보를 얻어내기가 쉽지는 않습니다.
Processed documentation: Cabin feature is 80% of NaN.
===========================================================================================
Cleaned documentation: Ticket feature 는 NaN 은 없습니다. 일단 string data 이므로 우리가 어떤 작업들을 해주어야
Processed documentation: Ticket feature    ‘NaN’ means ‘no ticket’ in English.
===========================================================================================
Cleaned documentation: Embarked 에도 적용하겠습니다. Initial 때와 마찬가지로 one-hot encoding 을 사용해 표현하겠습니다.
Processed documentation: Embarked    “ one-hot encoding’. Initial  encoding was   “”.
===========================================================================================
Cleaned documentation: Preparation - Split dataset into train, valid(dev), test set target label(Survived)를 분리합니다. drop` 을 사용해 간단히 할 수 있습니다.
Processed documentation: Preparation - Split dataset into train, valid(dev), test set target label(Survived)
===========================================================================================
Cleaned documentation: submission(제출용) 이므로 결과는 leaderboard 에서 확인할 수 있습니다. gender_submission.csv 파일을 읽어서 제출 준비를 하겠습니다
Processed documentation: submission(제출용)  “ 결과는 leaderboard’. gender_submission.csv: ‘’’ ‘
===========================================================================================
Cleaned documentation: Follow multi-label classification Almost following fast.ai course: But `pretrained=False` With lwlrap as metric:
Processed documentation: Follow multi-label classification Almost following fast. But `pretrained=False` With lwlrap as metric:
===========================================================================================
Cleaned documentation: LH has fine-grained features for 4 seconds, and X has more abstract whole sample (for now). Converter
Processed documentation: LH has fine-grained features for 4 seconds, and X has more abstract whole sample (for now)
===========================================================================================
Cleaned documentation: Dataset related Utilities All the augmentatoion is done by data generators.
Processed documentation: All the augmentatoion is done by data generators.
===========================================================================================
Cleaned documentation: Save object as pickle binary file. Thanks to. Stack X as [X,X,X]. Standardize. Normalize to [0, 255]. Just zero
Processed documentation: Save object as pickle binary file. Standardize. Normalize to [0, 255]. Just zero.
===========================================================================================
Cleaned documentation: Now looking again at the returns metrics during this time, we see much cleaner values...
Processed documentation: Now looking again at the returns metrics during this time, we see much cleaner values.
===========================================================================================
Cleaned documentation: Taking another look at the HGSI.O data, we see much improved looking return data in 2009...
Processed documentation: Taking another look at the HGSI.O data, we see much improved looking return data in 2009.
===========================================================================================
Cleaned documentation: Comparisons Here, we test the time it takes for each of the 4 functions to run.
Processed documentation: We test the time it takes for each of the 4 functions to run.
===========================================================================================
Cleaned documentation: Validation Just to make sure all functions output the same for the same input images.
Processed documentation: Just to make sure all functions output the same for the same input images.
===========================================================================================
Cleaned documentation: train_text_df = pd.read_csv("../input/training_text", sep="\|\|", engine="python", skiprows=1, names=["ID", "Text"]). test_text_df = pd.read_csv("../input/test_text", sep="\|\|", engine="python", skiprows=1, names=["ID", "Text"])
Processed documentation: train_text_df = pd.read_csv("../input/training_text", sep="\|\|", engine="python", skiprows=1, names= ["ID", "Text"]). test_
===========================================================================================
Cleaned documentation: from :. get columns:. prev_returns_cols = ["returnsClosePrevMktres1","returnsOpenPrevRaw1","returnsClosePrevMktres10"] mktRes have NaNs!. fill nulls. df['cluster_novelty'] = cluster_modelling(novelty_cols)
Processed documentation: mktRes have NaNs!. fill nulls. get columns:. prev_returns_cols = ["returnsClosePrevMktres1","returnsOpenPrevRaw1"," returnsClose PrevMktRes
===========================================================================================
Cleaned documentation: Exploratory Data Analysis (Rohit Singh's Notebook listed in references was helpful for several aspects of this.)
Processed documentation: Exploratory Data Analysis (Rohit Singh's Notebook listed in references was helpful)
===========================================================================================
Cleaned documentation: Scans by Anatom Site Good... It looks like both datasets shared scanned body parts similary. Let's check it further.
Processed documentation: Scans by Anatom Site Good... It looks like both datasets shared scanned body parts similary.
===========================================================================================
Cleaned documentation: setting model hyperparameters, didn't include fine tuning here because of timing reasons...
Processed documentation: setting model hyperparameters, didn't include fine tuning here because of timing reasons.
===========================================================================================
Cleaned documentation: Let's drop image size and number related features to see if it's increase the randomness...
Processed documentation: Let's drop image size and number related features to see if it's increase the randomness.
===========================================================================================
Cleaned documentation: Handling categorical dataobjList = train_df.select_dtypes(include = "object").columnstrain_df = one_hot(train_df, objList) test_df = one_hot(test_df, objList) print (train_df.shape)
Processed documentation: Handling categorical dataobjList = train_df.select_dtypes(include = "object").columnstrain_df = one_hot(train_df, objList) test_DF = one-hot(
===========================================================================================
Cleaned documentation: getting dummy variables for location on train set. getting dummy variables for location on test set. dropping useless columns
Processed documentation: getting dummy variables for location on train set. dropping useless columns on test set.
===========================================================================================
Cleaned documentation: setting model hyperparameters, didn't include fine tuning here because of timing reasons...
Processed documentation: setting model hyperparameters, didn't include fine tuning here because of timing reasons.
===========================================================================================
Cleaned documentation: fitting and predicting. assigning predictions on submission df. creating submission csv file
Processed documentation: fitting and predicting. assigning predictions on submission. creating submission csv file.
===========================================================================================
Cleaned documentation: Every string has data of more than vehicles in Image ID_8a6e65317.jpg.
Processed documentation: Every string has data of more than vehicles in Image ID.
===========================================================================================
Cleaned documentation: DBNO - Down But Not Out. How many enemies DBNOs an average player scores.
Processed documentation: DBNO - Down But Not Out. How many enemies an average player scores.
===========================================================================================
Cleaned documentation: Driving vs. Walking ^ I filtered data to exclude for players who don't ride at all and don't walk.
Processed documentation: I filtered data to exclude for players who don't ride at all and don't walk.
===========================================================================================
Cleaned documentation: The oldest movie in our database is black and white [The Kid]( movie with Charlie Chaplin from 1921.
Processed documentation: The oldest movie in our database is black and white [The Kid with Charlie Chaplin from 1921]
===========================================================================================
Cleaned documentation: main_df: snap флагов. Теперь данные имеют более репрезентативный вид. id_dict: name_id: id_dict, но где ключи и значения переставлены.
Processed documentation: main_df: snap флагов. id_dict: name_id: id-dict.
===========================================================================================
Cleaned documentation: pair_corr = sns.pairplot(data=desс_pr_df, diag_kind='kde') Как еще один вариант исследования корреляции.. pair_corr.fig.set_size_inches(7,7)
Processed documentation: pair_corr = sns.pairplot(data=desс_pr_df, diag_kind='kde') pair_Corr.set_size_inches(7,7) pair_cor
===========================================================================================
Cleaned documentation: Spectrogram in 3d By the way, times change, and the tools change. Have you ever seen spectrogram in 3d?
Processed documentation: Spectrogram in 3d. Have you ever seen spectrogram in 2d or 3d?
===========================================================================================
Cleaned documentation: Anchor Box. anchors (1, 9, 4). shift (K, 1, 4). merge (K, 9, 4) = > (K 9, 4)
Processed documentation: Anchor Box. anchors (1, 9, 4). shift (K, 1, 4) = > (K 9,4)
===========================================================================================
Cleaned documentation: Learning Rate. lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, factor=0.5, mode='auto'). lr_schedule = tf.keras.callbacks.LearningRateScheduler(tf.keras.experimental.CosineDecayRestarts(5e-5, 10), verbose=1). lr_schedule = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, verbose=1)
Processed documentation: lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, factor=0.5, mode='auto') lr.LearningRateSched
===========================================================================================
Cleaned documentation: Updating the df_num dataframe after droping the features from original dataframe df.
Processed documentation: Updating the df_num dataframe after droping the features from original dataframe.
===========================================================================================
Cleaned documentation: Taking 0.25 as threshold on grounds of experimental changes . . .
Processed documentation: Taking 0.25 as threshold on grounds of experimental changes...
===========================================================================================
Cleaned documentation: Again, correlation threshold of 0.9 has been judged and taken after multiple experiments .....
Processed documentation: Again, correlation threshold of 0.9 has been judged and taken after multiple experiments.
===========================================================================================
Cleaned documentation: There are different number of categories in train and test datset. Encountered
Processed documentation: There are different number of categories in train and test datset.
===========================================================================================
Cleaned documentation: Check for output tab of the notebook and check the score after submitting it . . .
Processed documentation: Check for output tab of the notebook and check the score after submitting it.
===========================================================================================
Cleaned documentation: A simple kernel that uses a Keras model trained in my local system. c) 2019, Debanga Raj Neog
Processed documentation: A simple kernel that uses a Keras model trained in my local system.
===========================================================================================
Cleaned documentation: Utility functions. Parameters for contrast enhancement. cv2.normalize(frame, frame, 0, 255, cv2.NORM_MINMAX). frame = cv2.LUT(frame, lookUpTable)
Processed documentation: Utility functions. cv2.normalize(frame, frame, 0, 255, cv 2.NORM_MINMAX). frame = cv1.LUT( frame, lookUpTable) frame.normal
===========================================================================================
Cleaned documentation: verif['yhat'].plot(legend=True, color='red', figsize=(10,8)). dataset_for_prediction1['Weekly_Sales'].plot(legend=True, color='green', figsize=(20,8)). dataset_for_prediction1['Weekly_Sales'].plot(legend=True, color='green', figsize=(20,8)). verif['y'].plot(legend=True, color='blue', figsize=(10,8)). fig.set_figheight(4)
Processed documentation: verif['yhat'].plot(legend=True, color='red', figsize=(10,8)). dataset_for_prediction1['Weekly_Sales']. plot(Legend= true, color
===========================================================================================
Cleaned documentation: xgtrain = xgb.DMatrix(train_vectors, y_train). xgtest = xgb.DMatrix(test_vectors). model = xgb.train(params=list(params.items()), dtrain=xgtrain, num_boost_round=40). probs = model.predict(xgtest, ntree_limit=model.best_ntree_limit)
Processed documentation: xgtrain = xgb.DMatrix(train_vectors, y_train). xgtest = xGB.D Matrix(test_vector). model = Xgb.train(params=list(params
===========================================================================================
Cleaned documentation: Another simple feature is the sum of heals and boosts. Also the sum of total distance travelled.
Processed documentation: Another simple feature is the sum of heals and boosts. Also the total distance travelled.
===========================================================================================
Cleaned documentation: Numerical Features i. "True" Numerical Features First let's look at the numerical columns
Processed documentation: Numerical Features i.e. "True" Numerical features i.E. "true"
===========================================================================================
Cleaned documentation: true_numerical_columns = [. Census_ProcessorCoreCount',. Census_PrimaryDiskTotalCapacity',. Census_SystemVolumeTotalCapacity',. Census_TotalPhysicalRAM',. Census_InternalPrimaryDiagonalDisplaySizeInInches',. Census_InternalPrimaryDisplayResolutionHorizontal',. Census_InternalPrimaryDisplayResolutionVertical',. Census_InternalBatteryNumberOfCharges'
Processed documentation: true_numerical_columns = [. Census_ProcessorCoreCount, Census_PrimaryDiskTotalCapacity',. Census Census_SystemVolumeTotal Capacity,. Census  Census_TotalPhysicalRAM,
===========================================================================================
Cleaned documentation: The first active year show some big differences! The target variable increases with each year. Definitly an interesting pattern.
Processed documentation: The first active year show some big differences! The target variable increases with each year.
===========================================================================================
Cleaned documentation: Get back to building a CNN using Keras. Much better frameworks then others. You will enjoy for sure.
Processed documentation: Get back to building a CNN using Keras. Much better frameworks then others.
===========================================================================================
Cleaned documentation: In order to get the right prediction we have to get the classes in the range of 0-8.
Processed documentation: In order to get the right prediction we have to get classes in the range of 0-8.
===========================================================================================
Cleaned documentation: Merge the train and test dataframes and aggergated mercant and historical data together...
Processed documentation: Merge the train and test dataframes and aggergated mercant and historical data together.
===========================================================================================
Cleaned documentation: A little more memory clean up and some other data clean up....
Processed documentation: A little more memory clean up and some other data clean up.
===========================================================================================
Cleaned documentation: To hedge bets further, we can also generate predictions using model averaging, averaging over the models that seemed reasonable.
Processed documentation: To hedge bets further, we can also generate predictions using model averaging, averaging over models that seemed reasonable.
===========================================================================================
Cleaned documentation: We can see there are some entries with text count of 1. Lets have a look at those entries
Processed documentation: We can see some entries with text count of 1. Lets have a look at those entries.
===========================================================================================
Cleaned documentation: Create some lookup dictionaries and define constants You don't need to do it this way. :-)
Processed documentation: Create some lookup dictionaries and define constants. You don't need to do it this way.
===========================================================================================
Cleaned documentation: train['Lat'] = preprocessing.scale(train['Lat']). train['Long'] = preprocessing.scale(train['Long']). test['Lat'] = preprocessing.scale(test['Lat']). test['Long'] = preprocessing.scale(test['Long'])
Processed documentation: train['Lat'] = preprocessing.scale(train('Lat', 'Long', 'Lat'), test('Long')); test('Lat' & 'Long'!= 'Lat','Long','Lat' + '
===========================================================================================
Cleaned documentation: Format date. train["Date"] = train["Date"].apply(lambda x: x.replace("-","")). train["Date"] = train["Date"].astype(int). drop nan's. train = train.drop(['Province/State'],axis=1). train = train.dropna()
Processed documentation: train = train.drop(['Province/State'],axis=1). train.apply(lambda x: x.replace("-","")). train["Date"].astype(int). drop nan's.
===========================================================================================
Cleaned documentation: Create encoded variables to use in model. First Feature. group['title'] = group['title'].astype('category').cat.codes. Second Feature. Target variable from training labels
Processed documentation: Create encoded variables to use in model. First Feature. group['title'] = group.astype('category').cat.codes. Second Feature. target variable from training labels.
===========================================================================================
Cleaned documentation: you need to do some finetune to get real 𝜃ray. np.arctan2(x, z)
Processed documentation: you need to do some finetune to get real 𝜃ray.arctan2(x, z)
===========================================================================================
Cleaned documentation: Rotation Vector -> Rotation Matrix -> Euler Angles. Rotation Vector -> Rotation Matrix
Processed documentation: Rotation Vector -> Rotation Matrix. Euler Angles. Rotation Vector.
===========================================================================================
Cleaned documentation: valid_generator = test_datagen.flow_from_dataframe(. dataframe=val_x,. directory=TRAIN_DIR,. x_col="id",. y_col="attribute_ids",. batch_size=BATCH_SIZE,. seed=42,. shuffle=True,. class_mode="categorical",. classes=labels,. target_size=(IM_SIZE,IM_SIZE))
Processed documentation: valid_generator = test_datagen.flow_from_dataframe( dataframe=val_x,. directory=TRAIN_DIR,. x_col="id",. y_col='attribute_ids',
===========================================================================================
Cleaned documentation: Now let's see some of the images The images have different sizes, they may need resizing or some padding.
Processed documentation: Now let's see some of the images. The images have different sizes, they may need resizing.
===========================================================================================
Cleaned documentation: Mortality and Recovery Rates¶ It is worth seeing these stats as well. It might have a story for sure.
Processed documentation: Mortality and Recovery Rates¶ It is worth seeing these stats as well. It might have a story.
===========================================================================================
Cleaned documentation: Ensemble models Here you can ensemble any combination of models, and give the desired weight for each one.
Processed documentation: Ensemble models Here you can ensemble any combination of models, and give the desired weight for each.
===========================================================================================
Cleaned documentation: The features seems to be normalized. Process data for model Turn "wheezy-copper-turtle-magic" into a categorical feature
Processed documentation: The features seems to be normalized. Turn "wheezy-copper-turtle-magic" into a categorical feature.
===========================================================================================
Cleaned documentation: Distribution of subContinent The sub-continent from which sessions originated, based on IP address of the visitor.
Processed documentation: Distribution of subContinent based on IP address of the visitor.
===========================================================================================
Cleaned documentation: Distribution of adwordsClickInfo.isVideoAd True if it is a Trueview video ad.
Processed documentation: ClickInfo.isVideoAd True if it is a Trueview video ad.
===========================================================================================
Cleaned documentation: Distribution of campaign The campaign value. Usually set by the utm_campaign URL parameter.
Processed documentation: Distribution of campaign value. Usually set by the utm_campaign URL parameter.
===========================================================================================
Cleaned documentation: Distribution of campaignCode Value of the utm_id campaign tracking parameter, used for manual campaign tracking.
Processed documentation: Code Value of the utm_id campaign tracking parameter, used for manual campaign tracking.
===========================================================================================
Cleaned documentation: Visualizing parameters for randomForest By visualizing the parameters, we can check if the chosen parameter is really the best.
Processed documentation: Visualizing parameters for randomForest can help us check if the chosen parameter is really the best.
===========================================================================================
Cleaned documentation: plot first few dog images. define subplot. define filename. load image pixels. plot raw pixel data. show the figure
Processed documentation: plot first few dog images. define subplot. load image pixels. plot raw pixel data. show the figure.
===========================================================================================
Cleaned documentation: Read the data. Read market and news data. Will use daily data only. market.time = market.time.dt.date. news.time = news.time.dt.date
Processed documentation: Read the data. Read market and news data. Will use daily data only.
===========================================================================================
Cleaned documentation: As we can see, the most of labels lay between -0.2 and 0.2 and there are otliers.
Processed documentation: As we can see, the most of labels lay between -0.2 and 0.2. There are otliers.
===========================================================================================
Cleaned documentation: This is the largest base2 we can fit in GPU memory. Early stopping patience
Processed documentation: This is the largest base2 we can fit in GPU memory.
===========================================================================================
Cleaned documentation: submission1 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id') submission1['prediction'] = Model1.predict(pad_text(test[TEXT_COLUMN], tokenizer)) submission1.reset_index(drop=False, inplace=True) submission1.head() submission1.to_csv('submission1.csv', index=False)
Processed documentation: submission1 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id') submission1
===========================================================================================
Cleaned documentation: submission3 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id') submission3['prediction'] = Model3.predict(pad_text(test[TEXT_COLUMN], tokenizer)) submission3.reset_index(drop=False, inplace=True) submission3.head() submission3.to_csv('submission3.csv', index=False)
Processed documentation: submission3 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id') submission3
===========================================================================================
Cleaned documentation: PushOut + Median Stacking Pushout strategy is a bit agressive given what it does...
Processed documentation: PushOut + Median Stacking Pushout strategy is a bit agressive given what it does.
===========================================================================================
Cleaned documentation: MinMax + Mean Stacking MinMax seems more gentle and it outperforms the previous one given its peformance score.
Processed documentation: MinMax + Mean Stacking MinMax seems more gentle and it outperforms the previous one.
===========================================================================================
Cleaned documentation: Static Some images had more static noise than others. So perhaps adding noise is a useful augmentation to try?
Processed documentation: Some images had more static noise than others. So perhaps adding noise is a useful augmentation to try?
===========================================================================================
Cleaned documentation: Conclusions The images from both sets seem to be quite similar. Possibly some color differences and other minor differences?
Processed documentation: The images from both sets seem to be quite similar. Possibly some color differences and other minor differences?
===========================================================================================
Cleaned documentation: Different colormaps and extra brightness adjustments Someone asked if the colors could be brighter. Lets try a different colormap:
Processed documentation: Different colormaps and extra brightness adjustments. Lets try a different colormap:
===========================================================================================
Cleaned documentation: Create new features Train data Based on the timestamp we create new features which are cyclic.
Processed documentation: Create new features Train data is cyclic. Based on the timestamp we create new features.
===========================================================================================
Cleaned documentation: model.add(Embedding(input_length=input_dim)). model.add(LSTM(units=64, activation = 'relu')). model.add(Dense(128, activation='relu', input_dim=input_dim)). model.add(Dense(256, activation='relu')). model.add(Dense(512, activation='relu'))
Processed documentation: model.add(LSTM(units=64, activation ='relu')). model.add (Dense(128, activation='relu', input_dim=input_dim)). model. add(Dense
===========================================================================================
Cleaned documentation: Load Libraries We need the standard python libraries and some libraries of sklearn.
Processed documentation: We need the standard python libraries and some libraries of sklearn.
===========================================================================================
Cleaned documentation: Encoding The feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping.
Processed documentation: The feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping.
===========================================================================================
Cleaned documentation: Encoding The feature wind_direction is the compass direction (0-360) and cyclic.
Processed documentation: The feature wind_direction is the compass direction (0-360) and cyclic.
===========================================================================================
Cleaned documentation: lgb_train = lgb.Dataset(X_train, y_train). lgb_eval = lgb.Dataset(X_val, y_val). gbm = lgb.train(. lgb_params,. lgb_train,. num_boost_round=5000,. valid_sets=(lgb_train, lgb_eval),. verbose_eval = 50
Processed documentation: lgb_train = lgb.Dataset(X_train, y_train). lgb_eval = lGB. dataset (X_val, y-val). gbm = l GB.
===========================================================================================
Cleaned documentation: Scale target variable to log.. Split training examples into train/dev examples.. Calculate number of train/dev/test examples.
Processed documentation: Scale target variable to log. Split training examples into train/dev examples. Calculate number of train/Dev/test examples.
===========================================================================================
Cleaned documentation: Competitiveness, includes more game options - overfitting for now in Tournaments. for now
Processed documentation: Competitiveness, includes more game options - overfitting for now in Tournaments.
===========================================================================================
Cleaned documentation: assetCode In the assetCode there is a .Symbol. maybe that represent a market.
Processed documentation: assetCode In the assetCode there is a.Symbol maybe that represent a market.
===========================================================================================
Cleaned documentation: Resize image to target shape(model_input) or back to original shape. Helper functions
Processed documentation: Resize image to target shape(model_ input) or back to original shape.
===========================================================================================
Cleaned documentation: Depth checking the distribution As per below: depth is 'normal' distributed and train and test set have same distribution.
Processed documentation: depth is 'normal' distributed and train and test set have same distribution.
===========================================================================================
Cleaned documentation: Model architecture TOC](TOC) Below functions create the UNet model in a functional and recursive way.
Processed documentation: Model architecture TOC. Below functions create the UNet model in a functional and recursive way.
===========================================================================================
Cleaned documentation: Submission TOC](TOC) Submission is in csv form: id`: index (equals filename) rle_mask`: run-length format (down-then-right): `masked_pixel_start` `` `length_of_masked_pixels` ...
Processed documentation: Submission TOC is in csv form: id, index, rle_mask, run-length.
===========================================================================================
Cleaned documentation: define some models and functions. slide size distribution. score. number of slides. loss
Processed documentation: define some models and functions. slide size distribution. number of slides. loss.
===========================================================================================
Cleaned documentation: GiB + はまだ大きすぎて、（Kaggleでは）モデルをトレーニングできません. lag特徴量はまだありません. state_idまたはshop_idでトレーニングできるとしたらどうでしょう？. Full Grid: 1.2GiB. Full Grid: 321.2MiB. Notebookではlag特徴量について話します. Thank you.
Processed documentation: Kaggle.com is a free, open-source, cloud-based social network. Kaggle allows users to share data between their computers. The site has a Full Grid of 1.2GiB.
===========================================================================================
Cleaned documentation: plot for FP for all images. plot for FP for images with at least one fault
Processed documentation: plot for FP for images with at least one fault.
===========================================================================================
Cleaned documentation: Here is the distribution of the number of genres per movie. There are 3 films with 7 genres!
Processed documentation: There are 3 films with 7 genres!
===========================================================================================
Cleaned documentation: Credits go to Massoud Hosseinali. Compiled_data is converted into a DataFrame and deleted to save memmory
Processed documentation: Credits go to Massoud Hosseinali. Compiled_data is converted into a DataFrame and deleted.
===========================================================================================
Cleaned documentation: haxby dataset to have anatomical image, EPI images and masks. haxby_dataset = datasets.fetch_haxby(). localizer dataset to have contrast maps
Processed documentation: Haxby dataset to have anatomical image, EPI images and masks. haxby_dataset = datasets.fetch_haxby() localizer to have contrast maps.
===========================================================================================
Cleaned documentation: df_sample_submission = pd.DataFrame({'qid' : df_test['qid'], 'y_pred' : y_test_true}). index_insin = np.array(df_sample_submission[df_sample_submission['y_pred'] == 1].index). df_sample_submission['qid'][index_insin]
Processed documentation: df_sample_submission = pd.DataFrame({'qid' : df_test['qid', 'y_pred' : y_test_true}). index_insin = np.array(
===========================================================================================
Cleaned documentation: fig, axs = plt.subplots(ncols=len(contours_new),figsize=(20,5)). print(contour[:,1].min()-crop_gap ,contour[:,1].max()+crop_gap,contour[:,0].min()-crop_gap,contour[:,0].max()+crop_gap). cv2.imwrite((save_path+clusterId+"_"+name[:9]+"_"+str(i)+".jpg"),crop). if len(contours_new)==1 :. axs.imshow(crop). axs.set_title(clusterId+"_"+name[:9]+"_"+str(i)). else :. axs[i].imshow(crop). axs[i].set_title(clusterId+"_"+name[:9]+"_"+str(i)). plt.show(). ax.imshow(img)
Processed documentation: fig, axs = plt.subplots(ncols=len(contours_new),figsize=(20,5), cv2.imwrite((save_path+clusterId+"_
===========================================================================================
Cleaned documentation: for fct in [np.mean, min, max]:. col_name = '{}_{}_past{}'.format(col, fct.__name__, window). lag_columns.append(col_name). df_code[col_name] = rolled.apply(fct)
Processed documentation: for fct in [np.mean, min, max]:. col_name = '{}_{}_past{}'.format(col, fct.__name__, window). lag_columns.append
===========================================================================================
Cleaned documentation: liste = list(zip(two_sigma_model.model.feature_name(),. two_sigma_model.model.feature_importance('gain'))). liste.sort(key = lambda x:x[1], reverse=True). x[0] for x in liste[:50]]
Processed documentation: liste = list(zip(two_sigma_model.model.feature_name), liste.sort(key = lambda x:x[1], reverse=True). x[0] for x in liste
===========================================================================================
Cleaned documentation: Create Submission The test set for this week is from the 12th of March until the 23rd of April.
Processed documentation: The test set for this week is from the 12th of March until the 23rd of April.
===========================================================================================
Cleaned documentation: Read train and test files. Set labels for train/test data. Get the combined data. Train and test
Processed documentation: Read train and test files. Set labels for train/test data. Get the combined data.
===========================================================================================
Cleaned documentation: Do not forget to upvote :) Also fork and modify for your own use. ;)
Processed documentation: Do not forget to upvote :) Also fork and modify for your own use.
===========================================================================================
Cleaned documentation: Figure 1 This shows the distribution of the number of missing values for the training and test sets.
Processed documentation: Figure 1 shows the distribution of the number of missing values for the training and test sets.
===========================================================================================
Cleaned documentation: Pick your configurations These are the main configurations to run this kernel. Addapt it as you want.
Processed documentation: These are the main configurations to run this kernel. Addapt it as you want.
===========================================================================================
Cleaned documentation: The trainning code is hidden above. Calculate weights to give to each feature prediction---------. Calculates AUC score------------
Processed documentation: The trainning code is hidden above. Calculate weights to give to each feature prediction. Calculates AUC score.
===========================================================================================
Cleaned documentation: plt.plot(. range(max_merchant_bucket + 1),. train_cleaned_in_new[train_cleaned_in_new["new_merchant_count"] == i].mean(). for i in range(max_merchant_bucket + 1)]
Processed documentation: plt.plot(. range(max_merchant_bucket + 1),. train_cleaned_in_new[train_cle cleaned_in-new["new_merchants_count"] == i
===========================================================================================
Cleaned documentation: df_train["TimeSnap"].astype('datetime64[ns]').sort_values(ascending=False). max date in TimeSnap cloumn is 2018-12-31, so i'm not using today function
Processed documentation: df_train["TimeSnap"].astype('datetime64[ns]').sort_values(ascending=False). max date in TimeSnap cloumn is 2018-12-31, so i'm not using
===========================================================================================
Cleaned documentation: Calculate distances. Create nodes. Create edges. Create bonds. Create angles. concat all edge values
Processed documentation: Calculate distances. Create nodes. Create edges. Create bonds. concat all edge values.
===========================================================================================
Cleaned documentation: Load Dataset Now we will load the training as well as validation dataset.
Processed documentation: Load Dataset Now we will load the training and validation dataset.
===========================================================================================
Cleaned documentation: Split train in train and validate. train_df=train_df[:250000]. Set_indices=train_df.loc[:,'set_'][:250000]. y_train = train_df['target'][:250000]. Set_indices_labels=train_df.loc[:,'set_'][:250000]
Processed documentation: Split train in train and validate. train_df=train_df[:250000]. Set_indices=train.loc[:,'set_'], y_train = train.train[: 'target'
===========================================================================================
Cleaned documentation: Split train in train and validate. train_df=train_df[:250000]. Set_indices=train_df.loc[:,'set_'][:250000]. y_train = train_df['target'][:250000]. Set_indices_labels=train_df.loc[:,'set_'][:250000]
Processed documentation: Split train in train and validate. train_df=train_df[:250000]. Set_indices=train.loc[:,'set_'], y_train = train.train[: 'target'
===========================================================================================
Cleaned documentation: Load the dataset. all_file=os.listdir("../input/all-dogs/all-dogs/"). x_train_data = np.zeros((len(all_file),img_size,img_size,3)). for i in range(len(all_file)-1):. file=all_file[i]. path="../input/all-dogs/all-dogs/"+file. x_train_data[i] = load_image2(path)
Processed documentation: Load the dataset. all_file=os.listdir("../input/all-dogs/All- dogs/"). x_train_data = np.zeros((len(all_file),img_size,
===========================================================================================
Cleaned documentation: datetime The easiest and most well-known is converting dates to a datetime-type.
Processed documentation: The easiest and most well-known is converting dates to a datetime-type.
===========================================================================================
Cleaned documentation: How to recover rotating bounging boxes from given data. Lossless. I'm trying to code as simple as possible.
Processed documentation: How to recover rotating bounging boxes from given data. Lossless.
===========================================================================================
Cleaned documentation: preprocessing/decomposition. keras. model evaluation. supportive models. feature selection (from supportive model). to make results reproducible
Processed documentation: preprocessing/decomposition. model evaluation. supportive models. feature selection. to make results reproducible.
===========================================================================================
Cleaned documentation: Greedy Dumb Submission :) (0.2164845 Public LB Score) Lloyd level, still better than banana baseline][1]
Processed documentation: Greedy Dumb Submission :) (0.2164845 Public LB Score) Lloyd level, still better than banana baseline.
===========================================================================================
Cleaned documentation: Still Greedy But Smarter (Customer Will Take All Reordered) (0.2996690 Public LB Score) Lloyd appreciates this][1]
Processed documentation: Still Greedy But Smarter (Customer Will Take All Reordered) (0.2996690 Public LB Score) Lloyd appreciates this.
===========================================================================================
Cleaned documentation: Less Dumb - Repeat Last Order(0.3276746 Public LB Score) Lloyd appreciates this][1]
Processed documentation: Less Dumb - Repeat Last Order(0.3276746 Public LB Score) Lloyd appreciates this.
===========================================================================================
Cleaned documentation: Less Dumb - Repeat Last Order (Reordered Products Only)(0.3276826 Public LB Score) America is great again!][1]
Processed documentation: Less Dumb - Repeat Last Order (Reordered Products Only)(0.3276826 Public LB Score) America is great again!
===========================================================================================
Cleaned documentation: Having aspect ratio vary so much is not good :( , lets take a closer look...
Processed documentation: Having aspect ratio vary so much is not good :(, lets take a closer look.
===========================================================================================
Cleaned documentation: Well, that certainly looks promising. Found 76 runs, similar number than the number of groups. Build the runs:
Processed documentation: Found 76 runs, similar number than the number of groups. Build the runs:
===========================================================================================
Cleaned documentation: Have we found all samples? Have we used any sample twice? The answer is yes, and no. Perfect.
Processed documentation: Have we found all samples? Have we used any sample twice? The answer is yes, and no.
===========================================================================================
Cleaned documentation: Some really insightful comments about deep learning model optimization can be found here ( ).
Processed documentation: Some really insightful comments about deep learning model optimization can be found here.
===========================================================================================
Cleaned documentation: high_corr = pd.DataFrame(abs(train.corr()[:12])) high_corr_square = high_corr[high_corr.columns[:10].tolist()] sns.set_context("notebook", font_scale=1.2, rc={"lines.linewidth": 2}) plt.figure(figsize = (12,12)) sns.heatmap(high_corr_square,linecolor ='white',linewidths=1,annot=True)
Processed documentation: high_corr = pd.DataFrame(abs(train.corr()[:12]), high_corR_square = high_Corr[high_ Corr.columns[:10].
===========================================================================================
Cleaned documentation: Now,Let's see the weights distribution and rank the most important features by their feature weights.
Processed documentation: Let's see the weights distribution and rank the most important features by their feature weights.
===========================================================================================
Cleaned documentation: image reseize & centering & crop. centering. read image. resize. centering. out put 224224px
Processed documentation: image reseize & centering & crop. centering. read image. out put 224224px.
===========================================================================================
Cleaned documentation: from kaggle_secrets import UserSecretsClient. user_secrets = UserSecretsClient(). user_credential = user_secrets.get_gcloud_credential(). user_secrets.set_tensorflow_credential(user_credential)
Processed documentation: from kaggle_secrets import UserSecretsClient. user_credential = user.secrets.get_gcloud_ credential() user_secretes.set_tensorflow_credit
===========================================================================================
Cleaned documentation: if e == 3:. optimizer.param_groups[0]['lr'] = 1e-5. if e == 5:. optimizer.param_groups[0]['lr'] = 1e-8. clear_output()
Processed documentation: if e == 3:. optimizer.param_groups[0]['lr'] = 1e-5. if e == 5:.optimizer. param_groups [0] = 1E-8. if
===========================================================================================
Cleaned documentation: COMPETITION PERIOD. YOU ARE RESPONSIBLE FOR DETERMINING THE CORRESPONDING TIME ZONE IN YOUR LOCATION.
Processed documentation: COMPETITION PERIOD. YOU ARE RESPONSIBLE FOR DETERMINING THE CORRESPONDING TIME ZONE.
===========================================================================================
Cleaned documentation: And here comes the morphology. We will use: Dilation (read more: Erosion (read more:
Processed documentation: And here comes the morphology. We will use: Dilation (read more: Erosion)
===========================================================================================
Cleaned documentation: fill up the missing values. x_train, x_test, y_train, word_index = load_and_prec(). x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec()
Processed documentation: fill up the missing values. x_train, x_test, y_train,. word_index = load_and_prec()
===========================================================================================
Cleaned documentation: IDを抽出. IDを抽出. print(Fore.YELLOW +"Total Patients in Train set: ",Style.RESET_ALL,train_df['Patient'].count()). IDを識別するために、学習セットIDの「セット」データ構造を作成. IDを識別するために、検証セットIDの「セット」データ構造を作成
Processed documentation: IDを抽出. print(Fore.YELLOW +"Total Patients in Train set: ",Style.RESET_ALL,train_df['Patient'].count()). ID. print
===========================================================================================
Cleaned documentation: print(dicom_file_dataset.dir("pat")) print(dicom_file_dataset.data_element("ImageOrientationPatient")) print(dicom_file_dataset.data_element("ImagePositionPatient")) print(dicom_file_dataset.data_element("PatientID")) print(dicom_file_dataset.data_element("PatientName")) print(dicom_file_dataset.data_element("PatientSex")). df = pd.DataFrame(). imagePositionPatient = dicom_file_dataset.ImagePositionPatient. print(rows). print(columns). dcm_ImagePositionPatient':imagePositionPatient,
Processed documentation: print(dicom_file_dataset.dir("pat")). df = pd.DataFrame() imagePositionPatient = dicom.file.ImagePositionPatients. print(rows). print(
===========================================================================================
Cleaned documentation: Create featuretool entities. time_index="transaction_time",. variable_types={"SK_ID_CURR": ft.variable_types.Categorical},. EDUCATION": ft.variable_types.Categorical,. MARRIAGE": ft.variable_types.Categorical,. additional_variables=["DAYS_CREDIT"]
Processed documentation: create featuretool entities. time_index="transaction_time",. variable_types={"SK_ID_CURR": ft.variable_types.Categorical, EDUCATION": ft., MARRIA
===========================================================================================
Cleaned documentation: Run 'Deep Feature Synthesis' and record times. Print elapsed time. print('Chunk size =', chunk_size, ', Time = ', time_elapsed)
Processed documentation: Run 'Deep Feature Synthesis' and record times. Print elapsed time.
===========================================================================================
Cleaned documentation: molecule_atom_index_0_dist_max_diff',. molecule_atom_index_0_dist_max_div',. molecule_atom_index_0_dist_std_diff',. molecule_atom_index_0_dist_std_div',. molecule_atom_index_1_dist_std_diff',. molecule_atom_index_0_dist_mean_div',. molecule_atom_index_1_dist_max_diff',. molecule_atom_index_1_dist_mean_diff',. molecule_atom_index_1_dist_min_diff',. molecule_atom_index_1_dist_min_div',. molecule_atom_index_1_dist_max_div',. molecule_atom_index_0_dist_min_diff',. molecule_atom_index_0_y_1_max_diff',. molecule_atom_index_0_y_1_mean_div',. molecule_type_dist_mean_div'
Processed documentation: molecule_atom_index_0_dist_max_diff',. molecule_atom-index_1_dist-min-diff,. molecule-atom- index-1-dist-std-diff and.
===========================================================================================
Cleaned documentation: lin_model = sm.RLM(y_train, X_train, M=sm.robust.norms.HuberT()).fit(). lin_model = sm.RLM(y_train, X_train, M=sm.robust.norms.Hampel()).fit(). lin_model = sm.RLM(y_train, X_train, M=sm.robust.norms.RamsayE()).fit()
Processed documentation: lin_model = sm.RLM(y_train, X_train), M=sm.robust.norms.HuberT()).fit() lin_model: sm. RLM (y_ train,
===========================================================================================
Cleaned documentation: reduce_mem_usageは、データのメモリを減らすためにデータ型を変更する関数です。 reduce_mem_usage' is a functin which reduce memory usage by changing data type.)
Processed documentation: reduce_mem_ usage' is a functin which reduce memory usage by changing data type.
===========================================================================================
Cleaned documentation: read_dataはデータの読み込みと, reduce_mem_usageの適用を行う関数 read data' is a function to read the files and apply the 'reduce_mem_usage'.)
Processed documentation: read_data is a function to read the files and apply the'reduce_mem_ usage' function.
===========================================================================================
Cleaned documentation: submission fileを同じくmelt処理し、sales_train_valとつなげる。 submission fileの列名を"d_xx"形式に変更する. submission fileで縦に結合されたvalidationとevaluationを一度分割し、それぞれことなる28日間の列名"d_xx"をそれぞれ付与。 submission fileには, idの詳細（item, department, state等）が無いためidをキーに, sales validationから取得したproductを結合 test2は、6/1まで不要なため削除
Processed documentation: submission file.submissionfile. Submission file  melt処理  sales_train_val    submission. file. submission file   "d_xx"
===========================================================================================
Cleaned documentation: LightGBMのMetricとして, WRMSSEの効率的な計算を行う。あくまで, 28day-lagで1つのモデルの予測するときにLGBMで効率的なWRMSSEの計算を行う場合である。 weight_matという0 or 1の疎行列で、効率的にaggregation levelを行列積で計算出来るようにしている LightGBMのMetricを効率的に計算するためにGroupby fucntionを使うことを避けているが、そのため、non-rezo demandのデータを除くと効率的な計算ができない。そのためすべてのitemでnon-zero demand dataとなっている最後の28日分のみで検証するコードとなっている. Sparce matrixは順序がProductのItem通りになっていないといけないので注意。
Processed documentation: LightGBM, WRMSSE, 28day-lag, weight_mat, Sparce matrix, demand data.
===========================================================================================
Cleaned documentation: load data, we only need the transactions dataset for now. train_id = pd.read_csv('/kaggle/input/train_identity.csv'). test_id = pd.read_csv('/kaggle/input/test_identity.csv'). sample_submission = pd.read_csv('/kaggle/input/sample_submission.csv')
Processed documentation: load data, we only need the transactions dataset for now. train_id = pd.read_csv('/kaggle/ input/train_identity.csv'). test_id means test data, sample_sub
===========================================================================================
Cleaned documentation: Both train and test samples are processed in the exact same way. Train. Test
Processed documentation: Both train and test samples are processed in the exact same way.
===========================================================================================
Cleaned documentation: test.parquet test.parquet is too big (20337, 800000) so I will read test data in 6 parts columns)
Processed documentation: test.parquet is too big (20337, 800000) so I will read test data in 6 parts columns.
===========================================================================================
Cleaned documentation: some config values. fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values
Processed documentation: some config values. fill up the missing values. Pad the sentences. Get the target values.
===========================================================================================
Cleaned documentation: Description Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.
Processed documentation: The primary language used is English, with some in Malay or Chinese.
===========================================================================================
Cleaned documentation: Noise Injection It simply add some random value into data by using numpy.
Processed documentation: Noise Injection simply adds some random value into data by using numpy.
===========================================================================================
Cleaned documentation: PitchShift This augmentation is a wrapper of librosa function. It change pitch randomly
Processed documentation: PitchShift augmentation is a wrapper of librosa function. It change pitch randomly.
===========================================================================================
Cleaned documentation: Create ```my_files``` folder inside darknet directory for nessesary files. Copy prepaired files and add pre-trained weights.
Processed documentation: Copy prepaired files and add pre-trained weights. Create a new folder inside darknet directory.
===========================================================================================
Cleaned documentation: Returns the confusion matrix between rater's ratings. The following 3 functions have been taken from Ben Hamner's github repository
Processed documentation: The following 3 functions have been taken from Ben Hamner's GitHub repository.
===========================================================================================
Cleaned documentation: Source: :param a1: :param a2: :param max_rat: :return:. y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)
Processed documentation: Source: :param a1: : Parameter a2: :Param max_rat: :return:. y_pred = y.reshape(len(np.unique(y_true), -1).arg
===========================================================================================
Cleaned documentation: Returns the confusion matrix between rater's ratings. The following 3 functions have been taken from Ben Hamner's github repository
Processed documentation: The following 3 functions have been taken from Ben Hamner's GitHub repository.
===========================================================================================
Cleaned documentation: Encoder Decoder Model Model has two main parts: encoder and decoder. encoder-decoder.png](attachment:encoder-decoder.png)
Processed documentation: Encoder Decoder Model Model has two main parts: encoder and decoder.
===========================================================================================
Cleaned documentation: if bi % 10 == 0:. break. print(t1.shape). if torch.cuda.device_count() > 1:. model = nn.DataParallel(model)
Processed documentation: if bi % 10 == 0:. print(t1.shape). if torch.cuda.device_count() > 1:. model = nn.DataParallel(model)
===========================================================================================
Cleaned documentation: if bi % 10 == 0:. break. print(t1.shape). if torch.cuda.device_count() > 1:. model = nn.DataParallel(model)
Processed documentation: if bi % 10 == 0:. print(t1.shape). if torch.cuda.device_count() > 1:. model = nn.DataParallel(model)
===========================================================================================
Cleaned documentation: g = np.argmax(g.cpu().detach().numpy(), axis=1). v = np.argmax(v.cpu().detach().numpy(), axis=1). c = np.argmax(c.cpu().detach().numpy(), axis=1)
Processed documentation: g = np.argmax(g.cpu().detach().numpy(), axis=1). v = np arg max(v.cpu() detach() numpy()() axis=1) c
===========================================================================================
Cleaned documentation: Fit the model. x_train = train_df.drop(['meter_reading','timestamp'],axis = 1). y_train = train_df['meter_reading']. regression_model.fit(x_train,y_train)
Processed documentation: Fit the model. x_train = train_df.drop(['meter_reading','timestamp'],axis = 1). y_train.drop('meter']. regression_model.fit(x_train,
===========================================================================================
Cleaned documentation: Checking Actual and Predicted values side by side. pd.DataFrame(zip(y_train,predicted_train),columns = ['Actual','Predicted']).iloc[2000000:10000000,].head(10)
Processed documentation: Checking Actual and Predicted values side by side. pd.DataFrame(zip(y_train,predicted_train),columns = ['Actual','Predicted']).iloc[2000000:10000000
===========================================================================================
Cleaned documentation: plt.figure(figsize=(17,8)). plt.hist(df_building_metadata['year_built'],bins = 20). plt.title("Histogram showing the Distribution of the Year in which Builidings were built"). plt.show()
Processed documentation: plt.hist(df_building_metadata['year_built'],bins = 20). plt.title("Histogram showing the Distribution of the Year in which Builidings were built").
===========================================================================================
Cleaned documentation: plt.figure(figsize=(17,8)). plt.hist(df_building_metadata['year_built'],bins = 20). plt.title("Histogram showing the Distribution of the Year in which Builidings were built"). plt.show()
Processed documentation: plt.hist(df_building_metadata['year_built'],bins = 20). plt.title("Histogram showing the Distribution of the Year in which Builidings were built").
===========================================================================================
Cleaned documentation: plt.figure(figsize=(17,8)). plt.hist(df_building_metadata['floor_count'],bins = 20). plt.title("Histogram showing the distribution of number of Floors in Buildings"). plt.show()
Processed documentation: plt.hist(df_building_metadata['floor_count'],bins = 20). plt.title("Histogram showing the distribution of number of Floors in Buildings").
===========================================================================================
Cleaned documentation: Load the data We define a function to read all the data and report the shape of datasets.
Processed documentation: We define a function to read all the data and report the shape of datasets.
===========================================================================================
Cleaned documentation: EDA Now, I will try to visualize the sales data and gain some insights from it.
Processed documentation: EDA will try to visualize the sales data and gain some insights from it.
===========================================================================================
Cleaned documentation: Please upvote if you like it. It motivates me. Thank you ☺️ .
Processed documentation: Please upvote if you like it. It motivates me. Thank you.
===========================================================================================
Cleaned documentation: Model Prepare the model Let's start by preparing the model. Prepare the train data
Processed documentation: Model Prepare the model Let's start by preparing the model.
===========================================================================================
Cleaned documentation: Let's plot the total amount of non-zero transactions per day, grouped by channelGrouping.
Processed documentation: Let's plot the total amount of non-zero transactions per day, grouped by channel group.
===========================================================================================
Cleaned documentation: Sensitive topics features Let's check now the distribution of sensitive topics features values.
Processed documentation: Sensitive topics features Let's check now the distribution of sensitive topics features.
===========================================================================================
Cleaned documentation: We extracted the topics using LDA. Let's represent the topics using the `pyLDAvis` tool.
Processed documentation: We extracted the topics using LDA. Let's represent them using the `pyLDAvis` tool.
===========================================================================================
Cleaned documentation: Merchant data Let's check the distribution of merchant data features. Let's start with merchant_category_id, subsector_id.
Processed documentation: Let's check the distribution of merchant data features. Let's start with merchant_category_id, subsector_id.
===========================================================================================
Cleaned documentation: Majority of sequence number frames are 1 (61%), followed by 3 (37%), the rest (1.2%) having 5. Locations distribution
Processed documentation: Majority of sequence number frames are 1 (61%), followed by 3 (37%), the rest (1.2%) having 5.
===========================================================================================
Cleaned documentation: Extract date and time information We extract date and time information from the `date_time` column.
Processed documentation: Extract date and time information from the `date_time` column.
===========================================================================================
Cleaned documentation: Aggregated features and time to failure Let's also show aggregated features and time to failure on the same graph.
Processed documentation: Aggregated features and time to failure on the same graph.
===========================================================================================
Cleaned documentation: Image samples Let's plot some image samples from train_images. For this visualization, I reused the code from this Kernel:
Processed documentation: Image samples Let's plot some image samples from train_images. I reused the code from this Kernel:
===========================================================================================
Cleaned documentation: Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1.
Processed documentation: Let's see now the distribution of kurtosis on rows in train separated for values of 0 and 1.
===========================================================================================
Cleaned documentation: Target feature - surface and group_id distribution Let's show now the distribution of target feature - surface and group_id.
Processed documentation: Target feature - surface and group_id distribution. Let's show now the distribution of target feature.
===========================================================================================
Cleaned documentation: Go to top Integer features Let's see now the distribution of the sum of integer features values per column.
Processed documentation: Go to top of the page to see the distribution of the sum of integer features values per column.
===========================================================================================
Cleaned documentation: Target and class Let's plot the number of examinations for each class detected, grouped by Target value.
Processed documentation: Let's plot the number of examinations for each class detected, grouped by Target value.
===========================================================================================
Cleaned documentation: Patient Age Let's examine now the data for the Patient Age for the train set. Train dataset
Processed documentation: Patient Age for the train set. Train dataset for the Patient Age.
===========================================================================================
Cleaned documentation: Let's check also the distribution of patient age for the test data set. Test dataset
Processed documentation: Let's check also the distribution of patient age for the test data set.
===========================================================================================
Cleaned documentation: Let's check as well the distribution of Patient Sex for the test data. Test dataset
Processed documentation: Let's check as well the distribution of Patient Sex for the test data.
===========================================================================================
Cleaned documentation: Generate data Generate data for KNN classifier; then train the classifier on these data.
Processed documentation: Generate data for KNN classifier; then train the classifier on these data.
===========================================================================================
Cleaned documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns. Let's glimpse train and test dataset.
Processed documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns.
===========================================================================================
Cleaned documentation: Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1.
Processed documentation: Let's see now the distribution of kurtosis on rows in train separated for values of 0 and 1.
===========================================================================================
Cleaned documentation: num_train = train.shape[0]. num_test = test.shape[0]. num_features = test.shape[1]. print('Test data is {:.2%}'.format(num_test/(num_train+num_test)), 'of total train/test data')
Processed documentation: num_train = train.shape[0]. num_test = test.shape [0]. print('Test data is {:.2%}'.format(num_test%), 'of total train/test data')
===========================================================================================
Cleaned documentation: could correct for time difference in later iteration, for now, just drop column. train.drop(['TransactionDT'], axis=1). test.drop(['TransactionDT'], axis=1). print('dropped TransactionDT')
Processed documentation: could correct for time difference in later iteration, for now, just drop column. train.drop(['TransactionDT'], axis=1). test.drop('dropped TransactionDT')
===========================================================================================
Cleaned documentation: sample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'), index_col='TransactionID'). y_pred = clf.predict_proba(test). del clf, test. sample_submission['isFraud'] = y_pred[:,1]. del y_pred. sample_submission.to_csv('xgboost_with_tuning2.csv')
Processed documentation: sample_submission = pd.read_csv(os.path.join( input_path,'sample_ submission.csv'), index_col=' transactionID'), y_pred = clf.predict
===========================================================================================
Cleaned documentation: pip install torchtoolbox --quiet. from torchtoolbox.tools import summary. model.cuda(). summary(model, torch.rand((1, 3, 150, 150)).cuda())
Processed documentation: pip install torchtoolbox --quiet.cuda(). summary(model, torch.rand((1, 3, 150, 150)).cuda())
===========================================================================================
Cleaned documentation: train['block'] = np.NaN. for model, ix in enumerate(blocks):. train.loc[ix, 'block'] = model. distributions = train.groupby(['block', 'open_channels'])['signal'].agg(['mean', 'std']). distributions
Processed documentation: train.train['block'] = np.NaN. for model, ix in enumerate(blocks):. distributions = train.groupby(['block', 'open_channels'],['signal'].agg
===========================================================================================
Cleaned documentation: Multi-processing. If the vocabulary is not passed, build from text. Value 0 is reserved for the padding character ()
Processed documentation: Multi-processing. If the vocabulary is not passed, build from text. Value 0 is reserved for the padding character.
===========================================================================================
Cleaned documentation: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=0.5, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=3, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1, oob_score=False, random_state=42, verbose=0, warm_start=False)
Processed documentation: RandomForestRegressor.bootstrap=True, criterion='mse', max_depth=None, max_features=0.5, min_leaf_nodes=None,. min_impurity_decrease=
===========================================================================================
Cleaned documentation: Denoising training batches except Batch 7. Denoising Batch 7 Part 1. Denoising Batch 7 Part 2
Processed documentation: Denoising training batches except Batch 7.
===========================================================================================
Cleaned documentation: Denoising test set sub batches part by part. Denoising test set second half
Processed documentation: Denoising test set sub batches part by part.
===========================================================================================
Cleaned documentation: One-hot encoding of anatom_site_general_challenge feature. Sex features. Age features. n_image per user. image size
Processed documentation: One-hot encoding of anatom_site_general_challenge feature. Sex features. Age features. n_image per user.
===========================================================================================
Cleaned documentation: print(row_id). print(target). print(submission_df.shape). print(len(target)). print(len(row_id)). print(target). print(row_id). submission_df['target'] = np.array(target).astype(np.int). submission_df['row_id'] = row_id
Processed documentation: submission_df.shape = np.array(target).astype(np.int). submission_df['row_id'] = row_id.shape. print(len(target)). print (len(row
===========================================================================================
Cleaned documentation: train=pd.merge(left=train , right=card_id_cnt, how = 'left', on ='card_id') test=pd.merge(left=test , right=card_id_cnt, how = 'left', on ='card_id')
Processed documentation: train=pd.merge(left=train, right=card_id_cnt, how = 'left', on ='card-id') test=PDParser(train, test, card_id
===========================================================================================
Cleaned documentation: train=pd.merge(left=train , right=card_id_cnt_new, how = 'left', on ='card_id') test=pd.merge(left=test , right=card_id_cnt_new, how = 'left', on ='card_id')
Processed documentation: train=pd.merge(left=train, right=card_id_cnt_new, how = 'left', on ='card_ id') test=PDP.Merge( left=test,
===========================================================================================
Cleaned documentation: for f in ['feature_1','feature_2','feature_3']:. order_label = train.groupby([f])['outliers'].mean(). train[f+'map'] = train[f].map(order_label). test[f+'map'] = test[f].map(order_label)
Processed documentation: for f in ['feature_1','feature_2',' feature_3']:. order_label = train.groupby([f])['outliers'].mean() train[f+'map'] = train
===========================================================================================
Cleaned documentation: oof_xgb_3[val_idx] = clf_xg.predict(xgb.DMatrix(train.iloc[val_idx][rest]), ntree_limit=clf_xg.best_ntree_limit). oof[val_idx] = clf.predict(X_valid). predictions += clf.predict(test_stack) / folds.n_splits"""
Processed documentation: oof_xgb_3[val_idx] = clf_xg.predict(xgb.DMatrix(train.iloc[val idx][rest), ntree_limit=clf
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: As expected, we have 22 of each playid. Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: train = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv'). train1 = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv'). valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_valid_translated.csv'). valid1 = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv'). test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')
Processed documentation: train = pd.read_csv('/kaggle/ input/jigsaw-multilingual-toxic-comment-classification/Jigsaw-Toxic-Comment-train.csv') train1 = p
===========================================================================================
Cleaned documentation: Games with many events have more than 3000 events per play, while most games have fewer than 500 events.
Processed documentation: Games with many events have more than 3000 events per play. Most games have fewer than 500 events.
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: As expected, we have 22 of each playid. Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: train["len_user_name"]= train.question_user_name.apply(lambda x : 1 if len(x)<5 else 0). test["len_user_name"]= test.question_user_name.apply(lambda x : 1 if len(x)<5 else 0)
Processed documentation: train.train["len_user_name"]= train.question_user name.apply(lambda x : 1 if len(x)<5 else 0). test.train[len_ user_name]= test
===========================================================================================
Cleaned documentation: Competitiveness, includes more game options - overfitting for now in Tournaments. for now
Processed documentation: Competitiveness, includes more game options - overfitting for now in Tournaments.
===========================================================================================
Cleaned documentation: import calendar. list(calendar.day_name). train['pickup_week'] = train['pickup_week'].apply(lambda x: calendar.day_name[x]). test['pickup_week'] = test['pickup_week'].apply(lambda x: calendar.day_name[x])
Processed documentation: import calendar. list(calendar.day_name). train['pickup_week'] = train.apply(lambda x: calendar. day_name[x]. test['pick up_week'].apply( lambda x
===========================================================================================
Cleaned documentation: Here, I just want to simplify `passenger_count` as either one or more than one person. So,
Processed documentation: Here, I just want to simplify `passenger_count` as either one or more than one person.
===========================================================================================
Cleaned documentation: dataset_show = CloudDataset(df_train, 'train', 'val', image_size, transform=None). dataset_show = CloudDataset(df_test, 'test', 'test', image_size, transform=None)
Processed documentation: dataset_show = CloudDataset(df_train, 'train', 'val', image_size, transform=None). dataset_ show = Cloud dataset (df_test, 'test', '
===========================================================================================
Cleaned documentation: Create the model Lets manually try some parameters and check what the model performance looks like.
Processed documentation: Lets manually try some parameters and check what the model performance looks like.
===========================================================================================
Cleaned documentation: Fit a random forest and extract predictions. Fitting the forest may take a few minutes. Write the test results
Processed documentation: Fit a random forest and extract predictions. Fitting the forest may take a few minutes.
===========================================================================================
Cleaned documentation: Generate the training/testing visualizations for each CV split. Fill in indices with the training/test groups. Visualize the results. Formatting
Processed documentation: Generate training/testing visualizations for each CV split. Fill in indices with the training/test groups. Visualize the results.
===========================================================================================
Cleaned documentation: print('type of use_correct_labels is {}, cm_images_ds_numpy is {}'.format(type(use_correct_labels), type(cm_images_ds_numpy))). print('shape of use_correct_labels is {}, cm_images_ds_numpy is {}'.format(use_correct_labels.shape, cm_images_ds_numpy.shape))
Processed documentation: print('type of use_correct_labels is {}, cm_images_ds_numpy is {}'format(type(use_correct-labels), type(cm_images-ds_ numpy) print
===========================================================================================
Cleaned documentation: Dark Images Thanks to this [discussion]( we are aware the some scans are dark for instance as below.
Processed documentation: Dark Images Thanks to this discussion we are aware the some scans are dark for instance as below.
===========================================================================================
Cleaned documentation: Load the data. t0 = time(). if len(testfiles) == 5:. f= open("/kaggle/working/training.log","w+"). lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
Processed documentation: Load the data. t0 = time(). if len(testfiles) == 5:. f= open("/kaggle/working/training.log","w+"). lr_scheduler = torch.optim
===========================================================================================
Cleaned documentation: class myCallback(tf.keras.callbacks.Callback):. def on_epoch_end(self, epoch, logs={}):. if(logs.get('loss')<0.4):. print("\nReached 60% accuracy so cancelling training!"). self.model.stop_training = True. callbacks = myCallback()
Processed documentation: class myCallback(tf.keras.callbacks.Callback):. on_epoch_end(self, epoch, logs={}):. if(logs.get('loss')<0.4):. print
===========================================================================================
Cleaned documentation: Generate text features:. Initialize decomposition methods:. Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
Processed documentation: Generate text features: Initialize decomposition methods: Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
===========================================================================================
Cleaned documentation: Weeks Distribution Similar to Age Distribution Above, now let's plot a histogram of total Weeks Distribution of both genders.
Processed documentation: Weeks Distribution Similar to Age Distribution. Now plot the total Weeks Distribution of both genders.
===========================================================================================
Cleaned documentation: U-Net with pytorch Drift removal: Five Unet models (1f, 1s, 3, 5, 10).
Processed documentation: U-Net with pytorch removal: Five Unet models (1f, 1s, 3, 5, 10)
===========================================================================================
Cleaned documentation: slow. fast. split. train_idx, val_idx = model_selection.train_test_split(idx, random_state=seed, test_size=test_size). optimizer = optim.SGD(model.parameters(), lr=0.05, weight_decay=1e-5). input_shape=(1, time_step),
Processed documentation: slow. fast. split. train_idx, val _idx = model_selection.train_test_split(idx), random_state=seed, test_size=test_size. optimizer = optim
===========================================================================================
Cleaned documentation: PANNsCNN14Att.cnn_feature_extractor()` method will take this as input and output feature map. Let's check the output of the feature extractor.
Processed documentation: PANNsCNN14Att.cnn_feature_extractor() will take this as input and output feature map. Let's check the output of the feature extractor.
===========================================================================================
Cleaned documentation: Very simple Neural Network model.. This can be improved by many ways. e.g., more layers, batch normalization and etc.
Processed documentation: Very simple Neural Network model. Can be improved by more layers, batch normalization and etc.
===========================================================================================
Cleaned documentation: V. Submission Again, thank you to [Xhlulu]( notebook [here]( for providing this submission-formatting code
Processed documentation: V. Submission Again, thank you to [Xhlulu] for providing this submission-formatting code.
===========================================================================================
Cleaned documentation: kappa_opt = KappaOptimizer([0.8, 1.0, 2.5, 3.2]). fit on validation set. kappa_opt.fit(preds, targets). preds = kappa_opt.predict(preds).tolist()
Processed documentation: kappa_opt = KappaOptimizer([0.8, 1.0, 2.5, 3.2]). fit on validation set. kappa_ opt.fit(preds, targets). preds = k
===========================================================================================
Cleaned documentation: Apply language multipliers as in Thanks @christofhenkel this improves our public LB from [0.9475]( to [0.9487]( !!
Processed documentation: Apply language multipliers as in Thanks @christofhenkel this improves our public LB from [0.9475] to [ 0.9487]!!
===========================================================================================
Cleaned documentation: folder = '/kaggle/working/faces'. Path(folder).mkdir(parents=True, exist_ok=True). for file in tqdm(list_files):. save_frame(file, folder). face_files = [str(x) for x in Path(folder).glob('')]
Processed documentation: folder = '/kaggle/working/faces' for file in tqdm(list_files):. save_frame(file, folder). face_files = [str(x) for x in Path
===========================================================================================
Cleaned documentation: if p_x > image.shape[1] or p_y > image.shape[0]:. print('Point', p_x, p_y, 'is out of image with shape', image.shape)
Processed documentation: if p_x > image.shape[1] or p_y < image. shape[0]:. print('Point', p_X, p_Y, 'is out of image with shape', image.
===========================================================================================
Cleaned documentation: You will also need functions from the previous cells. Get values. Math. Drawing
Processed documentation: You will also need functions from the previous cells. Get values.
===========================================================================================
Cleaned documentation: time. svm_predictions = svm.predict_proba(test). svm_submission = pd.DataFrame({'id': test_id, 'target': svm_predictions[:, 0]}). svm_submission.to_csv('svm.csv', index=False). svm_submission.head()
Processed documentation: svm_predictions = svm.predict_proba(test). sVM_submission = pd.DataFrame({'id': test_id, 'target': svm predictions[
===========================================================================================
Cleaned documentation: plt.subplot(5,3,i3+1). sns.barplot(x="match_type", y='W'+col, data=tmp). plt.subplot(5,3,i3+2). sns.barplot(x="match_type", y='L'+col, data=tmp). plt.subplot(5,3,i3+3). tmp['W'+col+'_L'+col] = tmp['W'+col] - tmp['L'+col]. sns.barplot(x="match_type", y='W'+col+'_L'+col, data=tmp)
Processed documentation: plt.subplot(5,3,i3+1). sns.barplot(x="match_type", y='W'+col, data=tmp). plt. subplot( 5,3
===========================================================================================
Cleaned documentation: yard_line_left = offense.YardLine.iloc[0]+10 yard_line 加10偏移量，这个10是左侧的达阵区. yard_line_right = offense.YardLine.iloc[0]+2(50-offense.YardLine.iloc[0])+10. yard_line = yard_line_left if np.abs(yard_line_left-rusher.X.iloc[0])<=(yard_line_right-rusher.X.iloc[0]) else yard_line_right. plt.plot([yard_line,yard_line],[-100,100],c='orange',linewidth=1.5)
Processed documentation: yard_line_left = offense.YardLine.iloc[0]+2(50-offense.Yarline.X[0] + 10 yard_line = yard_ line_left if np.abs
===========================================================================================
Cleaned documentation: Visualize image of selected randmark id Double-click the image to see a larger image.
Processed documentation: Visualize image of selected randmark id. Double-click to see a larger image.
===========================================================================================
Cleaned documentation: We're ready to put together our models. Root-mean-squared percent error is the metric Kaggle used for this competition.
Processed documentation: We're ready to put together our models. Root-mean-squared percent error is the metric Kaggle used.
===========================================================================================
Cleaned documentation: Building an image classifier Material from fast.ai lesson 1 ( adapted to run on Kaggle kernels
Processed documentation: Building an image classifier from fast.ai lesson 1 ( adapted to run on Kaggle)
===========================================================================================
Cleaned documentation: load test data. test_df['comment_text'] = test_df['comment_text'].replace({r'\s+$': '', r'^\s+': ''}, regex=True).replace(r'\n', ' ', regex=True)
Processed documentation: load test data. test_df['comment_text'] = test_DF[' comment_text'].replace({r'\s+$': '', r'^\s'+': '', r'
===========================================================================================
Cleaned documentation: text preparation. lower everything. remove punctuation. remove stopwords. stem. df["stem_txt"] =df["txt"][0:len(df)].apply(lambda x: " ".join([rs.stem(word) for word in x.split()]))
Processed documentation: lower everything. remove punctuation. remove stopwords. stem. df["stem_txt"] =df["txt"].apply(lambda x: " ".join([rs.stem(word for word in x.split()]
===========================================================================================
Cleaned documentation: Input format number_of_photos horizontal_or_vertical number_of_tags tag1 tag2 tag3 ... horizontal_or_vertical number_of_tags tag1 tag2 tag3 ...
Processed documentation: input format number_of_photos horizontal_or_vertical number of_tags tag1 tag2 tag3... horizontal_ or_verticals number of photos.
===========================================================================================
Cleaned documentation: Games with many events have more than 3000 events per play, while most games have fewer than 500 events.
Processed documentation: Games with many events have more than 3000 events per play. Most games have fewer than 500 events.
===========================================================================================
Cleaned documentation: Plot training progress. It's nicer to use something like TensorBoard for this, but a simple plot also works. ;-)
Processed documentation: Plot training progress. It's nicer to use something like TensorBoard for this, but a simple plot also works.
===========================================================================================
Cleaned documentation: As expected, these first five values mirror the first five values of hpg_station_distances['(35.6436746642265, 139.668220854814)']. Let's keep going.
Processed documentation: As expected, these first five values mirror the first 5 values of hpg_station_distances. Let's keep going.
===========================================================================================
Cleaned documentation: Class for accumulation the statistics on each batch and culculation F1 score for entire val dataset.
Processed documentation: Class for accumulation the statistics on each batch and culculation F1 score for entire dataset.
===========================================================================================
Cleaned documentation: Is there time leak in numerical features? Obviously there is information about time.
Processed documentation: Is there time leak in numerical features?
===========================================================================================
Cleaned documentation: Fast Fourier Tranform Denoising In this kernel I will show a quick trick to denoise your signal.
Processed documentation: Fast Fourier Tranform Denoising. In this kernel I show a quick trick to denoise your signal.
===========================================================================================
Cleaned documentation: random.shuffle(fam_size_order). Now try to trade days between three families. jit(nopython=False). random.shuffle(fam_size_order)
Processed documentation: jit(nopython=False). random.shuffle(fam_size_order). Now try to trade days between three families.
===========================================================================================
Cleaned documentation: model.summary(). model.fit(. x=train_images, y=train_labels,. steps_per_epoch=int(np.ceil(float(len(train_indexes)) / float(batch_size))4),. validation_data=(val_images, val_labels),. validation_steps=int(np.ceil(float(len(valid_indexes)) / float(batch_size))4),. epochs=2,. verbose=1)
Processed documentation: model.summary(). model.fit(. x=train_images, y= train_labels,. steps_per-epoch=int(np.ceil(float(len(train_indexes) / float
===========================================================================================
Cleaned documentation: train all layers. model.fit(. x=train_images, y=train_labels,. steps_per_epoch=int(np.ceil(float(len(train_indexes)) / float(batch_size))4),. validation_data=(val_images, val_labels),. validation_steps=int(np.ceil(float(len(valid_indexes)) / float(batch_size))4),. epochs=epochs,. verbose=1,. callbacks=callbacks_list)
Processed documentation: train all layers. model.fit(. x=train_images, y=train-labels, steps_per-epoch=int(np.ceil(float(len(train_indexes) / float
===========================================================================================
Cleaned documentation: if b==5: break. zz=0. debug-----------------------------. cv2.imwrite(out_dir +'/valid/%s.png'%(image_id), result). cv2.waitKey(1). debug-----------------------------. print(valid_loss)
Processed documentation: if b==5: break. zz=0. cv2.imwrite(out_dir +'/valid/%s.png'%(image_id), result). cv1.waitKey(1
===========================================================================================
Cleaned documentation: Takes a scalar and returns a string with the css property `'color: red'` for negative strings, black otherwise.
Processed documentation: Takes a scalar and returns a string with the css property `'color: red'` for negative strings.
===========================================================================================
Cleaned documentation: read csv - PUBLIC LB. public_lb2 = do_read_csv(f'{CSV_DIR}/siim-isic-melanoma-classification-publicLB_19082020.csv') 19/08/2020 22:00. create df for future use
Processed documentation: read csv - PUBLIC LB. public_lb2 = do_read_csv(f'{CSV_DIR}/siim-isic-melanoma-classification-publicLB_19082020.
===========================================================================================
Cleaned documentation: Create some lookup dictionaries and define constants You don't need to do it this way. :-)
Processed documentation: Create some lookup dictionaries and define constants. You don't need to do it this way.
===========================================================================================
Cleaned documentation: time. del train, test. X_train_sparse = csr_matrix(hstack([X_train_content, X_train_tags, X_train_feats.values])). X_test_sparse = csr_matrix(hstack([X_test_content, X_test_tags, X_test_feats.values])). print(X_train_sparse.shape, X_test_sparse.shape)
Processed documentation: time. del train, test. X_train_sparse = csr_matrix(hstack), X_test_s parse = ccsrmatrix (hstack) print(X_train-sparse
===========================================================================================
Cleaned documentation: debug = True. Colab': "./drive/My Drive/Colab/Peking/Output/Out_eff_B2_28_12",. prev_out_path = "./drive/My Drive/Colab/Peking/Output/Keep_outputs/PL_047_cv_3045_eff_b0_dropout_change_resume_KGL". prev_model_fn_path = osj(prev_out_path, "model_fld_3_islastepoch_0_date_02_01_time_13_59.pth")
Processed documentation: debug = True. Colab': "./drive/My Drive/Colab/Peking/Output/Out_eff_B2_28_12",. prev_out_path = "/.drive/ my-drive
===========================================================================================
Cleaned documentation: for key in ['grapheme_root','vowel_diacritic','consonant_diacritic','grapheme']:. dataset[key] = dataset[key].astype('category') ensures groupby().count() shows zeros
Processed documentation: for key in ['grapheme_root','vowel_diacritic','consonant']:. dataset[key] = dataset [key].astype('category') ensures groupby() shows zeros.
===========================================================================================
Cleaned documentation: layer_fc1 = create_fc_layer(input=combined_layer,. num_inputs=combined_layer.get_shape()[1:4].num_elements(),. num_outputs=fc_layer_size1,. use_relu=True,. dropout =True,. keep_prob = keep_prob)
Processed documentation: layer_fc1 = create_fc_layer( input=combined_layer,. num_inputs=comb combined_layer.get_shape()[1:4].num_elements(). num_outputs
===========================================================================================
Cleaned documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', 'PuBuGn', 'PuRd', 'RdPu','YlGn', 'YlGnBu', 'YlOrBr', and 'YlOrRd'.
Processed documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', and 'OrGn' are used to spell the words 'Bu' and 'RdPu'
===========================================================================================
Cleaned documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', 'PuBuGn', 'PuRd', 'RdPu','YlGn', 'YlGnBu', 'YlOrBr', and 'YlOrRd'.
Processed documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', and 'OrGn' are used to spell the words 'Bu' and 'RdPu'
===========================================================================================
Cleaned documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', 'PuBuGn', 'PuRd', 'RdPu','YlGn', 'YlGnBu', 'YlOrBr', and 'YlOrRd'.
Processed documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', and 'OrGn' are used to spell the words 'Bu' and 'RdPu'
===========================================================================================
Cleaned documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', 'PuBuGn', 'PuRd', 'RdPu','YlGn', 'YlGnBu', 'YlOrBr', and 'YlOrRd'.
Processed documentation: BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', and 'OrGn' are used to spell the words 'Bu' and 'RdPu'
===========================================================================================
Cleaned documentation: Quick Unit-Tests. Test data preparation. Test batch matrix shape coherences. Test batch reproducibility. Test classes label encoder
Processed documentation: Quick Unit-Tests. Test data preparation. Test batch matrix shape coherences. Tests batch reproducibility.
===========================================================================================
Cleaned documentation: blockSize检查. addSize. New height and new width. list)存储数据. percent)的位置的像素点在原图像中的最高亮度值. t小于0的情况. t限制最小值为0.1
Processed documentation: blockSize. addSize. New height and new width. list)存储数据.
===========================================================================================
Cleaned documentation: Get top n tfidf values in row and return them with their corresponding feature names.. modified for multilabel milticlass
Processed documentation: Get top n tfidf values in row and return them with their corresponding feature names.
===========================================================================================
Cleaned documentation: target_x. Strat k fold due to imbalanced classes. split = StratifiedKFold(n_splits=2, random_state=1)
Processed documentation: Strat k fold due to imbalanced classes. split = StratifiedKFold(n_splits=2, random_state=1)
===========================================================================================
Cleaned documentation: plt.subplot2grid((4,2),(3,0),colspan=2). sns.barplot(SELECTED_COLS,importance[6][0],color=color[0]). plt.title("class : Clean",fontsize=15). locs, labels = plt.xticks(). plt.setp(labels, rotation=90). plt.xlabel('Feature', fontsize=12). plt.ylabel('Importance', fontsize=12)
Processed documentation: plt.subplot2grid((4,2),(3,0),colspan=2). sns.barplot( SELECTED_COLS,importance[6],color=color[0]. plt
===========================================================================================
Cleaned documentation: The above plot is Interactive. Hover over the states to get the individual numbers of that state. Time:
Processed documentation: The above plot is Interactive. Hover over the states to get the individual numbers of that state.
===========================================================================================
Cleaned documentation: Making the second essay column : project_description. performing the adjustmen. df.loc[selection criteria, columns I want] = value. check
Processed documentation: Making the second essay column : project_description. performing the adjustmen. df[selection criteria, columns I want] = value. check.
===========================================================================================
Cleaned documentation: taking some FE ideas from public kernals. thanks owl, -->. and jmbull -->
Processed documentation: taking some FE ideas from public kernals. thanks owl, and jmbull.
===========================================================================================
Cleaned documentation: d_train = xgb.DMatrix(X_train, y_train). d_valid = xgb.DMatrix(X_valid, y_valid). d_test = xgb.DMatrix(test_x). for eli5
Processed documentation: d_train = xgb.DMatrix(X_train, y_train). d_valid = xGB.D Matrix(X-valid, y-valid) d_test = x GBMatrix(test_x
===========================================================================================
Cleaned documentation: eli5.explain_weights_lgb(model_xgb, vec=vectorizer) out of bounds error. text_features=vectorizer.get_feature_names(). other_features=non_text_cols. all_features=text_features+non_text_cols. eli5.explain_weights_xgboost(model_xgb, feature_names=all_features)
Processed documentation: eli5.explain_weights_lgb(model_xgb, vec=vectorizer) out of bounds error. text_features= vectorizer.get_feature_names() other_features = non_text_
===========================================================================================
Cleaned documentation: Preparing model for training. Defininig Optimizer. Learning Rate Scheduler. lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.5). No of epochs
Processed documentation: Preparing model for training. No of epochs. Learning Rate Scheduler.
===========================================================================================
Cleaned documentation: Hope you liked it 😜, comment below your suggestions / feedback. I will upload another notebook for inference soon.
Processed documentation: Hope you liked it. Comment below your suggestions / feedback. I will upload another notebook for inference soon.
===========================================================================================
Cleaned documentation: df = pd.read_csv(LABELS).set_index('Image'). trn_imgs['target'] = 1. trn_imgs1['target'] = 0. trn_imgs = trn_imgs.append(trn_imgs1)
Processed documentation: df = pd.read_csv(LABELS).set_index('Image'). trn_imgs['target'] = 1. trn-imgs.append(trn_ imgs1)df.append
===========================================================================================
Cleaned documentation: fn2label = {row[1].Image: row[1].Id for row in df.iterrows()}. path2fn = lambda path: re.search('\w\.jpg$', path).group(0)
Processed documentation: fn2label = {row[1].Image: row[1] for row in df.iterrows()}. path2fn = lambda path: re.search('\w\.jpg$', path).group(0)
===========================================================================================
Cleaned documentation: a=[one_hot(i.unsqueeze(-1),5004 ) for i in tensor(data1.train_ds.y.items[0:5])]. listify(x). np.where(a[0]==[1]). tensor(data1.train_ds.y.items[0:5]). type(a). torch.from_numpy(np.array(a)).size(). data1.show_batch(2). import pylot as plt. i=PIL.Image('data/train/3ece2140f.jpg'). print(i.shape). plt.imshow(i)
Processed documentation: a=[one_hot(i.unsqueeze(-1),5004) for i in tensor(data1.train_ds.y.items[0:5])]. listify(x). np.
===========================================================================================
Cleaned documentation: w=model.features[0].weight. model.features[0]=nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False). model.fc = (nn.Linear(1024, 5004)). print(w.shape). model.features[0].weight=torch.nn.Parameter(torch.cat((w, w[:,:1,:,:]),dim=1)). print(model.features[0].weight.shape)
Processed documentation: w=model.features[0].weight. model.fc = (nn.Linear(1024, 5004) w.shape = w[:,:1,:,:],dim=1). print(w.shape
===========================================================================================
Cleaned documentation: x,y=next(iter(learn1.data.train_dl)). rm -rf ./models. learn1.recorder.plot(). for i in trainable_params(learn2.model[1]):. print(i.size()). push
Processed documentation: learn1.recorder.plot(). for i in trainable_params(learn2.model[1]):. print(i.size()). push learn1.data.train_dl.
===========================================================================================
Cleaned documentation: learn2.fit_one_cycle(8,slice(2e-4,lr/2)) run for 30 epochs. learn2.unfreeze(). learn2.load('save'). learn2.load('dense_ar_c56') 0.0260300.6567740.892308. learn1.fit_one_cycle(2,slice(2e-4,lr/2)). learn1.save('resnet_ar_c224')
Processed documentation: learn2.unfreeze(). learn2.load('save') 0.0260300.6567740.892308. learn1.fit_one_cycle(2,slice(2e-4,lr
===========================================================================================
Cleaned documentation: learn1.load('resnet_ar_c424_1')learn1.model.eval()%%timesims = []with torch.no_grad(): for feat in preds1: dists = F.cosine_similarity(trn_centre, feat.unsqueeze(0).repeat(5004, 1)) predicted_similarity = dists.cuda()learn.model.head(dists.cuda()) sims.append(predicted_similarity.squeeze().detach().cpu())
Processed documentation: Learn1.load('resnet_ar_c424_1')learn1.model.eval()%%timesims = []with torch.no_grad(): for feat in preds1: dists = F.cos
===========================================================================================
Cleaned documentation: learn1.model.eval()learn1.load('resnet_ar_c356_1')predsv,y_v = learn1.TTA(ds_type=DatasetType.Valid,beta=0.30,with_loss=False,scale=1.08). cp /kaggle/working/models/resnet_ar_c356.pth /kaggle/working/. ls -l ./models/. cd models. torch.max(preds1,preds2).shape. torch.mean(preds1,preds2). FileLink('resnet_ar_c356.pth')
Processed documentation: learn1.model.load('resnet_ar_c356_1')predsv,y_v = learn1.TTA(ds_type=DatasetType.Valid,beta=0.30,
===========================================================================================
Cleaned documentation: sub.to_csv('resnetpred6.csv',index=False). sub.head(10). kaggle competitions submit -c humpback-whale-identification -f 'resnetpred6.csv' -m "bestresnet324_525"
Processed documentation: Kaggle competitions submit -c humpback-whale-identification -f'resnetpred6.csv' -m "bestresnet324_525"
===========================================================================================
Cleaned documentation: StackingSubmission = pd.DataFrame({ '201610': predictions_201610,. predictions_201611,. predictions_201612,. predictions_201710,. predictions_201711,. predictions_201712,. ParcelId': ParcelID,. print(StackingSubmission)
Processed documentation: StackingSubmission = pd.DataFrame('201610', '201611', '1612', '17', '2017', '18', '19', '20', '22', '23', '24', '
===========================================================================================
Cleaned documentation: time. del train, test. X_train_sparse = csr_matrix(hstack([X_train_content, X_train_tags, X_train_feats.values])). X_test_sparse = csr_matrix(hstack([X_test_content, X_test_tags, X_test_feats.values])). print(X_train_sparse.shape, X_test_sparse.shape)
Processed documentation: time. del train, test. X_train_sparse = csr_matrix(hstack), X_test_s parse = ccsrmatrix (hstack) print(X_train-sparse
===========================================================================================
Cleaned documentation: fill up the missing values. x_train, x_test, y_train, word_index = load_and_prec(). x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec()
Processed documentation: fill up the missing values. x_train, x_test, y_train,. word_index = load_and_prec()
===========================================================================================
Cleaned documentation: fig, ax = plt.subplots(figsize=(30,10)). ax.set_yticks([0, 25000,50000, 75000, 100000]). sns.scatterplot(data['Date'],data['ConfirmedCases']). fig2, ax2 = plt.subplots(figsize=(30,10)). ax2.bar(data['Date'],data['ConfirmedCases'], align="center", width=0.5, alpha=0.5)
Processed documentation: fig, ax = plt.subplots(figsize=(30,10)). ax.set_yticks([0, 25000,50000, 75000, 100000]. sns.scatterplot(data
===========================================================================================
Cleaned documentation: fig6, ax6 = plt.subplots(figsize=(30,10)). ax6.set_yticks([0,1000,2000,3000,4000]). ax6.set_title("Fatalities"). ax6.set_xticks(). sns.scatterplot(data['Date'],data['Fatalities']). fig8, ax8 = plt.subplots(figsize=(30,10)). ax8.bar(data['Date'],data['Fatalities'], align="center", width=0.5, alpha=0.5)
Processed documentation: fig6, ax6 = plt.subplots(figsize=(30,10)). ax6.set_yticks([0,1000,2000,3000,4000]. ax6 set_title("Fatalities
===========================================================================================
Cleaned documentation: mortality=[]. for i in range(0,data.shape[0]):. if i==0:. mortality.append(data.iloc[0]['Fatalities']). else:. mortality.append(data.iloc[i]['Fatalities']-data.iloc[i-1]['Fatalities']). data['daily_fatalities']=mortality. fig5, ax5 = plt.subplots(figsize=(30,10)). sns.barplot(x='Date',y='daily_fatalities',data=data)
Processed documentation: mortality.mortality=[]. for i in range(0,data.shape[0]):. if i==0:. mortality.append(data.iloc[0],'Fatalities'). else:
===========================================================================================
Cleaned documentation: Create an input function for training. drop_remainder = True for using TPUs. train_input_fn = input_fn_builder( features=train_features, seq_length=MAX_SEQ_LENGTH, is_training=True, drop_remainder=False)
Processed documentation: create an input function for training. drop_remainder = True for using TPUs. train_input_fn = input_fn_builder( features=train_features, seq_length=MAX_SEQ_
===========================================================================================
Cleaned documentation: Competitiveness, includes more game options - overfitting for now in Tournaments. for now
Processed documentation: Competitiveness, includes more game options - overfitting for now in Tournaments.
===========================================================================================
Cleaned documentation: Four Models Applied Here. train here. model = make_model.train(trainTemp). predict here. subTemp['EncodedPixels'] = model.predict(subTemp)
Processed documentation: Four Models Applied Here. train here. predict here. subTemp['EncodedPixels'] = model.predict(subTemp)
===========================================================================================
Cleaned documentation: df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis=1)
Processed documentation: df['Text'] = df.apply(lambda r: r['Text'], axis=1) df['Pronoun']: r.define('pronountarget'', axis: 1), axis: 'pronount',
===========================================================================================
Cleaned documentation: KMeans法によって、画像データを100分類に分割. Divide image data into 100 classifications by KMeans method. Get 5 classifications with many from 400 classifications (display)
Processed documentation: Divide image data into 100 classifications by KMeans method. Get 5 classifications with many from 400 classifications (display)
===========================================================================================
Cleaned documentation: Basic data augmentation I'll try basic data augmentation by just flipping the images horizontally.
Processed documentation: I'll try basic data augmentation by just flipping the images horizontally.
===========================================================================================
Cleaned documentation: Let's see how the model performs (last model after training, not the saved best one). On the train set
Processed documentation: Let's see how the model performs (last model after training, not the saved best one)
===========================================================================================
Cleaned documentation: summarize history for loss. summarize history for accuracy. summarize history for competition metric. Save image for reports
Processed documentation: summarize history for loss. summary history for accuracy. summarize history for competition metric.
===========================================================================================
Cleaned documentation: Image clustering Use a t-SNE plot to identify clusters of images. More details on t-SNE [here](
Processed documentation: Image clustering. Use a t-SNE plot to identify clusters of images.
===========================================================================================
Cleaned documentation: Predicting the Test Set Finishing off with predictions on the test set. Scored LB 1.279
Processed documentation: Predicting the Test Set. Scored LB 1.279.
===========================================================================================
Cleaned documentation: For submission. submitte_predictions = predictions.iloc[:,1:]. submitte_predictions.columns = ['F'+str(x+1) for x in np.arange(28)]. Make submissions file
Processed documentation: For submission. submitte_predictions = predictions.iloc[:,1:]. subMitte_Predictions.columns = ['F'+str(x+1) for x in np.arange
===========================================================================================
Cleaned documentation: Splitting the LAB image to different channels-------------------------. cv2.imshow('l_channel', l). cv2.imshow('a_channel', a). cv2.imshow('b_channel', b). Applying CLAHE to L-channel-------------------------------------------
Processed documentation: Splitting the LAB image to different channels. cv2.imshow('a_channel', a). cv3.im show('b_channel'', b). Applying CLAHE to L-channel.
===========================================================================================
Cleaned documentation: img = np.random.randint(10, size=(h,w)). res_image = resize_image(img). sub_med = subtract_median_bg_image(res_image). img_rad_red=Radius_Reduction(sub_med, PARAM)
Processed documentation: img = np.random.randint(10, size=(h,w) res_image = resize_image(img) sub_med = subtract_median_bg_image (res_image) img_rad
===========================================================================================
Cleaned documentation: We see that the train dataframe has an extra column with missing values (ps_car_12). Visualizing missing values
Processed documentation: We see that the train dataframe has an extra column with missing values (ps_car_12)
===========================================================================================
Cleaned documentation: Column ps_reg_03 does not seem to show anything at all, hence can be removed. Visualizing car features
Processed documentation: Column ps_reg_03 does not seem to show anything at all, hence can be removed.
===========================================================================================
Cleaned documentation: features_list = X_train.columns.valuesfeature_importance = GBR.feature_importances_sorted_idx = np.argsort(feature_importance)print(sorted_idx). List of important features for Gradient Boosting Regressor ---
Processed documentation: features_list = X_train.columns.values feature_importance = GBR.feature_importances_sorted_idx = np.argsort(feature_ importance)print(sorted-id
===========================================================================================
Cleaned documentation: Predicting Gradient boost result for test data ---. y_GBR = GBR.predict(X_test)
Processed documentation: Predicting Gradient boost result for test data. y_GBR = GBR.predict(X_test)
===========================================================================================
Cleaned documentation: The train.csv file has been reduced from 15MB to 8MB. Let us now check the transactions.csv file transactions.csv
Processed documentation: The train.csv file has been reduced from 15MB to 8MB. Let us now check the transactions. CSV file.
===========================================================================================
Cleaned documentation: is_churn First and foremost let us analyze the output variable is_churn.
Processed documentation: is_churn is the output variable of the is_churned function.
===========================================================================================
Cleaned documentation: lw = re.findall(r"[^|]+", genre[i]). print('Genre ids NOT present in both train set OR after splitting: ', len(set(genre) ^ set(genre_new)))
Processed documentation: lw = re.findall(r "[^|]+", genre[i]. print('Genre ids NOT present in both train set OR after splitting: ', len(set(genre) ^ set(genre_new
===========================================================================================
Cleaned documentation: Feature 6 : artist_composer New column to check whther the artist and composer is the same person.
Processed documentation: Feature 6 : artist_composer New column to check if the artist and composer is the same person.
===========================================================================================
Cleaned documentation: Replace the missing values with the maximum occurences ---. cross check for missing values ---
Processed documentation: Replace the missing values with the maximum occurences. cross check for missing values.
===========================================================================================
Cleaned documentation: Some dogs The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world.
Processed documentation: The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world.
===========================================================================================
Cleaned documentation: this takes 6-7 mins. You can click and check the ``` output ```
Processed documentation: this takes 6-7 mins. You can click and check the output.
===========================================================================================
Cleaned documentation: Create the folder ```kminst``` where I'm going to save all the chars from all the pictures.
Processed documentation: Create the folder where I'm going to save all the chars from all the pictures.
===========================================================================================
Cleaned documentation: fill up the missing values. x_train, x_test, y_train, word_index = load_and_prec(). x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec()
Processed documentation: fill up the missing values. x_train, x_test, y_train,. word_index = load_and_prec()
===========================================================================================
Cleaned documentation: Uncomment this to view some processed images. for i,(ax,fn) in enumerate(zip(subplots(2,4)[1].flat,fns)):. jpgfn = dest/Path(fn).with_suffix('.jpg').name. a = jpgfn.jpg16read(). show_image(a,ax=ax)
Processed documentation: Uncomment this to view some processed images. for i,(ax,fn) in enumerate(zip(subplots(2,4)[1].flat,fns)):. jpgfn = dest/Path
===========================================================================================
Cleaned documentation: This creates a sparse matrix with only a small number of non-zero elements (stored elements in the representation below).
Processed documentation: This creates a sparse matrix with only a small number of non-zero elements.
===========================================================================================
Cleaned documentation: train['t_n'] = train_t_len. test['t_n'] = test_t_len. train['d_per'] = np.array(train['description'].astype(str).apply(find_punc))/np.array(train_d_len). test['d_per'] = np.array(test['description'].astype(str).apply(find_punc))/np.array(test_d_len)
Processed documentation: train['t_n'] = train_t_len. train['d_per'] = np.array(train['description'].astype(str).apply(find_punc))/np.array (train
===========================================================================================
Cleaned documentation: cupy uses on-the-fly kernel synthesis so the running time for the first execution will also be slower.
Processed documentation: cupy uses on-the-fly kernel synthesis so the running time for the first execution will be slower.
===========================================================================================
Cleaned documentation: define helper functions. auc, plot_history. auc = tf.metrics.auc(y_true, y_pred)[1]. plt.plot([0, 1], [0, 1], 'k--')
Processed documentation: define helper functions. auc, plot_history. aUC = tf.metrics.auc(y_true, y_pred)[1]. plt.plot([0, 1], [0,1], 'k
===========================================================================================
Cleaned documentation: The `training` folder has 400 JSON tasks. `training_tasks` lists some of them.
Processed documentation: The `training` folder has 400 tasks. `training_tasks` lists some of them.
===========================================================================================
Cleaned documentation: print("TRAIN:", train_index, "TEST:", test_index). model = get_model_class(model_type, X_tr, y_tr, X_val, y_val, X_test). model.params['fold_by_target'] = fold_by_target
Processed documentation: Model.prototype.fold_by_target is called with the train_index and test_index as parameters.
===========================================================================================
Cleaned documentation: lgb modeling. reference:. define hyperparammeter (using bayesian optimization extracted with 151 features)
Processed documentation: lgb modeling. define hyperparammeter (using bayesian optimization extracted with 151 features)
===========================================================================================
Cleaned documentation: reference. some of TJ Klein's code, who helped me make this notebook with his examples. Get the filter coefficients
Processed documentation: reference. TJ Klein's code, who helped me make this notebook with his examples.
===========================================================================================
Cleaned documentation: Install [simple-transformers]( a tool to train and test transformers model easily.
Processed documentation: install [simple-transformers] to train and test transformers model easily.
===========================================================================================
Cleaned documentation: Create Target Variable. Plot the distributions. Prepare classification. Run a simple linear classifier. Calculate AUC. Plot ROC
Processed documentation: Create Target Variable. Prepare classification. Run a simple linear classifier. Calculate AUC. Plot ROC.
===========================================================================================
Cleaned documentation: There's definiteiy more to explore here. Maybe these ideas can improve your model. Good luck!
Processed documentation: There's definiteiy more to explore here. Maybe these ideas can improve your model.
===========================================================================================
Cleaned documentation: Cluster Centers Here are the geometric centers of all the clusters. The big red dot is the North Pole.
Processed documentation: Cluster Centers are the geometric centers of all the clusters. The big red dot is the North Pole.
===========================================================================================
Cleaned documentation: The dendrogram view shows how missing values are related across columns by using hierarchical clustering. Pretty cool!
Processed documentation: The dendrogram view shows how missing values are related across columns by using hierarchical clustering.
===========================================================================================
Cleaned documentation: Visits by time train. Visits by time test. Plot visits. Revenue by time. Plot
Processed documentation: Visits by time train. Revenue by time.
===========================================================================================
Cleaned documentation: Platform and OS All machines are running windows with most being windows 10; nothing relevant here.
Processed documentation: All machines are running windows with most being windows 10; nothing relevant here.
===========================================================================================
Cleaned documentation: Let's try different simple thresholding methods. Description of threshold types can be found [here]( and [here](
Processed documentation: Let's try different simple thresholding methods. Description of threshold types can be found here.
===========================================================================================
Cleaned documentation: App 183, if you zoom in, the pattern is very regular...
Processed documentation: App 183, if you zoom in, the pattern is very regular.
===========================================================================================
Cleaned documentation: Add this back if you want separate weather plots. for site, df in weather.groupby('site_id'):. display_plot(weather_rgb(df), f'Weather at site {site}')
Processed documentation: Add this back if you want separate weather plots. for site, df in weather.groupby('site_id'):. display_plot('Weather at site {site}')
===========================================================================================
Cleaned documentation: print(index, building_id, meter, degree_hour, len(ts), len(dh)). print(np.ma.corrcoef(np.ma.masked_invalid(ts), np.ma.masked_invalid(dh))). get_corr(0, "1", "electricity"). timeseries
Processed documentation: print(index, building_id, meter, degree_hour, len(ts), len(dh), get_corr(0, "1", "electricity"), timeseries) print(np.ma.corr
===========================================================================================
Cleaned documentation: In test set, there are {n_samples_gap:,} gaps for a total of {n_samples:,} samples.The ratio of gaps is: {n_samples_gap/n_samples:.2%}.
Processed documentation: In test set, there are {n_samples_gap,.} gaps for a total of {n samples,.} samples. ratio of gaps is: { n_s samples_gap/n_Samples:
===========================================================================================
Cleaned documentation: Most repeated timestamps for timeseries which contains only one NaN:{pd.DataFrame(one_nan_value_counts[one_nan_value_counts > 10]).to_html()}
Processed documentation: Most repeated timestamps for timeseries which contains only one NaN.
===========================================================================================
Cleaned documentation: Most repeated timestamps for timeseries which contains exactly three NaNs:{pd.DataFrame(three_nans_value_counts[three_nans_value_counts > 10]).to_html()}
Processed documentation: Most repeated timestamps for timeseries which contains exactly three NaNs.
===========================================================================================
Cleaned documentation: card1 value 7919 seems periodic, at about 30 days, with some double peaks too...
Processed documentation: card1 value 7919 seems periodic, at about 30 days, with some double peaks too.
===========================================================================================
Cleaned documentation: Date columns appear to mean the same thing, no daylight savings shift...
Processed documentation: Date columns appear to mean the same thing, no daylight savings shift.
===========================================================================================
Cleaned documentation: Read json files Pandas has a read_json method. Let's use it!
Processed documentation: Pandas has a read_json method. Let's use it!
===========================================================================================
Cleaned documentation: Reference: code from Ashish Patel(阿希什)Repeated KFOLD Approach: RMSE[3.70]. Kfold cross-validation. folds = KFold(n_splits=5, shuffle=True, random_state=11)
Processed documentation: Repeated KFOLD Approach: RMSE[3.70]. Kfold cross-validation. folds = KFold(n_splits=5, shuffle=True, random_state=11)
===========================================================================================
Cleaned documentation: df_test = pd.read_csv(test_path). df_test.drop(['ID_code'], axis=1, inplace=True). Samples which have unique values are real the others are fake
Processed documentation: df_test = pd.read_csv(test_path). df_test.drop(['ID_code'], axis=1, inplace=True). Samples which have unique values are real the others are
===========================================================================================
Cleaned documentation: Here, after splitting, the third category is pretty specific. The mean price is much higher than the second category.
Processed documentation: The third category is pretty specific. The mean price is much higher than the second category.
===========================================================================================
Cleaned documentation: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.fit(np.hstack([train_test.subcategory_2]))train_test['category2'] = le.transform(train_test.subcategory_2)le.fit(np.hstack([train_test.brand_name]))train_test['brand'] = le.transform(train_test.brand_name)le.fit(np.hstack([train_test.subcategory_1]))train_test['category1'] = le.transform(train_test.subcategory_1)le.fit(np.hstack([train_test.general_category]))train_test['general_category1'] = le.transform(train_test.general_category)
Processed documentation: from sklearn.preprocessing import LabelEncoderle = Label Encoder()le.fit(np.hstack([train_test.subcategory_2]))train_ test['category2'] = le.transform(
===========================================================================================
Cleaned documentation: Feature engineering Oil.csv - Replace missing values We can observe some missing values in the Oil prices database.
Processed documentation: Feature engineering Oil.csv - Replace missing values.
===========================================================================================
Cleaned documentation: Abstract I've been doing LB probe, and I will report all of them because probably completed.
Processed documentation: I've been doing LB probe, and I will report all of them because probably completed.
===========================================================================================
Cleaned documentation: Introduction This will be the longest EDA you've ever seen... Let's load some libraries and the data.
Processed documentation: Introduction This will be the longest EDA you've ever seen. Let's load some libraries and the data.
===========================================================================================
Cleaned documentation: pair_dict = {'A':'K','U':'K','G':'P','C':'P'}. df['pairs'] = df[['sequence']].applymap(lambda seq: ('').join([pair_dict[x] for x in seq]))
Processed documentation: pair_dict = {'A':'K','U':'k','G':'P','C':'p'}. df['pairs'] = df[['sequence']].applymap(lambda seq: (''
===========================================================================================
Cleaned documentation: Cross validation model. Create arrays and dataframes to store results. k-fold. set data structure. params optimized by optuna
Processed documentation: Cross validation model. Create arrays and dataframes to store results. k-fold. set data structure. optuna.
===========================================================================================
Cleaned documentation: time. vw -i model_only_text_part.vw -t -d valid_only_text.vw -p valid_pred1.txt --random_seed 17 -r valid_prob1.txt
Processed documentation: time. vw -i model_only_text_part.vw -t -d valid_ only_text.vW -p valid_pred1.txt --random_seed 17 -r valid_prob
===========================================================================================
Cleaned documentation: time. vw -i model_only_text.vw -t -d test_only_text.vw -p test_pred1.txt --random_seed 17 -r test_prob1.txt --quiet
Processed documentation: time. vw -i model_only_text.vw --random_seed 17 -r test_prob1.txt --quiet --quiet test.pred1.vW -t -d test.only_
===========================================================================================
Cleaned documentation: time. vw -i model_text_feat_part.vw -t -d valid_text_feat.vw -p valid_pred2.txt --random_seed 17 -r valid_prob2.txt --quiet
Processed documentation: time. vw -i model_text_feat_part.vw --random_seed 17 -r valid_prob2.txt --quiet --quiet valid_pred2. Vw -t -d valid_
===========================================================================================
Cleaned documentation: Now, let's look at 'v18q1', which indicates how many tablets the household owns.
Processed documentation: 'v18q1' indicates how many tablets the household owns.
===========================================================================================
Cleaned documentation: sub = pd.read_csv("../input/birdsong-recognition/sample_submission.csv"). sub.to_csv("submission.csv", index=False) this will be overwritten if everything goes well
Processed documentation: sub = pd.read_csv("../input/birdsong-recognition/sample_submission.csv"). sub.to_ CSV("submission", index=False) this will be overwritten if everything goes
===========================================================================================
Cleaned documentation: full_train_df.loc[(full_train_df['primary_use'] == le.transform(['Education'])[0]) & (full_train_df['month'] >= 6) & (full_train_df['month'] <= 8), 'is_vacation_month'] = np.int8(1). full_train_df.loc[full_train_df['is_vacation_month']!=1, 'is_vacation_month'] = np.int8(0)
Processed documentation: 'is_vacation_month'] = np.int8(1). full_train_df.loc = le.transform(['Education'')[0] & (full_train-df.month']!=
===========================================================================================
Cleaned documentation: full_test_df.loc[(full_test_df['primary_use'] == le.transform(['Education'])[0]) & (full_test_df['month'] >= 6) & (full_test_df['month'] <= 8), 'is_vacation_month'] = np.int8(1). full_test_df.loc[full_test_df['is_vacation_month']!=1, 'is_vacation_month'] = np.int8(0)
Processed documentation: 'is_vacation_month'] = np.int8(1). full_test_df.loc = le.transform(['Education'')[0] & (full_ test_df['month'] >=
===========================================================================================
Cleaned documentation: lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0.00001, eps=1e-08)
Processed documentation: lr_scheduler = torch.optim.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0, threshold_mode
===========================================================================================
Cleaned documentation: Now let's split the entire dataset into 5 folds with stratification on source. Reference:
Processed documentation: Now let's split the entire dataset into 5 folds with stratification on source.
===========================================================================================
Cleaned documentation: Let's plot all the crops separately and verify that all the crops have meaningful details and are not empty.
Processed documentation: Let's plot all the crops separately and verify that they have meaningful details and are not empty.
===========================================================================================
Cleaned documentation: Once again, thees rules are the important bit. overrides. overrides. fatality special. general
Processed documentation: Once again, thees rules are the important bit. overrides. fatality special.
===========================================================================================
Cleaned documentation: Let's iterate over the test IDs and generate run-length encodings for each seperate mask identified by skimage ...
Processed documentation: Let's iterate over the test IDs and generate run-length encodings for each seperate mask identified by skimage.
===========================================================================================
Cleaned documentation: Parameter Setting. I also use 'tracking'thanks Rob!!. prams for validation. eval_metric': 'rmse', This did not work.4/26
Processed documentation: parameter setting. eval_metric': 'rmse', This did not work. I also use 'tracking'thanks Rob!!. prams for validation.
===========================================================================================
Cleaned documentation: Only fold1 calculation, so commented out here.. oof_f1 = f1_score(y, oof_pred, average = "macro"). print(f'f1_score_oof = {oof_f1:0.5f}')
Processed documentation: Only fold1 calculation, so commented out here. oof_f1 = f1_score(y, oof-pred, average = "macro"). print(f'f1_ score_oof = {oof
===========================================================================================
Cleaned documentation: d level category details Now, we start investigation by category_name. This is a rough statistics of categories.
Processed documentation: This is a rough statistics of categories. Now, we start investigation by category_name.
===========================================================================================
Cleaned documentation: c32 = pd.DataFrame(). c32["logprice"] = group3["logprice"].mean(). c32["num"] = group3["category_name"].count(). c32 = c32.sort_values(by="num", ascending=False)
Processed documentation: c32 = pd.DataFrame() c32.sort_values(by="num", ascending=False) c32["logprice"] = group3 ["logprice"].mean(). c32[num] = group
===========================================================================================
Cleaned documentation: min_child_samples': 33,. min_child_weight': 11.1583,. min_split_gain': 0.1,. reg_alpha': 1.2456,. reg_lambda': 0.1950,. max_bin': 238,
Processed documentation: min_child_samples': 33,. min-child_weight': 11.1583,. min_split_gain': 0.1,. reg_alpha': 1.2456,. reg-lambda: 0.1950,.
===========================================================================================
Cleaned documentation: time. sz = 640. for fname in tqdm(train_video_files[:10]):. with IPyExperimentsPytorch() as exp:. t, _, (H, W) = get_decord_video_batch(fname, sz)
Processed documentation: For each fname in tqdm(train_video_ files[:10]):. with IPyExperimentsPytorch() as exp:. t, _, (H, W) = get_decord
===========================================================================================
Cleaned documentation: Old style model is stored with all names of parameters sharing common prefix 'module.'
Processed documentation: Old style model is stored with all names of parameters sharing common prefix'module'
===========================================================================================
Cleaned documentation: char offset to token offset. token offset to sentence offset. buckets. relative distance. buckets. absolute position in the sentence
Processed documentation: char offset to token offset. token offset to sentence offset. buckets. relative distance. absolute position in the sentence.
===========================================================================================
Cleaned documentation: Example: filter_data(train_df, 'Patient', ['ID00007637202177411956430']). Kevin - Data functions - filter dataframe according to specific value(s) in column "str_col"
Processed documentation: Example: filter_data(train_df, 'Patient', ['ID00007637202177411956430']). Kevin - Data functions - filter dataframe according to specific value(s) in column "str_
===========================================================================================
Cleaned documentation: Below, we see that Patients made 6-10 visits (`FVC` measurements), with the majority of them having made 9 visits.
Processed documentation: . Patients made 6-10 visits (FVC measurements), with the majority of them having made 9 visits.
===========================================================================================
Cleaned documentation: Natalia - 6 Categories. plt_fvc_vs_sns(df = patient_info_df, x = 'Weeks_range', y = 'FVC_mean', z = 'Sex_Smoke', color_palette = 'CMRmap_r')
Processed documentation: Natalia - 6 Categories. plt_fvc_vs_sns(df = patient_info_df, x = 'Weeks_range', y = 'FVC_mean', z = 'Sex_Sm
===========================================================================================
Cleaned documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. x = L.Dense(100, activation="relu", name="d3")(x). model.compile(loss=qloss, optimizer="adam", metrics=[score])
Processed documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. x = L.Dense(100, activation="relu", name="d3")(x). model.compile(loss=
===========================================================================================
Cleaned documentation: Ensemble of Resnext & Xception Resnext single model public score: 0.46441 Xception single model public score: 0.50480
Processed documentation: Ensemble of Resnext & Xception Resnext single model public score: 0.46441 Xception single modelpublic score:0.50480
===========================================================================================
Cleaned documentation: Apply the post-processing technique in one line (as explained in the pseudo-code of my post.
Processed documentation: Apply the post-processing technique in one line (as explained in the pseudo-code)
===========================================================================================
Cleaned documentation: Brunei. Myanmar. ConfirmedCases_date_Myanmar = train[train['Country_Region']=='Myanmar'].groupby(['Date']).agg({'ConfirmedCases':['sum']}). fatalities_date_Myanmar = train[train['Country_Region']=='Myanmar'].groupby(['Date']).agg({'Fatalities':['sum']}). total_date_Myanmar = ConfirmedCases_date_Myanmar.join(fatalities_date_Myanmar). plt.subplot(2, 2, 2). total_date_Myanmar.plot(ax=plt.gca(), title='Myanmar')
Processed documentation: Brunei. Myanmar. ConfirmedCases_date_Myanmar = train[train['Country_Region']=='Myanmar'].groupby(['Date']).agg({'ConfirmedC cases':['sum
===========================================================================================
Cleaned documentation: Wow, more than 800 names in common, let's see a few of them...
Processed documentation: Wow, more than 800 names in common, let's see a few of them.
===========================================================================================
Cleaned documentation: ax.text(5,5,"Boxplot After removing outliers", fontsize=18, color="r", ha="center", va="center"). Distplot to see the distribution after outliers have been removed. plt.xticks(rotation=90)
Processed documentation: Plt.ax.text(5,5,"Boxplot After removing outliers", fontsize=18, color="r", ha="center", va="center"). Distplot to see the distribution after outliers have been removed
===========================================================================================
Cleaned documentation: Returns the confusion matrix between rater's ratings. The following 3 functions have been taken from Ben Hamner's github repository
Processed documentation: The following 3 functions have been taken from Ben Hamner's GitHub repository.
===========================================================================================
Cleaned documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. TODO: Try others encoders
Processed documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. Try others encoders.
===========================================================================================
Cleaned documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. TODO: Try others encoders
Processed documentation: Dropping molecule_name and encode atom_0, atom_1 and type_0. Try others encoders.
===========================================================================================
Cleaned documentation: Generate predictions Finally, we'll use our trained multilingual model to generate predictions for the test data.
Processed documentation: We'll use our trained multilingual model to generate predictions for the test data.
===========================================================================================
Cleaned documentation: pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[IMAGE_SIZE, 3]). pretrained_model.trainable = False tramsfer learning. pretrained_model,. models.append(model)
Processed documentation: pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=[IMAGE_SIZE, 3]). pretrained_ model.train
===========================================================================================
Cleaned documentation: Let's iterate over the test IDs and generate run-length encodings for each seperate mask identified by skimage ...
Processed documentation: Let's iterate over the test IDs and generate run-length encodings for each seperate mask identified by skimage.
===========================================================================================
Cleaned documentation: I'd like to show some graph. You can see the obvious relationship between 'angle' and 'scalar_coupling_constant'.
Processed documentation: I'd like to show some graph. You can see the obvious relationship between 'angle' and'scalar_coupling_constant'
===========================================================================================
Cleaned documentation: again, only using 10% of data for the committed kernel to work...
Processed documentation: again, only using 10% of data for the committed kernel to work.
===========================================================================================
Cleaned documentation: This kernel is written in both English and Japanese.------ CornerNet派生の『CenterNet』と呼ばれるもので、YOLOなどのようにアンカーを使用せず、セグメンテーション(U-Net)のようなヒートマップで対象物の中心点を検出する手法です。(シングルアンカーのような雰囲気ですが、ヒートマップだけでいいので実装しやすい印象です) DeNAさんはじめとした、様々な日本語の記事でも勉強させていただいておりますので、この場を借りてお礼申し上げます。
Processed documentation: This kernel is written in both English and Japanese.
===========================================================================================
Cleaned documentation: First of all, let's convert the input data into the labels for CenterNet. CenterNet用に入力データを形成しておきます。
Processed documentation: First of all, let's convert the input data into the labels for CenterNet.
===========================================================================================
Cleaned documentation: Let's create CNN model(almost ResNet). Input: Image (resized into 512x512x3) Output: Ratio of letter_size to picture_size ResNetっぽいモデルで試してみます。入力は1ページごとの画像。出力は文字とピクチャのサイズ比とします。
Processed documentation: Let's create CNN model(almost ResNet) Image: Image (resized into 512x512x3) Output: Ratio of letter_size to picture_size ResNet.
===========================================================================================
Cleaned documentation: Step1 is not main topic of this kernel. Run only 15 epochs. step2(CenterNet)なので、少し短めのエポックで切り上げます。
Processed documentation: Step1 is not main topic of this kernel. Run only 15 epochs.
===========================================================================================
Cleaned documentation: Start training. This kernel runs 30 epochs. Longer training can improve the accuracy. epochほど計算しますが、多分もっと長い方がよいです。
Processed documentation: Start training. This kernel runs 30 epochs. Longer training can improve the accuracy.
===========================================================================================
Cleaned documentation: ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: ime": "is_month_end",. ims: "is-month_start",. dt.drop( ["d", "wm_yr_wk", "weekday"], axis=1, inplace =
===========================================================================================
Cleaned documentation: for tdelta in range(0, 2):. te_sub.loc[te.date >= fday+ timedelta(days=h), "id"] = te_sub.loc[te.date >= fday+timedelta(days=h),. id"].str.replace("validation$", "evaluation"). te_sub.to_csv("submission.csv",index=False)
Processed documentation: for tdelta in range(0, 2):. te_sub.loc[te.date >= fday+timedelta(days=h),. id"].str.replace("validation$", "evaluation
===========================================================================================
Cleaned documentation: ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: ime": "is_month_end",. ims: "is-month_start",. dt.drop( ["d", "wm_yr_wk", "weekday"], axis=1, inplace =
===========================================================================================
Cleaned documentation: te_sub.loc[te.date >= fday+ timedelta(days=h), "id"] = te_sub.loc[te.date >= fday+timedelta(days=h),. id"].str.replace("validation$", "evaluation")
Processed documentation: te_sub.loc[te.date >= fday+timedelta(days=h),. id"].str.replace("validation$", "evaluation")
===========================================================================================
Cleaned documentation: KERAS MODEL DEFINITION. params. Inputs. Embeddings layers. rnn layer. main layer. output. model
Processed documentation: KERAS MODEL DEFINITION. Inputs. Embeddings layers. rnn layer. main layer. output. model.
===========================================================================================
Cleaned documentation: dt = pd.read_csv("../input/m5-forecasting-accuracy/sales_train_validation.csv",. nrows = nrows, usecols = catcols + numcols, dtype = dtype)
Processed documentation: dt = pd.read_csv("../input/m5-forecasting-accuracy/sales_train_validation",. nrows = nrows, usecols = catcols + numcols
===========================================================================================
Cleaned documentation: ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: ime": "is_month_end",. ims: "is-month_start",. dt.drop( ["d", "wm_yr_wk", "weekday"], axis=1, inplace =
===========================================================================================
Cleaned documentation: TurnOff]( ) You cannot use the internet in this competition. Turn it off. SettingsからインターネットをOFFにします。
Processed documentation: You cannot use the internet in this competition. Turn it off.
===========================================================================================
Cleaned documentation: X_train = tk.texts_to_sequences(X_train). X_val = tk.texts_to_sequences(X_val). X_train = pad_sequences(X_train, maxlen = max_len). X_val = pad_sequences(X_val, maxlen = max_len)
Processed documentation: X_train = tk.texts_to_sequences(X_ train). X_val = tK. texts_ to sequences (X_val, maxlen = max_len)
===========================================================================================
Cleaned documentation: TurnOff]( ) You cannot use the internet in this competition. Turn it off. SettingsからインターネットをOFFにします。
Processed documentation: You cannot use the internet in this competition. Turn it off.
===========================================================================================
Cleaned documentation: First. Check the contents of the data. image data and landmark_id are written in train.csv train.csvには画像データとlandmark_idについて書いてある。
Processed documentation: First. Check the contents of the data. image data and landmark_id are written in train.csv.
===========================================================================================
Cleaned documentation: At the first glance, 'returnsOpenPrevMktres10' and 'reuturnsOpenNextMktres10' have the same trend over time.
Processed documentation: At the first glance,'returnsOpenPrevMktres10' and'reuturnsOpenNextMktre10' have the same trend over time.
===========================================================================================
Cleaned documentation: Tf-idf. prepare tokenizer. create sparse matrices. vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize, min_df=3, max_df=0.9, strip_accents='unicode',. use_idf=1, smooth_idf=1, sublinear_tf=1 ). max_features=250000). max_features=250000)
Processed documentation: Tf-idf. prepare tokenizer. create sparse matrices. vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize, min_df=3, max_
===========================================================================================
Cleaned documentation: Case $x_{start}^ \leq x_{end}^$. Subcase $[x_{start}, x_{end}] \cap [x_{start}^, x_{end}^] \neq \varnothing $. Case images for better understanding:
Processed documentation: Case $x_{start}^ \leq x_{end}^$. Subcase $[x_start, x_end] \cap [x_ start, x end] $. Case images for better understanding:
===========================================================================================
Cleaned documentation: Load random image and mask.. Compute Bounding box. Display image and additional stats. Display image and instances
Processed documentation: Load random image and mask. Compute Bounding box. Display image and additional stats.
===========================================================================================
Cleaned documentation: custom function to build the LightGBM model.. boosting":"rf". model = lgb.cv(params, lgtrain, 1000, early_stopping_rounds=20, verbose_eval=20, stratified=False )
Processed documentation: custom function to build the LightGBM model. boosting":"rf". model = lgb.cv(params, lgtrain, 1000, early_stopping_rounds=20, verbose_eval=20), strat
===========================================================================================
Cleaned documentation: custom function to build the LightGBM model.. max_depth':-1,. boosting":"rf". model = lgb.cv(params, lgtrain, 1000, early_stopping_rounds=20, verbose_eval=20, stratified=False )
Processed documentation: custom function to build the LightGBM model.. max_depth':-1,. boosting":"rf". model = lgb.cv(params, lgtrain, 1000, early_stopping_rounds=20, verb
===========================================================================================
Cleaned documentation: fill in mean for floats. fill in -999 for categoricals. Label Encoding
Processed documentation: fill in mean for floats. fill in -999 for categoricals.
===========================================================================================
Cleaned documentation: margin = ArcMarginProduct(. n_classes=n_classes,. s=scale,. m=margin,. name='head/arc_margin',. dtype='float32'). opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
Processed documentation: margin = ArcMarginProduct(. n_classes=n_classes,. s=scale,. m=margin,. name='head/arc_margin',. dtype='float32') opt = tf.keras.optim
===========================================================================================
Cleaned documentation: Basic Statistic using Pandas and Numpy Pandas DataFrame/Numpy API df.mean(), np.mean(df)`
Processed documentation: Basic Statistic using Pandas and Numpy Pandas DataFrame/Numpy API.
===========================================================================================
Cleaned documentation: if you dont wanna load any word vectors. TEXT.build_vocab(train_data, max_size=50000). LABEL.build_vocab(train_data)
Processed documentation: if you dont wanna load any word vectors. TEXT.build_vocab(train_data, max_size=50000). LABEL.build _vocab()
===========================================================================================
Cleaned documentation: Missing Values Analysis Thanks Pedro Schoen for pointing missing are encoded as -1 Let's encode -1 as `np.nan`
Processed documentation: Missing values are encoded as -1. Let's encode -1 as `np.nan`
===========================================================================================
Cleaned documentation: clf_2 = xgb.XGBClassifier(random_state=42,n_jobs=-1,verbosity=1)params = {"max_depth":[3,4,5,6,7,8,9], "n_estimators":list(range(50,500,50)), "learning_rate":[0.01,0.05,0.1,0.3], "subsample":[0.5,0.6,0.7,0.8,0.9], "colsample_bytree":[0.5,0.6,0.7,0.8,0.9], "reg_alpha":[0.5,1,2,5,10], "reg_lambda":[0.5,1,2,5,10]}random_search_2 = RandomizedSearchCV(estimator=clf_2,param_distributions=params,cv=10,scoring='roc_auc')random_search_2.fit(X_Train,y_Train)
Processed documentation: clf_2 = xgb.XGBClassifier(random_state=42,n_jobs=-1,verbosity=1)params = {"max_depth":3,4,5,6,7,
===========================================================================================
Cleaned documentation: import pandas as pd. import numpy as np. df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False). df_play = df.drop_duplicates(subset='PlayId'). postprocess はありえない確率の削除； 上の方のセル参照）。
Processed documentation: import pandas as pd.read_csv('/kaggle/ input/nfl-big-data-bowl-2020/train.csv', low_memory=False). df_play = df.drop_
===========================================================================================
Cleaned documentation: Build X_train and y_train. PCA. ADD column prediction log or knn. TRAIN
Processed documentation: Build X_train and y_train. PCA. ADD column prediction log or knn.
===========================================================================================
Cleaned documentation: with ZipFile(zip_path) as myzip:. files_in_zip = myzip.namelist(). with ZipFile(zip_path) as myzip:. with myzip.open(files_in_zip[3]) as myfile:. img = Image.open(myfile)
Processed documentation: with ZipFile(zip_path) as myzip:. files_in_zip = myzip.namelist() with Image.open( myfile) as img:. img = Image. open(myfile)
===========================================================================================
Cleaned documentation: datetime속성을 분리하여 추출속성으로 활용하기 위해 split함수를 사용하여 년-월-일 과 시간을 분리한다.
Processed documentation: datetime.datetime.com:    ‘’’   ‘”’ ””  “” ” ’”
===========================================================================================
Cleaned documentation: heatmap 상관관계를 참조하여 이전의 시각화와는 달리 두 개의 서로다른 컬럼이 적용된 count를 시각화해보자. count. count. count. count
Processed documentation: heatmap.com:    ‘  ‘”’   “”  ‘’ ’’. count.’”:  ”�
===========================================================================================
Cleaned documentation: M columns. Converting Strings to ints(or floats if nan in column)
Processed documentation: M columns. Strings to ints(or floats if nan in column)
===========================================================================================
Cleaned documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns. Let's glimpse train and test dataset.
Processed documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns.
===========================================================================================
Cleaned documentation: Model. To be sure that some features are categoricals. Our features
Processed documentation: Model. To be sure that some features are categoricals.
===========================================================================================
Cleaned documentation: Some additional cleaning. Part 1. Convert 'd' to int. Remove 'wm_yr_wk'. as test values are not in train set
Processed documentation: Some additional cleaning. Convert 'd' to int. Remove 'wm_yr_wk' as test values are not in train set.
===========================================================================================
Cleaned documentation: The result. Same for NaNs values - it's normal. because there is no data for. rolling_period),-1(rolling_period),-2(rolling_period)
Processed documentation: The result. Same for NaNs values - it's normal. because there is no data for. rolling_period.
===========================================================================================
Cleaned documentation: Resume: On average, to have 100 percent, you need to have ≈ 3495 of FVC.
Processed documentation: Resume: On average, to have 100 percent, you need to have 3495 of FVC.
===========================================================================================
Cleaned documentation: Average and overall metric. mean_score = np.mean(fold_metrics). overal_score_minimizing = np.mean(fold_metrics)+np.std(fold_metrics). print(mean_score,overal_score_minimizing)
Processed documentation: Average and overall metric. mean_score = np.mean(fold_metrics) overal_score_minimizing = numpy.mean (fold_ Metrics) print(mean_score,overal_
===========================================================================================
Cleaned documentation: This seems to be fine. Now we can apply above functions on our dataframe. Lets check our dataframe. again.
Processed documentation: This seems to be fine. Lets check our dataframe. again.
===========================================================================================
Cleaned documentation: helper class to keep track of loss and loss per iteration. source:
Processed documentation: helper class to keep track of loss and loss per iteration.
===========================================================================================
Cleaned documentation: TODO: find if current challenge has any. with open('../input/humpback-whale-identification-model-files/rotate.txt', 'rt') as f: rotate = f.read().split('\n')[:-1]. rotate = set(rotate). rotate
Processed documentation: TODO: find if current challenge has any. with open('../input/humpback-whale-identification-model- files/rotate.txt', 'rt') as f: rotate = f.read
===========================================================================================
Cleaned documentation: if p in rotate: img = img.rotate(180). p = list(rotate)[0]. imgs = [pil_image.open(expand_path(p)), read_raw_image(p)]. show_whale(imgs)
Processed documentation: if p in rotate: img = img.rotate(180). p = list(rotate)[0]. imgs = [pil_image.open(expand_path(p), read_raw_image(
===========================================================================================
Cleaned documentation: GPU version. Return torch.uint8 located on gpu. Have proble with memory
Processed documentation: GPU version. Return torch.uint8 located on gpu.
===========================================================================================
Cleaned documentation: We named this parameter `mask_fraction`. We use supercategories to compare how `mask_fraction` distribution differs from supercategory to supercategory:
Processed documentation: We use supercategories to compare how `mask_fraction` distribution differs from supercategory to supercategory.
===========================================================================================
Cleaned documentation: Use the torch dataloader to iterate through the dataset. functions to show an image. get some images. show images
Processed documentation: Use the torch dataloader to iterate through the dataset. functions to show an image.
===========================================================================================
Cleaned documentation: Use the torch dataloader to iterate through the dataset. get some images. show images
Processed documentation: Use the torch dataloader to iterate through the dataset. get some images.
===========================================================================================
Cleaned documentation: Use the torch dataloader to iterate through the dataset. get some images. show images
Processed documentation: Use the torch dataloader to iterate through the dataset. get some images.
===========================================================================================
Cleaned documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. x = L.Dense(100, activation="relu", name="d3")(x). model.compile(loss=qloss, optimizer="adam", metrics=[score])
Processed documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. x = L.Dense(100, activation="relu", name="d3")(x). model.compile(loss=
===========================================================================================
Cleaned documentation: submission_df['answer_relevance'] = submission_df['answer_relevance'].apply(lambda x : 0.33333334326744 if x < 0.7 else x)
Processed documentation: submission_df['answer_relevance'] = submission_df.apply(lambda x : 0.33333334326744 if x < 0.7 else x)
===========================================================================================
Cleaned documentation: Logistic Regression without Standardization. This gives an accuracy score of 0.6875
Processed documentation: Logistic Regression without Standardization gives an accuracy score of 0.6875.
===========================================================================================
Cleaned documentation: modified from the original. drop lag col. ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: modified from the original. drop lag col. ime": "is_month_end",. ims: "is month_start",. dt.drop( ["d", "wm_yr_wk", "
===========================================================================================
Cleaned documentation: Delete columns we will not use. Type change. Only use confirmed > 0. Show data
Processed documentation: Delete columns we don't use. Type change. Only use confirmed > 0. Show data
===========================================================================================
Cleaned documentation: d step: estimate $\theta$ and $\sigma$ With fixed $(\kappa, \rho, \tau)$, we will estimate $\theta$ and $\sigma$ here.
Processed documentation: d step: estimate $\theta$ and $\sigma$ With fixed $(\kappa, \rho, \tau)$, we will estimate theta and sigma here.
===========================================================================================
Cleaned documentation: Show how to Saving Training Time, Show Over-fitting in this kernel. let's start
Processed documentation: Show how to Saving Training Time, Show Over-fitting in this kernel.
===========================================================================================
Cleaned documentation: Aggregating by season clearly lack in granularity. Therefore, I'll try with months. Months
Processed documentation: Aggregating by season clearly lack in granularity. Therefore, I'll try with months.
===========================================================================================
Cleaned documentation: ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: ime": "is_month_end",. ims: "is-month_start",. dt.drop( ["d", "wm_yr_wk", "weekday"], axis=1, inplace =
===========================================================================================
Cleaned documentation: We notice that for neutral sentiment, the selected text is nearly always the text itself. Let's check it :
Processed documentation: We notice that for neutral sentiment, the selected text is nearly always the text itself.
===========================================================================================
Cleaned documentation: Compare the similarity between "cat" vs. "kitten" and "cat" vs. "cats" from GloVe
Processed documentation: Compare the similarity between "cat" vs. "kitten" from GloVe.
===========================================================================================
Cleaned documentation: Compare the similarity between "cat" vs. "kitten" and "cat" vs. "cats" from FastText
Processed documentation: Compare the similarity between "cat" and "kitten" from FastText.
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: As expected, we have 22 of each playid. Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: Reading the data. Separating target and ids. Classifying variables in binary, high and low cardinality nominal, ordinal and dates
Processed documentation: Reading the data. Separating target and ids. Classifying variables in binary, high and low cardinality nominal, ordinal.
===========================================================================================
Cleaned documentation: Setting all to dtype float32. Defining categorical variables. Setting categorical variables to int64
Processed documentation: Setting all to dtype float32. Defining categorical variables to int64.
===========================================================================================
Cleaned documentation: X_dnn = pd.read_csv("/kaggle/input/categorical-feature-encoding-with-tensorflow/oof.csv")Xt_dnn = pd.read_csv("/kaggle/input/categorical-feature-encoding-with-tensorflow/dnn_cv_submission.csv")X['dnn_preds'] = X_dnn.dnn_oofXt['dnn_preds'] = Xt_dnn.targetdel((X_dnn, Xt_dnn)). Stacking
Processed documentation: X_dnn = pd.read_csv("/kaggle/ input/categorical-feature-encoding-with-tensorflow/oof.csv")Xt_dNN = pD.
===========================================================================================
Cleaned documentation: Setting all to dtype float32. Defining categorical variables. Setting categorical variables to int64
Processed documentation: Setting all to dtype float32. Defining categorical variables to int64.
===========================================================================================
Cleaned documentation: Returns a batch from X, y random_state allows determinism different scikit-learn CV strategies are possible. train/validation batch generator
Processed documentation: returns a batch from X, y random_state. Different scikit-learn CV strategies are possible. train/validation batch generator.
===========================================================================================
Cleaned documentation: Time Series df. Mask for time series data points that are actually used in RMSSE computation
Processed documentation: Time Series. Mask for time series data points that are actually used in RMSSE computation.
===========================================================================================
Cleaned documentation: clean up names a bit and take the first as validation. TODO: use train_test_split to do validation rows
Processed documentation: clean up names a bit and take the first as validation. Use train_test_split to do validation rows.
===========================================================================================
Cleaned documentation: Overview This script will load the iNat2019 dataset, print a summary, and display a random image.
Processed documentation: This script will load the iNat2019 dataset, print a summary, and display a random image.
===========================================================================================
Cleaned documentation: Lets check how the distribution of test and vaidation set looks like ...
Processed documentation: Lets check how the distribution of test and vaidation set looks like.
===========================================================================================
Cleaned documentation: generate word cloud. generate word cloud. generate word cloud. generate word cloud. generate word cloud. generate word cloud
Processed documentation: generate word cloud. generate wordCloud.com to create a word cloud of words.
===========================================================================================
Cleaned documentation: Features extraction Applying functions-defined above-to extract features based on characteristics of author's writing style...
Processed documentation: Features extraction Applying functions-defined above-to extract features based on characteristics of author's writing style.
===========================================================================================
Cleaned documentation: Let's check correlation using a heatmap and check how the features are correlated ..
Processed documentation: Let's check correlation using a heatmap and check how the features are correlated.
===========================================================================================
Cleaned documentation: Lets check how the distribution of test and vaidation set looks like ...
Processed documentation: Lets check how the distribution of test and vaidation set looks like.
===========================================================================================
Cleaned documentation: Timer Util. torch.backends.cudnn.deterministic = True type: ignore. torch.backends.cudnn.benchmark = True type: ignore
Processed documentation: Timer Util. torch.backends.cudnn.deterministic = True type: ignore.
===========================================================================================
Cleaned documentation: logger = get_logger("main.log"). TRAIN_RESAMPLED_AUDIO_DIRS = [. INPUT_ROOT / "birdsong-resampled-train-audio-{:0>2}".format(i) for i in range(5)
Processed documentation: logger = get_logger("main.log"). TRAIN_RESAMPLED_AUDIO_DIRS = [. INPUT_ROOT / "birdsong-resampled-train-audio-{
===========================================================================================
Cleaned documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. x = L.Dense(100, activation="relu", name="d3")(x). model.compile(loss=qloss, optimizer="adam", metrics=[score])
Processed documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. x = L.Dense(100, activation="relu", name="d3")(x). model.compile(loss=
===========================================================================================
Cleaned documentation: use sklearn.imputer object instead of pd.fillna(). use sklearn.imputer object instead of pd.fillna()
Processed documentation: use sklearn.imputer object instead of pd.fillna() to get the same result.
===========================================================================================
Cleaned documentation: Data📁 extend()`: Extend list by appending elements from the iterable. The code below does the following:
Processed documentation: The code below does the following:. extend()`: Extend list by appending elements from the iterable.
===========================================================================================
Cleaned documentation: ResNet34. eff_net34 = ResNet34Network(output_size = num_classes).to(device). Uncomment and train the model. train(model=eff_net34, epochs=epochs, batch_size=batch_size, num_workers=num_workers,. learning_rate=learning_rate, weight_decay=weight_decay, plot_loss=plot_loss)
Processed documentation: ResNet34.Resnet34.Network(output_size = num_classes).to(device). Uncomment and train the model. train(model=eff_net34, epochs=epochs, batch_
===========================================================================================
Cleaned documentation: A list with order_id repeated len(user_products) time.. df['dow'] = df.order_id.map(orders.order_dow). df['UP_same_dow_as_last_order'] = df.UP_last_order_id.map(orders.order_dow) == \. df.order_id.map(orders.order_dow)
Processed documentation: A list with order_id repeated len(user_products) time.. df['dow'] = df.order_id.map(orders.order _dow). df['UP_same_dow_as_
===========================================================================================
Cleaned documentation: Install Torch geometric Very long process as it recompiles sparse/scatter and CPU in instance are not very fast.
Processed documentation: . Very long process as it recompiles sparse/scatter and CPU. Not very fast.
===========================================================================================
Cleaned documentation: submission = pd.DataFrame({'fullVisitorId':visitorid,'PredictedLogRevenue':y_pred_submit}). submission['fullVisitorId']= submission['fullVisitorId'].astype(str). submission['PredictedLogRevenue']=submission['PredictedLogRevenue'].apply(lambda x: 0 if x<0 else x)
Processed documentation: submission = pd.DataFrame({'fullVisitorId':visitorid,'PredictedLogRevenue':y_pred_submit}). submission['fullVisitorsId']= submission.astype(str
===========================================================================================
Cleaned documentation: You may want to use this to find the best value of number of workers...
Processed documentation: You may want to use this to find the best value of number of workers.
===========================================================================================
Cleaned documentation: f1 = f1_score(target, output). sk_f1 = sk_f1_score(target.cpu().data.numpy(), (output > 0.5).cpu().data.numpy(), average='macro')
Processed documentation: f1 = f1_score(target, output). sk_f1= sk_ f1(target.cpu).data.numpy(), (output > 0.5).cpu()()() (average='macro
===========================================================================================
Cleaned documentation: Let's recreate the functions and have a closer look:. and calculate precision array. and calculate recall array
Processed documentation: Let's recreate the functions and have a closer look:. calculate precision array. and calculate recall array
===========================================================================================
Cleaned documentation: For each image id, determine the list of pictures. Notice how 25460 images use only 20913 distinct image ids.
Processed documentation: For each image id, determine the list of pictures. 25460 images use only 20913 distinct image ids.
===========================================================================================
Cleaned documentation: image = pydicom.read_file(os.path.join(train_images_dir,'ID_'+images[im]+ '.dcm')).pixel_array i = im // width j = im % width axs[i,j].imshow(image, cmap=plt.cm.bone) axs[i,j].axis('off')
Processed documentation: image = pydicom.read_file(os.path.join(train_images_dir,'ID_'+images[im]+ '.dcm')).pixel_array i = im // width j = im
===========================================================================================
Cleaned documentation: face file naming rule REAL: [ream movie]_r_[frame no]_[center x]_[center y].png FAKE: [base real movie]-[fake movie]_f_[frame no]_[center x]_[center y]_[diff-std].png
Processed documentation: face file naming rule REAL: [ream movie]_r_[frame no]_[center x]_ [center y].png FAKE: [base real movie]-[fake movie] f_[ frame no]
===========================================================================================
Cleaned documentation: We can see that ```max_depth``` = 6 is the best choice, valid-gini:0.293288, n_trees_341 Now, let's tweak ```min_child_weight``` parameter
Processed documentation: We can see that ```max_depth``` = 6 is the best choice, valid-gini:0.293288, n_trees_341. Now, let's tweak ```min_child_weight
===========================================================================================
Cleaned documentation: So, ```min_child_weight``` = 2 is the best, valid-gini:0.293653, n_tree = 392 Finally, we can tweak ```colsample_bytree``` parameter
Processed documentation: So, ```min_child_weight``` = 2 is the best, valid-gini:0.293653, n_tree = 392. Finally, we can tweak ```colsample_bytree``
===========================================================================================
Cleaned documentation: Final model. watchlist = [(d_train, 'train'), (d_valid, 'valid')]. model = xgb.train(xgb_params, d_train, 392, watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)
Processed documentation: Final model. watchlist = [(d_train, 'train'), (d_valid, 'valid')]. model = xgb.train(xgb_params, d_train,. watchlist, feval=gini
===========================================================================================
Cleaned documentation: Let's make pairplots. We can clearly see correlation with winPlacePerc (but maybe only with weaponsAcquired it's difficult to see)
Processed documentation: Let's make pairplots. We can clearly see correlation with winPlacePerc (but maybe only with weaponsAcquired)
===========================================================================================
Cleaned documentation: Import datasets For train. Import datasets For test. test_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/test.csv'). weather_test_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_test.csv'). sample_submission = pd.read_csv('/kaggle/input/ashrae-energy-prediction/sample_submission.csv')
Processed documentation: Import datasets For train. Import datasets For test. test_df = pd.read_csv('/kaggle/ input/ashrae-energy-prediction/test.csv'). weather_test_
===========================================================================================
Cleaned documentation: test['mean_'+feature] = (train[feature].mean()-test[feature]). test['z_'+feature] = (test[feature] - train[feature].mean())/train[feature].std(ddof=0). test['sqrt_'+feature] = np.abs(test[feature])(1/2). test['p4_'+feature] = (test[feature])4. test['r1_'+feature] = np.round(test[feature], 1)
Processed documentation: test['mean_'+feature] = (train[feature].mean()-test[feature]). test['z_' +feature' = (test[ Feature] - train[ Feature]. mean())/train[ Feature
===========================================================================================
Cleaned documentation: y = np.array(y). print(y). print((1 - lam) y[index_array]). print((lam y).shape,((1 - lam) y[index_array]).shape). print(X.shape[0], y.shape[0]). print('before', X_batch.shape, y_batch.shape). print('')
Processed documentation: y = np.array(y). print(y), print((1 - lam) y[index_array]), print(X.shape[0], y. shape[0]). print('before', X_batch.shape
===========================================================================================
Cleaned documentation: plt.plot(history.history['loss']). plt.plot(history.history['val_loss']). plt.title('Training for ' +str(epochs)+ ' epochs'). plt.legend(['Training Loss', 'Validation Loss'], loc='upper right'). plt.show()
Processed documentation: plt.title('Training for'+str(epochs)+'epochs'). plt.legend(['Training Loss', 'Validation Loss'], loc='upper right'). pl t.plot(history.
===========================================================================================
Cleaned documentation: plt.plot(history.history['acc']). plt.plot(history.history['val_acc']). plt.title('Training for ' +str(epochs)+ ' epochs'). plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right'). plt.show()
Processed documentation: plt.title('Training for'+str(epochs) +'epochs'). plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right') Plt.plot(history
===========================================================================================
Cleaned documentation: x = Conv2D(32, kernel_size=(1,1), activation='relu')(base_model.output). x = Flatten()(x). x = Dense(1024, activation='relu')(x). x = Dropout(0.3)(x)
Processed documentation: x = Conv2D(32, kernel_size=(1,1), activation='relu')(base_model.output). x = Flatten()(x).x = Dense(1024, activation=' re
===========================================================================================
Cleaned documentation: Loading Data. time_to_failure" has so little difference. So loading by float64. time_to_failure"カラムは、その値が非常に細かい差異しかもっていないので、float64にて読み込むようにします。
Processed documentation: Loading Data. time_to_failure has so little difference. So loading by float64. time-to-failure is better.
===========================================================================================
Cleaned documentation: Loading Data. time_to_failure" has slight differences, so read by np.float64. time_to_failure"はデータの値の差が非常に小さいので、np.float64で読み込む。
Processed documentation: Loading Data. time_to_failure has slight differences, so read by np.float64.
===========================================================================================
Cleaned documentation: backbone. embedding model. anchor encoding. positive encoding. anchor encoding. construct model
Processed documentation: backbone. embedding model. anchor encoding. positive encoding. construct model.
===========================================================================================
Cleaned documentation: m = MyModel(). served_function = m.call. tf.saved_model.save(. m,. export_dir="./model",. signatures={'serving_default': served_function}
Processed documentation: m = MyModel() served_function = m.call.saved_model.save(m, export_dir="./model",. signatures={'serving_default': served_ function}
===========================================================================================
Cleaned documentation: D reconstruction. This code for converting 2-D slices to a 3-D image is written by [mrbean-bremen]( Orignal Code [here](
Processed documentation: D reconstruction. This code for converting 2-D slices to a 3-D image is written by [mrbean-bremen]
===========================================================================================
Cleaned documentation: X-Ray Images with Masks Can you identify the diseases? I could not...
Processed documentation: X-Ray Images with Masks Can you identify the diseases? I could not.
===========================================================================================
Cleaned documentation: Some records in curated train data have 2, 3, 4, 6 labels. labels in curated train data
Processed documentation: Some records in curated train data have 2, 3, 4, 6 labels.
===========================================================================================
Cleaned documentation: First common 200 brandsmost_common_brands = data['brand_name'].value_counts().sort_values(ascending=False)[:150]. If a brand not in common brands, it was labeled as other_brand
Processed documentation: most_common_brands = data['brand_name'].value_counts().sort_values(ascending=False)[:150]. If a brand not in common brands, it was labeled as other_brand.
===========================================================================================
Cleaned documentation: Using XGBoost to obtain high-quality features. I run 2 XGBoost models to obtain better features.
Processed documentation: Using XGBoost to obtain high-quality features. I run 2 models to obtain better features.
===========================================================================================
Cleaned documentation: Averaging evaluations. Converting np.array to pd.Series. Filling NaN values with mean. Some teams aren't evaluated after 117th day.
Processed documentation: Averaging evaluations. Filling NaN values with mean. Some teams aren't evaluated after 117 days.
===========================================================================================
Cleaned documentation: It seems that there is a corrleation of 0.1 between depth and saltPercentage. Let's dig deeper.
Processed documentation: It seems that there is a corrleation of 0.1 between depth and saltPercentage.
===========================================================================================
Cleaned documentation: Let's try to transform the images in some way to enhance the contrast between the salt and the background.
Processed documentation: Let's try to transform the images to enhance the contrast between the salt and the background.
===========================================================================================
Cleaned documentation: Transpose the input volume CXY to XYC order, which is what matplotlib requires.. plt.imshow(np.hstack((im.transpose(1,2,0)[...,:3], target_as_rgb)))
Processed documentation: Transpose the input volume CXY to XYC order, which is what matplotlib requires. plt.imshow(np.hstack((im.transpose(1,2,0)[...,:3],
===========================================================================================
Cleaned documentation: Distribution of deg_50C Averaged over position. Distribution of deg_pH10 Averaged over position
Processed documentation: Distribution of deg_50C Averaged over position.
===========================================================================================
Cleaned documentation: partition the data into training and testing splits using 75% training and 25% for validation. Refer
Processed documentation: partition the data into training and testing splits using 75% training and 25% for validation.
===========================================================================================
Cleaned documentation: Process the training, testing and 'other' datasets, and then check to ensure the arrays look reasonable.
Processed documentation: Process the training, testing and 'other' datasets. Then check to ensure the arrays look reasonable.
===========================================================================================
Cleaned documentation: Apply the post-processing technique in one line (as explained in the pseudo-code of my post.
Processed documentation: Apply the post-processing technique in one line (as explained in the pseudo-code)
===========================================================================================
Cleaned documentation: The training data. In this kernel, I'll use the `question_title`, `question_body` and `answer` columns.
Processed documentation: The training data. In this kernel, I'll use the `question_title` and 'question_body' columns.
===========================================================================================
Cleaned documentation: submission fileを同じくmelt処理し、sales_train_valとつなげる。 submission fileの列名を"d_xx"形式に変更する. submission fileで縦に結合されたvalidationとevaluationを一度分割し、それぞれことなる28日間の列名"d_xx"をそれぞれ付与。 submission fileには, idの詳細（item, department, state等）が無いためidをキーに, sales validationから取得したproductを結合 test2は、6/1まで不要なため削除
Processed documentation: submission file.submissionfile. Submission file  melt処理  sales_train_val    submission. file. submission file   "d_xx"
===========================================================================================
Cleaned documentation: addr1 - addr2 The host of the competition stated that these features are categorical even if they look numerical.
Processed documentation: . The host of the competition stated that these features are categorical even if they look numerical.
===========================================================================================
Cleaned documentation: Technical Analysis Technical indicators are a big part of financial analysis, this computes them.
Processed documentation: Technical indicators are a big part of financial analysis, this computes them.
===========================================================================================
Cleaned documentation: Scores: EfficientNetB0: 0.8733 EfficientNetB1: 0.8909(best) EfficientNetB2: 0.87 EfficientNetB5: 0.88 Resnet_152_v2: 0.85 DenseNet201: 0.83
Processed documentation: Scores: EfficientNetB0: 0.8733 E efficientNetB1:0.8909(best) Efficient netB2: 0,87 EfficientnetB5: 0,.88 Resnet_
===========================================================================================
Cleaned documentation: solution_3 = pd.DataFrame({'ID_code': test, "target" : pred_1 }). creating csv file. solution_3.to_csv("santander_3.csv", index = False). solution_3.sample(10)
Processed documentation: solution_3 = pd.DataFrame({'ID_code': test, "target" : pred_1 }). creating csv file. solution_3.to_csv("santander_3"), index
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: As expected, we have 22 of each playid. Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: for f in tqdm(features[2:]):. train_df.loc[:, f] = gaussian_to_uniform(train_df.loc[:, f].values). test_df.loc[:, f] = gaussian_to_uniform(test_df.loc[:, f].values)
Processed documentation: for f in tqdm(features[2:]):. train_df.loc[:, f] = gaussian_to_uniform(train_DF.loc [:, f].values). test_df,.
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results Arguments: seed {int} -- Number of the seed. random.seed(seed). os.environ["PYTHONHASHSEED"] = str(seed)
Processed documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed. random.seed(seed). os.environ ["PYTHONHASHSEED"] = str(
===========================================================================================
Cleaned documentation: test_var = pd.read_csv('/kaggle/input/msk-redefining-cancer-treatment/test_variants.zip')test_text =pd.read_csv("/kaggle/input/msk-redefining-cancer-treatment/test_text.zip",sep="\|\|",engine="python",names=["ID","TEXT"],skiprows=1)test = pd.merge(test_var, test_text, how = 'left', on = 'ID').fillna('')test.head()
Processed documentation: test_var = pd.read_csv('/kaggle/input/msk-redefining-cancer-treatment/test_variants.zip') test_text =pd.read-csv(' /
===========================================================================================
Cleaned documentation: df_all['Gene_cnt'] = df_all.apply(lambda x: count_words(x['TEXT'], x['Gene']), axis=1). df_all['Variation_cnt'] = df_all.apply(lambda x: count_words(x['TEXT'], x['Variation']), axis=1)
Processed documentation: df_all['Gene_cnt'] = df_all.apply(lambda x: count_words(x['TEXT'], x['Gene']), axis=1). df_ all['Variation_ cnt']
===========================================================================================
Cleaned documentation: print(os.path.join("/kaggle/input/deepfake-detection-challenge/test_videos/", vi)). print(vi,": ",i , " : ",classifier.predict(np.array([inp]))). submit.append([vi,re_video]). submit[vi] = 1.0-re_video. print(vi,": ",str(1.0-re_video))
Processed documentation: print(os.path.join("/kaggle/ input/deepfake-detection-challenge/test_videos/", vi). print(vi,": ",i, " : ",classifier.predict
===========================================================================================
Cleaned documentation: Bird songs Right, let's look at some bird song recordings. Will resample all recordings to mono, 22050Hz.
Processed documentation: Bird song recordings. Will resample all recordings to mono, 22050Hz.
===========================================================================================
Cleaned documentation: Create TFRecords So we will convert the Global Wheat Detection dataset to TFRecords, for use in TensorFlow-based models.
Processed documentation: We will convert the Global Wheat Detection dataset to TFRecords, for use in TensorFlow-based models.
===========================================================================================
Cleaned documentation: Helper functions A few helper functions are defined below for visualizing images.
Processed documentation: A few helper functions are defined below for visualizing images.
===========================================================================================
Cleaned documentation: mjd': ['min', 'max', 'size'],. passband': ['min', 'max', 'mean', 'median', 'std','skew'],. del train_feats['mjd_max'], train_feats['mjd_min']
Processed documentation: mjd': ['min','max','size'],. passband: ['min,'max'','mean','median','std','skew',. train_feats: ['m
===========================================================================================
Cleaned documentation: Grid search gives best accuracy for max_depth=8,min_child_weight=6,gamma=0.4,colsample_bytree=0.6,subsample=0.6 Training the model again with these new parameters.
Processed documentation: Grid search gives best accuracy for max_depth=8,min_child_weight=6,gamma=0.4,colsample_bytree= 0.6,subsample =0.6 Training the model
===========================================================================================
Cleaned documentation: Data Collection I start Collection Data by the training and testing datasets into Pandas DataFrames. Go to top](top)
Processed documentation: Data Collection I start Collection Data by the training and testing datasets into Pandas DataFrames. Go to top.
===========================================================================================
Cleaned documentation: clf_inputs = {. xt200_test': (ExtraTreesClassifier(n_estimators=200, max_depth=1, max_features='auto',random_state=314, n_jobs=4,. class_weight=test_weight_map),. max_depth': 25},
Processed documentation: clf_inputs = {. xt200_test': (ExtraTreesClassifier(n_estimators=200, max_depth=1, max-features='auto',random_state=314,
===========================================================================================
Cleaned documentation: cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'fe_people_weird_stat', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'v14a']. cols_2_drop=[]
Processed documentation: cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18 parentesco10_ MEAN',  ' parentesco11_MEONE',  'parentes
===========================================================================================
Cleaned documentation: Cluster Resolver for Google Cloud TPUs.. Connects to the given cluster.. Initialize the TPU devices.. TPU distribution strategy implementation.
Processed documentation: Cluster Resolver for Google Cloud TPUs. Connects to the given cluster. Initialize the TPU devices. TPU distribution strategy implementation.
===========================================================================================
Cleaned documentation: This post]( from [@Bojan]( and [this post]( from [@Bibek]( will be very helpful to the development of this project.
Processed documentation: This post and this post will be very helpful to the development of this project.
===========================================================================================
Cleaned documentation: Copy pretrained model weights to the default path. cp '../input/resnet18/resnet18.pth' '/tmp/.torch/models/resnet18-5c106cde.pth'. cp '../input/densenet121/densenet121.pth' '/tmp/.torch/models/densenet121-a639ec97.pth'
Processed documentation: Copy pretrained model weights to the default path. cp '../input/resnet18/res Net 18.pth' /tmp/.torch/models/resNet18-5c106cde.pTH.
===========================================================================================
Cleaned documentation: labels = F.array(labels). preds = F.array(preds). print("IOU_INFO:: bg:{}, 1:{}, 2:{}, 3:{}, 4:{}, mean_iou:{}".format(ious))
Processed documentation: labels = F.array(labels). preds = F'sarray(preds). print("IOU_INFO:: bg,. 1,. 2,. 3,. 4,. 5,. 6,. 7,. 8,. 9
===========================================================================================
Cleaned documentation: model = torchvision.models.resnext50_32x4d(pretrained=True). model.load_state_dict(torch.load("../input/pytorch-pretrained-models/resnet101-5d3b4d8f.pth")). model.avg_pool = nn.AdaptiveAvgPool2d(1). model.load_state_dict(torch.load("../input/pytorch-transfer-learning-baseline/model.bin")). device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
Processed documentation: model = torchvision.models.resnext50_32x4d(pretrained=True). model.avg_pool = nn.AdaptiveAvgPool2d(1) model.load_state_
===========================================================================================
Cleaned documentation: from model import Unet import Unet model from the script. pip install git+
Processed documentation: from model import Unet import model from the script. pip install git+
===========================================================================================
Cleaned documentation: import pandas as pd. train_df.to_csv('train_df.csv',index=False). test_df=pd.read_csv('../input/submission/submission_v2.csv'). for index,row in test_df.iterrows():. row['selected_text']
Processed documentation: import pandas as pd. train_df.to_csv('train_df',index=False). test_df=pd.read_ CSV('../input/submission/submissions_v2.csv').
===========================================================================================
Cleaned documentation: i=0. for index,row in test_df.iterrows():. print(row['selected_text']). if len(row['selected_text'])>100:. print(row['selected_text']). test_df.at[index,'selected_text']=''. i=i+1. print(i)
Processed documentation: i=0. for index,row in test_df.iterrows():. print(row['selected_text']). if len(row('selected_ text'')>100:. print (row(' selected_text
===========================================================================================
Cleaned documentation: import os. os.getcwd(). import shutil. source='/kaggle/test1.csv'. destination='/kaggle/working/test1.csv'. dest = shutil.copyfile(source, destination)
Processed documentation: import os.getcwd(). import shutil.copyfile(source, destination) source = 'kaggle/test1.csv' destination ='kaggle-working/ test1. CSV' dest = '
===========================================================================================
Cleaned documentation: import os. for subdir, dirs, files in os.walk('../input/model4/'):. for file in files:. print(file) file
Processed documentation: import os.walk('../input/model4/'):. for file in files:. print(file)
===========================================================================================
Cleaned documentation: x = torch.randn((2,64,300))print(x.shape)x = x.permute(0, 2, 1)print(x.shape)x = x.permute(0, 2, 1)attention = Attention(300,64)attention(x)attention
Processed documentation: x = torch.randn((2,64,300) x = x.permute(0, 2, 1)attention = Attention(300,64)
===========================================================================================
Cleaned documentation: Run KMeans on each image. Centroids provide dominant colors (based on provided colorspace). More details [here](
Processed documentation: Run KMeans on each image. Centroids provide dominant colors (based on provided colorspace)
===========================================================================================
Cleaned documentation: Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.. Memory optimization
Processed documentation: Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.
===========================================================================================
Cleaned documentation: Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.. Memory optimization
Processed documentation: Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.
===========================================================================================
Cleaned documentation: Convert UCL data to ASHREA format.. UCL data is by half hour so we have to sum per hour.
Processed documentation: Convert UCL data to ASHREA format. UCLData is by half hour so we have to sum per hour.
===========================================================================================
Cleaned documentation: Let's fix that problem by specifying a HP cat_features=[i1, i2, ... , in] (list of integers):
Processed documentation: Let's fix that problem by specifying a HP cat_features=[i1, i2,..., in] (list of integers):
===========================================================================================
Cleaned documentation: Frames at regular interval. Frames with stride interval. return RGB image, frame index tuple. Next batch?. Next batch available?
Processed documentation: Frames at regular interval. Frames with stride interval. return RGB image, frame index tuple.
===========================================================================================
Cleaned documentation: readers = ["CV2-CPU", "Decord-CPU","Decord-GPU", "DALI-GPU"]. Decord-GPU random crashes, DALI crashes after 100.
Processed documentation: readers = ["CV2-CPU", "Decord-CPU","Decord -GPU", "DALI-GPU", 'DALi-GPU']. Decord-GPU random crashes, DALI
===========================================================================================
Cleaned documentation: And reactivity_error,deg_error_Mg_pH10,deg_error_pH10,deg_error_Mg_50C,deg_error_50C,reactivity,deg_Mg_pH10,deg_pH10,deg_Mg_50C,deg_50C; these columns have length 68 as its measured on first 68 bases.
Processed documentation: And reactivity_error,deg_error_Mg_pH10,deg error pH 10,deg  error_mg_50C,deg #50C. These columns have length 68 as
===========================================================================================
Cleaned documentation: Preparing the data and the model I'm going for a very simple data loading and model here.
Processed documentation: Preparing the data and the model I'm going for a very simple data loading and model.
===========================================================================================
Cleaned documentation: Cluster Resolver for Google Cloud TPUs.. Connects to the given cluster.. Initialize the TPU devices.. TPU distribution strategy implementation.
Processed documentation: Cluster Resolver for Google Cloud TPUs. Connects to the given cluster. Initialize the TPU devices. TPU distribution strategy implementation.
===========================================================================================
Cleaned documentation: We can then also easily look at our training and validation datasets by calling `.train` or `.valid`
Processed documentation: We can then also easily look at our training and validation datasets by calling `train` or `valid`
===========================================================================================
Cleaned documentation: Content. Size of data. du -hs /kaggle/input/landmark-retrieval-2020/train/ 101 GB. du -hs /kaggle/input/landmark-retrieval-2020/test/ 0.07 GB. du -hs /kaggle/input/landmark-retrieval-2020/index/ 4.9 GB
Processed documentation: Content. Size of data. du -hs /kaggle/ input/landmark-retrieval-2020/train/ 101 GB. du --hs / kaggle. input/ landmark- retrieval
===========================================================================================
Cleaned documentation: Returns the confusion matrix between rater's ratings. The following 3 functions have been taken from Ben Hamner's github repository
Processed documentation: The following 3 functions have been taken from Ben Hamner's GitHub repository.
===========================================================================================
Cleaned documentation: optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4). train. backprop. record. eval. record
Processed documentation: Train.optimizer = torch.optim.SGD(net.parameters() lr=0.1, momentum= 0.9, weight_decay=1e-4). train. backprop. record.
===========================================================================================
Cleaned documentation: author__ : Najeeb Khan, Yasir Mir, Zafarullah Mahmood team__ : artificial_stuPiDity institution__ : Jamia Millia Islamia email__ : najeeb.khan96@gmail.com
Processed documentation: Author: Najeeb Khan, Yasir Mir, Zafarullah Mahmood.
===========================================================================================
Cleaned documentation: Kernel Density Estimation plots : Gives the normal/gaussian distribution of data points conditioning on required features.[seaborn-kdeplots](
Processed documentation: Kernel Density Estimation plots : Gives the normal/gaussian distribution of data points conditioning on required features.
===========================================================================================
Cleaned documentation: dfx = pd.read_csv(config.TRAINING_FILE). dfx['text']=dfx['text'].apply(lambda x: x.strip()). dfx=dfx[dfx['textID']=="77e645b46c"]. dfx. dfx[dfx['selected_text']=='found any decently priced breakfast yet? i hope you do']['selected_text']
Processed documentation: dfx = pd.read_csv(config.TRAINING_FILE). dfx['text']=dfx['text'].apply(lambda x: x.strip()). dFX=dfx[dfx
===========================================================================================
Cleaned documentation: Solutions The following are hand-crafted solutions for some tasks viable by this DSL as demostrations for its capability.
Processed documentation: Solutions The following are hand-crafted solutions for some tasks viable by this DSL.
===========================================================================================
Cleaned documentation: pgm = mfmap[(split >> ffilter[has_color[5]] >> first >> inot >> split_conn), ident] \. debug_solution('training', 5, pgm_tr5)
Processed documentation: pgm = mfmap[(split >> ffilter[has_color[5], first >> inot >> split_conn), ident] \. debug_solution('training', 5, pgm_tr5)
===========================================================================================
Cleaned documentation: pgm = mfmap[(split >> ffilter[has_color[5]] >> first >> inot >> split_conn), ident] \. debug_solution('training', 25, pgm)
Processed documentation: pgm = mfmap[(split >> ffilter[has_color[5], first >> inot >> split_conn), ident] \. debug_solution('training', 25, pgm)
===========================================================================================
Cleaned documentation: Trivia: There is a huge imbalance between targets 0 (81.3%) and 1 (18.7%)
Processed documentation: There is a huge imbalance between targets 0 (81.3%) and 1 (18.7%)
===========================================================================================
Cleaned documentation: Columns to drop because there is no variation in training set. Removing duplicates. Taken from:
Processed documentation: Columns to drop because there is no variation in training set. Removing duplicates.
===========================================================================================
Cleaned documentation: visualize the data using bokeh. output_file("top_artists.html", title="top artists"). TOOLS = "pan,wheel_zoom,box_zoom,reset,hover,previewsave". draw circles
Processed documentation: visualize the data using bokeh. output_file("top_artists.html", title="top artists"). TOOLS = "pan,wheel_zoom,box_z Zoom,reset,hover,pre
===========================================================================================
Cleaned documentation: It looks like EBRYY.OB is an index, lets have a list assets with 0 returnsOpenPrevRaw1
Processed documentation: It looks like EBRYY.OB is an index, lets have a list assets with 0 returns.
===========================================================================================
Cleaned documentation: best_split = clf.best_estimator_.min_samples_split. print(best_split). this code is copied from here:. this code is copied from here:
Processed documentation: Best_split = clf.best_estimator_.min_samples_split. print(best_split). this code is copied from here:.
===========================================================================================
Cleaned documentation: corr = train.corr(). corr.head(403). Correlation with output variable. cor_target = abs(corr["isFraud"]). Selecting highly correlated features. relevant_features = cor_target[cor_target>0.05]. relevant_features
Processed documentation: corr = train.corr(). corr.head(403). Correlation with output variable. cor_target = abs(corr["isFraud"]). Selecting highly correlated features. relevant_features = cor_
===========================================================================================
Cleaned documentation: non_nan = ["C1", "C2", "C3", "C4", "C5", "C6", "C7", "C8", "C9", "C10", "C11", "C12", "C13", "C14",\. D1", "M_na"]
Processed documentation: non_nan = ["C1, "C2", "C3," "C4", "M_na", "D1", "N_na"] Non_nan is a type of non-nonsense language.
===========================================================================================
Cleaned documentation: X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1). y = train.sort_values('TransactionDT')['isFraud']. test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)
Processed documentation: X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT','transactionID'], axis=1). y =Train.sort _values(' transaction_id') and test = train
===========================================================================================
Cleaned documentation: Training and Validation Set. from sklearn.model_selection import train_test_split. X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.20, random_state=23)
Processed documentation: Training and Validation Set. X_train, X_valid, y_train,. y_valid = train_test_split(train, target, test_size=0.20, random_state=23)
===========================================================================================
Cleaned documentation: megadetector_results_df was done the following operation in the previous cell.. megadetector_results_df = megadetector_results_df[megadetector_results_df['detected_num'] < 2]. ax.set(ylim=(0,80000))
Processed documentation: megadetector_results_df was done the following operation in the previous cell.
===========================================================================================
Cleaned documentation: show_df.sample(frac=1).reset_index(drop=True). dataset_mean = [152.86581, 78.63777, 20.671019]. dataset_std = [43.88956 25.214554 18.860243]. y_pred = predictions raw
Processed documentation: show_df.sample(frac=1).reset_index(drop=True). dataset_mean = [152.86581, 78.63777, 20.671019]. dataset_std = [43
===========================================================================================
Cleaned documentation: Bedrooms graphs. Number of occurrences. Average number of Bedrooms per Interest Level. Average interest for every number of bedrooms
Processed documentation: Bedrooms graphs. Number of occurrences. Average number of Bedrooms per Interest Level.
===========================================================================================
Cleaned documentation: Any violet pixels. Actually, in grayscale images (fluorescent and brightfield) each pixel has equal values for 3 channels.
Processed documentation: Any violet pixels. In grayscale images (fluorescent and brightfield) each pixel has equal values for 3 channels.
===========================================================================================
Cleaned documentation: if not os.path.isdir(clusters_path):. os.mkdir(clusters_path). for type_name in type_names.values():. if not os.path.isdir(os.path.join(clusters_path, type_name)):. os.mkdir(os.path.join(clusters_path, type_name))
Processed documentation: if not os.path.isdir(clusters_path):. os.mkdir(cl clusters_path). for type_name in type_names.values():. if not os-path.join(cluster
===========================================================================================
Cleaned documentation: word_vectorizer.fit(all_text). train_word_features = word_vectorizer.transform(train_x). val_word_features = word_vectorizer.transform(test_x). char_vectorizer.fit(all_text). train_char_features = char_vectorizer.transform(train_x). val_char_features = char_vectorizer.transform(test_x)
Processed documentation: word_vectorizer.fit(all_text). train_word_features = word_ vectorizer.transform(train_x). val_word-features =word_ vectorsizer. transform(test_x) char_
===========================================================================================
Cleaned documentation: submit_df = pd.DataFrame({"qid": test.index, "prediction": model.predict(testing).astype(int)}). submit_df.to_csv("submission.csv", index=False). print(submit_df.head()). del submit_df
Processed documentation: submit_df = pd.DataFrame({"qid": test.index, "prediction": model.predict(testing).astype(int) }); submit_df.to_csv("submission.csv
===========================================================================================
Cleaned documentation: Iterative Tuning: My current learning rate is 0.005. Say I what to see how well it's neigbors perform..
Processed documentation: My current learning rate is 0.005. Say I what to see how well it's neigbors perform.
===========================================================================================
Cleaned documentation: More Missingno.. In the heamap, red means strong negative correlation (-1), and blue signifies positive correlation.
Processed documentation: In the heamap, red means strong negative correlation (-1), and blue signifies positive correlation.
===========================================================================================
Cleaned documentation: This dendrogram clusters the missing occurence tendencies together. EXT_SOURCE_1 and OWN_CAR_AGE are close. EXT_SOURCE_3 and AMT_REQ_CREDIT bundle aswell.
Processed documentation: This dendrogram clusters the missing occurence tendencies together. EXT_SOURCE_1 and OWN_CAR_AGE are close.
===========================================================================================
Cleaned documentation: Iterative Tuning: My current learning rate is 0.05. Say I what to see how well it's neigbors perform..
Processed documentation: My current learning rate is 0.05. Say I what to see how well it's neigbors perform.
===========================================================================================
Cleaned documentation: Take Mean over Seed prediction. Output position with highest probability. Submit. Save Results..
Processed documentation: Take Mean over Seed prediction. Output position with highest probability. Submit.
===========================================================================================
Cleaned documentation: Let's visualize the embeddings by projecting embeddings in 2-dimensional space using singular value decomposition.
Processed documentation: Let's visualize the embeddings by projecting them in 2-dimensional space using singular value decomposition.
===========================================================================================
Cleaned documentation: Submission Here's your submission csv that should get you a LB score around .897 !
Processed documentation: Submission Here's your submission csv that should get you a LB score around.897!
===========================================================================================
Cleaned documentation: def remove_stopwords(x):. x = str(x). return ' '.join([w for w in x.split(' ') if w not in sw])
Processed documentation: def remove_stopwords(x):. x = str(x) return''.join([w for w in x.split(' ') if w not in sw])
===========================================================================================
Cleaned documentation: time. dprint("Prepared Model....."). cat_model = cat_model(). cat_model.fit(X, y). dprint("Done Model....."). dprint("Predict Test Value....."). target_cat = cat_model.predict(test).astype("int64"). dprint("Done Prediction.....")
Processed documentation: time.print("Prepared Model....."). cat_model.fit(X, y). dprint("Done Model...."). target_cat = cat.predict(test).astype("int64"). dprint
===========================================================================================
Cleaned documentation: more feature extraction with mean, mode, std, variance, min, max and so on...
Processed documentation: more feature extraction with mean, mode, std, variance, min, max and so on.
===========================================================================================
Cleaned documentation: from sklearn.model_selection import train_test_split. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y). X_train.shape,y_train.shape,X_test.shape
Processed documentation: from sklearn.model_selection import train_test_split. X_train, X_test, y_train,. y_test = train_ test_split(X, y, test_size=0.20
===========================================================================================
Cleaned documentation: time. dprint("Prepared Model....."). cat_model = cat_model(). cat_model.fit(X, y). dprint("Done Model....."). dprint("Predict Test Value....."). target_cat = cat_model.predict(test).astype("int64"). dprint("Done Prediction.....")
Processed documentation: time.print("Prepared Model....."). cat_model.fit(X, y). dprint("Done Model...."). target_cat = cat.predict(test).astype("int64"). dprint
===========================================================================================
Cleaned documentation: Stay connected for more parameter tunning updates. Glad to hear Comment from you guys... 🙂 Happy Kaggling
Processed documentation: Stay connected for more parameter tunning updates. Glad to hear Comment from you guys...
===========================================================================================
Cleaned documentation: more feature extraction with mean, mode, std, variance, min, max and so on...
Processed documentation: more feature extraction with mean, mode, std, variance, min, max and so on.
===========================================================================================
Cleaned documentation: Concatenate the new features (free energy) to the training and testing dataframes ...
Processed documentation: Concatenate the new features (free energy) to the training and testing dataframes.
===========================================================================================
Cleaned documentation: Load test image filenames (since we're using os.listdir(), these already have .jpg)
Processed documentation: Load test image filenames (since we're using os.listdir() these already have.jpg)
===========================================================================================
Cleaned documentation: HERE IS THE CORRECT WAY TO USE CUML KNN. model = KNeighborsClassifier(n_neighbors=KNN). model.fit(X_train,y_train). y_hat = model.predict(X_test). test_pred[test.group==g][1:-1] = y_hat
Processed documentation: HERE IS THE CORRECT WAY TO USE CUML KNN. model = KNeighborsClassifier(n_neighbors=KNN). model.fit(X_train,y_train). y_hat =
===========================================================================================
Cleaned documentation: from sklearn.metrics import classification_report,confusion_matrix. print(classification_report(test,test_pred)). print('\n'). print(confusion_matrix(test,test_pred)). from sklearn.metrics import f1_score
Processed documentation: from sklearn.metrics. import classification_report,confusion_matrix. print(classification_report(test,test_pred), print('\n'). print(confusion matrix( test,test
===========================================================================================
Cleaned documentation: Correlation matrix (heatmap) Correlation requires continuous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary values
Processed documentation: Correlation matrix (heatmap) Correlation requires continuous data. ignore Wilderness_Area and Soil_Type as they are binary values.
===========================================================================================
Cleaned documentation: max_features': 0.3, 'n_estimators': 100, 'max_depth': 15, 'min_samples_leaf: 1'. yy_pred = etc.predict(X_test)
Processed documentation: max_features': 0.3, 'n_estimators': 100,'max_depth': 15,'min_samples_leaf: 1' yy_pred = etc.predict(X_test
===========================================================================================
Cleaned documentation: img = cv2.imread('../input/pku-autonomous-driving/test_images/ID_23495df5e.jpg'). img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB). fig = plt.figure(figsize=(8,8)). plt.imshow(img)
Processed documentation: img = cv2.imread('../input/pku-autonomous-driving/test_images/ID_23495df5e.jpg') fig = plt.figure(figsize=(8,8
===========================================================================================
Cleaned documentation: fig, (axis1) = plt.subplots(1,1,figsize=(8,3)). sns.countplot(x = 'Open', hue = 'DayOfWeek', data = data_train,)
Processed documentation: Fig.fig, (axis1) = plt.subplots(1,1,figsize=(8,3), sns.countplot(x = 'Open', hue = 'DayOfWeek', data
===========================================================================================
Cleaned documentation: fig, (axis1, axis2, axis3) = plt.subplots(1, 3, figsize=(12,3)). sns.barplot(average_store_type.index, average_store_type['Sales'], ax=axis1). sns.barplot(average_store_type.index, average_store_type['Customers'], ax=axis2). sns.barplot(average_store_type.index, average_store_type['CompetitionDistance'], ax=axis3)
Processed documentation: fig, (axis1, axis2, axis3) = plt.subplots(1, 3, figsize=(12,3), sns.barplot(average_store_type.index, average_
===========================================================================================
Cleaned documentation: Plots with a distribution for 5 top features before removing an outliers and after removing them.
Processed documentation: Plots with a distribution for 5 top features before removing outliers and after removing them.
===========================================================================================
Cleaned documentation: OsBuild': 'int16',. OsPlatformSubRelease': 'category',. IsProtected': 'float16',. Census_OSArchitecture': 'category',. Census_OSSkuName': 'category',. Census_OSUILocaleIdentifier': 'int16',
Processed documentation: OsBuild': 'int16',. OsPlatformSubRelease': 'category',. IsProtected': 'float16,'. Census_OSArchitecture':'category',. Census-OSSkuName: 'category
===========================================================================================
Cleaned documentation: Predicting on validation set. Predicting on validation set once again to obtain data for OOF. Predicting on test set
Processed documentation: Predicting on validation set once again to obtain data for OOF.
===========================================================================================
Cleaned documentation: Scale target variable to log.. Split training examples into train/dev examples.. Calculate number of train/dev/test examples.
Processed documentation: Scale target variable to log. Split training examples into train/dev examples. Calculate number of train/Dev/test examples.
===========================================================================================
Cleaned documentation: The above basically just fixes up the pixel representation attribute of the DICOM file for us to use.
Processed documentation: The above basically just fixes up the pixel representation attribute of the DICOM file.
===========================================================================================
Cleaned documentation: Modified version of the SED work by Hidehisa Arai.. x: (n_samples, n_in, n_time)
Processed documentation: Modified version of the SED work by Hidehisa Arai.
===========================================================================================
Cleaned documentation: Is supports backbones "resnet18", "resnet34","resnet50", "resnet101", "resnet152", "resnext50_32x4d", "resnext101_32x8d", "wide_resnet50_2", "wide_resnet101_2", as resnets with fpn backbones.
Processed documentation: Is supports backbones "resnet18", " resnet34","resnet50", "Resnet101", " Resnet152", "resnext50_32x4d" and "Resnext101_ 32x
===========================================================================================
Cleaned documentation: Is supports backbones "resnet18", "resnet34","resnet50", "resnet101", "resnet152", "resnext50_32x4d", "resnext101_32x8d", "wide_resnet50_2", "wide_resnet101_2", as resnets with fpn backbones.
Processed documentation: Is supports backbones "resnet18", " resnet34","resnet50", "Resnet101", " Resnet152", "resnext50_32x4d" and "Resnext101_ 32x
===========================================================================================
Cleaned documentation: contiune with Date. Plot average sales & customers for every year. Drop Date column. rossmann_df.drop(['Date'], axis=1,inplace=True). test_df.drop(['Date'], axis=1,inplace=True)
Processed documentation: contiune with Date. Drop Date column. Plot average sales & customers for every year.
===========================================================================================
Cleaned documentation: country_destination. test_df['booked'] = (test_df['country_destination'] != 'NDF').astype(int). Plot the frequency for every country_destination value
Processed documentation: country_destination.booked = (test_df['booked']!= 'NDF').astype(int). Plot the frequency for every country_destinations value.
===========================================================================================
Cleaned documentation: first_affiliate_tracked. fill NaN values randomly. drop columns. airbnb_df.drop(['first_affiliate_tracked'], axis=1,inplace=True). test_df.drop(['first_affiliate_tracked'], axis=1,inplace=True)
Processed documentation: first_affiliate_tracked. fill NaN values randomly. drop columns. airbnb_df.drop(['first.affiliate-tracked'], axis=1,inplace=True). test_df
===========================================================================================
Cleaned documentation: Random Forests. random_forest = RandomForestClassifier(n_estimators=100). random_forest.fit(X_train, Y_train). Y_pred = random_forest.predict(X_test). random_forest.score(X_train, Y_train)
Processed documentation: random_forest = RandomForestClassifier(n_estimators=100). random_forest.fit(X_train, Y_train). Y_pred = random_ forest.predict( X_test). random
===========================================================================================
Cleaned documentation: Random Forests. random_forest = RandomForestClassifier(n_estimators=100). random_forest.fit(X_train, Y_train). Y_pred = random_forest.predict_proba(X_test)[:,1]. random_forest.score(X_train, Y_train)
Processed documentation: Random Forests. random_forest = RandomForestClassifier(n_estimators=100). random_ forest.fit(X_train, Y_train). Y_pred = random_Forest.predict_pro
===========================================================================================
Cleaned documentation: get Coefficient of Determination(R^2) for each feature using Logistic Regression. preview
Processed documentation: get Coefficient of Determination(R^2) for each feature using Logistic Regression.
===========================================================================================
Cleaned documentation: IMPORTANT! - Another Method for Hotel Cluster Prediction. Expedia Hotel Cluster Predictions. Link:
Processed documentation: IMPORTANT! - Another Method for Hotel Cluster Prediction. Expedia Hotel Cluster Predictions.
===========================================================================================
Cleaned documentation: month. Which month has higher number of customers purchased insurance plan. Plot
Processed documentation: month. Which month has higher number of customers purchased insurance plan.
===========================================================================================
Cleaned documentation: Random Forests. random_forest = RandomForestClassifier(n_estimators=100). random_forest.fit(X_train, Y_train). Y_pred2 = random_forest.predict(X_test). random_forest.score(X_train, Y_train)
Processed documentation: random_forest = RandomForestClassifier(n_estimators=100). random_forest.fit(X_train, Y_train). Y_pred2 = random_ forest.predict(X), random_
===========================================================================================
Cleaned documentation: This big function below will make the training data suitable in the form of image.png](attachment:image.png)
Processed documentation: This big function below will make the training data suitable in the form of image.
===========================================================================================
Cleaned documentation: Multiple LSTM layers We can Enhance the net by using multiple LSTM layers. LSTMnet2.JPG](attachment:LSTMnet2.JPG)
Processed documentation: Multiple LSTM layers. We can Enhance the net by using multiple LSTm layers. L STMnet2.JPG.
===========================================================================================
Cleaned documentation: Preventing Overfitting: To prevent overfitting, we need to create a function that can randomly change image characterisitics during fitting.
Processed documentation: To prevent overfitting, we need to create a function that can randomly change image characterisitics during fitting.
===========================================================================================
Cleaned documentation: Defining the Convolutional Neural Network: This model has 4 convolution layers. This model has 3 fully connected layers.
Processed documentation: This model has 4 convolution layers. This model has 3 fully connected layers.
===========================================================================================
Cleaned documentation: Let's plot some locations on the map with a very cool `folium` module, to get some ideas.
Processed documentation: Let's plot some locations on the map with a very cool `folium` module.
===========================================================================================
Cleaned documentation: PCA+QDA+NuSVC+KNN [0.96774] Thanks to kernels: Another model for your blending]( PCA+NuSVC+KNN](
Processed documentation: PCA+QDA+NuSVC+KNN [0.96774] Thanks to kernels: Another model for your blending.
===========================================================================================
Cleaned documentation: dept_group grouped_state_sales.columns.get_level_values(1).unique(). tmp_grouped_state_sales = tmp_grouped_state_sales.div(tmp_grouped_state_sales.min()). tmp_grouped_state_sales -=1. tmp_grouped_state_sales = tmp_grouped_state_sales.groupby(level=[0,1]).diff(axis=0). tmp_grouped_state_sales = tmp_grouped_state_sales.droplevel(2). tmp_grouped_state_sales = tmp_grouped_state_sales.iloc[[1,3,5,7]]. tmp_grouped_state_sales.index.set_levels(["","diff SNAP/non-SNAP"],level=2,inplace=True)
Processed documentation: dept_group grouped_state_sales.columns.get_level_values(1).unique() tmp_grouped_state.sales = tmp_Grouped_State_sale.div(tmp
===========================================================================================
Cleaned documentation: cat_names = ["", 'National', 'Religious', 'Cultural', 'Sporting']. cmap = colors.ListedColormap(['gainsboro', 'red','blue','yellow','green'])
Processed documentation: cat_names = ["", 'National', 'Religious', 'Cultural', 'Sporting']. cmap = colors.ListedColormap(['gainsboro','red','blue','yellow','green'
===========================================================================================
Cleaned documentation: store config in yapl global variable. Detect hardware, return appropriate distribution strategy. configuration. yapl.config.TOTAL_TRAIN_IMG = len(pd.read_csv(yapl.config.TRAIN_CSV).image_name)
Processed documentation: store config in yapl global variable. Detect hardware, return appropriate distribution strategy. configuration. yapl.config.TOTAL_TRAIN_IMG = len(pd.read_csv(yapl. config.T
===========================================================================================
Cleaned documentation: INTRODUCTION In this kernel, we will be working on Humpback Whale Identification Dataset (Implementing with Keras).
Processed documentation: In this kernel, we will be working on Humpback Whale Identification Dataset (Implementing with Keras).
===========================================================================================
Cleaned documentation: Generate indexes of the batch. Find list of IDs. Initialization. Generate data. Store samples
Processed documentation: Generate indexes of the batch. Find list of IDs. Generate data. Store samples.
===========================================================================================
Cleaned documentation: Initialize. Load model. Get names and colors. Run inference. Inference. Process detections. xywh[2] = xywh[2]-xywh[0]. xywh[3] = xywh[3]-xywh[1]
Processed documentation: Initialize. Load model. Get names and colors. Run inference. Process detections.
===========================================================================================
Cleaned documentation: tr2 = bar_hor(app_train, "NAME_INCOME_TYPE", "Distribution of NAME_FAMILY_STATUS" ,"a4c5f9", w=700, lm=100, return_trace = True). fig.append_trace(tr2, 1, 2);
Processed documentation: tr2 = bar_hor(app_train, "NAME_INCOME_TYPE", "Distribution of NAME_FAMILY_STATUS","a4c5f9", w=700, lm
===========================================================================================
Cleaned documentation: A, 予測 = (A: 0.1, B: 0.7, 0.2)の場合。 A: 3, B: 1, C:2) Score = 1/3 = 0.33 となる。
Processed documentation: A, 予測 = (A: 3, B: 1, C:2) Score = 1/3 = 0.33. A,  B: 0.7, C:
===========================================================================================
Cleaned documentation: PCA - Principal component analysis. For the sake of simplicity, do a 2-dimensional PCA. That makes plotting simpler.
Processed documentation: PCA - Principal component analysis. For the sake of simplicity, do a 2-dimensional PCA.
===========================================================================================
Cleaned documentation: my_submission = pd.DataFrame({'ID_code': test_IDs, 'target': cv_results_df.iloc[0].predictions_total}) you could use any filename. We choose submission heremy_submission.to_csv('submission.csv', index=False)
Processed documentation: my_submission = pd.DataFrame({'ID_code': test_IDs, 'target': cv_results_df.iloc[0].predictions_total}) you could use any filename. We
===========================================================================================
Cleaned documentation: Find all correlations and sort. Print the most negative correlations. Print the most positive correlations
Processed documentation: Find all correlations and sort. Print the most negative correlations and the most positive correlations.
===========================================================================================
Cleaned documentation: Let's apply the power of H2O AutoML to the ["Flight delays" competition]( (it's a binary classification task) from [mlcourse.ai](
Processed documentation: Let's apply the power of H2O AutoML to the ["Flight delays" competition]
===========================================================================================
Cleaned documentation: Let's compare the result of the model `XGBoost_1_AutoML_20190417_212831` with that of the CatBoostRegressor with the default parameters.
Processed documentation: Let's compare the result of the model with that of the CatBoostRegressor with the default parameters.
===========================================================================================
Cleaned documentation: for blending if necessary.. ovr.predict(test_vectorized) + svc.predict(test_vectorized) + np.round(np.argmax(pred, axis=1)).astype(int)) / 3
Processed documentation: for blending if necessary.. ovr.predict(test_ vectorized) + svc.p predict( test_vectorized), np.round(np.argmax(pred, axis=1),astype(
===========================================================================================
Cleaned documentation: Data Section 1- The Basics This includes the details about the Team, Seasons, Seeds Information,Game Results. The Team
Processed documentation: Data Section 1- The Basics This includes the details about the Team, Seasons, Seeds Information,Game Results.
===========================================================================================
Cleaned documentation: This is a better representation. About 40 percent of the tweets are neutral followed by positive and negative tweets.
Processed documentation: This is a better representation. About 40 percent of the tweets are neutral.
===========================================================================================
Cleaned documentation: Wordclouds Let's create wordclouds to see which words contribute to which type of polarity.
Processed documentation: Wordclouds let you see which words contribute to which type of polarity.
===========================================================================================
Cleaned documentation: Loading an audio file I'll be using a Python Library called Librosa for analysing the audio file.
Processed documentation: I'll be using a Python Library called Librosa for analysing the audio file.
===========================================================================================
Cleaned documentation: Adjust the size of your images. Iterate and plot random images. Adjust subplot parameters to give specified padding
Processed documentation: Adjust the size of your images. Adjust subplot parameters to give specified padding.
===========================================================================================
Cleaned documentation: Importing the necessary libraries Incase you fork the notebook, make sure to keep the Internet in `ON` mode.
Processed documentation: Make sure to keep the Internet in `ON' mode.Importing the necessary libraries.
===========================================================================================
Cleaned documentation: Gaussian Blur with opencv The Gaussian filter is a low-pass filter that removes the high-frequency components are reduced.
Processed documentation: Gaussian filter is a low-pass filter that removes the high-frequency components are reduced.
===========================================================================================
Cleaned documentation: fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(20,12)). ax[0].imshow(image). ax[0].set_title("Original Image", size=20). ax[1].imshow(brightened_image). ax[1].set_title("Brightened Image", size=20). ax[2].imshow(darkened_image). ax[2].set_title("darkened_image", size=20)
Processed documentation: fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(20,12), ax[0].imshow(image). ax [0].set_title("Original
===========================================================================================
Cleaned documentation: Visitor Profile Lets create the visitor profile by aggregating the rows for every customer.
Processed documentation: Visitor Profile Lets create a visitor profile by aggregating the rows for every customer.
===========================================================================================
Cleaned documentation: num_leaves': 94,. min_data_in_leaf': 61,. max_depth': 31,. bagging_fraction' : 0.12033530139527615,. feature_fraction' : 0.18631314159464357,. lambda_l1': 0.0628583713734685,. lambda_l2': 1.2728208225275608
Processed documentation: num_leaves': 94,. min_data_in_leaf': 61,. max_depth': 31,. bagging_fraction' : 0.12033530139527615,. feature_fractions' :
===========================================================================================
Cleaned documentation: Creates a day of the week feature, encoded as 0-6.. From
Processed documentation: Creates a day of the week feature, encoded as 0-6.
===========================================================================================
Cleaned documentation: num_leaves': study.best_trial.params['num_leaves'],. min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],. min_child_weight': study.best_trial.params['min_child_weight'],. max_depth': study.best_trial.params['max_depth'],. bagging_fraction' : study.best_trial.params['bagging_fraction'],. feature_fraction' : study.best_trial.params['feature_fraction'],. lambda_l1': study.best_trial.params['lambda_l1'],. lambda_l2': study.best_trial.params['lambda_l2']
Processed documentation: num_leaves': study.best_trial.params['num_Leaves'],. min_data_in_leaf: study. Best Trial's min_ data_in-leaf, min_child_weight:
===========================================================================================
Cleaned documentation: Some points are far away from the boundaries of NYC. Let's limit the scope for visualization purposes, at least.
Processed documentation: Some points are far away from the boundaries of NYC. Let's limit the scope for visualization purposes.
===========================================================================================
Cleaned documentation: Feature Importances Time for some modeling! I'll make use of sklearn in a first step to visualize feature importances.
Processed documentation: Feature Importances Time for some modeling! I'll make use of sklearn to visualize feature importances.
===========================================================================================
Cleaned documentation: Submission It will be necessary apply all the transformations and feature engineering to the test data set.
Processed documentation: It will be necessary apply all the transformations and feature engineering to the test data set.
===========================================================================================
Cleaned documentation: Bengali.AI Quick data exploration In the next couple of days, I'll continue to explore the BengaliAI dataset, stay tuned.
Processed documentation: I'll continue to explore the BengaliAI dataset, stay tuned.
===========================================================================================
Cleaned documentation: Handwritten variants of the most common `grapheme_root` The samples below are the handwritten pairs of the digital ones above.
Processed documentation: Handwritten variants of the most common `grapheme_root`
===========================================================================================
Cleaned documentation: Calculate overlap area. Calculate union area. Already matched GT-box. tp. Showing GT boxes nearby. noisy targets. fn
Processed documentation: Calculate overlap area. Calculate union area. Already matched GT-box. Showing GT boxes nearby.
===========================================================================================
Cleaned documentation: Benchmark This is a very simple benchmark to find out how many `new_whales` are in the test set.
Processed documentation: This is a very simple benchmark to find out how many `new_whales` are in the test set.
===========================================================================================
Cleaned documentation: TIP \4: 🥇 Cleanup after usage Remove every variable once you don't need it anymore.
Processed documentation: TIP \4: Cleanup after usage Remove every variable once you don't need it anymore.
===========================================================================================
Cleaned documentation: For debugging.. train_df = train_df.sample(n=100). train_df.reset_index(drop=True, inplace=True). Need for the StratifiedKFold split
Processed documentation: Train_df = train_df.sample(n=100). train_DF.reset_index(drop=True, inplace=True). Need for the StratifiedKFold split.
===========================================================================================
Cleaned documentation: print(' Epoch {}/{}'.format(epoch + 1, N_EPOCHS)). print(' ' + ('-' 20)). Validate
Processed documentation: print(' Epoch {}/{}'.format(epoch + 1, N_EPOCHS). print(''+ ('-' 20). Validate
===========================================================================================
Cleaned documentation: Params. My weights dataset for this compeititon; feel free to vote the dataste ;)
Processed documentation: My weights dataset for this compeititon; feel free to vote the dataste ;)
===========================================================================================
Cleaned documentation: Ok, that's cool, but let's go a bit deeper! (Scroll down to see.)
Processed documentation: Ok, that's cool, but let's go a bit deeper! (Scroll to see.)
===========================================================================================
Cleaned documentation: I don't know why I need to fill NA a second time, but alas here we are...
Processed documentation: I don't know why I need to fill NA a second time, but alas here we are.
===========================================================================================
Cleaned documentation: Preprocess. You should keep this. Replace this to your weight dataset. Demo model's weights
Processed documentation: You should keep this. Replace this to your weight dataset. Demo model's weights.
===========================================================================================
Cleaned documentation: We now have a model with a CV score of 0.8032. Nice! Let's submit that. Make submission
Processed documentation: We now have a model with a CV score of 0.8032. Let's submit that.
===========================================================================================
Cleaned documentation: Thống kê số lượng các class trong mỗi nhóm đối với dữ liệu dạng object.
Processed documentation: Thống kê sồ lợng các class trong mỗi nhóm đỉnh với d�
===========================================================================================
Cleaned documentation: Vẽ biểu đồ tỷ lệ Repaid/Not Repaid theo các biến dự báo dạng object.
Processed documentation: Vẽ biểu đồ tỷ lệ Repaid/Not Repaid theo các biến dự báo d
===========================================================================================
Cleaned documentation: Thống kê các giá trị khác biệt trong toàn bộ các biến.
Processed documentation: Thống kê các giá trị khác biệt trong toàn bộ.
===========================================================================================
Cleaned documentation: Phân phối xác xuất của các biến trên theo các nhóm của biến mục tiêu.
Processed documentation: Phân phối xác xuất của các biến trên. Các nhóm công nhâ
===========================================================================================
Cleaned documentation: Biểu đồ tỷ lệ trả nợ các hợp đồng theo từng nhóm tuổi.
Processed documentation: Biểu đồ tỷ lệ trả nợ các hỢp ēng theo tính nhó
===========================================================================================
Cleaned documentation: Biểu diễn cả 2 đường ROC của hồi qui logistic và random forest trên cùng một đồ thị.
Processed documentation: Biểu diễn của hồi qui logistic và random forest trên cùng một đỉn
===========================================================================================
Cleaned documentation: metres from Kremlin. What is it? Putin's cordyard, Lenin's Mausoleum? Let's take a look on prices.
Processed documentation: metres from Kremlin. Putin's cordyard, Lenin's Mausoleum? Let's take a look at prices.
===========================================================================================
Cleaned documentation: param img: path to image. convert to 1d image. draw rectangle. draw = Draw(rimg). draw.rectangle(bbox, outline='red')
Processed documentation: param img: path to image. convert to 1d image. draw = Draw(rimg). draw.rectangle(bbox, outline='red')
===========================================================================================
Cleaned documentation: Load my data This data is loaded from Kaggle and automatically sharded to maximize parallelization.
Processed documentation: This data is loaded from Kaggle and automatically sharded to maximize parallelization.
===========================================================================================
Cleaned documentation: sub3 = np.mean([rankdata(sub1.isFraud) / len(sub1), rankdata(sub2.isFraud) / len(sub2)], axis=0). sub3 = 0.6rankdata(sub1.isFraud) / len(sub1) + 0.4rankdata(sub2.isFraud) / len(sub2)
Processed documentation: sub3 = np.mean([rankdata(sub1.isFraud) / len( sub1), rank data(sub2. isFraud), axis=0). sub3 = 0.6rank data(
===========================================================================================
Cleaned documentation: Computes and stores the average and current value. xm.master_print(f'{outputs.shape}'). xm.master_print(f'{targets.shape}'). break. break. if bi == 2:. break. model.eval()
Processed documentation: Computes and stores the average and current value. xm.master_print(f'{targets.shape}'). break. if bi == 2:. model.eval()
===========================================================================================
Cleaned documentation: loading dataset for epoch. del train_dataset. del train_sampler. del train_data_loader. using xm functionality for memory-reduced model saving. del o,t,i
Processed documentation: loading dataset for epoch. Using xm functionality for memory-reduced model saving. del train_dataset. delTrain_sampler. using train_data_loader.
===========================================================================================
Cleaned documentation: t-SNE with cervix indicators Now let's project the 100x100x3 images to three dimensions to check for low dimensional patterns.
Processed documentation: t-SNE with cervix indicators. Project the 100x100x3 images to three dimensions.
===========================================================================================
Cleaned documentation: Both seem to be very different kind of questions . Lets verify how many Mike's we have ?
Processed documentation: Both seem to be very different kind of questions. Lets verify how many Mike's we have?
===========================================================================================
Cleaned documentation: Lets see the words of first answer. Create and generate a word cloud image:. Display the generated image:
Processed documentation: Lets see the words of first answer. Create and generate a word cloud image.
===========================================================================================
Cleaned documentation: Load all questions in word cloud. Generate a word cloud image. Display the generated image:. the matplotlib way:
Processed documentation: Load all questions in word cloud. Generate a word cloud image. Display the generated image the matplotlib way.
===========================================================================================
Cleaned documentation: Lets do some unnecessary but nice masking. Generate a word cloud image. Display the generated image:. the matplotlib way:
Processed documentation: Lets do some unnecessary but nice masking. Generate a word cloud image. Display the generated image the matplotlib way.
===========================================================================================
Cleaned documentation: Generate a word cloud image. Display the generated image:. the matplotlib way:
Processed documentation: Generate a word cloud image. Display the generated image the matplotlib way.
===========================================================================================
Cleaned documentation: Add Augmentations as suited from Albumentations library. A lot of heavy augmentations
Processed documentation: Add Augmentations as suited from Albumentations library.
===========================================================================================
Cleaned documentation: A Small but useful test of the Model by using dummy input . .
Processed documentation: A Small but useful test of the Model by using dummy input.
===========================================================================================
Cleaned documentation: tags = 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'. data = 'd_1' -> 'd_1913'
Processed documentation: tags = 'id', 'item_id',  'dept_id'  'cat_id,''store-id' and'state-id', data = 'd_1'    'd
===========================================================================================
Cleaned documentation: Add Augmentations as suited from Albumentations library. A lot of heavy augmentations
Processed documentation: Add Augmentations as suited from Albumentations library.
===========================================================================================
Cleaned documentation: shake-shake network. compute conv feature size. self.fc = nn.Linear(self.feature_size, n_classes). vowel_diacritic. grapheme_root. consonant_diacritic. initialize weights
Processed documentation: shake-shake network. compute conv feature size. self.fc = nn.Linear(self.feature_size, n_classes). vowel_diacritic. grapheme_root. consonant_d
===========================================================================================
Cleaned documentation: A Small but useful test of the Model by using dummy input . .
Processed documentation: A Small but useful test of the Model by using dummy input.
===========================================================================================
Cleaned documentation: settings. getting data batch. calculating loss. settings. getting data batch. settings. log.info('lr = {}'.format(scheduler.get_lr())). getting data batch. calculating loss
Processed documentation: settings. log.info('lr = {}'format(scheduler.get_lr())). getting data batch. calculating loss. getting data batches. calculating losses.
===========================================================================================
Cleaned documentation: It will generate 12 faces from each set of videos. cap.set(cv2.CAP_PROP_FRAME_COUNT, frame_seq-1). Print first face. plt.imsave(f"{OUTPUT_PATH}{fn.split('.')[0]}-{label}-{str(i)}-{str(count)}.png",cropnpad(bounding_box,image,0),format='png')
Processed documentation: It will generate 12 faces from each set of videos. cap.set(cv2.CAP_PROP_FRAME_COUNT, frame_seq-1). Print first face. plt.imsave(f"
===========================================================================================
Cleaned documentation: Get the paths of all train videos. Get path of metadata.json. Get metadata
Processed documentation: Get the paths of all train videos. Get path of metadata.
===========================================================================================
Cleaned documentation: highlight matched region. ax3.set_title('`match_template`\nresult'). highlight matched region. ax3.autoscale(False). ax3.plot(x, y, 'o', markeredgecolor='r', markerfacecolor='none', markersize=10)
Processed documentation: highlight matched region. ax3.set_title('`match_template`\nresult'). highlight matchedRegion.autoscale(False). ax3-plot(x, y, 'o', markeredge
===========================================================================================
Cleaned documentation: maxVisitNumber = max(all_data['visitNumber']). fvid = all_data[all_data['visitNumber'] == maxVisitNumber]['fullVisitorId']. all_data[all_data['fullVisitorId'] == fvid.values[0]].sort_values(by='visitNumber')
Processed documentation: maxVisitNumber = max(all_data['visitNumber']). fvid = all_data[all_ data['visitedNumber'] == maxVisitNumber]. all_ data[all-data['fullVisitorId
===========================================================================================
Cleaned documentation: X_train = pd.concat([X_train, pd.get_dummies(X_train_gm, prefix='_gm')], axis=1). X_test = pd.concat([X_test, pd.get_dummies(X_test_gm, prefix='_gm')], axis=1)
Processed documentation: X_train = pd.get_dummies(X_ train_gm, prefix='_gm') X_test = p d.concat([X_test, pd ], axis=1) X_
===========================================================================================
Cleaned documentation: Number of optimal trees. You can test other values. Optimal trees. Train data split. Accuracy test data split
Processed documentation: Number of optimal trees. You can test other values. Train data split.
===========================================================================================
Cleaned documentation: confirm unique winPlace in group. nuniquePlace = train.groupby(['matchId','groupId'])['winPlacePerc'].nunique(). print('not unique winPlace in group:', len(nuniquePlace[nuniquePlace > 1])). del nuniquePlace
Processed documentation: confirm unique winPlace in group. nuniquePlace = train.groupby(['matchId','groupId'], ['winPlacePerc'].nunique). print('not unique win place in group:', len(
===========================================================================================
Cleaned documentation: print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],. index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True)[:10])
Processed documentation: T.sort_values('size', ascending=False).reset_index(drop=True)[:10] T.DataFrame.print(pd.Dataframe.get('name','size', index=['name','
===========================================================================================
Cleaned documentation: suicide: solo and teamKills > 0. all_data['_suicide'] = ((all_data['players'] == 1) & (all_data['teamKills'] > 0)).astype(int)
Processed documentation: suicide: solo and teamKills > 0. all_data['_suicide'] = ((all_ data['players'] == 1) & (all_data ["teamKills'] > 0)
===========================================================================================
Cleaned documentation: noneffectivefeats = ["var_{}".format(i) for i in range(200)]for f in feats: all_data[f] = pd.cut(all_data[f], 100, labels=range(100))
Processed documentation: noneffectivefeats = ["var_{}".format(i) for i in range(200)]for f in feats: all_data[f] = pd.cut(all_ data[f], 100,
===========================================================================================
Cleaned documentation: noneffectivefrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import f_classiffeat = SelectKBest(f_classif, k=50)feat.fit(X_train, Y_train)X_train = X_train.loc[:,feat.get_support()]X_test = X_test.loc[:,feat.get_support()]
Processed documentation: noneffectivefrom sklearn.feature_selection import SelectKBestfrom sklearning.feature.selection import f_classiffeat f_train = X_train.fit(X_train, Y_train) X
===========================================================================================
Cleaned documentation: Autocorrelogram & Partail Autocorrelogram is useful that to estimate each models parametaers.
Processed documentation: Autocorrelogram & Partail is useful that to estimate each models parametaers.
===========================================================================================
Cleaned documentation: This model's resid have few autocorrelation. It means that We were able to make a good model.
Processed documentation: This model's have few autocorrelation. It means that We were able to make a good model.
===========================================================================================
Cleaned documentation: ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: ime": "is_month_end",. ims: "is-month_start",. dt.drop( ["d", "wm_yr_wk", "weekday"], axis=1, inplace =
===========================================================================================
Cleaned documentation: List comprehension to make out list of lists. Plot it on the map. Display the map
Processed documentation: List comprehension to make out list of lists. Plot it on the map.
===========================================================================================
Cleaned documentation: Trains the model on data generated batch-by-batch by a Python generator. callbacks = get_callbacks(patience=2))
Processed documentation: Trains the model on data generated batch-by-batch by a Python generator.
===========================================================================================
Cleaned documentation: Quick exploration of Duke Team in 2018 Here's a quick overview of the Blue Devils performance in 2018.
Processed documentation: Quick exploration of Duke Team in 2018. Here's a quick overview of the Blue Devils performance.
===========================================================================================
Cleaned documentation: y_train = np.zeros(shape=(df.shape[0], 199)). for i,yard in enumerate(df['Yards'][::22]):. y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))
Processed documentation: y_train = np.zeros(shape=(df.shape[0], 199). for i,yard in enumerate(df['Yards'][::22]):. y_train[i, yard+99
===========================================================================================
Cleaned documentation: SelectPercentile Table of Contents](0.1) Select features according to a percentile of the highest scores. Source :
Processed documentation: SelectPercentile Table of Contents. Select features according to a percentile of the highest scores.
===========================================================================================
Cleaned documentation: Stdout of the training Table of Contents](0.1) Stdout stands for Standard Output in Python.
Processed documentation: Stdout stands for Standard Output in Python. Stdout of the training Table of Contents.
===========================================================================================
Cleaned documentation: Preview the data Table of Contents](0.1) Now, let's preview the data to gain more insights about the data.
Processed documentation: Preview the data to gain more insights about the data.
===========================================================================================
Cleaned documentation: Check for missing values Table of Contents](0.1) Now, let's check for missing values in training data.
Processed documentation: Check for missing values in training data. Now, let's check for missingvalues in data.
===========================================================================================
Cleaned documentation: Correlation Heatmap Table of Contents](0.1) Correlation Heatmap is used to visualize feature interactions in training set.
Processed documentation: Correlation Heatmap is used to visualize feature interactions in training set.
===========================================================================================
Cleaned documentation: Pair Plot Table of Contents](0.1) We will draw pairplot to visualize relationship between the target variables.
Processed documentation: Pair Plot Table of Contents. We will draw pairplot to visualize relationship between the target variables.
===========================================================================================
Cleaned documentation: from. T00:44:05.000Z'to '00:44:05.000Z' as time matters more than date. T00:44:05.000Z'to '00:44:05.000Z' as time matters more than date
Processed documentation: from. T00:44:05.000Z'to '00:45:00.000 Z' as time matters more than date.
===========================================================================================
Cleaned documentation: params = {'objective': 'regression', 'boosting': 'gbdt', 'metric': 'rmse', 'learning_rate': 0.01, 'seed' : SEEDS}
Processed documentation: params = {'objective':'regression', 'boosting': 'gbdt','metric': 'rmse', 'learning_rate': 0.01,'seed' : SEEDS}
===========================================================================================
Cleaned documentation: time. pred1 = kfold_lightgbm(y1). pred2 = kfold_lightgbm(y2). pred3 = kfold_lightgbm(y3). pred4 = kfold_lightgbm(y4). pred5 = kfold_lightgbm(y5). pred6 = kfold_lightgbm(y6)
Processed documentation: time. pred1 = kfold_lightgbm(y1). pred2 = k fold_lightGBM(y2), pred3 = k FoldLightGBM (y3), pred4 = k foldsLight
===========================================================================================
Cleaned documentation: we see that after 14-15 iterations , score does not improve . Time to think something else.
Processed documentation: we see that after 14-15 iterations, score does not improve. Time to think something else.
===========================================================================================
Cleaned documentation: File Structure The dataset has one file (`train.csv`) and three folders (`train`, `test`, `index`) in the root path.
Processed documentation: File Structure The dataset has one file (`train.csv`) and three folders (‘train, test, index’)
===========================================================================================
Cleaned documentation: Thanks for Yury Dzerin 's notebook . I have tried some ideas to further improve the score.
Processed documentation: Thanks for Yury Dzerin's notebook. I have tried some ideas to further improve the score.
===========================================================================================
Cleaned documentation: Please upvote this kernel if you like it. It motivates me to produce more quality content :)
Processed documentation: Please upvote this kernel if you like it. It motivates me to produce more quality content.
===========================================================================================
Cleaned documentation: Now we oversample the negative class. There is likely a much more elegant way to do this...
Processed documentation: Now we oversample the negative class. There is likely a much more elegant way to do this.
===========================================================================================
Cleaned documentation: Submission Next, we load the best weights and run model on the testing dataset.
Processed documentation: Next, we load the best weights and run model on the testing dataset.
===========================================================================================
Cleaned documentation: Generate a timemask like YYYYMMDD and YYYYMM to be able to groupby and fill the missing values.
Processed documentation: Generate a timemask like YYYYMMDD to be able to groupby and fill the missing values.
===========================================================================================
Cleaned documentation: Starting point. Let's see our missing values and fill the values for each site and mask YYYYMMDD
Processed documentation: Starting point. Let's see our missing values and fill the values for each site. mask YYYYMMDD
===========================================================================================
Cleaned documentation: if p_x > image.shape[1] or p_y > image.shape[0]:. print('Point', p_x, p_y, 'is out of image with shape', image.shape)
Processed documentation: if p_x > image.shape[1] or p_y < image. shape[0]:. print('Point', p_X, p_Y, 'is out of image with shape', image.
===========================================================================================
Cleaned documentation: You will also need functions from the previous cells. Get values. Math. Drawing
Processed documentation: You will also need functions from the previous cells. Get values.
===========================================================================================
Cleaned documentation: Now, all numpy arrays with model parameters are saved into eager Tensors in the `weights` list.
Processed documentation: All numpy arrays with model parameters are saved into eager Tensors in the `weights` list.
===========================================================================================
Cleaned documentation: Well, it seems ps_ind_06_bin, ps_ind_07_bin, ps_ind_08_bin, ps_ind_09_bin are already one hot encoded features of a categoricat feature.
Processed documentation: Well, it seems ps_ind_06_bin, ps ind_07_bin and ps_Ind_08_bin are already one hot encoded features of a categoricat.
===========================================================================================
Cleaned documentation: nice trick to filter string value in a column with different values. small_df = processed_df[processed_df["WindSpeed1"].apply(lambda x: type(x) == str)]["WindSpeed1"].value_counts()
Processed documentation: nice trick to filter string value in a column with different values. small_df = processed_df[processed_df ["WindSpeed1"].apply(lambda x: type(x) == str)].value_count
===========================================================================================
Cleaned documentation: pred = self.model.predict(self.validation_data[0]). auc = roc_auc_score(self.validation_data[1], pred). print ("Validation AUC: " + str(auc))
Processed documentation: pred = self.model.predict(self.validation_data[0], pred) auc = roc_auc_score( self.validated_ data[1], pred). print ("Validation AUC
===========================================================================================
Cleaned documentation: print((1 - lam) y[index_array]). print((lam y).shape,((1 - lam) y[index_array]).shape). print(X_batch.shape,y_batch.shape). print("{} iteration".format(i+1)). history= model.fit(X_train,Y_train,batch_size=512,epochs=500,verbose=1,callbacks=callbacks_list,validation_data=(X_val,Y_val))
Processed documentation: print((1 - lam) y[index_array). print((lam y).shape,((1 -- lam y[ index_array].shape). print(X_batch.shape,y_batch). print("{
===========================================================================================
Cleaned documentation: Try early stopping. from keras.callbacks import EarlyStopping. callback = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
Processed documentation: Try early stopping. from keras.callbacks import EarlySt stopping. callback = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0), mode='auto', baseline=
===========================================================================================
Cleaned documentation: trafficSource.keyword Transaction revenue must related to goods. The keyword may have relation to goods. continue to analyse
Processed documentation: Keyword Transaction revenue must related to goods. continue to analyse.
===========================================================================================
Cleaned documentation: param = {'num_leaves': 45.61216380347129,. max_depth': 11.578579827303919,. lambda_l2': 0.0107663924764632,. lambda_l1': 0.046541310399201855,. bagging_fraction': 0.7851516324443661,. feature_fraction': 0.7944881085591733,. min_child_samples': 28.5601473698899}. prediction,model = lgb_train(param)
Processed documentation: param = {'num_leaves': 45.61216380347129,. max_depth': 11.578579827303919,. lambda_l2': 0.0107663924764632,.
===========================================================================================
Cleaned documentation: random sampling. Negatives. Create a Rectangle patch. Positives. Create a Rectangle patch
Processed documentation: random sampling. Create a Rectangle patch. Positives. Negatives.
===========================================================================================
Cleaned documentation: seg_img[:,:,0] = np.ones([seed_image.shape[0], seed_image.shape[1]], dtype=np.uint8) - seg_img[:,:,1] - seg_img[:,:,2] - seg_img[:,:,3] - seg_img[:,:,4]
Processed documentation: seg_img[:,:,0] = np.ones([seed_image.shape[0], seed_ image. shape[1]], dtype=np.uint8) - seg_ img[:,
===========================================================================================
Cleaned documentation: time. means, stds = [], []. for batch in dbch.train_dl:. reshaped = batch[0].permute(1,0,2,3).reshape((3, -1)). means.append(reshaped.mean(1)), stds.append(reshaped.std(1)). torch.stack(means).mean(0). torch.stack(stds).mean(0)
Processed documentation: time. means, stds = [], []. for batch in dbch.train_dl:. reshaped = batch[0].permute(1,0,2,3).reshape((3, -1
===========================================================================================
Cleaned documentation: param = {"objective": "regression",. boosting": "gbdt",. num_leaves": 1280,. learning_rate": 0.05,. feature_fraction": 0.85,. reg_lambda": 2,. metric": "rmse"
Processed documentation: param = {"objective": "regression",. boosting": "gbdt",. num_leaves": 1280,. learning_rate": 0.05,. feature_fraction: 0.85,. reg_lambda":
===========================================================================================
Cleaned documentation: Dummy submission. Predict. Predict in batches of 9600. Get target dictionary
Processed documentation: Dummy submission. Predict in batches of 9600. Get target dictionary.
===========================================================================================
Cleaned documentation: submission = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/sample_submission.csv'). data2 = pd.DataFrame(all_preds).stack(). data2 = pd.DataFrame(data2). submission['Target'] = data2[0].values. submission.to_csv('lgbm_baseline_fs.csv', index=False)
Processed documentation: submission = pd.read_csv('/kaggle/ input/bigquery-geotab-intersection-congestion/sample_submission.csv'). data2 = pD.DataFrame(
===========================================================================================
Cleaned documentation: Lets check out the memory usage of the merchants.csv by pandas dataframeb
Processed documentation: Lets check out the memory usage of the merchants.
===========================================================================================
Cleaned documentation: EXTRACTING DAY_OF_WEEK, HOUR, DAY, MONTH FROM DATE. ADDING ANOTHER FEATURE revenue_status TO INDICATE PRESENCE/ABSENCE OF REVENUE FOR EACH OBSERVATION
Processed documentation: EXTRACTING DAY_OF_WEEK, HOUR, DAY, MONTH FROM DATE. ADDING ANOTHER FEATURE revenue_status TO INDICATE PRESENCE/ABSENCE OF REVENUE FOR E
===========================================================================================
Cleaned documentation: CONVERTING ALL THE STRINGS IN CATEGORICAL FEATURES TO LOWER CASE. REPLACING STRING 'nan' WITH np.nan
Processed documentation: Convert all the STRINGS IN CATEGORICAL FEATURES TO LOWER CASE. REPLACING STRING 'nan' WITH np.nan.
===========================================================================================
Cleaned documentation: GENERATING RANKS FOR CATEGORICAL FEATURES WITH UNIQUE VALUES GREATER THAN 10. RANKS ARE GENERATED USING REVENUE PERCENTAGE
Processed documentation: GENERATING RANKS FOR CATEGORICAL FEATURES WITH UNIQUE VALUES. RANKs ARE GENERATED USING REVENUE PERCENTAGE.
===========================================================================================
Cleaned documentation: USING TfidfVectorizer TO EXTRACT FEATURES FROM geoNetwork_networkDomain. DIMENSIONALITY REDUCTION ON EXTRACTED FEATURES. CREATING DATAFRAMES AFTER FEATURE EXTRACTION AND REDUCTION
Processed documentation: USING TfidfVectorizer TO EXTRACT FEATURES FROM geoNetwork_networkDomain. DIMENSIONALITY REDUCTION ON EXTRACTED FEATURES. CREATING DATAFRAMES AFTER FEATURES
===========================================================================================
Cleaned documentation: Sorting only True Test data... Yes, you already know the [kernel]( ;) One of the true winners: [YaG320](
Processed documentation: Sorting only True Test data... Yes, you already know the [ kernel]: [YaG320]
===========================================================================================
Cleaned documentation: Thses boxes should be remove which almost can't be see anything in it.What's about area 500
Processed documentation: Thses boxes should be remove which almost can't be see anything in it.
===========================================================================================
Cleaned documentation: TRAINING MODELS. SPLITTING TRAINING DATA BY METER TYPE. FITTING MODEL. print(val_y.shape,preds_x.shape). print(acct.shape,preds.shape)
Processed documentation: TRAINING MODELS. SPLITTING TRAINING DATA BY METER TYPE. FITTING MODEL. print(val_y.shape,preds_x.shape). print(acct.shape,.
===========================================================================================
Cleaned documentation: subf['meter_reading1']=[x if x>=0 else 0 for x in subf['meter_reading1']]. print('Shape of final predicted set (41697600,2):',subf.shape). subf.to_csv('sub_initial.csv',index=False). subf
Processed documentation: subf['meter_reading1']=[x if x>=0 else 0 for x in subf]. print('Shape of final predicted set (41697600,2):',subf.shape). subf.
===========================================================================================
Cleaned documentation: time. JOINING TRAIN SET AND BUILDING METADATA. df_test_BM=pd.merge(df_test,df_building_metadata,how='left',on='building_id'). print('AFTER JOINING OF BUILDING METADATA WITH TRAIN SET'). feature_summary(df_test_BM)
Processed documentation: df_test_BM=pd.merge(df_ test,df_building_metadata,how='left',on='building_id'). print('AFTER JOINING OF BUILDING METADATA WITH TRA
===========================================================================================
Cleaned documentation: time. JOINING JOINED(TRAIN,BUILDING METADATA) WITH WEATHER TRAIN SET ON site_id AND timestamp. cols=['site_id','timestamp','air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr',. sea_level_pressure','wind_direction','wind_speed']. df_test_BMW=pd.merge(df_test_BM,df_weather_test[cols],how='left',on=['site_id','timestamp']). feature_summary(df_test_BMW)
Processed documentation: join(TRAIN,BUILDING METADATA) WITH WEATHER TRAIN SET ON site_id AND timestamp. cols=['site_id','timestamp','air_temperature','cloud_coverage','
===========================================================================================
Cleaned documentation: time. EXTRACTING INFORMATION FROM timestamp FEATURE. df_test_BMW['month']=df_test_BMW.timestamp.apply(lambda x:x.month). df_test_BMW['day']=df_test_BMW.timestamp.apply(lambda x:x.day). df_test_BMW['hour']=df_test_BMW.timestamp.apply(lambda x:x.hour). df_test_BMW['week_day']=df_test_BMW.timestamp.apply(lambda x:x.weekday()). df_test_BMW['week']=df_test_BMW.timestamp.apply(lambda x:x.isocalendar()[1]). GARBAGE COLLECTION. gc.collect()
Processed documentation: time.apply(lambda x:x.month). df_test_BMW['day']=df_test-BMW.timestamp.apply('day', 'hour', 'week' }); df_ test-BM
===========================================================================================
Cleaned documentation: var_12, var_15 ,var_25, var_34, var_43, var_108, var_125 have very low range of values further elaborated by the histogram below.
Processed documentation: var_12, var_15,var_25, var 34, var  43,  108,   125 have very low range of values.
===========================================================================================
Cleaned documentation: pd.options.display.max_columns = None. Dcols =train_transaction.columns[train_transaction.columns.str.startswith('D')]. select=train_transaction.columns[1:55]. group1_details=pd.merge(group1,train_transaction[select],on=by,how='right'). group1_details.sort_values(by=['TransactionID','TransactionDT'],ascending=False). group1_details[(group1_details.card1==16075) & (group1_details.TransactionID==60)]. group1_details[(group1_details[['D1','D2','D3']].notnull().all(1)) & (group1_details.TransactionID>30 )].head(5). group1_details[(group1_details.card1==1342) & (group1_details.TransactionID==39)]
Processed documentation: pd.options.display.max_columns = None. Dcols =train_transaction.columns.str.startswith('D')]. select=train_ transaction.Columns[1:55
===========================================================================================
Cleaned documentation: Fit model. earlystop = EarlyStopping(patience=10, verbose=1, restore_best_weights=True). model.fit(x=X_train,y=Y_train,. validation_split=0.20,. batch_size=8,. steps_per_epoch=6len(X_train)/8,. epochs=20,. callbacks=[earlystop]
Processed documentation: Fit model. earlystop = EarlyStopping(patience=10, verbose=1, restore_best_weights=True). model. fit(x=X_train,y=Y_train,. validation_split
===========================================================================================
Cleaned documentation: siim19_csv['year'] = '2019'. siim_all = pd.concat([siim19_csv,siim20_csv],ignore_index = True). train = siim_all.loc[siim_all.target == 1]
Processed documentation: siim19_csv['year'] = '2019' siim_all = pd.concat([siim 19_csv,siim20_csv],ignore_index = True). train = siim.loc
===========================================================================================
Cleaned documentation: CROP WITH BOUNDING BOXES TO GET DOGS ONLY. if idxIn%1000==0: print(idxIn). RANDOMLY CROP FULL IMAGES. DISPLAY CROPPED IMAGES
Processed documentation: CROP WITH BOUNDING BOXES TO GET DOGS ONLY. if idxIn%1000==0: print(idxIn). RANDOMLY CROP FULL IMAGES.
===========================================================================================
Cleaned documentation: fig = px.box(train_df, y="game_time_log",x = "type",color='weekday_name',title={'text': "Distribution of game_time by type based on weekday",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},. color_discrete_sequence=cl.scales['3']['qual']['Dark2']). fig.show()
Processed documentation: fig = px.box(train_df, y="game_time_log",x = "type", color='weekday_name', title={'text': "Distribution of game_time by type based on
===========================================================================================
Cleaned documentation: fig = px.box(train_df, y="game_time_log",x = "type",color='world',title={'text': "Distribution of game_time by type based on world",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},. color_discrete_sequence=cl.scales['3']['qual']['Dark2']). fig.show()
Processed documentation: fig = px.box(train_df, y="game_time_log",x = "type",color='world',title={'text': "Distribution of game_time by type based on world",'
===========================================================================================
Cleaned documentation: fig = px.strip(train_df, y="game_time_log",x = "world",title={'text': "Distribution of game_time by world",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},. color_discrete_sequence=cl.scales['3']['qual']['Dark2']). fig.show()
Processed documentation: fig = px.strip(train_df, y="game_time_log",x = "world",title={'text': "Distribution of game_time by world",'y':0.9,'x
===========================================================================================
Cleaned documentation: Generate text features:. Initialize decomposition methods:. Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
Processed documentation: Generate text features: Initialize decomposition methods: Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
===========================================================================================
Cleaned documentation: Start TensorBoard visualization. All summaries are written into the. logs directory which is contained in the current working directory.
Processed documentation: Start TensorBoard visualization. All summaries are written into the logs directory.
===========================================================================================
Cleaned documentation: Generate text features:. Initialize decomposition methods:. Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
Processed documentation: Generate text features: Initialize decomposition methods: Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
===========================================================================================
Cleaned documentation: Manually adjusted coefficients:. print("Train Predictions Counts = ", Counter(train_predictions)). print("Test Predictions Counts = ", Counter(test_predictions))
Processed documentation: Manually adjusted coefficients:. print("Train Predictions Counts = ", Counter(train_predictions). print("Test Predictionscounts =!", Counter(test_prediction))
===========================================================================================
Cleaned documentation: train_stack = np.concatenate(( train_pred_keras, train_predictions_lgb), axis=1). test_stack = np.concatenate(( test_pred_keras, train_predictions_lgb), axis=1)
Processed documentation: train_stack = np.concatenate (train_pred_keras, train_predictions_lgb), axis=1). test_stack is the same but has a different axis.
===========================================================================================
Cleaned documentation: class_500 = value_counts_df[value_counts_df['landmark_id']>500].index.values. class_500_df = train_df[train_df['landmark_id'].isin(class_500)]. class_500_df.reset_index(inplace=True,drop=True). class_500_dataset = Load_Dataset(class_500_df). class_500_dataset_loader = torch.utils.data.DataLoader(class_500_dataset,batch_size=64)
Processed documentation: class_500 = value_counts_df[value_counted_df['landmark_id']>500].index.values. class_500_df.reset_index(inplace=True,drop=
===========================================================================================
Cleaned documentation: Save+Data The below code saves the files and checks the number of records and the size.
Processed documentation: Save+Data saves the files and checks the number of records and the size.
===========================================================================================
Cleaned documentation: ebird code of bird species is unique or not. ebird code of bird species
Processed documentation: ebird code of bird species is unique or not.
===========================================================================================
Cleaned documentation: Evaluate predictions. xgb_accuracy = accuracy_score(xgbt1_val_y, xgb_round_predictions). print("Accuracy: %.2f%%" % (xgb_accuracy 100.0))
Processed documentation: Evaluate predictions. xgb_accuracy = accuracy_score(xgbt1_val_y, xgbt_round_predictions). print("Accuracy: %.2f%%" % (x
===========================================================================================
Cleaned documentation: Calculate overlap area. Calculate union area. Already matched GT-box. tp. Showing GT boxes nearby. noisy targets. fn
Processed documentation: Calculate overlap area. Calculate union area. Already matched GT-box. Showing GT boxes nearby.
===========================================================================================
Cleaned documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_per_epoch=int(len(train_dataset) / batch_size),. pct_start=0.1,. anneal_strategy='cos',. final_div_factor=105
Processed documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_
===========================================================================================
Cleaned documentation: pip install tensorflow==2.0.0-alpha0. pip install tensorflow-gpu==2.0.0-alpha0. pip install tf-nightly-gpu-2.0-preview tfp-nightly. from tensorflow.python.ops import control_flow_util. control_flow_util.ENABLE_CONTROL_FLOW_V2 = True
Processed documentation: pip install tensorflow==2.0.0-alpha0. pip install tf-nightly-gpu-2. 0-preview tfp-nightley.
===========================================================================================
Cleaned documentation: featurewise_center=True,. featurewise_std_normalization= True,. zca_whitening = False,. data_format='channels_last',. rescale=1./255. train_datagen.fit(load_all_images('../input/extraimages/extraimages/', IMAGE_HT_WID, IMAGE_HT_WID))
Processed documentation: featurewise_center=True,. featurewise_std_normalization= True,. zca_whitening = False,. data_format='channels_last',. rescale=1./255. train_datagen
===========================================================================================
Cleaned documentation: EPOCHS= 4 8. history_tune = model.fit_generator(generator=train_generator,. steps_per_epoch=STEP_SIZE_TRAIN,. validation_data=valid_generator,. validation_steps=STEP_SIZE_VALID,. epochs=EPOCHS,. verbose=2,. callbacks=[reduce_lr, early_stop]
Processed documentation: EPOCHS= 4 8. history_tune = model.fit_generator(generator=train_generators, steps_per-epoch=STEP_SIZE_TRAIN,. validation_data=
===========================================================================================
Cleaned documentation: Get axes for multiple plots. Original data. Transformed data. Inverse Transformed data
Processed documentation: Get axes for multiple plots. Transformed data. Inverse data.
===========================================================================================
Cleaned documentation: longestKill Lucky or Cheater? My friend told me that 1km sniper shot is not impossible in this game.
Processed documentation: Kill Lucky or Cheater? My friend told me that 1km sniper shot is not impossible in this game.
===========================================================================================
Cleaned documentation: This can be an important feature! Predict game mode I'm working on clustering matches now...)
Processed documentation: This can be an important feature! Predict game mode I'm working on clustering matches now.
===========================================================================================
Cleaned documentation: Downloaded from. Author: Daewon Lee, github: @dwgoon. Wheel packaged for python 3.7
Processed documentation: The Wheel is packaged for python 3.7. It is available on GitHub.
===========================================================================================
Cleaned documentation: plt.subplot(2,1,2)plt.plot(df_news_small['time'], df_news_small['sentimentNegative'], '--ro', label='Neg')plt.plot(df_news_small['time'], df_news_small['sentimentNeutral'], '--bo', label='Neu')plt.plot(df_news_small['time'], df_news_small['sentimentPositive'], '--go', label='Pos')plt.legend(). plt.subplot(2,1,1)
Processed documentation: plt.legend(). plt.subplot(2,1,2)
===========================================================================================
Cleaned documentation: for i, fn in enumerate(os.listdir('..input/freesound_audio_tagging/train_noisy')): print(i) path = '..input/freesound_audio_tagging/train_noisy/' + fn save_image_from_sound(path)
Processed documentation: for i, fn in enumerate(os.listdir('..input/freesound_audio_tagging/train_noisy')): print(i) path = '.. input/ freesound/audio/
===========================================================================================
Cleaned documentation: This plot indicates a strong correlation between Percent and FVC. Again, there might have an heteroskedasticity issue.
Processed documentation: This plot indicates a strong correlation between Percent and FVC. There might have an heteroskedasticity issue.
===========================================================================================
Cleaned documentation: Calculates the modified Laplace Log Likelihood score for this competition. Credits:
Processed documentation: Calculates the modified Laplace Log Likelihood score for this competition.
===========================================================================================
Cleaned documentation: Create submission file Here I am combining the probabilities from the two models, using parameter alpha.
Processed documentation: Use alpha to combine the probabilities from the two models.
===========================================================================================
Cleaned documentation: Thank you for reading this. Comments and tips are most welcomed. Please upvote if you find it useful. Cheers!
Processed documentation: Thank you for reading this. Comments and tips are most welcomed. Please upvote if you find it useful.
===========================================================================================
Cleaned documentation: matplotlib notebook. matplotlib inline. train_full_id = df_id_tr.filter(regex='id'). plt.figure(figsize=(18,9)). sns.heatmap(train_full_id.isnull(), cbar= False)
Processed documentation: matplotlib notebook. matplotlib inline. train_full_id = df_id_tr.filter(regex='id'). plt.figure(figsize=(18,9), sns.heatmap(
===========================================================================================
Cleaned documentation: filling null C values in test tran dataset with median. for c in df_tran_ts.filter(regex='C2|C8|C12|C1|C4|C10|C11|C6|C7'):. df_tran_ts[c]=df_tran_ts[c].fillna(df_tran_ts[c].median())
Processed documentation: filling null C values in test tran dataset with median. for c in df_tran_ts.filter(regex='C2|C8|C12|C1|C4|C10|C
===========================================================================================
Cleaned documentation: commented out for memory error and directly fed to model below. making df ready for model training. inp_df=df_tran_tr.drop(['isFraud'],axis=1). out_df=df_tran_tr[['isFraud']]
Processed documentation: commented out for memory error and directly fed to model below. making df ready for model training. inp_df=df_tran_tr.drop(['isFraud'],axis=1). out_df
===========================================================================================
Cleaned documentation: Find edges by Canny method with OpenCV Parameter shall be tuned more. Here, below's is moderate
Processed documentation: Find edges by Canny method with OpenCV Parameter shall be tuned more.
===========================================================================================
Cleaned documentation: new_df = sample_submission.merge(test,how="inner",on="textID"). new_df["selected_text"] = np.where((new_df["sentiment"] == "neutral"),new_df["text"], new_df["selected_text"]). submission = new_df[["textID", "selected_text"]]
Processed documentation: new_df = sample_submission.merge(test,how="inner",on="textID"), new_df["selected_text"] = np.where((new_DF["sentiment"] == "neutral"),
===========================================================================================
Cleaned documentation: row_id` is a foreign_key to `test.csv`, we have to predict `meter_reading` for corresponding row in the test.csv
Processed documentation: row_id` is a foreign_key to `test.csv` We have to predict the corresponding row in the test.csv.
===========================================================================================
Cleaned documentation: scheduler.step(loss). print (f"epoch {epoch}, datapoints {idx10}"). scheduler.step(loss). print (f"epoch {epoch}, datapoints {idx10}")
Processed documentation: scheduler.step(loss). print (f"epoch {epoch}, datapoints {idx10}"). scheduler. step(loss)
===========================================================================================
Cleaned documentation: submission fileを同じくmelt処理し、sales_train_valとつなげる。 submission fileの列名を"d_xx"形式に変更する. submission fileで縦に結合されたvalidationとevaluationを一度分割し、それぞれことなる28日間の列名"d_xx"をそれぞれ付与。 submission fileには, idの詳細（item, department, state等）が無いためidをキーに, sales validationから取得したproductを結合 test2は、6/1まで不要なため削除
Processed documentation: submission file.submissionfile. Submission file  melt処理  sales_train_val    submission. file. submission file   "d_xx"
===========================================================================================
Cleaned documentation: holidays_events = pd.read_csv('../input/holidays_events.csv'). items = pd.read_csv('../input/items.csv'). oil = pd.read_csv('../input/oil.csv'). stores = pd.read_csv('../input/stores.csv'). transactions = pd.read_csv('../input/transactions.csv')
Processed documentation: holidays_events = pd.read_csv('../input/holidays.events'), items = pD. read_ CSV('../ input/items.csv'), oil = p d.read-csv('../
===========================================================================================
Cleaned documentation: shift train predictions for plotting. shift test predictions for plotting. plot baseline and predictions
Processed documentation: shift train predictions for plotting. shift test predictions for plots. plot baseline and predictions.
===========================================================================================
Cleaned documentation: Testing face_detection/xcepion models These models are specific to detecting just the face. face_detection/xcepion all_raw.p model
Processed documentation: Testing face_detection/xcepion models. These models are specific to detecting just the face.
===========================================================================================
Cleaned documentation: seq_length Train data consists of only 107 sequence length. The test data contains mostly 130 sequence lengths.
Processed documentation: Train data consists of only 107 sequence length. Test data contains mostly 130 sequence lengths.
===========================================================================================
Cleaned documentation: And there you have it. We've fixed it. Thanks for your patience, and good luck!
Processed documentation: And there you have it. We've fixed it. Thanks for your patience.
===========================================================================================
Cleaned documentation: Training Function. scheduler.step(). m = nn.LogSoftmax(dim=1). outputs = m(outputs). outputs = outputs.squeeze(). print(m(outputs).type())
Processed documentation: Training Function. scheduler.step(). m = nn.LogSoftmax(dim=1). outputs = m(outputs). outputs.squeeze() print(m( outputs).type())
===========================================================================================
Cleaned documentation: Setup Set `PLANET_KAGGLE_ROOT` to the proper directory where we've got the TIFF and JPEG zip files, and accompanying CSVs.
Processed documentation: Set `PLANET_KAGGLE_ROOT to the proper directory where we've got the TIFF and JPEG zip files, and accompanying CSVs.
===========================================================================================
Cleaned documentation: we want counts & frequency of the labels. plotly for interactive graphs. display
Processed documentation: we want counts & frequency of the labels. plotly for interactive graphs.
===========================================================================================
Cleaned documentation: Classification report This will allow us to evaluate the model with other metrics (Precision, Recall, F1 score, etc...)
Processed documentation: This will allow us to evaluate the model with other metrics (Precision, Recall, F1 score, etc...)
===========================================================================================
Cleaned documentation: Clear the previous gradients. Precit the output for Given input. Compute Cross entropy loss. Compute gradients. Adjust weights
Processed documentation: Clear the previous gradients. Precit the output for Given input. Compute Cross entropy loss. Adjust weights.
===========================================================================================
Cleaned documentation: Checking players over 140kg. Found only 1 player: Akiem Hicks. The image below shows this player
Processed documentation: Checking players over 140kg. Found only 1 player: Akiem Hicks.
===========================================================================================
Cleaned documentation: Checking players under 70kg. Found only 1 player: JoJo Natson. The image below shows this player
Processed documentation: Checking players under 70kg. Found only 1 player: JoJo Natson.
===========================================================================================
Cleaned documentation: year and month. year and week. year. month of year. week of year. day of week
Processed documentation: year and month. year and week. year. week of year. day of week.
===========================================================================================
Cleaned documentation: msno_repeat['repeat_percentage_msno'] = msno_repeat['repeat_percentage_msno'].replace(100.0,np.nan). it can be meaningful so do not erase 100.0
Processed documentation: msno_repeat.replace(100.0,np.nan). it can be meaningful so do not erase 100.0. msno.repeat['repeat_percentage_msno'] = msno_ repeat.replace
===========================================================================================
Cleaned documentation: Preparing data Preparing the train and test data with weather features and in the format required by Prophet.
Processed documentation: Preparing data with weather features and in the format required by Prophet.
===========================================================================================
Cleaned documentation: Reading train data Reading train data along with building and weather metadata.
Processed documentation: Reading train data along with building and weather metadata.
===========================================================================================
Cleaned documentation: Preparing test data Preparing test data with same features as train data.
Processed documentation: Preparing test data with same features as train data.
===========================================================================================
Cleaned documentation: print rawData. print data. print train_data.loc[0]. in return csv file will save in working directory
Processed documentation: print rawData. print train_data.loc[0]. in return csv file will save in working directory.
===========================================================================================
Cleaned documentation: print rawData. print data. print train_data.loc[0]. in return csv file will save in working directory
Processed documentation: print rawData. print train_data.loc[0]. in return csv file will save in working directory.
===========================================================================================
Cleaned documentation: evaluating submission from public kernel M5 - Three shades of Dark: Darker magic. from
Processed documentation: evaluating submission from public kernel M5 - Three shades of Dark: Darker magic.
===========================================================================================
Cleaned documentation: to set up scoring parameters. X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)
Processed documentation: to set up scoring parameters. X_train, X_valid, y_train,. y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: As expected, we have 22 of each playid. Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: Referring to the following discussions. We have filtered the data in the next step Ref1. Ref2.
Processed documentation: Referring to the following discussions. We have filtered the data in the next step Ref1.
===========================================================================================
Cleaned documentation: Check for missing values in training set. print(info). print("There are", len(nullcols), "columns with missing values in test set")
Processed documentation: Check for missing values in training set. print(info). print("There are", len(nullcols), "columns with missing values"
===========================================================================================
Cleaned documentation: Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded
Processed documentation: Impute any values will significantly affect the RMSE score for test set. Imputations have been excluded.
===========================================================================================
Cleaned documentation: Check for missing values in training set. print(info). print("There are", len(nullcols), "columns with missing values in test set")
Processed documentation: Check for missing values in training set. print(info). print("There are", len(nullcols), "columns with missing values"
===========================================================================================
Cleaned documentation: Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded
Processed documentation: Impute any values will significantly affect the RMSE score for test set. Imputations have been excluded.
===========================================================================================
Cleaned documentation: set extensions for PPE. observe, report. ppe_extensions.ParameterStatistics(model, prefix='model'),. ppe_extensions.VariableStatisticsPlot(model),. ppe_extensions.ProgressBar(update_interval=100),. evaluation. save model snapshot.. set extensions to manager
Processed documentation: set extensions for PPE. observe, report. save model snapshot.. set extensions to manager.
===========================================================================================
Cleaned documentation: Create Model. Input Layer. Create and Compile Model and show Summary. UnFreeze all layers. GeM. multi output. model
Processed documentation: Create Model. Input Layer. Create and Compile Model. UnFreeze all layers. GeM. multi output. model
===========================================================================================
Cleaned documentation: SINGLE THREAD. for i in cat_cols:. train[i+'_reordered'],values_dict=reorderCategorical(train,i,'target',verbose=True,max_iterations=5). predict[i+'_reordered']=predict[i].replace(values_dict). print('Nice job! =]')
Processed documentation: SINGLE THREAD. for i in cat_cols:. train[i+'_reordered'],values_dict=reorderCategorical(train,i,'target',verbose=True,max
===========================================================================================
Cleaned documentation: tipovivi2, "=1 own, paying in installments". tipovivi3, =1 rented. tipovivi4, =1 precarious. tipovivi5, "=1 other(assigned, borrowed)
Processed documentation: tipovivi2, "=1 own, paying in installments". tipovIVi3, =1 rented. tipovivivi4, = 1 precarious. tipOVivi5, " =1 other(
===========================================================================================
Cleaned documentation: aggregate per item item 別に和を取る discard other information e.g. store / category item 以外の情報 (store, category など) は捨てて考える
Processed documentation: aggregate per item item    特   扉   鉬   (store, category) e.g. store / category item  情    (store
===========================================================================================
Cleaned documentation: Here we adopt the parameters (perp = 30, dof = 0.7): perp = 30, dof = 0.7) を採用する
Processed documentation: Here we adopt the parameters (perp = 30, dof = 0.7): perp + dof + 30 = 30.
===========================================================================================
Cleaned documentation: print(pose_record). box.render(ax[0], view=np.eye(4), colors=(c, c, c)). print(x,y,z, name). boxes, _ = level5data.get_sample_data(sample_lidar_token, box_vis_level=BoxVisibility.ANY, flat_vehicle_coordinates=True). ax[0].add_patch(rect)
Processed documentation: print(pose_record). box.render(ax[0], view=np.eye(4), colors=(c, c, c)). print(x,y,z, name). boxes, _ = level5data
===========================================================================================
Cleaned documentation: from. Stack X as [X,X,X]. X = np.stack([X, X, X], axis=-1). Standardize. Normalize to [0, 255]. Just zero
Processed documentation: X = np.stack([X, X, X], axis=-1). Standardize. Normalize to [0, 255]. Just zero.
===========================================================================================
Cleaned documentation: from. Stack X as [X,X,X]. X = np.stack([X, X, X], axis=-1). Standardize. Normalize to [0, 255]. Just zero
Processed documentation: X = np.stack([X, X, X], axis=-1). Standardize. Normalize to [0, 255]. Just zero.
===========================================================================================
Cleaned documentation: The number of days excpet NaN (only in test data). only in test data)
Processed documentation: The number of days excpet NaN (only in test data)
===========================================================================================
Cleaned documentation: NewYear (新年)[29] National Federal holiday. Events such as fireworks, parties, etc.
Processed documentation: NewYear (新年) National Federal holiday. Events such as fireworks.
===========================================================================================
Cleaned documentation: OrthodoxChristmas (正教会のクリスマス)[16] Religious January 7.　Orthodox people abstain from eating until this day.
Processed documentation: Orthodox people abstain from eating until this day.
===========================================================================================
Cleaned documentation: ValentinesDay (バレンタインデー)[2][3][4] Cultural People send other people cards, flowers, chocolates, etc. with message to show their affection.
Processed documentation: Valentines Day is a day when people send cards, flowers, chocolates, etc. with message to show their affection.
===========================================================================================
Cleaned documentation: Cinco De Mayo (シンコ・デ・マヨ)[31] Cultural Mexican holiday. People enjoy Mexican Products - Drinks, food, and music.
Processed documentation: Cinco De Mayo is a cultural Mexican holiday. People enjoy Mexican Products - Drinks, food, and music.
===========================================================================================
Cleaned documentation: Ramadan starts (ラマダーン開始)[14] Religious Muslims fast from sunrise to sunset. They also avoids smoking.
Processed documentation: Ramadan starts when Muslims fast from sunrise to sunset. They also avoids smoking.
===========================================================================================
Cleaned documentation: Eid al-Fitr (イド・アル＝フィトル)[23] Religious Islamic holiday. A big festival to celebrate the end of Ramadan.
Processed documentation: Eid al-Fitr is a religious Islamic holiday. A big festival to celebrate the end of Ramadan.
===========================================================================================
Cleaned documentation: VeteransDay (復員軍人の日)[25] National Most schools are not closed and businesses operate normally.
Processed documentation: Most schools are not closed and businesses operate normally.
===========================================================================================
Cleaned documentation: Plot the distribution of the toxicity and identity annotator count. ax2.hist(train_data.identity_annotator_count)
Processed documentation: Plot the distribution of the toxicity and identity annotator count.
===========================================================================================
Cleaned documentation: cnt_srs = test_df['first_active_month'].dt.date.value_counts(). cnt_srs = cnt_srs.sort_index(). Plot the distribution of the comments with identity values greater than 0.5
Processed documentation: cnt_srs = test_df['first_active_month'].dt.date.value_counts. cnt_ srs = cnt srs.sort_index(). Plot the distribution of the comments
===========================================================================================
Cleaned documentation: Another way to visualize the observations made above is a pairplot. Again the same observations can be made.
Processed documentation: Another way to visualize the observations made above is a pairplot. The same observations can be made.
===========================================================================================
Cleaned documentation: Resets cycle iterations. Optional boundary/step size adjustment.. self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr)). self.history.setdefault('iterations', []).append(self.trn_iterations). for k, v in logs.items():. self.history.setdefault(k, []).append(v)
Processed documentation: Resets cycle iterations. Optional boundary/step size adjustment.. self.history.setdefault('iterations', []).append(self.trn_iterations). for k, v in logs.items():. self. history
===========================================================================================
Cleaned documentation: Create one hot weekday column from wday column to calculate correlation later.. sns.heatmap(temp_df[["value", "snap_CA", ]].corr(), annot=True)
Processed documentation: Create one hot weekday column from wday column to calculate correlation later. sns.heatmap(temp_df[["value", "snap_CA", ]].corr() annot=True)
===========================================================================================
Cleaned documentation: diff = difflib.unified_diff(test_comment.replace(" ", "\n ").split(), val_comment.replace(" ", "\n ").split(), "TEST", "VAL", lineterm='\n')
Processed documentation: diff = difflib.unified_diff(test_comment.replace(" ", "\n ").split() "TEST", "VAL", lineterm='\n')
===========================================================================================
Cleaned documentation: if bi % 10 == 0:. break. print(t1.shape). if torch.cuda.device_count() > 1:. model = nn.DataParallel(model)
Processed documentation: if bi % 10 == 0:. print(t1.shape). if torch.cuda.device_count() > 1:. model = nn.DataParallel(model)
===========================================================================================
Cleaned documentation: g = np.argmax(g.cpu().detach().numpy(), axis=1). v = np.argmax(v.cpu().detach().numpy(), axis=1). c = np.argmax(c.cpu().detach().numpy(), axis=1)
Processed documentation: g = np.argmax(g.cpu().detach().numpy(), axis=1). v = np arg max(v.cpu() detach() numpy()() axis=1) c
===========================================================================================
Cleaned documentation: Data EDA... Let first analyse the CSV file, and Understand the data contain in it...
Processed documentation: Data EDA... Let first analyse the CSV file, and Understand the data contain in it.
===========================================================================================
Cleaned documentation: Encoder Layers. Decoder Layers. Combine Encoder and Deocder layers. Compile the Model
Processed documentation: Encoder Layers. Combine Encoder and Deocder layers. Compile the Model.
===========================================================================================
Cleaned documentation: batches = 1model = getmodel(cfgfile, weightsfile)sub_str1 = detect(model,img_path,batches,0.5,0.4, coco_classes, weightsfile, cfgfile, "416")print(sub_str1). de752.jpg"
Processed documentation: batches = 1model = getmodel(cfgfile, weightsfile)sub_str1 = detect(model, img_path,batches, 0.5,0.4, coco_classes)print(sub
===========================================================================================
Cleaned documentation: model = Darknet("../input/yolov3cfg/yolov3.cfg"). inp = get_test_input("../input/google-ai-open-images-object-detection-track/test/challenge2018_test/00001a21632de752.jpg",416). print(inp). pred = model(inp, torch.cuda.is_available()). print (pred). print (pred.shape)
Processed documentation: model = Darknet("../ input/yolov3.cfg") inp = get_test_ input("../input/google-ai-open-images-object-detection-track/test/chall
===========================================================================================
Cleaned documentation: cp -r ../input/keras-yolo-new/ky ky. from ky.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body. print(yolo_model.output)
Processed documentation: cp -r../ input/keras-yolo-new/ky ky.keras_yolo. print(yolo_model.output)
===========================================================================================
Cleaned documentation: font = ImageFont.truetype(font='../input/firamonomedium/FiraMono-Medium.otf',size=np.floor(3e-2 image.size[1] + 0.5).astype('int32')). image, image_data = preprocess_image("../input/google-ai-open-images-object-detection-track/test/" + image_file, model_image_size = (608, 608))
Processed documentation: font = ImageFont.truetype(font='../input/firamonomedium/FiraMono-Medium.otf',size=np.floor(3e-2 image.size[1
===========================================================================================
Cleaned documentation: Similarly, in this case let's drop the missing rows for 'category_3' and 'merchant_id' (less than 1% of total).
Processed documentation: Let's drop the missing rows for 'category_3' and'merchant_id' (less than 1% of total)
===========================================================================================
Cleaned documentation: fig2.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 10. fig2.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 10. fig2.layout.sliders[0].pad.t = 10. fig2.layout.updatemenus[0].pad.t= 10
Processed documentation: fig2.layout.updatemenus[0].buttons[0]args[1] = 10. fig2. layout.sliders [0].pad.t= 10.
===========================================================================================
Cleaned documentation: Below, we will use the GradientBoostingRegressor to see the top features in our dataset.
Processed documentation: We will use the GradientBoostingRegressor to see the top features in our dataset.
===========================================================================================
Cleaned documentation: The above plot looks very cluttered. Instead, we will take a look at the `top 25 features`.
Processed documentation: The above plot looks very cluttered. Instead, we will take a look at the top 25 features.
===========================================================================================
Cleaned documentation: Create a the database file or use it if it was already created.
Processed documentation: Create a the database file or use it if it's already created.
===========================================================================================
Cleaned documentation: Below is the query which will allow us to show the top 25 IP Address by their activity.
Processed documentation: Below is the query which will allow us to show the top 25 IP addresses by activity.
===========================================================================================
Cleaned documentation: Path retrieval Taken from the public kernel...thanks for sharing this. It was quite helpful
Processed documentation: Path retrieval Taken from the public kernel...thanks for sharing this.
===========================================================================================
Cleaned documentation: df['RowId'] = df['RowId'].apply(str) + '_0'. df.rename(columns={'RowId': 'TargetId', 'TotalTimeStopped_p20': 'Target'}, inplace=True). df
Processed documentation: df['RowId'] = df.apply(str) + '_0' df.rename(columns={'RowId': 'TargetId', 'TotalTimeStopped_p20':'Target'}, in
===========================================================================================
Cleaned documentation: In additional train data from 2018, proportion of missing values is: {100nans_proportion(data['popularity2'], (data['dataset']=='Train') & (data['release_year']==2018)):.2f} %.
Processed documentation: In additional train data from 2018, proportion of missing values is: {100nans_proportion(data['popularity2'], (data['dataset']=='Train') & ( data['release_year']
===========================================================================================
Cleaned documentation: data = add_comparison_stats(data, cols=['rating', 'popularity_mean'], neighbourhood_in_weeks=2). data = add_comparison_stats(data, cols=['rating', 'popularity_mean'], neighbourhood_in_weeks=3)
Processed documentation: data = add_comparison_stats(data, cols=['rating', 'popularity_mean'], neighbourhood_in_weeks=2). data = add comparisons stats( data, col
===========================================================================================
Cleaned documentation: This part is going to be for explorind the dataset .... so we want the entire dataset ..
Processed documentation: This part is going to be for explorind the dataset.... so we want the entire dataset..
===========================================================================================
Cleaned documentation: This part is going to be for explorind the dataset .... so we want the entire dataset ..
Processed documentation: This part is going to be for explorind the dataset.... so we want the entire dataset..
===========================================================================================
Cleaned documentation: Lets now plot the ones that have the highest autocorrelations ...
Processed documentation: Lets now plot the ones that have the highest autocorrelations.
===========================================================================================
Cleaned documentation: cols = od.columns. scaler = preprocessing.StandardScaler(). od = scaler.fit_transform(od). od = np.nan_to_num(od). od = pd.DataFrame(od, columns=cols). od.describe()
Processed documentation: cols = od.columns. scaler = preprocessing.StandardScaler() od = scaler.fit_transform(od). od = np.nan_to_num(od) od = pd.Data
===========================================================================================
Cleaned documentation: inputs. x = keras.layers.Dropout(0.05)(x). x = keras.layers.Dense(num_class, activation='softmax')(x). model.compile(optimizer='adam', loss='mse'). model.compile(optimizer='sgd', loss='mse'). model.compile(optimizer='adam', loss=crps)
Processed documentation: x = keras.layers.Dense(num_class, activation='softmax')(x). model.compile(optimizer='adam', loss='mse'). model.Compile( Optimizer='
===========================================================================================
Cleaned documentation: use previous week test set in order to compare with previous week leaderboard. from kaz
Processed documentation: use previous week test set in order to compare with previous week leaderboard.
===========================================================================================
Cleaned documentation: enforce monotonicity. d = d.sort_values(['Loc','Date']).reset_index(drop=True). for y in yv:. ey = 'extra_'+y. d[ey] = d[ey].fillna(0.). d[ey] = d.groupby('Loc')[ey].cummax()
Processed documentation: enforce monotonicity. d = d.sort_values(['Loc','Date']).reset_index(drop=True). for y in yv:. ey = 'extra_'+y.fillna(
===========================================================================================
Cleaned documentation: enforce monotonicity. d = d.sort_values(['Loc','Date']).reset_index(drop=True). d['recov'] = d['recov'].fillna(0.). d['recov'] = d.groupby('Loc')['recov'].cummax()
Processed documentation: enforce monotonicity. d = d.sort_values(['Loc','Date']).reset_index(drop=True). d['recov'] = d('recov'].fillna(0.). d
===========================================================================================
Cleaned documentation: must start cas server from gevmlax02 before running this cell. ssh rdcgrd001 /opt/vb025/laxnd/TKGrid/bin/caslaunch stat -mode mpp -cfg /u/sasrdw/config.lua
Processed documentation: must start cas server from gevmlax02 before running this cell.
===========================================================================================
Cleaned documentation: q = (d.Date >= '2020-04-02') & (d.Loc=='Cabo Verde'). q = (d.Date >= '2020-04-02') & (d.Loc=='Congo (Brazzaville)')
Processed documentation: q = (d.Date >= '2020-04-02') & ( d.Loc=='Cabo Verde') & d.Date < '2020/04/02' q = 'Congo (Brazz
===========================================================================================
Cleaned documentation: K Fold Cross Validation approach for Deep Learning by Jeff Heaton [1] .
Processed documentation: K Fold Cross Validation approach for Deep Learning by Jeff Heaton.
===========================================================================================
Cleaned documentation: some config values. fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values
Processed documentation: some config values. fill up the missing values. Pad the sentences. Get the target values.
===========================================================================================
Cleaned documentation: CONFIGURE GPUs. os.environ["CUDA_VISIBLE_DEVICES"]="0". gpus = tf.config.list_physical_devices('GPU'); print(gpus). if len(gpus)==1: strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0"). else: strategy = tf.distribute.MirroredStrategy()
Processed documentation: CONFIGURE GPUs. os.environ["CUDA_VISIBLE_DEVICES"]="0". gpus = tf.config.list_physical_devices('GPU'); print(GPus). if len(gpus
===========================================================================================
Cleaned documentation: ENABLE MIXED PRECISION for speed. tf.config.optimizer.set_jit(True). tf.config.optimizer.set_experimental_options({"auto_mixed_precision": True}). print('Mixed precision enabled')
Processed documentation: ENABLE MIXED PRECISION for speed. tf.config.optimizer.set_jit(True). tf.optimized.options({"auto_mixed_precision": True), print('Mixed precision
===========================================================================================
Cleaned documentation: creating dummies of all categorical values. THIS METHORD IS BIT COMPLEX ONE 3. print(FE). print(data). break
Processed documentation: creating dummies of all categorical values. print(FE). print(data). break
===========================================================================================
Cleaned documentation: preds = random_proref_model.predict(test). test['random_a_coref'], test['random_b_coref'] = zip(preds). preds = token_distance_proref_model.predict(test). test['token_distance_a_coref'], test['token_distance_b_coref'] = zip(preds)
Processed documentation: preds = random_proref_model.predict(test). test['random_a_coref'], test ['random_b_coreF'] = zip(preds). preds = token_distance_
===========================================================================================
Cleaned documentation: OKAY! Best score is 0.600748907457. OKAY! Best weights are 0.45 0.45 0.1
Processed documentation: OKAY! Best score is 0.600748907457. Best weights are 0.45 0.1.
===========================================================================================
Cleaned documentation: optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.005, momentum=0.99). scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, )
Processed documentation: optimizer = optim.SGD(model_conv.fc.parameters() lr=0.005, momentum =0.99). scheduler = lr_scheduler.ReduceLROnPlateau
===========================================================================================
Cleaned documentation: Find elements from training sets not 'new_whale'. Dictionary of picture indices. Evaluate the model.. Generate the subsmission file.
Processed documentation: Find elements from training sets not 'new_whale' Dictionary of picture indices. Evaluate the model. Generate the subsmission file.
===========================================================================================
Cleaned documentation: Purpose Show the full dataset. This format might be easier and more accesible for participants. Have fun!
Processed documentation: Purpose Show the full dataset. This format might be easier and more accesible for participants.
===========================================================================================
Cleaned documentation: Generate Sample Text for each author using the independent chars model c_t \: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \: P(c_t|Author) $$
Processed documentation: Generate Sample Text for each author using the independent chars model c_t.
===========================================================================================
Cleaned documentation: Generate Sample Text for each Author using our Markov Model c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \: P(c_t|c_{t-1},Author) $$
Processed documentation: Generate Sample Text for each Author using our Markov Model.
===========================================================================================
Cleaned documentation: Generate Sample Text for each Author using our 2 time step Markov Model c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \: P(c_t|c_{t-1},c_{t-2},Author) $$
Processed documentation: Generate Sample Text for each Author using our 2 time step Markov Model.
===========================================================================================
Cleaned documentation: Generate Sample Text for each Author using our 3 time step Markov Model c_t\: {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} \: P(c_t|c_{t-1},c_{t-2},c_{t-3},Author) $$
Processed documentation: Generate Sample Text for each Author using our 3 time step Markov Model.
===========================================================================================
Cleaned documentation: Following lines do not contain data with no bbox. train image path list. validation image path list
Processed documentation: Following lines do not contain data with no bbox. train image path list.
===========================================================================================
Cleaned documentation: References I have learnt immensely from these kernels. Upvote them as well if you like my work
Processed documentation: I have learnt immensely from these kernels. Upvote them as well if you like my work.
===========================================================================================
Cleaned documentation: SAMPLE_LEN =100 def load_images(image_id): file_path =image_id +'.jpg' images = cv2.imread(image_path + file_path ) return cv2.cvtColor(images, cv2.COLOR_BGR2RGB) train_image = train['image_id'][:100].progress_apply(load_images)
Processed documentation: Cv2.SAMPLE_LEN =100 def load_images(image_id): file_path = image_id +'.jpg' images = cv2, train_image = train, train.progress_
===========================================================================================
Cleaned documentation: We can plot images from pyplot, however there are less functionalities in pyplot as compared to plotly.xpress
Processed documentation: We can plot images from pyplot, however there are less functionalities in pyplot as compared to plotly.
===========================================================================================
Cleaned documentation: source = pd.DataFrame(metadata_list, columns = ['source_id', 'square_feet', 'year', 'name', 'spaceUse', 'address', 'timezone', 'resources'])
Processed documentation: source = pd.DataFrame(metadata_list, columns = ['source_id','square_feet', 'year', 'name','spaceUse', 'address', 'timezone','resources'])
===========================================================================================
Cleaned documentation: preds_reshaped = np.reshape(preds, (int(len(preds)/10), -1)). preds_reduced_mean = np.mean(preds_reshaped, axis=1). preds_around = np.around(preds, decimals=1).astype(int)
Processed documentation: preds_reshaped = np.reshape(preds, (int(len(Preds)/10), -1), preds_reduced_mean =  numpy.mean(predS),
===========================================================================================
Cleaned documentation: print(j). print('-', cand). print('=', cand_2). print(answer). print(k, c, cand_0). print('+2', answer). print(answer). print(answer). print()
Processed documentation: print('-', cand). print('=', cand_2). print(answer) print(k, c, cand_0) print('+2', answer). print() print(j)
===========================================================================================
Cleaned documentation: bn1 = BatchNormalization()(conv1). re2 = LeakyReLU(0.2)(bn1). up2 = UpSampling2D(16, interpolation='bilinear')(re2). conv2 = Conv2D(1, (1, 1))(up2). conv2 = Activation('sigmoid')(conv2)
Processed documentation: bn1 = BatchNormalization()(conv1). re2 = LeakyReLU(0.2)(bn1). up2 = UpSampling2D(16, interpolation='bilinear')(re
===========================================================================================
Cleaned documentation: Who does not like finetuning? Lets make it simple with pipelines and GridSearchCV/RandomizedSearchCV.
Processed documentation: Pipelines and GridSearchCV/RandomizedSearchCV.
===========================================================================================
Cleaned documentation: print(datetime.now()). for j in range(start_model, end_model):. val_probabilities[j] = models[j].predict(images_ds). test_probabilities[j] = models[j].predict(test_images_ds). all_probabilities[j] = models[j].predict(images). print(datetime.now())
Processed documentation: print(datetime.now()). for j in range(start_model, end_model):. val_probabilities[j] = models[j].predict(images_ds). test_ Probabilities
===========================================================================================
Cleaned documentation: Let us submit the best path we have so far to the competition ...
Processed documentation: Let us submit the best path we have so far to the competition.
===========================================================================================
Cleaned documentation: Go to Content Menu](0.) Randomly Visualization of Samples in the Dataset
Processed documentation: Go to Content Menu. Randomly Visualization of Samples in the Dataset.
===========================================================================================
Cleaned documentation: Image loading from disk as JpegImageFile file format. Converting JpegImageFile to numpy array
Processed documentation: Image loading from disk as JpegImageFile file format. Converting to numpy array.
===========================================================================================
Cleaned documentation: number that identifies this image, This Attribute was named Image Number in earlier versions of this Standard. Instance Number
Processed documentation: number that identifies this image. This Attribute was named Image Number in earlier versions of this Standard.
===========================================================================================
Cleaned documentation: Data representation of the pixel samples. Each sample shall have the same pixel representation. Pixel Representation Attribute
Processed documentation: Data representation of the pixel samples. Each sample shall have the same pixel representation.
===========================================================================================
Cleaned documentation: image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY). image = np.array(image, dtype=np.uint8). image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY). print(image). image = image.reshape(768, 768, 1)
Processed documentation: image = cv2.cvtColor(image, cv 2.COLOR_BGR2GRAY). image = np.array(image), dtype=np.uint8. print(image). image.resh
===========================================================================================
Cleaned documentation: Sampling rate Sampling rate (audio) Sampling rate or sampling frequency defines the number of samples per second.
Processed documentation: Sampling rate or sampling frequency defines the number of samples per second.
===========================================================================================
Cleaned documentation: Merging transaction and identity dataset](4) We will firt merge our transactions and identity datasets.
Processed documentation: Merging transaction and identity dataset. We will firt merge our transactions and identity datasets.
===========================================================================================
Cleaned documentation: plot growing curve for top 5 most servere countries except China. TODO: optimize code. add traces
Processed documentation: plot growing curve for top 5 most servere countries except China.
===========================================================================================
Cleaned documentation: if choose to not apply normalization, however it generates NaN in output...
Processed documentation: if choose to not apply normalization, however it generates NaN in output.
===========================================================================================
Cleaned documentation: declaring only one model. model.add(LSTM(n_2, activation='relu')). adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False). print(model.summary())
Processed documentation: declaring only one model. model.add(LSTM(n_2, activation='relu')). adam = optimizers.Adam(learning_rate=0.01, beta_1= 0.9
===========================================================================================
Cleaned documentation: For tf.dataset. Data access. Configuration. Seed. Learning rate. cutmix prob. training filenames directory. test filenames directory. submission file
Processed documentation: For tf.dataset. Data access. Configuration. Seed. Learning rate. Training filenames directory. Test filename directory. submission file.
===========================================================================================
Cleaned documentation: Choosing a Sequence length train['title_description']= (train['title']+" "+train['description']).astype(str) train['length'] = train['title_description'].apply(lambda x: len(x.split(" "))) print(train['length'].mean()) print(train[train['length']>50].shape) max_seq_title_description_length = 100
Processed documentation: Choosing a Sequence length train['title_description']= (train['title']+" "+train['description']).astype(str) train['length'] = train[' title_description'].apply(lambda x:
===========================================================================================
Cleaned documentation: Let's run our random agent against the default one. Co-ordinates (0,0) are at the top left corner.
Processed documentation: Let's run our random agent against the default one. Co-ordinates are at the top left corner.
===========================================================================================
Cleaned documentation: res = np.zeros((img_size_target, img_size_target), dtype=img.dtype). res[:img_size_ori, :img_size_ori] = img. return res. return img[:img_size_ori, :img_size_ori]
Processed documentation: res = np.zeros((img_size_target, img_ size_target), dtype=img.dtype). res[: img_size-ori, :img_ size-ori] = img. return
===========================================================================================
Cleaned documentation: Generates batches of image data with data augmentation. Fits the model on batches with real-time data augmentation
Processed documentation: Generates batches of image data with data augmentation. Fits the model on batches with real-time data augmented.
===========================================================================================
Cleaned documentation: Now, that are too many number categories. Let's see the top 10 highest counted categories...
Processed documentation: Now, that is too many number categories. Let's see the top 10 highest counted categories.
===========================================================================================
Cleaned documentation: fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values
Processed documentation: fill up the missing values. Pad the sentences. Get the target values.
===========================================================================================
Cleaned documentation: fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values
Processed documentation: fill up the missing values. Pad the sentences. Get the target values.
===========================================================================================
Cleaned documentation: img = tf.ensure_shape(img, (1024,1024,3)). img = tf.keras.preprocessing.image.random_rotation(img, np.random.randint(360)). img = tf.image.rot90(img,k=np.random.randint(4))
Processed documentation: img = tf.ensure_shape(img, (1024,1024,3), img =tf.keras.preprocessing.image.random_rotation( img, np.random.randint(360),
===========================================================================================
Cleaned documentation: Missing value: reserve_visitors':0 visit_date':pd.to_datetime('01/01/2099') reserve_date':pd.to_datetime('01/01/2099') calendar_date':pd.to_datetime('01/01/2099') day_of_week':'unknown' holiday_flg':-99 latitude':-99 longitude':-99 air_genre_name':'unknown' air_area_name':'unknown'
Processed documentation: Missing value: reserve_visitors':0 visit_date':pd.to_datetime('01/01/2099') reserve_date:. to_date('01-01-2099'), date:. calendar
===========================================================================================
Cleaned documentation: for padding of the time-series. A_train = np.zeros((len(X),len(A[0])+4), dtype=np.float32). A_train[i,5:] = A[i][1:]
Processed documentation: for padding of the time-series. A_train = np.zeros((len(X),len(A[0])+4), dtype=np.float32). A_ train[i,5:] =
===========================================================================================
Cleaned documentation: import pickle. with open('models.pickle', 'wb') as f:. pickle.dump(models, f). with open('models.pickle', 'rb') as f:. models = pickle.load(f)
Processed documentation: import pickle. with open('models.pickle', 'wb') as f:. models = models.load(f) models.dump(models, f). with open(f):. dump( models,
===========================================================================================
Cleaned documentation: shift_days_set = [28, 35, 42]. for i in shift_days_set:. df_price['price_{}day_ago'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(i)). gc.collect()
Processed documentation: shift_days_set = [28, 35, 42]. for i in shift_ days_set:. df_price.groupby(['item_id','store_id'])['sell_price'].
===========================================================================================
Cleaned documentation: print("Preds shape :",preds_B1.shape). print("Preds shape :",preds_B2.shape). print("Preds shape :",preds_B4.shape). print("Preds shape :",preds_B7.shape)
Processed documentation: print("Preds shape :",preds_B1.shape). print('Predsshape :',preds B2.shape), print('preds shape:',Preds_ B3.shape,preds
===========================================================================================
Cleaned documentation: Visitor Profile Lets create the visitor profile by aggregating the rows for every customer. Visitor Profile Snapshot
Processed documentation: Visitor Profile Lets create a visitor profile by aggregating the rows for every customer. Visitor Profile Snapshot
===========================================================================================
Cleaned documentation: The variable cells.ch0 takes values between 0 to 1195 for this event cells.ch0 for some other events
Processed documentation: The variable cells.ch0 takes values between 0 to 1195 for this event.
===========================================================================================
Cleaned documentation: volume_id takes these values - 7,8,9,12,13,14,16,17,18 hits.volume_id for some other events
Processed documentation: volume_id takes these values - 7,8,9,12,13,14,16,17,18 hits.
===========================================================================================
Cleaned documentation: More than half particles have positive charge in this event as compared to negatively charged particles particles.nhits
Processed documentation: More than half particles have positive charge in this event as compared to negatively charged particles particles.
===========================================================================================
Cleaned documentation: Using [imagededup]( Thanks a lot [@ebouteillon]( for advise to use `imagededup`. Good tool, fast and good precision "in box"!
Processed documentation: Using [imagededup]: Good tool, fast and good precision "in box"!
===========================================================================================
Cleaned documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_per_epoch=int(len(train_dataset) / batch_size),. pct_start=0.1,. anneal_strategy='cos',. final_div_factor=105
Processed documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_
===========================================================================================
Cleaned documentation: there is only one class. target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]] yxyx: be warning
Processed documentation: there is only one class. target['boxes'], yxyx: be warning.
===========================================================================================
Cleaned documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_per_epoch=int(len(train_dataset) / batch_size),. pct_start=0.1,. anneal_strategy='cos',. final_div_factor=105
Processed documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_
===========================================================================================
Cleaned documentation: Reverse Geocoder takes a list of tuples(of latitude and longitude) as its input.
Processed documentation: Reverse Geocoder takes a list of tuples as its input.
===========================================================================================
Cleaned documentation: pred_val_y = np.sum([outputs[i][0] weights[i] for i in range(len(outputs))], axis = 0)
Processed documentation: pred_val_y = np.sum([outputs[i] for i in range(len(outputs))], axis = 0)
===========================================================================================
Cleaned documentation: pred_test_y = np.sum([outputs[i][1] weights[i] for i in range(len(outputs))], axis = 0)
Processed documentation: pred_test_y = np.sum([outputs[i], weights[i] for i in range(len(outputs))], axis = 0)
===========================================================================================
Cleaned documentation: pred_val_y = np.sum([outputs[i][0] weights[i] for i in range(len(outputs))], axis = 0)
Processed documentation: pred_val_y = np.sum([outputs[i] for i in range(len(outputs))], axis = 0)
===========================================================================================
Cleaned documentation: pred_test_y = np.sum([outputs[i][1] weights[i] for i in range(len(outputs))], axis = 0)
Processed documentation: pred_test_y = np.sum([outputs[i], weights[i] for i in range(len(outputs))], axis = 0)
===========================================================================================
Cleaned documentation: df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis=1)
Processed documentation: df['Text'] = df.apply(lambda r: r['Text'], axis=1) df['Pronoun']: r.define('pronountarget'', axis: 1), axis: 'pronount',
===========================================================================================
Cleaned documentation: model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33)). param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}. model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(param_dist))
Processed documentation: model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier) param_dist = {'objective': 'binary:logistic','max_depth': 1, 'n_estim
===========================================================================================
Cleaned documentation: x_train = train[['ip', 'app', 'device', 'os', 'channel', 'click_time']]. x_test = test[['ip', 'app', 'device', 'os', 'channel', 'click_time']]
Processed documentation: x_train = train[['ip', 'app', 'device', 'os', 'channel', 'click_time']]. x_test = test[ ["ip',  'app', 'device', '
===========================================================================================
Cleaned documentation: fill up the missing values. Tokenize the sentences. Pad the sentences. Get the target values. shuffling the data
Processed documentation: fill up the missing values. Pad the sentences. Get the target values. shuffling the data.
===========================================================================================
Cleaned documentation: We have 3583 unique questions. WordCloud Question Title Question Body Answer Let's check which words are used most
Processed documentation: We have 3583 unique questions. Let's check which words are used most.
===========================================================================================
Cleaned documentation: We can clearly see the features that are correlated like 'question_type_instructions' and 'answer_type_instructions'. Let's see how:
Processed documentation: We can clearly see the features that are correlated like 'question_type_instructions' and 'answer_type instruction' Let's see how.
===========================================================================================
Cleaned documentation: S values from AGG & WRMSSE Evaluator:. array([3.26268315e+05, 5.14239651e+05, 5.17917913e+05, ...,. e-01, 6.98666667e-02, 2.81004710e-01]). Good match:
Processed documentation: S values from AGG & WRMSSE Evaluator:. array([3.26268315e+05, 5.14239651e-05,5.17917913e-01,...,.
===========================================================================================
Cleaned documentation: From nevus , we see that the moles are concentrated and not spread out .Showing no signs of malignant.
Processed documentation: From nevus, we see that the moles are concentrated and not spread out.Showing no signs of malignant.
===========================================================================================
Cleaned documentation: X = train['image_name']. y = train['target']. X_train,X_val = tts(train_x, test_size=0.2, random_state=1234). y_train = np.array(y_train)
Processed documentation: X = train['image_name']. y = train ['target']. X_train,X_val = tts(train_x, test_size=0.2, random_state=1234). y
===========================================================================================
Cleaned documentation: Using Random Forest Regressor. from sklearn.ensemble import RandomForestRegressor. regressor = RandomForestRegressor(n_estimators = 100, random_state = 0). regressor.fit(x_train, y_train)
Processed documentation: Using Random Forest Regressor. regressor = RandomForestRegressor(n_estimators = 100, random_state = 0). regressor.fit(x_train, y_train)
===========================================================================================
Cleaned documentation: Finding the cross validation score with 10 folds. from sklearn.model_selection import cross_val_score. scores = cross_val_score(regressor, x_train, y_train, cv=10). print(scores)
Processed documentation: Finding the cross validation score with 10 folds.
===========================================================================================
Cleaned documentation: Finding the cross validation score with 10 folds. from sklearn.model_selection import cross_val_score. scores = cross_val_score(regressor, x_train, y_train, cv=10). print(scores)
Processed documentation: Finding the cross validation score with 10 folds.
===========================================================================================
Cleaned documentation: Kaggle paths. PATHS for Features. test_path = f'{path}/dm_files'. AUX(pretrained) Models paths
Processed documentation: Kaggle paths. test_path = f'{path}/dm_ files'. AUX(pretrained) Models paths.
===========================================================================================
Cleaned documentation: test_preds = (np.expm1(preds)).numpy().squeeze() inv log(x)-1.0. df_test['sales_p']=test_preds. for day_id in range(29, 57):
Processed documentation: test_preds = (np.expm1(preds).numpy().squeeze() inv log(x)-1.0.0 for day_id in range(29, 57): df_test['
===========================================================================================
Cleaned documentation: train.drop('AvSigVersion', inplace=True, axis=1). test.drop('AvSigVersion', inplace=True, axis=1). categorical_columns.remove('AvSigVersion'). train.drop('Census_OSVersion', inplace=True, axis=1). test.drop('Census_OSVersion', inplace=True, axis=1). categorical_columns.remove('Census_OSVersion')
Processed documentation: train.drop('AvSigVersion', inplace=True, axis=1). categorical_columns.remove(' AvSig version') train. drop('Census_OS version') test.drop ('Census
===========================================================================================
Cleaned documentation: categorical_feature=categorical_columns. categorical_feature=categorical_columns. mean_gain = feature_importances[['gain', 'feature']].groupby('feature').mean(). feature_importances['mean_gain'] = feature_importances['feature'].map(mean_gain['gain']). temp = feature_importances.sort_values('mean_gain', ascending=False)
Processed documentation: categorical_feature=category_columns. categorical_ feature=categororical_Columns. mean_gain = feature_importances[['gain', 'feature']].groupby('feature').mean
===========================================================================================
Cleaned documentation: But, what about the possible coordiantes that point to the water? Let's plot the southwest area of the city.
Processed documentation: But, what about the possiblecoordiantes that point to water? Let's plot the southwest area of the city.
===========================================================================================
Cleaned documentation: Now lets derive some more directional features and also speed which will add to our model...
Processed documentation: Now lets derive some more directional features and also speed which will add to our model.
===========================================================================================
Cleaned documentation: ime": "is_month_end",. ims": "is_month_start",. dt.drop(["d", "wm_yr_wk", "weekday"], axis=1, inplace = True)
Processed documentation: ime": "is_month_end",. ims: "is-month_start",. dt.drop( ["d", "wm_yr_wk", "weekday"], axis=1, inplace =
===========================================================================================
Cleaned documentation: te_sub.loc[te.date >= fday+ timedelta(days=h), "id"] = te_sub.loc[te.date >= fday+timedelta(days=h),. id"].str.replace("validation$", "evaluation")
Processed documentation: te_sub.loc[te.date >= fday+timedelta(days=h),. id"].str.replace("validation$", "evaluation")
===========================================================================================
Cleaned documentation: fig,axes=plt.subplots(1,1,figsize=(15,8)). axes.set_ylabel("Accuracy"). axes.set_title("GLOVE results for 67% training and 23% testing"). results_df.plot(kind="bar",ax=axes)
Processed documentation: Fig. 1 shows the results for 67% training and 23% testing.
===========================================================================================
Cleaned documentation: fig,axes=plt.subplots(1,1,figsize=(15,8)). axes.set_ylabel("Accuracy"). axes.set_title("PROGRAM results for 67% training and 23% testing"). results_df.plot(kind="bar",ax=axes)
Processed documentation: The results are based on 67% training and 23% testing. The results are presented as a plot.
===========================================================================================
Cleaned documentation: fig,axes=plt.subplots(1,1,figsize=(15,8)). axes.set_ylabel("Accuracy"). axes.set_title("GOOGLE_NEWS results for 67% training and 23% testing"). results_df.plot(kind="bar",ax=axes)
Processed documentation: Fig.fig,axes=plt.subplots(1,1,figsize=(15,8), axes.set_ylabel("Accuracy"), axes. set_title("GOOGLE_
===========================================================================================
Cleaned documentation: Drawing wordcloud ... Using a section of data beacause of avoiding memory overflows...
Processed documentation: Drawing wordcloud... Using a section of data beacause of avoiding memory overflows.
===========================================================================================
Cleaned documentation: Counter(tmp_list). fig,ax = plt.subplots(1,1,figsize=(30,10)). ax.set_xticklabels(dict(Counter(tmp_list)).keys(),rotation=90). font = {'family' : 'normal',. weight' : 'bold',. size' : 10}. plt.rc('font', font). ax.bar(dict(Counter(tmp_list)).keys(),dict(Counter(tmp_list)).values())
Processed documentation: fig,ax = plt.subplots(1,1,figsize=(30,10)). ax.set_xticklabels(dict(Counter(tmp_list)).keys()rotation=90). font
===========================================================================================
Cleaned documentation: B. READING DATA Reading data and caughting a glimpse of what data it is.
Processed documentation: B. Reading data and caughting a glimpse of what data it is.
===========================================================================================
Cleaned documentation: J. VISIT_NUMBER Number of visits have profound potential to be an important factor in regression progress.
Processed documentation: Number of visits has profound potential to be an important factor in regression progress.
===========================================================================================
Cleaned documentation: Loading first stage submissions For comparison, I’ll upload the first stage submissions.
Processed documentation: For comparison, I’ll upload the first stage submissions.
===========================================================================================
Cleaned documentation: haxby dataset to have anatomical image, EPI images and masks. haxby_dataset = datasets.fetch_haxby(). localizer dataset to have contrast maps
Processed documentation: Haxby dataset to have anatomical image, EPI images and masks. haxby_dataset = datasets.fetch_haxby() localizer to have contrast maps.
===========================================================================================
Cleaned documentation: only the 27 apparel items, plus 1 for background. model image size 224x224. get and show categories. train dataframe
Processed documentation: only the 27 apparel items, plus 1 for background. model image size 224x224. get and show categories.
===========================================================================================
Cleaned documentation: use fastai's open_mask for an easier-to-view image (and check it works...)
Processed documentation: use fastai's open_mask for an easier-to-view image.
===========================================================================================
Cleaned documentation: Transaction data The count of sales transactions for each date, store_nbr combination. Only included for the training data timeframe.
Processed documentation: The count of sales transactions for each date, store_nbr combination. Only included for the training data timeframe.
===========================================================================================
Cleaned documentation: Store data Store metadata, including city, state, type, and cluster. cluster is a grouping of similar stores.
Processed documentation: Store metadata, including city, state, type, and cluster. cluster is a grouping of similar stores.
===========================================================================================
Cleaned documentation: train_items['Year'] = train_items['date'].apply(lambda x: int(str(x)[:4])). train_items['Month'] = train_items['date'].apply(lambda x: int(str(x)[5:7])). train_items['date'] = train_items['date'].apply(lambda x: (str(x)[8:]))
Processed documentation: train_items['Year'] = train_Items['date'].apply(lambda x: int(str(x)[:4])). train_ items['Month'] =Train_Items('month', 'year', 'date
===========================================================================================
Cleaned documentation: ANSWER The amount of halite mined per step R is therefore R(n_1,n_2,m,H,C)=\frac{C+(1-.75^m) H}{n_1+n_2+m}$$
Processed documentation: The amount of halite mined per step R is therefore R(n_1,n_2,m,H,C) R=\frac{C+(1-.75^m) H}{ n_1+n
===========================================================================================
Cleaned documentation: ANSWER The amount of halite mined per step R is therefore R(n_1,n_2,m,H)=\frac{(1-.75^m) H}{n_1+n_2+m}$$
Processed documentation: The amount of halite mined per step R is therefore R(n_1,n_2,m,H) R=\frac{(1-.75^m) H}{n 1+n 2+
===========================================================================================
Cleaned documentation: Locations of bars, restaurants, clubs, and pubs with at least 10 noise complaints in 2016.
Processed documentation: Locations of bars, restaurants, clubs, and pubs with least 10 noise complaints in 2016.
===========================================================================================
Cleaned documentation: Thanks @abhishek for his kernel Due to 'wheezy-copper-turtle-magic' is categorical feature, I decided to add one-hot encoding.
Processed documentation: 'wheezy-copper-turtle-magic' is categorical feature. I decided to add one-hot encoding.
===========================================================================================
Cleaned documentation: fill up the missing values. x_train, x_test, y_train, word_index = load_and_prec(). x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec()
Processed documentation: fill up the missing values. x_train, x_test, y_train,. word_index = load_and_prec()
===========================================================================================
Cleaned documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_per_epoch=int(len(train_dataset) / batch_size),. pct_start=0.1,. anneal_strategy='cos',. final_div_factor=105
Processed documentation: SchedulerClass = torch.optim.lr_scheduler.OneCycleLR. scheduler_params = dict(. max_lr=0.001,. epochs=n_epochs,. steps_
===========================================================================================
Cleaned documentation: embdedding setup. Source. Based on. GloVe is the most comprehensive word embedding
Processed documentation: GloVe is the most comprehensive word embedding tool in the world.
===========================================================================================
Cleaned documentation: regions seem to provide a robust solution for feature extraction ([c.f. this article ](
Processed documentation: regions seem to provide a robust solution for feature extraction. (c.f. this article)
===========================================================================================
Cleaned documentation: This Mickey is getting roughly 280 million in USD per movie released...😲 (time to get some shirts?)
Processed documentation: This Mickey is getting roughly 280 million in USD per movie released. (time to get some shirts?)
===========================================================================================
Cleaned documentation: Nick is getting roughly 290 million in USD per movie released...😲
Processed documentation: Nick is getting roughly 290 million in USD per movie released.
===========================================================================================
Cleaned documentation: Steven Spieberg is getting roughly 310 million in USD per movie released...😲
Processed documentation: Steven Spieberg is getting roughly 310 million in USD per movie released.
===========================================================================================
Cleaned documentation: car / lidar top coords. lidar_top_ego_pose_data = level5data.get('ego_pose', lidar_top_ego_pose_token). lidar_top_coords = np.array(lidar_top_ego_pose_data['translation'])
Processed documentation: .car / lidar top coords. lidar_top_ego_pose_data = level5data.get('ego' + 'token' + lidar_ top_coords.car).
===========================================================================================
Cleaned documentation: Generating forecasts Once the estimator is fully trained, we can generate predictions from it for the test values.
Processed documentation: Generating forecasts Once the estimator is fully trained, we can generate predictions from it.
===========================================================================================
Cleaned documentation: We then reshape the forecasts into the correct data shape for submission ...
Processed documentation: We then reshape the forecasts into the correct data shape for submission.
===========================================================================================
Cleaned documentation: Plotting sample predictions Finally, we can also visualize our predictions for some of the time series.
Processed documentation: We can also visualize our predictions for some of the time series.Plotting sample predictions.
===========================================================================================
Cleaned documentation: Generating forecasts Once the estimator is fully trained, we can generate predictions from it for the test values.
Processed documentation: Generating forecasts Once the estimator is fully trained, we can generate predictions from it.
===========================================================================================
Cleaned documentation: Plotting sample predictions Finally, we can also visualize our predictions for some of the time series.
Processed documentation: We can also visualize our predictions for some of the time series.Plotting sample predictions.
===========================================================================================
Cleaned documentation: Simplest model using X0 ;) public lb (outlier) : 0.55453 public lb (no outlier) : 0.55477
Processed documentation: Simplest model using X0 ;) public lb : 0.55453 public lb (no outlier) :0.55477.
===========================================================================================
Cleaned documentation: Start with the sample submission values. loop over each family. loop over each family choice
Processed documentation: Start with the sample submission values. loop over each family.
===========================================================================================
Cleaned documentation: It maybe some encoding about hexadecimal. but length 9 is uncomfortable...
Processed documentation: It maybe some encoding about hexadecimal. but length 9 is uncomfortable.
===========================================================================================
Cleaned documentation: PushOut + Median Stacking Pushout strategy is a bit agressive given what it does...
Processed documentation: PushOut + Median Stacking Pushout strategy is a bit agressive given what it does.
===========================================================================================
Cleaned documentation: MinMax + Mean Stacking MinMax seems more gentle and it outperforms the previous one given its peformance score.
Processed documentation: MinMax + Mean Stacking MinMax seems more gentle and it outperforms the previous one.
===========================================================================================
Cleaned documentation: PushOut + Median Stacking Pushout strategy is a bit agressive given what it does...
Processed documentation: PushOut + Median Stacking Pushout strategy is a bit agressive given what it does.
===========================================================================================
Cleaned documentation: MinMax + Mean Stacking MinMax seems more gentle and it outperforms the previous one given its peformance score.
Processed documentation: MinMax + Mean Stacking MinMax seems more gentle and it outperforms the previous one.
===========================================================================================
Cleaned documentation: Just another visualisation of the numerical variables present in the dataset..!
Processed documentation: Just another visualisation of the numerical variables present in the dataset.
===========================================================================================
Cleaned documentation: Peaks are comparatively bigger than the train set. Any implications? RENTA: Gross income of the household.
Processed documentation: Peaks are comparatively bigger than the train set. RENTA: Gross income of the household.
===========================================================================================
Cleaned documentation: Now let us look at the same statewise distribution in US map for better visual understanding.
Processed documentation: Now let us look at same statewise distribution in US map for better visual understanding.
===========================================================================================
Cleaned documentation: September month has the second highest number of proposals and the least acceptance rate of all the months.
Processed documentation: September has the second highest number of proposals and the least acceptance rate of all the months.
===========================================================================================
Cleaned documentation: Simple notebook to explore the variables present in the dataset. Please upvote if you find it useful :)
Processed documentation: Simple notebook to explore the variables present in the dataset.
===========================================================================================
Cleaned documentation: X0 to X8 are the categorical columns. Missing values: Let us now check for the missing values.
Processed documentation: X0 to X8 are the categorical columns. Let us now check for the missing values.
===========================================================================================
Cleaned documentation: Produce is the largest department. Now let us check the reordered percentage of each department. Department wise reorder ratio:
Processed documentation: Produce is the largest department. Now let us check the reordered percentage of each department.
===========================================================================================
Cleaned documentation: Personal care has lowest reorder ratio and dairy eggs have highest reorder ratio. Aisle - Reorder ratio:
Processed documentation: Personal care has lowest reorder ratio and dairy eggs have highest reorder ratios.
===========================================================================================
Cleaned documentation: Peaks are comparatively bigger than the train set. Any implications? RENTA: Gross income of the household. To update
Processed documentation: Peaks are comparatively bigger than the train set. RENTA: Gross income of the household.
===========================================================================================
Cleaned documentation: Logerror: Target variable for this competition is "logerror" field. So let us do some analysis on this field first.
Processed documentation: Target variable for this competition is "logerror" field. So let us do some analysis on this field first.
===========================================================================================
Cleaned documentation: We have 4 categorical features in our data display_address manager_id building_id listing_id So let us label encode these features.
Processed documentation: We have 4 categorical features in our data display_address manager_id building_id listing_id. Let us label encode these features.
===========================================================================================
Cleaned documentation: roc_auc_score(yvl,mNB.predict_proba(xvl)[:,1]) not for multi class. print(cv_score). print('Mean accuracy score',np.mean(cv_score)). del xtr,ytr,xvl,yvl,X_tf,X_test_tf
Processed documentation: print(cv_score). print('Mean accuracy score',np.mean(CV_score)). del xtr,ytr,xvl,yvl,X_tf, X_test_tf. print
===========================================================================================
Cleaned documentation: Stratified KFold Stratified KFold is used to keep the distribution of each label consistent for each training batch.
Processed documentation: Stratified KFold is used to keep the distribution of each label consistent for each training batch.
===========================================================================================
Cleaned documentation: Find elements from training sets not 'new_whale'. Dictionary of picture indices. Evaluate the model.. Generate the subsmission file.
Processed documentation: Find elements from training sets not 'new_whale' Dictionary of picture indices. Evaluate the model. Generate the subsmission file.
===========================================================================================
Cleaned documentation: Cabin Initial 이 확보된 전체 데이터의 양이 크지 않다는점을 고려해야 함. (특히 G, T의 경우)
Processed documentation: Cabin Initial    ‘’’ ‘”’  ”‘   “”  ’” ‘. ’ ” ”. ‘
===========================================================================================
Cleaned documentation: Parameter 설정. Random Forest. max_features":0.2,. Extra Trees. max_features":0.5,. AdaBoost. Gradient Boosting. max_features" : 0.2. SVC -Support Vector Classifier
Processed documentation: Parameter  "Random Forest. max_features" : 0.2, Extra Trees. max-features: 0.5,. AdaBoost. Gradient Boosting. max features : 0, SVC -
===========================================================================================
Cleaned documentation: Remove columns we do not need. Aggregate cases by date and country. Indexing with Time Series Data
Processed documentation: Remove columns we do not need. Aggregate cases by date and country.
===========================================================================================
Cleaned documentation: we can now check for card6. fig, ax = plt.subplots(). train.groupby('card6').plot(x='card6', y='TransactionAmt',ax=ax). credit card has more value
Processed documentation: we can now check for card6. fig, ax = plt.subplots(). train.groupby('card6').plot(x='card6', y='TransactionAmt',ax=ax). credit card has
===========================================================================================
Cleaned documentation: clf = CatBoostClassifier(task_type='GPU', eval_metric='AUC', loss_function='Logloss',. class_weights=[0.1, 0.9],. random_state=42, iterations=5000, od_type='Iter', od_wait=200, grow_policy='Lossguide', max_depth=8). clf.fit(X1, y)
Processed documentation: clf = CatBoostClassifier(task_type='GPU', eval_metric='AUC', loss_function='Logloss',. class_weights=[0.1, 0.9],. random_state=
===========================================================================================
Cleaned documentation: we can now check for card6. fig, ax = plt.subplots(). train.groupby('card6').plot(x='card6', y='TransactionAmt',ax=ax). credit card has more value
Processed documentation: we can now check for card6. fig, ax = plt.subplots(). train.groupby('card6').plot(x='card6', y='TransactionAmt',ax=ax). credit card has
===========================================================================================
Cleaned documentation: replace strange punctuations and raplace diacritics. remove_diacritics放前面会导致 'dont' 被处理为 'don t'
Processed documentation: replace strange punctuations and raplace diacritics. remove_diacritics: 'dont'
===========================================================================================
Cleaned documentation: convert r"[math]\vec{x} + \vec{y}" to English. edge case. post process for look up. init re
Processed documentation: convert r "[math]\vec{x} + \vec{y}" to English. post process for look up.
===========================================================================================
Cleaned documentation: Whyare' -> 'Why are'. why. How. what. why. How. which. where
Processed documentation: Whyare' -> 'Why are'. why. what. why. how. which. where.
===========================================================================================
Cleaned documentation: Remove columns we do not need. Aggregate cases by date and country. Indexing with Time Series Data
Processed documentation: Remove columns we do not need. Aggregate cases by date and country.
===========================================================================================
Cleaned documentation: z = Variable(torch.cuda.FloatTensor((np.random.normal(0, 1, (1, latent_dim))))). z = torch.randn(im_batch_size, latent_dim, device=device)
Processed documentation: z = Variable(torch.cuda.FloatTensor) z = torch.randn(im_batch_size, latent_dim, device=device)
===========================================================================================
Cleaned documentation: Examine M1-M9 The transaction data that has comple identity returns mostly NaN except for M4. Let's check it out:
Processed documentation: Examine M1-M9 The transaction data that has no identity returns mostly NaN except for M4. Let's check it out:
===========================================================================================
Cleaned documentation: Observation: Higher values of C3 associated with no-fraud. C5 and C9 are homogeneous columns. Examine D features
Processed documentation: Observation: Higher values of C3 associated with no-fraud. C5 and C9 are homogeneous columns.
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results Arguments: seed {int} -- Number of the seed. random.seed(seed). os.environ["PYTHONHASHSEED"] = str(seed)
Processed documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed. random.seed(seed). os.environ ["PYTHONHASHSEED"] = str(
===========================================================================================
Cleaned documentation: To download validation data on to your drive.... print img_url. print img_id. print label_id. print img_name. print img_name_actual. time.sleep(0.05)
Processed documentation: To download validation data on to your drive... print img_url. print label_id. print image_name. time.sleep(0.05)
===========================================================================================
Cleaned documentation: epsilon = 0.9. generating new samples. updating target network. if itr%save_model_every == 0:. print("Saving model"). dqn.save_weights(SAVE_DIR + 'dqn')
Processed documentation: epsilon = 0.9. generating new samples. updating target network. if itr%save_model_every == 0:. print("Saving model"). dqn.save_weights(SAVE_DIR
===========================================================================================
Cleaned documentation: Thanks [this kernel]( by [Julian]( for handling the columns with JSON data.
Processed documentation: Thanks [this kernel] for handling the columns with JSON data.
===========================================================================================
Cleaned documentation: Actions Actions can generate Halite commands. Most likely you do not need to write new actions.
Processed documentation: Actions can generate Halite commands. Most likely you do not need to write new actions.
===========================================================================================
Cleaned documentation: Strategies Strategies have mainly the task to implement `strategy.make_plans()`. They are stored as objects in a global variable.
Processed documentation: Strategies are stored as objects in a global variable. Strategies have mainly the task to implement strategy.make_plans()
===========================================================================================
Cleaned documentation: State updater We also need rules which decide which strategies to assign to new bots.
Processed documentation: State updater needs rules to decide which strategies to assign to new bots.
===========================================================================================
Cleaned documentation: Function to predict for a single image or folder of images. FINDING INPUT IMAGES. Searching folder for images
Processed documentation: Function to predict for a single image or folder of images. Searching folder for images.
===========================================================================================
Cleaned documentation: Function to predict for a single image or folder of images. FINDING INPUT IMAGES. Searching folder for images
Processed documentation: Function to predict for a single image or folder of images. Searching folder for images.
===========================================================================================
Cleaned documentation: Function to predict for a single image or folder of images. FINDING INPUT IMAGES. Searching folder for images
Processed documentation: Function to predict for a single image or folder of images. Searching folder for images.
===========================================================================================
Cleaned documentation: pip install pretrainedmodels <- need internet connection. Ref: for installation. pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/
Processed documentation: pip install pretrainedmodels <- need internet connection. Ref: for installation. pip install../ input/pretrainedmodels/pret trainedmodels-0.7.4.
===========================================================================================
Cleaned documentation: Read in the data CSV files. train = pd.read_csv(datadir/'train.csv'). test = pd.read_csv(datadir/'test.csv'). sample_submission = pd.read_csv(datadir/'sample_submission.csv'). class_map = pd.read_csv(datadir/'class_map.csv')
Processed documentation: Read in the data CSV files. train = pd.read_csv(datadir/'train.csv') test = pD. read_ CSV( datadir /'test. CSV') sample_submission
===========================================================================================
Cleaned documentation: Test data We can find multiple segments of sequences in the test folder. Let's peek at their names:
Processed documentation: We can find multiple segments of sequences in the test folder. Let's peek at their names.
===========================================================================================
Cleaned documentation: Embeddings To start we'll just take the FastText Common Crawl embeddings. Later, we'll hopefully combine multiple embeddings.
Processed documentation: Embeddings. We'll use the FastText Common Crawl embeddings. Later, we'll hopefully combine multiple embeds.
===========================================================================================
Cleaned documentation: Series全体に含まれる全ての単語とその個数を集計する関数. The function to count all words and the number of those through overall texts
Processed documentation: Series. The function to count all words and the number of those through overall texts.
===========================================================================================
Cleaned documentation: The function to select useful words based on probability of binomial distribution. print(binom.cdf(k, N, p)). if (binom.cdf(k, N, p)(1-significance_level/2)):
Processed documentation: The function to select useful words based on probability of binomial distribution. print(binom.cdf(k, N, p), 1-significance_level/2)
===========================================================================================
Cleaned documentation: df_result_aggs, df_result_filter_aggs = aggregation(df_result, plot=True). valid = merge(df_result, df_result_aggs, df_result_filter_aggs). print(scoring(valid))
Processed documentation: df_result_aggs, df_result-filter-aggs = aggregation(df_ result, plot=True), valid = merge(DF_result), scoring = scoring(valid), plot = plot, valid
===========================================================================================
Cleaned documentation: df = px.data.gapminder(). px.scatter(df, x="gdpPercap", y="lifeExp", animation_frame="year", animation_group="country",. size="pop", color="continent", hover_name="country",. log_x=True, size_max=55, range_x=[100,100000], range_y=[25,90])
Processed documentation: df = px.data.gapminder(). pxscatter(df, x="gdpPercap", y="lifeExp", animation_frame="year," animation_group="country",. size="
===========================================================================================
Cleaned documentation: interactive plot is too heavy. fig = go.Figure(). fig.add_trace(go.Histogram(x=test_hu.flatten(), nbinsx=80, histnorm='percent')). fig.update_layout(title='Hounsfield Units(HU) of Patient ID00007637202177411956430',. width=800,. height=600)
Processed documentation: interactive plot is too heavy. fig = go.Figure(). fig.add_trace(go.Histogram(x=test_hu.flatten() nbinsx=80, histnorm='percent')). fig
===========================================================================================
Cleaned documentation: if A.(bd8f989f1, 29ab304b9, 8dc7f1eb9) eq B.(a75d400b8, 1d9078f84, 7d287013b) then B.target is A.eeb9cd3aa
Processed documentation: if A. eq B. (a75d400b8, 1d9078f84, 7d287013b) then B.target is A.eeb9cd3aa.
===========================================================================================
Cleaned documentation: VISUALIZTION. Plotting Metric. plt.plot([-x for x in model.history['train']['metric']]). plt.plot([-x for x in model.history['valid']['metric']])
Processed documentation: VISUALIZTION. plt.plot([-x for x in model.history['train'].'metric'].plt. Plotting Metric. Plt. plot('metric', 'train', '
===========================================================================================
Cleaned documentation: The distribution of recipe length is right-skewed as we can see from the histogram above.
Processed documentation: The distribution of recipe length is right-skewed as we can see from the histogram.
===========================================================================================
Cleaned documentation: KERAS MODEL DEFINITION. params. Inputs. Embeddings layers. rnn layer. main layer. output. model
Processed documentation: KERAS MODEL DEFINITION. Inputs. Embeddings layers. rnn layer. main layer. output. model.
===========================================================================================
Cleaned documentation: sns.distplot([polarity['neg'] for polarity in polarity_0], color='darkorange'). sns.distplot([polarity['neg'] for polarity in polarity_1], color='purple'). plt.show()
Processed documentation: sns.distplot([polarity['neg'] for polarity in polarity_0], color='darkorange'). sns.dist Plotting() shows how polarity changes over time. plt.show()
===========================================================================================
Cleaned documentation: pitch The pitch describes the change in frequency of the bird's call over time (increasing, descreasing, etc).
Processed documentation: The pitch describes the change in frequency of the bird's call over time.
===========================================================================================
Cleaned documentation: channels The channels indicates the number of channels in the audio clip (1: mono or 2: stereo).
Processed documentation: channels indicates the number of channels in the audio clip (1: mono or 2: stereo).
===========================================================================================
Cleaned documentation: Define functions to calculate melspectrogram features Below we define some functions to calculate the melspectrogram features from audio signals.
Processed documentation: Define functions to calculate melspectrogram features from audio signals.
===========================================================================================
Cleaned documentation: Define categorical cross entropy and accuracy Next, we define our loss function (categorical cross entropy) and evaluation metric (accuracy).
Processed documentation: Define categorical cross entropy and accuracy. Next, we define our loss function (categoricalCrossEntropy) and evaluation metric (accuracy)
===========================================================================================
Cleaned documentation: The KDE plot has highest density (darkness) around an almost vertical line.
Processed documentation: The plot has highest density (darkness) around an almost vertical line.
===========================================================================================
Cleaned documentation: Now, I will look at how the xy-coordinate distribution of players is different for different offense formations.
Processed documentation: Now, I will look at how the xy-coordinate distribution is different for different offense formations.
===========================================================================================
Cleaned documentation: scaler = MinMaxScaler(feature_range=(-1, 1)). scaler.fit(np.concatenate([train_features, test_features], axis=0)). train_features = scaler.transform(train_features). test_features = scaler.transform(test_features)
Processed documentation: scaler = MinMaxScaler(feature_range=(-1, 1)). scaler.fit(np.concatenate([train_features, test_features], axis=0)). train_features = scaler
===========================================================================================
Cleaned documentation: Map 0 Target values → {age: 57.44, domain1_var1: 30.57, domain1_var2: 62.55, domain2_var1: 55.33, domain2_var2: 51.43}
Processed documentation: Map 0 Target values → {age: 57.44, domain1_var1: 30.57, domain 1: 62.55, domain2: 55.33, domain 2: 51.43}
===========================================================================================
Cleaned documentation: Map 1 Target values → {age: 59.58, domain1_var1: 50.97, domain1_var2: 67.47, domain2_var1: 60.65, domain2_var2: 58.31}
Processed documentation: Map 1 Target values → {age: 59.58, domain1_var1: 50.97,Domain1: 67.47, domain2: 60.65,Domain2: 58.31}
===========================================================================================
Cleaned documentation: Map 2 Target values → {age: 71.41, domain1_var1: 53.15, domain1_var2: 58.01, domain2_var1: 52.42, domain2_var2: 62.54}
Processed documentation: Map 2 Target values → {age: 71.41, domain1_var1: 53.15, domain 1_var2: 58.01, domain 2: 52.42, domain2: 62.54,
===========================================================================================
Cleaned documentation: Target vs. Proportion (per anatom_site_general_challenge) Next, let us look at how anatom_site_general_challenge affects the target.
Processed documentation: Target vs. Proportion (per anatom_site_general_challenge)
===========================================================================================
Cleaned documentation: Stochastic Gradient Descent and Random Search Random search is a random (obviously) search over specified parameter values.
Processed documentation: Stochastic Gradient Descent and Random Search Random search is a random search over specified parameter values.
===========================================================================================
Cleaned documentation: Finding most common words. Define function to cleanup text by removing personal pronouns, stopwords, and puncuation
Processed documentation: Finding most common words. Define function to cleanup text by removing personal pronouns, stopwords, and puns.
===========================================================================================
Cleaned documentation: Sub file We have to create our sub file by concatenating both holdout and test names.
Processed documentation: Sub file is created by concatenating both holdout and test names.
===========================================================================================
Cleaned documentation: ensembles. kfold = KFold(n_splits=num_folds, random_state=seed). cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring). results.append(cv_results). names.append(name)
Processed documentation: KFold is a type of image-folding tool. It can be used to store data from a model or a training set.
===========================================================================================
Cleaned documentation: params_cat = {'task_type': "CPU",'iterations':1000,'learning_rate':0.1,'random_seed': 42,'depth':2}. result_dict_cat = train_model_classification(X=X[0:5000008-1], X_test=X_test, y=y[0:5000008-1], params=params_cat, model_type='cat', eval_metric='f1score', plot_feature_importance=False)
Processed documentation: params_cat = {'task_type': "CPU','iterations':1000,'learning_rate':0.1,'random_seed': 42,'depth':2}. result_dict_cat is train_model_
===========================================================================================
Cleaned documentation: Batch 5 We may have lost some signal when filtering on this batch.
Processed documentation: Batch 5 may have lost some signal when filtering this batch.
===========================================================================================
Cleaned documentation: Batch 7 This looks a lot like Batch 3 where it is clear that filtering helped reduce noise.
Processed documentation: Batch 7 looks a lot like Batch 3 where it is clear that filtering helped reduce noise.
===========================================================================================
Cleaned documentation: Use the filtered signal for train 'signal'. Apply Kalman filter to test data and set as test 'signal'
Processed documentation: Use the filtered signal for train'signal' Apply Kalman filter to test data and set as test'signal'
===========================================================================================
Cleaned documentation: Lemmatization converted it back to list, so change to str again and remove the unncessary words.
Processed documentation: Lemmatization converted it back to list. Change to str again and remove the unncessary words.
===========================================================================================
Cleaned documentation: from sklearn.svm import LinearSVC. linearsvm = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500). linearsvm.fit(X_train, y_train)
Processed documentation: from sklearn.svm import LinearSVC. linearsvm = LinearS VC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False,
===========================================================================================
Cleaned documentation: function that creates more features. df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std'). df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std'). call feature engineering function
Processed documentation: function that creates more features. call feature engineering function that creates new features. df['installation_duration_std'] = df.groupby('installation', 'duration','std')
===========================================================================================
Cleaned documentation: null_data = pd.DataFrame(X_train.isnull().sum()/X_train.shape[0]100). null_data = pd.DataFrame(). null_data = pd.concat([pd.DataFrame(X_train.isnull().sum()/X_train.shape[0]100 ,columns=['train']) ,pd.DataFrame(X_test.isnull().sum()/X_test.shape[0]100,columns=['test']) ] ,axis = 1).reset_index(). null_data.head()
Processed documentation: null_data = pd.DataFrame(X_train.isnull().sum()/X_ train.shape[0]100). null_data.concat (X_test.is null%). null_ data
===========================================================================================
Cleaned documentation: columns_drop = list(set(columns_drop)). X_train.drop(columns = columns_drop , inplace = True). X_test.drop(columns = columns_drop , inplace = True)
Processed documentation: columns_drop = list(set( columns_drop), inplace = True). X_train.drop(columns = columns_ drop, inplaces = True), X_test.drop('columns', columns_
===========================================================================================
Cleaned documentation: Looks like it's the case. Google Trends shows high traffic associated with Shopping related keywords during Black Friday season.
Processed documentation: Google Trends shows high traffic associated with Shopping related keywords during Black Friday season.
===========================================================================================
Cleaned documentation: Models for hierarchical image generation.. encoders = [QuarterEncoder(3, 128, 512), HalfEncoder(128, 128, 512)]. decoders = [HalfDecoder(128, 128), HalfQuarterDecoder(128, 3)]
Processed documentation: Models for hierarchical image generation.. encoders = [QuarterEncoder(3, 128, 512), HalfEncoder (128,128, 512)]. decoders mean [HalfDecoder(128, 128), Half
===========================================================================================
Cleaned documentation: model = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.1, num_leaves=8, subsample=0.6, colsample_bytree=0.6). model.fit(X, y). y_test_preds_lgb = model.predict_proba(X_test)[:, 1]
Processed documentation: model = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.1, num_leaves=8, subsample= 0.6, colsample_bytree
===========================================================================================
Cleaned documentation: x = GaussianNoise(self.gaussian_noise)(inp). d1 = TimeDistributed(Dropout(0.2))(d1). d2 = PReLU()(d2). d2 = TimeDistributed(Dropout(0.2))(d2)
Processed documentation: x = GaussianNoise(self.gaussian_noise)(inp). d1 = TimeDistributed(Dropout(0.2))(d1). d2 = PReLU()(d2).
===========================================================================================
Cleaned documentation: scaler = StandardScaler(). if len(train[col].cat.categories) > 10:. print ('column `{}` has more than ten unique levels'.format(col))
Processed documentation: scaler = StandardScaler(). if len(train[col].cat.categories) > 10:. print ('column `{}` has more than ten unique levels'
===========================================================================================
Cleaned documentation: Statistics for the given data We will use the ```textstat``` package by kaggler Shivam Bansal(@shivamb) for this purpose.
Processed documentation: Statistics for the given data. We will use the ```textstat``` package by kaggler Shivam Bansal(@shivamb)
===========================================================================================
Cleaned documentation: Xgboost. print(accuracy_score(clf.predict(X_val), y_val)). more functions from LightGBM baseline:. checking the accuracy of the model. print(rfc.score(X_val, y_val))
Processed documentation: Xgboost. print(accuracy_score(clf.predict(X_val), y_val)). more functions from LightGBM baseline:. checking the accuracy of the model. print (rfc.score
===========================================================================================
Cleaned documentation: Loading embeddings In this v2, I am using GloVe only, for memory usage purposes mostly.
Processed documentation: In this v2, I am using GloVe only, for memory usage purposes mostly.
===========================================================================================
Cleaned documentation: Not a lot of contractions are known. (FastText knows none) We use the map to replace them
Processed documentation: Not a lot of contractions are known. We use the map to replace them.
===========================================================================================
Cleaned documentation: Submission Let us now train a model on the entire train set. We will gain a bit with that.
Processed documentation: Let us now train a model on the entire train set. We will gain a bit with that.
===========================================================================================
Cleaned documentation: Nearest Neighbour Starting from the North Pole, travel to the nearest city (without concern for prime-ness of a city).
Processed documentation: Starting from the North Pole, travel to the nearest city (without concern for prime-ness of a city).
===========================================================================================
Cleaned documentation: Feature engineering In this notebook I'd like to expand [@eikedehling search for features]( and in part revise his findings.
Processed documentation: Feature engineering notebook: expand [@eikedehling search for features] and revise his findings.
===========================================================================================
Cleaned documentation: plt.scatter(x[train.shape[0]:,0],x[train.shape[0]:,1], cmap=cm, marker='.', s=15, label='test'). plt.title('t-SNE embedding of train & test data', fontsize=20)
Processed documentation: plt.scatter('t-SNE embedding of train & test data', fontsize=20), cmap=cm, marker='.', s=15, label='test'), plt.title('t.
===========================================================================================
Cleaned documentation: Define face detection pipeline. Get all test videos. Load frames and find faces. Calculate embeddings
Processed documentation: Define face detection pipeline. Get all test videos. Load frames and find faces. Calculate embeds.
===========================================================================================
Cleaned documentation: Full resolution detection In this example, we demonstrate how to detect faces using full resolution frames (i.e., `resize=1`).
Processed documentation: Full resolution detection. We demonstrate how to detect faces using full resolution frames (i.e., `resize=1`)
===========================================================================================
Cleaned documentation: Half resolution detection In this example, we demonstrate how to detect faces using half resolution frames (i.e., `resize=0.5`).
Processed documentation: Half resolution detection. We demonstrate how to detect faces using half resolution frames (i.e., `resize=0.5`).
===========================================================================================
Cleaned documentation: file = open("results_{:%Y-%m-%d %H:%M:%S}.txt".format(datetime.datetime.now()),"w"). print(hyperopt.space_eval(space, best), file = file). print(trials.best_trial, file = file). file.close()
Processed documentation: file = open("results,"w") print(hyperopt.space_eval(space, best), file = file). print(trials.best_trial, file =file). file.close()
===========================================================================================
Cleaned documentation: I borrow some code from [pytorch-pretrained-BERT/example]( and [[PyTorch] BERT Baseline (Public Score ~ 0.54)](
Processed documentation: I borrow some code from [pytorch-pretrained-BERT/example] and [[PyTorch] BERT Baseline (Public Score ~ 0.54)]
===========================================================================================
Cleaned documentation: The main model.. Only use the bottom n layers. Add BertLayer(s). input_ids, offsets, label_ids = batch. label_ids = batch[-1]
Processed documentation: The main model.. Only use the bottom n layers. Add BertLayer(s). input_ids, offsets, label_ids = batch[-1]
===========================================================================================
Cleaned documentation: You can pass the and process images directly with help of ImageDataGenerator of tensorflow utilizing Argument preprocessing_function tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=crop_and_zoom)
Processed documentation: You can pass the and process images directly with help of ImageDataGenerator of tensorflow utilizing Argument preprocessing_function.
===========================================================================================
Cleaned documentation: Error is measured as categorical crossentropy or multiclass logloss. Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam
Processed documentation: Error measured as categorical crossentropy or multiclass logloss. Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam
===========================================================================================
Cleaned documentation: Prepare results for Submission The total CV score matches Kaggle's score pretty closely.
Processed documentation: The total CV score matches Kaggle's score pretty closely.
===========================================================================================
Cleaned documentation: Let's join in information from `resources.csv`. We'll use [DataFrame.merge]( (which corresponds to a [SQL JOIN]( operation).
Processed documentation: Let's join in information from `resources.csv`. We'll use [DataFrame.merge], which corresponds to a [SQL JOIN] operation.
===========================================================================================
Cleaned documentation: This matirx is $(\textbf{a}_1, \textbf{a}_2, \textbf{a}_3)^t$ where $\textbf{a}_i$ is $i$-th lattice vector.
Processed documentation: This matirx is $i$-th lattice vector. $i_i$ is the length of the lattice.
===========================================================================================
Cleaned documentation: Geting the codes of ordinal categoy's - train. Geting the codes of ordinal categoy's - test
Processed documentation: Geting the codes of ordinal categoy's - train.
===========================================================================================
Cleaned documentation: Allright! There is only ord_5 left. This feature is ordered by letters. Recap: ord_5 : 192 uniques
Processed documentation: Allright! There is only ord_5 left. This feature is ordered by letters. Ord_5: 192 uniques.
===========================================================================================
Cleaned documentation: Display a histogram of the FVC of the training data. Graph Title. Show Histogram
Processed documentation: Display a histogram of the FVC of the training data.
===========================================================================================
Cleaned documentation: Combine the Patient ID and Week columns. Confirming the converted value
Processed documentation: Combine the Patient ID and Week columns. Confirm the converted value.
===========================================================================================
Cleaned documentation: Let's visualize the FVC of Training Data. plt.figure(figsize=(18,6)). plt.plot(train_x5["FVC"], label = "Train_Data"). plt.legend()
Processed documentation: Let's visualize the FVC of Training Data. plt.figure(figsize=(18,6), label = "Train_Data"), legend = "FVC"
===========================================================================================
Cleaned documentation: span]( Ths span (Omega) describes the magnitude of the shielding anisotropy.
Processed documentation: Ths span (Omega) describes the magnitude of the shielding anisotropy.
===========================================================================================
Cleaned documentation: skew]( The skew (kappa) describes degree of axial symmetry of the shielding tensor.
Processed documentation: Skew describes degree of axial symmetry of the shielding tensor.
===========================================================================================
Cleaned documentation: Setting all to dtype float32. Defining categorical variables. Setting categorical variables to int64
Processed documentation: Setting all to dtype float32. Defining categorical variables to int64.
===========================================================================================
Cleaned documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.1,decay_steps=50000,decay_rate=0.9)
Processed documentation: sigma_clip = sigma + C1. Pinball loss for multiple quantiles. lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.
===========================================================================================
Cleaned documentation: train_gen = CovidGenerator(X_node, As, y,train.signal_to_noise, 64, shuffle=True, min_crop=68, crop_length=91, crop_ratio=1.0, left_padd=0). kk = train_gen.__getitem__(0). kk[0][0].shape
Processed documentation: train_gen = CovidGenerator(X_node, As, y,train.signal_to_noise, 64, shuffle=True, min_crop=68, crop_length=91, crop
===========================================================================================
Cleaned documentation: for i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):. if i<= config['skip_fold']:. print('config[skip_fold]',config['skip_fold']). continue. print('val=train[va_idx]', val.shape). crop_ratio=config['crop_ratio']min_crop=config['min_crop'],
Processed documentation: for i, (tr_idx, va_idX) in enumerate(kfold.split(X_node, As)):. print('config[skip_fold]',config[' skip_fold']).
===========================================================================================
Cleaned documentation: configs.append(dict(use_seper=True, inner_dropout = 0.2, last_dropout=0.0. crop_ratio=0.3, last_forward_n=512, min_crop=68, last_layer_n =1)). profiles.append('gnncnnpaddcrop_'+ dict_to_path(configs[-1])). sub_df = train_and_predict(save_dir, config, skip_ae=True, foldid=None, trainskip=True)
Processed documentation: configs.append('gnncnnpaddcrop_'+ dict_to_path(configs[-1])). sub_df = train_and_predict(save_dir, config, skip_ae
===========================================================================================
Cleaned documentation: KERAS MODEL DEFINITION. params. Inputs. Embeddings layers. rnn layer. main layer. output. model
Processed documentation: KERAS MODEL DEFINITION. Inputs. Embeddings layers. rnn layer. main layer. output. model.
===========================================================================================
Cleaned documentation: tr_data = lgb.Dataset(tr_x, label = tr_y, categorical_feature = categorical). vl_data = lgb.Dataset(vl_x, label = vl_y, categorical_feature = categorical)
Processed documentation: tr_ data = lgb.Dataset(tr_x, label = tr_y, categorical_feature = categorical). vl_data = lGB. dataset (vl_x,. vl
===========================================================================================
Cleaned documentation: history = aggregate_transactions(historical_transactions)history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]history[:5]
Processed documentation: history = aggregate_transactions(historical_transaction)history.columns = ['hist_' + c if c!= 'card_id' else c for c in history.Columns]
===========================================================================================
Cleaned documentation: authorized = aggregate_transactions(authorized_transactions)authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]authorized[:5]
Processed documentation: authorized = aggregate_transactions(authorized_transaction)authorized.columns = ['auth_' + c if c!= 'card_id' else c for c in authorized.Columns]authorized[:5]
===========================================================================================
Cleaned documentation: historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])historical_transactions = reduce_mem_usage(historical_transactions)new_transactions = reduce_mem_usage(new_transactions)
Processed documentation: historical_transactions = pd.get_dummies(historical.transactions, columns=['category_2', 'category_3'])new_transaction = pD. get_dummy(new
===========================================================================================
Cleaned documentation: history = aggregate_transactions(historical_transactions)history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]history[:5]
Processed documentation: history = aggregate_transactions(historical_transaction)history.columns = ['hist_' + c if c!= 'card_id' else c for c in history.Columns]
===========================================================================================
Cleaned documentation: Check for missing values in training set. print(info). print("There are", len(nullcols), "columns with missing values in test set")
Processed documentation: Check for missing values in training set. print(info). print("There are", len(nullcols), "columns with missing values"
===========================================================================================
Cleaned documentation: Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded
Processed documentation: Impute any values will significantly affect the RMSE score for test set. Imputations have been excluded.
===========================================================================================
Cleaned documentation: train = train[featues_to_use]. test = test[featues_to_use]. classifier = LogisticRegression(C=1, solver='sag'). cv_score = np.mean(cross_val_score(classifier, train, target, cv=3, scoring='roc_auc')). print(cv_score)
Processed documentation: train = train[featues_to_use]. test = test[featuees_to-use]. classifier = LogisticRegression(C=1, solver='sag'). cv_score =
===========================================================================================
Cleaned documentation: submission['target'] = pred_lrsubmission.to_csv('submission_0.csv', index=False)submission['target'] = 0.9pred_lr + 0.1pred_lgbsubmission.to_csv('submission.csv', index=False)submission['target'] = 0.8pred_lr + 0.2pred_lgbsubmission.to_csv('submission_2.csv', index=False)
Processed documentation: submission['target'] = 0.9pred_lr + 0.1pred_lgbsubmission.to_csv('submission.csv', index=False)submission ['target']: 0.8pred
===========================================================================================
Cleaned documentation: cv_prediction = cross_val_predict(classifier, train_features, train_target, cv=3, method='predict_proba'). predictions.append(cv_prediction[:, 1]). print('CV score for class {} is {}'.format(i, cv_score))
Processed documentation: cv_prediction = cross_val_predict(classifier, train_features, train target, cv=3, method='predict_proba'). predictions.append(cv_Prediction[:, 1
===========================================================================================
Cleaned documentation: cv_prediction = cross_val_predict(classifier, train_features, train_target, cv=3, method='predict_proba'). predictions_2.append(cv_prediction[:, 1]). print('CV score for class {} is {}'.format(i, cv_score))
Processed documentation: cv_prediction = cross_val_predict(classifier, train_features, train target, cv=3, method='predict_proba'). predictions_2.append(cv_Prediction[
===========================================================================================
Cleaned documentation: cv_prediction = cross_val_predict(classifier, train_word_features, train_target, cv=3, method='predict_proba'). predictions_words.append(cv_prediction[:, 1]). print('CV score for class {} is {}'.format(i, cv_score))
Processed documentation: cv_prediction = cross_val_predict(classifier, train_word_features, train-target, cv=3, method='predict_proba') predictions_words.append(cv_
===========================================================================================
Cleaned documentation: cv_prediction = cross_val_predict(classifier, train_char_features, train_target, cv=3, method='predict_proba'). predictions_chars.append(cv_prediction[:, 1]). print('CV score for class {} is {}'.format(i, cv_score))
Processed documentation: cv_prediction = cross_val_predict(classifier, train_char_features, train-target, cv=3, method='predict_proba') predictions_chars.append(cv_
===========================================================================================
Cleaned documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred)). spearman_corr. score += roc_auc_score(val_target, val_pred)/n_splits
Processed documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred). spearman_corr. score += roc _auc_
===========================================================================================
Cleaned documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred)). score += roc_auc_score(val_target, val_pred)/n_splits
Processed documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred). score += roc auc_ score(val target,
===========================================================================================
Cleaned documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred)). spearman_corr. score += roc_auc_score(val_target, val_pred)/n_splits. print(test_preds_1.max()). print(test_preds_1.min())
Processed documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred). spearman_corr. score += roc auc_
===========================================================================================
Cleaned documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred)). score += roc_auc_score(val_target, val_pred)/n_splits
Processed documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred). score += roc auc_ score(val target,
===========================================================================================
Cleaned documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred)). score += roc_auc_score(val_target, val_pred)/n_splits
Processed documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred). score += roc auc_ score(val target,
===========================================================================================
Cleaned documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred)). score += roc_auc_score(val_target, val_pred)/n_splits
Processed documentation: print("Fitting fold", jj+1). print("Fold auc:", roc_auc_score(val_target, val_pred). score += roc auc_ score(val target,
===========================================================================================
Cleaned documentation: To do: Meke some plots Build a few models Do feature importance analysis To be continued ...
Processed documentation: To do: Meke some plots Build a few models Do feature importance analysis.
===========================================================================================
Cleaned documentation: Plot feature importance (from [here][1]) Interesting, it looks like by reindexing the data, the feature importance has changed.
Processed documentation: It looks like by reindexing the data, the feature importance has changed.Plot feature importance (from [here][1])
===========================================================================================
Cleaned documentation: POLY_COLS = ['t1']calendar['t1'] = calendar.n_week / 300MAX_DEG = 10for deg in range(2, MAX_DEG+1): calendar[f't{deg}'] = calendar['t1']deg POLY_COLS.append(f't{deg}')
Processed documentation: POLY_COLS = ['t1']calendar = calendar.n_week / 300MAX_DEG = 10for deg in range(2, MAX_Deg+1): calendar[f't{deg}
===========================================================================================
Cleaned documentation: import tensorflow.keras.backend as K. import tensorflow.keras.layers as L. import tensorflow.keras.models as M
Processed documentation: import tensorflow.keras.backend as K. import tensor flow.layers as L. import TensorFlow.ker as M.
===========================================================================================
Cleaned documentation: time. clf = ElasticNet(alpha=0.3, l1_ratio = 0.7). clf = RandomForestClassifier(max_depth=4, random_state=777, n_estimators=50). print("predict val..."). print("predict test...")
Processed documentation: clf = ElasticNet(alpha=0.3, l1_ratio = 0.7), RandomForestClassifier(max_depth=4, random_state=777, n_estimators=50).
===========================================================================================
Cleaned documentation: Resulting dataframe can be used as stage 2 training file with `[3689, 4] ` shape.
Processed documentation: Dataframe can be used as stage 2 training file with `[3689, 4] ` shape.
===========================================================================================
Cleaned documentation: grid mask augmentation. for way one - data generator. training generator. validation generator: no shuffle , not augmentation
Processed documentation: grid mask augmentation. for way one - data generator. training generator. validation generator: no shuffle, not augmented.
===========================================================================================
Cleaned documentation: Define time series analysis function. Stats model. Visualization. raw price data. trend. seasonaly. residual. distribution. auto correlation
Processed documentation: Define time series analysis function. Stats model. Visualization raw price data. trend. seasonaly. residual distribution. auto correlation
===========================================================================================
Cleaned documentation: temp['isFraud'] = 0.35sub1['isFraud'] + 0.30sub2['isFraud'] + 0.25sub3['isFraud'] + 0.10sub4['isFraud']. temp.to_csv('submission8.csv', index=False )
Processed documentation: temp.temp['isFraud'] = 0.35sub1.sub2.sub3.sub4.sub5.sub6.sub7.sub8.sub9.sub10.sub11.sub12
===========================================================================================
Cleaned documentation: with open(mels['train_curated'], 'rb') as curated, open(mels['train_noisy'], 'rb') as noisy:. x_train = pickle.load(curated). x_train.extend(pickle.load(noisy))
Processed documentation: with open(mels['train_curated'], 'rb') as curated, open('train_noisy') as noisy:. x_train = pickle.load(curated). x_ train.
===========================================================================================
Cleaned documentation: model.load_weights('../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5'). Code above loads pre-trained data and. Learning rate is changed to 0.001
Processed documentation: model.load_weights('../ input/vgg16/vGG16_weights_tf_dim_ordering_tf-kernels.h5') Code above loads pre-trained data and. Learning rate is changed to
===========================================================================================
Cleaned documentation: Comparison of the all feature importance diagrams Back to Table of Contents](0.1)
Processed documentation: Comparison of the all feature importance diagrams Back to Table of Contents.
===========================================================================================
Cleaned documentation: Comparison of all options of selected features Back to Table of Contents](0.1)
Processed documentation: Comparison of all options of selected features Back to Table of Contents.
===========================================================================================
Cleaned documentation: Relative error between predicted y_pred and measured y_meas values. RMSE between predicted y_pred and measured y_meas values
Processed documentation: Relative error between predicted y_pred and measured y_meas values.
===========================================================================================
Cleaned documentation: Commit 4 Dropout_model = 0.36 FVC_weight = 0.25 Confidence_weight = 0.26 LB = -6.8105
Processed documentation: Commit 4 Dropout_model = 0.36 FVC_weight =0.25 Confidence_weight=0.26 LB = -6.8105
===========================================================================================
Cleaned documentation: Commit 6 Dropout_model = 0.36 FVC_weight = 0.35 Confidence_weight = 0.36 LB = -6.8158
Processed documentation: Commit 6 Dropout_model = 0.36 FVC_weight =0.35 Confidence_weight=0.36 LB = -6.8158
===========================================================================================
Cleaned documentation: Commit 8 Dropout_model = 0.35 FVC_weight = 0.25 Confidence_weight = 0.26 LB = -6.8107
Processed documentation: Commit 8 Dropout_model = 0.35 FVC_weight =0.25 Confidence_weight=0.26 LB = -6.8107
===========================================================================================
Cleaned documentation: Commit 9 Dropout_model = 0.35 FVC_weight = 0.3 Confidence_weight = 0.3 LB = -6.8125
Processed documentation: Commit 9 Dropout_model = 0.35 FVC_weight =0.3 Confidence_weight: 0.3 LB = -6.8125
===========================================================================================
Cleaned documentation: Commit 14 Dropout_model = 0.36 FVC_weight = 0.225 Confidence_weight = 0.225 LB = -6.8100
Processed documentation: Commit 14 Dropout_model = 0.36 FVC_weight =0.225 Confidence_weight= 0.225 LB = -6.8100
===========================================================================================
Cleaned documentation: Commit 17 Dropout_model = 0.25 FVC_weight = 0.5 Confidence_weight = 0.5 LB = -6.8283
Processed documentation: Commit 17 Dropout_model = 0.25 FVC_weight =0.5 Confidence_weight: 0.5 LB = -6.8283
===========================================================================================
Cleaned documentation: Commit 18 Dropout_model = 0.38 FVC_weight = 0.2 Confidence_weight = 0.2 LB = -6.8087
Processed documentation: Commit 18 Dropout_model = 0.38 FVC_weight =0.2 Confidence_weight: 0.2 LB = -6.8087
===========================================================================================
Cleaned documentation: Commit 19 Dropout_model = 0.39 FVC_weight = 0.2 Confidence_weight = 0.2 LB = -6.8090
Processed documentation: Commit 19 Dropout_model = 0.39 FVC_weight =0.2 Confidence_weight: 0.2 LB = -6.8090
===========================================================================================
Cleaned documentation: Commit 20 Dropout_model = 0.37 FVC_weight = 0.2 Confidence_weight = 0.2 LB = -6.8092
Processed documentation: Commit 20 Dropout_model = 0.37 FVC_weight =0.2 Confidence_weight= 0.2 LB = -6.8092
===========================================================================================
Cleaned documentation: Commit 23 Dropout_model = 0.38 FVC_weight = 0.2 Confidence_weight = 0.2 LB = -6.8402
Processed documentation: Commit 23 Dropout_model = 0.38 FVC_weight =0.2 Confidence_weight: 0.2 LB = -6.8402
===========================================================================================
Cleaned documentation: Commit 26 Dropout_model = 0.385 FVC_weight = 0.2 Confidence_weight = 0.2 LB = -6.8092
Processed documentation: Commit 26 Dropout_model = 0.385 FVC_weight =0.2 Confidence_weight: 0.2 LB = -6.8092
===========================================================================================
Cleaned documentation: Commit 29 Dropout_model = 0.38 FVC_weight = 0.2 Confidence_weight = 0.2 GaussianNoise_stddev = 0.15 LB = -6.8092
Processed documentation: Commit 29 Dropout_model = 0.38 FVC_weight =0.2 Confidence_weight: 0.2 GaussianNoise_stddev = 0, LB = -6.8092.
===========================================================================================
Cleaned documentation: GameClock. Height. Time. Age. WindSpeed. Weather. Rusher. dense -> categorical. Orientation and Dir. diff Score. Turf. OffensePersonnel. DefensePersonnel. sort
Processed documentation: GameClock. Height. Time. Age. WindSpeed. Weather. Rusher dense -> categorical. Orientation and Dir. diff Score. Turf. OffensePersonnel. DefensePersonnel sort
===========================================================================================
Cleaned documentation: y_train_ridge = ridge_reg.predict(train_reg). y_preds_ridge = ridge_reg.predict(test_reg). y_train_ridge = np.clip(y_train_ridge, 0, 10).astype(int). y_preds_ridge = np.clip(y_preds_ridge, 0, 10).astype(int)
Processed documentation: y_train_ridge = ridge_reg.predict(train_reg). y_preds_ridge.p predict(test_reg) y_train ridge = np.clip(y_ train_ridge
===========================================================================================
Cleaned documentation: y_train_sgd = sgd.predict(train_reg). y_preds_sgd = sgd.predict(test_reg). y_train_sgd = np.clip(y_train_sgd, 0, 10).astype(int). y_preds_sgd = np.clip(y_preds_sgd, 0, 10).astype(int)
Processed documentation: y_train_sgd = sgd.predict(train_reg). y_preds.p predict(test_reg), y_train.p Predict(test) y_Preds.Predict
===========================================================================================
Cleaned documentation: y_logreg_train = logreg.predict(train_reg). y_logreg_pred = logreg.predict(test_reg). y_logreg_train = np.clip(y_logreg_train, 0, 10).astype(int). y_logreg_pred = np.clip(y_logreg_pred, 0, 10).astype(int)
Processed documentation: y_logreg_train = logreg.predict(train_reg). y_logReg_pred = logReg.p predict(test_reg) y_ logreg_ train = np.clip(y_
===========================================================================================
Cleaned documentation: Wavenet with SHIFTED-RFC Proba and CBR Back to Table of Contents](0.1)
Processed documentation: Wavenet with SHIFT-RFC Proba and CBR.
===========================================================================================
Cleaned documentation: Thanks to. Set a 3 KMeans clustering. Compute cluster centers and predict cluster indices
Processed documentation: Set a 3 KMeans clustering. Compute cluster centers and predict cluster indices.
===========================================================================================
Cleaned documentation: check that the images are now in the test_images. Should now be 57458 images in the test_images folder
Processed documentation: check that the images are now in the test_images. folder. Should now be 57458 images.
===========================================================================================
Cleaned documentation: define text data. initialize the tokenizer. integer encode the text data. pad the vectors to create uniform length
Processed documentation: define text data. tokenizer. integer encode the textData. pad the vectors to create uniform length.
===========================================================================================
Cleaned documentation: Relative error between predicted y_pred and measured y_meas values. RMSE between predicted y_pred and measured y_meas values
Processed documentation: Relative error between predicted y_pred and measured y_meas values.
===========================================================================================
Cleaned documentation: Check the val_generator(). val_gen = \. val_generator(df_val_images, df_val, batch_size=2000, num_rows=500, num_cols=500)
Processed documentation: Check the val_generator(). val_gen = \. val_Generator(df_val_images, df_val)
===========================================================================================
Cleaned documentation: INPUTS. Set the batch sizes:. Set the image size:. train_generator. val_generator. test_generator
Processed documentation: Set the batch sizes for each generator. Set the image size for the generator.
===========================================================================================
Cleaned documentation: Comparison of all options of selected features Back to Table of Contents](0.1)
Processed documentation: Comparison of all options of selected features Back to Table of Contents.
===========================================================================================
Cleaned documentation: from sklearn.cluster import KMeans. kmeans = KMeans(n_clusters=np_classes). kmeans.fit(X). y_kmeans = kmeans.predict(X). plt.hist(y_kmeans)
Processed documentation: from sklearn.cluster import KMeans. kmeans = Kmeans(n_clusters=np_classes). k means.fit(X), ymeans.predict(X).
===========================================================================================
Cleaned documentation: Relative error between predicted y_pred and measured y_meas values. RMSE between predicted y_pred and measured y_meas values
Processed documentation: Relative error between predicted y_pred and measured y_meas values.
===========================================================================================
Cleaned documentation: The following variables are either discrete numerical or continuous numerical variables.So the will be imputed by median
Processed documentation: The following variables are either discrete numerical or continuous numerical variables.
===========================================================================================
Cleaned documentation: data preparation. plot. data preparation. Create ColumnDataSource from data frame. Add Plot
Processed documentation: data preparation. plot. Create ColumnDataSource from data frame. Add Plot
===========================================================================================
Cleaned documentation: print('on'). print('ok2'). for x_batch, y_batch in progress_bar(train_loader, parent=mb):. print('ok3 \n'). print('okin'). print('OK4 \n')
Processed documentation: print('on'). print('ok2'). for x_batch, y_batch in progress_bar(train_loader, parent=mb):.print('ok3') print('okin')
===========================================================================================
Cleaned documentation: len([inputs[i][0] for i in range(3)]+[df_train[[i for i in df_train.columns if i.startswith('cat_') or i.startswith('host_')]].iloc[6078,:].values])
Processed documentation: i for i in df_train.columns if i.startswith('cat_') or i. startswith ('host_') and i.len([ inputs[i] for i. in range(3
===========================================================================================
Cleaned documentation: Install Dependencies All the required dependencies are saved in requirements.txt file to install all of then once run
Processed documentation: All the required dependencies are saved in requirements.txt file to install all of then once run.
===========================================================================================
Cleaned documentation: black box LGBM. learning_rate,. LightGBM expects next three parameters need to be integer.. learning_rate' : learning_rate,
Processed documentation: black box LGBM. learning_rate,. LightGBM expects next three parameters need to be integer..
===========================================================================================
Cleaned documentation: Last, carry out some post processing tricks for OpenCL to work properly, and clean up.
Processed documentation: Last, carry out some post processing tricks for OpenCL to work properly.
===========================================================================================
Cleaned documentation: Classification Metrices Dataset: Pima Indians onset of diabetes dataset. Evaluation Algorithm: Logistic Regression, SGDClassifier, RandomForestClassifier.
Processed documentation: Classification Metrices Dataset: Pima Indians onset of diabetes dataset. Evaluation Algorithm: Logistic Regression.
===========================================================================================
Cleaned documentation: Now lets do the same thing for the actual tourney. NCAATourney_SummaryW['LOSSES'].fillna(0, inplace=True). NCAATourney_SummaryW['GAMES'] = NCAATourney_SummaryW['WINS'] + NCAATourney_SummaryW['LOSSES']
Processed documentation: Now lets do the same thing for the actual tourney. NCAATourney_SummaryW['LOSSES'].fillna(0, inplace=True). NCAatourney_ SummaryW['GAMES'] =
===========================================================================================
Cleaned documentation: Generate a list of all matchups in the tourney since 2003. df_tourney_list = pd.read_csv('NCAATourneyCompactResults.csv')
Processed documentation: Generate a list of all matchups in the tourney since 2003.
===========================================================================================
Cleaned documentation: This is the Full Models predictions for stage 1. yearlypredictions.to_csv('stage1_predictions_fullmodel.csv', index=False)
Processed documentation: This is the Full Models predictions for stage 1. yearlypredictions.to_csv('stage1_predictions_fullmodel', index=False)
===========================================================================================
Cleaned documentation: Try early stopping. from keras.callbacks import EarlyStopping. callback = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
Processed documentation: Try early stopping. from keras.callbacks import EarlySt stopping. callback = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0), mode='auto', baseline=
===========================================================================================
Cleaned documentation: types of category. train['cat_len'] = train['category_name'].apply(lambda x: len( x.split('/'))). test['cat_len'] = test['category_name'].apply(lambda x: len( x.split('/')))
Processed documentation: types of category. train['cat_len'] = train['category_name'].apply(lambda x: len( x.split('/'))). test['cat-len'], test['category-name'],
===========================================================================================
Cleaned documentation: df_text['full_text'] = df_text['full_text'].apply(lambda x: stem(x)) - don't think about it :)
Processed documentation: df_text['full_text'] = df_text.apply(lambda x: stem(x) - don't think about it :)
===========================================================================================
Cleaned documentation: Melanoma Classification SIIM-ISIC 2020 fast.ai with EfficientNetB0 and additional data (images only)
Processed documentation: Melanoma Classification SIIM-ISIC 2020 fast.ai with EfficientNetB0 and additional data.
===========================================================================================
Cleaned documentation: train_trigram = pd.Series(nltk.ngrams(train["text"], 3)).value_counts()[:20]. train_trigram.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8)). plt.title('20 Most Frequently Occuring Bigrams'). plt.ylabel('Trigram'). plt.xlabel(' of Occurances')
Processed documentation: train_trigram = pd.Series(nltk.ngrams(train["text"], 3).value_counts()[:20]. train_trigsram.sort_values() = train.sort
===========================================================================================
Cleaned documentation: pre_voc_file = transformers.RobertaTokenizer.pretrained_vocab_files_map. merges_file = pre_voc_file.get('merges_file').get(ROBERTA_PATH). vocab_file = pre_voc_file.get('vocab_file').get(ROBERTA_PATH). model_bin = transformers.modeling_roberta.ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP.get(ROBERTA_PATH)
Processed documentation: pre_voc_file = transformers.RobertaTokenizer.pretrained_vocab_files_map. merges_file.get('merges_ file').get(ROBERTA_PATH).
===========================================================================================
Cleaned documentation: json_f = requests.get(vocab_file). txt_f = requests.get(merges_file). mod_bin = requests.get(model_bin). data = json_f.json()
Processed documentation: json_f = requests.get(vocab_file), txt_f.get('merges_file','mod','model', 'data'), data.get ('data', 'f','merges
===========================================================================================
Cleaned documentation: device = torch.device("cuda"). model_config = transformers.RobertaConfig.from_pretrained('../input/roberta-vocab-file/config.json') to download from internet. model_config.output_hidden_states = True
Processed documentation: Device = torch.device("cuda"). model_config = transformers.RobertaConfig.from_pretrained('../input/roberta-vocab-file/config.json') to download from internet. model
===========================================================================================
Cleaned documentation: final_output = []. test_dataset = TweetDataset(. tweet=df_test.text.values,. sentiment=df_test.sentiment.values,. selected_text=df_test.selected_text.values. data_loader = torch.utils.data.DataLoader(. test_dataset,. shuffle=False,. batch_size=VALID_BATCH_SIZE,. num_workers=0
Processed documentation: final_output = []. test_dataset = TweetDataset( tweet=df_test.text.values,. sentiment=DF_ test.sentiment.values, selected_text=df.selected_text
===========================================================================================
Cleaned documentation: Loading data We will be loading training and test datasets using pandas read_csv function.
Processed documentation: We will be loading training and test datasets using pandas read_csv function.
===========================================================================================
Cleaned documentation: EDA и полный вывод параметров. Главная задача - показать воспроизводимость результата и кратко описать принятые решения
Processed documentation: EDA    ‘принятые рещения’ -   описать  раз
===========================================================================================
Cleaned documentation: There are some weird products that weight 42 Kilos. Check out this Exhibidor : Exhibidor bimbo](
Processed documentation: There are some weird products that weight 42 Kilos. Check out this Exhibidor.
===========================================================================================
Cleaned documentation: plot the coefficients derived by the best estimators of each model. plt.subplot(len(list_bestest),1,im+1). plt.savefig('feature_importance.png',bbox_inches='tight',transparent=True). plt.close(0)g('feature_importance.png',bbox_inches='tight',transparent=True). plt.close(0)
Processed documentation: plot the coefficients derived by the best estimators of each model. Plt.subplot(len(list_bestest),1,im+1). plt.savefig('feature_importance.png',bbox
===========================================================================================
Cleaned documentation: Show one Example of landmark images Taken from the public kernel...thanks for sharing this. It was quite helpful
Processed documentation: Show one Example of landmark images Taken from the public kernel.
===========================================================================================
Cleaned documentation: Reference. label_count = df_labels["label"].value_counts(). plt.figure(figsize=(16,10)). sns.barplot(label_count.index, label_count.values, alpha=0.8). plt.title('all unicode counts'). plt.ylabel('Number of count', fontsize=12). plt.xlabel('unicode', fontsize=12). plt.show()
Processed documentation: Reference. label_count = df_labels["label"].value_counts. plt.figure(figsize=(16,10), sns.barplot(label_count.index, label_ count.
===========================================================================================
Cleaned documentation: Use the torch dataloader to iterate through the dataset. functions to show an image. get some images. show images
Processed documentation: Use the torch dataloader to iterate through the dataset. functions to show an image.
===========================================================================================
Cleaned documentation: Loading of training/testing ids and depths. test_df = pd.read_csv(data_source + "test", index_col="id", usecols=[0]). train_df.head()
Processed documentation: Training/testing ids and depths. Test_df = pd.read_csv(data_source + "test", index_col="id", usecols=[0]). train_df.head()
===========================================================================================
Cleaned documentation: train_df[['xof','yof','mos_num']]. train_df = pd.merge(train_df,mosaic_df,how = 'left',left_on='img_id', right_on = 'img_id'). test_df = pd.merge(test_df,mosaic_df,how = 'left',left_on='img_id', right_on = 'img_id'). mosaic_df.head()
Processed documentation: train_df = pd.train_df[['xof','yof','mos_num']]. mosaic_df.merge(train_DF,mosaic_df,how = 'left',left_
===========================================================================================
Cleaned documentation: feature_importances_比较小，是否说明这几个特征没有用？ Feature_importances of features in cate0 and cate1 is small. Does it mean they are less important?
Processed documentation: feature_importances_ is small. Does it mean they are less important?
===========================================================================================
Cleaned documentation: Generator check. DISPLAY IMAGES WITH DEFECTS. plt.figure(figsize=(14,50)) 20,18. plt.subplot(batch_size,1,k+1). prediction. print(train2['ImageId'].iloc[data_check.indexes])
Processed documentation: Generator check. DISPLAY IMAGES WITH DEFECTS. plt.subplot(batch_size,1,k+1). prediction. print(train2['ImageId'].iloc[data_check.index
===========================================================================================
Cleaned documentation: Dictionary of category names (Russian to English). Dictionary of parent category names (Russian to English)
Processed documentation: Dictionary of category names. Dictionary of parent category names (Russian to English)
===========================================================================================
Cleaned documentation: The test set data features have a higher low mean value (left plot), and higher low variance (right plot).
Processed documentation: The test set data features have a higher low mean value (left plot) and higher low variance (right plot)
===========================================================================================
Cleaned documentation: Warning__: This next cell takes a very long time to run (>> 10 min).
Processed documentation: Warning: This next cell takes a very long time to run (>> 10 min)
===========================================================================================
Cleaned documentation: resnet_backbone = resnet.__dict__['resnet152'](pretrained=pretrained,norm_layer=norm_layer). select layers that wont be frozen. freeze layers only if pretrained backbone is used
Processed documentation: resnet_backbone = resnet.__dict__['resnet152'](pretrained=pretrained, norm_layer=norm_layer). select layers that wont be frozen. freeze layers only if pretrained backbone is
===========================================================================================
Cleaned documentation: Imports We'll use a familiar stack of data science libraries: `Pandas`, `numpy`, `matplotlib`, `seaborn`, and eventually `sklearn` for modeling.
Processed documentation: We'll use a familiar stack of data science libraries:Pandas, numpy, matplotlib, seaborn, and sklearn.
===========================================================================================
Cleaned documentation: Iterate through the float columns. Iterate through the poverty levels. Plot each poverty level as a separate line
Processed documentation: Iterate through the float columns. Plot each poverty level as a separate line.
===========================================================================================
Cleaned documentation: Labels for training. Extract the training data. Submission base which is used for making submissions to the competition
Processed documentation: Labels for training. Extract the training data. Submission base which is used for making submissions.
===========================================================================================
Cleaned documentation: TSNE has no transform method. Add components to test data. Add components to training data for visualization and modeling
Processed documentation: TSNE has no transform method. Add components to test data for visualization and modeling.
===========================================================================================
Cleaned documentation: Read the image on which data augmentaion is to be performed. albumentations.VerticalFlip(1), Verticlly flip the image. albumentations.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.8, p=1),
Processed documentation: Read the image on which data augmentaion is to be performed. albumentations.VerticalFlip(1), Verticlly flip the image. RandomBrightnessContrast(brightness_limit=0
===========================================================================================
Cleaned documentation: Plot the pickups. Legend. Adjust alpha of legend markers. Show map in background (zorder = 0)
Processed documentation: Plot the pickups. Adjust alpha of legend markers. Show map in background (zorder = 0)
===========================================================================================
Cleaned documentation: Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.
Processed documentation: Standard imports for data science work. LightGBM library is used for the gradient boosting machine.
===========================================================================================
Cleaned documentation: Bureau only features. Previous only features. Original features will be in both datasets
Processed documentation: Bureau only features. Original features will be in both datasets.
===========================================================================================
Cleaned documentation: Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes.
Processed documentation: Let's drop the columns, one-hot encode the dataframes, and then align the columns.
===========================================================================================
Cleaned documentation: Granted applications per number of children Granted applications stacked by number of children.
Processed documentation: Granted applications stacked by number of children.
===========================================================================================
Cleaned documentation: Train a word2vec model Train a word2vec model with the given training sentences.
Processed documentation: Train a word2vec model with the given training sentences.
===========================================================================================
Cleaned documentation: Assume 0.5 threshold for mask binarization.. This can be optimized!. Save submission with specified run_name.
Processed documentation: Assume 0.5 threshold for mask binarization. Save submission with specified run_name.
===========================================================================================
Cleaned documentation: Generate text features:. Initialize decomposition methods:. Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
Processed documentation: Generate text features: Initialize decomposition methods: Combine all extracted features:. Concatenate with main DF:. Remove raw text columns:
===========================================================================================
Cleaned documentation: Helper: Load training images and masks and compute coverage based on them. Masks are needed for computing the coverage.
Processed documentation: Helper: Load training images and masks and compute coverage. Masks are needed for computing the coverage.
===========================================================================================
Cleaned documentation: print (len(by_date)). print ('Rates for country/region : ' + pd.unique(country['Country/Region'])). print (by_date). Add need fields. print (by_date.loc[i])
Processed documentation: print (len(by_date)). print ('Rates for country/region :'+ pd.unique(country['Country/Region'])). print (by_ date). Add need fields.
===========================================================================================
Cleaned documentation: watch out for overfitting!. V20. VALIDATION_MISMATCHES_IDS = ['861282b96','df1fd14b4','b402b6acd','741999f79','4dab7fa08','6423cd23e','617a30d60','87d91aefb','2023d3cac','5f56bcb7f','4571b9509','f4ec48685','f9c50db87','96379ff01','28594d9ce','6a3a28a06','fbd61ef17','55a883e16','83a80db99','9ee42218f','b5fb20185','868bf8b0c','d0caf04b9','ef945a176','9b8f2f5bd','f8da3867d','0bf0b39b3','bab3ef1f5','293c37e25','f739f3e83','5253af526','f27f9a100','077803f97','b4becad84']. V22. VALIDATION_MISMATCHES_IDS = ['861282b96','617a30d60','4571b9509','f4ec48685','28594d9ce','6a3a28a06','55a883e16','9b8f2f5bd','293c37e25']. V23
Processed documentation: watch out for overfitting!. V20. VALIDATION_MISMATCHES_IDS = ['861282b96','617a30d60','4571b9509','f4ec48685','28594
===========================================================================================
Cleaned documentation: Modeling: Xgboost](6) The parameters are not optimised. The xgboost model is based on [this](copied from great kernel.
Processed documentation: Modeling: Xgboost. The parameters are not optimised. The xgboost model is based on [this]
===========================================================================================
Cleaned documentation: del X_train, y_train, X_val, y_val, dtrain, dvalidation, y_val_preds, y_val_preds_2. del X, y, cols, tscv. code]
Processed documentation: Del X, y, cols, tscv. code, X_train, y_train,. X_val, y-val, dtrain, dvalidation, y.val_preds, y val
===========================================================================================
Cleaned documentation: temp_df.columns = ['fullVisitorId', 'predictedLogRevenue']. sub_df['predictedLogRevenue'] = np.log1p(sub_df['predictedLogRevenue']). sub_df.to_csv(file_name, index = False)
Processed documentation: temp_df.columns = ['fullVisitorId', 'predictedLogRevenue']. sub_df['predictedlogRevenue'] = np.log1p(sub_df)] sub_DF.to
===========================================================================================
Cleaned documentation: Loss function Focal loss is good for unbalanced datasets, like this one.
Processed documentation: Loss function Focal loss is good for unbalanced datasets.
===========================================================================================
Cleaned documentation: load data. only look as train tasks for now. evaluation_tasks = load_data('abstraction-and-reasoning-challenge/evaluation/'). test_tasks = load_data('abstraction-and-reasoning-challenge/test/')
Processed documentation: load data. only look as train tasks for now. evaluation_tasks = load_data('abstraction-and-reasoning-challenge/evaluation/'). test_t tasks = load. data('ab
===========================================================================================
Cleaned documentation: sub_sample = pd.read_csv(main_dir + 'sample_submission.csv')sub_sample = sub_sample.drop(['EncodedPixels'], axis = 1)submission = sub_sample.merge(sub, on = ['ImageId_ClassId'])submission.head(10)
Processed documentation: sub_sample = pd.read_csv(main_dir +'sample_submission.csv')sub_ sample.drop(['EncodedPixels'], axis = 1)submission = sub_sample.
===========================================================================================
Cleaned documentation: x = Dropout(0.4)(x). out2 = Dense(6, activation="linear")(x)tensor 6(xx,yy,zz). out4 = Dense(1, activation="linear")(x)scalar_coupling_constant. model = Model(inputs=inp, outputs=[out])
Processed documentation: x = Dropout(0.4)(x). out2 = Dense(6, activation="linear")(x)tensor 6(xx,yy,zz). out4 = Densing(1,activation="linear
===========================================================================================
Cleaned documentation: mask = train.query('signal_to_noise > 1')[mes_cols].apply(lambda row: np.any([np.any(np.array(c) == 0) for c in row]), axis=1). filtred = train.query('signal_to_noise > 1')[mask]
Processed documentation: mask = train.query('signal_to_noise > 1')[mes_cols].apply(lambda row: np.any(np.array(c) == 0) for c in row]), axis=
===========================================================================================
Cleaned documentation: test_df['building_mean'] = test_df['building_id'].map(building_mean). test_df['building_min'] = test_df['building_id'].map(building_min). test_df['building_max'] = test_df['building_id'].map(building_max). test_df['building_std'] = test_df['building_id'].map(building_std). add_lag_feature(weather_test_df, window=3). add_lag_feature(weather_test_df, window=72)
Processed documentation: test_df['building_mean'] = test_df ['building_id'].map(building_ mean). test_DF[' building_min'] =    'building_min'  'building_
===========================================================================================
Cleaned documentation: test_df['building_mean'] = test_df['building_id'].map(building_mean). test_df['building_min'] = test_df['building_id'].map(building_min). test_df['building_max'] = test_df['building_id'].map(building_max). test_df['building_std'] = test_df['building_id'].map(building_std). add_lag_feature(weather_test_df, window=3). add_lag_feature(weather_test_df, window=72)
Processed documentation: test_df['building_mean'] = test_df ['building_id'].map(building_ mean). test_DF[' building_min'] =    'building_min'  'building_
===========================================================================================
Cleaned documentation: test_df['building_mean'] = test_df['building_id'].map(building_mean). test_df['building_min'] = test_df['building_id'].map(building_min). test_df['building_max'] = test_df['building_id'].map(building_max). test_df['building_std'] = test_df['building_id'].map(building_std). add_lag_feature(weather_test_df, window=3). add_lag_feature(weather_test_df, window=72)
Processed documentation: test_df['building_mean'] = test_df ['building_id'].map(building_ mean). test_DF[' building_min'] =    'building_min'  'building_
===========================================================================================
Cleaned documentation: load site 0 building meta data. leak_meta_df = pd.read_pickle(ucf_root/'building_metadata_external.pkl'). leak_meta_df.drop(['eui','leed'], axis=1, inplace=True). building_meta_df = pd.concat([building_meta_df, leak_meta_df]). building_meta_df.reset_index(inplace=True)
Processed documentation: load site 0 building meta data. leak_meta_df = pd.read_pickle(ucf_root/'building_metadata_external.pkl'). leak_ meta_df.drop(['eui
===========================================================================================
Cleaned documentation: test_df['building_mean'] = test_df['building_id'].map(building_mean). test_df['building_min'] = test_df['building_id'].map(building_min). test_df['building_max'] = test_df['building_id'].map(building_max). test_df['building_std'] = test_df['building_id'].map(building_std). add_lag_feature(weather_test_df, window=3). add_lag_feature(weather_test_df, window=72)
Processed documentation: test_df['building_mean'] = test_df ['building_id'].map(building_ mean). test_DF[' building_min'] =    'building_min'  'building_
===========================================================================================
Cleaned documentation: As you see:The Neutered pets are accepted mostly.The points should be on the Neutereds!
Processed documentation: As you see: The Neutered pets are accepted mostly.
===========================================================================================
Cleaned documentation: Let's start by loading the dataset using the good old `pd.read_csv` and timeit (using the `%%timeit` magic command).
Processed documentation: Let's start by loading the dataset using the good old `pd.read_csv` and timeit (using the `timeit` magic command).
===========================================================================================
Cleaned documentation: Alright, the `purchase_date` contains temporal information, so let's turn it into a `datetime` type using the [`pandas.to_datetime`]( function.
Processed documentation: Alright, the `purchase_date` contains temporal information, so let's turn it into a `datetime` type using the [`pandas.to_datetime' function.
===========================================================================================
Cleaned documentation: wavelet transform takes too much time. extractor = FeatureExtractor([SummaryTransformer(), WaevletSummaryTransformer(WAVELET_WIDTH), SpectrogramSummaryTransformer(. sample_rate= sample_rate, fft_length=200, stride_length=100)])
Processed documentation: wavelet transform takes too much time. extractor = FeatureExtractor([SummaryTransformer(), Waevlet summaryTransformer(WAVELET_WIDTH), SpectrogramSummaryTrans transformer(. sample_rate= sample_
===========================================================================================
Cleaned documentation: max_depth = trial.suggest_int('max_depth', ). num_iterations = trial.suggest_int("num_iterations", 100, 500). fit_params = {"early_stopping_rounds":20,. eval_metric": matthews_corrcoef}
Processed documentation: max_depth = trial.suggest_int('max_depth', 100, 500), num_iterations =trial.suggest int("num_iteration", 100,500), fit_params = {early_stopping
===========================================================================================
Cleaned documentation: node_size: the number of the game paly. node_color: game type link_color: the number of the game trainsit
Processed documentation: node_size: the number of the game paly. node_color: game type. link_ color: theNumber of game trainsit.
===========================================================================================
Cleaned documentation: grouped_2['proportion']=grouped_2['Weekly_Sales']/sum(grouped_2['Store']). grouped_2['Count']=grouped_2['Weekly_Sales']. data=grouped_2[['Date_2','Count']]. print(data.head(100)). train['Date_2']=train['Month'].astype(str) + '-' + train['Day'].astype(str). train=pd.merge(train, data, how='left', on='Date_2' ). train.head(150)
Processed documentation: grouped_2['proportion']=grouped-2['Weekly_Sales']/sum(grouping-2]'s-1. grouped_2 ['Count']= grouped-1's-2
===========================================================================================
Cleaned documentation: Get Datasets from TF Record files A method to get datasets from tf_record_file. See next section for the usage.[](
Processed documentation: Get Datasets from TF Record files A method to get datasets from TF_record_file. See next section for the usage.
===========================================================================================
Cleaned documentation: raw_train = pd.read_csv('/kaggle/input/osic-pulmonary-fibrosis-progression/train.csv'). size_image = pd.read_csv('/kaggle/input/prep-data/size_image.csv'). list_files = pd.read_csv('/kaggle/input/prep-data/list_files.csv', converters={"files": lambda x: x.strip("[]").replace("'","").split(", ")})
Processed documentation: raw_train = pd.read_csv('/kaggle/ input/osic-pulmonary-fibrosis-progression/train.csv') size_image = pD.read-csv('
===========================================================================================
Cleaned documentation: pre_trained_model = tf.keras.models.load_model('/kaggle/input/3d-cnn-mlp/model_9', custom_objects={'loss':mloss(LAMBDA_LOSS), 'score':score}). pre_trained_CNN = tf.keras.models.load_model('/kaggle/input/3d-cnn-mlp/CNN_9', custom_objects={'loss':mloss(LAMBDA_LOSS), 'score':score}). CNN.set_weights(pre_trained_CNN.get_weights()). model.set_weights(pre_trained_model.get_weights())
Processed documentation: pre_trained_model = tf.keras.models.load_model('/kaggle/ input/3d-cnn-mlp/model_9', custom_objects={'loss':mloss(
===========================================================================================
Cleaned documentation: list_patient_score=[]. for i in train.Patient.unique():. model = create_model(LAMBDA_LOSS). print(i). history = model.fit(x=train[~train.Patient.isin([i])][SELECTED_COLUMNS], y=train[~train.Patient.isin([i])][['FVC']], validation_data=(train[train.Patient.isin([i])][SELECTED_COLUMNS], train[train.Patient.isin([i])][['FVC']]), epochs=250). list_patient_score.append([i, history.history['val_score']])
Processed documentation: list_patient_score=[]. for i in train.Patient.unique():. model = create_model(LAMBDA_LOSS). print(i). history = model.fit(x=train[~
===========================================================================================
Cleaned documentation: import matplotlib.pyplot as plt. plt.figure(figsize=(30,10)). for i in range(1,11):. plt.subplot(3,4,i). plt.plot(history[i].history['score']). plt.plot(history[i].history['val_score'])
Processed documentation: import matplotlib.pyplot as plt.figure(figsize=(30,10)). for i in range(1,11):.plt.plot(history[i].history['score']). Plt.
===========================================================================================
Cleaned documentation: If this kernel helps you fortunately, please upvote it. Thanks you. :)
Processed documentation: If this kernel helps you fortunately, please upvote it. Thanks.
===========================================================================================
Cleaned documentation: to save time, I only use 3 columns as targets.. targets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']
Processed documentation: to save time, I only use 3 columns as targets. targets = ['reactivity', 'deg_Mg_pH10','Deg_PH10' and 'deg Mg 50C'
===========================================================================================
Cleaned documentation: CNN 1D. line1 = BatchNormalization()(Input_layer). line2 = BatchNormalization()(Input_layer). line3 = BatchNormalization()(Input_layer). RNN. CNN 2D
Processed documentation: CNN 1D. RNN. line1 = BatchNormalization()(Input_layer). line2. Batch normalization() (Output_layer) line3. BATCH normalization(Output) line4. B
===========================================================================================
Cleaned documentation: CNN 1D. line1 = BatchNormalization()(Input_layer). line2 = BatchNormalization()(Input_layer). line3 = BatchNormalization()(Input_layer). RNN. CNN 2D
Processed documentation: CNN 1D. RNN. line1 = BatchNormalization()(Input_layer). line2. Batch normalization() (Output_layer) line3. BATCH normalization(Output) line4. B
===========================================================================================
Cleaned documentation: CNN 1D. line1 = BatchNormalization()(Input_layer). line2 = BatchNormalization()(Input_layer). line3 = BatchNormalization()(Input_layer). RNN. CNN 2D
Processed documentation: CNN 1D. RNN. line1 = BatchNormalization()(Input_layer). line2. Batch normalization() (Output_layer) line3. BATCH normalization(Output) line4. B
===========================================================================================
Cleaned documentation: I referred below helpful kernels. Thanks for the authors! If you think this kernel is helpful, please upvote them!
Processed documentation: I referred below helpful kernels. If you think this kernel is helpful, please upvote them!
===========================================================================================
Cleaned documentation: Census_OSBuildNumber - OS Build number extracted from the OsVersionFull. Example - OsBuildNumber = 10512 or 10240
Processed documentation: Census_OSBuildNumber - OS Build number extracted from OsVersionFull. Example - OsBuildNumber = 10512 or 10240.
===========================================================================================
Cleaned documentation: Find Null data We need to find some features containing null data. reference: [Anisotropic's work](
Processed documentation: Find Null data We need to find some features containing null data. reference: Anisotropic's work.
===========================================================================================
Cleaned documentation: Onehot encoding for categorical data For categorical data, I used pd.get_dummy() function to do onehot encoding!
Processed documentation: Onehot encoding for categorical data. I used pd.get_dummy() function to do onehot encoding.
===========================================================================================
Cleaned documentation: res = np.zeros((img_size_target, img_size_target), dtype=img.dtype). res[:img_size_ori, :img_size_ori] = img. return res. return img[:img_size_ori, :img_size_ori]
Processed documentation: res = np.zeros((img_size_target, img_ size_target), dtype=img.dtype). res[: img_size-ori, :img_ size-ori] = img. return
===========================================================================================
Cleaned documentation: model1 = Model(input_layer, output_layer). c = optimizers.adam(lr = lr). model1.compile(loss="binary_crossentropy", optimizer=c, metrics=[my_iou_metric])
Processed documentation: model1 = Model(input_layer, output_layer). c = optimizers.adam(lr = lr). model1.compile(loss="binary_crossentropy", optimizer=c, metrics=[
===========================================================================================
Cleaned documentation: The colour of monsters doesn't seem to be essential. Maybe it disturb the classification. Let's classify without colour...
Processed documentation: The colour of monsters doesn't seem to be essential. Let's classify without colour...
===========================================================================================
Cleaned documentation: The function to calculate score.. The beginning and end of the paths must be City'0'.
Processed documentation: The function to calculate score. The beginning and end of the paths must be City'0'
===========================================================================================
Cleaned documentation: obj_pca = model = PCA(n_components = components)X_pca = obj_pca.fit_transform(pca_df)X_t_pca = obj_pca.fit_transform(pca_test_df). Implement PCA
Processed documentation: obj_pca = model = PCA(n_components = components)X_pCA = obj_Pca.fit_transform(pca_df) X_t_ pca = obj Pca
===========================================================================================
Cleaned documentation: add_decomposition(train, X_pca, 90, 'pca')_add_decomposition(test, X_t_pca, 90, 'pca'). pca_train = train[['ID_code','target']]. pca_test = test[['ID_code']]
Processed documentation: pca_train = train[['ID_code','target']]. pca test = test[[' ID_code']] pca.add_decomposition(train, X_pca, 90, '
===========================================================================================
Cleaned documentation: avg_auc = 0.. print('cycle_LR'). avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader). avg_val_auc = 0.. avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)
Processed documentation: avg_auc = 0.. print('cycle_LR'). avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach()),4) / len(train_loader
===========================================================================================
Cleaned documentation: This notebook requires PyTorch-Lighning. To use PyTorch-Lighning on Internet-off notebook, the following dataset is helpful: pytorch-lightning 0.7.1]( by @[higepon](
Processed documentation: This notebook requires PyTorch-Lighning. To use it on Internet-off notebook, use the following dataset.
===========================================================================================
Cleaned documentation: The previous cell will save prediction results as pos_pred.csv and neg_pred.csv. Let us load them.
Processed documentation: The previous cell will save prediction results as pos_pred and neg_pred. Let us load them.
===========================================================================================
Cleaned documentation: bashcd LightGBMrm -r buildmkdir buildcd buildcmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..make -j$(nproc)
Processed documentation: bashcd LightGBMrm -r buildmkdir buildcd buildcmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so
===========================================================================================
Cleaned documentation: fill in mean for floats. fill in -999 for categoricals. Label Encoding
Processed documentation: fill in mean for floats. fill in -999 for categoricals.
===========================================================================================
Cleaned documentation: python train.py -l 0.01 -g 0 -classes 1 -dir /kaggle/working/convertor -pretrained ../input/yolov4coco/yolov4.conv.137.pth -optimizer sgd -iou-type giou -train_label_path convertor/train.txt
Processed documentation: python train.py -l 0.01 -g 0 -classes 1 -dir /kaggle/working/convertor -pretrained../ input/yolov4coco/yOLov4.137
===========================================================================================
Cleaned documentation: img = segmentation_color3(img.copy()). print(int(midly),int(midlx)). max_LENGTN=5580. max_LENGTN+=1015. result = result[int(top_crop):int(bottom_crop),int(left_crop):int(right_crop),:]. result = cv2.bilateralFilter(result,9,100,100). result = segmentation_color3(result). print (sss)
Processed documentation: img = segmentation_color3(img.copy()). print(int(midly),int( midlx), max_LENGTN=5580). result = cv2.bilateralFilter(result
===========================================================================================
Cleaned documentation: DSL Implementation We start with the functions that take one image and produce an a list of images.](
Processed documentation: DSL Implementation We start with the functions that take one image and produce an a list of images.
===========================================================================================
Cleaned documentation: Plotting the error with the number of iterations. With each iteration the error reduces smoothly. plt.plot(history.history['loss'],'o-')
Processed documentation: Plotting the error with the number of iterations. With each iteration the error reduces smoothly.
===========================================================================================
Cleaned documentation: fill up the missing values. x_train, x_test, y_train, word_index = load_and_prec(). x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec()
Processed documentation: fill up the missing values. x_train, x_test, y_train,. word_index = load_and_prec()
===========================================================================================
Cleaned documentation: we don't need these for now. xgbtest = xgb.DMatrix(test). p_test = model.predict(xgbtest, ntree_limit=model.best_ntree_limit). p_test = p_test + 1
Processed documentation: we don't need these for now. xgbtest = xgb.DMatrix(test). p_test = model.predict(xgbtest, ntree_limit=model.best_ntree_limit).
===========================================================================================