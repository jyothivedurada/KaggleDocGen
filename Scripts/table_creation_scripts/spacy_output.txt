
Cleaned documentation: We will be visualizing images using the `pydicom` package. Let's define a function to visualize DICOM images
Processed documentation: Let's define a function to visualize DICOM images
===========================================================================================
Cleaned documentation: Define paths in the fancy way, after all we have pathlib now. No more os.path.join...whatever
Processed documentation: Define paths in the fancy way, after all we have pathlib now.
===========================================================================================
Cleaned documentation: The train directory comtaims png files. Let's get all the files and check a few samples
Processed documentation: The train directory comtaims png files.
===========================================================================================
Cleaned documentation: Iterate over data. I have just shown it for 500 images just to save time
Processed documentation: I have just shown it for 500 images just to save time
===========================================================================================
Cleaned documentation: So there are four outliers in the datset. Should we drop them? We will see about that
Processed documentation: So there are four outliers in the datset.
===========================================================================================
Cleaned documentation: Looks good. No surprising thing here. Let's see how many pickups were there for each month
Processed documentation: Let's see how many pickups were there for each month
===========================================================================================
Cleaned documentation: Loads scans from a folder and into a list. Parameters: path (Folder path) Returns: slices (List of slices
Processed documentation: Parameters: path (Folder path) Returns: slices (List of slices
===========================================================================================
Cleaned documentation: Converts raw images to Hounsfield Units (HU. Parameters: scans (Raw images) Returns: image (NumPy array
Processed documentation: Parameters: scans (Raw images) Returns: image (NumPy array
===========================================================================================
Cleaned documentation: Generates markers for a given image. Parameters: image. Returns: Internal Marker, External Marker, Watershed Marker.
Processed documentation: Returns: Internal Marker, External Marker, Watershed Marker.
===========================================================================================
Cleaned documentation: The idea is to create a mask with regions we know for sure are blue rooftops. Hold on...
Processed documentation: The idea is to create a mask with regions we know for sure are blue rooftops.
===========================================================================================
Cleaned documentation: The most interesting model is LGBM with the first draft score .757. The least interesting one is KNearest.
Processed documentation: The most interesting model is LGBM with the first draft score .757.
===========================================================================================
Cleaned documentation: The behavior of this metric completely coincides with the custom function. Only absolute values differ.
Processed documentation: The behavior of this metric completely coincides with the custom function.
===========================================================================================
Cleaned documentation: split data. No need to use "the answer to life, the universe and everything" as random_state
Processed documentation: No need to use "the answer to life, the universe and everything" as random_state
===========================================================================================
Cleaned documentation: Reload from h5py all data. Makes sense for subsequent runs where data is not generated again
Processed documentation: Makes sense for subsequent runs where data is not generated again
===========================================================================================
Cleaned documentation: Data has (Sample, Product, Feature. This function will keep (Sample, Product, 0 to number of non-cat features
Processed documentation: This function will keep (Sample, Product, 0 to number of non-cat features
===========================================================================================
Cleaned documentation: The input features represent channels in the convolution. The idea behind using a convolution is to
Processed documentation: The input features represent channels in the convolution.
===========================================================================================
Cleaned documentation: Data has (Sample, Feature. This function will keep (Sample, 0 to number of non-cat features
Processed documentation: This function will keep (Sample, 0 to number of non-cat features
===========================================================================================
Cleaned documentation: es = tf.keras.callbacks.EarlyStopping(monitor = "val_loss" , verbose = 1 , mode = 'min' , patience
Processed documentation: "val_loss" , verbose = 1 , mode = 'min' , patience
===========================================================================================
Cleaned documentation: Assumption Rate of increase in confirmed cases for ensuing period = 22. Fatalities change linearly with confirmed cases
Processed documentation: Assumption Rate of increase in confirmed cases for ensuing period = 22.
===========================================================================================
Cleaned documentation: TODO More Feature Selection method like Boruta or Correaltion based FS. Create feature interactions of some important features.
Processed documentation: TODO More Feature Selection method like Boruta or Correaltion based FS.
===========================================================================================
Cleaned documentation: Extract the exif data from any image. Data includes GPS coordinates, Focal Length, Manufacture, and more.
Processed documentation: Data includes GPS coordinates, Focal Length, Manufacture, and more.
===========================================================================================
Cleaned documentation: Returns a dictionary from the exif data of an PIL Image item. Also converts the GPS Tags.
Processed documentation: Returns a dictionary from the exif data of an PIL Image item.
===========================================================================================
Cleaned documentation: Helper function to convert the GPS coordinates. stored in the EXIF to degress in float format.
Processed documentation: Helper function to convert the GPS coordinates.
===========================================================================================
Cleaned documentation: this is the main ensembling class. how to use it is in the next cell
Processed documentation: this is the main ensembling class.
===========================================================================================
Cleaned documentation: It is worth seeing these stats as well. It might have a story for sure.
Processed documentation: It is worth seeing these stats as well.
===========================================================================================
Cleaned documentation: First death again in France by Feb 17. Italy follows suite. The rest is history
Processed documentation: First death again in France by Feb 17.
===========================================================================================
Cleaned documentation: Images can be found in attached datasets. They are flattened so you no longer need to do
Processed documentation: Images can be found in attached datasets.
===========================================================================================
Cleaned documentation: Simple function to get the dimensions of a square-ish shape for plotting. num images.
Processed documentation: Simple function to get the dimensions of a square-ish shape for plotting.
===========================================================================================
Cleaned documentation: fold_train_files = [f for f in train_files if any([int(re.match("^train([0-9]+)", f.split("/")[-1]).group(1)) % 15 == i for i in idTrain
Processed documentation: [f for f in train_files if any([int(re.match("^train([0-9]+)", f.split("/")[-1]).group(1))
===========================================================================================
Cleaned documentation: Points to Note Data is available till June month. Not much trend to look out for.
Processed documentation: Points to Note Data is available till June month.
===========================================================================================
Cleaned documentation: save_grid4 = open("Tfidf_LogR_4.pickle", 'wb') wb= write in bytes. Tfidf_LogR.pickle' is the name of the file saved
Processed documentation: open("Tfidf_LogR_4.pickle", 'wb') wb= write in bytes.
===========================================================================================
Cleaned documentation: Player group... In solo no group, in duo minimum 50 group and in squad (25 4) minimum 25 group
Processed documentation: In solo no group, in duo minimum 50 group and in squad (25 4) minimum 25 group
===========================================================================================
Cleaned documentation: Cross validate score and print result and print previous result. And show the score and the previous best score.
Processed documentation: Cross validate score and print result and print previous result.
===========================================================================================
Cleaned documentation: All Stores have same trend... Weird Seems like the dataset is A Synthetic One..
Processed documentation: Weird Seems like the dataset is A Synthetic One..
===========================================================================================
Cleaned documentation: series - dataframe with timeseries. window - rolling window size. plot_intervals - show confidence intervals. plot_anomalies - show anomalies.
Processed documentation: window - rolling window size.
===========================================================================================
Cleaned documentation: Plots exponential smoothing with different alphas. series - dataset with timestamps. alphas - list of floats, smoothing parameters.
Processed documentation: Plots exponential smoothing with different alphas.
===========================================================================================
Cleaned documentation: Join the history with the forecast. The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.
Processed documentation: The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.
===========================================================================================
Cleaned documentation: Now we calculate the values of e_i and p_i according to the formulas given in the article above.
Processed documentation: Now we calculate the values of e_i and p_i
===========================================================================================
Cleaned documentation: Use a simple train-test split. I have found that this gives a better local CV score than
Processed documentation: I have found that this gives a better local CV score than
===========================================================================================
Cleaned documentation: For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code
Processed documentation: For an fast model/feature evaluation, get only 10% of dataset.
===========================================================================================
Cleaned documentation: For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code
Processed documentation: For an fast model/feature evaluation, get only 10% of dataset.
===========================================================================================
Cleaned documentation: For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code
Processed documentation: For an fast model/feature evaluation, get only 10% of dataset.
===========================================================================================
Cleaned documentation: train['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]') target_df = train.sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count'], 'target':['mean']}) target_df.columns = ['Date', 'Count', 'Toxicity Rate
Processed documentation: target_df = train.sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count'], 'target':['mean']}) target_df.columns =
===========================================================================================
Cleaned documentation: sklearn is only imported for splitting the data. The rest is up to cuML.
Processed documentation: sklearn is only imported for splitting the data.
===========================================================================================
Cleaned documentation: Since the data is unbalanced, our mean prediction is around 0.. So this is the reason of unbalanced thresholds
Processed documentation: Since the data is unbalanced, our mean prediction is around 0..
===========================================================================================
Cleaned documentation: Build dataframe by iterating over chunks. Option to skip chunks and. therefore read in less data.
Processed documentation: Build dataframe by iterating over chunks.
===========================================================================================
Cleaned documentation: build_year has an erronus value. Since its unclear which it should be, let's replace with
Processed documentation: build_year has an erronus value.
===========================================================================================
Cleaned documentation: Loading data. Converting "categorical" variables, even though in this dataset they are actually numeric.
Processed documentation: Converting "categorical" variables, even though in this dataset they are actually numeric.
===========================================================================================
Cleaned documentation: There are lots of comments within the code below. I think the callback section is particularly import.
Processed documentation: I think the callback section is particularly import.
===========================================================================================
Cleaned documentation: Let's split the data into folds. I always use the same random number for reproducibility,
Processed documentation: Let's split the data into folds.
===========================================================================================
Cleaned documentation: This is where we define and compile the model. These parameters are not optimal, as they were chosen
Processed documentation: This is where we define and compile the model.
===========================================================================================
Cleaned documentation: to get a notebook to complete in 60 minutes. Other than leaving BatchNormalization and last sigmoid
Processed documentation: Other than leaving BatchNormalization and last sigmoid
===========================================================================================
Cleaned documentation: The model needs to be initialized anew every time you run a different fold. If not, it will continue
Processed documentation: The model needs to be initialized anew every time you run a different fold.
===========================================================================================
Cleaned documentation: This is where we repeat the runs for each fold. If you choose runs=1 above, it will run a
Processed documentation: This is where we repeat the runs for each fold.
===========================================================================================
Cleaned documentation: The first callback prints out roc_auc and gini values at the end of each epoch. It must be listed
Processed documentation: The first callback prints out roc_auc and gini values at the end of each epoch.
===========================================================================================
Cleaned documentation: before the EarlyStopping callback, which monitors gini values saved in the previous callback. Make
Processed documentation: before the EarlyStopping callback, which monitors gini values saved in the previous callback.
===========================================================================================
Cleaned documentation: CSVLogger creates a record of all iterations. Not really needed but it doesn't hurt to have it.
Processed documentation: CSVLogger creates a record of all iterations.
===========================================================================================
Cleaned documentation: ModelCheckpoint saves a model each time gini improves. Its mode also must be set to "max" for reasons
Processed documentation: ModelCheckpoint saves a model each time gini improves.
===========================================================================================
Cleaned documentation: will never be reached anyway because of early stopping. I usually put 5000 there. Because why not.
Processed documentation: will never be reached anyway because of early stopping.
===========================================================================================
Cleaned documentation: model instance and load the model from the last saved checkpoint. Next we predict values both for
Processed documentation: and load the model from the last saved checkpoint.
===========================================================================================
Cleaned documentation: We average all runs from the same fold and provide a parameter summary for each fold. Unless something
Processed documentation: We average all runs from the same fold and provide a parameter summary for each fold.
===========================================================================================
Cleaned documentation: We add up predictions on the test data for each fold. Create out-of-fold predictions for validation data.
Processed documentation: We add up predictions on the test data for each fold.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: I will skip augmentation here in this kernel. If you wish to use augmentation, uncomment the "if" line
Processed documentation: If you wish to use augmentation, uncomment the "if" line
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset. Overrides values in the base Config class.
Processed documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset.
===========================================================================================
Cleaned documentation: Train on 1 GPU and 8 images per GPU. We can put multiple images on each
Processed documentation: Train on 1 GPU and 8 images per GPU.
===========================================================================================
Cleaned documentation: Now it's time to train the model. Note that training even a basic model can take a few hours.
Processed documentation: Note that training even a basic model can take a few hours.
===========================================================================================
Cleaned documentation: How does the predicted box compared to the expected value? Let's use the validation dataset to check.
Processed documentation: How does the predicted box compared to the expected value?
===========================================================================================
Cleaned documentation: There are 262 records with negative fare. We will remove these records from the data
Processed documentation: There are 262 records with negative fare.
===========================================================================================
Cleaned documentation: if there are 7 passengers, fare amount is lower. Fare amount is higher for 6 passengers
Processed documentation: if there are 7 passengers, fare amount is lower.
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: fold_valid_files = [f for f in train_files if any([int(re.match("^train([0-9]+)", f.split("/")[-1]).group(1)) % 15 == i for i in idValid
Processed documentation: [f for f in train_files if any([int(re.match("^train([0-9]+)", f.split("/")[-1]).group(1))
===========================================================================================
Cleaned documentation: fold_train_files = [f for f in train_files if any([int(re.match("^train([0-9]+)", f.split("/")[-1]).group(1)) % 15 == i for i in idTrain
Processed documentation: [f for f in train_files if any([int(re.match("^train([0-9]+)", f.split("/")[-1]).group(1))
===========================================================================================
Cleaned documentation: fold_valid_names = np.concatenate([np.array([ni.decode("utf-8") for ni in n.numpy()]) for n in ds_valid.map(lambda i, n: n)],
Processed documentation: np.concatenate([np.array([ni.decode("utf-8") for ni in n.numpy()]) for n in ds_valid.map(lambda i, n: n)],
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Preprocessing Normalizing the images. Reshape the images to be of shape 224,224, Basic image augmentation like rotation, flipping etc.
Processed documentation: Reshape the images to be of shape 224,224, Basic image augmentation like rotation, flipping etc.
===========================================================================================
Cleaned documentation: Run cross validation on model against tournaments in. For the tournaments since 2014, the winning score
Processed documentation: Run cross validation on model against tournaments in.
===========================================================================================
Cleaned documentation: Let's check how it works. The example below creates UserID based on card_x, addr1, and FirstTransaction features.
Processed documentation: The example below creates UserID based on card_x, addr1, and FirstTransaction features.
===========================================================================================
Cleaned documentation: Masks tissue in image. Uses gray-scaled image, as well as. dilation kernels and 'gap filling
Processed documentation: Uses gray-scaled image, as well as.
===========================================================================================
Cleaned documentation: Inputs an image and transposes it, accepts. both 2-d (mask) and 3-d (rgb image) arrays.
Processed documentation: both 2-d (mask) and 3-d (rgb image) arrays.
===========================================================================================
Cleaned documentation: filters out a portion of excessive coordinates. coordinates with higher values are filtered out.
Processed documentation: filters out a portion of excessive coordinates.
===========================================================================================
Cleaned documentation: thus needing to erode the mask to get rid of it. Then some dilatation is
Processed documentation: thus needing to erode the mask to get rid of it.
===========================================================================================
Cleaned documentation: SO we have more GPE Entities than any other. We can also visualize the most common tokens per entity.
Processed documentation: We can also visualize the most common tokens per entity.
===========================================================================================
Cleaned documentation: cat_train_df.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in cat_train_df.columns
Processed documentation: ["".join (c if c.isalnum() else "_" for c in str(x)) for x in cat_train_df.columns
===========================================================================================
Cleaned documentation: lgb_train_df.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in lgb_train_df.columns
Processed documentation: ["".join (c if c.isalnum() else "_" for c in str(x)) for x in lgb_train_df.columns
===========================================================================================
Cleaned documentation: lgb_test_df.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in lgb_test_df.columns
Processed documentation: ["".join (c if c.isalnum() else "_" for c in str(x)) for x in lgb_test_df.columns
===========================================================================================
Cleaned documentation: III. Run it all model.create() -> dataset.create() -> train(train predict(val).decode() -> predict(test).decode() -> submit
Processed documentation: Run it all model.create() -> dataset.create() -> train(train predict(val).decode() -> predict(test).decode() -> submit
===========================================================================================
Cleaned documentation: Will be used to calculate the log loss. of the validation set in PredictionCheckpoint
Processed documentation: Will be used to calculate the log loss.
===========================================================================================
Cleaned documentation: Ohh my gosh!! we have class imbalance problem, need to balance it otherwise we will get skewed results.
Processed documentation: we have class imbalance problem, need to balance it otherwise we will get skewed results.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation. and remove words containing numbers.
Processed documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: All totals colums are in object datatype but needts to be in int or float64 datatype. Hence converting.
Processed documentation: All totals colums are in object datatype but needts to be in int or float64 datatype.
===========================================================================================
Cleaned documentation: Since there is only one value in the totals.visits column it is useless. Hence droping it.
Processed documentation: Since there is only one value in the totals.visits column it is useless.
===========================================================================================
Cleaned documentation: ind_cco_fin_ult1" is the dominant product. Now let's exclude it to investigate further the other products.
Processed documentation: Now let's exclude it to investigate further the other products.
===========================================================================================
Cleaned documentation: Conclusions The __dataset is very imbalanced. Data augmentation and resampling techniques will be required to perform the defect detection.
Processed documentation: Data augmentation and resampling techniques will be required to perform the defect detection.
===========================================================================================
Cleaned documentation: Function to plot an image and segmentation masks. INPUT: image_filename - filename of the image (with full path
Processed documentation: Function to plot an image and segmentation masks.
===========================================================================================
Cleaned documentation: Function to visualize several segmentation maps. INPUT: image_id - filename of the image. RETURNS: np_mask - numpy segmentation map.
Processed documentation: RETURNS: np_mask - numpy segmentation map.
===========================================================================================
Cleaned documentation: Function to visualize the image and the mask. INPUT: line_id - id of the line to visualize the masks.
Processed documentation: INPUT: line_id - id of the line to visualize the masks.
===========================================================================================
Cleaned documentation: Function to plot several random images with segmentation masks. INPUT: n_images - number of images to visualize.
Processed documentation: Function to plot several random images with segmentation masks.
===========================================================================================
Cleaned documentation: Distribution of mask area sizes That's an interesting question. I'll observe the mask area sizes distribution for each label.
Processed documentation: Distribution of mask area sizes That's an interesting question.
===========================================================================================
Cleaned documentation: Generate a neuron network of a given size. Return a vector of two dimensional points in the interval [0,1].
Processed documentation: Generate a neuron network of a given size.
===========================================================================================
Cleaned documentation: python_trace_param_automata" applies sequence of rules to the input grid. What are these commands are written below.
Processed documentation: python_trace_param_automata" applies sequence of rules to the input grid.
===========================================================================================
Cleaned documentation: We never use it. We just store zeros and pass it through all the commands.
Processed documentation: We just store zeros and pass it through all the commands.
===========================================================================================
Cleaned documentation: This is the program that containes these 4 parts. All of them are trained with genetic algorithm.
Processed documentation: This is the program that containes these 4 parts.
===========================================================================================
Cleaned documentation: These are grid->grid rules. There are many of them, so this function is pretty big.
Processed documentation: These are grid->grid rules.
===========================================================================================
Cleaned documentation: This is the application of the given cellular automata to the grid. CA-rules used to solve something like this
Processed documentation: This is the application of the given cellular automata to the grid.
===========================================================================================
Cleaned documentation: Here are some parameters of the genetics. Better not to touch them, it may be dangerous
Processed documentation: Here are some parameters of the genetics.
===========================================================================================
Cleaned documentation: Return 2 lists of tuples: [(class_id, user_id, path), ...] for train. class_id, user_id, path), ...] for validation.
Processed documentation: [(class_id, user_id, path), ...] for train.
===========================================================================================
Cleaned documentation: Define some params. Move model hyperparams (optimizer, extractor, num of layers, activation fn, ...) here
Processed documentation: Move model hyperparams (optimizer, extractor, num of layers, activation fn, ...) here
===========================================================================================
Cleaned documentation: define evaluation method for a given model. we use k-fold cross validation on the training set.
Processed documentation: we use k-fold cross validation on the training set.
===========================================================================================
Cleaned documentation: param sentences: a list of list of words. return: a dictionary of words and their frequency.
Processed documentation: param sentences: a list of list of words.
===========================================================================================
Cleaned documentation: good coverage but the top missing words all have contractions. We deal with those next...
Processed documentation: good coverage but the top missing words all have contractions.
===========================================================================================
Cleaned documentation: Much better... Quorans is the top word missed now...I wonder if Quora or quora is in the embeddings_index vocabulary..
Processed documentation: I wonder if Quora or quora is in the embeddings_index vocabulary..
===========================================================================================
Cleaned documentation: Good...So let us replace all heights of the form \d\'\d such as 5'4 with the "5 foot 4"..
Processed documentation: So let us replace all heights of the form \d\'\d such as 5'4 with the "5 foot 4"..
===========================================================================================
Cleaned documentation: There are also frequent mentions of height..I wonder how that will affect the results...
Processed documentation: There are also frequent mentions of height..
===========================================================================================
Cleaned documentation: Better! Quorans seems to be a repeatedly missed word in this embedding as well...
Processed documentation: Quorans seems to be a repeatedly missed word in this embedding as well...
===========================================================================================
Cleaned documentation: This is inference code for [Yandex Praktikum PyTorch train baseline - LB 0. I was inspired thos [code
Processed documentation: This is inference code for [Yandex Praktikum PyTorch train baseline - LB 0.
===========================================================================================
Cleaned documentation: Let us apply the simple integer mapping for now and go ahead. We sort of flatten the dataframe
Processed documentation: Let us apply the simple integer mapping for now and go ahead.
===========================================================================================
Cleaned documentation: the ugly hack again above. Somehow unable to get list into a cell without this hack
Processed documentation: Somehow unable to get list into a cell without this hack
===========================================================================================
Cleaned documentation: For those that changed more than 1, I would say yes. Let's have a closer look for them
Processed documentation: Let's have a closer look for them
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to apply transformation to. Returns: PIL Image: Image with transformation.
Processed documentation: Args: img (PIL Image): Image to apply transformation to.
===========================================================================================
Cleaned documentation: Ok, both groups look somehow far away from the others. Let's take the smaller one: 3456, 5184.
Processed documentation: Ok, both groups look somehow far away from the others.
===========================================================================================
Cleaned documentation: First of all, we need some fooling targets. For our example digit all others are possible
Processed documentation: First of all, we need some fooling targets.
===========================================================================================
Cleaned documentation: With -320 we are separating between lungs (-700) /air (-1000) and tissue with values close to water (0).
Processed documentation: and tissue with values close to water (0).
===========================================================================================
Cleaned documentation: We can find multiple segments of sequences in the test folder. Let's peek at their names
Processed documentation: We can find multiple segments of sequences in the test folder.
===========================================================================================
Cleaned documentation: In this case all predictions are zero. Let's compute the loss for the median
Processed documentation: Let's compute the loss for the median
===========================================================================================
Cleaned documentation: Gets bin nb and offset from that number. params: - angle: Angle in radians [0, 2π
Processed documentation: Gets bin nb and offset from that number.
===========================================================================================
Cleaned documentation: Takes bin + offset and using the ray angle. returns the global rotation of the car.
Processed documentation: Takes bin + offset and using the ray angle.
===========================================================================================
Cleaned documentation: Returns the shortest distance in degrees between two angles. Parameters: - angle1, angle2: (Degrees
Processed documentation: Returns the shortest distance in degrees between two angles.
===========================================================================================
Cleaned documentation: Balance bin number to reduce (as much as possible) biases. Cars in 0/4 (facing frontwards/backwards) are the majority
Processed documentation: Balance bin number to reduce (as much as possible) biases.
===========================================================================================
Cleaned documentation: I will use a simple ResNet approach. The speed must be a requirement for a Autonomous driving application.
Processed documentation: The speed must be a requirement for a Autonomous driving application.
===========================================================================================
Cleaned documentation: The actual `brown` corpus data is packaged as raw text files. And you can find their IDs with
Processed documentation: The actual `brown` corpus data is packaged as raw text files.
===========================================================================================
Cleaned documentation: Gotcha! The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags.
Processed documentation: The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags.
===========================================================================================
Cleaned documentation: Parameters. sceneId : str. sceneId like "6010_0_4" Returns. polygonsList : dict. Keys are CLASSES. Values are shapely polygons.
Processed documentation: sceneId like "6010_0_4" Returns.
===========================================================================================
Cleaned documentation: Co-ordinate values are varying too much. We can take test_dataset coordinate min and max limit to drop the outliers.
Processed documentation: We can take test_dataset coordinate min and max limit to drop the outliers.
===========================================================================================
Cleaned documentation: This looks very interesting. It seems like there is some order. What do you think
Processed documentation: This looks very interesting.
===========================================================================================
Cleaned documentation: Most correlation values are around 0.5, we can consider our variables moderately correlated. Categorical variables
Processed documentation: Most correlation values are around 0.5, we can consider our variables moderately correlated.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Some callbacks for stopping the training process if validation loss stops decreasing. And a learning rate reduction during training
Processed documentation: Some callbacks for stopping the training process if validation loss stops decreasing.
===========================================================================================
Cleaned documentation: Following code takes the first [0] (or zero) column of the train data. Then divides by rows (or 150,000).
Processed documentation: Following code takes the first [0] (or zero) column of the train data.
===========================================================================================
Cleaned documentation: Then rounds the value down (or floor. Then sets the value as an integer.
Processed documentation: Then rounds the value down (or floor.
===========================================================================================
Cleaned documentation: Thank You for visiting this Notebook!!If you like it Please UPVOTE!!It motivates me to learn more.
Processed documentation: Thank You for visiting this Notebook!!If you like it
===========================================================================================
Cleaned documentation: Now, let's do the actual counting. We are going to use pandas dataframe groupby method for that.
Processed documentation: We are going to use pandas dataframe groupby method for that.
===========================================================================================
Cleaned documentation: GCDM is needed to load certain DICOM files. Not strictly needed here, but just in case.
Processed documentation: GCDM is needed to load certain DICOM files.
===========================================================================================
Cleaned documentation: Read in test data | Remember! We're using data augmentation like we use for Train data.
Processed documentation: We're using data augmentation like we use for Train data.
===========================================================================================
Cleaned documentation: Randomly pull timestamps and subtract from each other. Get histogram of these values to estimate autocorrelation
Processed documentation: Get histogram of these values to estimate autocorrelation
===========================================================================================
Cleaned documentation: Returns a plot of the original Image and Encoded ones. n: number of images to display.
Processed documentation: Returns a plot of the original Image and Encoded ones.
===========================================================================================
Cleaned documentation: extend()`: Extend list by appending elements from the iterable. The code below does the following
Processed documentation: extend()`: Extend list by appending elements from the iterable.
===========================================================================================
Cleaned documentation: Alaska2 Dataset. If data is test or eval, it skips the transformations applied to training part.
Processed documentation: If data is test or eval, it skips the transformations applied to training part.
===========================================================================================
Cleaned documentation: Convert mask to rle. img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Let us look at the JSON subcolumns in `data1`... Lets first look at the first observation in `customDimensions
Processed documentation: Let us look at the JSON subcolumns in `data1`...
===========================================================================================
Cleaned documentation: Creates a world plot, mark the north pole. Option to show primes in different collor.
Processed documentation: Creates a world plot, mark the north pole.
===========================================================================================
Cleaned documentation: Now we can bulk insert all cities from a dataframe into our world. Let's do that
Processed documentation: Now we can bulk insert all cities from a dataframe into our world.
===========================================================================================
Cleaned documentation: The first id seems to represent three different cars or video sessions. How many unique camera angles are there
Processed documentation: The first id seems to represent three different cars or video sessions.
===========================================================================================
Cleaned documentation: Plots Multiple Images. Reads, resizes, applies preprocessing if desired and plots multiple images from a given dataframe.
Processed documentation: Reads, resizes, applies preprocessing if desired and plots multiple images from a given dataframe.
===========================================================================================
Cleaned documentation: Preparing test data to get predictions. I had to split the data into parts, because of memory constraints.
Processed documentation: I had to split the data into parts, because of memory constraints.
===========================================================================================
Cleaned documentation: Lets look at the number of rows for each state. Value_counts give you that
Processed documentation: Lets look at the number of rows for each state.
===========================================================================================
Cleaned documentation: Let's have a look at the ratio of the number of rows. Normalize = True gives you the ratio
Processed documentation: Let's have a look at the ratio of the number of rows.
===========================================================================================
Cleaned documentation: Plotting a bar graph of avg. sales on the weekend days before the event to see the impact
Processed documentation: sales on the weekend days before the event to see the impact
===========================================================================================
Cleaned documentation: Plotting a bar graph of avg. sales on the weekend days before the event to see the impact
Processed documentation: sales on the weekend days before the event to see the impact
===========================================================================================
Cleaned documentation: XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.
Processed documentation: We can use LabelEncoder to assign labels to categorical variables.
===========================================================================================
Cleaned documentation: Calculate average number of the visitors per day of the week. Holiday is treated as day of the week
Processed documentation: Calculate average number of the visitors per day of the week.
===========================================================================================
Cleaned documentation: Demanda_uni_equil` is the target value that we are trying to predict. Let's take a look at the distribution
Processed documentation: Demanda_uni_equil` is the target value that we are trying to predict.
===========================================================================================
Cleaned documentation: However we found that it had almost no impact. Applying it to the total text, would kill correct spellings.
Processed documentation: Applying it to the total text, would kill correct spellings.
===========================================================================================
Cleaned documentation: The following function is used to format the numbers. In the beginning "th, st, nd, rd" are removed.
Processed documentation: In the beginning "th, st, nd, rd" are removed.
===========================================================================================
Cleaned documentation: print("Yr and hr have been replaced by year and hour. Time since start: ", timediff(time
Processed documentation: print("Yr and hr have been replaced by year and hour.
===========================================================================================
Cleaned documentation: Randomly sampling indexes to add images to Validation set. Make changes to validation set if required.
Processed documentation: Randomly sampling indexes to add images to Validation set.
===========================================================================================
Cleaned documentation: q1 and q2 are lists. Function returns a list of POS tagged tokens for both.
Processed documentation: Function returns a list of POS tagged tokens for both.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: creating a 3-D representation of the lungs try other filters analyze the metadata more closely. Thanks for reading
Processed documentation: creating a 3-D representation of the lungs try other filters analyze the metadata more closely.
===========================================================================================
Cleaned documentation: Get the fourier power spectrum of a given segment. Returns the quake_time, frequencies, and power_spectrum.
Processed documentation: Get the fourier power spectrum of a given segment.
===========================================================================================
Cleaned documentation: Calculate similarities among all image pairs. Divide the value by 256 to normalize (0-1).
Processed documentation: Calculate similarities among all image pairs.
===========================================================================================
Cleaned documentation: Looks better. Next `ind_nuevo`, which indicates whether a customer is new or not. How many missing values are there
Processed documentation: Next `ind_nuevo`, which indicates whether a customer is new or not.
===========================================================================================
Cleaned documentation: That number again. Probably the same people that we just determined were new customers. Double check.
Processed documentation: Probably the same people that we just determined were new customers.
===========================================================================================
Cleaned documentation: Objective function for Gradient Boosting Machine Hyperparameter Optimization. Writes a new line to `outfile` on every iteration.
Processed documentation: Objective function for Gradient Boosting Machine Hyperparameter Optimization.
===========================================================================================
Cleaned documentation: train.groupby(['primary_use','meter'])['meter_reading'].mean().sort_values(ascending = False).reset_index().plot(kind = 'bar', figsize = (20,6), color = 'green', title = 'Mean Energy consumption by Primary use
Processed documentation: False).reset_index().plot(kind = 'bar', figsize = (20,6), color = 'green', title = 'Mean Energy consumption by Primary use
===========================================================================================
Cleaned documentation: Finally lets get down to building level. Lets check the general trend of energy consumption by buildings.
Processed documentation: Lets check the general trend of energy consumption by buildings.
===========================================================================================
Cleaned documentation: train.groupby(['primary_use','meter'])['meter_reading'].mean().sort_values(ascending = False).reset_index().plot(kind = 'bar', figsize = (20,6), color = 'green', title = 'Mean Energy consumption by Primary use
Processed documentation: False).reset_index().plot(kind = 'bar', figsize = (20,6), color = 'green', title = 'Mean Energy consumption by Primary use
===========================================================================================
Cleaned documentation: train.groupby(['primary_use','meter'])['meter_reading'].mean().sort_values(ascending = False).reset_index().plot(kind = 'bar', figsize = (20,6), color = 'green', title = 'Mean Energy consumption by Primary use
Processed documentation: False).reset_index().plot(kind = 'bar', figsize = (20,6), color = 'green', title = 'Mean Energy consumption by Primary use
===========================================================================================
Cleaned documentation: It is site 13. Lets check the consumption trend only for Site 13 after removing building 1099.
Processed documentation: Lets check the consumption trend only for Site 13 after removing building 1099.
===========================================================================================
Cleaned documentation: Thanks to this kernel: for the following code. Check her script out, it's really cool.
Processed documentation: Thanks to this kernel: for the following code.
===========================================================================================
Cleaned documentation: There is a lot of variation here. Probably if we could exploit the inherent
Processed documentation: There is a lot of variation here.
===========================================================================================
Cleaned documentation: Pretty neat now huh? Let's see if all this monkeying around has been useful or not.
Processed documentation: Let's see if all this monkeying around has been useful or not.
===========================================================================================
Cleaned documentation: Seems like action type gave a good kick to our prediction capability. The difference in
Processed documentation: Seems like action type gave a good kick to our prediction capability.
===========================================================================================
Cleaned documentation: Ordering it only made our graph look nicer. It would impact another classifier though,
Processed documentation: Ordering it only made our graph look nicer.
===========================================================================================
Cleaned documentation: And we lose improvement again. Let's see what matchup has to offer. We notice that there are two
Processed documentation: Let's see what matchup has to offer.
===========================================================================================
Cleaned documentation: Some minor improvements have occured. Try forking this notebook and looking at other fields
Processed documentation: Try forking this notebook and looking at other fields
===========================================================================================
Cleaned documentation: We focus our attention on tuning the classifier. To keep things simple we will use the same
Processed documentation: We focus our attention on tuning the classifier.
===========================================================================================
Cleaned documentation: Let's predict based on all this work for now. We avoid looking at Leakage for the time
Processed documentation: Let's predict based on all this work for now.
===========================================================================================
Cleaned documentation: After we build or model, we compile it. We use Adam as the optimizer (safest choice
Processed documentation: We use Adam as the optimizer (safest choice
===========================================================================================
Cleaned documentation: returns the dataframe describing nulls and unique counts. inp: dataframe. returns: dataframe with unique and null counts.
Processed documentation: returns the dataframe describing nulls and unique counts.
===========================================================================================
Cleaned documentation: The above graph doesn't provide a very clear picture. It is better to view this data with bigger resolution.
Processed documentation: It is better to view this data with bigger resolution.
===========================================================================================
Cleaned documentation: main function for ploting calendar heatmaps. inp: year (which year to be plotted) data_col (data column) returns nothing.
Processed documentation: inp: year (which year to be plotted) data_col (data column) returns nothing.
===========================================================================================
Cleaned documentation: Let's do something more. Let's drop all the features where 98% of that feature is zero.
Processed documentation: Let's drop all the features where 98% of that feature is zero.
===========================================================================================
Cleaned documentation: This function is used to create spherical regions in binary masks. at the given locations and radius.
Processed documentation: This function is used to create spherical regions in binary masks.
===========================================================================================
Cleaned documentation: Check relationship between SNAP days and sales. Are there more products being sold on snap days
Processed documentation: Check relationship between SNAP days and sales.
===========================================================================================
Cleaned documentation: Check relationship between Event days and sales. Are there more products being sold on event days
Processed documentation: Check relationship between Event days and sales.
===========================================================================================
Cleaned documentation: So you have patients that have multiple images. Also apparently the data is imbalanced. Let's verify
Processed documentation: So you have patients that have multiple images.
===========================================================================================
Cleaned documentation: What to do with a batch in a forward. Usually simple if everything is already defined in the model.
Processed documentation: Usually simple if everything is already defined in the model.
===========================================================================================
Cleaned documentation: Simply define a pytorch dataloader here that will take care of batching. Note it works well with dictionnaries
Processed documentation: Simply define a pytorch dataloader here that will take care of batching.
===========================================================================================
Cleaned documentation: Same but for validation. Pytorch lightning allows multiple validation dataloaders hence why I return a list.
Processed documentation: Pytorch lightning allows multiple validation dataloaders hence why I return a list.
===========================================================================================
Cleaned documentation: This is what happens at the end of validation epoch. Usually gathering all predictions
Processed documentation: This is what happens at the end of validation epoch.
===========================================================================================
Cleaned documentation: Step 5-6. Continuation of the halite collection and the flight of the created ship to the new halite deposit.
Processed documentation: Continuation of the halite collection and the flight of the created ship to the new halite deposit.
===========================================================================================
Cleaned documentation: Step 7. Delivery of collected halite to the shipyard and continuation of halite collection.
Processed documentation: Delivery of collected halite to the shipyard and continuation of halite collection.
===========================================================================================
Cleaned documentation: model6_1 = build_model3(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, dense_units=16, dr=0.1, use_attention=False
Processed documentation: 0.3, dense_units=16, dr=0.1, use_attention=False
===========================================================================================
Cleaned documentation: model8 = build_model4(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs
Processed documentation: model8 = build_model4(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr =
===========================================================================================
Cleaned documentation: model9 = build_model5(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs
Processed documentation: 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs
===========================================================================================
Cleaned documentation: And now let's try stacking :) We can use the same function for it.
Processed documentation: And now let's try stacking :)
===========================================================================================
Cleaned documentation: So there are 4 numerical variables and 1 categorical. And no missing values, which is nice
Processed documentation: So there are 4 numerical variables and 1 categorical.
===========================================================================================
Cleaned documentation: Funny, but many colors are evenly distributes among the monsters. So they maybe nor very useful for analysis.
Processed documentation: Funny, but many colors are evenly distributes among the monsters.
===========================================================================================
Cleaned documentation: I used the best parameters and validation accuracy is ~68-72%. Not bad. But let's try something else.
Processed documentation: I used the best parameters and validation accuracy is ~68-72%.
===========================================================================================
Cleaned documentation: So this means that only 278 loans have some other type. Let's fo deeper.
Processed documentation: So this means that only 278 loans have some other type.
===========================================================================================
Cleaned documentation: Now we draw them on a chord chart. The symmetry in the graph is clearly visible.
Processed documentation: Now we draw them on a chord chart.
===========================================================================================
Cleaned documentation: Let's create some aggregations. There is no logic in them - simply aggregations on top features.
Processed documentation: Let's create some aggregations.
===========================================================================================
Cleaned documentation: Well, these were some random companies. But it would be more interesting to see general trends of prices.
Processed documentation: But it would be more interesting to see general trends of prices.
===========================================================================================
Cleaned documentation: if open price is too far from mean open price for this company, replace it. Otherwise replace close price.
Processed documentation: if open price is too far from mean open price for this company, replace it.
===========================================================================================
Cleaned documentation: There are three main user_types. Let's see prices of their wares, where prices are below 100000.
Processed documentation: Let's see prices of their wares, where prices are below 100000.
===========================================================================================
Cleaned documentation: Plotting countplot. Making annotations is a bit more complicated, because we need to iterate over axes.
Processed documentation: Making annotations is a bit more complicated, because we need to iterate over axes.
===========================================================================================
Cleaned documentation: It seems that almost one thousand pets have mismatch in breeds and fur lengths. Let's see
Processed documentation: It seems that almost one thousand pets have mismatch in breeds and fur lengths.
===========================================================================================
Cleaned documentation: We can see that budget and revenue are somewhat correlated. Logarithm transformation makes budget distribution more managable.
Processed documentation: Logarithm transformation makes budget distribution more managable.
===========================================================================================
Cleaned documentation: param cols: Categorical columns. param encoder: Encoder class. param folds: Folds to split the data.
Processed documentation: param folds: Folds to split the data.
===========================================================================================
Cleaned documentation: SHAP Another interesting tool is SHAP. It also provides explanations for a variety of models.
Processed documentation: SHAP Another interesting tool is SHAP.
===========================================================================================
Cleaned documentation: Sklearn feature selection Sklearn has several methods to do feature selection. Let's try some of them
Processed documentation: Sklearn feature selection Sklearn has several methods to do feature selection.
===========================================================================================
Cleaned documentation: Now, let's make some plots. I'd like to see sales over time in different shops.
Processed documentation: I'd like to see sales over time in different shops.
===========================================================================================
Cleaned documentation: Well... this doesn't look pretty. Let's make is smoother - I'll plot rolling mean over 30 days
Processed documentation: Let's make is smoother - I'll plot rolling mean over 30 days
===========================================================================================
Cleaned documentation: We have more than 200 columns which seem to be anonymized. Let's have a quick look at them.
Processed documentation: Let's have a quick look at them.
===========================================================================================
Cleaned documentation: The column seem to have a normal distribution. This reminds me of the recent Santander competition...
Processed documentation: This reminds me of the recent Santander competition...
===========================================================================================
Cleaned documentation: Plot default feature importance. param drop_null_importance: drop columns with null feature importance. param top_n: show top n columns. return
Processed documentation: param drop_null_importance: drop columns with null feature importance.
===========================================================================================
Cleaned documentation: printing because I'm too lazy to write everything by hand. Open output to see.
Processed documentation: printing because I'm too lazy to write everything by hand.
===========================================================================================
Cleaned documentation: In `TeamSpellings` we have info about all spellings of all teams. Let's use this as a feature
Processed documentation: In `TeamSpellings` we have info about all spellings of all teams.
===========================================================================================
Cleaned documentation: Helper function that adds columns relevant to a date in the column `field_name` of `df. from fastai
Processed documentation: Helper function that adds columns relevant to a date in the column `field_name` of `df.
===========================================================================================
Cleaned documentation: Plot default feature importance. param drop_null_importance: drop columns with null feature importance. param top_n: show top n columns. return
Processed documentation: param drop_null_importance: drop columns with null feature importance.
===========================================================================================
Cleaned documentation: Categorical transformer. This is a wrapper for categorical encoders. param cat_cols: :param drop_original: :param encoder
Processed documentation: param cat_cols: :param drop_original: :param encoder
===========================================================================================
Cleaned documentation: Replaces unusual punctuation with normal. param text: text to clean. param mapping: dict with mapping. return: cleaned text.
Processed documentation: param text: text to clean.
===========================================================================================
Cleaned documentation: columns = top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i
Processed documentation: columns = top_features =
===========================================================================================
Cleaned documentation: So most of these features are useless. In fact, let's repeat this analysis for all features.
Processed documentation: In fact, let's repeat this analysis for all features.
===========================================================================================
Cleaned documentation: And now let's try to add some new features. I'll use NearestNeighbors model and find statistics on similar rows.
Processed documentation: I'll use NearestNeighbors model and find statistics on similar rows.
===========================================================================================
Cleaned documentation: Most of comments aren't toxic. We can also see some spikes in the distribution...
Processed documentation: Most of comments aren't toxic.
===========================================================================================
Cleaned documentation: Convert mask to rle. img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Plot image and masks. If two pairs of images and masks are passes, show both.
Processed documentation: If two pairs of images and masks are passes, show both.
===========================================================================================
Cleaned documentation: Plot image and masks. If two pairs of images and masks are passes, show both.
Processed documentation: If two pairs of images and masks are passes, show both.
===========================================================================================
Cleaned documentation: Post processing of each predicted mask, components with lesser number of pixels. than `min_size` are ignored.
Processed documentation: Post processing of each predicted mask, components with lesser number of pixels.
===========================================================================================
Cleaned documentation: This is how original image and its masks look like. Let's try adding some augmentations
Processed documentation: This is how original image and its masks look like.
===========================================================================================
Cleaned documentation: Load in the training data as follows. Note that there are not many columns in our dataset.
Processed documentation: Load in the training data as follows.
===========================================================================================
Cleaned documentation: of players didn't have a single kill. 88% of players have only less than 3 kills.
Processed documentation: of players didn't have a single kill.
===========================================================================================
Cleaned documentation: Good, it will make it easier for us to process data. now we get the whole info
Processed documentation: Good, it will make it easier for us to process data.
===========================================================================================
Cleaned documentation: Now that we've downloaded our datasets, we can load them into pandas dataframes. First for the test data
Processed documentation: Now that we've downloaded our datasets, we can load them into pandas dataframes.
===========================================================================================
Cleaned documentation: Let's now look at scoring all the annotations at once. To do that, here's our annotation matrix
Processed documentation: Let's now look at scoring all the annotations at once.
===========================================================================================
Cleaned documentation: Unclear why fails to open [encoding error], format is same as for glove. Will Debug, Dan
Processed documentation: [encoding error], format is same as for glove.
===========================================================================================
Cleaned documentation: Finds features with only a single unique value. NaNs do not count as a unique value.
Processed documentation: Finds features with only a single unique value.
===========================================================================================
Cleaned documentation: Check the identified features before removal. Returns a list of the unique features identified.
Processed documentation: Returns a list of the unique features identified.
===========================================================================================
Cleaned documentation: Sales_dist used for Checing Sales Distribution. data : contain data frame which contain sales data.
Processed documentation: data : contain data frame which contain sales data.
===========================================================================================
Cleaned documentation: Finds features with only a single unique value. NaNs do not count as a unique value.
Processed documentation: Finds features with only a single unique value.
===========================================================================================
Cleaned documentation: Check the identified features before removal. Returns a list of the unique features identified.
Processed documentation: Returns a list of the unique features identified.
===========================================================================================
Cleaned documentation: The X shape here is very important. It is also important undertand a little how a LSTM works
Processed documentation: It is also important undertand a little how a LSTM works
===========================================================================================
Cleaned documentation: This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an
Processed documentation: This checkpoint helps to avoid overfitting.
===========================================================================================
Cleaned documentation: used for converting the decoded image to rle mask. Fast compared to previous one.
Processed documentation: used for converting the decoded image to rle mask.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: For valid data, keep only those with universe. This will help calculate the metric
Processed documentation: For valid data, keep only those with universe.
===========================================================================================
Cleaned documentation: Get accuracy of model on validation data. It's not AUC but it's something at least
Processed documentation: Get accuracy of model on validation data.
===========================================================================================
Cleaned documentation: Get accuracy of model on validation data. It's not AUC but it's something at least
Processed documentation: Get accuracy of model on validation data.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: Both the plot looks pretty same. For a minute I thought that I have done some mistake but not.
Processed documentation: For a minute I thought that I have done some mistake but not.
===========================================================================================
Cleaned documentation: This function helps to investigate the proportion of visits and total of transction revenue. by each category.
Processed documentation: This function helps to investigate the proportion of visits and total of transction revenue.
===========================================================================================
Cleaned documentation: Let's now try to predict using random guess but stratified, using dummy classifier. Note difference between score and QWK.
Processed documentation: Let's now try to predict using random guess but stratified, using dummy classifier.
===========================================================================================
Cleaned documentation: Let's use stratified strategy. So, this classifier will generate predictions by respecting the training set’s class distribution.
Processed documentation: So, this classifier will generate predictions by respecting the training set’s class distribution.
===========================================================================================
Cleaned documentation: We now have a model with a CV score of 0.. Nice! Let's submit that
Processed documentation: We now have a model with a CV score of 0..
===========================================================================================
Cleaned documentation: Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces
Processed documentation: Disable multiprocesing for numpy/opencv.
===========================================================================================
Cleaned documentation: De-normalizes database to create reverse indices for common cases. Args: verbose: Whether to print outputs.
Processed documentation: De-normalizes database to create reverse indices for common cases.
===========================================================================================
Cleaned documentation: We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.
Processed documentation: We quantize to uint8 here to conserve memory.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: There are two ways we can compute the autocorrelation in Python. The first method is numpy.correlate
Processed documentation: There are two ways we can compute the autocorrelation in Python.
===========================================================================================
Cleaned documentation: Listen to the original audio plus the detected onsets. One way is to add the signals together, sample-wise
Processed documentation: Listen to the original audio plus the detected onsets.
===========================================================================================
Cleaned documentation: Notice how the vertical lines denoting each segment boundary appears before each rise in energy. Concatenate the segments
Processed documentation: Notice how the vertical lines denoting each segment boundary appears before each rise in energy.
===========================================================================================
Cleaned documentation: The tempo of this excerpt is about 58/116 BPM. Compute the onset envelope, i.e. novelty function
Processed documentation: Compute the onset envelope, i.e. novelty function
===========================================================================================
Cleaned documentation: Grosche, Peter, Meinard Müller, and Frank Kurth. Cyclic tempogram - A mid-level tempo representation for music signals.” ICASSP, 2010.
Processed documentation: Grosche, Peter, Meinard Müller, and Frank Kurth.
===========================================================================================
Cleaned documentation: Estimating Global TempoÂ We will use librosa.beat.tempo to estimate the global tempo in an audio file. Estimate the tempo
Processed documentation: We will use librosa.beat.tempo to estimate the global tempo in an audio file.
===========================================================================================
Cleaned documentation: Visually, it's difficult to tell how correct the estimated beats are. Let's listen to a click track
Processed documentation: Visually, it's difficult to tell how correct the estimated beats are.
===========================================================================================
Cleaned documentation: We are dealing with images with different size som RGB some in Grey Scale. I
Processed documentation: We are dealing with images with different size som RGB some in Grey Scale.
===========================================================================================
Cleaned documentation: from this kernel the idea of using X derivaate. What an amazing 3D effect
Processed documentation: from this kernel the idea of using X derivaate.
===========================================================================================
Cleaned documentation: Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces
Processed documentation: Disable multiprocesing for numpy/opencv.
===========================================================================================
Cleaned documentation: We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.
Processed documentation: We quantize to uint8 here to conserve memory.
===========================================================================================
Cleaned documentation: First of all we are going to explore the Members dataset. We are dealing with 34403 members.
Processed documentation: First of all we are going to explore the Members dataset.
===========================================================================================
Cleaned documentation: We drop the "Date" column and print all the places once. PS: I do not like asterisks
Processed documentation: We drop the "Date" column and print all the places once.
===========================================================================================
Cleaned documentation: Submissions are evaluated on the mean column-wise Spearman's correlation coefficient. Let's see how our model performs on validation data
Processed documentation: Submissions are evaluated on the mean column-wise Spearman's correlation coefficient.
===========================================================================================
Cleaned documentation: Let's deal with regionidzip first. We could apply the same trick onto regionidzip as well as regionidcity
Processed documentation: We could apply the same trick onto regionidzip as well as regionidcity
===========================================================================================
Cleaned documentation: FC (fully conected) слоев. Накинуть скор можно за счет изменения параметров, конфигурации самой сети, подключения предобученной модели (например, ResNet).
Processed documentation: Накинуть скор можно за счет изменения параметров, конфигурации самой сети, подключения предобученной модели (например, ResNet).
===========================================================================================
Cleaned documentation: FVC is highly co-related to Percentage. They both share Linear Relationship. if Percentage Increases, so does FVC.
Processed documentation: FVC is highly co-related to Percentage.
===========================================================================================
Cleaned documentation: Let's check out the test results. For this we create a dataframe of actual and predicted classes
Processed documentation: For this we create a dataframe of actual and predicted classes
===========================================================================================
Cleaned documentation: Now to view the images. `show_images` provides a convenient way of viewing mutiple images.
Processed documentation: `show_images` provides a convenient way of viewing mutiple images.
===========================================================================================
Cleaned documentation: Although skewness changes, correlation doesn't seem to change at all. We can decide to drop it.
Processed documentation: Although skewness changes, correlation doesn't seem to change at all.
===========================================================================================
Cleaned documentation: That's it. Let's make the submission. Last time I checked, the submission file returned 2688.84 (Private) and 2673.97 (Public).
Processed documentation: Last time I checked, the submission file returned 2688.84
===========================================================================================
Cleaned documentation: Seems like the dataset is free of missing value. Now, let's see each text length.
Processed documentation: Seems like the dataset is free of missing value.
===========================================================================================
Cleaned documentation: Ensembling some models often gives you better score. Let's try some combination of ensemble models
Processed documentation: Ensembling some models often gives you better score.
===========================================================================================
Cleaned documentation: Creating dataframe. Do not run this code to get dataframe. It is available in input data
Processed documentation: Do not run this code to get dataframe.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: One last correction term for the final log-probabilities. Again, it is not important for
Processed documentation: One last correction term for the final log-probabilities.
===========================================================================================
Cleaned documentation: Finally, here is the prediction plot for var_108. The spike in the middle is nicely recognized.
Processed documentation: Finally, here is the prediction plot for var_108.
===========================================================================================
Cleaned documentation: Criterion for the grapheme root head. Select `criterion_key` to determine which loss you want to use for this head
Processed documentation: Select `criterion_key` to determine which loss you want to use for this head
===========================================================================================
Cleaned documentation: Now unfreeze the model and train with different settings. It is upto you !. Uncomment this cell to continue
Processed documentation: Now unfreeze the model and train with different settings.
===========================================================================================
Cleaned documentation: Criterion for the grapheme root head. Select `criterion_key` to determine which loss you want to use for this head
Processed documentation: Select `criterion_key` to determine which loss you want to use for this head
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: create group- and match- level stat features. Merge all useful features together while removing useless ones.
Processed documentation: Merge all useful features together while removing useless ones.
===========================================================================================
Cleaned documentation: remove outliers. Also returns data for the outliers. Maye I need to keep them...
Processed documentation: Also returns data for the outliers.
===========================================================================================
Cleaned documentation: use transform to prepare the test data. It will use some data calculated in fit_transform.
Processed documentation: use transform to prepare the test data.
===========================================================================================
Cleaned documentation: Step 4. Swap the groups' predicted winPlacePercs if the second neural net predicts that they should be swapped
Processed documentation: Swap the groups' predicted winPlacePercs if the second neural net predicts that they should be swapped
===========================================================================================
Cleaned documentation: Looks good! We see significant progress being made during training. Now, let's unleash the beast
Processed documentation: We see significant progress being made during training.
===========================================================================================
Cleaned documentation: Done! We have our final predictions. Let's find out by how much this model outperforms the others.
Processed documentation: Let's find out by how much this model outperforms the others.
===========================================================================================
Cleaned documentation: PS: not all of them are shiny objects. Some other are just plainly screwed images, like this one
Processed documentation: PS: not all of them are shiny objects.
===========================================================================================
Cleaned documentation: total reward = `value` in the future. We took an action with log probability
Processed documentation: We took an action with log probability
===========================================================================================
Cleaned documentation: Perform KS-Test for each feature from train/test. Draw its distribution. Count features based on statistics.
Processed documentation: Perform KS-Test for each feature from train/test.
===========================================================================================
Cleaned documentation: Plots are hidden. If you'd like to look at them - press "Output" button.
Processed documentation: If you'd like to look at them - press "Output" button.
===========================================================================================
Cleaned documentation: Scores much better on values it's seen! if only the world were so easy...
Processed documentation: Scores much better on values it's seen!
===========================================================================================
Cleaned documentation: Plots are hidden. If you'd like to look at them - press "Output" button.
Processed documentation: If you'd like to look at them - press "Output" button.
===========================================================================================
Cleaned documentation: Here, x should be one row of a DataFrame. This method is meant to be
Processed documentation: Here, x should be one row of a DataFrame.
===========================================================================================
Cleaned documentation: Subset rows where the player is the rusher. This should create a complete set of unique PlayIds
Processed documentation: This should create a complete set of unique PlayIds
===========================================================================================
Cleaned documentation: Accuracy has some interesting structure, but is relatively consistent across train/test. Check time distributions
Processed documentation: Accuracy has some interesting structure, but is relatively consistent across train/test.
===========================================================================================
Cleaned documentation: OK, so most places appear around 100 times. Let's see if accuracy changes with "time" at all
Processed documentation: Let's see if accuracy changes with "time" at all
===========================================================================================
Cleaned documentation: not really. OK, let's revisit this x-axis stretching business by visualising these top 20 locations on a map.
Processed documentation: OK, let's revisit this x-axis stretching business by visualising these top 20 locations on a map.
===========================================================================================
Cleaned documentation: This highlights the variance, looking suspiciously like streets. We can look at the standard deviations to illustrate this
Processed documentation: This highlights the variance, looking suspiciously like streets.
===========================================================================================
Cleaned documentation: Crop using `mask` as input. others` are optional arguments that will be croped using `mask` as reference.
Processed documentation: others` are optional arguments that will be croped using `mask` as reference.
===========================================================================================
Cleaned documentation: We will use affine transform to crop image. It will deal with padding image if necessary
Processed documentation: We will use affine transform to crop image.
===========================================================================================
Cleaned documentation: So I applied uniform noise to a target image in order to visualize its effects. Here are the results
Processed documentation: So I applied uniform noise to a target image in order to visualize its effects.
===========================================================================================
Cleaned documentation: The event_code == 4020 provides helpful information for assessment session type. Let's explore it.
Processed documentation: The event_code == 4020 provides helpful information for assessment session type.
===========================================================================================
Cleaned documentation: Here, we can count the true and false attemps for the above event. Let's see how to find it.
Processed documentation: Here, we can count the true and false attemps for the above event.
===========================================================================================
Cleaned documentation: There is a interesing feature found for event_code==4025 for Cauldron Filler (Assessment). Let's discover it.
Processed documentation: There is a interesing feature found for event_code==4025 for Cauldron Filler (Assessment).
===========================================================================================
Cleaned documentation: If you found these fetures are helpful to you, please do upvote. for reading
Processed documentation: If you found these fetures are helpful to you, please do upvote.
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: There are two columns exist in train dataset but not in test dataset. Let's find these two features.
Processed documentation: There are two columns exist in train dataset but not in test dataset.
===========================================================================================
Cleaned documentation: You can change n_splits to any number you like. fold split is the most common
Processed documentation: You can change n_splits to any number you like.
===========================================================================================
Cleaned documentation: It looks pretty OK to me. What do you think? Let's save it then
Processed documentation: It looks pretty OK to me.
===========================================================================================
Cleaned documentation: Getting 65% accuracy with only LinearSVC. Try different ensemble models to get more accurate model.
Processed documentation: Try different ensemble models to get more accurate model.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Post processing of each predicted mask, components with lesser number of pixels. than `min_size` are ignored.
Processed documentation: Post processing of each predicted mask, components with lesser number of pixels.
===========================================================================================
Cleaned documentation: Our market data has a total of 4.072.956 rows and 16 columns. We now look at datatypes.
Processed documentation: Our market data has a total of 4.072.956 rows and 16 columns.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Post processing of each predicted mask, components with lesser number of pixels. than `min_size` are ignored.
Processed documentation: Post processing of each predicted mask, components with lesser number of pixels.
===========================================================================================
Cleaned documentation: model = Prophet(holidays = holidays, uncertainty_samples = False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.
Processed documentation: False, n_changepoints = 50, changepoint_range = 0.8, changepoint_prior_scale = 0.
===========================================================================================
Cleaned documentation: The structure of the annotations are classic XML with the bbox at "annotation/object/bndbox". Structure of XML is as follows
Processed documentation: The structure of the annotations are classic XML with the bbox at "annotation/object/bndbox".
===========================================================================================
Cleaned documentation: Initializes from a list/iterable of indexes or. a filename of tour in csv/tsplib/linkern format.
Processed documentation: a filename of tour in csv/tsplib/linkern format.
===========================================================================================
Cleaned documentation: Convert predictions to labels such that the labels have. the same distribution as in the y_train array.
Processed documentation: Convert predictions to labels such that the labels have.
===========================================================================================
Cleaned documentation: Process text for other models (LGB, FFM. Additionally replaces numbers by and stems the text.
Processed documentation: Process text for other models (LGB, FFM.
===========================================================================================
Cleaned documentation: Build logarithmically scaled numerical features. Note the +2 because NA values are filled with -1.
Processed documentation: Note the +2 because NA values are filled with -1.
===========================================================================================
Cleaned documentation: Another step that many others have already done. Again, the same progress as in the kernel from above.
Processed documentation: Again, the same progress as in the kernel from above.
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: First, define 5-Fold cross-validation. The `random_state` here is important to make sure this is deterministic too.
Processed documentation: First, define 5-Fold cross-validation.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: can use np. unique only on images of one size, flatten first, then select
Processed documentation: unique only on images of one size, flatten first, then select
===========================================================================================
Cleaned documentation: Now let's calculate $G^1_C$ for the carbon atom. First let's identify which one is the carbon atom
Processed documentation: Now let's calculate $G^1_C$ for the carbon atom.
===========================================================================================
Cleaned documentation: We need to free a bit of memory for Kaggle servers. Kudos to artgor for this function.
Processed documentation: We need to free a bit of memory for Kaggle servers.
===========================================================================================
Cleaned documentation: Now, "1" is assigned the day before an event exist. Other days will remain as "0".
Processed documentation: Now, "1" is assigned the day before an event exist.
===========================================================================================
Cleaned documentation: is assigned to the days before the event_name. Since "event_name_2" is rare, it was not added.
Processed documentation: is assigned to the days before the event_name.
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: unseen data. More the number of splits/folds, less the test will be impacted by randomness
Processed documentation: More the number of splits/folds, less the test will be impacted by randomness
===========================================================================================
Cleaned documentation: fits the model using .loc at the full dataset to select the splits indexes and features used
Processed documentation: at the full dataset to select the splits indexes and features used
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: unseen data. More the number of splits/folds, less the test will be impacted by randomness
Processed documentation: More the number of splits/folds, less the test will be impacted by randomness
===========================================================================================
Cleaned documentation: fits the model using .loc at the full dataset to select the splits indexes and features used
Processed documentation: at the full dataset to select the splits indexes and features used
===========================================================================================
Cleaned documentation: the second index is the x-direction. Also, the y-direction needs to be reversed for
Processed documentation: the second index is the x-direction.
===========================================================================================
Cleaned documentation: the second index is the x-direction. Also, the y-direction needs to be reversed for
Processed documentation: the second index is the x-direction.
===========================================================================================
Cleaned documentation: direction of a trip, from 180 to -180 degrees. Horizontal axes = 0 degrees.
Processed documentation: direction of a trip, from 180 to -180 degrees.
===========================================================================================
Cleaned documentation: Note: some gift weight distributions are (almost) normal, others not at all. Let's look at bigger bags now
Processed documentation: Note: some gift weight distributions are (almost) normal, others not at all.
===========================================================================================
Cleaned documentation: Helper function, constructing basic positional encoding. The code is partially based on implementation from Tensor2Tensor library
Processed documentation: Helper function, constructing basic positional encoding.
===========================================================================================
Cleaned documentation: This kernel is for beginners only. Please upvote this kernel which motivates me to do more.
Processed documentation: Please upvote this kernel which motivates me to do more.
===========================================================================================
Cleaned documentation: same applies to test set as said in train set. Now we'll see in visual way.
Processed documentation: same applies to test set as said in train set.
===========================================================================================
Cleaned documentation: More to come ... Please encourage me to do more by upvoting Thank you
Processed documentation: Please encourage me to do more by upvoting Thank you
===========================================================================================
Cleaned documentation: I'll update more in coming future so watch this space. Please upvote to encourage me. Thank you all
Processed documentation: I'll update more in coming future so watch this space.
===========================================================================================
Cleaned documentation: taking this step from Michael Lopez's R kernel. I don't understand why it's necessary, but let's be safe
Processed documentation: taking this step from Michael Lopez's R kernel.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: Note: pd.read_json() is tricky as you have different data in the json. Its easier to convert to dic
Processed documentation: Note: pd.read_json() is tricky as you have different data in the json.
===========================================================================================
Cleaned documentation: Question1. What is the good impact of Price between item condition 1 and condition (4,
Processed documentation: What is the good impact of Price between item condition 1 and condition (4,
===========================================================================================
Cleaned documentation: Convert wind direction into categorical feature. We can split 360 degrees into 16-wind compass rose.
Processed documentation: Convert wind direction into categorical feature.
===========================================================================================
Cleaned documentation: Nullity Matrix The msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion.
Processed documentation: The msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion.
===========================================================================================
Cleaned documentation: The max values of electricity are caused by only 7 buildings. Practically, two of them
Processed documentation: The max values of electricity are caused by only 7 buildings.
===========================================================================================
Cleaned documentation: Below we initiate the ImageDataGenerator. The params I use can be tweaked to your desire.
Processed documentation: The params I use can be tweaked to your desire.
===========================================================================================
Cleaned documentation: Image: An example of an ROC curve. AOC is a typo and should be AUC.
Processed documentation: Image: An example of an ROC curve.
===========================================================================================
Cleaned documentation: Changes Scikit learn's random forests to give each tree a random sample of. n random rows.
Processed documentation: Changes Scikit learn's random forests to give each tree a random sample of.
===========================================================================================
Cleaned documentation: Let's plot the total kills for every player first. It doesn't look like there are too many outliers.
Processed documentation: Let's plot the total kills for every player first.
===========================================================================================
Cleaned documentation: For keeping time. GPU limit for this competition is set to ± 9 hours.
Processed documentation: GPU limit for this competition is set to ± 9 hours.
===========================================================================================
Cleaned documentation: A custom Keras callback for saving the best model. according to the Quadratic Weighted Kappa (QWK) metric.
Processed documentation: A custom Keras callback for saving the best model.
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: Check kernels run-time. GPU limit for this competition is set to ± 9 hours.
Processed documentation: GPU limit for this competition is set to ± 9 hours.
===========================================================================================
Cleaned documentation: Let's normalize from 0 to 255 to 0 to. It's a good practice when dealing with
Processed documentation: It's a good practice when dealing with
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: A custom Keras callback for saving the best model. according to the Quadratic Weighted Kappa (QWK) metric.
Processed documentation: A custom Keras callback for saving the best model.
===========================================================================================
Cleaned documentation: There are some pre-processing to be done. Let's tackle them one step at a time. crop_bounding_box.py
Processed documentation: Let's tackle them one step at a time.
===========================================================================================
Cleaned documentation: Now we have prepared: x_train, y_train, x_val, y_val and x_test. Time to build our CNN model. First import keras
Processed documentation: Now we have prepared: x_train, y_train, x_val, y_val and x_test.
===========================================================================================
Cleaned documentation: Again, LB is not fooled. No variable can predict target better than 0.54 LB.
Processed documentation: No variable can predict target better than 0.54 LB.
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: if numpy_labels.dtype == object: binary string in this case, these are image ID strings
Processed documentation: object: binary string in this case, these are image ID strings
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: and pass the dictionary. If you don't use one of 4 above, then you can
Processed documentation: and pass the dictionary.
===========================================================================================
Cleaned documentation: Below are functions to manipulate `rle` masks. Click "code" to the right to see the code.
Processed documentation: Click "code" to the right to see the code.
===========================================================================================
Cleaned documentation: ord_1 and ord_2 can be encoded like the following. They include ordering such as Warm>Cold>Freezing etc.
Processed documentation: They include ordering such as Warm>Cold>Freezing etc.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Linear regression model guessed almost the same as the baseline model. Let's see how light gbm will perform.
Processed documentation: Linear regression model guessed almost the same as the baseline model.
===========================================================================================
Cleaned documentation: No significant trend is observed in above pair plot. Next, we will convert categorical variables to continuous ones.
Processed documentation: No significant trend is observed in above pair plot.
===========================================================================================
Cleaned documentation: So, we have got the list of top 30 important features. Let's visualize it.
Processed documentation: So, we have got the list of top 30 important features.
===========================================================================================
Cleaned documentation: About 500 ids are present the entire time series. Some ids have hardly any data (minimum of 2 timestamps!).
Processed documentation: About 500 ids are present the entire time series.
===========================================================================================
Cleaned documentation: Get all of the polygon overlays for a tile. Returns a dict: {LABEL: POLYGON
Processed documentation: Get all of the polygon overlays for a tile.
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: Split the data into K folds for cross validation. Use the fixed random seed for reproducibility.
Processed documentation: Split the data into K folds for cross validation.
===========================================================================================
Cleaned documentation: Santa : "Seems like family of 4 is the norm nowadays. Luckily, there are only a few huge families
Processed documentation: Santa : "Seems like family of 4 is the norm nowadays.
===========================================================================================
Cleaned documentation: Input : Previous Day Occupancy, Current Day Occupancy. Output : Account Cost of the Day.
Processed documentation: Input : Previous Day Occupancy, Current Day Occupancy.
===========================================================================================
Cleaned documentation: padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.
Processed documentation: padding done to equalize the lengths of all input reviews.
===========================================================================================
Cleaned documentation: Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.
Processed documentation: Therefore reviews lesser than max length will be made equal using extra zeros at end.
===========================================================================================
Cleaned documentation: This is done for learning purpose only. One can play around with different hyper parameters combinations
Processed documentation: One can play around with different hyper parameters combinations
===========================================================================================
Cleaned documentation: and try increase the accuracy even more. For example, a different learning rate, an extra dense layer
Processed documentation: For example, a different learning rate, an extra dense layer
===========================================================================================
Cleaned documentation: before output layer, etc. Cross validation could be used to evaluate the model and grid search
Processed documentation: Cross validation could be used to evaluate the model and grid search
===========================================================================================
Cleaned documentation: further to find unique combination of parameters that give maximum accuracy. This model has a validation
Processed documentation: further to find unique combination of parameters that give maximum accuracy.
===========================================================================================
Cleaned documentation: Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.
Processed documentation: Visualize learning curve.
===========================================================================================
Cleaned documentation: fig = go.Figure() fig = fig.add_trace(go.Scatter(y=total_test['Cumulative total tests'], x=total_test['Date'], mode='lines+markers', name='sk')) fig = fig.add_trace(go.Scatter(y=testingHistory['testing_pos'], x=testingHistory['time_stamp'], mode='lines+markers', name='in
Processed documentation: x=total_test['Date'], mode='lines+markers', name='sk')) fig = fig.add_trace(go.
===========================================================================================
Cleaned documentation: tf_logs" is my Tensorboard folder. Change this to your setup if you want to use TB
Processed documentation: Change this to your setup if you want to use TB
===========================================================================================
Cleaned documentation: First we printout all layers. Than we freeze all layers up to the last conv block and compile again.
Processed documentation: Than we freeze all layers up to the last conv block and compile again.
===========================================================================================
Cleaned documentation: tf_logs" is my Tensorboard folder. Change this to your setup if you want to use TB
Processed documentation: Change this to your setup if you want to use TB
===========================================================================================
Cleaned documentation: Timewise the measurements are quite evenly distributed. Only February and March stick out with a little less measurements.
Processed documentation: Only February and March stick out with a little less measurements.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: gmm_pred = np.zeros((len(data), 5)) for j in range(5): gmm = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data) gmm_pred[:, j] += gmm.predict(data
Processed documentation: = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data)
===========================================================================================
Cleaned documentation: gmm_pred = np.zeros((len(data), 5)) for j in range(5): gmm = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data) gmm_pred[:, j] += gmm.predict(data
Processed documentation: = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data)
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: killStreaks : Max number of enemy players killed in a short amount of time.
Processed documentation: Max number of enemy players killed in a short amount of time.
===========================================================================================
Cleaned documentation: so it is good. i use kills_assists and drop kills because of high corr
Processed documentation: i use kills_assists and drop kills because of high corr
===========================================================================================
Cleaned documentation: As shown above,, `''` is recognized as object. So we have to change these values as missing.
Processed documentation: As shown above,, `''` is recognized as object.
===========================================================================================
Cleaned documentation: Does NaN means no numbers, can '' be replaced with nan? I do not know this part.
Processed documentation: Does NaN means no numbers, can '' be replaced with nan?
===========================================================================================
Cleaned documentation: Does NaN means no numbers, can '' be replaced with nan? I do not know this part.
Processed documentation: Does NaN means no numbers, can '' be replaced with nan?
===========================================================================================
Cleaned documentation: Applies masks to the orignal image and. returns the a preprocessed image with. channels.
Processed documentation: Applies masks to the orignal image and.
===========================================================================================
Cleaned documentation: Constructs a EfficientNet model for FastAI. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet.
Processed documentation: Args: pretrained (bool): If True, returns a model pre-trained on ImageNet.
===========================================================================================
Cleaned documentation: Check kernels run-time. GPU limit for this competition is set to ± 9 hours.
Processed documentation: GPU limit for this competition is set to ± 9 hours.
===========================================================================================
Cleaned documentation: need to reset index to take care of columns. comment next line out to see what i mean
Processed documentation: need to reset index to take care of columns.
===========================================================================================
Cleaned documentation: Let's do the Data-Prep and all related legwork ourselfs, w/o sklearn. How to use sklearn, you can read here
Processed documentation: Let's do the Data-Prep and all related legwork ourselfs, w/o sklearn.
===========================================================================================
Cleaned documentation: Returns a lr_scheduler callback which is used for training. Feel free to change the values below
Processed documentation: Returns a lr_scheduler callback which is used for training.
===========================================================================================
Cleaned documentation: In model.train() mode, model(images) is returning losses. We are using model.eval() mode --> it will return boxes and scores.
Processed documentation: We are using model.eval() mode --> it will return boxes and scores.
===========================================================================================
Cleaned documentation: Returns a lr_scheduler callback which is used for training. Feel free to change the values below
Processed documentation: Returns a lr_scheduler callback which is used for training.
===========================================================================================
Cleaned documentation: limit y-values for better zoom-scale. Remember that roughly -4.5 is the best possible score
Processed documentation: limit y-values for better zoom-scale.
===========================================================================================
Cleaned documentation: There are 4787 unique characters. But how many of those are in the training set
Processed documentation: There are 4787 unique characters.
===========================================================================================
Cleaned documentation: There is some seasonality in the data, for example, weekly and 4-weekly. Lets try and take advantage of it.
Processed documentation: There is some seasonality in the data, for example, weekly and 4-weekly.
===========================================================================================
Cleaned documentation: param sentences: list of list of words. return: dictionary of words and their count.
Processed documentation: param sentences: list of list of words.
===========================================================================================
Cleaned documentation: Next we import the embeddings we want to use in our model later. For illustration I use GoogleNews here.
Processed documentation: Next we import the embeddings we want to use in our model later.
===========================================================================================
Cleaned documentation: Hmm seems like numbers also are a problem. Lets check the top 10 embeddings to get a clue.
Processed documentation: Lets check the top 10 embeddings to get a clue.
===========================================================================================
Cleaned documentation: Looks good. No obvious oov words there we could quickly fix. Thank you for reading and happy kaggling
Processed documentation: No obvious oov words there we could quickly fix.
===========================================================================================
Cleaned documentation: There are some check point. The train and test row are similar. The column size so many.
Processed documentation: The train and test row are similar.
===========================================================================================
Cleaned documentation: Ok, now we have our input data and label. So let's patch up a model. Feel free to improve
Processed documentation: Ok, now we have our input data and label.
===========================================================================================
Cleaned documentation: idを関連付けてconfidenceを出すのが課題。 The challenge is to associate the id of the training image with this test image to give confidence.
Processed documentation: The challenge is to associate the id of the training image with this test image to give confidence.
===========================================================================================
Cleaned documentation: Interesting, actually optimal parameters differ across models. Let's use optimal parameters for each model and submit.
Processed documentation: Let's use optimal parameters for each model and submit.
===========================================================================================
Cleaned documentation: Now we look at each file one by one:) I try to visualize the content of each file.
Processed documentation: I try to visualize the content of each file.
===========================================================================================
Cleaned documentation: Let's look at the charge distribution. Note that I sampled data to reduce the computational burden.
Processed documentation: Note that I sampled data to reduce the computational burden.
===========================================================================================
Cleaned documentation: There are relatively a lot of object. We may need to convert them to numerical values somehow. nans
Processed documentation: We may need to convert them to numerical values somehow.
===========================================================================================
Cleaned documentation: There are relatively many nans for some columns such as "WindSpeed" and "WindDirection". number of uniques in each column
Processed documentation: There are relatively many nans for some columns such as "WindSpeed" and "WindDirection".
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: initial position or vertex (in millimeters) in global coordinates. 'X' and 'Y' axis with particle charge(positive or negative
Processed documentation: initial position or vertex (in millimeters) in global coordinates.
===========================================================================================
Cleaned documentation: sample_submission` shows the submission format. It seems we need to predict `accuracy_group` for each `installation_id`.
Processed documentation: sample_submission` shows the submission format.
===========================================================================================
Cleaned documentation: Again, let's focus on specific `installation_id, 0006a69f` for the data analysis. This user solved Assessment 5 times.
Processed documentation: Again, let's focus on specific `installation_id, 0006a69f` for the data analysis.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: To install without internet connection, we can install library as "dataset". It is already uploaded here
Processed documentation: To install without internet connection, we can install library as "dataset".
===========================================================================================
Cleaned documentation: potential_energy.csv stores the potential energy for each molecule. It is scalar value, only one value is stored.
Processed documentation: It is scalar value, only one value is stored.
===========================================================================================
Cleaned documentation: To install without internet connection, we can install library as "dataset". It is uploaded by @rishabhiitbhu
Processed documentation: To install without internet connection, we can install library as "dataset".
===========================================================================================
Cleaned documentation: Put everything together with `Transform` class. Update] I added albumentations augmentations introduced in [Bengali: albumentations data augmentation tutorial
Processed documentation: Update] I added albumentations augmentations introduced in [Bengali: albumentations data augmentation tutorial
===========================================================================================
Cleaned documentation: The dataset consists of 5 categories: "Technology", "Stackoverflow", "Culture", "Science", "Life arts". Train/Test distribution is almost same.
Processed documentation: The dataset consists of 5 categories: "Technology", "Stackoverflow", "Culture", "Science", "Life arts".
===========================================================================================
Cleaned documentation: Seems several users are in both train & test dataset. Also, it seems many users ask question and answer.
Processed documentation: Seems several users are in both train & test dataset.
===========================================================================================
Cleaned documentation: This is basic preprocessing. This time, symbols and words are attached, so they are separated here.
Processed documentation: This time, symbols and words are attached, so they are separated here.
===========================================================================================
Cleaned documentation: Here we use a trained word2vec model that is easily available with nltk. We used SWEM with max pooling.
Processed documentation: Here we use a trained word2vec model that is easily available with nltk.
===========================================================================================
Cleaned documentation: We're almost there. We compute a rank in percentagfe for each hit along a given particle track.
Processed documentation: We compute a rank in percentagfe for each hit along a given particle track.
===========================================================================================
Cleaned documentation: The speedup looks better if we factor the sorting time out. Let's measure it.
Processed documentation: The speedup looks better if we factor the sorting time out.
===========================================================================================
Cleaned documentation: One way to display numeric data and spot trend is to use a moving average. Let's try it here.
Processed documentation: One way to display numeric data and spot trend is to use a moving average.
===========================================================================================
Cleaned documentation: Lets validate the test files. This verifies that they all contain 150,000 samples as expected.
Processed documentation: Lets validate the test files.
===========================================================================================
Cleaned documentation: Let's now look at the available gifts. We one hot encode the gift type.
Processed documentation: Let's now look at the available gifts.
===========================================================================================
Cleaned documentation: Probabilities can be identical for several values as pointed out by @Commander. Let's test it.
Processed documentation: Probabilities can be identical for several values as pointed out by @Commander.
===========================================================================================
Cleaned documentation: This data has been normalized as Raddar claimed. Indeed, its mean is close to
Processed documentation: This data has been normalized as Raddar claimed.
===========================================================================================
Cleaned documentation: Is this better than Raddar's kernel? He computed this for new transactions only. Let's see what we get.
Processed documentation: Is this better than Raddar's kernel?
===========================================================================================
Cleaned documentation: The telescope only works during the night. When is the night in term of mjd
Processed documentation: The telescope only works during the night.
===========================================================================================
Cleaned documentation: Now the night starts at around 0.25 and ends around 0.75. We can define the night oindex accordingly.
Processed documentation: Now the night starts at around 0.25 and ends around 0.75.
===========================================================================================
Cleaned documentation: Let's see how plays are rendered here. I use 2 plays for which I have videos.
Processed documentation: Let's see how plays are rendered here.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: There is clearly a shift of 90 degrees from 2017 to 2018. A possible fix is this.
Processed documentation: There is clearly a shift of 90 degrees from 2017 to 2018.
===========================================================================================
Cleaned documentation: We can now standardise orientation as we did for direction. Let's repeat all the code for clarity.
Processed documentation: Let's repeat all the code for clarity.
===========================================================================================
Cleaned documentation: Now the night starts at around 0.25 and ends around 0.75. We can define the night index accordingly.
Processed documentation: Now the night starts at around 0.25 and ends around 0.75.
===========================================================================================
Cleaned documentation: It also takes about 1.5 second. CPMP method. We run it once to compile it.
Processed documentation: It also takes about 1.5 second.
===========================================================================================
Cleaned documentation: Find all the contours in thresh. In your case the 3 and the additional strike
Processed documentation: Find all the contours in thresh.
===========================================================================================
Cleaned documentation: Find all the contours in thresh. In your case the 3 and the additional strike
Processed documentation: Find all the contours in thresh.
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: sentences starting with a dot (as below) is my personal opinion. have fun!`enter code here
Processed documentation: sentences starting with a dot (as below) is my personal opinion.
===========================================================================================
Cleaned documentation: Prophet predict linear trend, because have only one year data. We cannot capture seasoneal trends.
Processed documentation: Prophet predict linear trend, because have only one year data.
===========================================================================================
Cleaned documentation: count값이 다른 feature가 보입니다. Null)가 있는 것으로 보이는데 한번 알아보도록 하겠습니다. Null Data) 확인
Processed documentation: Null)가 있는 것으로 보이는데 한번 알아보도록 하겠습니다.
===========================================================================================
Cleaned documentation: Pclass 가 큰 영향을 미친다고 생각해볼 수 있으며, feature 를 사용하는 것이 좋을 것이라 판단할 수 있습니다. Sex
Processed documentation: Pclass 가 큰 영향을 미친다고 생각해볼 수 있으며, feature 를 사용하는 것이 좋을 것이라
===========================================================================================
Cleaned documentation: pandas 의 crosstab 을 이용하여 우리가 추출한 Initial 과 Sex 간의 count 를 살펴봅시다.
Processed documentation: pandas 의 crosstab 을 이용하여 우리가 추출한
===========================================================================================
Cleaned documentation: feature 간의 상관관계를 한번 보려고 합니다. Pearson correlation 을 구하면 (-1, 1) 사이의 값을 얻을 수 있습니다.
Processed documentation: Pearson correlation 을 구하면 (-1, 1) 사이의 값을 얻을 수 있습니다.
===========================================================================================
Cleaned documentation: Model is working well with certain range, but not with higher values. Balance Dataset, Naive Oversampling
Processed documentation: Model is working well with certain range, but not with higher values.
===========================================================================================
Cleaned documentation: LH has fine-grained features for 4 seconds, and X has more abstract whole sample (for now). Converter
Processed documentation: LH has fine-grained features for 4 seconds, and X has more abstract whole sample (for now).
===========================================================================================
Cleaned documentation: SEResNet was basically used for competition submission. Here AlexNet based model is enabled by default for Kaggle kernel.
Processed documentation: Here AlexNet based model is enabled by default for Kaggle kernel.
===========================================================================================
Cleaned documentation: Datasets and model are ready, now prepare for training. Utilities for Balancing Category Distribution
Processed documentation: Datasets and model are ready, now prepare for training.
===========================================================================================
Cleaned documentation: Get list with all files in the directory. param directory: Path to the directory. return result: List with filenames.
Processed documentation: param directory: Path to the directory.
===========================================================================================
Cleaned documentation: Change between output and input height. output is smaller. output has same height. output is higher.
Processed documentation: Change between output and input height.
===========================================================================================
Cleaned documentation: Change between output and input width. output is smaller. output has same width. output is larger.
Processed documentation: Change between output and input width.
===========================================================================================
Cleaned documentation: Ratio of the colors. param frame: 2d array. return result: dictionary with ratio per color.
Processed documentation: return result: dictionary with ratio per color.
===========================================================================================
Cleaned documentation: Check if the output color is the most common input color. return result: Boolean.
Processed documentation: Check if the output color is the most common input color.
===========================================================================================
Cleaned documentation: Get the color of the output if it is unique. return result: Integer for color.
Processed documentation: Get the color of the output if it is unique.
===========================================================================================
Cleaned documentation: features = [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID
Processed documentation: [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID
===========================================================================================
Cleaned documentation: We have been given sensor data from robots driving on different surfaces. I imagine these robots similar to this
Processed documentation: We have been given sensor data from robots driving on different surfaces.
===========================================================================================
Cleaned documentation: Unclear why fails to open [encoding error], format is same as for glove. Will Debug, Dan
Processed documentation: [encoding error], format is same as for glove.
===========================================================================================
Cleaned documentation: For now remove universe 0 rows. We could use the mfor context later though
Processed documentation: For now remove universe 0 rows.
===========================================================================================
Cleaned documentation: We now define a function that will be useful to us several times. Thanks to Rohit Singh's notebook.
Processed documentation: We now define a function that will be useful to us several times.
===========================================================================================
Cleaned documentation: useful "hidden" function - df._get_numeric_data() - returns only numeric columns from a pandas dataframe. Useful for scikit learn models
Processed documentation: useful "hidden" function - df._get_numeric_data() - returns only numeric columns from a pandas dataframe.
===========================================================================================
Cleaned documentation: We can see some kind of relation between size and target, but is it meaningful? Too soon to say...
Processed documentation: We can see some kind of relation between size and target, but is it meaningful?
===========================================================================================
Cleaned documentation: Handling categorical data. objList = train_df.select_dtypes(include = "object").columns. train_df = one_hot(train_df, objList) test_df = one_hot(test_df, objList) print (train_df.shape
Processed documentation: train_df = one_hot(train_df, objList) test_df =
===========================================================================================
Cleaned documentation: All layers in mask image has same data. That we can visualize from following code line
Processed documentation: All layers in mask image has same data.
===========================================================================================
Cleaned documentation: Our target column is named "HasDetections". it is worth looking how many detections we have in our database.
Processed documentation: Our target column is named "HasDetections".
===========================================================================================
Cleaned documentation: Below a simple histogram of countries identifier. I'll dwell on this a bit more soon.
Processed documentation: Below a simple histogram of countries identifier.
===========================================================================================
Cleaned documentation: DBNO - Down But Not Out. How many enemies DBNOs an average player scores.
Processed documentation: How many enemies DBNOs an average player scores.
===========================================================================================
Cleaned documentation: The oldest movie in our database is black and white [The Kid]( movie with Charlie Chaplin from 1921.
Processed documentation: The oldest movie in our database is black and white
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: main_df snap флагов. Теперь данные имеют более репрезентативный вид. id_dict name_id id_dict, но где ключи и значения переставлены.
Processed documentation: id_dict name_id id_dict, но где ключи и значения переставлены.
===========================================================================================
Cleaned documentation: The datasets can be distinguished quite good. Let's check which features are most important
Processed documentation: Let's check which features are most important
===========================================================================================
Cleaned documentation: Plot violinplots for given words (waves in dirs) and frequency freq_ind. from all frequencies freqs.
Processed documentation: Plot violinplots for given words (waves in dirs) and frequency freq_ind.
===========================================================================================
Cleaned documentation: This clearly shows the outliers are above a value of approx. 137.5. Well we will remove outliers after 150.
Processed documentation: This clearly shows the outliers are above a value of approx.
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased
Processed documentation: Learning rate will be half after 3 epochs if accuracy is not increased
===========================================================================================
Cleaned documentation: Now let's look at their Fourier transforms. Even though the signals are totally different, their FTs are the same.
Processed documentation: Now let's look at their Fourier transforms.
===========================================================================================
Cleaned documentation: Multi-threaded processes to utilize all available CPUs for this task. Note that many threads will block on IO
Processed documentation: Multi-threaded processes to utilize all available CPUs for this task.
===========================================================================================
Cleaned documentation: data = [[lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop] for sent in train.text.values
Processed documentation: [[lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop] for sent in train.text.values
===========================================================================================
Cleaned documentation: Destroying a vehicle in my experience shows that a player has skills. Let's check it.
Processed documentation: Destroying a vehicle in my experience shows that a player has skills.
===========================================================================================
Cleaned documentation: Almost no one swims. Let's group the swimming distances in 4 categories and plot vs winPlacePerc.
Processed documentation: Let's group the swimming distances in 4 categories and plot vs winPlacePerc.
===========================================================================================
Cleaned documentation: Another simple feature is the sum of heals and boosts. Also the sum of total distance travelled.
Processed documentation: Another simple feature is the sum of heals and boosts.
===========================================================================================
Cleaned documentation: Earlier in the kernel we did EDA for Solos, Duos and Squads. Lets create a column for them.
Processed documentation: Earlier in the kernel we did EDA for Solos, Duos and Squads.
===========================================================================================
Cleaned documentation: A little bit modified `preprocess`. Now it returns only words which model's vocabulary contains.
Processed documentation: Now it returns only words which model's vocabulary contains.
===========================================================================================
Cleaned documentation: Hmm, i expected everything to change, because they are question with "?". Let's see the ones that didn't change.
Processed documentation: Hmm, i expected everything to change, because they are question with "?".
===========================================================================================
Cleaned documentation: Some question are written only in lowercase. This happens when they start with a number.
Processed documentation: Some question are written only in lowercase.
===========================================================================================
Cleaned documentation: Let's follow the same steps for the test set. Just read in numeric columns.
Processed documentation: Let's follow the same steps for the test set.
===========================================================================================
Cleaned documentation: df['day_diff1'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[0][0])) have to make this less clunky, write a function
Processed documentation: = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[0][0])) have to make this less clunky, write a function
===========================================================================================
Cleaned documentation: Their performances seem very different, even for people with similar number of entries. Indeed they are..
Processed documentation: Their performances seem very different, even for people with similar number of entries.
===========================================================================================
Cleaned documentation: the y value associated to each point. PCA decomposition is provided as well for comparison
Processed documentation: the y value associated to each point.
===========================================================================================
Cleaned documentation: This takes incredibly long and is therefore commented out. It however looks very similar to the plot above
Processed documentation: This takes incredibly long and is therefore commented out.
===========================================================================================
Cleaned documentation: The first active year show some big differences! The target variable increases with each year. Definitly an interesting pattern.
Processed documentation: The first active year show some big differences!
===========================================================================================
Cleaned documentation: density 1..100 percents. The initial density can be reduced into smaller subset os hits
Processed documentation: The initial density can be reduced into smaller subset os hits
===========================================================================================
Cleaned documentation: The generator endlessly selects "batch_size" ending positions of sub-time series. For each ending position,
Processed documentation: The generator endlessly selects "batch_size" ending positions of sub-time series.
===========================================================================================
Cleaned documentation: Get back to building a CNN using Keras. Much better frameworks then others. You will enjoy for sure.
Processed documentation: Get back to building a CNN using Keras.
===========================================================================================
Cleaned documentation: There are 9 classes into which data has to be classified. Lets get the frequency of each class.
Processed documentation: There are 9 classes into which data has to be classified.
===========================================================================================
Cleaned documentation: We can see there are some entries with text count of 1. Lets have a look at those entries
Processed documentation: We can see there are some entries with text count of 1.
===========================================================================================
Cleaned documentation: People rarely use mediawiki, commons or zh. Quick check for access and agents as well
Processed documentation: People rarely use mediawiki, commons or zh.
===========================================================================================
Cleaned documentation: This code uses only Giba's original features. Note that the more features we have - the better the result.
Processed documentation: This code uses only Giba's original features.
===========================================================================================
Cleaned documentation: This functions smooths data, thanks to Dan Pearson. We will use it to smooth the data for growth factor.
Processed documentation: This functions smooths data, thanks to Dan Pearson.
===========================================================================================
Cleaned documentation: Applying OLS model and below is the R squared result. It shows an R squared score of 0.
Processed documentation: Applying OLS model and below is the R squared result.
===========================================================================================
Cleaned documentation: There are missing values in `sex`, `age_approx`, and `anatom_site_general_challenge` column. Let's check how many missing values we have
Processed documentation: There are missing values in `sex`, `age_approx`, and `anatom_site_general_challenge` column.
===========================================================================================
Cleaned documentation: There are missing values in `anatom_site_general_challenge` column in test dataset. Let's check how many missing values we have
Processed documentation: There are missing values in `anatom_site_general_challenge` column in test dataset.
===========================================================================================
Cleaned documentation: This function will plot different type of histogram with Bokeh. It takes dataframe, column for which we want
Processed documentation: This function will plot different type of histogram with Bokeh.
===========================================================================================
Cleaned documentation: From visualization we can see that we have images with different shapes. So. let's visualize images shape.
Processed documentation: From visualization we can see that we have images with different shapes.
===========================================================================================
Cleaned documentation: Now you can output predictions for each individual model and the ensembled models as well. Averaged models submission
Processed documentation: Now you can output predictions for each individual model and the ensembled models as well.
===========================================================================================
Cleaned documentation: The features seems to be normalized. Process data for model Turn "wheezy-copper-turtle-magic" into a categorical feature
Processed documentation: Process data for model Turn "wheezy-copper-turtle-magic" into a categorical feature
===========================================================================================
Cleaned documentation: Hitting End Of Dataset exceptions is a problem in this setup. Using a repeated validation set instead.
Processed documentation: Hitting End Of Dataset exceptions is a problem in this setup.
===========================================================================================
Cleaned documentation: Distribution of subContinent The sub-continent from which sessions originated, based on IP address of the visitor.
Processed documentation: The sub-continent from which sessions originated, based on IP address of the visitor.
===========================================================================================
Cleaned documentation: Distribution of bounces Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null.
Processed documentation: Distribution of bounces Total bounces (for convenience).
===========================================================================================
Cleaned documentation: Distribution of adContent The ad content of the traffic source. Can be set by the utm_content URL parameter.
Processed documentation: The ad content of the traffic source.
===========================================================================================
Cleaned documentation: Distribution of adwordsClickInfo.slot Position of the Ad. Takes one of the following values:{“RHS", "Top
Processed documentation: Distribution of adwordsClickInfo.slot Position of the Ad.
===========================================================================================
Cleaned documentation: Submissions are evaluated on the log loss. We are going to use it for evaluating our model.
Processed documentation: Submissions are evaluated on the log loss.
===========================================================================================
Cleaned documentation: This concludes the exploratory analysis. I will now proceed on data preprocessing and model building
Processed documentation: I will now proceed on data preprocessing and model building
===========================================================================================
Cleaned documentation: Let's define all things we need. And we will need: Frequency Encoding, Label Encoding, SVD encoding and target encoding.
Processed documentation: And we will need: Frequency Encoding, Label Encoding, SVD encoding and target encoding.
===========================================================================================
Cleaned documentation: Clean bad data. We fit on train dataset and it's ok to remove bad data
Processed documentation: We fit on train dataset and it's ok to remove bad data
===========================================================================================
Cleaned documentation: Fit preprocessing scalers, encoders on given train df. param idx: index with time, assetCode.
Processed documentation: Fit preprocessing scalers, encoders on given train df.
===========================================================================================
Cleaned documentation: News are rare for an asset. We get mean value for 10 days. param df: :return
Processed documentation: News are rare for an asset.
===========================================================================================
Cleaned documentation: One row in news contains many asset codes. Extend it to news_X with one asset code - one row
Processed documentation: One row in news contains many asset codes.
===========================================================================================
Cleaned documentation: Get random asset and it's last market data indices. Repeat for next asset until we reach batch_size.
Processed documentation: Get random asset and it's last market data indices.
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to apply transformation to. Returns: PIL Image: Image with transformation.
Processed documentation: Args: img (PIL Image): Image to apply transformation to.
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to draw hairs on. Returns: PIL Image: Image with drawn hairs.
Processed documentation: Args: img (PIL Image): Image to draw hairs on.
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to draw hairs on. Returns: PIL Image: Image with drawn hairs.
Processed documentation: Args: img (PIL Image): Image to draw hairs on.
===========================================================================================
Cleaned documentation: We have 13GB of RAM available. So to utilise it completely, I will reduce the size of the data.
Processed documentation: So to utilise it completely, I will reduce the size of the data.
===========================================================================================
Cleaned documentation: The first hurdle we get is contaction. Our next step is to clean the contractions in the text
Processed documentation: Our next step is to clean the contractions in the text
===========================================================================================
Cleaned documentation: Model1= build_model(lr = 1e-3, lr_d = 1e-7, units = u, spatial_dr = 0.3, kernel_size1=4, kernel_size2=2, dense_units=32, dr=0.1, conv_size=64, epochs
Processed documentation: build_model(lr = 1e-3, lr_d = 1e-7, units = u, spatial_dr = 0.3, kernel_size1=4, kernel_size2=2, dense_units=32, dr=0.1, conv_size=64, epochs
===========================================================================================
Cleaned documentation: Model1= build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=4, kernel_size2=2, dense_units=d, dr=0.1, conv_size=64, epochs
Processed documentation: build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=4, kernel_size2=2, dense_units=d, dr=0.1, conv_size=64, epochs
===========================================================================================
Cleaned documentation: Model1= build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=4, kernel_size2=2, dense_units=64, dr=0.1, conv_size=c, epochs
Processed documentation: build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=4, kernel_size2=2, dense_units=64, dr=0.1, conv_size=c, epochs
===========================================================================================
Cleaned documentation: Model1= build_model(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = s, kernel_size1=4, kernel_size2=2, dense_units=64, dr=0.1, conv_size=128, epochs
Processed documentation: s, kernel_size1=4, kernel_size2=2, dense_units=64, dr=0.1, conv_size=128, epochs
===========================================================================================
Cleaned documentation: Import modules we would be frequently using. We would import many more along the way as they are needed.
Processed documentation: We would import many more along the way as they are needed.
===========================================================================================
Cleaned documentation: Observations Majority of the patients lie between the Age group of 60 - 75. Has a guassian distribution.
Processed documentation: Observations Majority of the patients lie between the Age group of 60 - 75.
===========================================================================================
Cleaned documentation: Let's calculate for the other groups as well. We use mean to measure how reliable the score is
Processed documentation: We use mean to measure how reliable the score is
===========================================================================================
Cleaned documentation: This function is merely for compatibilty & for providing utility for. fine tuning, cross_val_score and as such.
Processed documentation: This function is merely for compatibilty & for providing utility for.
===========================================================================================
Cleaned documentation: This is tremendously accurate, although lets see if we can do better. Some more analysis and probing
Processed documentation: This is tremendously accurate, although lets see if we can do better.
===========================================================================================
Cleaned documentation: The graph says it all. Greater the Base_FVC, greater the factor. Age, Sex are furthermore used for computing factor.
Processed documentation: Greater the Base_FVC, greater the factor.
===========================================================================================
Cleaned documentation: Let's now create a function to visualize the scans as an animation. Code copied in part from [here
Processed documentation: Let's now create a function to visualize the scans as an animation.
===========================================================================================
Cleaned documentation: We determine the indices of the most important features. After that the training data is loaded
Processed documentation: We determine the indices of the most important features.
===========================================================================================
Cleaned documentation: Some images had more static noise than others. So perhaps adding noise is a useful augmentation to try
Processed documentation: So perhaps adding noise is a useful augmentation to try
===========================================================================================
Cleaned documentation: I mentioned before how the teeth are sometimes very visible. This one was especially so
Processed documentation: I mentioned before how the teeth are sometimes very visible.
===========================================================================================
Cleaned documentation: The images from both sets seem to be quite similar. Possibly some color differences and other minor differences
Processed documentation: Possibly some color differences and other minor differences
===========================================================================================
Cleaned documentation: Define a function to plot a given asset for a given date range. Borrowed initial code from kernel: Thanks
Processed documentation: Define a function to plot a given asset for a given date range.
===========================================================================================
Cleaned documentation: This still uncovers a few more spikes that were not shown by the previous visualizations. To add those
Processed documentation: This still uncovers a few more spikes that were not shown by the previous visualizations.
===========================================================================================
Cleaned documentation: there are varying number of slices under each patient. each numbered file is a 2d-slice of a 3d image
Processed documentation: there are varying number of slices under each patient.
===========================================================================================
Cleaned documentation: Randomly generated unique identifier grouping events within a single game or video play session. There are dublictated codes.
Processed documentation: Randomly generated unique identifier grouping events within a single game or video play session.
===========================================================================================
Cleaned documentation: The feature title is a categorical feature. For the first we use a simple mapping.
Processed documentation: The feature title is a categorical feature.
===========================================================================================
Cleaned documentation: The feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping.
Processed documentation: The feature primary_use is a categorical feature with 16 categories.
===========================================================================================
Cleaned documentation: The feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping.
Processed documentation: The feature primary_use is a categorical feature with 16 categories.
===========================================================================================
Cleaned documentation: param df: Dataframe containing a "timestamp" field which will be broken down in hour, year, day,...
Processed documentation: Dataframe containing a "timestamp" field which will be broken down in hour, year, day,...
===========================================================================================
Cleaned documentation: param df: Dataframe with unnecessary cols. returns: df -> dataframe containing only the desired columns.
Processed documentation: returns: df -> dataframe containing only the desired columns.
===========================================================================================
Cleaned documentation: param inputs: input tensor. param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
Processed documentation: param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
===========================================================================================
Cleaned documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset. Overrides values in the base Config class.
Processed documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset.
===========================================================================================
Cleaned documentation: Train on 1 GPU and 8 images per GPU. We can put multiple images on each
Processed documentation: Train on 1 GPU and 8 images per GPU.
===========================================================================================
Cleaned documentation: Now it's time to train the model. Note that training even a basic model can take a few hours.
Processed documentation: Note that training even a basic model can take a few hours.
===========================================================================================
Cleaned documentation: How does the predicted box compared to the expected value? Let's use the validation dataset to check.
Processed documentation: How does the predicted box compared to the expected value?
===========================================================================================
Cleaned documentation: For majority of categorical variables, the results are pretty close. They differ most for high-cardinality like breed1 and breed2.
Processed documentation: For majority of categorical variables, the results are pretty close.
===========================================================================================
Cleaned documentation: device ids have duplicate entries in phone dataframe. Are duplicate rows identical or different
Processed documentation: device ids have duplicate entries in phone dataframe.
===========================================================================================
Cleaned documentation: Bar widths represent group counts, ratio of genders in each group is shown. The line represents overall genders ratio.
Processed documentation: Bar widths represent group counts, ratio of genders in each group is shown.
===========================================================================================
Cleaned documentation: Way too many device models here... Age distributions by phone brand Color = gender
Processed documentation: Age distributions by phone brand Color = gender
===========================================================================================
Cleaned documentation: This looks about right Going international Let's check international trips to and from New York.
Processed documentation: Going international Let's check international trips to and from New York.
===========================================================================================
Cleaned documentation: Calculate Relative Strength Index(RSI) for given data. param df: pandas.Series. param n: :return: pandas.DataFrame.
Processed documentation: Calculate Relative Strength Index(RSI) for given data.
===========================================================================================
Cleaned documentation: id_31` contains browser in use during a card-not-present transaction (CNP). What are the most used browsers
Processed documentation: id_31` contains browser in use during a card-not-present transaction (CNP).
===========================================================================================
Cleaned documentation: Retrieve Patient Scan Number : eg. What visit they are getting the scan on
Processed documentation: Retrieve Patient Scan Number : eg.
===========================================================================================
Cleaned documentation: we pass the values through the algorithm to get an index of the rel. maximums
Processed documentation: we pass the values through the algorithm to get an index of the rel.
===========================================================================================
Cleaned documentation: we use the index returned to create a dataframe of the values for those index. in this case
Processed documentation: we use the index returned to create a dataframe of the values for those index.
===========================================================================================
Cleaned documentation: We have few variables with missing values that we need to deal with. Let's start with the `CompetitionDistance`.
Processed documentation: We have few variables with missing values that we need to deal with.
===========================================================================================
Cleaned documentation: Continuing further with missing data. What about `Promo2SinceWeek`? May it be that we observe unsusual data points
Processed documentation: May it be that we observe unsusual data points
===========================================================================================
Cleaned documentation: Step 4: Among vertices, that still have odd rank, for each vertex find 5 closest vertices.
Processed documentation: Among vertices, that still have odd rank, for each vertex find 5 closest vertices.
===========================================================================================
Cleaned documentation: Step 7: Join found edges with edges found previously to produce Eulerian graph. Find Eulerian Circuit in that graph
Processed documentation: Join found edges with edges found previously to produce Eulerian graph.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: This error is created by the use of ancient greek words. Possible to improve the model
Processed documentation: This error is created by the use of ancient greek words.
===========================================================================================
Cleaned documentation: They do not require a class instance creation. So, they are not dependent on the state of the object.
Processed documentation: They do not require a class instance creation.
===========================================================================================
Cleaned documentation: Base Transformer model that uses Pytorch Lightning as a PyTorch wrapper. T5 specific methods are implemented in T5Trainer.
Processed documentation: Base Transformer model that uses Pytorch Lightning as a PyTorch wrapper.
===========================================================================================
Cleaned documentation: Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients. Reference for optimizer_step
Processed documentation: Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients.
===========================================================================================
Cleaned documentation: NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py
Processed documentation: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py
===========================================================================================
Cleaned documentation: Here is the distribution of the number of genres per movie. There are 3 films with 7 genres
Processed documentation: Here is the distribution of the number of genres per movie.
===========================================================================================
Cleaned documentation: For the test folder, we need to create a dummy subfolder. In this case, we created
Processed documentation: For the test folder, we need to create a dummy subfolder.
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: Plot default feature importance. param drop_null_importance: drop columns with null feature importance. param top_n: show top n columns. return
Processed documentation: param drop_null_importance: drop columns with null feature importance.
===========================================================================================
Cleaned documentation: Categorical transformer. This is a wrapper for categorical encoders. param cat_cols: :param drop_original: :param encoder
Processed documentation: param cat_cols: :param drop_original: :param encoder
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: The spread is very fast. It started in China and spread to the complete world
Processed documentation: It started in China and spread to the complete world
===========================================================================================
Cleaned documentation: Well , we have to appreaciate India in maintaining constant cases.US, Italy, China, Spain, Gemany has to take take
Processed documentation: Well , we have to appreaciate India in maintaining constant cases.
===========================================================================================
Cleaned documentation: Well , we have to appreaciate India in maintaining constant cases.US, Italy, China, Spain, Gemany has to take take
Processed documentation: Well , we have to appreaciate India in maintaining constant cases.
===========================================================================================
Cleaned documentation: It is ~~almost certain~~ that all data have the same slope => 0.3. Let's remove it.
Processed documentation: It is ~~almost certain~~ that all data have the same slope => 0.3.
===========================================================================================
Cleaned documentation: Given an iterable, it'll return size n chunks per iteration. Handles the last chunk too.
Processed documentation: Given an iterable, it'll return size n chunks per iteration.
===========================================================================================
Cleaned documentation: Takes an iterator/generator and makes it thread-safe by. serializing call to the `next` method of given iterator/generator.
Processed documentation: Takes an iterator/generator and makes it thread-safe by.
===========================================================================================
Cleaned documentation: These variables contain integers. Below, I represent the number of categories in each of these variables
Processed documentation: Below, I represent the number of categories in each of these variables
===========================================================================================
Cleaned documentation: Utility functions We first load some utility functions. The first, written by [Guillaume Martin]( allows to manage memory
Processed documentation: Utility functions We first load some utility functions.
===========================================================================================
Cleaned documentation: For valid data, keep only those with universe. This will help calculate the metric
Processed documentation: For valid data, keep only those with universe.
===========================================================================================
Cleaned documentation: Let's plot several images with their predictions. Note that the model can be wrong even when very confident.
Processed documentation: Let's plot several images with their predictions.
===========================================================================================
Cleaned documentation: There are NaN values in Top 5 record. To learn the number of NaN values
Processed documentation: There are NaN values in Top 5 record.
===========================================================================================
Cleaned documentation: As we can see, datetime's type is object. We should convert it to datetime.
Processed documentation: As we can see, datetime's type is object.
===========================================================================================
Cleaned documentation: Yes! we converted it. Now, we will separate the datetime column as year,month,day,hour and week
Processed documentation: Now, we will separate the datetime column as year,month,day,hour and week
===========================================================================================
Cleaned documentation: LightGBM expects next three parameters need to be integer. So we make them integer
Processed documentation: LightGBM expects next three parameters need to be integer.
===========================================================================================
Cleaned documentation: The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score
Processed documentation: Let's see parameters is responsible for this score
===========================================================================================
Cleaned documentation: So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like
Processed documentation: So we got 0.90 AUC in 5 fold cross validation.
===========================================================================================
Cleaned documentation: Do not forget to upvote :) Also fork and modify for your own use.
Processed documentation: Also fork and modify for your own use.
===========================================================================================
Cleaned documentation: CRYSTALCAVES" and "TREETOPCITY" ratio has varied on test and train dataset. But they are still close to each other.
Processed documentation: CRYSTALCAVES" and "TREETOPCITY" ratio has varied on test and train dataset.
===========================================================================================
Cleaned documentation: Gets the training data and extracts the target. Returns: train (pd.DataFrame): training data. target (np.ndarray): target values, binary.
Processed documentation: Gets the training data and extracts the target.
===========================================================================================
Cleaned documentation: We also introduce a few more features to have more information in the input dataset. These are showed below.
Processed documentation: We also introduce a few more features to have more information in the input dataset.
===========================================================================================
Cleaned documentation: We plot some histogram to visualize the distribution of the predictions and make some comparisons. We expect
Processed documentation: We plot some histogram to visualize the distribution of the predictions and make some comparisons.
===========================================================================================
Cleaned documentation: Data about atoms that we use in our calculations. The variable radius depends on the bonds,
Processed documentation: Data about atoms that we use in our calculations.
===========================================================================================
Cleaned documentation: save the generated NN features for the LGB model. They are the 5 predictions + a 32 neurons layer
Processed documentation: save the generated NN features for the LGB model.
===========================================================================================
Cleaned documentation: Not all features are like that though. Take a look at the feature 117 and 120.
Processed documentation: Take a look at the feature 117 and 120.
===========================================================================================
Cleaned documentation: This is how I would classify the columns. You might desagree. Feel free to change.
Processed documentation: Feel free to change.
===========================================================================================
Cleaned documentation: Not used for this model, but included in case somebody needs it. For math, see (Page
Processed documentation: Not used for this model, but included in case somebody needs it.
===========================================================================================
Cleaned documentation: Earlier we extracted paths of all directories. So, we will access these images from there.
Processed documentation: Earlier we extracted paths of all directories.
===========================================================================================
Cleaned documentation: It's a tad difficult to analyze like this! Therefore we'll plot them in one.
Processed documentation: It's a tad difficult to analyze like this!
===========================================================================================
Cleaned documentation: There are 49 columns and 11 columns have null values. Total null values of these are below
Processed documentation: There are 49 columns and 11 columns have null values.
===========================================================================================
Cleaned documentation: There are a lot of version of stadium types. It must be cleaned up.
Processed documentation: There are a lot of version of stadium types.
===========================================================================================
Cleaned documentation: OLs are the minority group among the defensive players with 15 total. So they aren't visible on the chart.
Processed documentation: OLs are the minority group among the defensive players with 15 total.
===========================================================================================
Cleaned documentation: Submission We trained the model for 10 folds using default parameters. We will make a rank-averaged submission.
Processed documentation: Submission We trained the model for 10 folds using default parameters.
===========================================================================================
Cleaned documentation: We observe our training_files object stores all tfrecord files. Let's pick one to analyze.
Processed documentation: We observe our training_files object stores all tfrecord files.
===========================================================================================
Cleaned documentation: Find training and testing molecules, and split structrues into test and train. Then group by molecule
Processed documentation: Find training and testing molecules, and split structrues into test and train.
===========================================================================================
Cleaned documentation: Let's save the acoustic data in compressed form. This is only done once. Yep, we're below 400MB
Processed documentation: Let's save the acoustic data in compressed form.
===========================================================================================
Cleaned documentation: If it's indeed a Markov process, this should be identical to the actual probability distribution of open_channel states. Check
Processed documentation: If it's indeed a Markov process, this should be identical to the actual probability distribution of open_channel states.
===========================================================================================
Cleaned documentation: examine links between samples. left/right run edges are those samples which do not have a link on that side.
Processed documentation: left/right run edges are those samples which do not have a link on that side.
===========================================================================================
Cleaned documentation: Well, that certainly looks promising. Found 76 runs, similar number than the number of groups. Build the runs
Processed documentation: Found 76 runs, similar number than the number of groups.
===========================================================================================
Cleaned documentation: Have we found all samples? Have we used any sample twice? The answer is yes, and no. Perfect.
Processed documentation: Have we found all samples?
===========================================================================================
Cleaned documentation: Interesting. Some runs contain 2, 3 and even 4 groups. So several groups were cut from one run
Processed documentation: Some runs contain 2, 3 and even 4 groups.
===========================================================================================
Cleaned documentation: We can quickly check the quality of the predictions... One thing is clear, there is room for improvement
Processed documentation: One thing is clear, there is room for improvement
===========================================================================================
Cleaned documentation: Return a single 256x256 sample. with possibility of returning a gleason score using the masks.
Processed documentation: with possibility of returning a gleason score using the masks.
===========================================================================================
Cleaned documentation: Load an image and select random areas. Return a list of 3 images from areas where there is data.
Processed documentation: Load an image and select random areas.
===========================================================================================
Cleaned documentation: Just to double-check. We know generate images with 3 images with 3 channels each.
Processed documentation: We know generate images with 3 images with 3 channels each.
===========================================================================================
Cleaned documentation: high_corr = pd.DataFrame(abs(train.corr high_corr_square = high_corr[high_corr.columns[:10].tolist sns.set_context("notebook", font_scale=1.2, rc={"lines.linewidth plt.figure(figsize = (12, sns.heatmap(high_corr_square,linecolor ='white',linewidths=1,annot=True
Processed documentation: DataFrame(abs(train.corr high_corr_square = high_corr[high_corr.columns[:10].tolist sns.set_context("notebook", font_scale=1.2, rc={"lines.linewidth plt.figure(figsize = (12, sns.heatmap(high_corr_square,linecolor ='white',linewidths=1,annot=True
===========================================================================================
Cleaned documentation: As we see, more than 50 features are eliminated when lambda\_w=2e-5. That's why it called feature elimination using LASSO.
Processed documentation: That's why it called feature elimination using LASSO.
===========================================================================================
Cleaned documentation: Loads scans from a folder and into a list. Parameters: path (Folder path) Returns: slices (List of slices
Processed documentation: Parameters: path (Folder path) Returns: slices (List of slices
===========================================================================================
Cleaned documentation: Generates markers for a given image. Parameters: image. Returns: Internal Marker, External Marker, Watershed Marker.
Processed documentation: Returns: Internal Marker, External Marker, Watershed Marker.
===========================================================================================
Cleaned documentation: Generates markers for a given image. Parameters: image. Returns: Internal Marker, External Marker, Watershed Marker.
Processed documentation: Returns: Internal Marker, External Marker, Watershed Marker.
===========================================================================================
Cleaned documentation: caculate volume of lung from mask. Parameters: list dicom scans,list patient CT image. Returns: volume cm³　(float
Processed documentation: Parameters: list dicom scans,list patient CT image.
===========================================================================================
Cleaned documentation: caculate hisgram kurthosis of lung hounsfield. Parameters: list patient CT image 512512,thresh divide lung. Returns: histgram statistical characteristic(Mean,Skew,Kurthosis
Processed documentation: Parameters: list patient CT image 512512,thresh divide lung.
===========================================================================================
Cleaned documentation: Vast majority of the annotated objects is just car. We have other vehicles and pedestrians to detect.
Processed documentation: Vast majority of the annotated objects is just car.
===========================================================================================
Cleaned documentation: And here comes the morphology. We will use Dilation (read more Erosion (read more
Processed documentation: We will use Dilation (read more Erosion (read more
===========================================================================================
Cleaned documentation: We create a new cell for "clean" comments. Comments that correspond to none of the 6 categories.
Processed documentation: We create a new cell for "clean" comments.
===========================================================================================
Cleaned documentation: Just a list that contains all the text data. For me not to load the whole dataset everytime
Processed documentation: Just a list that contains all the text data.
===========================================================================================
Cleaned documentation: Till now we have worked with raw text. Let's do some text processing now
Processed documentation: Till now we have worked with raw text.
===========================================================================================
Cleaned documentation: Without Lemmatization, the roots of words can be separated in different bars. So we won't use this processed data
Processed documentation: Without Lemmatization, the roots of words can be separated in different bars.
===========================================================================================
Cleaned documentation: Another Kaggler, Jagan, proposed to identify spam comments by their percentage of unique word. Great idea
Processed documentation: Another Kaggler, Jagan, proposed to identify spam comments by their percentage of unique word.
===========================================================================================
Cleaned documentation: This is the end of my first EDA. Let's summarize what we have learned from that
Processed documentation: Let's summarize what we have learned from that
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: Normalize PIL image. Normalizes luminance to (mean,std)=(0,1), and applies a [1%, 99%] contrast stretch.
Processed documentation: Normalizes luminance to (mean,std)=(0,1), and applies a [1%, 99%] contrast stretch.
===========================================================================================
Cleaned documentation: Resize PIL image. Resizes image to be square with sidelength size. Pads with black if needed.
Processed documentation: Resizes image to be square with sidelength size.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Unfortunately this throws a size error here but not in my local notebook. I am just leaving the
Processed documentation: Unfortunately this throws a size error here but not in my local notebook.
===========================================================================================
Cleaned documentation: Find all the features with covariate shift. Print during the procedure and then save in array
Processed documentation: Find all the features with covariate shift.
===========================================================================================
Cleaned documentation: Find all the features with covariate shift. Print during the procedure and then save in array
Processed documentation: Find all the features with covariate shift.
===========================================================================================
Cleaned documentation: param img: image to be rescaled. param scale_range: (tuple) (min, max) of the desired rescaling.
Processed documentation: param scale_range: (tuple) (min, max) of the desired rescaling.
===========================================================================================
Cleaned documentation: Pneumonia dataset that contains radiograph lung images as .dcm. Each patient has one image named patientId.dcm.
Processed documentation: Pneumonia dataset that contains radiograph lung images as .dcm.
===========================================================================================
Cleaned documentation: A simple class that maintains the running average of a quantity. Example: ``` loss_avg = RunningAverage() loss_avg.update(2) loss_avg.update(4) loss_avg
Processed documentation: A simple class that maintains the running average of a quantity.
===========================================================================================
Cleaned documentation: Given a patient ID, return useful meta-data from the corresponding dicom image header. Return: attribute value.
Processed documentation: Given a patient ID, return useful meta-data from the corresponding dicom image header.
===========================================================================================
Cleaned documentation: Given a patient ID, return useful meta-data from the corresponding dicom image header. Return: attribute value.
Processed documentation: Given a patient ID, return useful meta-data from the corresponding dicom image header.
===========================================================================================
Cleaned documentation: I cannot tell them apart. Is this noise or dameged Did I maked a mistake in plot.....
Processed documentation: Is this noise or dameged Did I maked a mistake in plot.....
===========================================================================================
Cleaned documentation: Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.
Processed documentation: The primary language used is English, with some in Malay or Chinese.
===========================================================================================
Cleaned documentation: Same as changing pitch, this augmentation is performed by librosa function. It stretches times series by a fixed rate.
Processed documentation: Same as changing pitch, this augmentation is performed by librosa function.
===========================================================================================
Cleaned documentation: Create ```my_files``` folder inside darknet directory for nessesary files. Copy prepaired files and add pre-trained weights.
Processed documentation: Create ```my_files``` folder inside darknet directory for nessesary files.
===========================================================================================
Cleaned documentation: Now we have log.txt with all bboxes in it. Time to parse this data to panads data frame.
Processed documentation: Time to parse this data to panads data frame.
===========================================================================================
Cleaned documentation: This event occurs when the player clicks to start. the game from the starting screen.
Processed documentation: This event occurs when the player clicks to start.
===========================================================================================
Cleaned documentation: This event occurs when the player clicks to start. the game from the starting screen.
Processed documentation: This event occurs when the player clicks to start.
===========================================================================================
Cleaned documentation: Autocorrelation for single data series. param series: traffic series. param lag: lag, days. return
Processed documentation: param series: traffic series.
===========================================================================================
Cleaned documentation: REGULARIZING RNNS BY STABILIZING ACTIVATIONS. param rnn_output: [time, batch, features] :return: loss value. extra_out['hiddens'].norm(dim=-1) - args.norm_stabilizer_fixed_point).pow(2).mean
Processed documentation: param rnn_output: [time, batch, features] :return: loss value.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: Let's also show a selection of the train set. We prepare the test set.
Processed documentation: Let's also show a selection of the train set.
===========================================================================================
Cleaned documentation: Let's also show the geographical features on a plotly map. We will show the country feature distribution.
Processed documentation: Let's also show the geographical features on a plotly map.
===========================================================================================
Cleaned documentation: Let's group now on `site` name. We process both the train and test data.
Processed documentation: We process both the train and test data.
===========================================================================================
Cleaned documentation: Extract the exif data from any image. Data includes GPS coordinates, Focal Length, Manufacture, and more.
Processed documentation: Data includes GPS coordinates, Focal Length, Manufacture, and more.
===========================================================================================
Cleaned documentation: Returns a dictionary from the exif data of an PIL Image item. Also converts the GPS Tags.
Processed documentation: Returns a dictionary from the exif data of an PIL Image item.
===========================================================================================
Cleaned documentation: Helper function to convert the GPS coordinates. stored in the EXIF to degress in float format.
Processed documentation: Helper function to convert the GPS coordinates.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: For more visibility, we can show samples of data. We will use time intervals of 50 ms.
Processed documentation: We will use time intervals of 50 ms.
===========================================================================================
Cleaned documentation: We extracted the topics using LDA. Let's represent the topics using the `pyLDAvis` tool.
Processed documentation: Let's represent the topics using the `pyLDAvis` tool.
===========================================================================================
Cleaned documentation: credits to. credits to. input: list of list of words. output: dictionary of words and their count.
Processed documentation: input: list of list of words.
===========================================================================================
Cleaned documentation: credits to. credits to. input: vocabulary, embedding matrix. output: modify the embeddings to include the lower case from vocabulary.
Processed documentation: output: modify the embeddings to include the lower case from vocabulary.
===========================================================================================
Cleaned documentation: Let's show the purchase amount grouped by purchase time types. Before this, let's extract the date.
Processed documentation: Let's show the purchase amount grouped by purchase time types.
===========================================================================================
Cleaned documentation: Let's check the distribution of the purchase amount grouped by various features. We will represent log(purchase_amount + 1).
Processed documentation: Let's check the distribution of the purchase amount grouped by various features.
===========================================================================================
Cleaned documentation: Majority of sequence number frames are 1 (61%), followed by 3 (37%), the rest (1.2%) having 5. Locations distribution
Processed documentation: Majority of sequence number frames are 1 (61%), followed by 3 (37%), the rest (1.2%) having 5.
===========================================================================================
Cleaned documentation: Let's plot some image samples from train_images. For this visualization, I reused the code from this Kernel
Processed documentation: Let's plot some image samples from train_images.
===========================================================================================
Cleaned documentation: All `reg` features shows well balanced train and test sets. Let's continue with `car` features.
Processed documentation: All `reg` features shows well balanced train and test sets.
===========================================================================================
Cleaned documentation: All `ind` features are well balanced between `train` and `test` sets. Let's check now `calc` features.
Processed documentation: All `ind` features are well balanced between `train` and `test` sets.
===========================================================================================
Cleaned documentation: The first 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The first 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: The next 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The next 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: The next 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The next 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: The next 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The next 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: The next 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The next 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: There are no missing values in train and test data. Let's check also train labels.
Processed documentation: There are no missing values in train and test data.
===========================================================================================
Cleaned documentation: Also, train labels has no missing data. Let's check now the data distribution using describe.
Processed documentation: Let's check now the data distribution using describe.
===========================================================================================
Cleaned documentation: Let's check the confusion matrix. We will use a simplifed version of the plot function defined here
Processed documentation: We will use a simplifed version of the plot function defined here
===========================================================================================
Cleaned documentation: There are no missing data in the train set. Let's check the missing data for test set.
Processed documentation: Let's check the missing data for test set.
===========================================================================================
Cleaned documentation: Let's put together all these data and compare them. For convenience, we will compare the densities.
Processed documentation: Let's put together all these data and compare them.
===========================================================================================
Cleaned documentation: We create the split with 5 folds. We build the model for training and we init the predictions.
Processed documentation: We build the model for training and we init the predictions.
===========================================================================================
Cleaned documentation: Train and fit the lgb model with 5 folds. Then calculate the Full Out-Of-Fold score according to [3].
Processed documentation: Then calculate the Full Out-Of-Fold score according to [3].
===========================================================================================
Cleaned documentation: Let's explore the two loaded files. We will take out a 5 rows samples from each dataset.
Processed documentation: Let's explore the two loaded files.
===========================================================================================
Cleaned documentation: The files names are the patients IDs. Let's check how many images are in the train and test folders.
Processed documentation: Let's check how many images are in the train and test folders.
===========================================================================================
Cleaned documentation: Let's sample few images having the Target = 1. Plot DICOM images with Target
Processed documentation: Let's sample few images having the Target = 1.
===========================================================================================
Cleaned documentation: Go to top Modality Let's check how many modalities are used. Both train and test set are checked.
Processed documentation: Go to top Modality Let's check how many modalities are used.
===========================================================================================
Cleaned documentation: Let's get into more details for the train dataset. First, let's check the distribution of PA and AP.
Processed documentation: First, let's check the distribution of PA and AP.
===========================================================================================
Cleaned documentation: Let's check also the distribution of patient age for the test data set. Test dataset
Processed documentation: Let's check also the distribution of patient age for the test data set.
===========================================================================================
Cleaned documentation: Let's check as well the distribution of Patient Sex for the test data. Test dataset
Processed documentation: Let's check as well the distribution of Patient Sex for the test data.
===========================================================================================
Cleaned documentation: Predicting the labels seems to be working just fine. Let's try predicting probabilities using the `predict_proba` method.
Processed documentation: Let's try predicting probabilities using the `predict_proba` method.
===========================================================================================
Cleaned documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns. Let's glimpse train and test dataset.
Processed documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns.
===========================================================================================
Cleaned documentation: The first 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The first 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: The next 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The next 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: The next 100 values are displayed in the following cell. Press Output to display the plots.
Processed documentation: The next 100 values are displayed in the following cell.
===========================================================================================
Cleaned documentation: for i in range(max_range): X_train,X_test,y_train,y_test = tts(X,y,test_size =0.3,random_state =i) model = regr() model.fit(X_train,y_train) print("Random state {}\n".format(i)) print(r2_score(y_test,model.predict(X_test
Processed documentation: 0.3,random_state =i) model = regr() model.fit(X_train,y_train) print("Random state {}\n".format(i))
===========================================================================================
Cleaned documentation: When trying to encode variabels, receive the following ValueError: y contains previously unseen labels: [nan, nan, nan,... nan
Processed documentation: [nan, nan, nan,... nan
===========================================================================================
Cleaned documentation: Normalize numerical variables. Args: df (pd.DataFrame): dataframe to be normalized. Returns: df (pd.Dataframe): dataframe where each column has mean
Processed documentation: Args: df (pd.
===========================================================================================
Cleaned documentation: Still there are a lot of NaNs in the card2 and card5. Let's try some other fill combinations.
Processed documentation: Still there are a lot of NaNs in the card2 and card5.
===========================================================================================
Cleaned documentation: Calculate the probability of being in state `k` at time `t`, given all previous observations `x_1 ... x_t
Processed documentation: Calculate the probability of being in state `k` at time `t`, given all previous observations `x_1 ...
===========================================================================================
Cleaned documentation: Calculate the probability of observing `x_{t + 1} ... x_n` if we. start in state `k` at time `t`.
Processed documentation: x_n` if we. start in state `k` at time `t`.
===========================================================================================
Cleaned documentation: TSP will minimize costs, but we want to maximize. This is a simple hack
Processed documentation: TSP will minimize costs, but we want to maximize.
===========================================================================================
Cleaned documentation: will be a dummy starting location. We use the last element of the previous
Processed documentation: will be a dummy starting location.
===========================================================================================
Cleaned documentation: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=0.5, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=3, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1, oob_score=False, random_state=42, verbose=0, warm_start=False
Processed documentation: None, min_samples_leaf=3, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1, oob_score=False, random_state=42, verbose=0, warm_start=False
===========================================================================================
Cleaned documentation: if numpy_labels.dtype == object: binary string in this case, these are image ID strings
Processed documentation: object: binary string in this case, these are image ID strings
===========================================================================================
Cleaned documentation: Now we need to drop the columns that we will not use to train our model. key pickup_datetime
Processed documentation: Now we need to drop the columns that we will not use to train our model.
===========================================================================================
Cleaned documentation: Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.
Processed documentation: Block of code used for hypertuning parameters.
===========================================================================================
Cleaned documentation: See the structure of the annotation. It is a classic XML with the bbox at `annotation/object/bndbox`.
Processed documentation: It is a classic XML with the bbox at `annotation/object/bndbox`.
===========================================================================================
Cleaned documentation: check total number of lines of file for large files. Args: fpath: string. file path. Returns: None.
Processed documentation: check total number of lines of file for large files.
===========================================================================================
Cleaned documentation: Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces
Processed documentation: Disable multiprocesing for numpy/opencv.
===========================================================================================
Cleaned documentation: We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.
Processed documentation: We quantize to uint8 here to conserve memory.
===========================================================================================
Cleaned documentation: There are only 9996 non-zero revenue visitors out of 714167 visitors (1.3%). Let's analyze contributions of non-zero revenue visitors.
Processed documentation: There are only 9996 non-zero revenue visitors out of 714167 visitors (1.3%).
===========================================================================================
Cleaned documentation: train=pd.merge(left=train , right=card_id_cnt, how = 'left', on ='card_id test=pd.merge(left=test , right=card_id_cnt, how = 'left', on ='card_id
Processed documentation: pd.merge(left=train , right=card_id_cnt, how = 'left', on ='card_id test=
===========================================================================================
Cleaned documentation: train=pd.merge(left=train , right=card_id_cnt_new, how = 'left', on ='card_id test=pd.merge(left=test , right=card_id_cnt_new, how = 'left', on ='card_id
Processed documentation: pd.merge(left=train , right=card_id_cnt_new, how = 'left', on ='card_id test=
===========================================================================================
Cleaned documentation: Let's visualize now the data. We select first a list of fake videos. Few fake videos
Processed documentation: We select first a list of fake videos.
===========================================================================================
Cleaned documentation: Let's try now the same for few of the images that are real. Few real videos
Processed documentation: Let's try now the same for few of the images that are real.
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: returns a tuple of topic categories and their. accompanying magnitudes for a given list of keys.
Processed documentation: accompanying magnitudes for a given list of keys.
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: train['GameWeather'] = train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x
Processed documentation: train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x
===========================================================================================
Cleaned documentation: Let's now do the same for the test set. I submitted this in version 1 of this kernel
Processed documentation: Let's now do the same for the test set.
===========================================================================================
Cleaned documentation: returns a tuple of topic categories and their. accompanying magnitudes for a given list of keys.
Processed documentation: accompanying magnitudes for a given list of keys.
===========================================================================================
Cleaned documentation: returns a tuple of topic categories and their. accompanying magnitudes for a given list of keys.
Processed documentation: accompanying magnitudes for a given list of keys.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Here, I just want to simplify `passenger_count` as either one or more than one person. So,
Processed documentation: Here, I just want to simplify `passenger_count` as either one or more than one person.
===========================================================================================
Cleaned documentation: Produce an equivalent .csv + DataFrame to output with the train ground truth data
Processed documentation: + DataFrame to output with the train ground truth data
===========================================================================================
Cleaned documentation: Now lets import the data. The dtypes come from this kernel: [Load the Totality of the Data
Processed documentation: Now lets import the data.
===========================================================================================
Cleaned documentation: Changes Scikit learn's random forests to give each tree a random sample of. n random rows.
Processed documentation: Changes Scikit learn's random forests to give each tree a random sample of.
===========================================================================================
Cleaned documentation: It seems that some of the features are highly correclated. Lets try to remove these one at a time.
Processed documentation: Lets try to remove these one at a time.
===========================================================================================
Cleaned documentation: In the limit of a dirac delta, skew should be 0 and variance. However, in the discrete limit,
Processed documentation: In the limit of a dirac delta, skew should be 0 and variance.
===========================================================================================
Cleaned documentation: In the limit of a dirac delta, kurtosis should be 3 and variance. However, in the discrete limit,
Processed documentation: In the limit of a dirac delta, kurtosis should be 3 and variance.
===========================================================================================
Cleaned documentation: Loop over the words in the review. If the word is in the vocabulary,
Processed documentation: Loop over the words in the review.
===========================================================================================
Cleaned documentation: Checking the distribution of labels and basic EDA.. Todo: Training with old competition data, data imbalance etc.
Processed documentation: Todo: Training with old competition data, data imbalance etc.
===========================================================================================
Cleaned documentation: Retunrs a color patch. The color will be `true_color` if `x` is True otherwise `false_color`.
Processed documentation: The color will be `true_color` if `x` is True otherwise `false_color`.
===========================================================================================
Cleaned documentation: So the percentage of NeverSmoked in Female is more. We will consider this information during our Experiments.
Processed documentation: So the percentage of NeverSmoked in Female is more.
===========================================================================================
Cleaned documentation: There seems be 5 female patients with FVC > 2800. Let us examine if these patients ever had FVC
Processed documentation: Let us examine if these patients ever had FVC
===========================================================================================
Cleaned documentation: Calculate spectrogram using pytorch. The STFT is implemented with. Conv1d. The function has the same output of librosa.core.stft.
Processed documentation: Calculate spectrogram using pytorch.
===========================================================================================
Cleaned documentation: Calculate logmel spectrogram using pytorch. The mel filter bank is. the pytorch implementation of as librosa.filters.mel.
Processed documentation: Calculate logmel spectrogram using pytorch.
===========================================================================================
Cleaned documentation: PANNsCNN14Att.cnn_feature_extractor()` method will take this as input and output feature map. Let's check the output of the feature extractor.
Processed documentation: PANNsCNN14Att.cnn_feature_extractor()` method will take this as input and output feature map.
===========================================================================================
Cleaned documentation: Remove unnecessary data. Well ID_code might have some leak, but we don't deep dive for now
Processed documentation: Remove unnecessary data.
===========================================================================================
Cleaned documentation: This can be improved by many ways. e.g., more layers, batch normalization and etc.
Processed documentation: e.g., more layers, batch normalization and etc.
===========================================================================================
Cleaned documentation: As you can see, our signal become much smoother than before. Here's a closer comparison
Processed documentation: As you can see, our signal become much smoother than before.
===========================================================================================
Cleaned documentation: As this is a multi class classification problem. Lets try Random Forest Classifier algorithm.
Processed documentation: Lets try Random Forest Classifier algorithm.
===========================================================================================
Cleaned documentation: Args: index (int): Index. Returns: tuple: (sample, target) where target is class_index of the target class.
Processed documentation: Returns: tuple: (sample, target) where target is class_index of the target class.
===========================================================================================
Cleaned documentation: Constructs a EfficientNetB0 model for FastAI. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet.
Processed documentation: Args: pretrained (bool): If True, returns a model pre-trained on ImageNet.
===========================================================================================
Cleaned documentation: Constructs a EfficientNet model for FastAI. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet.
Processed documentation: Args: pretrained (bool): If True, returns a model pre-trained on ImageNet.
===========================================================================================
Cleaned documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset. Overrides values in the base Config class.
Processed documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset.
===========================================================================================
Cleaned documentation: Train on 1 GPU and 8 images per GPU. We can put multiple images on each
Processed documentation: Train on 1 GPU and 8 images per GPU.
===========================================================================================
Cleaned documentation: Now it's time to train the model. Note that training even a basic model can take a few hours.
Processed documentation: Note that training even a basic model can take a few hours.
===========================================================================================
Cleaned documentation: How does the predicted box compared to the expected value? Let's use the validation dataset to check.
Processed documentation: How does the predicted box compared to the expected value?
===========================================================================================
Cleaned documentation: The Trainer automates the rest. Trains on 8 TPU cores, GPU or CPU - whatever is available.
Processed documentation: Trains on 8 TPU cores, GPU or CPU - whatever is available.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset. Overrides values in the base Config class.
Processed documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset.
===========================================================================================
Cleaned documentation: Train on 1 GPU and 8 images per GPU. We can put multiple images on each
Processed documentation: Train on 1 GPU and 8 images per GPU.
===========================================================================================
Cleaned documentation: Now it's time to train the model. Note that training even a basic model can take a few hours.
Processed documentation: Note that training even a basic model can take a few hours.
===========================================================================================
Cleaned documentation: How does the predicted box compared to the expected value? Let's use the validation dataset to check.
Processed documentation: How does the predicted box compared to the expected value?
===========================================================================================
Cleaned documentation: Let's look at the distribution of all points. Image is here just for reference.
Processed documentation: Let's look at the distribution of all points.
===========================================================================================
Cleaned documentation: mask_loss = mask (1 - pred_mask)2 torch.log(pred_mask + 1e-12) + (1 - mask) pred_mask2 torch.log(1 - pred_mask + 1e
Processed documentation: mask_loss = mask (1 - pred_mask)2 torch.log(pred_mask + 1e-12) + (1 - mask)
===========================================================================================
Cleaned documentation: Let's see what six plays look like. Below, I take three plays in each direction
Processed documentation: Let's see what six plays look like.
===========================================================================================
Cleaned documentation: this feature map is very important. It allows the TPU to parse the tfrecord and will be used later
Processed documentation: It allows the TPU to parse the tfrecord and will be used later
===========================================================================================
Cleaned documentation: with strategy.scope(): model = your_model. metrics = [tf.keras.metrics.AUC()] model.compile( optimizer='adam', loss = losses, metrics= metrics
Processed documentation: AUC()] model.compile( optimizer='adam', loss = losses, metrics= metrics
===========================================================================================
Cleaned documentation: Here we import the libraries we need. We'll learn about what each does during the course.
Processed documentation: Here we import the libraries we need.
===========================================================================================
Cleaned documentation: Next we'll extract features "CompetitionOpenSince" and "CompetitionDaysOpen". Note the use of `apply()` in mapping a function across dataframe values.
Processed documentation: Note the use of `apply()` in mapping a function across dataframe values.
===========================================================================================
Cleaned documentation: We're ready to put together our models. Root-mean-squared percent error is the metric Kaggle used for this competition.
Processed documentation: Root-mean-squared percent error is the metric Kaggle used for this competition.
===========================================================================================
Cleaned documentation: Some categorical variables have a lot more levels than others. Store, in particular, has over a thousand
Processed documentation: Some categorical variables have a lot more levels than others.
===========================================================================================
Cleaned documentation: There is a lot less variables we have to check one by one. OWN_CAR_AGE
Processed documentation: There is a lot less variables we have to check one by one.
===========================================================================================
Cleaned documentation: So the proportion is about 2 ddf objects to each 5 objects detected. Now, the clusters...
Processed documentation: So the proportion is about 2 ddf objects to each 5 objects detected.
===========================================================================================
Cleaned documentation: Well, that's a fairly high correlation. Let's plot to visually confirm what's going on here
Processed documentation: Let's plot to visually confirm what's going on here
===========================================================================================
Cleaned documentation: Most users don't have batteries configured to be always connected. What about the RAM
Processed documentation: Most users don't have batteries configured to be always connected.
===========================================================================================
Cleaned documentation: make sure all preprocessing done in the training. image generator is done in test generator as well.
Processed documentation: image generator is done in test generator as well.
===========================================================================================
Cleaned documentation: Given a time series (signal) and an epsilon, return the modified Recurrent Plot matrix. using sigmoid rather than heaviside.
Processed documentation: Given a time series (signal) and an epsilon, return the modified Recurrent Plot matrix.
===========================================================================================
Cleaned documentation: Now each object has a 6-channel image. They all look like the image below
Processed documentation: Now each object has a 6-channel image.
===========================================================================================
Cleaned documentation: Input format number_of_photos horizontal_or_vertical number_of_tags tag1 tag2 tag3 ... horizontal_or_vertical number_of_tags tag1 tag2 tag3 ...
Processed documentation: Input format number_of_photos horizontal_or_vertical number_of_tags tag1 tag2 tag3 ...
===========================================================================================
Cleaned documentation: This is basic preprocessing. This time, symbols and words are attached, so they are separated here.
Processed documentation: This time, symbols and words are attached, so they are separated here.
===========================================================================================
Cleaned documentation: Converts an image tensor that was previously Normalize'd. back to an image with pixels in the range [0, 1].
Processed documentation: Converts an image tensor that was previously Normalize'd.
===========================================================================================
Cleaned documentation: Before we train, let's run the model on the validation set. This should give a logloss of about 0.6931.
Processed documentation: Before we train, let's run the model on the validation set.
===========================================================================================
Cleaned documentation: Plot training progress. It's nicer to use something like TensorBoard for this, but a simple plot also works.
Processed documentation: It's nicer to use something like TensorBoard for this, but a simple plot also works.
===========================================================================================
Cleaned documentation: Convert mask to rle. img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Plot image and masks. If two pairs of images and masks are passes, show both.
Processed documentation: If two pairs of images and masks are passes, show both.
===========================================================================================
Cleaned documentation: Plot image and masks. If two pairs of images and masks are passes, show both.
Processed documentation: If two pairs of images and masks are passes, show both.
===========================================================================================
Cleaned documentation: Post processing of each predicted mask, components with lesser number of pixels. than `min_size` are ignored.
Processed documentation: Post processing of each predicted mask, components with lesser number of pixels.
===========================================================================================
Cleaned documentation: This is how original image and its masks look like. Let's try adding some augmentations
Processed documentation: This is how original image and its masks look like.
===========================================================================================
Cleaned documentation: As expected, these first five values mirror the first five values of hpg_station_distances['(35.6436746642265, 139.668220854814)']. Let's keep going.
Processed documentation: As expected, these first five values mirror the first five values of hpg_station_distances['(35.6436746642265, 139.668220854814)'].
===========================================================================================
Cleaned documentation: Create neural network. Densenet121 with imagenet pretrain in this kernel. U can change it ofc.
Processed documentation: Create neural network.
===========================================================================================
Cleaned documentation: Implementation QWK loss. Be careful, implementation have some changes for probabilistic output, confusion matrix compute with no rounded output.
Processed documentation: Be careful, implementation have some changes for probabilistic output, confusion matrix compute with no rounded output.
===========================================================================================
Cleaned documentation: Plot the distribution of lables to see how the model performs. Be aware about log scale.
Processed documentation: Plot the distribution of lables to see how the model performs.
===========================================================================================
Cleaned documentation: The classes are imbalanced, more severe cases are underrepresented. Let's start preparing the images for training.
Processed documentation: The classes are imbalanced, more severe cases are underrepresented.
===========================================================================================
Cleaned documentation: Check data types of our columns. Many columns have a larger data type than they should
Processed documentation: Many columns have a larger data type than they should
===========================================================================================
Cleaned documentation: ship will move in expanding circles clockwise or counterclockwise. until reaching maximum radius, then radius will be minimal again.
Processed documentation: until reaching maximum radius, then radius will be minimal again.
===========================================================================================
Cleaned documentation: Index of the class in the list is its ID. For example, to get ID of
Processed documentation: Index of the class in the list is its ID.
===========================================================================================
Cleaned documentation: v ranges from 0 to. This gives an extra flexibility of measuring distance from any of the 4 corners
Processed documentation: This gives an extra flexibility of measuring distance from any of the 4 corners
===========================================================================================
Cleaned documentation: As you can see, our signal become much smoother than before. Here's a closer comparison
Processed documentation: As you can see, our signal become much smoother than before.
===========================================================================================
Cleaned documentation: Thank you for reading my first kernel. I hope this kernel can be usefull for your competition
Processed documentation: I hope this kernel can be usefull for your competition
===========================================================================================
Cleaned documentation: Takes a scalar and returns a string with. the css property `'color: red'` for negative. strings, black otherwise.
Processed documentation: the css property `'color: red'` for negative.
===========================================================================================
Cleaned documentation: The `training` folder has 400 JSON tasks. The names of the first three are shown below.
Processed documentation: The `training` folder has 400 JSON tasks.
===========================================================================================
Cleaned documentation: Takes a scalar and returns a string with. the css property `'color: red'` for negative. strings, black otherwise.
Processed documentation: the css property `'color: red'` for negative.
===========================================================================================
Cleaned documentation: The largest number of post falls in the last years. Let`s leave only 2015-2017.
Processed documentation: The largest number of post falls in the last years.
===========================================================================================
Cleaned documentation: I've train in local machine of all 5 folds. Let's define the models and load the weights.
Processed documentation: Let's define the models and load the weights.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: Training a CapsuleNet. param model: the CapsuleNet model. param args: arguments. return: The trained model.
Processed documentation: Training a CapsuleNet. param model: the CapsuleNet model.
===========================================================================================
Cleaned documentation: Here I will be following [xhlulu]( approach. Appreciate his effort if you his notebook.
Processed documentation: Appreciate his effort if you his notebook.
===========================================================================================
Cleaned documentation: Returns the sum of original image, prediction and product of them. for the number of required iterations.
Processed documentation: Returns the sum of original image, prediction and product of them.
===========================================================================================
Cleaned documentation: Computes dice for all the images given and the number. of erodes and dilates required.
Processed documentation: Computes dice for all the images given and the number.
===========================================================================================
Cleaned documentation: one token at a time. This makes more sense than truncating an equal percent
Processed documentation: This makes more sense than truncating an equal percent
===========================================================================================
Cleaned documentation: mask_loss = mask torch.log(pred_mask + 1e-12) + (1 - mask) torch.log(1 - pred_mask + 1e
Processed documentation: mask_loss = mask torch.log(pred_mask + 1e-12)
===========================================================================================
Cleaned documentation: I can't get the pretrained weight on Kaggle kernel, so we will set weights = None for now.
Processed documentation: I can't get the pretrained weight on Kaggle kernel, so we will set weights =
===========================================================================================
Cleaned documentation: tbCallback = TensorBoard(log_dir = './Graph', histogram_freq = 0, write_graph = True, write_images = True
Processed documentation: True, write_images =
===========================================================================================
Cleaned documentation: Probably this is the effect of fraud access. Or I may possibly have some misunderstanding.
Processed documentation: Probably this is the effect of fraud access.
===========================================================================================
Cleaned documentation: This result seems to be indefinite for 'X' and 'M'. I dont know why...
Processed documentation: This result seems to be indefinite for 'X' and 'M'.
===========================================================================================
Cleaned documentation: EfficientNet models for Keras. Reference paper: - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks] ( (ICML
Processed documentation: Reference paper: - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks] ( (ICML
===========================================================================================
Cleaned documentation: Let's see the result without taking FN into account. I guess the previous LB was similar to this.
Processed documentation: Let's see the result without taking FN into account.
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: EfficientNet models for Keras. Reference paper: - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks] ( (ICML
Processed documentation: Reference paper: - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks] ( (ICML
===========================================================================================
Cleaned documentation: Mean value of signal `x` squared. param x: Dynamic quantity. returns: Mean squared of `x`.
Processed documentation: Mean value of signal `x` squared.
===========================================================================================
Cleaned documentation: Decorator function intended for grouping the functions which are applied over the output of an output path retrieval. function.
Processed documentation: Decorator function intended for grouping the functions which are applied over the output of an output path retrieval.
===========================================================================================
Cleaned documentation: Decorator function intended for grouping the functions which are applied over the output of an input path retrieval. function.
Processed documentation: Decorator function intended for grouping the functions which are applied over the output of an input path retrieval.
===========================================================================================
Cleaned documentation: on multiple cpus. I have choosen to process in chunks of 1000 (plus the remainder
Processed documentation: I have choosen to process in chunks of 1000 (plus the remainder
===========================================================================================
Cleaned documentation: From @randxie. Modified to work with scipy version 1.1.0 which does not have the fs parameter.
Processed documentation: Modified to work with scipy version 1.1.0 which does not have the fs parameter.
===========================================================================================
Cleaned documentation: Fault pattern usually exists in high frequency band. According to literature, the pattern is visible above 10^4 Hz.
Processed documentation: Fault pattern usually exists in high frequency band.
===========================================================================================
Cleaned documentation: calculate Cramers V statistic for categorial-categorial association. uses correction from Bergsma and Wicher, Journal of the Korean Statistical Society
Processed documentation: calculate Cramers V statistic for categorial-categorial association.
===========================================================================================
Cleaned documentation: Get Wordcount,Unique Wordcount and WordCount Percent and make appropriate visuals. Todo: Add more text stats.
Processed documentation: Get Wordcount,Unique Wordcount and WordCount Percent and make appropriate visuals.
===========================================================================================
Cleaned documentation: The above plot is Interactive. Hover over the states to get the individual numbers of that state. Time
Processed documentation: Hover over the states to get the individual numbers of that state.
===========================================================================================
Cleaned documentation: Hope you liked it 😜, comment below your suggestions / feedback. I will upload another notebook for inference soon.
Processed documentation: Hope you liked it 😜, comment below your suggestions / feedback.
===========================================================================================
Cleaned documentation: we use this to ensure values remain between 0 and. Since std deviation reduces to almost half every layer
Processed documentation: we use this to ensure values remain between 0 and.
===========================================================================================
Cleaned documentation: Get LR using best lf find . 1e-2 to 3e-2 is the suitable LR.
Processed documentation: Get LR using best lf find .
===========================================================================================
Cleaned documentation: Now let's proceed to cleaning. First we binarize both the mask and contours using global, otsu thresholding method
Processed documentation: First we binarize both the mask and contours using global, otsu thresholding method
===========================================================================================
Cleaned documentation: There is a problem with this approach. We are left with artifacts. Let's use binary_openning to drop them.
Processed documentation: Let's use binary_openning to drop them.
===========================================================================================
Cleaned documentation: which are not in the bureau data table. As such, there are no corresponding
Processed documentation: which are not in the bureau data table.
===========================================================================================
Cleaned documentation: max_bin. max_bin = 255 allows to uses 8 bits to store a single value.
Processed documentation: 255 allows to uses 8 bits to store a single value.
===========================================================================================
Cleaned documentation: Zooming in and out can be performed using `cv2.resize()` and `skimage.measure.block_reduce()`. We get 4 matches.
Processed documentation: Zooming in and out can be performed using `cv2.resize()` and `skimage.measure.block_reduce()`.
===========================================================================================
Cleaned documentation: The largest number of post falls in the last years. Let`s leave only 2015-2017.
Processed documentation: The largest number of post falls in the last years.
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: tokens: [CLS] is this jack son ville ? [SEP] no it is not. SEP
Processed documentation: tokens: [CLS] is this jack son ville ?
===========================================================================================
Cleaned documentation: The mask has 1 for real tokens and 0 for padding tokens. Only real
Processed documentation: The mask has 1 for real tokens and 0 for padding tokens.
===========================================================================================
Cleaned documentation: This is for demo purposes and does NOT scale to large data sets. We do
Processed documentation: This is for demo purposes and does NOT scale to large data sets.
===========================================================================================
Cleaned documentation: tokens: [CLS] is this jack son ville ? [SEP] no it is not. SEP
Processed documentation: tokens: [CLS] is this jack son ville ?
===========================================================================================
Cleaned documentation: The mask has 1 for real tokens and 0 for padding tokens. Only real
Processed documentation: The mask has 1 for real tokens and 0 for padding tokens.
===========================================================================================
Cleaned documentation: one token at a time. This makes more sense than truncating an equal percent
Processed documentation: This makes more sense than truncating an equal percent
===========================================================================================
Cleaned documentation: lets take a second look at the confusion matrix. See if how much we improved.
Processed documentation: lets take a second look at the confusion matrix.
===========================================================================================
Cleaned documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset. Overrides values in the base Config class.
Processed documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset.
===========================================================================================
Cleaned documentation: Train on 1 GPU and 8 images per GPU. We can put multiple images on each
Processed documentation: Train on 1 GPU and 8 images per GPU.
===========================================================================================
Cleaned documentation: Now it's time to train the model. Note that training even a basic model can take a few hours.
Processed documentation: Note that training even a basic model can take a few hours.
===========================================================================================
Cleaned documentation: How does the predicted box compared to the expected value? Let's use the validation dataset to check.
Processed documentation: How does the predicted box compared to the expected value?
===========================================================================================
Cleaned documentation: df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis
Processed documentation: + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis
===========================================================================================
Cleaned documentation: Some quick adjustments based on lookups. Not really cryptography but helps to move forward
Processed documentation: Some quick adjustments based on lookups.
===========================================================================================
Cleaned documentation: Divide the available space on an image into 16 sectors. In the [0] image these
Processed documentation: Divide the available space on an image into 16 sectors.
===========================================================================================
Cleaned documentation: test = pd.read_csv('../input/test.csv') Don't import test dataset as anticipate only performing EDA in this kernel and not making submission
Processed documentation: pd.read_csv('../input/test.csv') Don't import test dataset as anticipate only performing EDA in this kernel and not making submission
===========================================================================================
Cleaned documentation: Use a t-SNE plot to identify clusters of images. More details on t-SNE [here
Processed documentation: Use a t-SNE plot to identify clusters of images.
===========================================================================================
Cleaned documentation: As stated earlier, let us first reduce memory wherever possible. But first how much memory does df_train dataframe consume
Processed documentation: As stated earlier, let us first reduce memory wherever possible.
===========================================================================================
Cleaned documentation: We have reduced memory almost by 50%. Let us see the column types to notice the changes
Processed documentation: Let us see the column types to notice the changes
===========================================================================================
Cleaned documentation: We see that the train dataframe has an extra column with missing values (ps_car_12). Visualizing missing values
Processed documentation: We see that the train dataframe has an extra column with missing values (ps_car_12).
===========================================================================================
Cleaned documentation: what_col = ind_cols_no_bin_cat. for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique()) pp = pd.value_counts(df_train[what_col[col]]) pp.plot.bar() plt.show
Processed documentation: for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique()) pp = pd.value_counts(df_train[what_col[col]]) pp.plot.bar() plt.show
===========================================================================================
Cleaned documentation: what_col = reg_cols_no_bin_cat. for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique()) pp = pd.value_counts(df_train[what_col[col]]) pp.plot.bar() plt.show
Processed documentation: for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique())
===========================================================================================
Cleaned documentation: Column ps_reg_03 does not seem to show anything at all, hence can be removed. Visualizing car features
Processed documentation: Column ps_reg_03 does not seem to show anything at all, hence can be removed.
===========================================================================================
Cleaned documentation: what_col = car_cols_no_bin_cat. for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique()) pp = pd.value_counts(df_train[what_col[col]]) pp.plot.bar() plt.show
Processed documentation: for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique())
===========================================================================================
Cleaned documentation: what_col = calc_cols_no_bin_cat. for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique()) pp = pd.value_counts(df_train[what_col[col]]) pp.plot.bar() plt.show
Processed documentation: for col in range(0, len(what_col)): print (what_col[col]) print (df_train[what_col[col]].unique())
===========================================================================================
Cleaned documentation: other_cols_to_delete = ['ps_ind_14', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_reg_03'] for col in other_cols_to_delete: df_train.drop([col], axis=1, inplace=True) df_test.drop([col], axis=1, inplace=True
Processed documentation: ['ps_ind_14', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_reg_03'] for col in other_cols_to_delete: df_train.drop([col], axis=1, inplace=True) df_test.drop([col], axis=1, inplace=True
===========================================================================================
Cleaned documentation: X_train = df_train.drop(['id'],axis = 1) X_id_train = df_train['id'].values. Y_train = target.values. X_test = df_test.drop(['id'], axis=1) X_id_test = df_test['id'].values.
Processed documentation: df_test.drop(['id'], axis=1) X_id_test = df_test['id'].values.
===========================================================================================
Cleaned documentation: The dataframe consumes ~273MB. Our aim is to REDUCE this to a minimum possible.
Processed documentation: The dataframe consumes ~273MB.
===========================================================================================
Cleaned documentation: The train.csv file has been reduced from 15MB to 8MB. Let us now check the transactions.csv file transactions.csv
Processed documentation: The train.csv file has been reduced from 15MB to 8MB.
===========================================================================================
Cleaned documentation: Here we have two date columns as well of type integer. We proceed as before.
Processed documentation: Here we have two date columns as well of type integer.
===========================================================================================
Cleaned documentation: Let us look into the data now. Below I am merging the three dataframes baesd on column msno.
Processed documentation: Below I am merging the three dataframes baesd on column msno.
===========================================================================================
Cleaned documentation: We will see how to fill them later. Let us see the other variables. is_churn vs gender
Processed documentation: Let us see the other variables.
===========================================================================================
Cleaned documentation: There are only 5 ways of registration. Let us plot them against the count.
Processed documentation: Let us plot them against the count.
===========================================================================================
Cleaned documentation: We are unable to visualize any form of trend in the above two plots. Variable: 'payment_method_id
Processed documentation: We are unable to visualize any form of trend in the above two plots.
===========================================================================================
Cleaned documentation: Interesting! We can see a change in the number of unique values in this column.
Processed documentation: We can see a change in the number of unique values in this column.
===========================================================================================
Cleaned documentation: We don't have any. So we can simply impute missing values with a common string.
Processed documentation: So we can simply impute missing values with a common string.
===========================================================================================
Cleaned documentation: Create a map on which to draw. We're using a mercator projection, and showing the whole world.
Processed documentation: We're using a mercator projection, and showing the whole world.
===========================================================================================
Cleaned documentation: Only ONE column 'assessmentyear' has a single distinct value throughout. We can cross check that
Processed documentation: Only ONE column 'assessmentyear' has a single distinct value throughout.
===========================================================================================
Cleaned documentation: New feature: Weekend/weekday transaction Transactions are high during the weekdays. So we can create another categorical feature Weekdays/Weekends
Processed documentation: New feature: Weekend/weekday transaction Transactions are high during the weekdays.
===========================================================================================
Cleaned documentation: We can clearly see 4 distinct outliers. Let us see the statistics of the column.
Processed documentation: Let us see the statistics of the column.
===========================================================================================
Cleaned documentation: WOW!!! Now reduced from ~35MB to ~24MB Let us see what we can do with columns of type `int64`.
Processed documentation: Now reduced from ~35MB to ~24MB Let us see what we can do with columns of type `int64`.
===========================================================================================
Cleaned documentation: examine links between samples. left/right run edges are those samples which do not have a link on that side.
Processed documentation: left/right run edges are those samples which do not have a link on that side.
===========================================================================================
Cleaned documentation: We need to classify on which surface our robot is standing. Multi-class Multi-output classes (suface
Processed documentation: Multi-class Multi-output classes (suface
===========================================================================================
Cleaned documentation: I will analyze my best submissions in order to find something interesting. Please, feel free to optimize this code.
Processed documentation: I will analyze my best submissions in order to find something interesting.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: This is more of a baseline test. This method does not account for a pixel's neighbors.
Processed documentation: This method does not account for a pixel's neighbors.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: The smaller the prob1, the more likely a sample is positive. let's see the performance.
Processed documentation: The smaller the prob1, the more likely a sample is positive.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: They seem to be totally same, so anything is OK to remove. Census_OSArchitecture` vs `Processor`: remove `Processor
Processed documentation: Census_OSArchitecture` vs `Processor`: remove `Processor
===========================================================================================
Cleaned documentation: and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
Processed documentation: A workaround is to add a very small positive number ε to the sum.
===========================================================================================
Cleaned documentation: now that leak is done we should get back ugly data for feature engineering. This might not be necessary.
Processed documentation: now that leak is done we should get back ugly data for feature engineering.
===========================================================================================
Cleaned documentation: Make training data from the original training plus the leak. This is key to getting a good score.
Processed documentation: Make training data from the original training plus the leak.
===========================================================================================
Cleaned documentation: Read in the sample submission. We can use that as a dataframe to grab the segment ids
Processed documentation: We can use that as a dataframe to grab the segment ids
===========================================================================================
Cleaned documentation: to find the DC offset (aka injection current. Assume it is a square wave as described in literature.
Processed documentation: to find the DC offset (aka injection current.
===========================================================================================
Cleaned documentation: x["dt_" + name + "_rel_work" ] = x[name + "_rel_work"] - x[name + "_rel_work"].shift
Processed documentation: + name + "_rel_work" ] = x[name + "_rel_work"] - x[name + "_rel_work"].shift
===========================================================================================
Cleaned documentation: x["dt_" + name + "_power" ] = x[name + "_power"] - x[name + "_power"].shift
Processed documentation: + name + "_power" ] = x[name + "_power"] - x[name + "_power"].shift
===========================================================================================
Cleaned documentation: add up the 100Hz power. Just use positive frequencies for test but apply to pos and neg spectrum
Processed documentation: Just use positive frequencies for test but apply to pos and neg spectrum
===========================================================================================
Cleaned documentation: print('100Hz power in Batch ',ii+1, ' is ', p100,. of Total is ', p100/p
Processed documentation: print('100Hz power in Batch ',ii+1, ' is ', p100,.
===========================================================================================
Cleaned documentation: add up the 100Hz power. Just use positive frequencies for test but apply to pos and neg spectrum
Processed documentation: Just use positive frequencies for test but apply to pos and neg spectrum
===========================================================================================
Cleaned documentation: print('100Hz power in Batch ',ii+1, ' is ', p100,. of Total is ', p100/p
Processed documentation: print('100Hz power in Batch ',ii+1, ' is ', p100,.
===========================================================================================
Cleaned documentation: Ok, so most popular breed in dataset is scottish deerhound with 126 images. Lets look on one of them
Processed documentation: Ok, so most popular breed in dataset is scottish deerhound with 126 images.
===========================================================================================
Cleaned documentation: It seems that the `sentiment` distributions follows somehow the distribution of the entire dataset. Nice.
Processed documentation: It seems that the `sentiment` distributions follows somehow the distribution of the entire dataset.
===========================================================================================
Cleaned documentation: k > 0: final epidemic size. r > 0: infection rate. a = (k - c_0) / c
Processed documentation: k > 0: final epidemic size.
===========================================================================================
Cleaned documentation: The test set has an ID column, however they're already in order. So we'll dispose of it quickly.
Processed documentation: The test set has an ID column, however they're already in order.
===========================================================================================
Cleaned documentation: Post processing of each predicted mask, components with lesser number of pixels. than `min_size` are ignored.
Processed documentation: Post processing of each predicted mask, components with lesser number of pixels.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: There's definiteiy more to explore here. Maybe these ideas can improve your model. Good luck
Processed documentation: Maybe these ideas can improve your model.
===========================================================================================
Cleaned documentation: Clusters can be of different sizes with model-based clustering. Here's the distribution of cities per cluster and the scatterplot.
Processed documentation: Clusters can be of different sizes with model-based clustering.
===========================================================================================
Cleaned documentation: Here are the geometric centers of all the clusters. The big red dot is the North Pole.
Processed documentation: The big red dot is the North Pole.
===========================================================================================
Cleaned documentation: Here's what the new path looks like. The points are all on cluster borders which makes sense.
Processed documentation: The points are all on cluster borders which makes sense.
===========================================================================================
Cleaned documentation: Here's the architecture for the CNN. It's fairly simple compared to what else you can do.
Processed documentation: Here's the architecture for the CNN.
===========================================================================================
Cleaned documentation: We'll start with the non-json columns first. Note that the bar plots only include the top 50 categories.
Processed documentation: Note that the bar plots only include the top 50 categories.
===========================================================================================
Cleaned documentation: There they are! Let's bring back the single, unconnected Questions to get our complete universe of Question Sets.
Processed documentation: Let's bring back the single, unconnected Questions to get our complete universe of Question Sets.
===========================================================================================
Cleaned documentation: The dendrogram view shows how missing values are related across columns by using hierarchical clustering. Pretty cool
Processed documentation: The dendrogram view shows how missing values are related across columns by using hierarchical clustering.
===========================================================================================
Cleaned documentation: Page views Number of pages visited in a single session. There are 100 missing values
Processed documentation: Page views Number of pages visited in a single session.
===========================================================================================
Cleaned documentation: The hidden cell above define the data type for each column. Now let's have a look at the data
Processed documentation: The hidden cell above define the data type for each column.
===========================================================================================
Cleaned documentation: In this graph i make the matrix sparse by considering only things that have perfect correlation. This
Processed documentation: In this graph i make the matrix sparse by considering only things that have perfect correlation.
===========================================================================================
Cleaned documentation: missing together. ie they have a corralation of. We also get the count. it stands to reason that
Processed documentation: it stands to reason that
===========================================================================================
Cleaned documentation: reflective of a non structural issue. This is actually very enlightening and we see there are
Processed documentation: reflective of a non structural issue.
===========================================================================================
Cleaned documentation: some odd pairs that satisfy this criteria. More importantly is that it happens for lots of tickers.
Processed documentation: some odd pairs that satisfy this criteria.
===========================================================================================
Cleaned documentation: lets see how many unique counts there are. it looks like a few of these counts happen multiple times.
Processed documentation: it looks like a few of these counts happen multiple times.
===========================================================================================
Cleaned documentation: finds core groups. eps is the distance cut off and min is how many elements
Processed documentation: eps is the distance cut off and min is how many elements
===========================================================================================
Cleaned documentation: clusters we want to use. This is another way for us to cluster. Here we use
Processed documentation: clusters we want to use.
===========================================================================================
Cleaned documentation: I chose this number pretty much at random, you can change it. using 22 features to describe
Processed documentation: I chose this number pretty much at random, you can change it.
===========================================================================================
Cleaned documentation: Let's try different simple thresholding methods. Description of threshold types can be found [here]( and [here
Processed documentation: Let's try different simple thresholding methods.
===========================================================================================
Cleaned documentation: reload and return the retrained model for the given fold. and make last layer identity.
Processed documentation: reload and return the retrained model for the given fold.
===========================================================================================
Cleaned documentation: Devices 3032, 3543, 3866, total cut outs in time. (Seems to cut off bottom of image in the notebook...
Processed documentation: Devices 3032, 3543, 3866, total cut outs in time.
===========================================================================================
Cleaned documentation: No count features in this version, the kernel runs out of memory. Add some simple extra features
Processed documentation: No count features in this version, the kernel runs out of memory.
===========================================================================================
Cleaned documentation: The max values of electricity are caused by only 7 buildings. Practically, two of them.
Processed documentation: The max values of electricity are caused by only 7 buildings.
===========================================================================================
Cleaned documentation: but value 126 starts appearing more about that time... perhaps they have a similar meaning
Processed documentation: but value 126 starts appearing more about that time...
===========================================================================================
Cleaned documentation: The "D" columns refer to time, as stated by organizers. D9 being 0.75 means 6pm-7pm for example
Processed documentation: The "D" columns refer to time, as stated by organizers.
===========================================================================================
Cleaned documentation: Seems like we have a few outliers. Let's visualize the data and see if we can spot the outliers.
Processed documentation: Let's visualize the data and see if we can spot the outliers.
===========================================================================================
Cleaned documentation: One way could be by using the median or mean. However, this is not accurate.
Processed documentation: One way could be by using the median or mean.
===========================================================================================
Cleaned documentation: Group the data by the "onpromotion" field. Make a count of the values and reset the index.
Processed documentation: Make a count of the values and reset the index.
===========================================================================================
Cleaned documentation: Group the data by the "perishable" field. Make a count of the values and reset the index.
Processed documentation: Make a count of the values and reset the index.
===========================================================================================
Cleaned documentation: Fit the text and transform it into a vector. This will return a sparse matrix.
Processed documentation: Fit the text and transform it into a vector.
===========================================================================================
Cleaned documentation: Fit the text and transform it into a vector. This will return a sparse matrix.
Processed documentation: Fit the text and transform it into a vector.
===========================================================================================
Cleaned documentation: Split the vectorized data. Here we pass the vectorized values and the author column.
Processed documentation: Here we pass the vectorized values and the author column.
===========================================================================================
Cleaned documentation: Finally, convert it to csv. Index=True tells pandas not to include the index as a column
Processed documentation: Index=True tells pandas not to include the index as a column
===========================================================================================
Cleaned documentation: but not efficient on large generic data structures due to the use of pickle & mp.Queue.
Processed documentation: but not efficient on large generic data structures due to the use of pickle & mp.
===========================================================================================
Cleaned documentation: Size of the smallest side of the image during testing. Set to zero to disable resize in testing.
Processed documentation: Size of the smallest side of the image during testing.
===========================================================================================
Cleaned documentation: Perform KS-Test for each feature from train/test. Draw its distribution. Count features based on statistics.
Processed documentation: Perform KS-Test for each feature from train/test.
===========================================================================================
Cleaned documentation: Plots are hidden. If you'd like to look at them - press "Output" button.
Processed documentation: If you'd like to look at them - press "Output" button.
===========================================================================================
Cleaned documentation: but not efficient on large generic data structures due to the use of pickle & mp.Queue.
Processed documentation: but not efficient on large generic data structures due to the use of pickle & mp.
===========================================================================================
Cleaned documentation: Rarely products have condition 4 or 5. Most of them are between 1 and 3 condition.
Processed documentation: Rarely products have condition 4 or 5.
===========================================================================================
Cleaned documentation: Now, move on to the brand_name part. I belive there are tons of unique values in brand_name
Processed documentation: I belive there are tons of unique values in brand_name
===========================================================================================
Cleaned documentation: Here, after splitting, the third category is pretty specific. The mean price is much higher than the second category.
Processed documentation: The mean price is much higher than the second category.
===========================================================================================
Cleaned documentation: Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.
Processed documentation: The values can vary depending on the loss function and should be tuned.
===========================================================================================
Cleaned documentation: of the training data. Both values need to be set for bagging to be used.
Processed documentation: Both values need to be set for bagging to be used.
===========================================================================================
Cleaned documentation: The average is less than zero. Some values are extreme( relative to others ).
Processed documentation: Some values are extreme( relative to others ).
===========================================================================================
Cleaned documentation: It's another way to plot our data. using a variable that contains the plot parameters
Processed documentation: using a variable that contains the plot parameters
===========================================================================================
Cleaned documentation: This function helps to investigate the proportion of visits and total of transction revenue. by each category.
Processed documentation: This function helps to investigate the proportion of visits and total of transction revenue.
===========================================================================================
Cleaned documentation: We can see a clear difference in the data. Almost all data is from the same device type.
Processed documentation: We can see a clear difference in the data.
===========================================================================================
Cleaned documentation: I will keep only the frequency encoding to evaluate the results. Feel free to change if you would
Processed documentation: I will keep only the frequency encoding to evaluate the results.
===========================================================================================
Cleaned documentation: Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.
Processed documentation: The values can vary depending on the loss function and should be tuned.
===========================================================================================
Cleaned documentation: of the training data. Both values need to be set for bagging to be used.
Processed documentation: Both values need to be set for bagging to be used.
===========================================================================================
Cleaned documentation: Nice, now we have the PCA features... Let's see the ratio of explanation of the first two Principal Components
Processed documentation: Let's see the ratio of explanation of the first two Principal Components
===========================================================================================
Cleaned documentation: The following code is design for the Kaggle Kernel. DO NOT COPY IT ON YOUR COMPUTER.
Processed documentation: The following code is design for the Kaggle Kernel.
===========================================================================================
Cleaned documentation: The following code is design for the Kaggle Kernel. DO NOT COPY IT ON YOUR COMPUTER.
Processed documentation: The following code is design for the Kaggle Kernel.
===========================================================================================
Cleaned documentation: The function below create a batch from a dataset. We use batch to train our model.
Processed documentation: The function below create a batch from a dataset.
===========================================================================================
Cleaned documentation: We split the train database with the function train_test_split (very useful and easy to use function).
Processed documentation: We split the train database with the function train_test_split
===========================================================================================
Cleaned documentation: Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.
Processed documentation: The values can vary depending on the loss function and should be tuned.
===========================================================================================
Cleaned documentation: of the training data. Both values need to be set for bagging to be used.
Processed documentation: Both values need to be set for bagging to be used.
===========================================================================================
Cleaned documentation: Create submission_file, df_any_{0, 1} means _any's label is {0, 1} else labels are 0.
Processed documentation: Create submission_file, df_any_{0, 1} means _
===========================================================================================
Cleaned documentation: Input LB socre: submission_{epidural, intraparenchymal, intraventricular, subarachnoid, subdural, any}_{0, 1}.csv, 'lb_score_ = - XX.XXX + ...' is LB SCORE
Processed documentation: Input LB socre: submission_{epidural, intraparenchymal, intraventricular, subarachnoid, subdural, any}_{0, 1}.csv, 'lb_score_
===========================================================================================
Cleaned documentation: out1 = L.Dense(128, activation='relu')(truncated) (None, 68, 5) the number 15 becomes kind of a hyperparameter here
Processed documentation: L.Dense(128, activation='relu')(truncated) (None, 68, 5) the number 15 becomes kind of a hyperparameter here
===========================================================================================
Cleaned documentation: This is left for user to be defined. Catalyst will take care of the rest.
Processed documentation: This is left for user to be defined.
===========================================================================================
Cleaned documentation: Simplified version of the same class by HuggingFace. See transformers/modeling_distilbert.py in the transformers repository.
Processed documentation: See transformers/modeling_distilbert.py in the transformers repository.
===========================================================================================
Cleaned documentation: Args: pretrained_model_name (str): HuggingFace model name. See transformers/modeling_auto.py. num_classes (int): the number of class labels. in the classification task.
Processed documentation: Args: pretrained_model_name (str): HuggingFace model name.
===========================================================================================
Cleaned documentation: here we specify that we pass masks to the runner. So model's forward method will be called with
Processed documentation: here we specify that we pass masks to the runner.
===========================================================================================
Cleaned documentation: As an entertainment, we can build a wordcloud for news titles. However, no useful insights from such a picture.
Processed documentation: As an entertainment, we can build a wordcloud for news titles.
===========================================================================================
Cleaned documentation: Here is one for the fork's. I do recomend you trying to play with these metrics though...
Processed documentation: I do recomend you trying to play with these metrics though...
===========================================================================================
Cleaned documentation: As an entertainment, we can build a wordcloud for reviews. However, no useful insights from such pictures.
Processed documentation: As an entertainment, we can build a wordcloud for reviews.
===========================================================================================
Cleaned documentation: Does this operation make sense? Maybe not. Let's try to train logisitic regression with this feature transformation.
Processed documentation: Let's try to train logisitic regression with this feature transformation.
===========================================================================================
Cleaned documentation: So all the nans here are either adults or children before school age. We can input 0 again.
Processed documentation: So all the nans here are either adults or children before school age.
===========================================================================================
Cleaned documentation: Someone commented in the discussions that the same household can have different target values. Let's look at it.
Processed documentation: Someone commented in the discussions that the same household can have different target values.
===========================================================================================
Cleaned documentation: Does this operation make sense? Maybe not. Let's try to train logisitic regression with this feature transformation.
Processed documentation: Let's try to train logisitic regression with this feature transformation.
===========================================================================================
Cleaned documentation: full_train_df.loc[(full_train_df['primary_use'] == le.transform(['Education'])[0]) & (full_train_df['month'] >= 6) & (full_train_df['month'] <= 8), 'is_vacation_month'] = np.int
Processed documentation: full_train_df.loc[(full_train_df['primary_use'] == le.transform(['Education'])[0]) & (full_train_df['month'] >= 6) & (full_train_df['month']
===========================================================================================
Cleaned documentation: full_test_df.loc[(full_test_df['primary_use'] == le.transform(['Education'])[0]) & (full_test_df['month'] >= 6) & (full_test_df['month'] <= 8), 'is_vacation_month'] = np.int
Processed documentation: == le.transform(['Education'])[0]) & (full_test_df['month']
===========================================================================================
Cleaned documentation: Save the input value. You'll need this later to add back to the main path.
Processed documentation: You'll need this later to add back to the main path.
===========================================================================================
Cleaned documentation: Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased
Processed documentation: Learning rate will be half after 3 epochs if accuracy is not increased
===========================================================================================
Cleaned documentation: Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased
Processed documentation: Learning rate will be half after 3 epochs if accuracy is not increased
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased
Processed documentation: Learning rate will be half after 3 epochs if accuracy is not increased
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: There are some buggy annonations in training images having huge bounding boxes. Let's remove those bboxes
Processed documentation: There are some buggy annonations in training images having huge bounding boxes.
===========================================================================================
Cleaned documentation: Let's define augmentations using albumentations library. We will define a single verticle flip with probability 1 for re-producibility.
Processed documentation: We will define a single verticle flip with probability 1 for re-producibility.
===========================================================================================
Cleaned documentation: Custom Cutout augmentation with handling of bounding boxes. Note: (only supports square cutout regions) Author: Kaushal28 Reference
Processed documentation: Custom Cutout augmentation with handling of bounding boxes.
===========================================================================================
Cleaned documentation: Applies the cutout augmentation on the given image. param image: The image to be augmented. returns augmented image.
Processed documentation: Applies the cutout augmentation on the given image.
===========================================================================================
Cleaned documentation: Below code uses all above augmentations except Mixup. Refer next section for only Mixup
Processed documentation: Below code uses all above augmentations except Mixup.
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased
Processed documentation: Learning rate will be half after 3 epochs if accuracy is not increased
===========================================================================================
Cleaned documentation: This looks promising. The only thing remaining is to discard the 'white' images. Let's do it.
Processed documentation: The only thing remaining is to discard the 'white' images.
===========================================================================================
Cleaned documentation: For Data Augmentation. I will mix train wav, and same length(1 sec) noise(10%) from '_background_noise
Processed documentation: I will mix train wav, and same length(1 sec) noise(10%) from '_background_noise
===========================================================================================
Cleaned documentation: resizes to IMAGE_SIZE/IMAGE_SIZE while keeping aspect ratio the same. pads on right/bottom as appropriate
Processed documentation: resizes to IMAGE_SIZE/IMAGE_SIZE while keeping aspect ratio the same.
===========================================================================================
Cleaned documentation: Randomize the samples from TRAIN_DIR and TEST_DIR. Split the TRAIN_DIR samples for a train/validation split.
Processed documentation: Split the TRAIN_DIR samples for a train/validation split.
===========================================================================================
Cleaned documentation: Convert gradients if it's tf.IndexedSlices. When computing gradients for operation concerning `tf.gather`, the type of gradients.
Processed documentation: When computing gradients for operation concerning `tf.gather`, the type of gradients.
===========================================================================================
Cleaned documentation: and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
Processed documentation: A workaround is to add a very small positive number ε to the sum.
===========================================================================================
Cleaned documentation: Below: Code to create barplot, with analysis of categorical data. Click "Code" to reveal.
Processed documentation: Below: Code to create barplot, with analysis of categorical data.
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Args: image (PIL Image): Image to draw insects on. Returns: PIL Image: Image with drawn insects.
Processed documentation: Args: image (PIL Image): Image to draw insects on.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: convert a str or list of strings to unique string. used for naming each of 42840 series.
Processed documentation: convert a str or list of strings to unique string.
===========================================================================================
Cleaned documentation: convert a str or list of strings to unique string. used for naming each of 42840 series.
Processed documentation: convert a str or list of strings to unique string.
===========================================================================================
Cleaned documentation: For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code
Processed documentation: For an fast model/feature evaluation, get only 10% of dataset.
===========================================================================================
Cleaned documentation: Income of the client. There are a huge number at the right of the plot (1.170000e
Processed documentation: There are a huge number at the right of the plot (1.170000e
===========================================================================================
Cleaned documentation: For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code
Processed documentation: For an fast model/feature evaluation, get only 10% of dataset.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Load the preprocessed dataset. See the demo notebook for sample code for performing this preprocessing.
Processed documentation: See the demo notebook for sample code for performing this preprocessing.
===========================================================================================
Cleaned documentation: Set up the pipeline for the given dataset. Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.
Processed documentation: Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.
===========================================================================================
Cleaned documentation: Get the tokenizer corresponding to our multilingual BERT model. See [TensorFlow Hub]( for more information about the model.
Processed documentation: Get the tokenizer corresponding to our multilingual BERT model.
===========================================================================================
Cleaned documentation: Helper function to prepare data for BERT. Converts sentence input examples. into the form ['input_word_ids', 'input_mask', 'segment_ids'].
Processed documentation: Helper function to prepare data for BERT.
===========================================================================================
Cleaned documentation: The mask has 1 for real tokens and 0 for padding tokens. Only real
Processed documentation: The mask has 1 for real tokens and 0 for padding tokens.
===========================================================================================
Cleaned documentation: Original setting in paper is. Set to 300 in here to save training time
Processed documentation: Set to 300 in here to save training time
===========================================================================================
Cleaned documentation: gta[bbox_num, 0] = (40 (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map
Processed documentation: (x in feature map
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: I'd like to show some graph. You can see the obvious relationship between 'angle' and 'scalar_coupling_constant'.
Processed documentation: You can see the obvious relationship between 'angle' and 'scalar_coupling_constant'.
===========================================================================================
Cleaned documentation: First of all, let's convert the input data into the labels for CenterNet. CenterNet用に入力データを形成しておきます
Processed documentation: First of all, let's convert the input data into the labels for CenterNet.
===========================================================================================
Cleaned documentation: Let's create CNN model(almost ResNet). Input: Image (resized into 512x512x Output: Ratio of letter_size to picture_size ResNetっぽいモデルで試してみます。入力は1ページごとの画像。出力は文字とピクチャのサイズ比とします
Processed documentation: Input: Image (resized into 512x512x Output: Ratio of letter_size to picture_size ResNetっぽいモデルで試してみます。入力は1ページごとの画像。出力は文字とピクチャのサイズ比とします
===========================================================================================
Cleaned documentation: Umm... not so good. Training is not enough. But I might as well use it.
Processed documentation: Umm... not so good.
===========================================================================================
Cleaned documentation: Start training. This kernel runs 30 epochs. Longer training can improve the accuracy. epochほど計算しますが、多分もっと長い方がよいです
Processed documentation: Longer training can improve the accuracy.
===========================================================================================
Cleaned documentation: Let's check a result of heatmap with validation data. You can see the centers of letters are detected.
Processed documentation: Let's check a result of heatmap with validation data.
===========================================================================================
Cleaned documentation: I like to use numbers instead of names for labels. Lets add the numbers as labels to the dataframe
Processed documentation: I like to use numbers instead of names for labels.
===========================================================================================
Cleaned documentation: About $30\%$ headlines' length are over A little have zero length. This implies that there are empty headlines.
Processed documentation: About $30\%$ headlines' length are over A little have zero length.
===========================================================================================
Cleaned documentation: As we can from above, we now have 2 features after applying t-SNE. To visualize t-SNE, we use Bokeh
Processed documentation: To visualize t-SNE, we use Bokeh
===========================================================================================
Cleaned documentation: Sklearn transformer to convert texts to indices list. e.g. the cute cat"], ["the dog"]] -> [[1, 2, 3], [1,
Processed documentation: Sklearn transformer to convert texts to indices list.
===========================================================================================
Cleaned documentation: First. Check the contents of the data. image data and landmark_id are written in train.csv train.csvには画像データとlandmark_idについて書いてある
Processed documentation: image data and landmark_id are written in train.csv train.csvには画像データとlandmark_idについて書いてある
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: The first output is the sequenced output. One 768 sized tensor for each of the 12 tokens.
Processed documentation: The first output is the sequenced output.
===========================================================================================
Cleaned documentation: Now let's take a look on a behavior of the losses. First part of The Splitted Loss looks expectedly
Processed documentation: Now let's take a look on a behavior of the losses.
===========================================================================================
Cleaned documentation: Sample 10,000 games from recent seasons. Record the expected wins and use this to calculate the logloss.
Processed documentation: Record the expected wins and use this to calculate the logloss.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: First we will construct the loss functions. We will construct the loss function according to the Keras documentation
Processed documentation: We will construct the loss function according to the Keras documentation
===========================================================================================
Cleaned documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset. Overrides values in the base Config class.
Processed documentation: Configuration for training pneumonia detection on the RSNA pneumonia dataset.
===========================================================================================
Cleaned documentation: Train on 1 GPU and 8 images per GPU. We can put multiple images on each
Processed documentation: Train on 1 GPU and 8 images per GPU.
===========================================================================================
Cleaned documentation: min,max) = (1,2) will mean you will have unigrams and bigrms in your vocabulary.
Processed documentation: = (1,2) will mean you will have unigrams and bigrms in your vocabulary.
===========================================================================================
Cleaned documentation: train_df['title'].values.tolist() this converts all the values in the title column into a list. appends two lists
Processed documentation: train_df['title'].values.tolist() this converts all the values in the title column into a list.
===========================================================================================
Cleaned documentation: split the train into development and validation sample. Take the last 100K rows as validation sample.
Processed documentation: split the train into development and validation sample.
===========================================================================================
Cleaned documentation: train_df['title'].values.tolist() this converts all the values in the title column into a list. appends two lists
Processed documentation: train_df['title'].values.tolist() this converts all the values in the title column into a list.
===========================================================================================
Cleaned documentation: split the train into development and validation sample. Take the last 100K rows as validation sample.
Processed documentation: split the train into development and validation sample.
===========================================================================================
Cleaned documentation: Let's separate input variables and target variable. Have also created a features list with all input variable names.
Processed documentation: Let's separate input variables and target variable.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: if numpy_labels.dtype == object: binary string in this case, these are image ID strings
Processed documentation: object: binary string in this case, these are image ID strings
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: If anyone has the depth knowledge on infer_datetime_format. Please, share it in a comment section.
Processed documentation: If anyone has the depth knowledge on infer_datetime_format.
===========================================================================================
Cleaned documentation: clf_2 = xgb.XGBClassifier(random_state=42,n_jobs=-1,verbosity=1) params = {"max_depth":[3,4,5,6,7,8,9], "n_estimators":list(range(50,500,50)), "learning_rate":[0.01,0.05,0.1,0.3], "subsample":[0.5,0.6,0.7,0.8,0.9], "colsample_bytree":[0.5,0.6,0.7,0.8,0.9], "reg_alpha":[0.5,1,2,5,10], "reg_lambda":[0.5,1,2,5,10]} random_search_2 = RandomizedSearchCV(estimator=clf_2,param_distributions=params,cv=10,scoring='roc_auc') random_search_2.fit(X_Train,y_Train
Processed documentation: {"max_depth":[3,4,5,6,7,8,9], "n_estimators":list(range(50,500,50)), "learning_rate":[0.01,0.05,0.1,0.3], "subsample":[0.5,0.6,0.7,0.8,0.9], "colsample_bytree":[0.5,0.6,0.7,0.8,0.9], "reg_alpha":[0.5,1,2,5,10], "reg_lambda":[0.5,1,2,5,10]} random_search_2 =
===========================================================================================
Cleaned documentation: Our generator will return next structure: ([RGB_images, grayscale_images], labels). As described above, now we will have two inputs
Processed documentation: Our generator will return next structure: ([RGB_images, grayscale_images], labels).
===========================================================================================
Cleaned documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns. Let's glimpse train and test dataset.
Processed documentation: Both train and test data have 200,000 entries and 202, respectivelly 201 columns.
===========================================================================================
Cleaned documentation: sum has perfect correaltion wth mean. So, I'd like to delete sum instead of mean.
Processed documentation: sum has perfect correaltion wth mean.
===========================================================================================
Cleaned documentation: I just put the most highly correlated feature in the table here. (either positive or negative correlated
Processed documentation: I just put the most highly correlated feature in the table here.
===========================================================================================
Cleaned documentation: The punctuations have been removed from the string. Lets write a similar function for removing
Processed documentation: Lets write a similar function for removing
===========================================================================================
Cleaned documentation: This seems to be fine. Now we can apply above functions on our dataframe. Lets check our dataframe
Processed documentation: Now we can apply above functions on our dataframe.
===========================================================================================
Cleaned documentation: Now we have processed and pre-processed text in our dataframe. Lets start making features from
Processed documentation: Now we have processed and pre-processed text in our dataframe.
===========================================================================================
Cleaned documentation: The usage of stop words can be another writing pattern. So the fourth feature is count of stopwords.
Processed documentation: The usage of stop words can be another writing pattern.
===========================================================================================
Cleaned documentation: Let's plot these features on a chart. To view these feature we will write a function
Processed documentation: Let's plot these features on a chart.
===========================================================================================
Cleaned documentation: The above features are common features which can indicate a writing pattern. There might be a possibility
Processed documentation: The above features are common features which can indicate a writing pattern.
===========================================================================================
Cleaned documentation: that a writer is using words which START WITH or END WITH particular characters. Lets try to identify them.
Processed documentation: that a writer is using words which START WITH or END WITH particular characters.
===========================================================================================
Cleaned documentation: The character locations will be stored in char_locs. These are in the form of x,y,width,height.
Processed documentation: These are in the form of x,y,width,height.
===========================================================================================
Cleaned documentation: We actually can see different parts of set with chenged constant stress. a classic laboratory earthquake model
Processed documentation: We actually can see different parts of set with chenged constant stress.
===========================================================================================
Cleaned documentation: After fix wrong flip score chenged from 0.684 to 0.696. Its still be only example
Processed documentation: After fix wrong flip score chenged from 0.684 to 0.696.
===========================================================================================
Cleaned documentation: We named this parameter `mask_fraction`. We use supercategories to compare how `mask_fraction` distribution differs from supercategory to supercategory
Processed documentation: We use supercategories to compare how `mask_fraction` distribution differs from supercategory to supercategory
===========================================================================================
Cleaned documentation: Simple dataset. Only save path to image and load it and transform to tensor when call __getitem__.
Processed documentation: Only save path to image and load it and transform to tensor when call __getitem__.
===========================================================================================
Cleaned documentation: Let's set `na` params to a blank string value. This way they'll be included in `groupby`.
Processed documentation: Let's set `na` params to a blank string value.
===========================================================================================
Cleaned documentation: The peaks in the FFT curve indicate strong periodic structure at that frequency. Let's zoom-in the see the numbers.
Processed documentation: The peaks in the FFT curve indicate strong periodic structure at that frequency.
===========================================================================================
Cleaned documentation: one token at a time. This makes more sense than truncating an equal percent
Processed documentation: This makes more sense than truncating an equal percent
===========================================================================================
Cleaned documentation: The images are actually quite big. We will resize to a much smaller size.
Processed documentation: We will resize to a much smaller size.
===========================================================================================
Cleaned documentation: We use the second scenario. Let's have a look at our Catalyst config file
Processed documentation: Let's have a look at our Catalyst config file
===========================================================================================
Cleaned documentation: We have seen a short demonstration of MLComp/Catalyst execution process. You can find inference here
Processed documentation: We have seen a short demonstration of MLComp/Catalyst execution process.
===========================================================================================
Cleaned documentation: There are four types of festivals: National, Religious, Sporting and Cultural. Below is a calendar view all the events.
Processed documentation: There are four types of festivals: National, Religious, Sporting and Cultural.
===========================================================================================
Cleaned documentation: Confirmed cases: Trend was changed on 16Mar2020, but this change is not significant. We will use all data.
Processed documentation: Confirmed cases: Trend was changed on 16Mar2020, but this change is not significant.
===========================================================================================
Cleaned documentation: Calculate measurable variables using the variables of the model. This function should be overwritten. df @return
Processed documentation: Calculate measurable variables using the variables of the model.
===========================================================================================
Cleaned documentation: Calculate 1/beta [day] etc. This function should be overwritten. param tau : tau value [hour
Processed documentation: param tau : tau value [hour
===========================================================================================
Cleaned documentation: Predict the values in the future. step_n : the number of steps. return : predicted data for measurable variables.
Processed documentation: return : predicted data for measurable variables.
===========================================================================================
Cleaned documentation: Return Estimater information. return : - : model. name, total_population, start_time, tau. values of parameters of model.
Processed documentation: values of parameters of model.
===========================================================================================
Cleaned documentation: The EDA and feature engineering for the dict columns seems similar. Create a few basic common utilities
Processed documentation: The EDA and feature engineering for the dict columns seems similar.
===========================================================================================
Cleaned documentation: A few movies don't have runtime. Get their imdb_id to look them up on imdb.
Processed documentation: Get their imdb_id to look them up on imdb.
===========================================================================================
Cleaned documentation: Most of them have values in imdb. Found one more online.Filling the remaining ones with the column average
Processed documentation: Filling the remaining ones with the column average
===========================================================================================
Cleaned documentation: Both cast and crew have lots of entries with gender == 0 (i.e. unknown).
Processed documentation: Both cast and crew have lots of entries with gender =
===========================================================================================
Cleaned documentation: The crew column is different than others as it has much more properties. Let's look at jobs
Processed documentation: The crew column is different than others as it has much more properties.
===========================================================================================
Cleaned documentation: Drop all columns which haven't been handled yet. We'll gradually add them back later on
Processed documentation: Drop all columns which haven't been handled yet.
===========================================================================================
Cleaned documentation: cast 2.3714 2.40004 v14 - no improvement. Maybe it means that we are overfitting with so many columns.
Processed documentation: cast 2.3714 2.40004 v14 - no improvement.
===========================================================================================
Cleaned documentation: Sometimes train auc also valuable for dealing with overfitting. We can re-calculate train_auc after training, predicting, writing....
Processed documentation: Sometimes train auc also valuable for dealing with overfitting.
===========================================================================================
Cleaned documentation: The class probability predictions need to be converted to a surface text prediction. The code below does exactly that.
Processed documentation: The class probability predictions need to be converted to a surface text prediction.
===========================================================================================
Cleaned documentation: This does have enough ground underneath to be partly cloudy. The ground seems to be primary.
Processed documentation: This does have enough ground underneath to be partly cloudy.
===========================================================================================
Cleaned documentation: Create a list of statistics that we can log transform. We will not log transform skew or kurtosis.
Processed documentation: Create a list of statistics that we can log transform.
===========================================================================================
Cleaned documentation: For now, we will finish by considering n_max. Let's pull every image 3IQR past Q1 and Q3.
Processed documentation: Let's pull every image 3IQR past Q1 and Q3.
===========================================================================================
Cleaned documentation: We'll work on buildings (class 1) from image 6120_2_2. Fist load grid sizes and polygons.
Processed documentation: We'll work on buildings (class 1) from image 6120_2_2.
===========================================================================================
Cleaned documentation: Next is the most interesting bit, creating polygons from bit masks. Please see inline comments
Processed documentation: Next is the most interesting bit, creating polygons from bit masks.
===========================================================================================
Cleaned documentation: We notice that for neutral sentiment, the selected text is nearly always the text itself. Let's check it
Processed documentation: We notice that for neutral sentiment, the selected text is nearly always the text itself.
===========================================================================================
Cleaned documentation: returns list of pos neg and objective score. But returns empty list if not present in senti wordnet.
Processed documentation: returns list of pos neg and objective score.
===========================================================================================
Cleaned documentation: This is very promising. Using this, I will probably be able to find symmetry.
Processed documentation: Using this, I will probably be able to find symmetry.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: X_dnn = pd.read_csv("/kaggle/input/categorical-feature-encoding-with-tensorflow/oof.csv") Xt_dnn = pd.read_csv("/kaggle/input/categorical-feature-encoding-with-tensorflow/dnn_cv_submission.csv") X['dnn_preds'] = X_dnn.dnn_oof. Xt['dnn_preds'] = Xt_dnn.target. del((X_dnn, Xt_dnn
Processed documentation: pd.read_csv("/kaggle/input/categorical-feature-encoding-with-tensorflow/oof.csv") Xt_dnn =
===========================================================================================
Cleaned documentation: Returns a batch from X, y. random_state allows determinism. different scikit-learn CV strategies are possible.
Processed documentation: Returns a batch from X, y. random_state allows determinism.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: This seems to be a highly skewed target variable. Let's take the log of it to check the distribution.
Processed documentation: Let's take the log of it to check the distribution.
===========================================================================================
Cleaned documentation: much neater now! Max number of passengers are. Which makes sense is the cab is an SUV
Processed documentation: Which makes sense is the cab is an SUV
===========================================================================================
Cleaned documentation: key and pickup_datetime seem to be datetime columns which are in object format. Let's convert them to datetime
Processed documentation: key and pickup_datetime seem to be datetime columns which are in object format.
===========================================================================================
Cleaned documentation: keep these. Since the fare_amount is not <2.5 (which is the base fare), these values seem legit to me.
Processed documentation: Since the fare_amount is not <2.5 (which is the base fare), these values seem legit to me.
===========================================================================================
Cleaned documentation: keep these too. Since the fare_amount is not <2.5, these values seem legit to me.
Processed documentation: Since the fare_amount is not <2.5, these values seem legit to me.
===========================================================================================
Cleaned documentation: SCENARIO Fare is not 0, but Distance is 0. These values need to be imputed.
Processed documentation: SCENARIO Fare is not 0, but Distance is 0.
===========================================================================================
Cleaned documentation: These 27159 rows need to be imputed using the following formula distance = (fare_amount - 2.5)/1.
Processed documentation: These 27159 rows need to be imputed using the following formula distance =
===========================================================================================
Cleaned documentation: Here we will merge both datasets. This causes that we diverge from the flow that we load.
Processed documentation: This causes that we diverge from the flow that we load.
===========================================================================================
Cleaned documentation: The ID and the target should not be scaled. Therefore, we update the `ignore_columns` variable as follows
Processed documentation: Therefore, we update the `ignore_columns` variable as follows
===========================================================================================
Cleaned documentation: I guess "No dscription present won't affact these features ... So, I am not removing them.
Processed documentation: I guess "No dscription present won't affact these features ...
===========================================================================================
Cleaned documentation: There are four continuous features along with `FVC` in tabular data. Those features are
Processed documentation: There are four continuous features along with `FVC` in tabular data.
===========================================================================================
Cleaned documentation: Unfreeze the encoder and do fine-tuning. We do the finetuning until the model starts to recognize class `2`.
Processed documentation: We do the finetuning until the model starts to recognize class `2`.
===========================================================================================
Cleaned documentation: Returns a plot of the original Image and Encoded ones. n: number of images to display.
Processed documentation: Returns a plot of the original Image and Encoded ones.
===========================================================================================
Cleaned documentation: extend()`: Extend list by appending elements from the iterable. The code below does the following
Processed documentation: extend()`: Extend list by appending elements from the iterable.
===========================================================================================
Cleaned documentation: Alaska2 Dataset. If data is test or eval, it skips the transformations applied to training part.
Processed documentation: If data is test or eval, it skips the transformations applied to training part.
===========================================================================================
Cleaned documentation: for key, item in self(keys): self[key] = self.__apply__(item, func) return self. def contiguous(self, keys): r.
Processed documentation: for key, item in self(keys): self[key] = self.__apply__(item, func) return self.
===========================================================================================
Cleaned documentation: attributes. return self.apply(lambda x: x.to(device, kwargs), keys) def to(self, device, non_blocking, keys, kwargs): r.
Processed documentation: return self.apply(lambda x: x.to(device, kwargs), keys) def to(self, device, non_blocking, keys, kwargs): r.
===========================================================================================
Cleaned documentation: ARC sample to Geometric Graph. Every pixel becomes a node and it has edges to its neighbor pixels.
Processed documentation: Every pixel becomes a node and it has edges to its neighbor pixels.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: we need to predict log revenue per customer. Lets group by full visitor id
Processed documentation: we need to predict log revenue per customer.
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: What didn't work Rounding. Metrics is a correlation Codeblocks tokenizing (replacing entire blocks with a token Removing columns separator
Processed documentation: Metrics is a correlation Codeblocks tokenizing (replacing entire blocks with a token Removing columns separator
===========================================================================================
Cleaned documentation: cv2.imwrite( pathOut + "\\frame%d.jpg" % count, image) Next I will save frame as JPEG
Processed documentation: cv2.imwrite( pathOut + "\\frame%d.jpg" % count, image)
===========================================================================================
Cleaned documentation: Now let's look at whether tickers change over time. Is either `assetCode` or `assetName` unique
Processed documentation: Now let's look at whether tickers change over time.
===========================================================================================
Cleaned documentation: Calculate the 50Hz Fourier coefficient of a signal. Assumes the signal is 800000 data points long and covering 20ms.
Processed documentation: Calculate the 50Hz Fourier coefficient of a signal.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: Convert data to a list of tuples. Each tuple contains the picture filename and a list of coordinates.
Processed documentation: Each tuple contains the picture filename and a list of coordinates.
===========================================================================================
Cleaned documentation: The coordinates represent points on the fluke edge. The extremum values can be used to construct a bounding box.
Processed documentation: The coordinates represent points on the fluke edge.
===========================================================================================
Cleaned documentation: Divide the signal in bags of "signal_size. Normalize the data dividing it by 15.
Processed documentation: Divide the signal in bags of "signal_size.
===========================================================================================
Cleaned documentation: Generate a Kaggle submission file. param threshold the score given to 'new_whale' @param filename the submission file name.
Processed documentation: param threshold the score given to 'new_whale' @param filename the submission file name.
===========================================================================================
Cleaned documentation: image = pydicom.read_file(os.path.join(train_images_dir,'ID_'+images[im]+ '.dcm')).pixel_array. i = im // width. j = im % width. axs[i,j].imshow(image, cmap=plt.cm.bone) axs[i,j].axis('off
Processed documentation: im // width.
===========================================================================================
Cleaned documentation: The one with the larger value of diff-std is the final candidate. Ignore diff-std = 4.5 or less.
Processed documentation: The one with the larger value of diff-std is the final candidate.
===========================================================================================
Cleaned documentation: Let's make pairplots. We can clearly see correlation with winPlacePerc (but maybe only with weaponsAcquired it's difficult to see
Processed documentation: We can clearly see correlation with winPlacePerc (but maybe only with weaponsAcquired it's difficult to see
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Cross-validation and adjustment of model hyperparameters. Creation of new features and description of this process
Processed documentation: Cross-validation and adjustment of model hyperparameters.
===========================================================================================
Cleaned documentation: get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT
Processed documentation: Convert them to lower case, since we're using the uncased version of BERT
===========================================================================================
Cleaned documentation: For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT
Processed documentation: For each word, find the offset not counting spaces.
===========================================================================================
Cleaned documentation: We want predictions for all development rows. So instead of removing rows, make them
Processed documentation: We want predictions for all development rows.
===========================================================================================
Cleaned documentation: The only missing value is a single value in `winPlacePerc`. Let's take a look at it.
Processed documentation: The only missing value is a single value in `winPlacePerc`.
===========================================================================================
Cleaned documentation: We already have `dtr` instantiated as our Decision Tree Regressor. Now it's time to train it.
Processed documentation: We already have `dtr` instantiated as our Decision Tree Regressor.
===========================================================================================
Cleaned documentation: point to have. Our data is sparse, so we chose a small value, 10.
Processed documentation: Our data is sparse, so we chose a small value, 10.
===========================================================================================
Cleaned documentation: Specify what columns will be used as features. This is for easy filtering of the datasets later.
Processed documentation: Specify what columns will be used as features.
===========================================================================================
Cleaned documentation: Reset internal data-dependent state of the scaler, if necessary. init__ parameters are not touched.
Processed documentation: Reset internal data-dependent state of the scaler, if necessary.
===========================================================================================
Cleaned documentation: There is an example which has 413 age. I guess he will be a wizard.
Processed documentation: There is an example which has 413 age.
===========================================================================================
Cleaned documentation: Some records in curated train data have 2, 3, 4, 6 labels. labels in curated train data
Processed documentation: Some records in curated train data have 2, 3, 4, 6 labels.
===========================================================================================
Cleaned documentation: Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings
Processed documentation: Remove rows with NAs except for submission rows.
===========================================================================================
Cleaned documentation: Now timing. Retrieving top 100 samples from the index images for each query image.
Processed documentation: Retrieving top 100 samples from the index images for each query image.
===========================================================================================
Cleaned documentation: Helper method handling downloading large files from `url` to `filename. Returns a pointer to `filename`.
Processed documentation: Helper method handling downloading large files from `url` to `filename.
===========================================================================================
Cleaned documentation: That's it! Use your own model and load the data from the dataset. Happy last minute training and fine-tuning
Processed documentation: Use your own model and load the data from the dataset.
===========================================================================================
Cleaned documentation: Using XGBoost to obtain high-quality features. I run 2 XGBoost models to obtain better features.
Processed documentation: Using XGBoost to obtain high-quality features.
===========================================================================================
Cleaned documentation: The code below is rounding the results. If the rank is equal to 34.2, it was rounded to 34.
Processed documentation: The code below is rounding the results.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: pretrained model in my pc. now i will train on all images for 2 epochs
Processed documentation: pretrained model in my pc.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: It seems that there is a corrleation of 0.1 between depth and saltPercentage. Let's dig deeper.
Processed documentation: It seems that there is a corrleation of 0.1 between depth and saltPercentage.
===========================================================================================
Cleaned documentation: I think the way we perform split is important. Just performing random split mayn't make sense for two reasons
Processed documentation: I think the way we perform split is important.
===========================================================================================
Cleaned documentation: pretrained model in my pc. now i will train on all images for 2 epochs
Processed documentation: pretrained model in my pc.
===========================================================================================
Cleaned documentation: Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces
Processed documentation: Disable multiprocesing for numpy/opencv.
===========================================================================================
Cleaned documentation: De-normalizes database to create reverse indices for common cases. Args: verbose: Whether to print outputs.
Processed documentation: De-normalizes database to create reverse indices for common cases.
===========================================================================================
Cleaned documentation: Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces
Processed documentation: Disable multiprocesing for numpy/opencv.
===========================================================================================
Cleaned documentation: De-normalizes database to create reverse indices for common cases. Args: verbose: Whether to print outputs.
Processed documentation: De-normalizes database to create reverse indices for common cases.
===========================================================================================
Cleaned documentation: We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.
Processed documentation: We quantize to uint8 here to conserve memory.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: The training data. In this kernel, I'll use the `question_title`, `question_body` and `answer` columns.
Processed documentation: In this kernel, I'll use the `question_title`, `question_body` and `answer` columns.
===========================================================================================
Cleaned documentation: PyMC3 uses the now semi-orphaned theano for the backend. PyMC4 will use tf-probability but that's not happening anytime soon.
Processed documentation: PyMC3 uses the now semi-orphaned theano for the backend.
===========================================================================================
Cleaned documentation: There's a bit of numpy bookkeeping to learn for working with posterior samples. The `np.apply_along_axis` function comes in handy.
Processed documentation: There's a bit of numpy bookkeeping to learn for working with posterior samples.
===========================================================================================
Cleaned documentation: Save the model and model weights. These files will going to output folder as expected. You can download them.
Processed documentation: Save the model and model weights.
===========================================================================================
Cleaned documentation: It is a 222x222 feature map with 32 channels. Below we can visualize some of the different channels
Processed documentation: It is a 222x222 feature map with 32 channels.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Hitting End Of Dataset exceptions is a problem in this setup. Using a repeated validation set instead.
Processed documentation: Hitting End Of Dataset exceptions is a problem in this setup.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Load the preprocessed dataset. See the demo notebook for sample code for performing this preprocessing.
Processed documentation: See the demo notebook for sample code for performing this preprocessing.
===========================================================================================
Cleaned documentation: Set up the pipeline for the given dataset. Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.
Processed documentation: Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.
===========================================================================================
Cleaned documentation: Figure 7: Spectra of different kinds of supernovae, with their atomic signatures labeled, Credit: Daniel Kasen (UC Berkeley).
Processed documentation: Spectra of different kinds of supernovae, with their atomic signatures labeled, Credit: Daniel Kasen (UC Berkeley).
===========================================================================================
Cleaned documentation: The list of features available with packages like `cesium` is huge. We'll only compute a subset now.
Processed documentation: The list of features available with packages like `cesium` is huge.
===========================================================================================
Cleaned documentation: Get accuracy of model on validation data. It's not AUC but it's something at least
Processed documentation: Get accuracy of model on validation data.
===========================================================================================
Cleaned documentation: Now compare noisy (left) and denoised test images (right). Our model has done great job with denoising
Processed documentation: Now compare noisy (left) and denoised test images (right).
===========================================================================================
Cleaned documentation: The experiment count in the train and test set is different. We will look into this in more detail.
Processed documentation: The experiment count in the train and test set is different.
===========================================================================================
Cleaned documentation: Time for a model... I'm fiddling with both a hand-built version and some pre-trained models such as ResNet50.
Processed documentation: I'm fiddling with both a hand-built version and some pre-trained models such as ResNet50.
===========================================================================================
Cleaned documentation: How does our days distributed like? Lets apply @ghostskipper's visualization ( to out solution.
Processed documentation: Lets apply @ghostskipper's visualization ( to out solution.
===========================================================================================
Cleaned documentation: Note you need to recompile the whole thing. Otherwise you are not traing first layers
Processed documentation: Note you need to recompile the whole thing.
===========================================================================================
Cleaned documentation: Then, train the classifier using this sample. Notice that data preprocessing is included into the training procedure.
Processed documentation: Notice that data preprocessing is included into the training procedure.
===========================================================================================
Cleaned documentation: Hmm ... This algorithm detected 7 failures, which are visualized in the graphs above.
Processed documentation: This algorithm detected 7 failures, which are visualized in the graphs above.
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed.
Processed documentation: Arguments: seed {int} -- Number of the seed.
===========================================================================================
Cleaned documentation: Initialize these variables which will be set in this if statement. Each of these
Processed documentation: Initialize these variables which will be set in this if statement.
===========================================================================================
Cleaned documentation: Within this function you can set .requires_grad = False for various parameters, if you don't want to learn them
Processed documentation: False for various parameters, if you don't want to learn them
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to draw hairs on. Returns: PIL Image: Image with drawn hairs.
Processed documentation: Args: img (PIL Image): Image to draw hairs on.
===========================================================================================
Cleaned documentation: then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants
Processed documentation: then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev.
===========================================================================================
Cleaned documentation: test_var = pd.read_csv('/kaggle/input/msk-redefining-cancer-treatment/test_variants.zip') test_text =pd.read_csv("/kaggle/input/msk-redefining-cancer-treatment/test_text.zip",sep="\|\|",engine="python",names=["ID","TEXT"],skiprows=1) test = pd.merge(test_var, test_text, how = 'left', on = 'ID').fillna('') test.head
Processed documentation: test_text =pd.read_csv("/kaggle/input/msk-redefining-cancer-treatment/test_text.zip",sep="\|\|",engine="python",names=["ID","TEXT"],skiprows=1) test =
===========================================================================================
Cleaned documentation: Right, let's look at some bird song recordings. Will resample all recordings to mono, 22050Hz.
Processed documentation: Right, let's look at some bird song recordings.
===========================================================================================
Cleaned documentation: We can clearly see the calls. Let's zoom in on a single call from the two species.
Processed documentation: Let's zoom in on a single call from the two species.
===========================================================================================
Cleaned documentation: A clear difference! For comparison, let's look at the spectrograms of the same signals.
Processed documentation: For comparison, let's look at the spectrograms of the same signals.
===========================================================================================
Cleaned documentation: Look at the samples above. Looks like quite a few users have no idea what an octagon looks like
Processed documentation: Looks like quite a few users have no idea what an octagon looks like
===========================================================================================
Cleaned documentation: Same as before, we have three columns of categorical data. We now convert them into one-hot encoding.
Processed documentation: Same as before, we have three columns of categorical data.
===========================================================================================
Cleaned documentation: this is the simplified original loss function by Olivier. It works excellently as an
Processed documentation: this is the simplified original loss function by Olivier.
===========================================================================================
Cleaned documentation: One more thing... We have to figure out if the same procedure can work well on non-DDF objects.
Processed documentation: We have to figure out if the same procedure can work well on non-DDF objects.
===========================================================================================
Cleaned documentation: Now that we have got our guns lock and loaded, it's time to shoot. lets begin the modelling process.
Processed documentation: Now that we have got our guns lock and loaded, it's time to shoot.
===========================================================================================
Cleaned documentation: Grid search gives best accuracy for max_depth=8,min_child_weight=6,gamma=0.4,colsample_bytree=0.6,subsample=0. Training the model again with these new parameters.
Processed documentation: Grid search gives best accuracy for max_depth=8,min_child_weight=6,gamma=0.4,colsample_bytree=0.6,subsample=0.
===========================================================================================
Cleaned documentation: So, let's add these rotated features to the other five targets. And plot the correlation matrix once more.
Processed documentation: So, let's add these rotated features to the other five targets.
===========================================================================================
Cleaned documentation: Data Collection I start Collection Data by the training and testing datasets into Pandas DataFrames. Go to top](top
Processed documentation: Data Collection I start Collection Data by the training and testing datasets into Pandas DataFrames.
===========================================================================================
Cleaned documentation: read the whole dataset into pd.DataFrame in memory and store into a single file
Processed documentation: DataFrame in memory and store into a single file
===========================================================================================
Cleaned documentation: Let's parametrise `Aspect` feature, which looks like a cosine function. We will not use it, but it is fun
Processed documentation: Let's parametrise `Aspect` feature, which looks like a cosine function.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Pad the sentences. We need to pad the sequence with 0's to achieve consistent length across examples.
Processed documentation: We need to pad the sequence with 0's to achieve consistent length across examples.
===========================================================================================
Cleaned documentation: We had train_X = [[1,2,4,3],[1,2,5,6,3]] lets say maxlen=6 We will then get. train_X = [[1,2,4,3,0,0],[1,2,5,6,3,
Processed documentation: We had train_X =
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Doesn't look to be correlated with ID. This is probably filler information from the camera capture.
Processed documentation: This is probably filler information from the camera capture.
===========================================================================================
Cleaned documentation: This is relatively small for Wasserstein Distances. Let's see if this is anything significant based on our train-test split.
Processed documentation: Let's see if this is anything significant based on our train-test split.
===========================================================================================
Cleaned documentation: defining a function that calculates the exponential weighted value of a series. alpha is the smoothing factor.
Processed documentation: defining a function that calculates the exponential weighted value of a series.
===========================================================================================
Cleaned documentation: Let's create new features based on name and area. We use [`LabelEncoder`]( class of preprocessing in python.
Processed documentation: Let's create new features based on name and area.
===========================================================================================
Cleaned documentation: img: numpy array, 1 -> mask, 0 -> background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 -> mask, 0 -> background.
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: n_estimators = np.linspace(start = 600 , stop = 1000, num = 8, dtype= int
Processed documentation: 600 , stop = 1000, num = 8, dtype= int
===========================================================================================
Cleaned documentation: Let's try randomized pca to ignore outliers. Pick the 100 most descriptive features for rpca.
Processed documentation: Let's try randomized pca to ignore outliers.
===========================================================================================
Cleaned documentation: Functions to load images. We keep only RGB channels as Alpha channel is always empty.
Processed documentation: We keep only RGB channels as Alpha channel is always empty.
===========================================================================================
Cleaned documentation: Run KMeans on each image. Centroids provide dominant colors (based on provided colorspace). More details [here
Processed documentation: Centroids provide dominant colors (based on provided colorspace).
===========================================================================================
Cleaned documentation: If we train a CNN, we will have to define a width/height. or 320x320 looks good.
Processed documentation: If we train a CNN, we will have to define a width/height.
===========================================================================================
Cleaned documentation: Wow, didn't expect it to be that low. Let's specify columns which we want to ignore
Processed documentation: Let's specify columns which we want to ignore
===========================================================================================
Cleaned documentation: How to get total frames with DALI? Is is required to avoid crash when asking frames beyond the end.
Processed documentation: Is is required to avoid crash when asking frames beyond the end.
===========================================================================================
Cleaned documentation: Image www2.deloitte.com - Women in Data Science and Analytics. Developing the next generation of data leaders
Processed documentation: Image www2.deloitte.com - Women in Data Science and Analytics.
===========================================================================================
Cleaned documentation: So out of all our features, we are given 260 float variables. What about the cardinality of our features
Processed documentation: So out of all our features, we are given 260 float variables.
===========================================================================================
Cleaned documentation: df: dataframe containing sequences and the features. col: column to apply encoding. valid values are: 'sequence', 'structure' and 'predicted_loop_type
Processed documentation: df: dataframe containing sequences and the features.
===========================================================================================
Cleaned documentation: bpps_arr = [] for i in tqdm(range(len(df))): idx = df.loc[i]['id'] bpps_arr.append(np.expand_dims(np.load(os.path.join(bbps_dir, str(idx)+'.npy')), axis=-1)) cnn_inp = np.array(bpps_arr) cnn data input.
Processed documentation: [] for i in tqdm(range(len(df))): idx = df.loc[i]['id'] bpps_arr.append(np.expand_dims(np.load(os.path.join(bbps_dir, str(idx)+'.npy')), axis=-1))
===========================================================================================
Cleaned documentation: Consists of four models, one seq_model each for sequence, structure and predicted_loop. and one CNN for BPPS files.
Processed documentation: Consists of four models, one seq_model each for sequence, structure and predicted_loop.
===========================================================================================
Cleaned documentation: Next new features are wrong. I will try to use this idea in the next version
Processed documentation: Next new features are wrong.
===========================================================================================
Cleaned documentation: I prepared several initial models. So far I used only Random Forest and XGB.
Processed documentation: I prepared several initial models.
===========================================================================================
Cleaned documentation: Given a text, cleans and normalizes it. Feel free to add your own stuff.
Processed documentation: Given a text, cleans and normalizes it.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: only over short distances. Even a 20km trip should not take 5 hours unless you drive super duper slow
Processed documentation: Even a 20km trip should not take 5 hours unless you drive super duper slow
===========================================================================================
Cleaned documentation: Create our OOF train and test predictions. These base results will be used as new features
Processed documentation: Create our OOF train and test predictions.
===========================================================================================
Cleaned documentation: get_y` needs to return the coordinates then the label. Let's look at an example quickly
Processed documentation: get_y` needs to return the coordinates then the label.
===========================================================================================
Cleaned documentation: Now let's start building our ground truth data. We'll want an initial array to add to
Processed documentation: Now let's start building our ground truth data.
===========================================================================================
Cleaned documentation: Let's build our `TabularDataLoader`. We'll want to define our `procs`, `cat` and `cont` names
Processed documentation: We'll want to define our `procs`, `cat` and `cont` names
===========================================================================================
Cleaned documentation: And that's all there is to it. How do we check? Let's check the `DataLoader`'s index's
Processed documentation: Let's check the `DataLoader`'s index's
===========================================================================================
Cleaned documentation: Just for example. In reality for this competition you would want to use: `submission['row_id'] = test['row_id
Processed documentation: In reality for this competition you would want to use: `submission['row_id'] = test['row_id
===========================================================================================
Cleaned documentation: It should be just as simple as replacing `df_np[1]` with `row`. Let's try it out
Processed documentation: It should be just as simple as replacing `df_np[1]` with `row`.
===========================================================================================
Cleaned documentation: We can see that `Normalize` lives here. To get those statistics we simply grab the `mean` and `std
Processed documentation: To get those statistics we simply grab the `mean` and `std
===========================================================================================
Cleaned documentation: To use our fold, we'll also want our images. This is as simple as
Processed documentation: To use our fold, we'll also want our images.
===========================================================================================
Cleaned documentation: param inputs: input tensor. param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
Processed documentation: param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
===========================================================================================
Cleaned documentation: This also looks fine. It gives us some mappings from the hit IDs to positions and detector IDs.
Processed documentation: It gives us some mappings from the hit IDs to positions and detector IDs.
===========================================================================================
Cleaned documentation: Things still persist as we select on high energy. High energies are dominated by forward-going tracks.
Processed documentation: Things still persist as we select on high energy.
===========================================================================================
Cleaned documentation: create an axes on the right side of ax. The width of cax will be
Processed documentation: create an axes on the right side of ax.
===========================================================================================
Cleaned documentation: Now comes the good and easy part. Let's install apex first which will make our training faster.
Processed documentation: Let's install apex first which will make our training faster.
===========================================================================================
Cleaned documentation: git clone && cd apex && pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext. user && cd .. rm -rf apex
Processed documentation: git clone && cd apex && pip install -v
===========================================================================================
Cleaned documentation: python ./train.py --img 1024 --batch 2 --epochs 1 --data "../input/configyolo5v1/configYolo5v1/wheat0.yaml" --cfg "../input/yolov5yaml/yolov5xx.yaml" --name yolov5x_fold0 --weights yolov5x.pt
Processed documentation: python ./train.py --img 1024 --batch 2 --epochs 1 --data "../input/configyolo5v1/configYolo5v1/wheat0.yaml" --cfg "../input/yolov5yaml/yolov5xx.yaml" --name yolov5x_fold0 --weights
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: author__ : Najeeb Khan, Yasir Mir, Zafarullah Mahmood team__ : artificial_stuPiDity institution__ : Jamia Millia Islamia email__ : najeeb.khan96@gmail.com
Processed documentation: author__ : Najeeb Khan, Yasir Mir, Zafarullah Mahmood team__ : artificial_stuPiDity institution__ :
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: data_for_prediction = mytrain[all_features].iloc[row_to_show] use 1 row of data here. Could use multiple rows if desired
Processed documentation: data_for_prediction = mytrain[all_features].iloc[row_to_show] use 1 row of data here.
===========================================================================================
Cleaned documentation: There is one missing data in the text field. There are 27486 unique records here and 3 sentiments.
Processed documentation: There is one missing data in the text field.
===========================================================================================
Cleaned documentation: Again this is a standard code from the prev competitions. To look into the distribution of ngrams
Processed documentation: Again this is a standard code from the prev competitions.
===========================================================================================
Cleaned documentation: The transformed data is in a numpy matrix. This may be inconvenient if we want to further
Processed documentation: The transformed data is in a numpy matrix.
===========================================================================================
Cleaned documentation: process the data, and have a more visual impression of what each column is etc. We therefore
Processed documentation: process the data, and have a more visual impression of what each column is etc.
===========================================================================================
Cleaned documentation: The transformed data is in a numpy matrix. This may be inconvenient and we therefore
Processed documentation: The transformed data is in a numpy matrix.
===========================================================================================
Cleaned documentation: Takes a scalar and returns a string with. the css property `'background-color: red'` for negative. strings, black otherwise.
Processed documentation: the css property `'background-color: red'` for negative.
===========================================================================================
Cleaned documentation: accept a user-supplied topic number and. print out a formatted list of the top terms.
Processed documentation: accept a user-supplied topic number and.
===========================================================================================
Cleaned documentation: look up the topn most similar terms to token. and print them as a formatted list.
Processed documentation: look up the topn most similar terms to token.
===========================================================================================
Cleaned documentation: Any violet pixels. Actually, in grayscale images (fluorescent and brightfield) each pixel has equal values for 3 channels.
Processed documentation: Actually, in grayscale images (fluorescent and brightfield) each pixel has equal values for 3 channels.
===========================================================================================
Cleaned documentation: Taken from Konstantin Lopuhin. in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s
Processed documentation: in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s
===========================================================================================
Cleaned documentation: Iterative Tuning My current learning rate is 0.005. Say I what to see how well it's neigbors perform..
Processed documentation: Iterative Tuning My current learning rate is 0.005.
===========================================================================================
Cleaned documentation: This dendrogram clusters the missing occurence tendencies together. EXT_SOURCE_1 and OWN_CAR_AGE are close. EXT_SOURCE_3 and AMT_REQ_CREDIT bundle aswell.
Processed documentation: This dendrogram clusters the missing occurence tendencies together.
===========================================================================================
Cleaned documentation: Iterative Tuning My current learning rate is 0.05. Say I what to see how well it's neigbors perform..
Processed documentation: Iterative Tuning My current learning rate is 0.05.
===========================================================================================
Cleaned documentation: To me, topic 1 seems the most interesting since it suggest some kind of existential theme. Mary Shelley
Processed documentation: To me, topic 1 seems the most interesting since it suggest some kind of existential theme.
===========================================================================================
Cleaned documentation: df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) apply stemming to each row
Processed documentation: df_in['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) apply stemming to each row
===========================================================================================
Cleaned documentation: GIVES WARNING AS LIBROSA IS NOT CAPABLE OF READING '.mp3' FILES AND FALLS BACK TO 'audioread
Processed documentation: GIVES WARNING AS LIBROSA IS NOT CAPABLE OF READING '.mp3'
===========================================================================================
Cleaned documentation: Looks like 20 Zero crossings to me. Lets check it out by the library function provided in librosa.
Processed documentation: Lets check it out by the library function provided in librosa.
===========================================================================================
Cleaned documentation: fill missing values in full_sq, life_sq, kitch_sq, num_room. use apartment strategy to fill the most likely values
Processed documentation: fill missing values in full_sq, life_sq, kitch_sq, num_room.
===========================================================================================
Cleaned documentation: Lookup some words and their nearest contextual siblings in embedding vector space. Seems to make a lot of sense
Processed documentation: Lookup some words and their nearest contextual siblings in embedding vector space.
===========================================================================================
Cleaned documentation: Stay connected for more parameter tunning updates. Glad to hear Comment from you guys... 🙂 Happy Kaggling
Processed documentation: Glad to hear Comment from you guys... 🙂 Happy Kaggling
===========================================================================================
Cleaned documentation: AC Type - Mostly 1's, which corresponds to central AC. Reasonable to assume most other properties are similar.
Processed documentation: AC Type - Mostly 1's, which corresponds to central AC.
===========================================================================================
Cleaned documentation: Takes a batched dataset of (image, label) Tensors and returns separate arrays. of images and labels.
Processed documentation: Takes a batched dataset of (image, label) Tensors and returns separate arrays.
===========================================================================================
Cleaned documentation: Plots the top 10 highest prediction confidences along with. the truth label for sample n.
Processed documentation: Plots the top 10 highest prediction confidences along with.
===========================================================================================
Cleaned documentation: Let's now add some non-image features. We can start with sex, and one-hot encode it.
Processed documentation: Let's now add some non-image features.
===========================================================================================
Cleaned documentation: final_pred_stacked = ((final_pred_lgb + final_pred_xgb) / 2).astype(int) submission_stacked = pd.DataFrame({'Id': test_id, 'Target': final_pred_stacked}) submission_stacked.to_csv('submissionStacked.csv', index=False
Processed documentation: DataFrame({'Id': test_id, 'Target': final_pred_stacked}) submission_stacked.to_csv('submissionStacked.csv', index=False
===========================================================================================
Cleaned documentation: Let us check the last feature : `n_people`. It gives the number of people registered per family.
Processed documentation: It gives the number of people registered per family.
===========================================================================================
Cleaned documentation: These IDS raise gdcm import error. Couldn't figure it out yet. So decided to skip them for now.
Processed documentation: These IDS raise gdcm import error.
===========================================================================================
Cleaned documentation: By default dataset is not sorted by time. We want to train on the past data to predict future.
Processed documentation: We want to train on the past data to predict future.
===========================================================================================
Cleaned documentation: site_id site_id - Foreign key for the weather files. In my understanding this is a location.
Processed documentation: site_id site_id - Foreign key for the weather files.
===========================================================================================
Cleaned documentation: Air temperature by site_id. Looks like some sites are more to north and some more to south.
Processed documentation: Looks like some sites are more to north and some more to south.
===========================================================================================
Cleaned documentation: And a scatter plot with a "decision boundary" of the model. White 'X' marks represents a test set examples.
Processed documentation: White 'X' marks represents a test set examples.
===========================================================================================
Cleaned documentation: And the distribution is almost the same. Again doing label encoding and looking at the correlation.
Processed documentation: Again doing label encoding and looking at the correlation.
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to draw hairs on. Returns: PIL Image: Image with drawn hairs.
Processed documentation: Args: img (PIL Image): Image to draw hairs on.
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to draw hairs on. Returns: PIL Image: Image with drawn hairs.
Processed documentation: Args: img (PIL Image): Image to draw hairs on.
===========================================================================================
Cleaned documentation: Args: img (PIL Image): Image to apply transformation to. Returns: PIL Image: Image with transformation.
Processed documentation: Args: img (PIL Image): Image to apply transformation to.
===========================================================================================
Cleaned documentation: Establish cutoffs (load layers between 0 and cutoff. if cutoff = -1 all are loaded
Processed documentation: Establish cutoffs (load layers between 0 and cutoff.
===========================================================================================
Cleaned documentation: Performs Non-Maximum Suppression on inference results. Returns detections with shape: nx6 (x1, y1, x2, y2, conf, cls
Processed documentation: Returns detections with shape: nx6 (x1, y1, x2, y2, conf, cls
===========================================================================================
Cleaned documentation: python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml --weights ../input/yolov5/bestv4.pt
Processed documentation: python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/
===========================================================================================
Cleaned documentation: but not efficient on large generic data structures due to the use of pickle & mp.Queue.
Processed documentation: but not efficient on large generic data structures due to the use of pickle & mp.
===========================================================================================
Cleaned documentation: Add the data elements -- not trying to set all required here. Check DICOM
Processed documentation: Add the data elements -- not trying to set all required here.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings
Processed documentation: Remove rows with NAs except for submission rows.
===========================================================================================
Cleaned documentation: We have a total time of 10.6s secs spent reading the data. Not bad, but it can be improved.
Processed documentation: We have a total time of 10.6s secs spent reading the data.
===========================================================================================
Cleaned documentation: Tracking function for keep track of model parameters and. CV scores. integer` forces the value to be an int.
Processed documentation: Tracking function for keep track of model parameters and.
===========================================================================================
Cleaned documentation: To really prove whether it's a HMC, we must identify the autocorrelation. How will this help
Processed documentation: To really prove whether it's a HMC, we must identify the autocorrelation.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: From @randxie. Modified to work with scipy version 1.1.0 which does not have the fs parameter.
Processed documentation: Modified to work with scipy version 1.1.0 which does not have the fs parameter.
===========================================================================================
Cleaned documentation: Fault pattern usually exists in high frequency band. According to literature, the pattern is visible above 10^4 Hz.
Processed documentation: Fault pattern usually exists in high frequency band.
===========================================================================================
Cleaned documentation: Just using the Fastai model for now. You can replace this with Light model as well.
Processed documentation: You can replace this with Light model as well.
===========================================================================================
Cleaned documentation: These are the first 30 melanoma images with benign tumors. Let's check the distribution of values in `benign_malignant`.
Processed documentation: Let's check the distribution of values in `benign_malignant`.
===========================================================================================
Cleaned documentation: So we have a bell (Gaussian or normal distribution) of train data. What about test
Processed documentation: So we have a bell (Gaussian or normal distribution) of train data.
===========================================================================================
Cleaned documentation: Now we can train the model. (NOTE: I HAVE COMMENTED THIS BECAUSE IT TAKES A LOT OF TIME
Processed documentation: I HAVE COMMENTED THIS BECAUSE IT TAKES A LOT OF TIME
===========================================================================================
Cleaned documentation: time. STEP_SIZE_TRAIN = train_generator.n//64 STEP_SIZE_VALID = valid_generator.n//64 history = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_generator, validation_steps=STEP_SIZE_VALID, epochs=1, verbose=1).history.
Processed documentation: model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_generator, validation_steps=STEP_SIZE_VALID, epochs=1, verbose=1).history.
===========================================================================================
Cleaned documentation: Defining the Convolutional Neural Network This model has 4 convolution layers. This model has 3 fully connected layers.
Processed documentation: This model has 4 convolution layers.
===========================================================================================
Cleaned documentation: Run cross validation on model against tournaments in. For the tournaments since 2014, the winning score
Processed documentation: Run cross validation on model against tournaments in.
===========================================================================================
Cleaned documentation: First let's check the missing value. Let's hope there is not many of it.
Processed documentation: First let's check the missing value.
===========================================================================================
Cleaned documentation: Both of the longtitudes has one points that is pretty far off the center(median). I will simply remove them.
Processed documentation: Both of the longtitudes has one points that is pretty far off the center(median).
===========================================================================================
Cleaned documentation: PR - puerto rico and VI - virgin islands out of USA. us-states.json doesn't have information about these regions
Processed documentation: PR - puerto rico and VI - virgin islands out of USA.
===========================================================================================
Cleaned documentation: are under 12 months. But for better understanding, let's cap at 60 months and see their distribution
Processed documentation: But for better understanding, let's cap at 60 months and see their distribution
===========================================================================================
Cleaned documentation: we can see maximum 30 photos are uploaded for a pet. Let's plot all possible photo numbers
Processed documentation: Let's plot all possible photo numbers
===========================================================================================
Cleaned documentation: INTRODUCTION In this kernel, we will be working on Humpback Whale Identification Dataset (Implementing with Keras).
Processed documentation: In this kernel, we will be working on Humpback Whale Identification Dataset (Implementing with Keras).
===========================================================================================
Cleaned documentation: The dependent variable is skewed (as expected for dollars). The easiest would be to take a log tranformation
Processed documentation: The dependent variable is skewed (as expected for dollars).
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: A, 予測 = (A: 0.1, B: 0.7, 0.2)の場合 A: 3, B: 1, C Score = 1/3 = 0.33 となる
Processed documentation: A, 予測 = (A: 0.1, B: 0.7, 0.2)の場合
===========================================================================================
Cleaned documentation: You can see the folds are not stratified well. OK. Let's do the more sophisticated splitting.
Processed documentation: Let's do the more sophisticated splitting.
===========================================================================================
Cleaned documentation: df: dataframe to optimize folds. size: number of data to change fold. steps: number of for loop.
Processed documentation: size: number of data to change fold.
===========================================================================================
Cleaned documentation: There are some categorical features in X0-X8. Let's count the cardinality and label encode those.
Processed documentation: Let's count the cardinality and label encode those.
===========================================================================================
Cleaned documentation: I've heard there is an outlier in the target variable. Let's look and remove it.
Processed documentation: I've heard there is an outlier in the target variable.
===========================================================================================
Cleaned documentation: Random search gives its best results faster than bayesian search. Good for trying new things.
Processed documentation: Random search gives its best results faster than bayesian search.
===========================================================================================
Cleaned documentation: You can provide preprocessed fold data [(fold1_train, fold1_y_train), (fold2_train, fold2_y_train) ...] Use this for oversampling.
Processed documentation: [(fold1_train, fold1_y_train), (fold2_train, fold2_y_train) ...] Use this for oversampling.
===========================================================================================
Cleaned documentation: my_submission = pd.DataFrame({'ID_code': test_IDs, 'target': cv_results_df.iloc[0].predictions_total}) you could use any filename. We choose submission here. my_submission.to_csv('submission.csv', index=False
Processed documentation: DataFrame({'ID_code': test_IDs, 'target': cv_results_df.iloc[0].predictions_total}) you could use any filename.
===========================================================================================
Cleaned documentation: Let's import the required packages and call `h2o.init()`. The specified arguments (`nthreads` and `max_mem_size`) are optional.
Processed documentation: Let's import the required packages and call `h2o.init()`.
===========================================================================================
Cleaned documentation: We will not use the column `DepTime` anymore. Split the target column from the features columns in `train_df
Processed documentation: Split the target column from the features columns in `train_df
===========================================================================================
Cleaned documentation: Select the columns common to the train set and test set; convert `pd.DataFrame` to `H2OFrame
Processed documentation: Select the columns common to the train set and test set; convert `pd.
===========================================================================================
Cleaned documentation: Plot VII notes As expected, gmail is at the top. There is one interesting 'anonymous.com' domain
Processed documentation: Plot VII notes As expected, gmail is at the top.
===========================================================================================
Cleaned documentation: The `text` and `selected_text` column have one row missing each. Let's get rid of the missing rows.
Processed documentation: The `text` and `selected_text` column have one row missing each.
===========================================================================================
Cleaned documentation: This is a better representation. About 40 percent of the tweets are neutral followed by positive and negative tweets.
Processed documentation: About 40 percent of the tweets are neutral followed by positive and negative tweets.
===========================================================================================
Cleaned documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation. and remove words containing numbers.
Processed documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation.
===========================================================================================
Cleaned documentation: We can plot the audio array using librosa.display.waveplot. Let's plot the amplitude envelope of a waveform.
Processed documentation: Let's plot the amplitude envelope of a waveform.
===========================================================================================
Cleaned documentation: Gaussian Blur with opencv The Gaussian filter is a low-pass filter that removes the high-frequency components are reduced.
Processed documentation: The Gaussian filter is a low-pass filter that removes the high-frequency components are reduced.
===========================================================================================
Cleaned documentation: So I have inserted the desired result.ULet me know if the above code works for you
Processed documentation: ULet me know if the above code works for you
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: print('Epoch %5d / %5d. Validation loss = %.5f' % (epoch + 1, self.n_epochs, current_loss
Processed documentation: print('Epoch %5d / %5d.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Interesting observation 1. Seems like by this condition we can fix target = 0 with 100% accuracy
Processed documentation: Seems like by this condition we can fix target = 0
===========================================================================================
Cleaned documentation: It seems that there is a proportional convergence between meter_reading and CHWTON. Lets check it...
Processed documentation: It seems that there is a proportional convergence between meter_reading and CHWTON.
===========================================================================================
Cleaned documentation: Some points are far away from the boundaries of NYC. Let's limit the scope for visualization purposes, at least.
Processed documentation: Let's limit the scope for visualization purposes, at least.
===========================================================================================
Cleaned documentation: All done. Let's check how these variables correlates to each other. Correlation Between Features
Processed documentation: Let's check how these variables correlates to each other.
===========================================================================================
Cleaned documentation: Time for some modeling! I'll make use of sklearn in a first step to visualize feature importances.
Processed documentation: I'll make use of sklearn in a first step to visualize feature importances.
===========================================================================================
Cleaned documentation: If you don't do this, the model takes forever... it is much much faster on sparse data
Processed documentation: If you don't do this, the model takes forever...
===========================================================================================
Cleaned documentation: We now have a model with a CV score of 0.. Nice! Let's submit that
Processed documentation: We now have a model with a CV score of 0..
===========================================================================================
Cleaned documentation: Loại bỏ cột SK_ID_CURR đầu tiên do cột này là key. Khi cần lấy từ app_train và app_test sang
Processed documentation: Loại bỏ cột SK_ID_CURR đầu tiên do cột này là key.
===========================================================================================
Cleaned documentation: metres from Kremlin. What is it? Putin's cordyard, Lenin's Mausoleum? Let's take a look on prices.
Processed documentation: Putin's cordyard, Lenin's Mausoleum?
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: We are given training images for each of cervix types. Lets first count them for each class.
Processed documentation: We are given training images for each of cervix types.
===========================================================================================
Cleaned documentation: wrap the grouped data into dataframe, since the inner is pd.Series, not what we need
Processed documentation: wrap the grouped data into dataframe, since the inner is pd.
===========================================================================================
Cleaned documentation: Both seem to be very different kind of questions . Lets verify how many Mike's we have
Processed documentation: Both seem to be very different kind of questions .
===========================================================================================
Cleaned documentation: Do a train-valid split of the data to create dataset and dataloader. Specify random seed to get reproducibility
Processed documentation: Do a train-valid split of the data to create dataset and dataloader.
===========================================================================================
Cleaned documentation: Creating the train and valid dataset for training. Training data has the transform flag ON
Processed documentation: Creating the train and valid dataset for training.
===========================================================================================
Cleaned documentation: Lookahead Optimizer Wrapper. Implementation modified from. Paper: `Lookahead Optimizer: k steps forward, 1 step back
Processed documentation: Paper: `Lookahead Optimizer: k steps forward, 1 step back
===========================================================================================
Cleaned documentation: param inputs: input tensor. param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
Processed documentation: param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
===========================================================================================
Cleaned documentation: Now create the model. Since its greyscale , I have used pretrained model with modified weight.
Processed documentation: Since its greyscale , I have used pretrained model with modified weight.
===========================================================================================
Cleaned documentation: There is a small thing. I trained using effnetb4 offline for 20 epochs and loaded the weight
Processed documentation: I trained using effnetb4 offline for 20 epochs and loaded the weight
===========================================================================================
Cleaned documentation: I forgot to change the naming convension and it still reads effnetb. But this is actually effnetb
Processed documentation: I forgot to change the naming convension and it still reads effnetb.
===========================================================================================
Cleaned documentation: scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, 2e-4) This didnt give good result need to correct and get the right scheduler .
Processed documentation: This didnt give good result need to correct and get the right scheduler .
===========================================================================================
Cleaned documentation: scheduler.step() Want to test with fixed learning rate .If you want to use scheduler please uncomment this .
Processed documentation: scheduler.step() Want to test with fixed learning rate .If
===========================================================================================
Cleaned documentation: Do a train-valid split of the data to create dataset and dataloader. Specify random seed to get reproducibility
Processed documentation: Do a train-valid split of the data to create dataset and dataloader.
===========================================================================================
Cleaned documentation: Creating the train and valid dataset for training. Training data has the transform flag ON
Processed documentation: Creating the train and valid dataset for training.
===========================================================================================
Cleaned documentation: Lookahead Optimizer Wrapper. Implementation modified from. Paper: `Lookahead Optimizer: k steps forward, 1 step back
Processed documentation: Paper: `Lookahead Optimizer: k steps forward, 1 step back
===========================================================================================
Cleaned documentation: There is a small thing. I trained using effnetb4 offline for 20 epochs and loaded the weight
Processed documentation: I trained using effnetb4 offline for 20 epochs and loaded the weight
===========================================================================================
Cleaned documentation: I forgot to change the naming convension and it still reads effnetb. But this is actually effnetb
Processed documentation: I forgot to change the naming convension and it still reads effnetb.
===========================================================================================
Cleaned documentation: scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, 2e-4) This didnt give good result need to correct and get the right scheduler .
Processed documentation: This didnt give good result need to correct and get the right scheduler .
===========================================================================================
Cleaned documentation: scheduler.step() Want to test with fixed learning rate .If you want to use scheduler please uncomment this .
Processed documentation: scheduler.step() Want to test with fixed learning rate .If
===========================================================================================
Cleaned documentation: Configs for training & testing. Written by Whalechen. Modified By Nirjhar for Kaggle Kernel.
Processed documentation: Modified By Nirjhar for Kaggle Kernel.
===========================================================================================
Cleaned documentation: Let's visualize now the data. We select first a list of fake videos. Few fake videos
Processed documentation: We select first a list of fake videos.
===========================================================================================
Cleaned documentation: Let's try now the same for few of the images that are real. Few real videos
Processed documentation: Let's try now the same for few of the images that are real.
===========================================================================================
Cleaned documentation: Extracts all videos of a specified method and compression in the. FaceForensics++ file structure.
Processed documentation: Extracts all videos of a specified method and compression in the.
===========================================================================================
Cleaned documentation: aces12frames dataset contains the data from 00 set. I downloaded locally cropped using MTCNN and uploaded here
Processed documentation: aces12frames dataset contains the data from 00 set.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formatted.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Load frames from an MP4 video and detect faces. Arguments: filename {str} -- Path to video.
Processed documentation: Load frames from an MP4 video and detect faces.
===========================================================================================
Cleaned documentation: Load frames from an MP4 video and detect faces. Arguments: filename {str} -- Path to video.
Processed documentation: Load frames from an MP4 video and detect faces.
===========================================================================================
Cleaned documentation: Format: [[x1 y1 w1 h1], [x2 y2 w2 h2], [x3 y3 w3 h3], ...
Processed documentation: [[x1 y1 w1 h1], [x2 y2 w2 h2], [x3 y3 w3 h3], ...
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Create a feature extractor to extract features from the data. Returns the extracted features and the feature extractor.
Processed documentation: Create a feature extractor to extract features from the data.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: need to predict the order of places for groups within each match. train on group-level instead of the user-level
Processed documentation: train on group-level instead of the user-level
===========================================================================================
Cleaned documentation: We took about 20 NN models and calculated average. Here is one of the examples
Processed documentation: We took about 20 NN models and calculated average.
===========================================================================================
Cleaned documentation: This model's resid have few autocorrelation. It means that We were able to make a good model.
Processed documentation: It means that We were able to make a good model.
===========================================================================================
Cleaned documentation: The outlier is the sales in '2017-06-28'. Is the date an anniversary or something
Processed documentation: The outlier is the sales in '2017-06-28'.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Yikes! This is not what we wanted to achieve. Maybe we've overfit it? Let's look at the parameters
Processed documentation: Let's look at the parameters
===========================================================================================
Cleaned documentation: Trip distance specifies lower bound for journey time. Speed Let's calculate speed now. Broom Broom
Processed documentation: Trip distance specifies lower bound for journey time.
===========================================================================================
Cleaned documentation: Except Catboost all the other algorithms give the same rmse score. Hence I'm giving higher weightage to catboost results
Processed documentation: Hence I'm giving higher weightage to catboost results
===========================================================================================
Cleaned documentation: In train set, there are multiple rows for one 'Patient'. Because Patient has different weeks, FVC, Percent.
Processed documentation: In train set, there are multiple rows for one 'Patient'.
===========================================================================================
Cleaned documentation: The meta elements in dcom file seem to be slightly different for each file. Be careful
Processed documentation: The meta elements in dcom file seem to be slightly different for each file.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: We can see that > 99% of the observations show one value, 0. Therefore, this feature is almost constant.
Processed documentation: We can see that > 99% of the observations show one value, 0.
===========================================================================================
Cleaned documentation: We can see that `domain1_var1` distribution is approximately normal. So, we will fill the missing values with mean.
Processed documentation: So, we will fill the missing values with mean.
===========================================================================================
Cleaned documentation: g2 = sns.lmplot(x=feature1, y=feature2, hue="HasDetections", n_boot=1, data = train_sample, height=9, markers=["o", "x"], palette=["b", "r
Processed documentation: =feature1, y=feature2, hue="HasDetections", n_boot=1, data =
===========================================================================================
Cleaned documentation: Also, let's already drop the ID column which is insignificant for us. Also, let's seperate the target column.
Processed documentation: Also, let's already drop the ID column which is insignificant for us.
===========================================================================================
Cleaned documentation: There are very few cis-trans isomerism in dataset. So the score won't improve significantly.
Processed documentation: There are very few cis-trans isomerism in dataset.
===========================================================================================
Cleaned documentation: That does not look stationary. Now lets plot after taking the first differential of technical 20 by asset Id
Processed documentation: Now lets plot after taking the first differential of technical 20 by asset Id
===========================================================================================
Cleaned documentation: We have not taken into account accounting cost.So , submission score will be little high.
Processed documentation: We have not taken into account accounting cost.
===========================================================================================
Cleaned documentation: we see that after 14-15 iterations , score does not improve . Time to think something else.
Processed documentation: we see that after 14-15 iterations , score does not improve .
===========================================================================================
Cleaned documentation: File Structure The dataset has one file (`train.csv`) and three folders (`train`, `test`, `index`) in the root path.
Processed documentation: The dataset has one file (`train.csv`) and three folders (`train`, `test`, `index`) in the root path.
===========================================================================================
Cleaned documentation: This is way too expensive to run on `X_train`. So we'll run it on a sample of the data
Processed documentation: This is way too expensive to run on `X_train`.
===========================================================================================
Cleaned documentation: Electrical meters should never be zero. Keep all zero-readings in this table so that
Processed documentation: Keep all zero-readings in this table so that
===========================================================================================
Cleaned documentation: Please upvote this kernel if you like it. It motivates me to produce more quality content
Processed documentation: It motivates me to produce more quality content
===========================================================================================
Cleaned documentation: I will use a function from my [other]( kernel. Please check it out if you have some time.
Processed documentation: I will use a function from my [other]( kernel.
===========================================================================================
Cleaned documentation: Sorte the values by site_id and the original timestamp. This will avoid mixing values.
Processed documentation: Sorte the values by site_id and the original timestamp.
===========================================================================================
Cleaned documentation: Starting point. Let's see our missing values and fill the values for each site and mask YYYYMMDD
Processed documentation: Let's see our missing values and fill the values for each site and mask YYYYMMDD
===========================================================================================
Cleaned documentation: Let's look at the distribution of all points. Image is here just for reference.
Processed documentation: Let's look at the distribution of all points.
===========================================================================================
Cleaned documentation: mask_loss = mask (1 - pred_mask)2 torch.log(pred_mask + 1e-12) + (1 - mask) pred_mask2 torch.log(1 - pred_mask + 1e
Processed documentation: mask_loss = mask (1 - pred_mask)2 torch.log(pred_mask + 1e-12) + (1 - mask)
===========================================================================================
Cleaned documentation: Transaction revenue must related to goods. The keyword may have relation to goods. continue to analyse
Processed documentation: Transaction revenue must related to goods.
===========================================================================================
Cleaned documentation: Even though we don't apply transformations here, we set two empty lists to tfms. Train and Validation augmentations
Processed documentation: Even though we don't apply transformations here, we set two empty lists to tfms.
===========================================================================================
Cleaned documentation: We define a convnet learner object where we set the model architecture and our data bunch. create_cnn` docs.fast.ai
Processed documentation: We define a convnet learner object where we set the model architecture and our data bunch.
===========================================================================================
Cleaned documentation: Now, smaller learning rates. This time we define the min and max lr of the cycle
Processed documentation: This time we define the min and max lr of the cycle
===========================================================================================
Cleaned documentation: lets take a second look at the confusion matrix. See if how much we improved.
Processed documentation: lets take a second look at the confusion matrix.
===========================================================================================
Cleaned documentation: empty_data = ImageDataBunch.load_empty('./') this will look for a file named export.pkl in the specified path
Processed documentation: this will look for a file named export.pkl in the specified path
===========================================================================================
Cleaned documentation: Plots specified Original image and five augmentations. five images created transforming original with function 'transform
Processed documentation: five images created transforming original with function 'transform
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: Sorting only True Test data... Yes, you already know the [kernel One of the true winners: [YaG
Processed documentation: Sorting only True Test data...
===========================================================================================
Cleaned documentation: Function that adds frequency columns for each variable (excluding 'ID_code', 'target. New columns names will be like 'var_X_count'.
Processed documentation: Function that adds frequency columns for each variable (excluding 'ID_code', 'target.
===========================================================================================
Cleaned documentation: Thses boxes should be remove which almost can't be see anything in it.What's about area
Processed documentation: Thses boxes should be remove which almost can't be see anything in it.
===========================================================================================
Cleaned documentation: This finding is confirmed by checking the transactions of card_id 'C_ID_21117571cf'. There are no records for monthlag
Processed documentation: This finding is confirmed by checking the transactions of card_id 'C_ID_21117571cf'.
===========================================================================================
Cleaned documentation: Let's check whether similiar pattern exists in the test_transaction data. The same groups exist in test data also !!!.
Processed documentation: Let's check whether similiar pattern exists in the test_transaction data.
===========================================================================================
Cleaned documentation: There are 67 columns in this category. Let's look at how data is distributed in these columns
Processed documentation: Let's look at how data is distributed in these columns
===========================================================================================
Cleaned documentation: strategy = tf.distribute.get_strategy() default distribution strategy in Tensorflow. Works on CPU and single GPU.
Processed documentation: tf.distribute.get_strategy() default distribution strategy in Tensorflow.
===========================================================================================
Cleaned documentation: note that month affects season and that effects wheteher people take bike or not. like climate conditions rainy,hazy etc..
Processed documentation: note that month affects season and that effects wheteher people take bike or not.
===========================================================================================
Cleaned documentation: NOW RANDOM FORETS REGRESSOR GIVES THE LEAST RMSLE. HENCE WE USE IT TO MAKE PREDICTIONS ON KAGGLE.
Processed documentation: NOW RANDOM FORETS REGRESSOR GIVES THE LEAST RMSLE.
===========================================================================================
Cleaned documentation: shape of input. each document has 12 element or words which is the value of our maxlen variable.
Processed documentation: each document has 12 element or words which is the value of our maxlen variable.
===========================================================================================
Cleaned documentation: that same team if that team is in the "Lteam" column. Number of tournament wins is just,
Processed documentation: that same team if that team is in the "Lteam" column.
===========================================================================================
Cleaned documentation: Cart Balancer is the most easily solved assessment. Most the kids have not finised the Chest Sorter
Processed documentation: Most the kids have not finised the Chest Sorter
===========================================================================================
Cleaned documentation: fig = px.box(train_df, y="game_time_log",x = "type",color='month',title={'text': "Distribution of game_time by type based on month",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},
Processed documentation: "type",color='month',title={'text': "Distribution of game_time by type based on month",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},
===========================================================================================
Cleaned documentation: fig = px.box(train_df, y="game_time_log",x = "type",color='weekday_name',title={'text': "Distribution of game_time by type based on weekday",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},
Processed documentation: "type",color='weekday_name',title={'text': "Distribution of game_time by type based on weekday",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},
===========================================================================================
Cleaned documentation: fig = px.box(train_df, y="game_time_log",x = "type",color='world',title={'text': "Distribution of game_time by type based on world",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},
Processed documentation: "type",color='world',title={'text': "Distribution of game_time by type based on world",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},
===========================================================================================
Cleaned documentation: Study the pixel intensity. On average the red, green and blue channels have similar
Processed documentation: On average the red, green and blue channels have similar
===========================================================================================
Cleaned documentation: intensities for all images. It should be noted that the background can be dark
Processed documentation: It should be noted that the background can be dark
===========================================================================================
Cleaned documentation: Study how many objects in the masks can be identified. This is a limiting factor
Processed documentation: Study how many objects in the masks can be identified.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: The above code work only for 1-channel. Here is my simple extension for 3-channels image
Processed documentation: Here is my simple extension for 3-channels image
===========================================================================================
Cleaned documentation: Visualizing Audio We can plot the audio array using librosa.display.waveplot. Let's plot the amplitude envelope of a waveform.
Processed documentation: Visualizing Audio We can plot the audio array using librosa.display.waveplot.
===========================================================================================
Cleaned documentation: I trained 5-folds; these validation results are the out-of-fold results.By combining these, we can analyze the entire training set.
Processed documentation: I trained 5-folds; these validation results are the out-of-fold results.
===========================================================================================
Cleaned documentation: Let's merge both the dataset. Remember that ID was common column. So lets use it to merge.
Processed documentation: Let's merge both the dataset.
===========================================================================================
Cleaned documentation: It's very important to look for missing values. Else they create problem in final analysis
Processed documentation: It's very important to look for missing values.
===========================================================================================
Cleaned documentation: If all went well yEWM_26 should corrispond quite strongly to y(t-1). Lets test it out.
Processed documentation: If all went well yEWM_26 should corrispond quite strongly to y(t-1).
===========================================================================================
Cleaned documentation: Counts of the Weeks field. We will look at the top 20 weeks contained in the train.csv
Processed documentation: Counts of the Weeks field.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Decode a JPEG-encoded image to a uint8 tensor. You can also use tf.io.decode_jpeg but according to
Processed documentation: Decode a JPEG-encoded image to a uint8 tensor.
===========================================================================================
Cleaned documentation: only the training set (We evaluate using the entire test set anyway, right? So why shuffle?).
Processed documentation: only the training set (We evaluate using the entire test set anyway, right?
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: headshot rate = 100%" doesn''t look cheaters to me. They look good players and actually they won the game
Processed documentation: headshot rate = 100%" doesn''t look cheaters to me.
===========================================================================================
Cleaned documentation: Lucky or Cheater? My friend told me that 1km sniper shot is not impossible in this game.
Processed documentation: My friend told me that 1km sniper shot is not impossible in this game.
===========================================================================================
Cleaned documentation: This cell reduces the training data for Kaggle limits. Remove this cell for real results.
Processed documentation: This cell reduces the training data for Kaggle limits.
===========================================================================================
Cleaned documentation: Returns trained word2vec. Args: sentences: iterator for sentences. location (str): Path to save/load word2vec.
Processed documentation: location (str): Path to save/load word2vec.
===========================================================================================
Cleaned documentation: Start training the word2vec model. Since word2vec training is unsupervised, you can use both training and test datasets.
Processed documentation: Since word2vec training is unsupervised, you can use both training and test datasets.
===========================================================================================
Cleaned documentation: Doesn't make a difference either. Let's try one-hot encoding + SVD the "Gene" and "Variation" features
Processed documentation: Let's try one-hot encoding + SVD the "Gene" and "Variation" features
===========================================================================================
Cleaned documentation: We can see the score and the execution time. Next, let's see aggregated features.
Processed documentation: We can see the score and the execution time.
===========================================================================================
Cleaned documentation: I saw some ingredients which contain special characters, misspellings, ... I should probabily normalize ingredients.
Processed documentation: I saw some ingredients which contain special characters, misspellings, ...
===========================================================================================
Cleaned documentation: plt.subplot(2,1,2) plt.plot(df_news_small['time'], df_news_small['sentimentNegative'], '--ro', label='Neg') plt.plot(df_news_small['time'], df_news_small['sentimentNeutral'], '--bo', label='Neu') plt.plot(df_news_small['time'], df_news_small['sentimentPositive'], '--go', label='Pos') plt.legend
Processed documentation: plt.subplot(2,1,2) plt.plot(df_news_small['time'], df_news_small['sentimentNegative'], '--ro', label='Neg')
===========================================================================================
Cleaned documentation: Let's check we get the same result with both techniques. We use methane, molecule_id
Processed documentation: Let's check we get the same result with both techniques.
===========================================================================================
Cleaned documentation: Okay, that is the distance matrix for molecule_id==0. Let's check which molecule it is.
Processed documentation: Okay, that is the distance matrix for molecule_id==0.
===========================================================================================
Cleaned documentation: We can see the distances are 0 and 1.09 pretty much. Let's check the distance matrix again.
Processed documentation: Let's check the distance matrix again.
===========================================================================================
Cleaned documentation: Now let's proceed to cleaning. First we binarize both the mask and contours using global, otsu thresholding method
Processed documentation: First we binarize both the mask and contours using global, otsu thresholding method
===========================================================================================
Cleaned documentation: There is a problem with this approach. We are left with artifacts. Let's use binary_openning to drop them.
Processed documentation: Let's use binary_openning to drop them.
===========================================================================================
Cleaned documentation: This plot indicates a strong correlation between Percent and FVC. Again, there might have an heteroskedasticity issue.
Processed documentation: This plot indicates a strong correlation between Percent and FVC.
===========================================================================================
Cleaned documentation: Returns sparse feature matrix with added feature. feature_to_add can also be a list of features.
Processed documentation: Returns sparse feature matrix with added feature.
===========================================================================================
Cleaned documentation: Returns sparse feature matrix with added feature. feature_to_add can also be a list of features.
Processed documentation: Returns sparse feature matrix with added feature.
===========================================================================================
Cleaned documentation: It seems that the reference data (green line) won't be uniformly distributed. We can visualize it
Processed documentation: It seems that the reference data (green line) won't be uniformly distributed.
===========================================================================================
Cleaned documentation: For demonstration purposes we select some data from the train and resources data. Also add a new feature
Processed documentation: For demonstration purposes we select some data from the train and resources data.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Calculate the number of extra spaces in the text. We will call this n_extra_spaces
Processed documentation: Calculate the number of extra spaces in the text.
===========================================================================================
Cleaned documentation: It seems we have non numerical data here. Let's take a look at it
Processed documentation: It seems we have non numerical data here.
===========================================================================================
Cleaned documentation: Problem solved. Now let's see how many NaNs are there in the macro data
Processed documentation: Now let's see how many NaNs are there in the macro data
===========================================================================================
Cleaned documentation: There is a lot of NaNs in the macro data. But that is not the only problem
Processed documentation: There is a lot of NaNs in the macro data.
===========================================================================================
Cleaned documentation: Find edges by Canny method with OpenCV Parameter shall be tuned more. Here, below's is moderate
Processed documentation: Find edges by Canny method with OpenCV Parameter shall be tuned more.
===========================================================================================
Cleaned documentation: Quadrant images are shown below. I consider it's difficult to recoginize it's whale tail with only one quadrant
Processed documentation: I consider it's difficult to recoginize it's whale tail with only one quadrant
===========================================================================================
Cleaned documentation: Each X_test has different length. That means each prediction has several predictions. Here, simply, those are averaged.
Processed documentation: That means each prediction has several predictions.
===========================================================================================
Cleaned documentation: img: numpy array, 1 -> mask, 0 -> background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 -> mask, 0 -> background.
===========================================================================================
Cleaned documentation: Given an index `idx`, this function reads ground truth and predicted strings and returns. corresponding boxes in `Box` format.
Processed documentation: Given an index `idx`, this function reads ground truth and predicted strings and returns.
===========================================================================================
Cleaned documentation: HAD to change the names so they are easy to type. what can I say ¯\_(ツ
Processed documentation: HAD to change the names so they are easy to type.
===========================================================================================
Cleaned documentation: this could perhaps be weighted by properties of the respective atoms, such as no. electrons
Processed documentation: this could perhaps be weighted by properties of the respective atoms, such as no.
===========================================================================================
Cleaned documentation: Text classification model by character CNN, the implementation of paper. Yoon Kim. Convolution Neural Networks for Sentence. Classification.
Processed documentation: Text classification model by character CNN, the implementation of paper.
===========================================================================================
Cleaned documentation: Override this method to support more print formats. param results: dict, (str: float) is (metrics name: value
Processed documentation: param results: dict, (str: float) is (metrics name: value
===========================================================================================
Cleaned documentation: MSeasons.csv & WSeasons.csv These files identify the different seasons included in the historical data, along with certain season-level properties.
Processed documentation: These files identify the different seasons included in the historical data, along with certain season-level properties.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: Ideas of what I should look into next? Let me know in the comments.
Processed documentation: Let me know in the comments.
===========================================================================================
Cleaned documentation: We are predicting for a given `id` found in the test set. The id corresponds with a given `molecule_name
Processed documentation: We are predicting for a given `id` found in the test set.
===========================================================================================
Cleaned documentation: These plots are beautiful. It's a shame we don't have this data for the test set.
Processed documentation: It's a shame we don't have this data for the test set.
===========================================================================================
Cleaned documentation: Given a video and model, starting frame and end frame. Predict on all frames.
Processed documentation: Given a video and model, starting frame and end frame.
===========================================================================================
Cleaned documentation: The bpps directory has a bunch of numpy data. It's about 670MB in size.
Processed documentation: The bpps directory has a bunch of numpy data.
===========================================================================================
Cleaned documentation: Train data consists of only 107 sequence length. The test data contains mostly 130 sequence lengths.
Processed documentation: Train data consists of only 107 sequence length.
===========================================================================================
Cleaned documentation: Attention based on a parameterized normal pdf taking a molecule's. euclidean distance matrix as input.
Processed documentation: Attention based on a parameterized normal pdf taking a molecule's.
===========================================================================================
Cleaned documentation: And there you have it. We've fixed it. Thanks for your patience, and good luck
Processed documentation: Thanks for your patience, and good luck
===========================================================================================
Cleaned documentation: Look through the directory tree to find the image you specified. e.g. train_10.tif vs. train_10.jpg
Processed documentation: Look through the directory tree to find the image you specified.
===========================================================================================
Cleaned documentation: params: mask - numpy array. returns: run-length encoding string (pairs of start & length of encoding
Processed documentation: returns: run-length encoding string (pairs of start & length of encoding
===========================================================================================
Cleaned documentation: open an image and draws clear masks, so we don't lose sight of the. interesting features hiding underneath.
Processed documentation: open an image and draws clear masks, so we don't lose sight of the.
===========================================================================================
Cleaned documentation: Credit to Simon Walker, whose method helped me to. circumvent the commit error. Check out his kernel at
Processed documentation: Credit to Simon Walker, whose method helped me to.
===========================================================================================
Cleaned documentation: Normalize the data. Before we need to connvert data type to float for computation.
Processed documentation: Before we need to connvert data type to float for computation.
===========================================================================================
Cleaned documentation: Transform valid data to sparse tensor. Fit data will be transformed into each mini-batch.
Processed documentation: Transform valid data to sparse tensor.
===========================================================================================
Cleaned documentation: Season has a correlation of 1.0 with GameId and PlayId. Example below with 2017 and
Processed documentation: Season has a correlation of 1.0 with GameId and PlayId.
===========================================================================================
Cleaned documentation: Vamos dar uma olhada em como trabalhar com séries temporais usando esta competição do Kaggle. Então vamos analisar
Processed documentation: em como trabalhar com séries temporais usando esta competição do Kaggle.
===========================================================================================
Cleaned documentation: series - dataframe with timeseries. window - rolling window size. plot_intervals - show confidence intervals. plot_anomalies - show anomalies.
Processed documentation: window - rolling window size.
===========================================================================================
Cleaned documentation: Plots exponential smoothing with different alphas. series - dataset with timestamps. alphas - list of floats, smoothing parameters.
Processed documentation: Plots exponential smoothing with different alphas.
===========================================================================================
Cleaned documentation: series - dataset with timeseries. plot_intervals - show confidence intervals. plot_anomalies - show anomalies.
Processed documentation: series - dataset with timeseries.
===========================================================================================
Cleaned documentation: claro que os resíduos são estacionários e não há autocorrelações aparentes. Vamos fazer previsões usando nosso modelo.
Processed documentation: claro que os resíduos são estacionários e não há autocorrelações aparentes.
===========================================================================================
Cleaned documentation: timo, geramos um conjunto de dados aqui. Por que agora não treinamos um modelo
Processed documentation: Por que agora não treinamos um modelo
===========================================================================================
Cleaned documentation: Adicionaremos algumas features de data. Para fazer isso, precisamos transformar a data no formato `datetime`.
Processed documentation: Para fazer isso, precisamos transformar a data no formato `datetime`.
===========================================================================================
Cleaned documentation: Saving predictions for the site. The final submission file will be combining the predictions of all sites together.
Processed documentation: The final submission file will be combining the predictions of all sites together.
===========================================================================================
Cleaned documentation: Lag features are created. Several differences, ratios and averages of historic values are created and used.
Processed documentation: Several differences, ratios and averages of historic values are created and used.
===========================================================================================
Cleaned documentation: Lag features are created. Several differences, ratios and averages of historic values are created and used.
Processed documentation: Several differences, ratios and averages of historic values are created and used.
===========================================================================================
Cleaned documentation: Right now, I'm x2 the number of multiple diseases labels as that appears to be the majorly unrepresented class.
Processed documentation: the number of multiple diseases labels as that appears to be the majorly unrepresented class.
===========================================================================================
Cleaned documentation: df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x
Processed documentation: x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x
===========================================================================================
Cleaned documentation: df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x
Processed documentation: x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x
===========================================================================================
Cleaned documentation: As expected, we have 22 of each playid since we have 22 players. Let's look at our target variable(Yards).
Processed documentation: Let's look at our target variable(Yards).
===========================================================================================
Cleaned documentation: Get accuracy of model on validation data. It's not AUC but it's something at least
Processed documentation: Get accuracy of model on validation data.
===========================================================================================
Cleaned documentation: Referring to the following discussions. We have filtered the data in the next step Ref1. Ref2.
Processed documentation: Referring to the following discussions.
===========================================================================================
Cleaned documentation: Converts datetimes to days. Also converts to np.datetime64 because the air_visit_data's date type np.datetime64[ns].
Processed documentation: Also converts to np.datetime64 because the air_visit_data's date type np.datetime64[ns].
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded
Processed documentation: Impute any values will significantly affect the RMSE score for test set.
===========================================================================================
Cleaned documentation: transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else
Processed documentation: = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else
===========================================================================================
Cleaned documentation: New Features with Key Shopping times considered in the dataset. if the purchase has been made within 60 days,
Processed documentation: New Features with Key Shopping times considered in the dataset.
===========================================================================================
Cleaned documentation: new_transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else
Processed documentation: = (pd.to_datetime('2017-06-04') - new_transactions['purchase_date']).dt.days.apply(lambda x: x
===========================================================================================
Cleaned documentation: Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded
Processed documentation: Impute any values will significantly affect the RMSE score for test set.
===========================================================================================
Cleaned documentation: Our team best submission : Taking ensemble of .9479 Kernel (Ashish) and .9480 (best stable submission Normalized
Processed documentation: Kernel (Ashish) and .9480 (best stable submission Normalized
===========================================================================================
Cleaned documentation: So like you see with only 5 Features we passed from valid Gini : 0.2858 to valid Gini :0.
Processed documentation: So like you see with only 5 Features we passed from valid Gini : 0.2858 to valid
===========================================================================================
Cleaned documentation: Run evaliation for valid. This function is applied to each batch of val loader.
Processed documentation: This function is applied to each batch of val loader.
===========================================================================================
Cleaned documentation: L1: 1500(in)-1500 'r'ReLU lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0 w:1501x1500 out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.
Processed documentation: L1: 1500(in)-1500 'r'ReLU lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0 w:1501x1500 out(x3):1501x128 (0.00977451 GB) init..
===========================================================================================
Cleaned documentation: L2: 1500(in)-1500 'r'ReLU lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0 w:1501x1500 out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.
Processed documentation: L2: 1500(in)-1500 'r'ReLU lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0 w:1501x1500 out(x3):1501x128 (0.00977451 GB) init..
===========================================================================================
Cleaned documentation: and add a very small amount to every prob. so none of them are
Processed documentation: and add a very small amount to every prob.
===========================================================================================
Cleaned documentation: This looks great and exactly matches the source. Let's now decipher all the test data for difficulty=1.
Processed documentation: Let's now decipher all the test data for difficulty=1.
===========================================================================================
Cleaned documentation: Lets identify the uncommon columns between test and train data. Replace uncommon columns with a common value
Processed documentation: Lets identify the uncommon columns between test and train data.
===========================================================================================
Cleaned documentation: uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time
Processed documentation: uncomment if you want to see hyper parameter tuning.
===========================================================================================
Cleaned documentation: cv_results_dt = pd.DataFrame(grid_search_dt.cv_results_) printing the optimal accuracy score and hyperparameters. print("Decison Tree grid search Accuracy : ", grid_search_dt.best_score_) print(grid_search_dt.best_estimator
Processed documentation: print("Decison Tree grid search Accuracy : ", grid_search_dt.best_score_)
===========================================================================================
Cleaned documentation: uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time
Processed documentation: uncomment if you want to see hyper parameter tuning.
===========================================================================================
Cleaned documentation: So 79 duplicate images in train data. Lets display one duplicate image and corresponding mask
Processed documentation: Lets display one duplicate image and corresponding mask
===========================================================================================
Cleaned documentation: which is actually true: min age - 7, max age. Assigning '0' to those people
Processed documentation: which is actually true: min age - 7, max age.
===========================================================================================
Cleaned documentation: No luck. But since the property is 'assigned, borrowed', let's assume there's no monthly rent associated with it
Processed documentation: But since the property is 'assigned, borrowed', let's assume there's no monthly rent associated with it
===========================================================================================
Cleaned documentation: Yay! There is no missing data in the training set. How about outliers in the target variable
Processed documentation: There is no missing data in the training set.
===========================================================================================
Cleaned documentation: We're getting there only a few more data sets to analyze. Next up is the transactions metadata.
Processed documentation: We're getting there only a few more data sets to analyze.
===========================================================================================
Cleaned documentation: Here we adopt the parameters (perp = 30, dof = 0. perp = 30, dof = 0.7) を採用する
Processed documentation: Here we adopt the parameters (perp = 30, dof = 0.
===========================================================================================
Cleaned documentation: Points live in the point sensor frame. So they need to be transformed via global to the image plane.
Processed documentation: Points live in the point sensor frame.
===========================================================================================
Cleaned documentation: Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.
Processed documentation: Leave a margin of 1 pixel for aesthetic reasons.
===========================================================================================
Cleaned documentation: in case we want to restart the search. uncomment and rerun the below cells
Processed documentation: in case we want to restart the search.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: We'll start with PIL to load the images. I am sure many of you are familiar with this library.
Processed documentation: We'll start with PIL to load the images.
===========================================================================================
Cleaned documentation: Pretty good but gives me a deprecation warning though. Let's try what they recommend.
Processed documentation: Pretty good but gives me a deprecation warning though.
===========================================================================================
Cleaned documentation: ValentinesDay (バレンタインデー Cultural People send other people cards, flowers, chocolates, etc. with message to show their affection.
Processed documentation: ValentinesDay (バレンタインデー Cultural People send other people cards, flowers, chocolates, etc.
===========================================================================================
Cleaned documentation: Cinco De Mayo (シンコ・デ・マヨ Cultural Mexican holiday. People enjoy Mexican Products - Drinks, food, and music.
Processed documentation: Cinco De Mayo (シンコ・デ・マヨ Cultural Mexican holiday.
===========================================================================================
Cleaned documentation: Ramadan starts (ラマダーン開始 Religious Muslims fast from sunrise to sunset. They also avoids smoking.
Processed documentation: Ramadan starts (ラマダーン開始 Religious Muslims fast from sunrise to sunset.
===========================================================================================
Cleaned documentation: Eid al-Fitr (イド・アル＝フィトル Religious Islamic holiday. A big festival to celebrate the end of Ramadan.
Processed documentation: Eid al-Fitr (イド・アル＝フィトル Religious Islamic holiday.
===========================================================================================
Cleaned documentation: Convert mask to rle. img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Plot image and masks. If two pairs of images and masks are passes, show both.
Processed documentation: If two pairs of images and masks are passes, show both.
===========================================================================================
Cleaned documentation: Plot image and masks. If two pairs of images and masks are passes, show both.
Processed documentation: If two pairs of images and masks are passes, show both.
===========================================================================================
Cleaned documentation: Post processing of each predicted mask, components with lesser number of pixels. than `min_size` are ignored.
Processed documentation: Post processing of each predicted mask, components with lesser number of pixels.
===========================================================================================
Cleaned documentation: This is how original image and its masks look like. Let's try adding some augmentations
Processed documentation: This is how original image and its masks look like.
===========================================================================================
Cleaned documentation: Another way to visualize the observations made above is a pairplot. Again the same observations can be made.
Processed documentation: Another way to visualize the observations made above is a pairplot.
===========================================================================================
Cleaned documentation: As you see, the process is really fast. An example of some of the lag/trend columns for Spain
Processed documentation: An example of some of the lag/trend columns for Spain
===========================================================================================
Cleaned documentation: We can use for loop, of course! And that'll be better, sorry, this is for my easy trial.
Processed documentation: And that'll be better, sorry, this is for my easy trial.
===========================================================================================
Cleaned documentation: This is the summary result of model fitting. It describes model information and some statistical values.
Processed documentation: It describes model information and some statistical values.
===========================================================================================
Cleaned documentation: OK, total count in one month is the same through the whole year. How about weekly distribution
Processed documentation: OK, total count in one month is the same through the whole year.
===========================================================================================
Cleaned documentation: Now we can do the same thing in item category. And let's try item clustering.
Processed documentation: And let's try item clustering.
===========================================================================================
Cleaned documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation. and remove words containing numbers.
Processed documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation.
===========================================================================================
Cleaned documentation: There are 641 test "input_word_ids" samples which are confirmed at validation data. And there are also deplicated samples.
Processed documentation: There are 641 test "input_word_ids" samples which are confirmed at validation data.
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Initialize these variables which will be set in this if statement. Each of these
Processed documentation: Initialize these variables which will be set in this if statement.
===========================================================================================
Cleaned documentation: Return 2 lists of tuples: [(class_id, user_id, path), ...] for train. class_id, user_id, path), ...] for validation.
Processed documentation: [(class_id, user_id, path), ...] for train.
===========================================================================================
Cleaned documentation: Set up the neural network. print("Loading network.....") model = Darknet(cfgfile) model.load_weights(weightsfile) print("Network successfully loaded
Processed documentation: print("Loading network.....") model = Darknet(cfgfile) model.load_weights(weightsfile)
===========================================================================================
Cleaned documentation: batches = 1 model = getmodel(cfgfile, weightsfile) sub_str1 = detect(model,img_path,batches,0.5,0.4, coco_classes, weightsfile, cfgfile, "416") print(sub_str
Processed documentation: detect(model,img_path,batches,0.5,0.4, coco_classes, weightsfile, cfgfile, "416") print(sub_str
===========================================================================================
Cleaned documentation: Step 3: Create a filtering mask based on "box_class_scores" by using "threshold. The mask should have the
Processed documentation: Step 3: Create a filtering mask based on "box_class_scores" by using "threshold.
===========================================================================================
Cleaned documentation: Getting out X and y variable. We will be training on the X variable and predicting the Y variable
Processed documentation: We will be training on the X variable and predicting the Y variable
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: Other way to drop duplicate columns is to transpose DatFrame and use pandas routine - drop_duplicates. Thanks Scirpus
Processed documentation: Other way to drop duplicate columns is to transpose DatFrame and use pandas routine - drop_duplicates.
===========================================================================================
Cleaned documentation: The above plot looks very cluttered. Instead, we will take a look at the `top 25 features`.
Processed documentation: The above plot looks very cluttered.
===========================================================================================
Cleaned documentation: No positive image has a single label. Let's check if the label ```any``` is redundant.
Processed documentation: No positive image has a single label.
===========================================================================================
Cleaned documentation: Convert mask to rle. img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: GradCAM method for visualizing input saliency. Same as grad_cam but processes multiple images in one run.
Processed documentation: Same as grad_cam but processes multiple images in one run.
===========================================================================================
Cleaned documentation: Path retrieval Taken from the public kernel...thanks for sharing this. It was quite helpful
Processed documentation: Path retrieval Taken from the public kernel...thanks for sharing this.
===========================================================================================
Cleaned documentation: Events more like to be annual. We will analyze event effect on Sales on following section
Processed documentation: We will analyze event effect on Sales on following section
===========================================================================================
Cleaned documentation: We can observe that Test Statistic is not below the 1% of the Critical value. So,Series is not stationary.
Processed documentation: We can observe that Test Statistic is not below the 1% of the Critical value.
===========================================================================================
Cleaned documentation: Correlation function cross upper confident value between 1 and 2. Hence chose 2 as p for ARIMA
Processed documentation: Correlation function cross upper confident value between 1 and 2.
===========================================================================================
Cleaned documentation: Partial Autocorrelation function drop to 0 when value is between 1 and 2. choose 2 as q value
Processed documentation: Partial Autocorrelation function drop to 0 when value is between 1 and 2.
===========================================================================================
Cleaned documentation: Get accuracy of model on validation data. It's not AUC but it's something at least
Processed documentation: Get accuracy of model on validation data.
===========================================================================================
Cleaned documentation: Bingo. As billion-revenue movies are not made with zero budget, missing values in budget were obviously substituted with zeros.
Processed documentation: As billion-revenue movies are not made with zero budget, missing values in budget were obviously substituted with zeros.
===========================================================================================
Cleaned documentation: Please note a movie with a budget of $35 and high revenue. Let's take a closer look.
Processed documentation: Please note a movie with a budget of $35 and high revenue.
===========================================================================================
Cleaned documentation: Fun isn't something one often considers when cleaning the data. But this... does put a smile on my face.
Processed documentation: Fun isn't something one often considers when cleaning the data.
===========================================================================================
Cleaned documentation: We have previously verified that stratification by year leads to reasonable mimicking of the train/test split. Let's do it.
Processed documentation: We have previously verified that stratification by year leads to reasonable mimicking of the train/test split.
===========================================================================================
Cleaned documentation: I had problems with ffm-predict command: it was outputting several GB of floats on each run. This modification helped
Processed documentation: I had problems with ffm-predict command: it was outputting several GB of floats on each run.
===========================================================================================
Cleaned documentation: Returns two arrays: x is an array of resized images. y is an array of labels.
Processed documentation: Returns two arrays: x is an array of resized images.
===========================================================================================
Cleaned documentation: Time to predict classification using the model on the test set. Generate X_test and Y_test
Processed documentation: Time to predict classification using the model on the test set.
===========================================================================================
Cleaned documentation: code for scraping the data. Since all of the urls are of stackoverflow, they have the same html hierarchy.
Processed documentation: Since all of the urls are of stackoverflow, they have the same html hierarchy.
===========================================================================================
Cleaned documentation: This function allows Tokenizer to create an index of the tokenized unique characters. Eg. a=1, b=2, etc
Processed documentation: This function allows Tokenizer to create an index of the tokenized unique characters.
===========================================================================================
Cleaned documentation: This function allows Tokenizer to create an index of the tokenized unique characters. Eg. a=1, b=2, etc
Processed documentation: This function allows Tokenizer to create an index of the tokenized unique characters.
===========================================================================================
Cleaned documentation: With the prediction values from the NN model, prepare for submission. The following cells are pretty self-explantory.
Processed documentation: With the prediction values from the NN model, prepare for submission.
===========================================================================================
Cleaned documentation: The unemployment info for 2016 was missing. So the unemployment rate were taken from OCED website
Processed documentation: So the unemployment rate were taken from OCED website
===========================================================================================
Cleaned documentation: the logic of these noisy features for no reason. However, as we can't use
Processed documentation: the logic of these noisy features for no reason.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: selecting which models will be trained and used in inference. Options: Ensamble, Model No.
Processed documentation: selecting which models will be trained and used in inference.
===========================================================================================
Cleaned documentation: Now we will create another model to create an ensemble of the 2. Which will be a LSTM model.
Processed documentation: Now we will create another model to create an ensemble of the 2.
===========================================================================================
Cleaned documentation: Generate a Kaggle submission file. param threshold the score given to 'new_whale' @param filename the submission file name.
Processed documentation: param threshold the score given to 'new_whale' @param filename the submission file name.
===========================================================================================
Cleaned documentation: Show the full dataset. This format might be easier and more accesible for participants. Have fun
Processed documentation: This format might be easier and more accesible for participants.
===========================================================================================
Cleaned documentation: third weekly peak is the biggest... maybe taco tuesday? which would put sunday 6am on left
Processed documentation: third weekly peak is the biggest... maybe taco tuesday?
===========================================================================================
Cleaned documentation: are there people on this site that do that?! We might not have the right expertise.
Processed documentation: are there people on this site that do that?!
===========================================================================================
Cleaned documentation: SAMPLE_LEN def load_images(image_id file_path =image_id +'.jpg images = cv2.imread(image_path + file_path return cv2.cvtColor(images, cv2.COLOR_BGR2RGB train_image = train['image_id'][:100].progress_apply(load_images
Processed documentation: SAMPLE_LEN def load_images(image_id file_path =image_id +'.jpg images = cv2.imread(image_path + file_path return cv2.cvtColor(images, cv2.COLOR_BGR2RGB train_image =
===========================================================================================
Cleaned documentation: This is time series problem, we can use various forecasting techniques. For starters I am using XGBoost Regression.
Processed documentation: This is time series problem, we can use various forecasting techniques.
===========================================================================================
Cleaned documentation: TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.
Processed documentation: TPUEstimator also supports training on CPU and GPU.
===========================================================================================
Cleaned documentation: In the sales dataset, each row represents one item in one specific store. since our target is sales,
Processed documentation: In the sales dataset, each row represents one item in one specific store.
===========================================================================================
Cleaned documentation: Return Dataframe for lags. Input is dataframe with last column as TARGET and others as (composite) key.
Processed documentation: Input is dataframe with last column as TARGET and others as (composite) key.
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed.
Processed documentation: Arguments: seed {int} -- Number of the seed.
===========================================================================================
Cleaned documentation: Usual torch forward function. Arguments: tokens {torch tensor} -- Sentence tokens. Returns: torch tensor -- Class logits.
Processed documentation: Arguments: tokens {torch tensor} -- Sentence tokens.
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: number that identifies this image, This Attribute was named Image Number in earlier versions of this Standard. Instance Number
Processed documentation: number that identifies this image, This Attribute was named Image Number in earlier versions of this Standard.
===========================================================================================
Cleaned documentation: Data representation of the pixel samples. Each sample shall have the same pixel representation. Pixel Representation Attribute
Processed documentation: Data representation of the pixel samples.
===========================================================================================
Cleaned documentation: and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
Processed documentation: A workaround is to add a very small positive number ε to the sum.
===========================================================================================
Cleaned documentation: Below function prepare and return the `tf.data.Dataset` object during each fold of cross validation.
Processed documentation: Dataset` object during each fold of cross validation.
===========================================================================================
Cleaned documentation: Couldn't find any evidence or relation between them in positive tweets.Let's check for negative tweets
Processed documentation: Couldn't find any evidence or relation between them in positive tweets.
===========================================================================================
Cleaned documentation: Now the comment text is prepared and encoded using this tokenizer easily. We here set the maxlen=128,(limit
Processed documentation: Now the comment text is prepared and encoded using this tokenizer easily.
===========================================================================================
Cleaned documentation: This is a multi-input model (comment+sentiment label). I have made a simple LSTM model concatenated both the inputs
Processed documentation: This is a multi-input model (comment+sentiment label).
===========================================================================================
Cleaned documentation: Now we will encode our target value.Keras inbuild library to_categorical() is used to do the on-hot encoding.
Processed documentation: Keras inbuild library to_categorical() is used to do the on-hot encoding.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: All images are of the same 2100 1400. There are 5546 distinct images in the training set.
Processed documentation: There are 5546 distinct images in the training set.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: mask_org = (train_df['date'] == (d - pd.Timedelta(days=lag))) & (train_df['country'] == i) & (train_df['province'] == j
Processed documentation: Timedelta(days=lag))) & (train_df['country']
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: Dummy column to calculate 1 or 0 values. for Positive CREDIT_ENDDATE and 0 for Negative
Processed documentation: Dummy column to calculate 1 or 0 values.
===========================================================================================
Cleaned documentation: calculate mask type for stratify, the difficuly of training different mask type is different.
Processed documentation: the difficuly of training different mask type is different.
===========================================================================================
Cleaned documentation: used for converting the decoded image to rle mask. Fast compared to previous one.
Processed documentation: used for converting the decoded image to rle mask.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: used for converting the decoded image to rle mask. Fast compared to previous one.
Processed documentation: used for converting the decoded image to rle mask.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Let's run our random agent against the default one. Co-ordinates (0,0) are at the top left corner.
Processed documentation: Let's run our random agent against the default one.
===========================================================================================
Cleaned documentation: used for converting the decoded image to rle mask. Fast compared to previous one.
Processed documentation: used for converting the decoded image to rle mask.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.
Processed documentation: Drop the first column 'Id' since it just has serial numbers.
===========================================================================================
Cleaned documentation: No attribute is missing as count is 15120 for all attributes. Hence, all rows can be used
Processed documentation: No attribute is missing as count is 15120 for all attributes.
===========================================================================================
Cleaned documentation: Negative value(s) present in Vertical_Distance_To_Hydrology. Hence, some tests such as chi-sq cant be used.
Processed documentation: Negative value(s) present in Vertical_Distance_To_Hydrology.
===========================================================================================
Cleaned documentation: Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis
Processed documentation: Wilderness_Area and Soil_Type are one hot encoded.
===========================================================================================
Cleaned documentation: Scales are not the same for all. Hence, rescaling and standardization may be necessary for some algos
Processed documentation: Hence, rescaling and standardization may be necessary for some algos
===========================================================================================
Cleaned documentation: Several attributes in Soil_Type show a large skew. Hence, some algos may benefit if skew is corrected
Processed documentation: Several attributes in Soil_Type show a large skew.
===========================================================================================
Cleaned documentation: We see that all classes have an equal presence. No class re-balancing is necessary
Processed documentation: We see that all classes have an equal presence.
===========================================================================================
Cleaned documentation: The plots show to which class does a point belong to. The class distribution overlaps in the plots.
Processed documentation: The plots show to which class does a point belong to.
===========================================================================================
Cleaned documentation: Elevation is has a separate distribution for most classes. Highly correlated with the target and hence an important attribute
Processed documentation: Elevation is has a separate distribution for most classes.
===========================================================================================
Cleaned documentation: Wilderness_Area3 gives no class distinction. As values are not present, others gives some scope to distinguish
Processed documentation: As values are not present, others gives some scope to distinguish
===========================================================================================
Cleaned documentation: Drop the first column 'id' since it just has serial numbers. Not useful in the prediction process.
Processed documentation: Drop the first column 'id' since it just has serial numbers.
===========================================================================================
Cleaned documentation: cat1 to cat116 have strings. The ML algorithms we are going to study require numberical data
Processed documentation: The ML algorithms we are going to study require numberical data
===========================================================================================
Cleaned documentation: Result obtained after running the algo. Comment the below two lines if you want to run the algo
Processed documentation: Comment the below two lines if you want to run the algo
===========================================================================================
Cleaned documentation: Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.
Processed documentation: Drop the first column 'Id' since it just has serial numbers.
===========================================================================================
Cleaned documentation: Best estimated performance is. Occurs when all features are used and without any transformation
Processed documentation: Occurs when all features are used and without any transformation
===========================================================================================
Cleaned documentation: Best estimated performance is close to. Original with 50% subset outperfoms all transformations of NB
Processed documentation: Original with 50% subset outperfoms all transformations of NB
===========================================================================================
Cleaned documentation: This is the main part of kernel. We will generate function for K-Opt move
Processed documentation: We will generate function for K-Opt move
===========================================================================================
Cleaned documentation: Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased
Processed documentation: Learning rate will be half after 3 epochs if accuracy is not increased
===========================================================================================
Cleaned documentation: This will just calculate parameters required to augment the given data. This won't perform any augmentations
Processed documentation: This will just calculate parameters required to augment the given data.
===========================================================================================
Cleaned documentation: The yellow bars indicate the presence of NaN values. It seems like few processed categorical have missing values.
Processed documentation: The yellow bars indicate the presence of NaN values.
===========================================================================================
Cleaned documentation: Now, that are too many number categories. Let's see the top 10 highest counted categories...
Processed documentation: Let's see the top 10 highest counted categories...
===========================================================================================
Cleaned documentation: Hmm.. Something clicked at top left and bottom right portion. Let's remove calc_feat and observe the heatmap
Processed documentation: Let's remove calc_feat and observe the heatmap
===========================================================================================
Cleaned documentation: The usual plot is same as the previous ones. Let's filter the early morning and evening times pickups
Processed documentation: Let's filter the early morning and evening times pickups
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: Hope you still remember, our mr_inspect. He is ready to call and jump in right now...
Processed documentation: Hope you still remember, our mr_inspect.
===========================================================================================
Cleaned documentation: Remember to Label, OneHot and Dummy Encode your features. As most of them are categorical here ...
Processed documentation: Remember to Label, OneHot and Dummy Encode your features.
===========================================================================================
Cleaned documentation: We have really long messages here upto length of 5000 words . Lets quickly check.
Processed documentation: We have really long messages here upto length of 5000 words .
===========================================================================================
Cleaned documentation: The following code is to save the augmented data locally. It is only useful for Linux operating system
Processed documentation: The following code is to save the augmented data locally.
===========================================================================================
Cleaned documentation: Return a feed dict for the graph including all the categorical features. to embed.
Processed documentation: Return a feed dict for the graph including all the categorical features.
===========================================================================================
Cleaned documentation: Get random selection of data for batch GD. Upsample positive classes to make it
Processed documentation: Get random selection of data for batch GD.
===========================================================================================
Cleaned documentation: Nice, there are no missing values in the training data. What about the test data
Processed documentation: Nice, there are no missing values in the training data.
===========================================================================================
Cleaned documentation: OSRM")) features. This data contains the fastest routes from specific starting points in NY.
Processed documentation: This data contains the fastest routes from specific starting points in NY.
===========================================================================================
Cleaned documentation: Test data has 892816 rows and train has 595212 rows. We have 59 independent variables.
Processed documentation: Test data has 892816 rows and train has 595212 rows.
===========================================================================================
Cleaned documentation: Changes Scikit learn's random forests to give each tree a random sample of. n random rows.
Processed documentation: Changes Scikit learn's random forests to give each tree a random sample of.
===========================================================================================
Cleaned documentation: In this Section, I used to predict test data using many models.　Now, It isn't necessary.
Processed documentation: In this Section, I used to predict test data using many models.
===========================================================================================
Cleaned documentation: pred of FVC at week 0 itself. Other weeks will be predicted using this as base
Processed documentation: Other weeks will be predicted using this as base
===========================================================================================
Cleaned documentation: Thanks a lot [@ebouteillon]( for advise to use `imagededup`. Good tool, fast and good precision "in box
Processed documentation: Good tool, fast and good precision "in box
===========================================================================================
Cleaned documentation: Thresholds I recommend choose on your validation set for English model! It is very important.
Processed documentation: Thresholds I recommend choose on your validation set for English model!
===========================================================================================
Cleaned documentation: swap_distance - distance for swapping words. swap_probability - probability of swapping for one word.
Processed documentation: swap_distance - distance for swapping words.
===========================================================================================
Cleaned documentation: So here we have the datatypes. Let's have a look at the data itself.
Processed documentation: Let's have a look at the data itself.
===========================================================================================
Cleaned documentation: df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis
Processed documentation: + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis
===========================================================================================
Cleaned documentation: desc: get nulls for each column in counts & percentages. arg: dataframe. return: dataframe.
Processed documentation: desc: get nulls for each column in counts & percentages.
===========================================================================================
Cleaned documentation: There are two types of features we have in dataset. Let's see them Categorical features
Processed documentation: There are two types of features we have in dataset.
===========================================================================================
Cleaned documentation: There are so many duplicate questions in our train data. Let's check number of unique questions.
Processed documentation: Let's check number of unique questions.
===========================================================================================
Cleaned documentation: We have 3583 unique questions. WordCloud Question Title Question Body Answer Let's check which words are used most
Processed documentation: WordCloud Question Title Question Body Answer Let's check which words are used most
===========================================================================================
Cleaned documentation: Most of the data is collected from Stackoverflow.com We have 30 target variables. Let's see them
Processed documentation: Most of the data is collected from Stackoverflow.com We have 30 target variables.
===========================================================================================
Cleaned documentation: We can clearly see the features that are correlated like 'question_type_instructions' and 'answer_type_instructions'. Let's see how
Processed documentation: We can clearly see the features that are correlated like 'question_type_instructions' and 'answer_type_instructions'.
===========================================================================================
Cleaned documentation: We have 4 columns less in application_test_dummies. Lets see which are those 4 columns
Processed documentation: We have 4 columns less in application_test_dummies.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: If you just need the score, set Score_only = True for slightly faster calculations.
Processed documentation: If you just need the score, set Score_only =
===========================================================================================
Cleaned documentation: The mode is 45 and median is 50. It's best to use median to fill the missing values.
Processed documentation: It's best to use median to fill the missing values.
===========================================================================================
Cleaned documentation: The risk of melanoma increases as people age. The average age of people when it is diagnosed is 65.
Processed documentation: The risk of melanoma increases as people age.
===========================================================================================
Cleaned documentation: From nevus , we see that the moles are concentrated and not spread out .Showing no signs of malignant.
Processed documentation: From nevus , we see that the moles are concentrated and not spread out .Showing
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Compressing the Wilderness_Areas and Soil_Type features into a single column. A comprehensive explaination provieded in the [Kernel
Processed documentation: Compressing the Wilderness_Areas and Soil_Type features into a single column.
===========================================================================================
Cleaned documentation: It looks like a right skewed distribution. One can apply logarithm to make it normally distributed.
Processed documentation: It looks like a right skewed distribution.
===========================================================================================
Cleaned documentation: But, what about the possible coordiantes that point to the water? Let's plot the southwest area of the city.
Processed documentation: Let's plot the southwest area of the city.
===========================================================================================
Cleaned documentation: Let's take a closer look at our one numeric feature, `teacher_number_of_previously_posted_projects`. Print some key stats using the `describe()` method
Processed documentation: Let's take a closer look at our one numeric feature, `teacher_number_of_previously_posted_projects`.
===========================================================================================
Cleaned documentation: Problem class includes the scoring code. There is a mostly-unused PyTorch variant of it here as well.
Processed documentation: Problem class includes the scoring code.
===========================================================================================
Cleaned documentation: i.e. we add that many blocks on each side of the central block. The
Processed documentation: i.e. we add that many blocks on each side of the central block.
===========================================================================================
Cleaned documentation: For the shapes of the things below, look at the args to `view. The terms
Processed documentation: For the shapes of the things below, look at the args to `view.
===========================================================================================
Cleaned documentation: resample with efficient pytorch resampler. filter='hann' and num_zeros=6 reproduce the settings in torchaudio's existing resample function from above.
Processed documentation: filter='hann' and num_zeros=6 reproduce the settings in torchaudio's existing resample function from above.
===========================================================================================
Cleaned documentation: G. GEO_NETWORK It is json and the similar manner to previous feature (device) should be done.
Processed documentation: It is json and the similar manner to previous feature (device) should be done.
===========================================================================================
Cleaned documentation: H.SOCIAL_ENGANEMENT_TYPE Describing this feature confirms its uniqueness. It should be dropped. Because its entropy is 0.
Processed documentation: H.SOCIAL_ENGANEMENT_TYPE Describing this feature confirms its uniqueness.
===========================================================================================
Cleaned documentation: vw -d train --loss_function squared --learning_rate 0.01 -f model -b 26 --passes 20 --cache_file cache --quiet
Processed documentation: vw -d train --loss_function squared --learning_rate 0.01 -f model -b 26
===========================================================================================
Cleaned documentation: Essentially most of the answerable questions aren't a yes/no type. Distribution of short answers
Processed documentation: Essentially most of the answerable questions aren't a yes/no type.
===========================================================================================
Cleaned documentation: Every custom transformer requires a fit method. In this case, we want to train
Processed documentation: Every custom transformer requires a fit method.
===========================================================================================
Cleaned documentation: sum of the other two. In some cases, that colinearity can actually be problematic.
Processed documentation: In some cases, that colinearity can actually be problematic.
===========================================================================================
Cleaned documentation: Every custom transformer also requires a transform method. This time we just want to
Processed documentation: Every custom transformer also requires a transform method.
===========================================================================================
Cleaned documentation: There's admittedly some voodoo in this step. Math can be more highly optimized in python
Processed documentation: Math can be more highly optimized in python
===========================================================================================
Cleaned documentation: than string processing, so spaCy really stores the parts of speech as numbers. If you
Processed documentation: than string processing, so spaCy really stores the parts of speech as numbers.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Light GBM also has a useful "early-stopping" option. The following access function will use this function
Processed documentation: The following access function will use this function
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Used to access model performance. Returns the mean rmsle score of cross validated data.
Processed documentation: Returns the mean rmsle score of cross validated data.
===========================================================================================
Cleaned documentation: Looks like it's the same. I think read_file will be removed in future. See this
Processed documentation: I think read_file will be removed in future.
===========================================================================================
Cleaned documentation: Dive into data. Points have x, y and z coords and intensity - always 100.
Processed documentation: Points have x, y and z coords and intensity - always 100.
===========================================================================================
Cleaned documentation: rotation in radians of bbox: x, y and z. For degrees see example above (next MPL Figure
Processed documentation: rotation in radians of bbox: x, y and z.
===========================================================================================
Cleaned documentation: These example images are from the training dataset. Images are tagged with more than one label.
Processed documentation: These example images are from the training dataset.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Takes a list of image paths (pathlib.Path objects), analyzes each, and returns a submission-ready DataFrame.
Processed documentation: Path objects), analyzes each, and returns a submission-ready DataFrame.
===========================================================================================
Cleaned documentation: Why Chest sorter and Bird Measurer is little low compare to all others? let dig more.
Processed documentation: Why Chest sorter and Bird Measurer is little low compare to all others?
===========================================================================================
Cleaned documentation: Training : Top 10 games played most. Looks like super intesting games with mostly animals, especially dinosaur.
Processed documentation: Looks like super intesting games with mostly animals, especially dinosaur.
===========================================================================================
Cleaned documentation: kids mostly interested in interactive things like Game and Activity. Wathcing videos is boring
Processed documentation: kids mostly interested in interactive things like Game and Activity.
===========================================================================================
Cleaned documentation: kids are interested in playing games related to hills. so many games provided in that segment.
Processed documentation: kids are interested in playing games related to hills.
===========================================================================================
Cleaned documentation: Peak installation was September 27th. Because of this: PBS KIDS Family Event At O'Neill Public Library.
Processed documentation: Because of this: PBS KIDS Family Event At O'Neill Public Library.
===========================================================================================
Cleaned documentation: Good number of games and activities in each world. Introduction world is having only videos.
Processed documentation: Good number of games and activities in each world.
===========================================================================================
Cleaned documentation: In terms of assessment, each world is having its own Title. title is not common to all world
Processed documentation: In terms of assessment, each world is having its own Title.
===========================================================================================
Cleaned documentation: This data seems to have a lot of missing value unlike last time. Let's look at the sorted values.
Processed documentation: This data seems to have a lot of missing value unlike last time.
===========================================================================================
Cleaned documentation: Oddly... it feels like the 2 graphs are gradually expanding... Using this part seems to minimize the feature.
Processed documentation: Oddly... it feels like the 2 graphs are gradually expanding...
===========================================================================================
Cleaned documentation: Surprisingly, the graph feels like something else. Any advice would be appreciated if it was just me.
Processed documentation: Surprisingly, the graph feels like something else.
===========================================================================================
Cleaned documentation: Good to see that the distribution is similar between train and test. ANTIGUEDAD Customer seniority in months.
Processed documentation: Good to see that the distribution is similar between train and test.
===========================================================================================
Cleaned documentation: Peaks are comparatively bigger than the train set. Any implications RENTA Gross income of the household.
Processed documentation: Any implications RENTA Gross income of the household.
===========================================================================================
Cleaned documentation: Looks like there are few outiers in the data. Let us check the actual count of it.
Processed documentation: Let us check the actual count of it.
===========================================================================================
Cleaned documentation: Looks like evenly distributed across the interest levels. Now let us look at the next feature 'bedrooms'. Bedrooms
Processed documentation: Looks like evenly distributed across the interest levels.
===========================================================================================
Cleaned documentation: Looks like there are some outliers in this feature. So let us remove them and then plot again.
Processed documentation: Looks like there are some outliers in this feature.
===========================================================================================
Cleaned documentation: So the latitude values are primarily between 40.6 and 40.9. Now let us look at the longitude values.
Processed documentation: Now let us look at the longitude values.
===========================================================================================
Cleaned documentation: Let us now look at the number of features variable and see its distribution. Number of features
Processed documentation: Let us now look at the number of features variable and see its distribution.
===========================================================================================
Cleaned documentation: Seems like a single data point is well above the rest. Now let us plot the distribution graph.
Processed documentation: Seems like a single data point is well above the rest.
===========================================================================================
Cleaned documentation: X0 to X8 are the categorical columns. Missing values Let us now check for the missing values.
Processed documentation: Missing values Let us now check for the missing values.
===========================================================================================
Cleaned documentation: Produce is the largest department. Now let us check the reordered percentage of each department. Department wise reorder ratio
Processed documentation: Now let us check the reordered percentage of each department.
===========================================================================================
Cleaned documentation: Personal care has lowest reorder ratio and dairy eggs have highest reorder ratio. Aisle - Reorder ratio
Processed documentation: Personal care has lowest reorder ratio and dairy eggs have highest reorder ratio.
===========================================================================================
Cleaned documentation: Function that plots the football field for viewing plays. Allows for showing or hiding endzones.
Processed documentation: Function that plots the football field for viewing plays.
===========================================================================================
Cleaned documentation: Good to see that the distribution is similar between train and test. ANTIGUEDAD Customer seniority in months.
Processed documentation: Good to see that the distribution is similar between train and test.
===========================================================================================
Cleaned documentation: Peaks are comparatively bigger than the train set. Any implications RENTA Gross income of the household. To update
Processed documentation: Any implications RENTA Gross income of the household.
===========================================================================================
Cleaned documentation: Logerror Target variable for this competition is "logerror" field. So let us do some analysis on this field first.
Processed documentation: Logerror Target variable for this competition is "logerror" field.
===========================================================================================
Cleaned documentation: There are so many NaN values in the dataset. So let us first do some exploration on that one.
Processed documentation: There are so many NaN values in the dataset.
===========================================================================================
Cleaned documentation: Almost all are float variables with few object (categorical) variables. Let us get the count.
Processed documentation: Almost all are float variables with few object (categorical) variables.
===========================================================================================
Cleaned documentation: stop_words = set(stopwords.words('english') + ['rbs', 'sas', 'ss', 'fas', 'des', 'ess', 'les', 'bas', 'poses', 'los', 'ros', 'cs
Processed documentation: + ['rbs', 'sas', 'ss', 'fas', 'des', 'ess', 'les', 'bas', 'poses', 'los', 'ros', 'cs
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: SVD on Character TFIDF We could also create svd features on character tfidf features and used them for modeling.
Processed documentation: We could also create svd features on character tfidf features and used them for modeling.
===========================================================================================
Cleaned documentation: We could see a long tail here as well. Now let us check the target variable distribution.
Processed documentation: Now let us check the target variable distribution.
===========================================================================================
Cleaned documentation: We have 4 categorical features in our data display_address manager_id building_id listing_id So let us label encode these features.
Processed documentation: We have 4 categorical features in our data display_address manager_id building_id listing_id
===========================================================================================
Cleaned documentation: The correlation coefficient for ps_calc is 0,so we will drop these from our dataset.
Processed documentation: The correlation coefficient for ps_calc is 0,so
===========================================================================================
Cleaned documentation: logreg = LogisticRegression(class_weight='balanced') param = {'C':[0.001,0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1]} clf = GridSearchCV(logreg,param,scoring='roc_auc',refit=True,cv=3) clf.fit(X,y) print('Best roc_auc: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_['C
Processed documentation: LogisticRegression(class_weight='balanced') param = {'C':[0.001,0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1]} clf = GridSearchCV(logreg,param,scoring='roc_auc',refit=True,cv=3) clf.fit(X,y)
===========================================================================================
Cleaned documentation: The correlation coefficient for ps_calc is 0,so we will drop these from our dataset.
Processed documentation: The correlation coefficient for ps_calc is 0,so
===========================================================================================
Cleaned documentation: The function will reduce memory of dataframe. Note: Apply this function after removing missing value.
Processed documentation: Note: Apply this function after removing missing value.
===========================================================================================
Cleaned documentation: Get accuracy of model on validation data. It's not AUC but it's something at least
Processed documentation: Get accuracy of model on validation data.
===========================================================================================
Cleaned documentation: Unclear why fails to open [encoding error], format is same as for glove. Will Debug, Dan
Processed documentation: [encoding error], format is same as for glove.
===========================================================================================
Cleaned documentation: Generate a Kaggle submission file. param threshold the score given to 'new_whale' @param filename the submission file name.
Processed documentation: param threshold the score given to 'new_whale' @param filename the submission file name.
===========================================================================================
Cleaned documentation: Cabin Initial 이 확보된 전체 데이터의 양이 크지 않다는점을 고려해야 함. (특히 G, T의 경우
Processed documentation: Cabin Initial 이 확보된 전체 데이터의 양이 크지 않다는점을 고려해야 함.
===========================================================================================
Cleaned documentation: Null Value가 적은 embarked부터. 총 2개뿐이 Null value이므로 가장 많은 수를 차지하는 "S" 로 다체하겠다.
Processed documentation: 총 2개뿐이 Null value이므로 가장 많은 수를 차지하는 "S" 로 다체하겠다.
===========================================================================================
Cleaned documentation: Age 의 결측치를 채워보도록 하겠다. Age를 가늠할 수 있는 요소는 가족의 수 또는 Initial이 있다.
Processed documentation: Age를 가늠할 수 있는 요소는 가족의 수 또는 Initial이 있다.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: significant seasonal thing going on. Then we should start to consider SARIMA to take seasonality into accuont
Processed documentation: Then we should start to consider SARIMA to take seasonality into accuont
===========================================================================================
Cleaned documentation: returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning
Processed documentation: returns a 2-tuple of the chi-squared statistic, and the associated p-value.
===========================================================================================
Cleaned documentation: for i in test_generator: print(test_generator.batch_index, test_generator.batch_size) idx = (test_generator.batch_index - 1) test_generator.batch_size. print(test_generator.filenames[idx : idx + test_generator.batch_size
Processed documentation: for i in test_generator: print(test_generator.batch_index, test_generator.batch_size) idx = (test_generator.batch_index - 1) test_generator.batch_size.
===========================================================================================
Cleaned documentation: The transaction data that has comple identity returns mostly NaN except for M4. Let's check it out
Processed documentation: The transaction data that has comple identity returns mostly NaN except for M4.
===========================================================================================
Cleaned documentation: Observation: Higher values of C3 associated with no-fraud. C5 and C9 are homogeneous columns. Examine D features
Processed documentation: Observation: Higher values of C3 associated with no-fraud.
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed.
Processed documentation: Arguments: seed {int} -- Number of the seed.
===========================================================================================
Cleaned documentation: Generate creds for the Drive FUSE library. Though the link asks you to verify twice, you don't have to
Processed documentation: Generate creds for the Drive FUSE library.
===========================================================================================
Cleaned documentation: Qnet predicts Q values for each action. Target Q values come from rewards of selected actions.
Processed documentation: Target Q values come from rewards of selected actions.
===========================================================================================
Cleaned documentation: Future rewards affect current reward for N steps. N = None: All future rewards.
Processed documentation: Future rewards affect current reward for N steps.
===========================================================================================
Cleaned documentation: Checking the one that should also flip the action. e.g. Action NORTH should become SOUTH
Processed documentation: Checking the one that should also flip the action.
===========================================================================================
Cleaned documentation: Function to visualize several segmentation maps. INPUT: image_id - filename of the image. RETURNS: np_mask - numpy segmentation map.
Processed documentation: RETURNS: np_mask - numpy segmentation map.
===========================================================================================
Cleaned documentation: Function to visualize the image and the mask. INPUT: line_id - id of the line to visualize the masks.
Processed documentation: INPUT: line_id - id of the line to visualize the masks.
===========================================================================================
Cleaned documentation: Function to plot several random images with segmentation masks. INPUT: n_images - number of images to visualize.
Processed documentation: Function to plot several random images with segmentation masks.
===========================================================================================
Cleaned documentation: v ranges from 0 to. This gives an extra flexibility of measuring distance from any of the 4 corners
Processed documentation: This gives an extra flexibility of measuring distance from any of the 4 corners
===========================================================================================
Cleaned documentation: Holds obs info with types Pos, ObservationShip, ObservationShipYard for clarity. Precalculates some frequently needed information.
Processed documentation: Holds obs info with types Pos, ObservationShip, ObservationShipYard for clarity.
===========================================================================================
Cleaned documentation: Bot plan with all information for resolution. Every plan probably should also have a StayAction to guarantee resolution.
Processed documentation: Bot plan with all information for resolution.
===========================================================================================
Cleaned documentation: Actions can generate Halite commands. Most likely you do not need to write new actions.
Processed documentation: Actions can generate Halite commands.
===========================================================================================
Cleaned documentation: Strategies have mainly the task to implement `strategy.make_plans()`. They are stored as objects in a global variable.
Processed documentation: Strategies have mainly the task to implement `strategy.make_plans()`.
===========================================================================================
Cleaned documentation: Now all we need is some global state. The agent function is pretty simple.
Processed documentation: Now all we need is some global state.
===========================================================================================
Cleaned documentation: We can find multiple segments of sequences in the test folder. Let's peek at their names
Processed documentation: We can find multiple segments of sequences in the test folder.
===========================================================================================
Cleaned documentation: To start we'll just take the FastText Common Crawl embeddings. Later, we'll hopefully combine multiple embeddings.
Processed documentation: To start we'll just take the FastText Common Crawl embeddings.
===========================================================================================
Cleaned documentation: We start by setting the labels according to the task. It’s as easy as
Processed documentation: We start by setting the labels according to the task.
===========================================================================================
Cleaned documentation: Define generator. Using keras ImageDataGenerator. You can change the method of data augmentation by changing data_gen_args.
Processed documentation: You can change the method of data augmentation by changing data_gen_args.
===========================================================================================
Cleaned documentation: The images are actually quite big. We will resize to a much smaller size.
Processed documentation: We will resize to a much smaller size.
===========================================================================================
Cleaned documentation: That's right, the images are a whopping 347 GB! This is expected with thousands of Whole-slide images (WSIs).
Processed documentation: This is expected with thousands of Whole-slide images (WSIs).
===========================================================================================
Cleaned documentation: If you want to use any another model just replace the model name in transformers._____ and use accordingly
Processed documentation: If you want to use any another model just replace the model name in transformers.
===========================================================================================
Cleaned documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation. and remove words containing numbers.
Processed documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation.
===========================================================================================
Cleaned documentation: OK, look like it is too long. Probably each number is one character. Let's count the numbers.
Processed documentation: Let's count the numbers.
===========================================================================================
Cleaned documentation: From @randxie. Modified to work with scipy version 1.1.0 which does not have the fs parameter.
Processed documentation: Modified to work with scipy version 1.1.0 which does not have the fs parameter.
===========================================================================================
Cleaned documentation: Fault pattern usually exists in high frequency band. According to literature, the pattern is visible above 10^4 Hz.
Processed documentation: Fault pattern usually exists in high frequency band.
===========================================================================================
Cleaned documentation: The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.
Processed documentation: Red graphs represent original sales and green graphs represent denoised sales.
===========================================================================================
Cleaned documentation: The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.
Processed documentation: Red graphs represent original sales and green graphs represent denoised sales.
===========================================================================================
Cleaned documentation: Timeline of the revenue. Note: I think that the decrease in August is caused by missing data.
Processed documentation: Note: I think that the decrease in August is caused by missing data.
===========================================================================================
Cleaned documentation: Timeline of revenue and weekdays. It is clear a correlationship between the both of them.
Processed documentation: Timeline of revenue and weekdays.
===========================================================================================
Cleaned documentation: We'll train the model using Cross Entropy Loss for 1000 epochs. We can see that the model converges nicely.
Processed documentation: We'll train the model using Cross Entropy Loss for 1000 epochs.
===========================================================================================
Cleaned documentation: Initialize a point cloud and check it has the correct dimensions. param points. d-dimensional input point cloud matrix.
Processed documentation: Initialize a point cloud and check it has the correct dimensions.
===========================================================================================
Cleaned documentation: Loads point cloud from disk. Args: file_name: Path of the pointcloud file on disk. Returns: PointCloud instance.
Processed documentation: Args: file_name: Path of the pointcloud file on disk.
===========================================================================================
Cleaned documentation: Removes point too close within a certain radius from origin. Args: radius: Radius below which points are removed. Returns
Processed documentation: Removes point too close within a certain radius from origin.
===========================================================================================
Cleaned documentation: Applies a translation to the point cloud. Args: x. Translation in x, y, z.
Processed documentation: Args: x. Translation in x, y, z.
===========================================================================================
Cleaned documentation: Returns the four bottom corners. Returns. Bottom corners. First two face forward, last two face backwards.
Processed documentation: Returns the four bottom corners.
===========================================================================================
Cleaned documentation: De-normalizes database to create reverse indices for common cases. Args: verbose: Whether to print outputs.
Processed documentation: De-normalizes database to create reverse indices for common cases.
===========================================================================================
Cleaned documentation: Instantiates a Box class from a sample annotation record. Args: sample_annotation_token: Unique sample_annotation identifier. Returns
Processed documentation: Instantiates a Box class from a sample annotation record.
===========================================================================================
Cleaned documentation: Points live in the point sensor frame. So they need to be transformed via global to the image plane.
Processed documentation: Points live in the point sensor frame.
===========================================================================================
Cleaned documentation: Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.
Processed documentation: Leave a margin of 1 pixel for aesthetic reasons.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: Lets start building the model. Uncomment the code (commented because it exceeds time limit
Processed documentation: Uncomment the code (commented because it exceeds time limit
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Looks like it's the case. Google Trends shows high traffic associated with Shopping related keywords during Black Friday season.
Processed documentation: Google Trends shows high traffic associated with Shopping related keywords during Black Friday season.
===========================================================================================
Cleaned documentation: Compute the codebook and commitment losses for an. input-output pair from a VQ layer.
Processed documentation: input-output pair from a VQ layer.
===========================================================================================
Cleaned documentation: The decoder from the original VQ-VAE paper that. upsamples the dimensions by a factor of 4 in both. directions.
Processed documentation: The decoder from the original VQ-VAE paper that.
===========================================================================================
Cleaned documentation: An attention layer that operates on images. Args: num_channels: the input image depth. num_heads: the number of attention heads.
Processed documentation: An attention layer that operates on images.
===========================================================================================
Cleaned documentation: A layer which applies layer normalization to the. channels at each spacial location separately.
Processed documentation: A layer which applies layer normalization to the.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: take the number of decimals on the Transaction Amount field. More than two decimals may indicate transactions made overseas.
Processed documentation: take the number of decimals on the Transaction Amount field.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: These are the first 30 melanoma images with benign tumors. Let's check the distribution of values in `benign_malignant`.
Processed documentation: Let's check the distribution of values in `benign_malignant`.
===========================================================================================
Cleaned documentation: So we have a bell (Gaussian or normal distribution) of train data. What about test
Processed documentation: So we have a bell (Gaussian or normal distribution) of train data.
===========================================================================================
Cleaned documentation: Now we can train the model. (NOTE: I HAVE COMMENTED THIS BECAUSE IT TAKES A LOT OF TIME
Processed documentation: I HAVE COMMENTED THIS BECAUSE IT TAKES A LOT OF TIME
===========================================================================================
Cleaned documentation: time. STEP_SIZE_TRAIN = train_generator.n//64 STEP_SIZE_VALID = valid_generator.n//64 history = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_generator, validation_steps=STEP_SIZE_VALID, epochs=1, verbose=1).history.
Processed documentation: model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_generator, validation_steps=STEP_SIZE_VALID, epochs=1, verbose=1).history.
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed.
Processed documentation: Arguments: seed {int} -- Number of the seed.
===========================================================================================
Cleaned documentation: Constructor. Arguments: df {pandas dataframe} -- Dataframe where the data is. Expects to be one of the []-processed-seqlen128.csv files.
Processed documentation: Arguments: df {pandas dataframe} -- Dataframe where the data is.
===========================================================================================
Cleaned documentation: Usual torch forward function. Arguments: tokens {torch tensor} -- Sentence tokens. Returns: torch tensor -- Class logits.
Processed documentation: Arguments: tokens {torch tensor} -- Sentence tokens.
===========================================================================================
Cleaned documentation: As we can see that some features have dictiories. I am droping all such columns for now.
Processed documentation: As we can see that some features have dictiories.
===========================================================================================
Cleaned documentation: Again, this is not very good. Let us check the influence of going back to the north pole
Processed documentation: Let us check the influence of going back to the north pole
===========================================================================================
Cleaned documentation: Let us now train a model on the entire train set. We will gain a bit with that.
Processed documentation: Let us now train a model on the entire train set.
===========================================================================================
Cleaned documentation: Lets continue as normal. Create our train/test split and prepare for LightGBM model training.
Processed documentation: Create our train/test split and prepare for LightGBM model training.
===========================================================================================
Cleaned documentation: Pretty impressive, but still not useful without a proper visualization. Let's see in heatmap form
Processed documentation: Pretty impressive, but still not useful without a proper visualization.
===========================================================================================
Cleaned documentation: After xgb.cv is done, this section puts its output into log file. Train and validation scores
Processed documentation: After xgb.cv is done, this section puts its output into log file.
===========================================================================================
Cleaned documentation: are also extracted in this section. Note the "diff" part in the printout below, which is the
Processed documentation: Note the "diff" part in the printout below, which is the
===========================================================================================
Cleaned documentation: difference between the two scores. Large diff values may indicate that a particular set of
Processed documentation: Large diff values may indicate that a particular set of
===========================================================================================
Cleaned documentation: Define the log file. If you repeat this run, new output will be added to it
Processed documentation: If you repeat this run, new output will be added to it
===========================================================================================
Cleaned documentation: Loading data. Converting "categorical" variables, even though in this dataset they are actually numeric.
Processed documentation: Converting "categorical" variables, even though in this dataset they are actually numeric.
===========================================================================================
Cleaned documentation: There are lots of comments within the code below. I think the callback section is particularly import.
Processed documentation: I think the callback section is particularly import.
===========================================================================================
Cleaned documentation: Let's split the data into folds. I always use the same random number for reproducibility,
Processed documentation: Let's split the data into folds.
===========================================================================================
Cleaned documentation: This is where we define and compile the model. These parameters are not optimal, as they were chosen
Processed documentation: This is where we define and compile the model.
===========================================================================================
Cleaned documentation: to get a notebook to complete in 60 minutes. Other than leaving BatchNormalization and last sigmoid
Processed documentation: Other than leaving BatchNormalization and last sigmoid
===========================================================================================
Cleaned documentation: The model needs to be initialized anew every time you run a different fold. If not, it will continue
Processed documentation: The model needs to be initialized anew every time you run a different fold.
===========================================================================================
Cleaned documentation: This is where we repeat the runs for each fold. If you choose runs=1 above, it will run a
Processed documentation: This is where we repeat the runs for each fold.
===========================================================================================
Cleaned documentation: The first callback prints out roc_auc and gini values at the end of each epoch. It must be listed
Processed documentation: The first callback prints out roc_auc and gini values at the end of each epoch.
===========================================================================================
Cleaned documentation: before the EarlyStopping callback, which monitors gini values saved in the previous callback. Make
Processed documentation: before the EarlyStopping callback, which monitors gini values saved in the previous callback.
===========================================================================================
Cleaned documentation: CSVLogger creates a record of all iterations. Not really needed but it doesn't hurt to have it.
Processed documentation: CSVLogger creates a record of all iterations.
===========================================================================================
Cleaned documentation: ModelCheckpoint saves a model each time gini improves. Its mode also must be set to "max" for reasons
Processed documentation: ModelCheckpoint saves a model each time gini improves.
===========================================================================================
Cleaned documentation: will never be reached anyway because of early stopping. I usually put 5000 there. Because why not.
Processed documentation: will never be reached anyway because of early stopping.
===========================================================================================
Cleaned documentation: model instance and load the model from the last saved checkpoint. Next we predict values both for
Processed documentation: and load the model from the last saved checkpoint.
===========================================================================================
Cleaned documentation: We average all runs from the same fold and provide a parameter summary for each fold. Unless something
Processed documentation: We average all runs from the same fold and provide a parameter summary for each fold.
===========================================================================================
Cleaned documentation: We add up predictions on the test data for each fold. Create out-of-fold predictions for validation data.
Processed documentation: We add up predictions on the test data for each fold.
===========================================================================================
Cleaned documentation: Load frames from an MP4 video and detect faces. Arguments: filename {str} -- Path to video.
Processed documentation: Load frames from an MP4 video and detect faces.
===========================================================================================
Cleaned documentation: takes in wave file path. and the fig size. Default 4,4 will make images 288 x
Processed documentation: takes in wave file path.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: score on the LB. So the idea is to see which configuration quite quickly gives a good loss/accuracy,
Processed documentation: So the idea is to see which configuration quite quickly gives a good loss/accuracy,
===========================================================================================
Cleaned documentation: first_labels_set (size: 4970) : List containing only first class for each training pattern.. to be used as approximation stratification
Processed documentation: List containing only first class for each training pattern.. to be used as approximation stratification
===========================================================================================
Cleaned documentation: DataGenerator based on keras.utils.Sequence. The nice thing about it is the random part selection that works like augmentation.
Processed documentation: The nice thing about it is the random part selection that works like augmentation.
===========================================================================================
Cleaned documentation: TestDataGenerator is bad software engineering from my part... Essentially the same generator used only for inference...
Processed documentation: TestDataGenerator is bad software engineering from my part...
===========================================================================================
Cleaned documentation: You can pass the and process images directly with help of ImageDataGenerator of tensorflow utilizing Argument preprocessing_function tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=crop_and_zoom
Processed documentation: You can pass the and process images directly with help of ImageDataGenerator of tensorflow utilizing Argument preprocessing_function tf.keras.preprocessing.image.
===========================================================================================
Cleaned documentation: Set to True if we want to train from scratch. False will reuse saved models as a starting point.
Processed documentation: False will reuse saved models as a starting point.
===========================================================================================
Cleaned documentation: EasyDict allows to access dict values as attributes (works recursively. A Javascript-like properties dot notation for python dicts.
Processed documentation: EasyDict allows to access dict values as attributes (works recursively.
===========================================================================================
Cleaned documentation: Batch size --> How many images are trained at one time. Lower it if you run out of memory
Processed documentation: Batch size --> How many images are trained at one time.
===========================================================================================
Cleaned documentation: We see that 292 descriptions are missing text. Our approach is to simply fill these with the character 'X
Processed documentation: Our approach is to simply fill these with the character 'X
===========================================================================================
Cleaned documentation: Note that the new DataFrame has a MultiIndex. In the following we collapse it to a flat index
Processed documentation: Note that the new DataFrame has a MultiIndex.
===========================================================================================
Cleaned documentation: Let's join in information from `resources.csv`. We'll use [DataFrame.merge]( (which corresponds to a [SQL JOIN]( operation).
Processed documentation: We'll use [DataFrame.merge]( (which corresponds to a [SQL JOIN]( operation).
===========================================================================================
Cleaned documentation: Let's discover unique values of columns. As you can see, there are 23 columns without id and target.
Processed documentation: Let's discover unique values of columns.
===========================================================================================
Cleaned documentation: Allright! There is only ord_5 left. This feature is ordered by letters. Recap ord_5 : 192 uniques
Processed documentation: Recap ord_5 : 192 uniques
===========================================================================================
Cleaned documentation: This has duplicate ID's now. We should add the date to the id to make each row unique.
Processed documentation: We should add the date to the id to make each row unique.
===========================================================================================
Cleaned documentation: NOTE - For now we are aggregating on the mean price of each item.
Processed documentation: For now we are aggregating on the mean price of each item.
===========================================================================================
Cleaned documentation: Split the segment into _n_ parts. Calculate the entropy on each part and take the mean to reduce noise.
Processed documentation: Calculate the entropy on each part and take the mean to reduce noise.
===========================================================================================
Cleaned documentation: fname = QtGui.QFileDialog.getOpenFileName(None, "Select a MATLAB data file...", '.', filter="MATLAB data file (.mat);;All files
Processed documentation: QFileDialog.getOpenFileName(None, "Select a MATLAB data file...", '.', filter="MATLAB data file (.mat);;All files
===========================================================================================
Cleaned documentation: Calculates the FFT of the epoch signal. Removes the DC component and normalizes the area to
Processed documentation: Removes the DC component and normalizes the area to
===========================================================================================
Cleaned documentation: N.B. the sqrt of the variance is the standard deviation. So let's just get std(dy/dt) / std(y
Processed documentation: N.B. the sqrt of the variance is the standard deviation.
===========================================================================================
Cleaned documentation: Loads scans from a folder and into a list. Parameters: path (Folder path) Returns: slices (List of slices
Processed documentation: Parameters: path (Folder path) Returns: slices (List of slices
===========================================================================================
Cleaned documentation: Converts raw images to Hounsfield Units (HU. Parameters: scans (Raw images) Returns: image (NumPy array
Processed documentation: Parameters: scans (Raw images) Returns: image (NumPy array
===========================================================================================
Cleaned documentation: We can see that values for our binary features have very similar fraud rates. Let's return to plotting
Processed documentation: We can see that values for our binary features have very similar fraud rates.
===========================================================================================
Cleaned documentation: Reading an RGB image through the scipy library. Provides an array. Sintaxe: basic_readImg(directory, filename).
Processed documentation: Reading an RGB image through the scipy library.
===========================================================================================
Cleaned documentation: Displays the image at the chosen size. The image (img) should be read through basic_readImg. Sintaxe: basic_showImg(img, size=4).
Processed documentation: Displays the image at the chosen size.
===========================================================================================
Cleaned documentation: C. Exploratory Data Analysis C.1. Creating a feature dataset Putting together all the training information we have.
Processed documentation: Creating a feature dataset Putting together all the training information we have.
===========================================================================================
Cleaned documentation: Seems that transaction date is the main "culprit". Let's see what happens when we remove time stamp.
Processed documentation: Seems that transaction date is the main "culprit".
===========================================================================================
Cleaned documentation: Guess season and Defense Personnel change significantly between train and test. let's remove them and see what happens.
Processed documentation: Guess season and Defense Personnel change significantly between train and test.
===========================================================================================
Cleaned documentation: OK, AUC of 0.999261 is not too shabby eaither. Let's see what's going on here.
Processed documentation: OK, AUC of 0.999261 is not too shabby eaither.
===========================================================================================
Cleaned documentation: Now let us look at the input folder. Here we find all the relevant files for this competition.
Processed documentation: Now let us look at the input folder.
===========================================================================================
Cleaned documentation: Now is the time to build our first model. We'll use make a simple LGBM model.
Processed documentation: We'll use make a simple LGBM model.
===========================================================================================
Cleaned documentation: Here is a gratuitous embedding of YouTube video of 'The Girl From Ipanema'. For no good reason.
Processed documentation: Here is a gratuitous embedding of YouTube video of 'The Girl From Ipanema'.
===========================================================================================
Cleaned documentation: history = aggregate_transactions(historical_transactions) history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns] history
Processed documentation: = 'card_id' else c for c in history.columns] history
===========================================================================================
Cleaned documentation: authorized = aggregate_transactions(authorized_transactions) authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns] authorized
Processed documentation: ['auth_' + c if c != 'card_id' else c for c in authorized.columns] authorized
===========================================================================================
Cleaned documentation: new = aggregate_transactions(new_transactions) new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns] new
Processed documentation: ['new_' + c if c != 'card_id' else c for c in new.columns] new
===========================================================================================
Cleaned documentation: historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3']) new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3']) historical_transactions = reduce_mem_usage(historical_transactions) new_transactions = reduce_mem_usage(new_transactions
Processed documentation: historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3']) new_transactions =
===========================================================================================
Cleaned documentation: history = aggregate_transactions(historical_transactions) history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns] history
Processed documentation: = 'card_id' else c for c in history.columns] history
===========================================================================================
Cleaned documentation: new = aggregate_transactions(new_transactions) new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns] new
Processed documentation: ['new_' + c if c != 'card_id' else c for c in new.columns] new
===========================================================================================
Cleaned documentation: transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else
Processed documentation: = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else
===========================================================================================
Cleaned documentation: New Features with Key Shopping times considered in the dataset. if the purchase has been made within 60 days,
Processed documentation: New Features with Key Shopping times considered in the dataset.
===========================================================================================
Cleaned documentation: new_transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else
Processed documentation: = (pd.to_datetime('2017-06-04') - new_transactions['purchase_date']).dt.days.apply(lambda x: x
===========================================================================================
Cleaned documentation: Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded
Processed documentation: Impute any values will significantly affect the RMSE score for test set.
===========================================================================================
Cleaned documentation: Wow, this is a very balanced dataset. No surprises, since this is all presumably artificial data.
Processed documentation: No surprises, since this is all presumably artificial data.
===========================================================================================
Cleaned documentation: submission['target'] = pred_lr. submission.to_csv('submission_0.csv', index=False) submission['target'] = 0.9pred_lr + 0.1pred_lgb. submission.to_csv('submission.csv', index=False) submission['target'] = 0.8pred_lr + 0.2pred_lgb. submission.to_csv('submission_2.csv', index=False
Processed documentation: submission.to_csv('submission.csv', index=False) submission['target'] = 0.8pred_lr
===========================================================================================
Cleaned documentation: Let's now add some non-image features. We can start with sex, and one-hot encode it.
Processed documentation: Let's now add some non-image features.
===========================================================================================
Cleaned documentation: Well, the AUC hardly changed. Let's see what happens if we leave out 'month' as well.
Processed documentation: Let's see what happens if we leave out 'month' as well.
===========================================================================================
Cleaned documentation: Most of these targets are fairly self-explanatory. let's look at tehir distributions in the train dataset.
Processed documentation: let's look at tehir distributions in the train dataset.
===========================================================================================
Cleaned documentation: Now let us look at the input folder. Here we find all the relevant files for this competition.
Processed documentation: Now let us look at the input folder.
===========================================================================================
Cleaned documentation: Now let us look at the input folder. Here we find all the relevant files for this competition.
Processed documentation: Now let us look at the input folder.
===========================================================================================
Cleaned documentation: Only marginal improvement - there is a verly small bump close to 15. Can the violin plot help
Processed documentation: Only marginal improvement - there is a verly small bump close to 15.
===========================================================================================
Cleaned documentation: OK, that's much more interesting. Let's do the same thing with the test data.
Processed documentation: Let's do the same thing with the test data.
===========================================================================================
Cleaned documentation: df: pandas dataframe. cols: list of columns composing the key. name_new_col: name given to the new column.
Processed documentation: cols: list of columns composing the key.
===========================================================================================
Cleaned documentation: Now we convert the training set into a classification problem. Create a new field for class.
Processed documentation: Now we convert the training set into a classification problem.
===========================================================================================
Cleaned documentation: Can't read the image in kernal. Anyone know how? Will try and add image in comments
Processed documentation: Will try and add image in comments
===========================================================================================
Cleaned documentation: GPU version It is just function verification, not for performance evaluation. So... 1 path / 1 batch
Processed documentation: GPU version It is just function verification, not for performance evaluation.
===========================================================================================
Cleaned documentation: Lets look at the dominant words in classes. And see if we can find any correlation.
Processed documentation: Lets look at the dominant words in classes.
===========================================================================================
Cleaned documentation: Try to improve accuracy. Review the features and create new features to improve accuracy. Adjust parameters to improve accuracy.
Processed documentation: Review the features and create new features to improve accuracy.
===========================================================================================
Cleaned documentation: param sentences: list of list of words. return: dictionary of words and their count.
Processed documentation: param sentences: list of list of words.
===========================================================================================
Cleaned documentation: We should give CatBoost names of categorical features. So, it will process them not like continuous values.
Processed documentation: We should give CatBoost names of categorical features.
===========================================================================================
Cleaned documentation: You can state below link to your copy of this MMDetection repo. Add .git in the end.
Processed documentation: You can state below link to your copy of this MMDetection repo.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background. Returns run length as list.
Processed documentation: x: numpy array of shape (height, width), 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Takes a list of image paths (pathlib.Path objects), analyzes each, and returns a submission-ready DataFrame.
Processed documentation: Path objects), analyzes each, and returns a submission-ready DataFrame.
===========================================================================================
Cleaned documentation: Performs a single optimization step. Arguments: closure (callable, optional): A closure that reevaluates the model. and returns the loss.
Processed documentation: Arguments: closure (callable, optional): A closure that reevaluates the model.
===========================================================================================
Cleaned documentation: As you see, the process is really fast. An example of some of the lag/trend columns for Spain
Processed documentation: An example of some of the lag/trend columns for Spain
===========================================================================================
Cleaned documentation: Dropout_model = 0. FVC_weight = 0. Confidence_weight = 0. GaussianNoise_stddev = 0. LB = -6.
Processed documentation: GaussianNoise_stddev = 0.
===========================================================================================
Cleaned documentation: Dropout_model = 0. FVC_weight = 0. Confidence_weight = 0. GaussianNoise_stddev = 0. LB = -6.
Processed documentation: GaussianNoise_stddev = 0.
===========================================================================================
Cleaned documentation: Dropout_model = 0. FVC_weight = 0. Confidence_weight = 0. GaussianNoise_stddev = 0. LB = -6.
Processed documentation: GaussianNoise_stddev = 0.
===========================================================================================
Cleaned documentation: Dropout_model = 0. FVC_weight = 0. Confidence_weight = 0. GaussianNoise_stddev = 0. LB = -6.
Processed documentation: GaussianNoise_stddev = 0.
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: We are only going to use this to make a prediction on the val set. That's
Processed documentation: We are only going to use this to make a prediction on the val set.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: Note: the type float32 is very important. It must be the same type as the output from
Processed documentation: Note: the type float32 is very important.
===========================================================================================
Cleaned documentation: define a function to output a row containing all box info incl. confidence scores
Processed documentation: define a function to output a row containing all box info incl.
===========================================================================================
Cleaned documentation: Run this cell 5 times. On the fifth time you should get a "StopIteration" error.
Processed documentation: On the fifth time you should get a "StopIteration" error.
===========================================================================================
Cleaned documentation: print('Finish - GP MAE: %.4f, Run: %d, Best Run: %d' % (mae, run, best_iter
Processed documentation: Run: %d, Best Run: %d' % (mae, run, best_iter
===========================================================================================
Cleaned documentation: Lets validate the test files. This verifies that they all contain 150,000 samples as expected.
Processed documentation: Lets validate the test files.
===========================================================================================
Cleaned documentation: All duplicated images in the training dataset are just black images. Do they have non-null masks
Processed documentation: All duplicated images in the training dataset are just black images.
===========================================================================================
Cleaned documentation: Good! We've got five cluster again. The number of samples in each cluster is the following
Processed documentation: The number of samples in each cluster is the following
===========================================================================================
Cleaned documentation: Lets map each CPU to each variable coming out of features list. This line helps in parallel computation
Processed documentation: Lets map each CPU to each variable coming out of features list.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: It's seems that meter reading variable is heavily positive skewed with outliears. Let's fixed that.
Processed documentation: It's seems that meter reading variable is heavily positive skewed with outliears.
===========================================================================================
Cleaned documentation: The following variables are either discrete numerical or continuous numerical variables.So the will be imputed by median.
Processed documentation: The following variables are either discrete numerical or continuous numerical variables.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Divide the available space on an image into 16 sectors. In the [0] image these
Processed documentation: Divide the available space on an image into 16 sectors.
===========================================================================================
Cleaned documentation: HITS and PAGEVIEWS have very similar distributions! We could probably drop one of them...., VISTNUMBER seems pretty different
Processed documentation: HITS and PAGEVIEWS have very similar distributions!
===========================================================================================
Cleaned documentation: including in test on this run. Without these I get a 1.7681 LB score
Processed documentation: including in test on this run.
===========================================================================================
Cleaned documentation: Pageviews is too similar to page hits might be removed when training is capped. totals.pageviews
Processed documentation: Pageviews is too similar to page hits might be removed when training is capped.
===========================================================================================
Cleaned documentation: As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases,
Processed documentation: As the predicted probability approaches 1, log loss slowly decreases.
===========================================================================================
Cleaned documentation: however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications
Processed documentation: Log loss penalizes both types of errors, but especially those predications
===========================================================================================
Cleaned documentation: against the total statistics in games they play in. PIE yields results which are
Processed documentation: against the total statistics in games they play in.
===========================================================================================
Cleaned documentation: Not much correlation here so we will not use this feature in the model. Maybe next
Processed documentation: Not much correlation here so we will not use this feature in the model.
===========================================================================================
Cleaned documentation: out the null hypothesis. We will also drop FT_RATE as they doesn't seem to have much predictive power with
Processed documentation: We will also drop FT_RATE as they doesn't seem to have much predictive power with
===========================================================================================
Cleaned documentation: As we would expect and hoped for it adds no value. This was just a sanity check.
Processed documentation: As we would expect and hoped for it adds no value.
===========================================================================================
Cleaned documentation: Logistic is the winner here. XGBoost has high accuracy in training but way lower in test due to overfitting.
Processed documentation: XGBoost has high accuracy in training but way lower in test due to overfitting.
===========================================================================================
Cleaned documentation: Logistic is the winner here. XGBoost has high accuracy in training but way lower in test due to overfitting.
Processed documentation: XGBoost has high accuracy in training but way lower in test due to overfitting.
===========================================================================================
Cleaned documentation: A simple Keras implementation that mimics that of. numpy.average(), specifically for the this competition.
Processed documentation: A simple Keras implementation that mimics that of.
===========================================================================================
Cleaned documentation: Calculate spectrogram using pytorch. The STFT is implemented with. Conv1d. The function has the same output of librosa.core.stft.
Processed documentation: Calculate spectrogram using pytorch.
===========================================================================================
Cleaned documentation: Calculate logmel spectrogram using pytorch. The mel filter bank is. the pytorch implementation of as librosa.filters.mel.
Processed documentation: Calculate logmel spectrogram using pytorch.
===========================================================================================
Cleaned documentation: The final touch.. I export these bounding boxes for everyone to use in a Pandas dataframe form.
Processed documentation: I export these bounding boxes for everyone to use in a Pandas dataframe form.
===========================================================================================
Cleaned documentation: Now we can read the masks for the specific image. We have stored them as png files`.
Processed documentation: Now we can read the masks for the specific image.
===========================================================================================
Cleaned documentation: first_labels_set (size: 4970) : List containing only first class for each training pattern.. to be used as approximation stratification
Processed documentation: List containing only first class for each training pattern.. to be used as approximation stratification
===========================================================================================
Cleaned documentation: DataGenerator based on keras.utils.Sequence. The nice thing about it is the random part selection that works like augmentation.
Processed documentation: The nice thing about it is the random part selection that works like augmentation.
===========================================================================================
Cleaned documentation: TestDataGenerator is bad software engineering from my part... Essentially the same generator used only for inference...
Processed documentation: TestDataGenerator is bad software engineering from my part...
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: normalize the data (standard scaler. We can also try other scalers for a better score
Processed documentation: normalize the data (standard scaler.
===========================================================================================
Cleaned documentation: We define the default preprocessing for resnet architectures and create train and validation generators (`keras.utils.Sequence
Processed documentation: We define the default preprocessing for resnet architectures and create train and validation generators (`keras.utils.
===========================================================================================
Cleaned documentation: Let's ignore embarrasing error output for updated version of pytorch and downgrade it to ver. 1.4.
Processed documentation: Let's ignore embarrasing error output for updated version of pytorch and downgrade it to ver.
===========================================================================================
Cleaned documentation: I'll assume that the whole Brazil's population is suscetible. So I can define the following initial conditions
Processed documentation: I'll assume that the whole Brazil's population is suscetible.
===========================================================================================
Cleaned documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation. and remove words containing numbers.
Processed documentation: Make text lowercase, remove text in square brackets,remove links,remove punctuation.
===========================================================================================
Cleaned documentation: a simple product aggregate. Kaggle computing power allows us to only calculate 1-2 fields at a time.
Processed documentation: Kaggle computing power allows us to only calculate 1-2 fields at a time.
===========================================================================================
Cleaned documentation: Let's preprocess products a little bit. I borrowed some of the preprocessing from [here
Processed documentation: Let's preprocess products a little bit.
===========================================================================================
Cleaned documentation: There are some weird products that weight 42 Kilos. Check out this Exhibidor Exhibidor bimbo
Processed documentation: Check out this Exhibidor Exhibidor bimbo
===========================================================================================
Cleaned documentation: There are interesting things. There are products for which Demanda_uni_equil_sum = 0 and other fields are not equal to
Processed documentation: There are products for which Demanda_uni_equil_sum = 0 and other fields are not equal to
===========================================================================================
Cleaned documentation: Now let's look at which proucts sell by week with interactive heatmaps. Let's use our quantiles here.
Processed documentation: Now let's look at which proucts sell by week with interactive heatmaps.
===========================================================================================
Cleaned documentation: Calculate the great circle distance between two points. on the earth (specified in decimal degrees
Processed documentation: Calculate the great circle distance between two points.
===========================================================================================
Cleaned documentation: used for converting the decoded image to rle mask. Fast compared to previous one.
Processed documentation: used for converting the decoded image to rle mask.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: Feature_importances of features in cate0 and cate1 is small. Does it mean they are less important
Processed documentation: Feature_importances of features in cate0 and cate1 is small.
===========================================================================================
Cleaned documentation: First, consider these figures as categorical features. I will plot some simple bar charts using seaborn's countplot
Processed documentation: I will plot some simple bar charts using seaborn's countplot
===========================================================================================
Cleaned documentation: Plot images from Avito's image data set. Args: image_id (list): Ids of the images to plot. Returns: matplotlib axis.
Processed documentation: Plot images from Avito's image data set.
===========================================================================================
Cleaned documentation: Add a new column with the count of another one after. grouping on a set of columns.
Processed documentation: Add a new column with the count of another one after.
===========================================================================================
Cleaned documentation: Add a new column with the unique count of another one after. grouping on a set of columns.
Processed documentation: Add a new column with the unique count of another one after.
===========================================================================================
Cleaned documentation: Add a new column with the cumulative count of another one after. grouping on a set of columns.
Processed documentation: Add a new column with the cumulative count of another one after.
===========================================================================================
Cleaned documentation: Add a new column with the mean value of a another one after. grouping on a set of columns.
Processed documentation: Add a new column with the mean value of a another one after.
===========================================================================================
Cleaned documentation: Add a new column with the variance value of another one after. grouping on a set of columns.
Processed documentation: Add a new column with the variance value of another one after.
===========================================================================================
Cleaned documentation: Over sample positive events. param positive_ratio: The ratio of positive events to maintain. return: Over sampled `DataFrame`.
Processed documentation: param positive_ratio: The ratio of positive events to maintain.
===========================================================================================
Cleaned documentation: And here is the initial position of the particles in a $z$, $y$ view. Colors show number of hits.
Processed documentation: And here is the initial position of the particles in a $z$, $y$ view.
===========================================================================================
Cleaned documentation: Warning__: This next cell takes a very long time to run (>> 10 min).
Processed documentation: This next cell takes a very long time to run (>> 10 min).
===========================================================================================
Cleaned documentation: examine links between samples. left/right run edges are those samples which do not have a link on that side.
Processed documentation: left/right run edges are those samples which do not have a link on that side.
===========================================================================================
Cleaned documentation: This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True. Source
Processed documentation: This function prints and plots the confusion matrix.
===========================================================================================
Cleaned documentation: There are some buggy annonations in training images having huge bounding boxes. Let's remove those bboxes
Processed documentation: There are some buggy annonations in training images having huge bounding boxes.
===========================================================================================
Cleaned documentation: Custom Cutout augmentation with handling of bounding boxes. Note: (only supports square cutout regions) Author: Kaushal28 Reference
Processed documentation: Custom Cutout augmentation with handling of bounding boxes.
===========================================================================================
Cleaned documentation: Applies the cutout augmentation on the given image. param image: The image to be augmented. returns augmented image.
Processed documentation: Applies the cutout augmentation on the given image.
===========================================================================================
Cleaned documentation: So we have added the same group_id in the train as well as valid set. GROUP_ID = 27(for "hard_tiles
Processed documentation: So we have added the same group_id in the train as well as valid set.
===========================================================================================
Cleaned documentation: The function below check the Train, Test, and CV Scores of the trained model. Lets Check.
Processed documentation: The function below check the Train, Test, and CV Scores of the trained model.
===========================================================================================
Cleaned documentation: No fare information here! It's our job to predict the fare for each test ride.
Processed documentation: It's our job to predict the fare for each test ride.
===========================================================================================
Cleaned documentation: Objective function for Gradient Boosting Machine Hyperparameter Optimization. Writes a new line to `outfile` on every iteration.
Processed documentation: Objective function for Gradient Boosting Machine Hyperparameter Optimization.
===========================================================================================
Cleaned documentation: Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.
Processed documentation: Standard imports for data science work.
===========================================================================================
Cleaned documentation: Objective function for grid and random search. Returns. the cross validation score from a set of hyperparameters.
Processed documentation: Objective function for grid and random search.
===========================================================================================
Cleaned documentation: Random search for hyperparameter optimization. Writes result of search to csv file every search iteration.
Processed documentation: Writes result of search to csv file every search iteration.
===========================================================================================
Cleaned documentation: That was easy, so let's do another run! Same as before but with the highly collinear variables removed.
Processed documentation: Same as before but with the highly collinear variables removed.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: A helper Module that performs 2 convolutions and 1 MaxPool. A ReLU activation follows each convolution.
Processed documentation: A helper Module that performs 2 convolutions and 1 MaxPool.
===========================================================================================
Cleaned documentation: A helper Module that performs 2 convolutions and 1 UpConvolution. A ReLU activation follows each convolution.
Processed documentation: A helper Module that performs 2 convolutions and 1 UpConvolution.
===========================================================================================
Cleaned documentation: Forward pass. Arguments: from_down: tensor from the encoder pathway. from_up: upconv'd tensor from the decoder pathway.
Processed documentation: Arguments: from_down: tensor from the encoder pathway.
===========================================================================================
Cleaned documentation: NY_FREQ_IDX = 75000 the test signals are 150k samples long, Nyquist is thus 75k.
Processed documentation: the test signals are 150k samples long, Nyquist is thus 75k.
===========================================================================================
Cleaned documentation: TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.
Processed documentation: TPUEstimator also supports training on CPU and GPU.
===========================================================================================
Cleaned documentation: We want predictions for all development rows. So instead of removing rows, make them
Processed documentation: We want predictions for all development rows.
===========================================================================================
Cleaned documentation: Let's see what columns are in the main training dataset. Wow there are 122 columns, see below
Processed documentation: Let's see what columns are in the main training dataset.
===========================================================================================
Cleaned documentation: Submission generation based on encoded model predictions. Arguments: y_test_pred_rle: RLEncoded predictions. Returns: submission: generated submission.
Processed documentation: Submission generation based on encoded model predictions.
===========================================================================================
Cleaned documentation: Return padding borders in case intermediate operations on original images. are needed. Returns: self.padding_pixels: tuple of padding borders.
Processed documentation: Return padding borders in case intermediate operations on original images.
===========================================================================================
Cleaned documentation: Helper function for images padding. Arguments: img: image as np.array. Returns: img: padded image as np.array.
Processed documentation: Helper function for images padding.
===========================================================================================
Cleaned documentation: Helper function for loading image. If mask is loaded, it is loaded in grayscale (, 0) parameter.
Processed documentation: If mask is loaded, it is loaded in grayscale (, 0) parameter.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Set the index of the scaled dataset. It is the same as the original dataset
Processed documentation: Set the index of the scaled dataset.
===========================================================================================
Cleaned documentation: I used a public kernel from [@rohanrao]( as example. Replace this part with yours.
Processed documentation: I used a public kernel from [@rohanrao]( as example.
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: img: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: img: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: units : units for fully-connected layer. return : helper function for fully-connected -> batch normalization -> activation.
Processed documentation: return : helper function for fully-connected -> batch normalization -> activation.
===========================================================================================
Cleaned documentation: input_shape : input dimension of single data. classes : class number of label. return : cnn model.
Processed documentation: input_shape : input dimension of single data.
===========================================================================================
Cleaned documentation: sub_sample = pd.read_csv(main_dir + 'sample_submission.csv') sub_sample = sub_sample.drop(['EncodedPixels'], axis = 1) submission = sub_sample.merge(sub, on = ['ImageId_ClassId']) submission.head
Processed documentation: sub_sample = pd.read_csv(main_dir + 'sample_submission.csv') sub_sample = sub_sample.drop(['EncodedPixels'], axis = 1) submission =
===========================================================================================
Cleaned documentation: Set to True if we want to train from scratch. False will reuse saved models as a starting point.
Processed documentation: False will reuse saved models as a starting point.
===========================================================================================
Cleaned documentation: Set to True if we want to train from scratch. False will reuse saved models as a starting point.
Processed documentation: False will reuse saved models as a starting point.
===========================================================================================
Cleaned documentation: param inputs: input tensor. param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
Processed documentation: param drop_connect_rate: drop connect rate (float, between 0 and 1) :return: output of block.
===========================================================================================
Cleaned documentation: This function takes a row and return signal to noise. ratio accross all measurments.
Processed documentation: This function takes a row and return signal to noise.
===========================================================================================
Cleaned documentation: mask = train.query('signal_to_noise > 1')[mes_cols].apply(lambda row: np.any([np.any(np.array(c) == 0) for c in row]), axis
Processed documentation: == 0) for c in row]), axis
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Modified to add option to use float16 or not. feather format does not support float16.
Processed documentation: Modified to add option to use float16 or not.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: As you see:The Neutered pets are accepted mostly.The points should be on the Neutereds
Processed documentation: As you see:The Neutered pets are accepted mostly.
===========================================================================================
Cleaned documentation: A lot of safari user are mac user. mac user may be less possibly a fraud maker.
Processed documentation: A lot of safari user are mac user.
===========================================================================================
Cleaned documentation: I focused on browser I am a very lazy person, so I rarely update my browser. How swindlers are
Processed documentation: I focused on browser I am a very lazy person, so I rarely update my browser.
===========================================================================================
Cleaned documentation: np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:]) use with center crop
Processed documentation: np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4
===========================================================================================
Cleaned documentation: img4 = img4[s // 2: int(s 1.5), s // 2:int(s 1.5)] center crop (WARNING, requires box pruning
Processed documentation: 2: int(s 1.5), s // 2:int(s 1.5)] center crop (WARNING, requires box pruning
===========================================================================================
Cleaned documentation: TODO: There is a bug when reading the saved parquet file. Check why and fix it
Processed documentation: There is a bug when reading the saved parquet file.
===========================================================================================
Cleaned documentation: of size, else we will not be able to train the model efficiently. For that we need the
Processed documentation: of size, else we will not be able to train the model efficiently.
===========================================================================================
Cleaned documentation: I want to see only game trainsition. Therefore, I remove events whose event_count is not 1.
Processed documentation: I want to see only game trainsition.
===========================================================================================
Cleaned documentation: node_size: the number of the game paly. node_color: game type link_color: the number of the game trainsit
Processed documentation: node_color: game type link_color: the number of the game trainsit
===========================================================================================
Cleaned documentation: disregarding data order. Order does not matter since we will be shuffling the data anyway.
Processed documentation: Order does not matter since we will be shuffling the data anyway.
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: Default to NQ_DIR. You have to change it to the dir containing your own working files.
Processed documentation: You have to change it to the dir containing your own working files.
===========================================================================================
Cleaned documentation: Let's take only the 1st pred for now. We can play with multiple preds if we want.
Processed documentation: We can play with multiple preds if we want.
===========================================================================================
Cleaned documentation: Args: preds: A list of `predicted_label` as defined in `compute_predictions. Returns: A string represented a long answer.
Processed documentation: Args: preds: A list of `predicted_label` as defined in `compute_predictions.
===========================================================================================
Cleaned documentation: Args: pred: A list of `predicted_label` as defined in `compute_predictions. Returns: A string represented a short answer.
Processed documentation: Args: pred: A list of `predicted_label` as defined in `compute_predictions.
===========================================================================================
Cleaned documentation: Default to NQ_DIR. You have to change it to the dir containing your own working files.
Processed documentation: You have to change it to the dir containing your own working files.
===========================================================================================
Cleaned documentation: Since we have a custom __call__() method, we pass cycle=False when calling `super().__init__()` and
Processed documentation: Since we have a custom __call__() method, we pass cycle=
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: this happens essentially for free on TPU. Data pipeline code is executed on the "CPU" part
Processed documentation: Data pipeline code is executed on the "CPU" part
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: Since we have a custom __call__() method, we pass cycle=False when calling `super().__init__()` and
Processed documentation: Since we have a custom __call__() method, we pass cycle=
===========================================================================================
Cleaned documentation: Diregarding data order. Order does not matter since we will be shuffling the data anyway
Processed documentation: Order does not matter since we will be shuffling the data anyway
===========================================================================================
Cleaned documentation: Plot the Receiver Operating Characteristic from a list. of true positive rates and false positive rates.
Processed documentation: of true positive rates and false positive rates.
===========================================================================================
Cleaned documentation: sub = pd.DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[0 for i in X_prediction.index], 'Confidence': [10000 for i in X_prediction.index
Processed documentation: DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[0 for i in X_prediction.index], 'Confidence':
===========================================================================================
Cleaned documentation: sub = pd.DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[100 for i in X_prediction.index], 'Confidence': [5000 for i in X_prediction.index
Processed documentation: DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[100 for i in X_prediction.index], 'Confidence':
===========================================================================================
Cleaned documentation: sub = pd.DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[500 for i in X_prediction.index], 'Confidence': [1000 for i in X_prediction.index
Processed documentation: DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[500 for i in X_prediction.index], 'Confidence':
===========================================================================================
Cleaned documentation: sub = pd.DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[1000 for i in X_prediction.index], 'Confidence': [500 for i in X_prediction.index
Processed documentation: DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[1000 for i in X_prediction.index], 'Confidence':
===========================================================================================
Cleaned documentation: sub = pd.DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[2000 for i in X_prediction.index], 'Confidence': [100 for i in X_prediction.index
Processed documentation: DataFrame(data={'Patient_Week':X_prediction['Patient_Week'], 'FVC':[2000 for i in X_prediction.index], 'Confidence':
===========================================================================================
Cleaned documentation: If there are any problems, I appreciate it if you inform me of it. Thanks
Processed documentation: If there are any problems, I appreciate it if you inform me of it.
===========================================================================================
Cleaned documentation: If you split data, you can calculate faster. Accuracy would not get so worse.
Processed documentation: If you split data, you can calculate faster.
===========================================================================================
Cleaned documentation: I recommend you save both the dataset. Because, it takes long to generate these.
Processed documentation: I recommend you save both the dataset.
===========================================================================================
Cleaned documentation: Seeds basic parameters for reproductibility of results. Arguments: seed {int} -- Number of the seed.
Processed documentation: Arguments: seed {int} -- Number of the seed.
===========================================================================================
Cleaned documentation: Usual torch forward function. Arguments: tokens {torch tensor} -- Sentence tokens. Returns: torch tensor -- Class logits.
Processed documentation: Arguments: tokens {torch tensor} -- Sentence tokens.
===========================================================================================
Cleaned documentation: I don't have any idea for this complex feature. For now, I want to remove this feature.
Processed documentation: I don't have any idea for this complex feature.
===========================================================================================
Cleaned documentation: With msno library, we could see the blanks in the dataset. Check null data in application train.
Processed documentation: Check null data in application train.
===========================================================================================
Cleaned documentation: Let's see the correlations between the int features. Heatmap helps us to see this easily.
Processed documentation: Let's see the correlations between the int features.
===========================================================================================
Cleaned documentation: It is hard to see the trend for now. Let's remove the samples. (overdue
Processed documentation: Let's remove the samples.
===========================================================================================
Cleaned documentation: SmartScreen - This is the SmartScreen enabled string value from registry. This is obtained by checking in order,
Processed documentation: SmartScreen - This is the SmartScreen enabled string value from registry.
===========================================================================================
Cleaned documentation: Census_DeviceFamily - AKA DeviceClass. Indicates the type of device that an edition of the OS is intended for.
Processed documentation: Indicates the type of device that an edition of the OS is intended for.
===========================================================================================
Cleaned documentation: Census_OSBuildNumber - OS Build number extracted from the OsVersionFull. Example - OsBuildNumber = 10512 or
Processed documentation: Census_OSBuildNumber - OS Build number extracted from the OsVersionFull.
===========================================================================================
Cleaned documentation: Stratified shuffle split returns the index set of splited train and test. Using this, we can train and predict.
Processed documentation: Stratified shuffle split returns the index set of splited train and test.
===========================================================================================
Cleaned documentation: calculate mask type for stratify, the difficuly of training different mask type is different.
Processed documentation: the difficuly of training different mask type is different.
===========================================================================================
Cleaned documentation: used for converting the decoded image to rle mask. Fast compared to previous one.
Processed documentation: used for converting the decoded image to rle mask.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: The colour of monsters doesn't seem to be essential. Maybe it disturb the classification. Let's classify without colour...
Processed documentation: The colour of monsters doesn't seem to be essential.
===========================================================================================
Cleaned documentation: ip, app, device, os and channel are actually categorical variables encoded as integers. Set them as categories for analysis.
Processed documentation: ip, app, device, os and channel are actually categorical variables encoded as integers.
===========================================================================================
Cleaned documentation: This notebook requires PyTorch-Lighning. To use PyTorch-Lighning on Internet-off notebook, the following dataset is helpful pytorch-lightning 0.7.1]( by @[higepon
Processed documentation: To use PyTorch-Lighning on Internet-off notebook, the following dataset is helpful pytorch-lightning 0.7.1]( by @[higepon
===========================================================================================
Cleaned documentation: Hugging Face Transformers tokenizers must be prepared in your Kaggle dataset to use in off-line notebook. See also [berttokenizer-base-uncased
Processed documentation: Hugging Face Transformers tokenizers must be prepared in your Kaggle dataset to use in off-line notebook.
===========================================================================================
Cleaned documentation: The previous cell will save prediction results as pos_pred.csv and neg_pred.csv. Let us load them.
Processed documentation: The previous cell will save prediction results as pos_pred.csv and neg_pred.csv.
===========================================================================================
Cleaned documentation: Hope this notebook will help you all. Your upvotes and comments will greatly be appreciated. Let us enjoy NLP
Processed documentation: Hope this notebook will help you all.
===========================================================================================
Cleaned documentation: bash. cd LightGBM. rm -r build. mkdir build. cd build. cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. make -j$(nproc
Processed documentation: cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so
===========================================================================================
Cleaned documentation: Fakes always have at least some pixel-level changes. That means that all audio fakes are also video fakes.
Processed documentation: That means that all audio fakes are also video fakes.
===========================================================================================
Cleaned documentation: im: numpy array, 1 - mask, 0 - background. Returns run length as string formated.
Processed documentation: im: numpy array, 1 - mask, 0 - background.
===========================================================================================
Cleaned documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title
Processed documentation: it it's a train, needs to be passed throught this clausule: session.query(f'event_code
===========================================================================================
Cleaned documentation: Optimize rounding thresholds. param X: The raw predictions. param y: The ground truth labels.
Processed documentation: param X: The raw predictions.
===========================================================================================
Cleaned documentation: How close the predicted image is to have the right width. Less is better.
Processed documentation: How close the predicted image is to have the right width.
===========================================================================================
Cleaned documentation: How close the predicted image is to have the right height. Less is better.
Processed documentation: How close the predicted image is to have the right height.
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: main layer. Note its a keras concatenate, meaning it will merge layers of neural network
Processed documentation: Note its a keras concatenate, meaning it will merge layers of neural network
===========================================================================================
Cleaned documentation: Again, this is not very good. Let us check the influence of going back to the north pole
Processed documentation: Let us check the influence of going back to the north pole
===========================================================================================
Cleaned documentation: NOTE Even tough it is automatic, we can incorporate some manual features. IF we know some domain specific information.
Processed documentation: NOTE Even tough it is automatic, we can incorporate some manual features.
===========================================================================================
Cleaned documentation: Lighting fast!. WE do have to use .compute() method every time, and not all pandas operations are covered
Processed documentation: WE do have to use .compute() method every time, and not all pandas operations are covered
===========================================================================================
Cleaned documentation: missing entries in the embedding are set using np.random.normal so we have to seed here too
Processed documentation: missing entries in the embedding are set using np.random.normal
===========================================================================================
Cleaned documentation: No need to shuffle the data again here. Shuffling happens when splitting for kfolds.
Processed documentation: Shuffling happens when splitting for kfolds.
===========================================================================================
Cleaned documentation: set train mode of the model. This enables operations which are only applied during training like dropout
Processed documentation: This enables operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: set evaluation mode of the model. This disabled operations which are only applied during training like dropout
Processed documentation: This disabled operations which are only applied during training like dropout
===========================================================================================
Cleaned documentation: Lets look at our output. Code above is standard, initialise the object, and fit&transform the data
Processed documentation: Code above is standard, initialise the object, and fit&transform the data
===========================================================================================
Cleaned documentation: We demonstrated differnt low-pass filtered signals. You can see 10-1000 Hz can capture a lot of low frequency features.
Processed documentation: We demonstrated differnt low-pass filtered signals.
===========================================================================================
Cleaned documentation: iterate through all the columns of a dataframe and modify the data type. to reduce memory usage.
Processed documentation: iterate through all the columns of a dataframe and modify the data type.
===========================================================================================
Cleaned documentation: Now let us take 5 random assets and plot them. Note that not all assets start measurament from
Processed documentation: Now let us take 5 random assets and plot them.
===========================================================================================
Cleaned documentation: Now let us see which time span(month) has the most unusual behavior. And does it coincide with financial crisis.
Processed documentation: Now let us see which time span(month) has the most unusual behavior.
===========================================================================================
Cleaned documentation: if open price is too far from mean open price for this company, replace it. Otherwise replace close price.
Processed documentation: if open price is too far from mean open price for this company, replace it.
===========================================================================================
Cleaned documentation: TF_IDF Score. In short the most important words (not too common not to rare) That could make a difference
Processed documentation: In short the most important words (not too common not to rare) That could make a difference
===========================================================================================