1309	send batch of data
228	Create the final ensemble with the first ensemble
51	Add the points to the groups
1518	Define loading method
563	Creating the description column
501	show the graphs
457	Importing important libraries
285	Specify the target column and label
1508	loop over all training samples
209	FIND ORIGIN PIXEL VALUES
1385	suppose all instances are not crowd
1516	Detect hardware , return appropriate distribution strategy
1116	Returns the counts of each type of rating that a rater made
178	Average Price per Category
1209	Save model and weights
864	Train the model on the training data and predict the test data
65	save pneumonia location in dictionary
61	Strange , smokers etc
191	Description length VS price
447	Encoding the Regions
476	Vectorize the text
1034	Separate the columns from the index
1232	Load Train , Validation and Test data
54	Taxi trip durations
1149	Function to call to preprocess the test set
407	Process the results
1466	Turn off gradients
1330	Encodes a list of BlockArgs to a list of strings
1436	Number of Patients and Images in Training Images Folder
1808	Number of transactions
859	This creates the RandomizedSearchCV that we will use
451	Only the classes that are true for each sample will be filled in
919	POS Cash balance
1206	Initialize and train model
569	Hour of the day when the order was created
1657	Bar chart of positive , negative and neutral sentiment
1780	The wordcloud of the raven for Edgar Allen Poe
13	Load train and test data
1554	Train the model
1650	Perform clustering with resa
326	Initialize patient entry into parsed
1429	make train features
865	Cast floats of integer parameters to int
696	Exclude background from the analysis
1786	Read a picture from file
318	Code in python
440	Top 50 most commmon Paths
1563	Define Root Mean Squared Logarithmic Error
689	functions to show an image
1811	Plot the oof curve for test data
189	Length of Coms , in characters
778	Remove Outliers from the correlation matrix
198	apply the threshold on the image
735	Classify an image with different models
1735	plotting color palette for accent
704	MODEL AND PREDICT WITH QDA
1236	My kingdom for a good redistributable image drawing library
541	Find Best Grid Search CV
1652	Load the data
88	Run all models
1494	Predict if test data is available
940	sort by score
1098	create a LightGBM dataset
255	Exploring the Results
775	plot the heatmap
161	Set the mask
1130	Create Data Loader
600	create mask directory
1698	Evaluate the algorithm
1287	Show Blurry Images
1266	Test and submission
740	CALCULATE RESULTS FOR SAMPLE
1182	Computes the value of theKp
393	Distribution of the best clusters
1442	Preprocess submissions
142	get the data fields ready for stacking
93	This is the important part
1354	Fast data loading
466	Evaluate the model on validation set
1583	Feature selection by correlation
592	Read the data
163	RLE encoding for the current mask
1800	plot baseline and predictions
206	CONVERT DEGREES TO RADIANS
1769	SAVE DATASET TO DISK
1776	Importing important libraries
928	Get a submission
1301	Process test data
1708	Importing all libraries
747	Make a backbone from the given layers
333	Read in the test images
758	There are some households where the family members do not have any target
727	Cheap Data Set
429	converting categorical data to uint
1372	Make the submission
546	Use the standardized version of the pipeline
1437	Number of Patients and Images in Test Set
1399	normalizing the data
1327	Convolutions like TensorFlow , for a fixed image size
146	Number of different values
1247	to truncate it
1300	Merge the pieces into one DataFrame
350	LSVR model on selected features
1093	Load and preprocess the data
1493	Print results per epoch
1815	Create the lyft dataset
334	Looking data format and types
946	altair is a very nice plotting library by the way
777	Visualizing feature plots
552	Find best algorithm
1310	Get feature importances
1409	Reference white noise
1140	We can now plot
449	Separate the target variables
1402	Read in the exogenous dataset
664	Time Series Analysis
1573	Some categorical features
1589	to truncate it
114	the lag time series
469	Data processing , metrics and modeling
1683	contrast , etc
1804	Plotting ROC Curve
1648	Load the data
646	Plot the distribution of fixations
821	Visualizing the non limited estimator
548	Find best algorithm
135	Fitting the model
432	Iterate through test and make predictions
1161	FIND ORIGIN PIXEL VALUES
1470	standardize the data
644	Finding the target vector class distribution
435	Lets bold the string
1342	Find the mean error
1022	Find the unique columns to remove
810	Save best score , end and score
1316	Remove duplicate atoms from the data
939	Get the predictions
292	Number of test images
542	Find best algorithm
1813	Plot the evaluation metrics over epochs
505	We can also plot the joint plot of transaction revenue
1525	Span logits minus the cls logits seems to be close to the best
1796	Create a function to calculate the stationarity of the data
1103	Import the datasets
538	Find best algorithm
1529	Read candidates with real multiple processes
1197	Creating the submission file
877	bayesian and random search results
1195	Make a prediction
817	Run method for selected data
741	Transform functions for visualization
1707	Find the program
283	Sample some data
1043	Print some summary information
1010	Add to aggregation list
186	Does shipping depend of prices
1547	Update data frame
96	Save the frames before normalization
224	Find the best score
313	Convert categorical features to labels
1285	create check points path
327	Add box if opacity is present
1622	Print the feature ranking
1393	Converting categorical columns to integer type
1805	Looking at the data
1221	Find the best parameter
130	Distribution of examples
788	Visualize the importance distribution
781	Change column names
1220	Drop target , fill in NaNs
958	List of primitives
1083	Load the data
514	Summary of Wins , Losses and LOSSES
1133	Split the predictions back into a single array
23	Remove the Outliers if any
1785	Avoid division by zero by setting zero values to tiny float
1476	MAKE CUTMIX LABEL
234	Filter selected features
1396	Sorting by date
1099	predict the validation set and test set
1537	Short Math Introduction
1725	The method for training is borrowed from
1574	Read Train and Test Data
1312	Create reduced train dataframe
1777	plot the correlation matrix
1819	Join market data with news data
601	And clear the mask
890	We need to compute the correlation matrix
323	Evaluate Segmentation Model
929	Check if some values are highly correlated
6	eliminate bad rows
1478	LIST DESTINATION PIXEL INDICES
1473	MAKE CUTMIX LABEL
539	BanglaLekha Some Prediction
1025	Create a LightGBM model
1560	We can now benchmark
365	Accuracy of the model using SVR
1039	Previous applications categorical features
217	MinMax scale all scores
1280	The following example of results
611	Save images to directory
1308	send batch of data
1623	Logistic Regression on Countvectorizer
1720	SAVE DATASET TO DISK
1795	fit the model
1661	Read in the order of preference
765	Draw each size in turn
1561	Here we define the columns to use for the sigmoid model
330	Returns the batch of results
1104	Credit card balance
1594	Tokenize the sentences
1086	Check if train set is ready
1	resizing with Dicom
1226	Plotting some random images to check how cleaning works
663	Count bookings , total and year
1000	Listing custom feature names
39	Fetch and preprocess data
229	Create the final ensemble with the first ensemble
743	Pivot the new table with the previously defined columns
629	GroupingBy the US state values
490	Distribuition of the missing values
118	Visualizing Pulmonary Condition Progression by Sex
493	Applicatoin train shape after merge
1692	Apply the function on all the inputs
1755	bureau and bureau balance
175	load the images
1498	get list of decay variable name
995	Show most common client type where contract was refused
141	Plot the fraudent transactions
1557	Creation of the External Marker
1090	Convert test set predictions to rle encoding
1568	PinbaLL QuanTILE
257	Fitting and predicting
262	OSIC PFP Solution
1351	Fast data loading
973	Saving values for future use
1125	Sigmoid output of the first layer
338	plot and visualise the training and validation losses
1682	An optimizer for rounding thresholds
1080	If the number of words is zero , use the original number of words
1242	Process the submission file
866	select appropriate boosting type
433	Toxic Comment data set
1611	Distribution of categorical features
1546	Parse date column
1760	Label encoding function
1412	Number of unique classes
411	Predict on multilabel dataset
1460	checking missing data
638	Lets generate a wordcloud of the given text
1671	Preprocess the data
1375	create some missing values
1793	Plot the cross days
764	Range for image
897	Cumulative importance plot
1059	Visualize image and mask
924	Visualize the importance distribution
247	Apply exponential transf
507	Split the data into train and test set
460	This enables operations which are only applied during training like dropout
131	Prepare Testing Data
692	Using previous sucessful run
43	Remove offending day from the analysis
1204	download and save fake image
1134	Let us plot some of the validation masks at random
471	Evaluate the model on validation set
1205	Load data and describe it a bit
1789	Forceasting with decompasable model
14	Target Value Distribution
145	Read raw data
1449	create look back part dataset
1292	train model no training data
120	Lets count the words in each sentence
468	Loading the data
138	Loading necessary modules
64	Log some continuous variables
676	keep it in list for further lookups
1551	Average time series for each day of the month
1052	Fit the model to the data
487	Exploration Road Map
570	Let us now look at the Order count across the week
1370	label encoding pd district
994	Most common client type where contract was approved
438	Lets look at the data dimensions
1577	Remove continuous features
270	Light GBM filter
1481	Univariate analysis for kobecod prophet
1169	Looking at the data
1180	update the weight matrix
968	Remove low information features
497	then get reduced data
1531	Previous applications previous applications
833	Distribution of Fare
389	Get list of empty images
193	apply the blackhat
1768	shuffling the data
1349	Leak Data loading and concat
882	Set up the hyperparameters
725	Predicting on Test Set
867	extract boosting parameters from config
841	Euclidean distance by Fare Amount
956	load dataframes into respective models
1716	FUNCTIONS TAKEN FROM
110	Plotting sales volumes per year
1379	if we have an affine transformation , we need to warp it
1338	The first block needs to take care of stride and filter size increase
1323	Parameters for an individual model block
201	Lets plot some neato images
124	This will change in the future
824	Applies the cutout augmentation on the given image
1491	Compute and Output Predictions
694	remove activation layer and use losvasz loss
223	Find the best score
509	This function is used to evaluate the threshold used
392	Resize train images
1527	Computes official answer key from raw logits
1758	Relationship between applications and pos balance
918	Preparacion de datos
287	Evaluate the model
1656	written by MJ Bahmani
375	Run multiprocessing build
1540	The SMAPE learning rate is
947	Saving values for future use
511	Sea lion counts
154	Distribution of missing values
907	To reduce memory usage
1127	Read the datasets
200	Importing relevant packages
103	Split the data into training and validation set
1335	Squeeze and Excitation
1107	Load sentiment file
30	Load the data
1802	Actual images used in competition
484	import the necessary packages
340	Pad the predictions with some new columns
832	Gaussian Mixture Clustering
1538	Difference between predict and actual
985	Visualizing Bureau balance over Time
437	Show some preview of the data
1696	Function to evaluate the models
1548	add time information
337	Train the model
776	plot the graph
4	Remove Unused Columns
799	Fit the model
543	BanglaLekha Some Prediction
931	count the combinations
584	Comparing BigQuery DataFrames
1579	Loop over all the columns
1426	Print the maximum and minimum length of strings
1138	Feature extraction from SHAP
1355	iterate through all the columns of a dataframe and modify the data type
996	Load the data
317	SGD model fit
388	Function to get histogram of given image and mask
607	Return a normalized weight vector for the contributions of each class
445	Extracting informations from street features
119	Visualizing Pulmonary Condition Progression by SmokingStatus
1186	update the counts of different session types
1110	Extract processed data and format them as DFs
1512	size and spacing
642	Top most common words
117	Preprocess the test sample
102	grid mask augmentation
1196	Save the best iteration results in a dataframe
976	Sorting by relevance
1029	aggregate and preprocess the submission
1087	add coverage class
322	Voting Regressor
116	Fitting a Random Search Model
1040	Previous applications categorical features
164	Read in image
380	Read exactly one item
140	Make PyTorch deterministic
1218	Time Series Analysis
139	Computing compiler version
1382	Import the library
481	Tokenize the sentence
826	Read the image on which data augmentaion is to be performed
245	Filter Andorra , run the Model for Andorra
1166	Preparing test data
504	The above plot looks very cluttered
1185	Add to list
1217	We will now explore the titles
81	to the last cell
1268	Weighted Kappa Score
167	Only the classes that are true for each sample will be filled in
858	ECDF of Predicted and Actual Validation
1346	plot prediction for each test task
1672	Initialize patient entry into parsed
1157	numpy and matplotlib defaults
1070	converting image to RGB
647	Importing the Deep Learning Libraries
534	Find best algorithm
418	Preview of Data types
1371	Train the model
643	Any results you write to the current directory are saved as output
488	Lets look at the categorical and numerical features in the data
1475	MAKE MIXUP IMAGE
1686	Split the pixmap into two parts , the left and the right
268	Create feature matrices
1569	Initialize Bayesian Optimization
1321	Set up the configuration
614	Weight of the class is inversely proportional to the population of the class
936	Trainig and Predict by Random Search
1428	add PAD to each sentence
148	Number of click by IP
19	Imputations and Data Transformation
938	Write column names
1272	find checked pairs
1153	The above one is our final working data set for training
204	Remove other air pockets insided body
150	Plot the heatmap
1101	Fitting a GBM model
436	Show some preview of the data
1036	get average repaid vs not repaid values
1422	Merge train and test sentences
271	from functools import partial
714	Find the number of missing values in each column
1447	Create submission file
500	Separate the zone and subject id into a df
756	Visualize the distribution of poverty values
583	compute probability of each batch
1632	Returning all the data for a given province
1566	same as above
1112	extract different column types
619	get area of all contours
1252	Load Model Weights
1339	Final linear layer
1649	extracting extra features from timeseries
16	To plot pretty figures
1367	Plot the districts of all districts
1135	load the timestamps
613	Import libs and load data
1358	iterate through all the columns of a dataframe and modify the data type
212	Save mean time
275	Parameters used to train the Model
1763	Split the data into train and test set
236	Filter Spain , run the Linear Regression workflow
219	About the data
1647	show the bar chart
1775	This enables operations which are only applied during training like dropout
557	Plot the interaction counts for each feature
577	Get sample to concatenate
1238	Initialize all TensorFlow layers
431	Drop unwanted columns
702	ADD PSEUDO LABELED DATA
416	Read in the data
1298	make a submission file
540	Use the standardized version of the pipeline
1035	Remove the background
1605	Find relevant columns
1752	Create entity with given dataframe
104	Compile and fit model
1770	missing entries in the embedding are set using np.random.normal
1299	Imbalanced dataset Check
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
566	Training the model
90	fast less accurate
7	declare target , categorical and numeric columns
683	Mostly look very similar
267	Remove colleges with less than
1304	Delete temperatory model
536	Use the standardized version of the pipeline
1612	Split the train dataset into development and valid based on time
904	Clean up memory
1129	Read in the data
875	Save the hyperparameters in df
1148	Checking size of images
1413	Not in training set
1603	Missing Values Analysis
1496	List of pretrained models
305	Evaluate the model
1117	Compute QWK based on OOF train predictions
73	create network and compiler
1723	missing entries in the embedding are set using np.random.normal
1192	fill in missing values based on
1131	Pack to dataloaders
303	Set paths to train and validation directories
880	Load simple features
261	Distribution of LB score
85	Reading the data
631	prime numbers are going into account
746	get the value counts for each series
1792	Plotting cross week days
732	Read an image from the dataset
430	we need to convert categorical data to numeric data
1497	Get the model
210	Protecting Categorical Columns
724	Keep only Full Data
1146	load mapping dictionaries
1485	Collect Nq features
1271	Check the neighbors of the background
316	Finetune a model with linear SVR
1487	If a checkpoint exists , restore the latest checkpoint
332	Read in all the images
362	Read and preprocess image
844	Fit the model
50	Official distribution of time series
367	SGD model fit
680	Lets prepare our data for our model
843	Split the train data into train and validation sets
508	This function is used to evaluate the threshold used
1639	Distribution of clicks and attributed dates
1784	Compute the STA and the LTA
221	Change the highlight color
783	Fitting a model
79	Resize the image to desired dimensions
963	Load feature matrices and names
455	Draw bounding boxes on the image
408	create vocabulary count vector
942	sort by score
716	evaluate model on first part
625	Sort by day
1742	SCALE target variable
456	Load the basic librairies
48	Bar chart colors
395	BanglaLekha Confusion Matrix
816	Set the column with the maximum probability prediction
672	List models to use
1499	If a checkpoint exists , restore the latest checkpoint
1745	using sieve of eratosthenes
571	Rearrange the numbers
719	Save model and preprocess
1667	Make a submission file
818	get the value associated to each column
1504	LIST DESTINATION PIXEL INDICES
678	using outliers column as labels instead of target column
56	Modeling with Fastai Library
1444	SHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR
1427	Set values for various parameters
1624	Table of Contents
1189	construct the sets using np.hstack
1404	Summarize the results
78	save dictionary as csv file
222	LSVR model on selected features
1655	Draw the heatmap using seaborn
889	 Align the train and test sets
707	QDA and Submission File
1459	checking missing data
893	Set data structure
1241	Read a sample test image
1047	Balance in cash
1291	Add extra convolutions
1653	Target Value Analysis
1532	Choose and initialize a model
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
521	Merge the Tourney Dataframe with the Tourney Model
1362	Preparing the Data
1621	Lets see the attributes
3	Reset Index for Fast Update
1064	Generate data for the BERT model
1102	Write the submission file
403	Train the model
745	build a dict to convert surface names into numbers
883	Hyperparameters search for best model
143	FITTING THE MODEL
1544	Update data frame
1281	Load in our libraries
1757	Relationship between applications and cc balance
615	An optimizer for rounding thresholds
1038	Previous applications categorical features
633	Store the data ready for training
836	Zooming in Bounding Box
668	Loading Training and Testing Data
1511	Input layer to work with
605	L is the number of samples
1388	Add boxes if necessary
260	Normalize LB score
1506	FIND ORIGIN PIXEL VALUES
861	Split into training and testing data
1629	Get time series for all counntries
356	Read the image from image id
1165	Dataframes recording information about models
616	Change the signal values
831	Now our data file sample size is same as target sample size
1122	We define the parameters for LGBM
0	Process DICOM image and set title
622	Helper functions of this section
587	Bathroom Count Vs Log Error
1334	Expansion and Depthwise Convolution
1341	Create dataloader for displaying measurable columns
1187	Last aggregation layer
659	Feature agglomeration on CountVectorizer
952	Separate the data
1469	load the binary data
905	aggregate and preprocess the submission
1482	Setup KaggLE input directory
1046	Merge by loan
969	Split the train and test sets
347	About the data
173	This is the important part
581	Pad the batch to have a large size
1055	aggregate and preprocess the submission
686	Create image generator with train dataframe
1488	Eval data available for a single example
1443	SHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR
635	Create the fitting model
1613	Split the train dataset into development and valid based on time
1616	Computes gradient of the Lovasz extension w.r.t sorted errors
301	copy image to val folder
1322	Split the data into training and validation sets
94	Returns the unique values present in a row
1715	Ensure determinism in the results
1585	fill all na as
149	quick look at distribution by IP
932	Evaluate the model
848	Random Forest Model
1178	take a look of .dcm extension
398	BanglaLekha Confusion Matrix
786	Visualizing normalized importances
1012	Add to list
1295	Creating a dataframe for train set
499	handle .ahi files
302	Write result to file
11	Compute the STA and the LTA
218	MinMax scale all feature scores
870	Write column names
448	Updated train and test data
360	Show some example datasets
1060	The same split was used to train the classifier
951	Load simple features
1457	checking missing data
1141	We can now plot
510	process remaining batch
248	Drop the target from training
934	Run grid search and evaluate
273	get lead and lags features
1199	plot boosting performance of each fold
1453	drop rows with NaN values
1144	add the growth rate to the dictionary
649	Define the model
906	Cumulative variance explained by PCA
1033	get the parent ids
873	Write column names
1244	Load the data
913	Start with the categorical features
325	LOAD DATASET FROM DISK
972	Visualizing Bayesian Optimization Results
921	Cumulativesum of all importances
530	Find best algorithm
506	The above plot looks very cluttered
567	A simple Keras implementation that mimics that of
1067	split training and validation data
992	Relationship between cash and installments
489	Helper function with groupby
562	Checking Best Feature for Final Model
900	None of the columns must be specified
158	Converting images to grayscale
585	Get the year built , and the rest of them
480	Tokenize the text
556	Putting it all together
687	Get image and mask
654	Read test data
1106	Load metadata file
165	Detect the Threshold
1668	Total sales over store
308	Splitting train and test sets
473	Loading the data
784	Function to calculate cross validation score
312	Remove categorical columns
1640	Read train.csv file and set datatypes
1625	Some new features in the table
849	Get feature importances
834	Exponential Decay function
677	filtering out outliers
1111	Extract processed data and format them as DFs
954	define train and test sets
851	get the time spent in the app so far
127	Shortest and longest categorical features
423	Distribution of meter reading columns with different buildings type
860	This disabled operations which are only applied during training like dropout
797	convert the list of arrays to numpy array
40	Load data into Python
779	normalization of all age values
1455	inverse the order
12	This block is very different to another block
720	Load libs and funcs
1620	checking missing data
798	Create the training and testing sets
1433	Plot the link count
1435	take a look of .dcm extension
1118	Manually adjusted coefficients
1553	Average daily means per week
999	If there is currently no parent , look at the rest
1669	gather input and output parts of the pattern
558	converting to integer order
892	Split into train and test missing data
1484	Process NqL JSONL files
59	Unfreezing the model and checking the best lr for another cycle
796	set up the model
688	draw bounding box as a rectangle
828	Read the image on which data augmentaion is to be performed
1741	HANDLE MISSING VALUES
957	Relationship between cash and installments
1340	Remove the noise
1717	LOAD PROCESSED TRAINING DATA FROM DISK
55	Clusters based on distance
806	Cast floats of Hyperparameters
1123	Create lightgbm dataset
171	the last pooling layer
1673	Add box if opacity is present
277	reorder the input data
945	iteration score 两列
372	Voting Regressor
1198	We create the submission file
532	Use the standardized version of the pipeline
1249	Parse and savetrials table
670	Function to transpose the data
1578	replace NaNs in new features with np.nan
1474	This could use some refactoring .
669	Loading time series data
691	Computes gradient of the Lovasz extension w.r.t sorted errors
1113	Subset text features
1766	cross validation and metrics
863	Set up the model
516	Merge the two datasets into one
1434	Add unknown label if present
1637	count of products in device and app
1604	Remove Null values
106	load the data
1619	checking missing data
459	This enables operations which are only applied during training like dropout
1401	transform to numpy array
82	Create submission file
63	What about the value distributions in general
1179	Where were the confusion matrix made
1710	Keras Libraries for Neural Networks
41	Lets look at the missing values
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1423	Extract word list from raw text
258	Fitting and Predicting with Ridge
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
1643	Distribution of clicks and proportion of downloads by device
446	Encoding the Regions
1764	Split the data into train and test set
524	Use the standardized version of the pipeline
755	replace some mapping with float
343	code reference above
1191	predicted accuracy group
335	Looking data format and types
637	If you like it , Please upvote
1315	Number of categorical variables
52	Bar chart colors
1571	select the best model
768	Plot the walls in the target
812	Write column names
406	New Binary Features
155	Time of the experiment
1530	add the min to the dataframe
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
617	Split into train and test
1773	for numerical stability in the loss
84	Class distribution over entries
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1119	Distribution inspection of original target and predicted train and test
1729	Add train leak
1592	some config values
1405	Find the mean of all stores
698	Applying CRF seems to have smoothed the model output
25	Load train and test data
1678	convert text into datetime
1003	Load the feature matrices
216	MinMax scale all numbers
887	Get a list of the original features
1670	Set seed for reproducability
941	Run the hyperopt Function
1615	show mask class example
891	Drop the columns from train and test set
1201	Detect hardware , return appropriate distribution strategy
553	BanglaLekha Some Prediction
990	Days paid Early
1085	if resize image to original size
1380	normalize the data
549	BanglaLekha Some Prediction
660	Computes and stores the average and current value
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
177	Most common category in train set
1726	for numerical stability in the loss
923	Cumulative importance plot
1207	load and preprocess data
1344	Plot some visualization
1490	Read candidates with real multiple processes
1743	EXTRACT DEVELOPTMENT TEST
58	you can play around with tfms and image sizes
1806	Read the data
665	Time Series Analysis
1791	extracting month , day from date column
998	normalize mode counts
434	Read data and prepare some stuff
726	Lets plot some of the images
529	Find Best Grid Search CV
697	Precision helper function
572	We can look at the order count by user id
565	Predict the target using the submissions
20	Imputations and Data Transformation
391	Plotting a test image
1595	Pad the sentences
1068	Print CV scores , as well as score on the test data
737	Image of test set
1373	fitting the model
1400	transform mean to log
568	Exploratory Data Analysis
1259	Using original generator
246	Set the dataframe where we will update the predictions
1282	Split the data into train and test
1541	The SMAPE model is reviewed
661	get different test sets and process each
728	Read and combine train and test set
1044	Drop the columns from train and test sets
458	Parameters from Tilii kernel
811	Create a file and open a connection
17	Now extract the data from the new transactions
95	This is the important part
1651	Perform clustering with resa
226	Visualizing LB score vs hidden dim
414	Light curves are irregularly spaced on the time axis
708	merge with pca
249	Accuracy of the model using SVR
1053	save scores in list
679	Splitting the labels
595	Get the model and train it
377	Print RMSE and mean squared error
1326	Round number of filters based on depth multiplier
1363	Bivariate between addr
1751	Load entity into the Index
352	Read the dataset
1689	Remove rows with empty shapes
763	find and plot
1816	Read the data and Exploratory
1575	Read the data
1051	Create a LightGBM model
1297	make a prediction
464	Merge datasets into full training and test dataframe
1691	The unlifted function is borrowed from
1001	The y value must be numerical
1588	The predictions have to be ordered
1277	identify the objects by color
123	Now processing the text
738	Image of test set
197	apply the blackhat
1397	There are some weird spikes ..
122	Special Cases
760	First , look at the heads
1695	Plot a few examples
1216	Calculate the Game Time Stats
780	Exploring the results
706	MODEL AND PREDICT WITH QDA
1366	Import all districts
196	original image example
1078	Set values for various parameters
1345	Plots evolution of several models
495	Read data and merge
1164	More to come Please leave a comment as i am learning
1227	Read the data
603	Binaryarize the features
1736	Plot the distribution of numerical features
537	Find Best Grid Search CV
1142	calculate the importance df
289	Ekush Some Prediction
1115	Check if columns between the two DFs are the same
852	Plot Fare Amount versus Time Since Start of Records
1679	get some sessions information
1688	Check if the images have different shapes
232	Double check that there are no informed ConfirmedCases and Fatalities after
369	Split the trainset into train and validation sets
183	Number of Item Brands
309	Create an embedding matrix of words in the data
1552	Average daily means
1522	FIND ORIGIN PIXEL VALUES
1317	Set up the configuration
129	See sample image
280	Show the predicted data
46	Official distribution of the time series
1121	Extract target variable
1468	The feature max might be different
299	Write result to file
1706	Print the best candidates length and score
949	Visualising Learning Rate Distribution
653	Function to read training data
1283	Cleaning the data
770	Target vs Bonus Variable
1011	function to count categorical variables
105	load the data
1328	Gets a block through a string notation of arguments
1190	Codes in train and test set
1279	identify by both
291	Create test generator
1170	Feature Importance and Conclusion
1600	Visualizing the correlations
1137	Distribution of Months
348	Features with correlations
188	Most of the missing words are punctuation and upper case words
1077	Extract meaningful words from text
1089	Resize predicted masks to original image size
1523	get training data with oversampling
1175	Add the actor to the list
1451	make a prediction
1520	LIST DESTINATION PIXEL INDICES
66	load and shuffle filenames
410	Predicting with OneVsRestClassifier
503	scale pixel values to grayscale
75	create train and validation generators
590	Gaussian Target Noise
1690	Remove all zero values
1374	Drop target , fill in NaNs
1797	Plot rolling statistics
1072	load the attribute image and plot it
152	How many times , each category of clickers download the app
830	Distribution of surface The surface value
576	These hyperparameters are copied from this colab notebook
311	Selecting the target columns
87	Medical Protein Interactions
254	run hyperopt model
121	Check current coverage
1042	Sort the table by percentage of missing descending
782	Change column names
426	We transform the square feet vector into log transformation
620	get area of all contours
610	Get relative path of the file in the directory
809	Fit the model
1258	Resize images for both train and test
231	Merge train and test , exclude overlap
794	Make a dataframe with the selected features
535	BanglaLekha Some Prediction
1274	loop to find colors
461	This enables operations which are only applied during training like dropout
453	draw the line
304	Weights of the class
1239	Run the prediction
602	Read the data
439	Top 50 most commmon IntersectionID's
1091	Convert test set prediction to submission dictionary
582	Calculate the probability of each model on small batch
1223	Predicting with best params
1803	landscape correlation image
624	Grouping everything by the country
757	Draw the bar chart
101	load the image file using cv
1762	Missing Data in Dataframe
1810	Importing the library
640	Top most common words
1057	If mask is a numpy array , convert to float
1535	Read the data
1263	Using original generator
83	Training the model
160	Select all labels for plotting
1441	Preprocess test images
1296	Pixel Normalization and Image Augmentation
1026	Fit the model to the data
76	load and shuffle filenames
874	Fitting the model on train data
2	Add new Features
1471	Order does not matter since we will be shuffling the data anyway
894	Time Series Difference
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
608	Get relative path of the file in the directory
1224	select proper model parameters
298	Write result to file
33	prophet expects the folllwing label names
237	Filter Spain , run the Linear Regression workflow
295	Binary target feature
723	Sort ordinal dataset
961	now we have to compute the feature matrices
988	Plot of balance over time
987	Plot previous loans amounts
72	define iou or jaccard loss function
703	STRATIFIED K FOLD
239	Filter Italy , run the Linear Regression workflow
1065	Model Hyper Parameters
1438	Create data generator
1421	Merge train and test set
879	No significant trend is observed in above pair plot
965	just to be sane
1155	Create strategy from tpu
823	Custom Cutout augmentation with handling of bounding boxes
202	Determine current pixel spacing
993	get interesting features
1458	checking missing data
1378	Add random labels
230	Implementing the SIR model
1606	Find relevant columns
912	aggregate data from child
272	configurations and main hyperparammeters
1543	Check the SMAPE score
1432	Most commonly used titles
885	Import the library
169	Add custom Batch Normalization
1009	separate the columns with group ids
314	Drop the target from training
609	Save images to directory
766	Size is so big
1056	Train and Validation Split
1790	Importing required libraries
1253	Train the model
1151	Preprocess TR set
479	The quick brown fox jumps over the lazy dog
1778	Load the data
909	get the parent ids
1641	We can see there is no missing data
