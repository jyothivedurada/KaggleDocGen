0	Retrieving the Data
1	Assign the Labels to the Data
2	Assign the Labels with the data
3	This was copied from
4	checking missing data
5	Removing the columns with too many missing values
6	Plotting AMT Income
7	Explore the distribution of the incomes
8	Distribution of Amount Credit
9	Contract Type Analysis
10	Show the distribution of people
11	Plot the customer ages
12	Bureau and Test
13	Bureau and Test
14	is one hot encoding
15	There are some weird spikes ..
16	Previous applications cash features
17	Distribution of Positive and Negative Categorical Features
18	Read in the holdout set and test set
19	This is the important part
20	Applying PCA to hold
21	Ensemble holdout set
22	Predict PCA and Submit
23	Ensemble holdout set
24	Freeze train and valid sets
25	Predict PCA and save to csv
26	Create the models
27	Predict the probabilities of model.predict
28	Predict Test Set
29	Ensemble with submissions
30	Scale and flip
31	CALCULATE MEANS AND STANDARD DEVIATIONS
32	SMOOTH A DISCRETE FUNCTION
33	STORE PROBABILITIES IN PR
34	DISPLAY PROBABILITY FUNCTION
35	Create holdout dataframes
36	Create arrays to store results
37	split the data back into train and test
38	Use Ridge model
39	set data structure
40	split into train and valid
41	Split the train and valid sets
42	Prepare the data
43	Read data and merge
44	Previous applications numeric features
45	Previous applications categorical features
46	Count pos cash accounts
47	Count installments accounts
48	Count credit card lines
49	Load in the data
50	creating the list of unique values
51	Create the original dataframe
52	we need to predict the original dataset
53	Read in the data
54	Load the data
55	Read rolling matrix
56	Prepare submission file
57	Find centroids using binary encoding
58	Find centroids using binary encoding
59	Clusters with the best mask
60	Find centroid of other hits
61	Sort according to max
62	Plot the dots on the standard deviations
63	Make a greedy approach
64	Andrews curves of the patients
65	Plot the autocorrelation of the items
66	Lag plot of the items
67	sort data with random Normal
68	norms are based on shortest paths
69	Load Train and Test data
70	CALCULATE MEANS AND STANDARD DEVIATIONS
71	SMOOTH A DISCRETE FUNCTION
72	STORE PROBABILITIES IN PR
73	DISPLAY PROBABILITY FUNCTION
74	Calculate sales gaps
75	Visualize the products of different departments
76	Edge kernel convolutions
77	Missing Values in train dataset
78	List of missing values in the test set
79	Missing Values in Test Dataset
80	Null values and sex columns
81	Creating a function to show the distribution of Age values
82	Defining the paths
83	Show the sample images
84	Visualize Benign images
85	Visualize similar images with Malignant
86	Preprocess Test Data
87	Load the data
88	Moving Average of signal
89	Load train data
90	Load the data
91	Specify training and test parameters
92	Plot the distribution of price
93	Protein Interactions with Disease
94	Correlation between Mktres
95	Compile and visualize model
96	Early Stopping and stuff
97	Make a prediction for test data
98	fillna with mean
99	Get the names of the variables
100	Count and find peaks
101	create histogram of length
102	Understanding the Variable
103	Exploratory Data Analysis
104	Load the data
105	Function for processing images
106	Define the image transformations here
107	Train the model
108	Plot some plots
109	Change columns and rows dimensions
110	importing keras files
111	Compile the model
112	Save the model
113	load json and create model
114	Check the distribution ..
115	delete the directory
116	Function to plot images and masks
117	idea from this kernel
118	Creating tf.data objects
119	Read in libraries
120	Read in the train file
121	Run all callbacks
122	Attention for text classification
123	Access the data
124	Define ExtraTrees Classifier
125	Understanding distribution of target variable i.e trip duration
126	Plot the distribution of trip duration
127	Calculate the great circle distance between two points
128	Same for test
129	Engineer features by latitude
130	plot the important features
131	Plot the distribution of pickup hour
132	we can see the distribution of the median trip duration
133	same applies to week days
134	Any results you write to the current directory are saved as output
135	Prepare the data
136	invoke my input fn
137	Predict my input
138	Predict my input
139	Print metric info
140	create a predict test function
141	save pneumonia location in dictionary
142	load and shuffle filenames
143	split into train and validation filenames
144	if augment then horizontal flip half the time
145	add trailing channel dimension
146	add trailing channel dimension
147	create numpy batch
148	define iou or jaccard loss function
149	create network and compiler
150	cosine learning rate annealing
151	Create train and validation generators
152	load and shuffle filenames
153	retrieve x , y , height and width
154	save dictionary as csv file
155	Extract feature importances
156	Print the feature ranking
157	aggregation rules over household
158	deal with those OHE , where there is a sum over columns
159	Process to prepare the data
160	do feature engineering and drop useless columns
161	converting these object type columns to int
162	Check for missing values in the instance level
163	Function for splitting data by model
164	split into train and test
165	Building the model
166	fit the estimator
167	Extract feature importances
168	Print the feature ranking
169	aggregation rules over household
170	deal with those OHE , where there is a sum over columns
171	Process to prepare the data
172	do feature engineering and drop useless columns
173	converting these object type columns to int
174	Check for missing values in the instance level
175	split into train and test
176	Building the model
177	Train the model
178	identical to FVC estimator
179	Check for useless features
180	Train the model
181	Check for useless features
182	Make a final vote
183	Clean up words
184	Compile the model
185	Any results you write to the current directory are saved as output
186	Select feature locations for putative matches
187	Perform geometric verification using RANSAC
188	Select feature locations for putative matches
189	Perform geometric verification using RANSAC
190	Lets define some parameters for our model
191	Some Global Variables
192	Train a simple LGBM model
193	Calculate the Hann window
194	Calculate the weights for each block
195	check and reshape
196	the matplotlib way
197	Vectorize the training and test set
198	Plot the results
199	Any results you write to the current directory are saved as output
200	Load sample data
201	Word Cloud for Training
202	the matplotlib way
203	Plot the Volume field
204	the matplotlib way
205	Lets generate a wordcloud
206	the matplotlib way
207	the matplotlib way
208	the matplotlib way
209	the matplotlib way
210	Load libs and funcs
211	check unique devices
212	Add new column to dataframe
213	Plot the daily revenue
214	Keywords related to NAs
215	Impute NaNs
216	Preparing the data
217	I will replace the first hem by the rest of the data
218	How many hemorrhage types are there
219	The most common hemorrhage type
220	Train and validation split
221	Root Mean Squared Scaled Error
222	Make the custom postprocessing
223	I need a few more libraries
224	set some global variables
225	byte strings , so we need to decode it
226	find the intersection between text and selected text
227	Remove the last sequence
228	predict validation set and compute jaccardian distances
229	decode test set and add to submission file
230	GCP Project Id
231	Example of results
232	Get training statistics
233	Detect hardware , return appropriate distribution strategy
234	Load Model into TPU
235	Protein Interactions with Disease
236	Show some examples
237	Visualize few examples
238	Visualizing the examples
239	We can see there is no missing data
240	Lets see least frequent landmarks
241	An original image is shown below
242	Peek of the input data folder
243	Get all the files in my directory
244	Importing necessary libraries
245	Read the sample videos
246	print basic information on the dataset
247	print basic information on the dataset
248	Brain Development Functional Datasets
249	Initialize DictLearning object
250	Show networks using plotting utilities
251	Mean of all correlations
252	Then find the center of the regions and plot a connectome
253	Add as an overlay all the regions of index
254	print basic information on the dataset
255	Plot the intesion of the masked data
256	Visualizing the Probabilits
257	Fit the model
258	Adds the predictions to the dataframe
259	This is the metric used by Kaggle in this competition
260	Calculate energy of signal using Principal Components Analysis
261	Get a list of label descriptions
262	Back to the table of contents
263	Read the data
264	Thanks to kernal , Kindly upvote this kernal also
265	Importing the libraries
266	There is plenty of room for improvement
267	Below I am merging the datasets lengths
268	Plotting Oil Price
269	This needs to be tuned , perhaps based on amount of halite left
270	initialize the global turn data for this turn
271	filled in by shipid as a ship takes up a square
272	Do initalization things
273	we are called in competition , quiet output
274	return new Position from pos when action is applied
275	we wrap around
276	we wrap around
277	Manhattan distance of the Point difference a to b , considering wrap around
278	return distance , position of nearest shipyard to pos
279	global ship targets should already exist
280	in the direct to shipyard section
281	Not efficient for long lists
282	Now check the rest to see if they should convert
283	CHECK if in danger without escape , convert if h
284	Libraries and Configurations
285	Finalize and plot model
286	Visualizing the result
287	Heatmap of every location
288	Apply high level aggregation to train and test data
289	number of photos per dataset
290	calculate team member balance
291	get time features
292	label encode some columns
293	Tune your parameters here
294	Number of teams by match
295	Create arrays and dataframes to store results
296	Create submission file
297	several prints in one cell
298	View Available Files
299	separate train and test sets
300	Write output to file
301	Load the Data
302	Remove some images
303	Splitting Train and Validation
304	Defining label
305	Defining label
306	building and compiling the model
307	Evaluate the model
308	Evaluate the model
309	So the IDs are the same thing
310	If the function fails , use the original file
311	Make the submission
312	Get some buildings with high interest
313	plot the feature ranking
314	predict fc in the test
315	Predict on test data
316	NB of listings and bedrooms
317	Support Vector Machine
318	Submit the predictions to Kaggle
319	Load Libraries and Data
320	Load the datasets
321	CatBoostRegressor with GPU
322	This is the important part
323	Replace the Noisy Type with st
324	wild zones elevation vs wild areas
325	Plotting the result
326	Plotting a distribution of VD i.e
327	Plotting a nice heatmap
328	Visualizing the Roadways
329	Ditribution ofHD Roadways
330	Ditribution ofHD Roadways
331	Show the distribution of the highly correlated pairs
332	Visualising the forest
333	Load Libraries and Data
334	Modeling with LGBM
335	Plot the images
336	The same for test
337	load the data
338	Bone Scan for Diagnosis of Metastatic Disease
339	Here we can see the markers
340	Any results you write to the current directory are saved as output
341	Plot the images
342	cross validation and metrics
343	Ensure determinism in the results
344	FUNCTIONS TAKEN FROM
345	LOAD PROCESSED TRAINING DATA FROM DISK
346	Tokenize the sentences
347	shuffling the data
348	SAVE DATASET TO DISK
349	LOAD DATASET FROM DISK
350	The mean of the two is used as the final embedding matrix
351	missing entries in the embedding are set using np.random.normal
352	text version of squash , slight different from original one
353	The method for training is borrowed from
354	for numerical stability in the loss
355	Shuffling happens when splitting for kfolds
356	This enables operations which are only applied during training like dropout
357	Computes and stores the average and current value
358	Process to prepare the data
359	This is how our data looks like
360	Generate a word cloud image
361	Display the wordcloud
362	load the GloVe vectors in a dictionary
363	Convert values to embeddings
364	Define a simple LSTM model
365	Train the model
366	Importing the libraries
367	replace the missing values with zero
368	The score function
369	changing the data type
370	delete the duplicate rows for all the merchants
371	concat train and test
372	We define the hyperparameters for LGBM
373	Make the predictions
374	prepare preds for model training
375	split into train and test words
376	Fit the models with the features we have
377	Make a submission
378	Categorical Features Encode
379	Importing the libraries
380	Reading in the test and train datasets
381	The following was copied from
382	add averaged properties to DataFrame
383	convert lattice angles from degrees to radians for volume calculation
384	Exploring the data
385	Exploring the missing values
386	Exploring the missing values
387	define the features
388	Perform Principal Components Analysis
389	Define RMSL Error Function
390	Deep Learning Begins ..
391	Define RMSL Error Function
392	Gradient Boosting Algoritm
393	We will use LightGBM for our tests
394	CatBoostRegressor with Depth
395	Predict Potential Energy
396	We again fit the data on clones of the original models
397	This code is copied from
398	We again fit the data on clones of the original models
399	build model instances
400	Imputing missing values
401	Most of the releases are male
402	Visualize the correlation matrix
403	from this kernel
404	Join all tweets in one string
405	Plot the distribution of revenue
406	Why do the attributes skew
407	Evolution of asix basket
408	Common log transformation
409	We split the data into train and validation
410	Select Best Feature for Final Model
411	Replace the missing values with
412	Create name matrix
413	Create the confusion matrix
414	Draw the lines and labels on the axes
415	Digitize the discrete values
416	slower but stable
417	Training the model
418	Draw the lines and labels on the axes
419	Exploratory Data Analysis
420	Correlation with the values of other variables
421	Test if all indices match
422	Calculate Absolute Normalized Error
423	Prepare submission file
424	Applying the model
425	skimage image processing packages
426	Add some features
427	The following was copied from
428	Determine current pixel spacing
429	We show a few images with windowed activations
430	Remove the mask
431	Pad the image to the new dimensions
432	Importing necessary modules and Reading the data
433	Plot Heatmap of Cases
434	scaling the other variables
435	Train cloned base models
436	This implements the VotingClassifier with prefitted classifiers
437	We again fit the data on clones of the original models
438	This is a list of IDs for easier use
439	Get IDs from train , test and test directories
440	Continue the preprocessing process
441	Loading and basic exploring of data
442	Train and predict
443	Merge the text fields with the train data
444	Libraries and Configurations
445	We need to add building and weather information into training set
446	Logarithmic transform of target variable
447	encode categorical features
448	Fillna with sklearn imputer
449	We need to add building and weather information into test
450	process test data
451	Compute the predictions
452	Make the submission
453	Plot the signals
454	Define the image transformations here
455	Here is the trick
456	Preprocess the data
457	Show a sample
458	split into left and right
459	split into left and right
460	calculate modified movements
461	Split data into train and validation set
462	Importing the libraries
463	Importing important libraries
464	Importing necessary libraries
465	Load train and store data
466	Load train data
467	Read the Test dataset
468	Fit the model
469	Rank features according to their ranks
470	Plot the feature importances of the forest
471	define the layers
472	Predict Test and Submit
473	Distribution of X , Y
474	Load the neural network libraries
475	Training the Deep API
476	Save the predictions for submission
477	Load the neural network libraries
478	Training the Deep API
479	Pad the sentences
480	Applying ndimage augmentation
481	Set the ticks
482	These are the important features
483	Train the model
484	Converting image to grayscale
485	Create labels array
486	Show only the labels that are
487	Set up the mask
488	Opening the cells
489	RLE encoding for the current mask
490	Read the image
491	Detect the objects
492	function to create a dataframe of encoded images
493	Format the data
494	Load the dataset
495	Count the positive and negative examples
496	Define some regular expression patterns
497	Lemmatize the tweet word
498	Train and test split
499	This is the important part
500	Model and evaluation
501	Save the models
502	load the vectoriser model
503	Predict for Test Set
504	Combine text and pred
505	pd.DataFrame to dataframe
506	Predict Potential Energy
507	Based on Thanks
508	get the data fields ready for stacking
509	We can also display a spectrogram using librosa.display.specshow
510	Display the spectrogram using librosa.display.specshow
511	Zero Crossing Rate
512	Zero Crossing Rate
513	Visualize the spectrogram using librosa.display.waveplot
514	For every slice we determine the largest solid structure
515	Remove other air pockets insided body
516	isolate lung from chest
517	Standardize the pixel values
518	to renormalize washed out images
519	Create a pie chart
520	Qualitative and categorical columns
521	Count Events per Game
522	Common Event Count
523	Type count in test data
524	Type count and percent
525	Type count and percent
526	Overall distribution of dates
527	week of year and type
528	No of games in title
529	No null values in the data
530	The time of day definitely plays an important role
531	How many types of worlds are there
532	We can see there are no missing values
533	Exploring unique values in train labels
534	Exploring unique values in train labels
535	Infections and Fatalities
536	Using our lookup dictionaries to make simpler variable names
537	using soft constraints instead of hard constraints
538	Loop over the rest of the days , keeping track of previous count
539	Start with the sample submission values
540	loop over each family choice
541	Load the data
542	We now have something we can pass to the encoder
543	params are from the above mentioned tutorial
544	define a color mapper
545	count of features in each bin
546	Let us now look at the distribution of the nominal variables
547	Read and concatenate submissions
548	get the data fields ready for stacking
549	get the data fields ready for stacking
550	Loading older submission files
551	Out with error
552	some config values
553	fill up the missing values
554	Tokenize the sentences
555	Pad the sentences
556	plot the important features
557	Calculate the reward
558	Get the train dataframe
559	Analysis of Y values
560	Select all target features
561	Train and test data
562	This is a very performative way to prepare the data for modeling
563	Preparing the data for model training
564	Save train predictions
565	Save test prediction
566	Preparing the data for model training
567	Save train predictions
568	Save submission file
569	Preparing the data for model training
570	Save train predictions
571	Save submission file
572	Load Train and Test Data
573	We will now merge the two datasets into one
574	Plot the distributions of X , Y
575	Data Cleaning and Preprocessing Steps
576	Distribution of the Target Variable
577	First SVD component on Title
578	Deal Probability distribution for Second SVD component on Title
579	Plotting the Deal Probability distribution for Third SVD component on Title
580	Deal Probability distribution for First SVD component on Description
581	Deal Probability distribution for Second SVD component on title
582	Deal Probability distribution for Third SVD component on Description
583	Splitting the data for model training
584	Making a submission file
585	plot the feature importances of the model
586	Split the train dataset into development and valid based on time
587	Draw the heatmap using seaborn
588	Set up train and test X , Y
589	Create submission file
590	Plot the score
591	add history for each card
592	Plot the distribution of wind directions
593	Graph for Cloud Coverage
594	Plot the number of occurrences
595	Understanding price variable
596	Replace the price with a ulimit
597	Wordcloud for Display Address
598	limit y value to
599	Find columns with missing values
600	Running the Model
601	plot the important features
602	custom function for ngram generation
603	custom function for horizontal bar chart
604	Get the bar chart from sincere questions
605	Get the bar chart from sincere questions
606	Creating two subplots
607	Creating two subplots
608	Creating two subplots
609	Get the tfidf vectors
610	Fit the model with early stopping
611	Plot the distribution by days since prior order
612	Plot the number of occurrences
613	Departments distribution of the Department
614	Please upvote this kernel which motivates me to do more
615	NFL seasons and games
616	Running strategies change as the game goes on ..
617	Lets import some libraries first
618	Load train data
619	plot the important features
620	Floor We will see the count plot of floor variable
621	Now let us see how the price changes with respect to floors
622	Are there seasonal patterns to the number of transactions
623	Plot the location coordinates
624	Datatypes of columns
625	Train Set Missing Values
626	Draw the heatmap using seaborn
627	Bathroom count plot
628	using log error
629	Exploring the data
630	Exploring the geometries
631	plot the important features
632	Wordcloud on the tags
633	Load the data
634	Number of punctuations by author
635	Prepare the data for modeling
636	plot the important features
637	Get the tfidf vectors
638	Get the tfidf vectors
639	Get the tfidf vectors
640	add the predictions as new features
641	add the predictions as new features
642	add the predictions as new features
643	plot the important features
644	Confussion matrix of XGB
645	Kagglegym import ..
646	Observation and Modeling
647	Observation from Kagglegym
648	Observation and Modeling
649	Target Variable Exploration
650	Plot the distribution of q1 frequency
651	Use the most efficient algorithm
652	Draw the heatmap using seaborn
653	Load train and test data
654	Read the image
655	Get the name of the landmark
656	visualize the image
657	Load the data
658	Vectorize the data
659	vect for char
660	Identity Hate Column
661	Load the data
662	Plot the evolution of formation energy and band gap
663	Pearson correlation between variables
664	get the average degree of the lattice atoms
665	Text after preprocessing
666	Calling our overwritten Count vectorizer
667	Create the submission file
668	Submit to Kaggle
669	Concatenate X and test
670	Create submission file
671	Data loading and inspection checks
672	percentile of target values
673	Load Train and Test Data
674	Visualizing Missing values
675	Missing value treatment
676	A basic dataframe describing the missing values and unique values
677	Prepare the data for modeling
678	Submit to Kaggle
679	Load Train and Test Data
680	Imputing missing values
681	Prepare the data for modeling
682	Submit to XGBoost
683	Draw the heatmap with the heatmap function
684	define parameters for LGBM model
685	Prepare the data for modeling
686	Create submission data
687	Convert our data into XGBoost format
688	Get accuracy of model on validation data
689	Convert values to embeddings
690	Replace the actual values with inf
691	Set the values of other variables to
692	some config values
693	Tokenize the sentences
694	Pad the sentences
695	shuffling the data
696	Duplicate image identification
697	Compute phash for each image in the training and test set
698	Find all images associated with a given phash value
699	Find all images associated with a given phash value
700	If an image id was given , convert to filename
701	Apply affine transformation
702	Normalize to zero mean and unit variance
703	For each whale , find the unambiguous images ids
704	Compute a derangement for matching whales
705	Construct unmatched whale pairs from the LAP solution
706	Force a different choice for an eventual next epoch
707	Map whale id to the list of associated training picture hash value
708	Collect history data
709	Evaluate the model
710	Resize to desired size
711	TA with MLP
712	Change the image size
713	deep copy the dataframe
714	Label encode categorical features
715	Has Cabin features
716	Reading the input Files from their respective Directory
717	Downsampling NOT fraud samples
718	Plot rolling statistics
719	Plot the norm of Residues
720	Finally plot the residual distribution
721	It means that We were able to make a good model
722	Finally plot the residual distribution
723	It means that We were able to make a good model
724	Applying the model
725	Plotting a few top countries
726	Plot a progress bar
727	Plot the Fatalities of Coronavirus
728	Create a Counter with the number of Confirmed Cases
729	Plot the cumulative total of Confirmed cases
730	Plot the cumulative total of Confirmed cases
731	New Confirmed cases throughout the time
732	Create Submission File
733	Checking for missing values
734	identifying the columns to drop
735	Lets check the categorical variables
736	Number of occurrences
737	Prepare the data
738	Filling NaN values with median of the column
739	from sklearn import tree
740	use cached rdkit mol object to save memory
741	this is faster than using dict
742	Compile the model
743	Data Preprocessing and Modeling
744	Apply model to test set and output predictions
745	Apply model to test set and output predictions
746	Show predicted image
747	Print current column type
748	make variables for Int , max and min
749	Integer does not support NA , therefore , NA needs to be filled
750	test if column can be converted to an integer
751	Print final result
752	Latex tagging
753	Checking for missing values
754	identifying the columns to drop
755	Lets check the categorical variables
756	Number of occurrences
757	Prepare the data
758	Filling NaN values with median of the column
759	to set up scoring parameters
760	Latex Tag In Text
761	some config values
762	Latex tagging
763	Load the data
764	remove extra spaces and ending space if any
765	add space before and after punctuation and symbols
766	Text cleaning functions
767	Tokenize the question text
768	Load an image from a file
769	Load the images from the given path
770	Prepare the model
771	save and load best model weights
772	Run our pipeline
773	Training the model
774	Process test images
775	Create a Counter with the number of Confirmed Cases
776	Cumulative line of confirmed cases
777	Fitting the NB transformer
778	adversarial layer
779	Create dummy variables
780	Plot a histogram of the transactionDT field
781	Rate of Fraud by Product Category
782	Rate of Fraud by Card
783	Rate of Fraud by Card Type
784	Protonmail fraud and rate
785	major oes
786	check for homogeneous sets
787	Preparing the data
788	model , criterion , optimizer
789	Cyclic Learning
790	Prepare the data for modeling
791	Prepare the data for modeling
792	Have to fix this
793	Prepare submission data
794	Load the data
795	Import some libraries
796	Imputing missing values
797	Checking if any ship exists in training set
798	Now we will sort the missing values descending
799	useful dataset for training
800	one hot encode
801	here goes the separation train images into train and validation datasets
802	For any classification problem , use accuracy metric
803	One Hot Encode
804	Modifying ordinal feature
805	Replace the NaN values with cross validations
806	Merge with OHEM
807	Codes from Mario Filho
808	Reading the Validation Data
809	Quick check on test data
810	reading all submission files
811	Modifying ordinal feature
812	Replace the NaN values with cross validations
813	Merge with OHEM
814	GCP Project Id
815	Kaggle user secrets
816	Create the TPU
817	Process the image
818	If you have label in images
819	Any results you write to the current directory are saved as output
820	Minimizing the problem
821	Fit the model
822	Weight average and divide by the number of classes
823	Prepare data for modeling
824	Split the dataset into training and validation
825	Classes to use
826	p_train , p_valid , test
827	LogReg with hyperopt
828	LogReg with hyperopt
829	Create and fit the model
830	Data Parameters and Encoding
831	Dictionary to hold previous und gains
832	Move to the crime location
833	Some test data
834	Same for data
835	Add to memory
836	update target every second
837	plotly offline imports
838	load dataframe with train labels
839	plotting a pie chart which demonstrates train and test sets
840	plotting a pie chart which demonstrates train and test sets
841	Explore the data
842	plotting a pie chart
843	Get dummy column
844	fill dummy columns
845	plotting a pie chart
846	Find out correlation between columns and plot
847	get sizes of images from test and train sets
848	Function to get the labels for the image by name
849	open image with a random index
850	plot the image
851	convert rle to mask
852	visualize the image and map
853	get segmentation masks
854	plot images and masks
855	plot images and masks
856	plot images and masks
857	plot images and masks
858	get masks for different classes
859	create a segmantation map
860	Function to add labels to the image
861	get masks for different classes
862	draw the map on image
863	visualize the image and map
864	draw segmentation maps and labels on image
865	plot the image
866	Reduce learning rate on plateau
867	Instantiate the model
868	Plot the data with dots
869	for one store
870	Format the date
871	fillna numeric feature
872	Merge train and store data
873	Create inverse label map
874	encode the targets
875	Reading and preparing data
876	Resize images to desired size
877	Read the image and resize it
878	INpaint with original image and mask
879	Data transformation for thehair
880	Extract species count
881	Coverting lat and lon
882	Visualizing Ebird code
883	Categorical and numerical features
884	Get feature name and label
885	Get the correlated values
886	Creating the new dataframe
887	Load the data
888	the ONE TABLE to rule them all
889	First Order the data
890	Applying log transformation
891	Convert test set to train set
892	import the libraries
893	use this for ploting the count of categorical features
894	use this for ploting the distribution of numercial features
895	Plot avg values for each target
896	for data manipulation
897	Label encode categoricals
898	Import the Libraries
899	Find missing values
900	Check it now
901	Label encode categoricals
902	Abstract reasoning dataset
903	Show some example tasks
904	Set columns to most suitable type to optimize for memory usage
905	This is the primary method this needs to be defined
906	Parameters for DataFrame API
907	update player info
908	Applying the mask
909	I hope it will help for you to create more accurate predictions
910	for future Affine transformation
911	Need to send lazy defined parameter to device ..
912	Depends on train configuration
913	Preprocessing with nltk
914	Visualize the words in the corpus
915	Setting up a validation strategy
916	Reshape the data
917	Transform training set
918	Transform test for submission
919	Visualize the images in the Batch
920	Visualize the test images
921	Transform training set
922	Transform test for submission
923	Visualize the images in the Batch
924	Visualize the test images
925	Concatenate train and test
926	Set some parameters
927	All comments must be truncated or padded to be the same length
928	Factorize categorical columns
929	Merge news and unstacked assets
930	Standard Deviation of the Time Series
931	Drop columns that are not features
932	Join market and news frames
933	Scaling the total
934	fold the sentences
935	Factorize categorical columns
936	Merge news and unstacked assets
937	Drop columns that are not features
938	Join market and news frames
939	Merge the results
940	add permutation to the data
941	Read in the data
942	Clearing first and last day from the data
943	Transfer calendar data
944	Number of words
945	Relationship between popularities and revenue of a movie
946	Training and Validation Data
947	Create Best Model
948	Evaluation of Validation Set
949	Create a summation
950	Age distribution of the trainset
951	Age and Smoking Status
952	Create a pie chart
953	Relationship between Percent and FVC
954	Determine current pixel spacing
955	For every slice we determine the largest solid structure
956	Save Train Data
957	Read test data
958	Importing the libraries
959	Only load those columns in order to save space
960	and reduced using summation and other summary stats
961	Number of teams by Date
962	Top LB Scores
963	Create Top Teams List
964	Count of LB Submissions that improved score
965	Preprocess the data
966	loop through all the attributes
967	the data mask
968	make predictions on the sets
969	Ensure determinism in the results
970	input validation sample
971	Show the heatmap
972	Importing necessary libraries
973	Shifting time axis
974	Shifting time axis
975	Shifting time axis
976	Add Gaussian Noise
977	Add Gaussian Noise
978	Defining tranformations using albumentations
979	ROC Curve and AUC
980	Calculate AUC
981	using keras tokenizer here
982	zero pad the sequences
983	create and start training of the model
984	load the GloVe vectors in a dictionary
985	create an embedding matrix for the words we have in the dataset
986	Create my own word embedding
987	Create my own word embedding
988	Create the model
989	Create my own word embedding
990	Machine Learning Specific
991	Load text data into memory
992	Tokenize the sentences
993	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
994	Sequence input word ids
995	Adding percents over bars
996	Define a linear layer
997	Set some parameters
998	setting the model
999	Train the model
1000	configurations and main hyperparammeters
1001	To use this feature we follow the following steps
1002	Define Adam optimizer
1003	lr scheduler
1004	history for learning curve
1005	Visualizing Some Images from Cover Section
1006	Check the algorithm
1007	Check the algorithm
1008	Merge with gk
1009	Reading the Data
1010	Performing some cleaning in the commnet text using regular expression
1011	Example of sentiment
1012	MosT common positive words
1013	MosT common negative words
1014	MosT common words
1015	Data loaded to the kernel
1016	This Function Saves model to
1017	Load the model , set up the pipeline and train the entity recognizer
1018	Returns Model output path
1019	Returns Trainong data in the format needed to train spacy NER
1020	Training for Positive and Negative tweets
1021	Read the train , test and sub files
1022	Make a dictionary for fast lookup of plaintext
1023	Same as above for plaintext
1024	Add the citation ID to the dataframe
1025	from util import
1026	Create cipher dictionary
1027	HANDLE MISSING VALUES
1028	SCALE target variable
1029	EXTRACT DEVELOPTMENT TEST
1030	FITTING THE MODEL
1031	Correlations of stopwords
1032	XG Boosting Feature Importance
1033	Breakdown of this notebook
1034	Dipole moment along X and Y axis
1035	Plot the distribution of potential energy for each type
1036	Define function to determine if outlier values are highly spread out
1037	Loading the necessary libraries
1038	Load test tasks
1039	Run these tasks
1040	Plotting the distribution of matrix means
1041	Useful function for csv.writer
1042	Flatten the list of predictions
1043	Prepare to start
1044	Change the paths
1045	Product Classes
1046	Sales by domain
1047	Breaking domain the purchaser domain
1048	More To Come
1049	Infections and Fatalities
1050	Credit card vs debit
1051	Credit card with credit card
1052	For data augmentation
1053	Setting X and y
1054	Train the model
1055	plot the important features of the model
1056	plot accuracy vs batch number
1057	Plot the model loss
1058	Set some parameters
1059	Number of characters in the sentence
1060	words in clean text
1061	Average Word Length
1062	Tokenize the sentences
1063	text version of squash , slight different from original one
1064	Save the Dictionary
1065	Loading the modules
1066	This is a list of the important features
1067	Load the data
1068	You can check the Google API Key
1069	This is the most accurate and convinient of them
1070	Print the error metrics
1071	create a dictionary for fast lookup of ebird code
1072	How we train the model
1073	Cross entropy loss
1074	Visualizing the test set
1075	ABOUT THE COMPETION
1076	Function to remove any numbers
1077	Replace all exlamation marks
1078	Replaces repetitions of exlamation marks
1079	Replaces repetitions of question marks
1080	Replace elongated words with single words
1081	Define the neural network
1082	Split the data
1083	Change the bar mode
1084	Change the learning rate
1085	Change the bar mode
1086	Change the learning rate
1087	Change the bar mode
1088	Change the learning rate
1089	Predict on train and test set
1090	Change the bar mode
1091	Change the learning rate
1092	Change the bar mode
1093	Change the learning rate
1094	Change the bar mode
1095	Change the learning rate
1096	Change the bar mode
1097	Change the learning rate
1098	Predict on train and test set
1099	The usual suspects
1100	Define some signal parameters
1101	Extra signal processing
1102	Processing the signal list
1103	Calculate the standard deviation
1104	For data augmentation
1105	Preprocess the data
1106	joint plot of perm entropies
1107	Display the joint plot
1108	DISPLAY TARGETS
1109	the joint plot of the app logits
1110	the joint plot of the app logits
1111	Linear Regression with log function
1112	Any results you write to the current directory are saved as output
1113	Joint plot of the target values
1114	Joint plot of katz fd and time to failure
1115	Load the libraries
1116	Extra signal processing
1117	Median of absolute values
1118	Filter out low pass signal
1119	Define some signal parameters
1120	Extra signal processing
1121	Processing the signal list
1122	Calculate the standard deviation
1123	For data augmentation
1124	Preprocess the data
1125	Calculate the signal using FFT or Welch algorithm
1126	spectral entropy plot
1127	Display the joint plot
1128	DISPLAY TARGETS
1129	Almost no difference
1130	Visualize the distribution of sample entropies
1131	Display the joint plot
1132	Add the slope and intercept to the trend
1133	Plot the joint graph
1134	Plot the joint plot
1135	Read in the data
1136	IMSE Loss Vs Model
1137	Written by Seaborn
1138	Read and shuffle filenames
1139	The magic happens here
1140	Prepare train and test targets
1141	Visualize few samples of current training dataset
1142	Constants and Directories
1143	Load the data
1144	Update gleason score
1145	Make a random sample
1146	Cross entropy loss
1147	configurations and main hyperparammeters
1148	Data Augmentation using sns
1149	Yards plot
1150	Data Augmentation using sns
1151	Density plot of S
1152	create a list of dictionaries with the values of the categorical features
1153	Convert categorical features to indices
1154	Get numerical features
1155	Build the full graph
1156	Compute mean and standard deviation
1157	Wordcloud of all comments
1158	Average word frequency vs country
1159	Compound sentiment
1160	Toxicity vs. Toxicity
1161	Visualizing Flesch reading ease
1162	Flesch Reading Ease
1163	Explore the distribution of automated readability
1164	automated readability vs toxicity
1165	Pie chart of labels
1166	Define helper functions and useful vars
1167	Create fast tokenizer
1168	create fast tokenizer
1169	Build datasets objects
1170	Load model into the TPU
1171	I save this in a callback
1172	Fitting on the new data
1173	Any results you write to the current directory are saved as output
1174	Finally train the model
1175	Any results you write to the current directory are saved as output
1176	Finally train the model
1177	Model initialization and fitting on train and valid sets
1178	Finally train the model
1179	Distilbert model initialization
1180	Finally train the model
1181	Constants and Directories
1182	Defining data path
1183	Loading the training images refer
1184	Channels Value Distribution
1185	Red Channel Values
1186	Green Channel Values
1187	Blue Channel Values
1188	Define helper functions and useful vars
1189	Exploratory Data Analysis
1190	Define learning rate scheduler
1191	Constants and Directories
1192	Cross entropy loss
1193	Finally , we will load the data
1194	Open slide and mask
1195	Set some parameters
1196	create path dict
1197	BCEWithLogitsLoss
1198	Target Mean Encoding
1199	Create dataset objects
1200	define training and validation dataloader
1201	Training the XLA model
1202	Look at Numpy Data
1203	I Visualization Check Distribution of comment Lenght and Word Numbers
1204	define training and testing sets
1205	CV scores and stdev
1206	Training the model
1207	I Visualization Check Distribution of comment Lenght and Word Numbers
1208	visualize the distribution of sincere and insincere in the raw data
1209	So the classes are the same
1210	Tokenize the question text
1211	num of sentences
1212	Insincere Texts
1213	sincere questions have multiple answers
1214	Defining auc score
1215	Get the coordinates of the rotated points
1216	in the direct to math
1217	See sample labels
1218	See sample labels
1219	Frame size and length
1220	get length of video
1221	Exploratory Data Analysis
1222	Standard plotly imports
1223	Binary features inspection
1224	Modeling with LGBM
1225	Forward pass
1226	Plot the distribution of yaw
1227	Networks and Object Dependencies
1228	Check dimensions of points
1229	Load object from file
1230	get the reference data
1231	get sample data token
1232	Move the particles from car to engo
1233	Get the car from the current spectrogram
1234	get time lag
1235	If there is a previous point in the sample , remove it
1236	Remove the points from the path
1237	Calculate distance Chanran Kim way
1238	We can simply return the instance without any change and
1239	written by MJ Bahmani for binary data
1240	Read data and find points
1241	Default empty class
1242	Get valid states for the class
1243	Return the corners of the graph
1244	Draw the sides
1245	Draw the lines
1246	Loads database and creates reverse indexes and shortcuts
1247	Initialize map mask for each map record in turn
1248	Load the table
1249	Store the mapping from token to table index for each table
1250	Get the index of a given table
1251	Update boxes based on predictions
1252	Get all the boxes found in the future
1253	Calculate the rotation of the track
1254	ego to sensor
1255	Get the sample annotation record
1256	Get the bounding box of the current sample annotation
1257	Pytorch lightning Dataset class
1258	Analyze the class balance
1259	count unique attributes
1260	radar plots
1261	plot the radar data
1262	The function to crop the image
1263	Check the lidar spectrogram
1264	Get the sample data
1265	Get aggregated point cloud in lidar frame
1266	limit visible range
1267	Get the sample data
1268	Get aggregated point cloud in lidar
1269	Limit visible axes
1270	Get the sample data
1271	Get sample data
1272	Get the sample info
1273	Show the video
1274	Get sample data
1275	Check if the image exists
1276	Get the mask for the map
1277	get all sample records
1278	pose of the map
1279	create plot with open map
1280	Show the scene
1281	Render the sample data
1282	My first sample
1283	Test Data Analisys
1284	Remove Drift from Training Data
1285	to set up scoring parameters
1286	Loading the needed libraries
1287	Label encode the categorical features
1288	Get the filter coefficients
1289	Get the filter coefficients
1290	Get the filter coefficients
1291	Filter with low pass Butterworth
1292	to set up scoring parameters
1293	Applying Kalman filter
1294	to set up scoring parameters
1295	Applying Kalman filter
1296	Loading the data
1297	Any results you write to the current directory are saved as output
1298	convert text into datetime
1299	get some sessions information
1300	the time spent in the app so far
1301	the accurace is the all time wins divided by the all time attempts
1302	Add some features
1303	Load all the data as pandas Dataframes
1304	Start by Looking at Historic Tournament Seeds
1305	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1306	PRINT CV AUC
1307	Defining the Gini metric
1308	just to visualize
1309	Perform the groupby
1310	Loading Training and Testing Data
1311	Remove overlap between train and test set
1312	Create aggregated dataframe
1313	Matching function between the ISO code and country names
1314	Hover over the country fig
1315	Hover over the country fig
1316	Plotting Fatalities Over Time
1317	DEALING WITH THE FOLLOWING FEATURES
1318	Matching function between the ISO code and country names
1319	Plot the graph
1320	prepaing with countries
1321	Replace Hits by
1322	Plot Confirmed Cases
1323	Validations with country info
1324	get the format of the series
1325	Define a model for latent features
1326	Preprocess theembedded matrix
1327	Add padding to the input
1328	Add positional info
1329	Add some modules
1330	Apply revive dead entries per encoder
1331	This implementation was copied from it is MIT licensed
1332	Base Transformer for LSTM
1333	Pixel CNN Model
1334	Define the layers
1335	Apply all the layers
1336	Lets use this library for training
1337	Train and Test Data
1338	define Gini metric
1339	Change column name
1340	An optimizer for rounding thresholds
1341	Adversarial ground truths
1342	Train the critic
1343	change the weights of critic layers
1344	SAVE Weights
1345	Import the necessary libraries
1346	Function to generate sentence strings
1347	Visualize Flesch Reading Ease
1348	All the questions are insincere
1349	Instantiate the class
1350	Latent Dirichilet Allocation
1351	Example of Selected Topic
1352	Prepare the plot
1353	Prepare the insincere questions vectorized
1354	Loading an aldfly dataset
1355	Example of results
1356	and it is for the example file
1357	function from EDA kernel
1358	more functions from LightGBM baseline
1359	Standard plotly imports
1360	Importing the librarys and datasets
1361	importing keras files
1362	Create the tokenizer
1363	Print the final result
1364	Training the Model
1365	Add test predictions
1366	Sort the table by percentage of missing descending
1367	Print some summary information
1368	Embedding the html string
1369	Benign image viewing
1370	added the grid lines for pixel purposes
1371	BY SERGEI ISSAEV
1372	added the grid lines for pixel purposes
1373	Finding unknown region
1374	Finding unknown region
1375	The basic structure of model
1376	Get all the possible combinations of products
1377	id value insertion
1378	FFT to filter out HF components and get main signal profile
1379	If you like it , Please upvote
1380	several prints in one cell
1381	shape of data
1382	Plot the releases by year
1383	Plot the movie popularity
1384	Plot of movie releases by day of month
1385	Distribution of Release Day of Week
1386	using sieve of eratosthenes
1387	Building Vocabulary and calculating coverage
1388	Adding lower case words to embeddings if missing
1389	Cleaning contractions using regular expression
1390	Building Vocabulary and calculating coverage
1391	Cleaning contractions using regular expression
1392	Turning our data into meaningful indices
1393	Using wavelet denoising
1394	left seat right seat
1395	Time of the experiment
1396	Galvanic Skin Response
1397	Read arguments for LGBM
1398	Load the data
1399	Plot rolling statistics
1400	Plot the residuals
1401	Function to solve the problem
1402	Now we can solve the problem
1403	Running the XGBRegressor Algorithm
1404	The usual suspects
1405	Plot the number of images imbalanced
1406	How many cases are there per image
1407	How is Pneumonia
1408	What is the age distribution by gender and target
1409	What are the areas of the bounding boxes by gender
1410	How is the pixel spacing distributed
1411	How are the bounding boxes areas distributed by the number of boxes
1412	Plot the distribution of black pixels
1413	How does the aspect ratio look like
1414	Linear Discriminant Analysis
1415	Linear Discriminant Analysis
1416	Train the model
1417	Calculate the right child id
1418	Define the loss function
1419	get the sum of gradients and hess
1420	Default empty parameters
1421	Get the gradient of each feature
1422	update loss with gradient
1423	Weight the feature by each node
1424	update the node with the feature id , and the feature value
1425	Create the node
1426	get the node and feature
1427	create the node ids data
1428	get the node and feature
1429	Returns the string representation of the parameters
1430	Calculate the right child id
1431	Create a new node
1432	get the sum of gradients and hess
1433	Calculate the loss
1434	deep copy the gradient
1435	Weight the feature by each node
1436	update the node with the feature id , and the feature value
1437	Create the node
1438	get the node and feature
1439	create the node ids data
1440	get the node and feature
1441	Returns the string representation of the parameters
1442	select the sample image
1443	Load and augment the image
1444	Heatmap of the correlations
1445	Load libs and funcs
1446	Calculate weighted average
1447	Helper functions of this section
1448	The usual suspects
1449	Back to training
1450	Apply the trainer to the observations
1451	Reset everything to initial state
1452	Append to .log
1453	Get the feature ranking
1454	times faster than the parallelized kernel
1455	Filter out features
1456	Check if the cut point gain is high enough
1457	Sort by boundary points
1458	Pick the best candidate for the feature
1459	Process the cut points
1460	sort by cutpoint
1461	Write the outputs to a file
1462	Fit the model
1463	Plot RFECV graph
1464	Save the ranking
1465	parameters for LGBM model
1466	parameters for LGBM model
1467	Run the random search
1468	We add up predictions on the test data for each fold
1469	Here we average all the predictions and provide the final summary
1470	Save the final prediction
1471	Rename to fit the model
1472	XGBOOST Best parameters
1473	These are all in one string
1474	Create MTCNN and Inception Resnet models
1475	Loop through frames
1476	Resize frame to desired size
1477	When batch is full , detect faces and reset frame list
1478	Read and preprocess test videos
1479	MTCNN model in action ....
1480	MTCNN model in action ....
1481	Frontal face detector
1482	MTCNN face detector
1483	Feed forward layer
1484	define discriminator and optimizer
1485	Predict and Submit
1486	Save images to zip file
1487	Many people are interested in losing weight
1488	Outputs for plotting
1489	Lookup Item Time Series
1490	Visualize the sales
1491	Create the item lookup table
1492	regularization of variable
1493	Calculate the count of duplicates
1494	Show the dendrogram
1495	Clusters per item
1496	The item lookup is shown below
1497	Let us plot this now
1498	Find the differences
1499	Create the Dict that shows the changes
1500	Using the same basis as above
1501	There are some weird spikes ..
1502	create card ID
1503	Importing relevant packages
1504	Audio paths and random paths
1505	Read all files in the folder list
1506	Comparing Spectrograms for different birds
1507	Plot the sample audio
1508	Plot the Test Sounds
1509	Applying a log transformation on the test signal
1510	Make zip files
1511	Model with Logistic Regression
1512	plot the scatter plot
1513	Predict Potential Energy
1514	Draw the contours of the data
1515	Define some Global Variables
1516	replace multiple spaces with one
1517	Tokenize the sentences
1518	Define the objective function
1519	Compile the model
1520	Inference and Testing
1521	Create the labels matrix
1522	Read raw audio samples
1523	L is the length of the wavelet
1524	Only the classes that are true for each sample will be filled in
1525	Return a normalized weight vector for the contributions of each class
1526	count unique values in each column
1527	Find Var Stats
1528	Find the mean mask
1529	Picking up the right plot
1530	View Available Files
1531	Custom GAP Dataset class
1532	Get the feature importances
1533	matplotlib offline imports
1534	writing an edge in the notebook
1535	neighborhoods with boro
1536	writing an edge in the notebook
1537	zoom in image
1538	merge oof scores and default mean
1539	Mapping of categorical features
1540	Importing necessary libraries
1541	Importing utils from sklearn
1542	Deep Learning Begins ..
1543	Read data from the CSV file
1544	Since the labels are textual , so we encode them categorically
1545	MinMax and Standards
1546	Stratified Split stuff ..
1547	Fit the model with early stopping callback
1548	summarize history for loss
1549	summarize history for accuracy
1550	load the additional data as well
1551	add some features
1552	Set up the session
1553	prepare train and test models
1554	Scaling the target data
1555	Same as before
1556	submitting the predictions to the competition
1557	Only the classes that are true for each sample will be filled in
1558	Wrapper for fast.ai library
1559	Special thanks to
1560	If you like it , Please upvote
1561	Table of Contents
1562	Table of Contents
1563	Data loading and short overview
1564	Detecting NaN values in train and test data
1565	Detecting NaN values in data
1566	Detecting NaN values in data
1567	Detecting NaN values in data
1568	Submission time in days
1569	Make dummy categorical features
1570	get common values in train and test set
1571	plot the text columns
1572	plot feature
1573	get all features
1574	Plot the distribution of the features
1575	calculate the vocab length
1576	using QuOTE ALL
1577	This was copied from
1578	Logarithmic transform for y
1579	grid search for logreg
1580	prepare test data for model training
1581	Checking the contents of data
1582	Breakdown of this notebook
1583	for model training
1584	splitting the data into train and val sets
1585	load the best model
1586	Run the model
1587	Checking the contents of data
1588	Maping the category values in our dict
1589	concat train and test
1590	Split the target variable into train and test
1591	Transforming ordinal Features
1592	Lets import some libraries first
1593	What is Melanoma
1594	Read in the data
1595	Reading in the data
1596	Preparing the test data
1597	Plot the correlation matrix
1598	Male and Female Sex
1599	Smoking Status Viz
1600	Smoking Status Viz
1601	Preprocess the data
1602	for model training
1603	splitting the data into train and val
1604	load the best model
1605	Test the model using the loader
1606	This didnt give good result need to correct and get the right scheduler
1607	the func is from
1608	size of images
1609	Size Info Analysis
1610	Using skimage to do color conversion
1611	Save original image with new compression
1612	Importing necessary libraries
1613	define serial iterators
1614	model.setAccuracy
1615	load an image
1616	create a list of polygons from the image
1617	Plot the polygons contained in the list
1618	Creating the polygons
1619	Plot the polygons contained in the list
1620	Images can be found in attached datasets
1621	Send the sampled results to the kernel
1622	Train the model
1623	load the data
1624	Create our with pandas dataframe
1625	Custom function for categorically encoding a column
1626	Replace categorical features by numeric features
1627	Number of items per department
1628	Total Sales by Category
1629	Visualizing Sales by State ID
1630	Plotting the sales distribution by store
1631	sale per item per day over time
1632	Preparing submission file
1633	Finally prepare the submission file
1634	for evaluation data
1635	Concatenate multiple files into one
1636	Save submission file
1637	in each column , replace the missing values by mode
1638	This code is copied from
1639	Separating target and ids
1640	Calculate the entropy of each fold
1641	Visualizing the entity recognition
1642	plot the objects
1643	Importing relevant packages
1644	Importing the librarys and datasets
1645	Quick data analysis using scipy.misc
1646	Open the image file
1647	data and mask
1648	Calculate the normalized FFT of a signal
1649	Define some required features
1650	Function to calculate Petrosian Daily
1651	set the delta
1652	Theoretical Katz loss
1653	Log of the maximum
1654	calculate the trend for the sample
1655	Calculate the fluctuations
1656	Remove all zero values
1657	Calculate segment indices
1658	make the synthesized signal
1659	Add Pearson correlation to the data
1660	Function to replace zero runs
1661	The following was copied from
1662	Function to get file paths with extension
1663	Process to prepare the data
1664	Display MAE and MSE losses
1665	Distribution of Age
1666	Age vs SmokingStatus
1667	Pulmonary Condition Progression by Sex
1668	Pulmonary condition progression by Smoking Status
1669	Calculate eval metric
1670	Preparing the submission file
1671	as test data is containing all weeks ,
1672	fill the df with the baseline FVC values
1673	same as above
1674	Data transformation pipelines
1675	define which attributes shall not be transformed , are numeric or categorical
1676	OneHot Encodes categorical features
1677	APPLY DEFINED TRANSFORMATIONS
1678	Combine Score and qloss
1679	ROI breakdown by patient type
1680	build and train model
1681	predict Train and Test
1682	Exponential Decay function
1683	Please consider upvoting this kernel if you find it helpful in any way
1684	Machine Learning Specific
1685	Plot the relationships between variables
1686	show the graph
1687	Short and Long ordinal features
1688	Load the packages
1689	Cluster Id K means
1690	Thanks to kernal , Kindly upvote this kernal also
1691	Preprocess before and after
1692	Read in the failure regions
1693	Region of interest
1694	Check if the input image is an RGB image
1695	Visualize DCT Coefficients
1696	Create the dataset
1697	Wrapper around Torch Dataset to perform binary classification
1698	The binary encoding of the values
1699	Train the model
1700	define eval functions
1701	Train the model
1702	Fill in missing values with mean
1703	Build the dataloader
1704	load the dict
1705	update model parameters
1706	set counter to true
1707	Weighted Random Sampler
1708	Train the model
1709	Kaggle Datasets Access
1710	Reshape and return image
1711	use this for training
1712	numpy and matplotlib defaults
1713	size and spacing
1714	Make the figure layout compact
1715	Peek at training data
1716	peer at test data
1717	apply to image
1718	Random horizontal flip
1719	CUSTOM LEARNING SCHEUDLE
1720	Calculate learning rate schedule
1721	Plot the learning rate schedule
1722	Define the model
1723	Xception model
1724	Define our Inception model
1725	Define the model
1726	Exploratory Data Analysis
1727	SAVE BEST MODEL EACH FOLD
1728	Submit to Kaggle
1729	plot distribution of target variable
1730	Load the dependancies
1731	Show some DCM images
1732	Visualize the distribution of the pixel values
1733	Visualize the histograms
1734	Show the DICOM images
1735	Extract DICOM data
1736	pivot to have one row per image
1737	Put all values into the mmap
1738	Define the layers
1739	Divide the values into channels
1740	Code from Whale Classification Model
1741	Build the graph
1742	Set up a saver
1743	A sanity check before training
1744	Build the graph
1745	Deep check the model
1746	HANDLE MISSING VALUES
1747	SCALE target variable
1748	EXTRACT DEVELOPTMENT TEST
1749	FITTING THE MODEL
1750	Read training , test and sample submission data
1751	Predict on test set
1752	Preprocess train data
1753	process validation set
1754	load best model scores
