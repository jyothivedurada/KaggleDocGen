26	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , train . columns ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 10 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 20 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1203	train = train . sort_values ( 'visit_date' ) target_train = np . log1p ( train [ 'visitors' ] . values ) col = [ c for c in train if c not in [ 'id' , 'air_store_id' , 'visitors' ] ] train = train [ col ] train . set_index ( 'visit_date' , inplace = True ) train . head ( )
1180	train_df = pd . read_csv ( '../input/prostate-cancer-grade-assessment/train.csv' ) test_df = pd . read_csv ( '../input/prostate-cancer-grade-assessment/test.csv' ) print ( train_df . shape ) print ( test_df . shape ) train_df . head ( )
411	train_mask = md5_df [ 'is_train' ] == True train_md5_df = md5_df [ train_mask ] test_md5_df = md5_df [ ~ train_mask ] same_hash_mask = test_md5_df [ 'hash' ] . isin ( train_md5_df [ 'hash' ] ) test_md5_df [ same_hash_mask ] [ 'hash' ] . unique ( ) , test_md5_df [ same_hash_mask ] [ 'id' ] . values [ : 3 ]
143	import random def set_seed ( seed ) : random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . backends . cudnn . deterministick = True torch . backends . cudnn . benchmark = False set_seed ( 27 )
1452	def calc_extra ( df_timeseries ) : gp = df_timeseries . groupby ( 'object_id' ) dfe = ( gp [ 'mjd' ] . max ( ) - gp [ 'mjd' ] . min ( ) ) . rename ( 'dmjd' ) . reset_index ( ) dfe [ 'dmjd' ] = dfe [ 'dmjd' ] / 1000 dfe [ 'std_flux' ] = gp . flux . std ( ) . reset_index ( ) . flux / 1000 return dfe
1476	import numpy as np import pandas as pd pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_rows' , 100 )
48	train_log_target = train_df [ [ 'target' ] ] train_log_target [ 'target' ] = np . log ( 1 + train_df [ 'target' ] . values ) train_log_target . describe ( )
1298	cat_cols = [ 'ProductCD' , 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'addr1' , 'addr2' , 'P_emaildomain' , 'R_emaildomain' , 'DeviceType' , 'DeviceInfo' , ] + [ f'M{n}' for n in range ( 1 , 10 ) ] + [ f'id_{n}' for n in range ( 12 , 39 ) ] num_cols = list ( set ( df_train . columns ) - set ( cat_cols ) )
981	import imageio from IPython . display import Image imageio . mimsave ( "pixels_top_bottom.gif" , pixels , duration = 0.001 ) Image ( filename = "pixels_top_bottom.gif" , format = 'png' )
749	from sklearn . model_selection import train_test_split X_train , X_valid , y_train , y_valid = train_test_split ( train_selected , train_labels , test_size = 1000 , random_state = 10 ) model = lgb . LGBMClassifier ( ** best_hyp , class_weight = 'balanced' , random_state = 10 ) model . fit ( X_train , y_train ) ;
1421	stats = [ ] df = full_table [ [ 'Province/State' , 'Country/Region' , 'Date' , 'Confirmed' , 'Deaths' , 'Recovered' , 'Active' ] ] . groupby ( 'Date' ) . sum ( ) print ( 'World COVID-19 Prediction (With China Data)' ) opt_display_model ( df , stats )
221	n = 2 commits_df . loc [ n , 'commit_num' ] = 4 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 256 commits_df . loc [ n , 'hidden_dim_second' ] = 256 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25887
1140	def load_image ( self , index ) : image_id = self . image_ids [ index ] imgpath = f'{DIR_INPUT}/global-wheat-detection/train' img = cv2 . imread ( f'{imgpath}/{image_id}.jpg' , cv2 . IMREAD_COLOR ) assert img is not None , 'Image Not Found ' + imgpath h0 , w0 = img . shape [ : 2 ] return img , ( h0 , w0 ) , img . shape [ : 2 ]
133	del tranformer del word_index del embedding_index gc . collect ( )
694	pd . options . display . max_columns = 150 train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
137	for column in all_df . columns : unique_values = all_df [ column ] . unique ( ) print ( f'Statistics fot column: {column}' ) print ( f'Column unique values:\n {unique_values}' ) print ( f'Number of unique values: {len(unique_values)}' ) print ( f'Number of NAN values: {all_df[column].isna().sum()}' ) print ( '_' * 50 )
1296	loss = history . history [ 'loss' ] val_loss = history . history [ 'val_loss' ] plt . plot ( loss ) plt . plot ( val_loss ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Val' ] , loc = 'upper left' ) plt . show ( )
1212	use_cols = [ col for col in train . columns if col not in [ 'card_id' , 'first_active_month' ] ] train = train [ use_cols ] test = test [ use_cols ] features = list ( train [ use_cols ] . columns ) categorical_feats = [ col for col in features if 'feature_' in col ]
399	import os import time import numpy as np import pandas as pd import scipy . signal as sg from tqdm import tqdm_notebook import matplotlib . pyplot as plt print ( 'ok' )
77	learn = cnn_learner ( data , models . resnet50 , metrics = [ F1 , accuracy ] , callback_fns = ShowGraph )
359	from gplearn . functions import make_function def th ( x ) : return np . tanh ( x ) gptanh = make_function ( th , 'tanh' , 1 ) print ( 'ok' )
441	bold ( '**READINGS HIGHEST DURING THE MIDDLE OF THE DAY**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( 'hour' ) . meter_reading . sum ( ) temp_df . plot ( linewidth = 5 , color = 'teal' ) plt . xlabel ( 'Reading Hour' , fontsize = 15 ) plt . ylabel ( 'Meter Reading' ) plt . show ( )
962	X_interaction = X_importance . iloc [ : 500 , : ] shap_interaction_values = shap . TreeExplainer ( lgb_model ) . shap_interaction_values ( X_interaction )
717	print ( 'Most negative Spearman correlations:' ) print ( scorrs . head ( ) ) print ( '\nMost positive Spearman correlations:' ) print ( scorrs . dropna ( ) . tail ( ) )
1186	N = train_df . shape [ 0 ] x_train = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( train_df [ 'Patient' ] ) ) : x_train [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
263	Xtrain , Xval , Ztrain , Zval = train_test_split ( trainb , targetb , test_size = 0.2 , random_state = 0 ) train_set = lgb . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgb . Dataset ( Xval , Zval , silent = False )
294	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'max' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmax ( ) , 'max' ] = 1
213	dfe = train . sample ( n = 5000 , replace = True , random_state = 1 ) dfe = dfe . drop_duplicates ( keep = False ) ; dfe [ 'index' ] = dfe . index . tolist ( ) dfe = dfe . fillna ( 0 ) ; dfe . info ( )
1338	temp_col = features_dtype_object [ 9 ] plot_count_percent_for_object ( application_train , temp_col ) plot_count_percent_for_object ( application_object_na_filled , temp_col )
1324	for col1 in [ 'lugar1' , 'lugar2' , 'lugar3' , 'lugar4' , 'lugar5' , 'lugar6' ] : for col2 in [ 'instlevel1' , 'instlevel2' , 'instlevel3' , 'instlevel4' , 'instlevel5' , 'instlevel6' , 'instlevel7' , 'instlevel8' , 'instlevel9' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
1169	plt . figure ( figsize = ( 10 , 10 ) ) plt . bar ( np . arange ( len ( x ) ) , x ) plt . xticks ( np . arange ( len ( x ) ) , col_names ) plt . xlabel ( 'Catagories' ) plt . ylabel ( 'Occurrence' )
1416	import re pattern = re . compile ( "^color_.*" ) cols_to_drop = [ x for x in df . columns if re . match ( pattern , x ) ] df = df . drop ( cols_to_drop , axis = 1 )
512	def spread_spectrum ( img ) : img = stats . threshold ( img , threshmin = 12 , newval = 0 ) clahe = cv2 . createCLAHE ( clipLimit = 2.0 , tileGridSize = ( 8 , 8 ) ) img = clahe . apply ( img ) return img
1146	from fastai . vision import ImageSegment mask = ImageSegment ( mask . data . transpose ( 2 , 1 ) ) mask
1414	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
200	first_patient = load_scan ( INPUT_FOLDER + patients [ 0 ] ) first_patient_pixels = get_pixels_hu ( first_patient ) plt . hist ( first_patient_pixels . flatten ( ) , bins = 80 , color = 'c' ) plt . xlabel ( "Hounsfield Units (HU)" ) plt . ylabel ( "Frequency" ) plt . show ( ) plt . imshow ( first_patient_pixels [ 80 ] , cmap = plt . cm . gray ) plt . show ( )
1007	model . load_weights ( 'model.h5' ) for layer in model . layers : layer . trainable = True train_history_step2 = model . fit_generator ( train_gen , validation_data = val_gen , steps_per_epoch = len ( train_gen ) , validation_steps = len ( val_gen ) , callbacks = [ checkpoint ] , epochs = 3 )
1254	import os import sys import json import tensorflow as tf import numpy as np import pandas as pd import matplotlib . pyplot as plt import absl import datetime
233	n = 14 commits_df . loc [ n , 'commit_num' ] = 18 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 248 commits_df . loc [ n , 'LB_score' ] = 0.25841
623	vt_model_accuracies = perform_variance_threshold ( X_train , y_train , X_test , y_test ) print ( vt_model_accuracies )
439	bold ( '**ELECTRICITY THE MOST FREQUENT METER TYPE MEASURED**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) ax = sns . countplot ( data = train , x = 'meter' , palette = 'CMRmap' , alpha = 0.5 ) ax . set_ylabel ( 'Count' , fontsize = 20 ) ax . set_xlabel ( 'Meter Type' , fontsize = 20 ) plt . show ( )
323	train_path = 'base_dir/train_dir' val_path = 'base_dir/val_dir' num_train_samples = len ( df_train ) num_val_samples = len ( df_val ) train_batch_size = 5 val_batch_size = 5 train_steps = np . ceil ( num_train_samples / train_batch_size ) val_steps = np . ceil ( num_val_samples / val_batch_size )
495	application_train = pd . read_csv ( '../input/application_train.csv' ) application_test = pd . read_csv ( '../input/application_test.csv' ) bureau = pd . read_csv ( '../input/bureau.csv' ) bureau_balance = pd . read_csv ( '../input/bureau_balance.csv' ) POS_CASH_balance = pd . read_csv ( '../input/POS_CASH_balance.csv' ) credit_card_balance = pd . read_csv ( '../input/credit_card_balance.csv' ) previous_application = pd . read_csv ( '../input/previous_application.csv' ) installments_payments = pd . read_csv ( '../input/installments_payments.csv' )
87	import numpy as np import pandas as pd from tqdm import tqdm import matplotlib . pyplot as plt from sklearn . metrics import mean_squared_error , mean_absolute_error
599	np . random . seed ( 8888 ) random_sub = np . random . rand ( length ) print ( f"Gini on random submission: {gini(target, random_sub):0.5f}" )
66	train_data = train_data . sample ( frac = 0.1 ) train_data . fillna ( train_data . mean ( axis = 0 ) , inplace = True ) train_X = train_data . drop ( 'y' , axis = 1 ) train_Y = train_data . y print ( "Data for model: X={}, y={}" . format ( train_X . shape , train_Y . shape ) )
890	bureau_balance [ 'MONTHS_BALANCE' ] = pd . to_timedelta ( bureau_balance [ 'MONTHS_BALANCE' ] , 'M' ) bureau_balance [ 'bureau_balance_date' ] = start_date + bureau_balance [ 'MONTHS_BALANCE' ] bureau_balance = bureau_balance . drop ( columns = [ 'MONTHS_BALANCE' ] ) example_credit = bureau_balance [ bureau_balance [ 'SK_ID_BUREAU' ] == 5001709 ] plt . plot ( example_credit [ 'bureau_balance_date' ] , example_credit [ 'STATUS' ] , 'ro' ) ; plt . title ( 'Loan 5001709 over Time' ) ; plt . xlabel ( 'Date' ) ; plt . ylabel ( 'Status' ) ;
174	plt . figure ( figsize = ( 10 , 5 ) ) df . resample ( 'H' ) . is_attributed . mean ( ) . plot ( ) plt . title ( 'Download rate evolution over the day' , fontsize = 15 ) plt . xlabel ( 'Time' ) plt . ylabel ( 'Download rate' )
737	from sklearn . ensemble import ExtraTreesClassifier model_results = cv_model ( train_set , train_labels , ExtraTreesClassifier ( n_estimators = 100 , random_state = 10 ) , 'EXT' , model_results )
525	from math import sqrt print ( sqrt ( metrics . mean_squared_error ( y_test , y_pred_class ) ) )
1431	import plotly . express as px fig = px . histogram ( df [ [ 'age' , 'gender' , 'hospital_death' , 'bmi' ] ] . dropna ( ) , x = "age" , y = "hospital_death" , color = "gender" , marginal = "box" , hover_data = df [ [ 'age' , 'gender' , 'hospital_death' , 'bmi' ] ] . columns ) fig . show ( )
330	sgd = SGDRegressor ( ) sgd . fit ( train , target ) acc_model ( 4 , sgd , train , test )
814	fig , axs = plt . subplots ( 1 , 2 , sharey = True , sharex = True ) random_params [ 'boosting_type' ] . value_counts ( ) . plot . bar ( ax = axs [ 0 ] , figsize = ( 14 , 6 ) , color = 'orange' , title = 'Random Search Boosting Type' ) bayes_params [ 'boosting_type' ] . value_counts ( ) . plot . bar ( ax = axs [ 1 ] , figsize = ( 14 , 6 ) , color = 'green' , title = 'Bayes Optimization Boosting Type' ) ;
791	plt . figure ( figsize = ( 10 , 8 ) ) fi [ 'importance' ] . plot . bar ( color = 'g' , edgecolor = 'k' ) ; plt . ylabel ( 'Importance' ) ; plt . title ( 'Feature Importances' ) ;
21	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train [ 'wheezy-copper-turtle-magic' ] . values , bins = 1000 ) plt . title ( 'Histogram muggy-smalt-axolotl-pembus counts' ) plt . xlabel ( 'Value' ) plt . ylabel ( 'Count' ) plt . show ( )
535	import librosa import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( 'ignore' )
700	missing = pd . DataFrame ( data . isnull ( ) . sum ( ) ) . rename ( columns = { 0 : 'total' } ) missing [ 'percent' ] = missing [ 'total' ] / len ( data ) missing . sort_values ( 'percent' , ascending = False ) . head ( 10 ) . drop ( 'Target' )
360	predictions = np . zeros ( len ( scaled_test_X ) ) n_fold = 5 folds = KFold ( n_splits = n_fold , shuffle = True , random_state = 42 ) fold_importance_df = pd . DataFrame ( ) fold_importance_df [ "Feature" ] = scaled_train_X . columns print ( 'ok' )
132	def clean_up_text_with_all_process ( text ) : text = text . lower ( ) text = clean_contractions ( text ) text = clean_special_chars ( text ) text = clean_small_caps ( text ) return text
1469	def melt_sales ( df ) : df = df . drop ( [ "item_id" , "dept_id" , "cat_id" , "store_id" , "state_id" , "total_sales" ] , axis = 1 ) . melt ( id_vars = [ 'id' ] , var_name = 'd' , value_name = 'demand' ) return df sales = melt_sales ( train_sales )
151	train_data , eval_data , train_label , eval_label = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( train_data . shape , eval_data . shape , train_label . shape , eval_label . shape )
101	print ( 'There are ' + str ( y . count ( 1 ) ) + ' fake train samples' ) print ( 'There are ' + str ( y . count ( 0 ) ) + ' real train samples' ) print ( 'There are ' + str ( val_y . count ( 1 ) ) + ' fake val samples' ) print ( 'There are ' + str ( val_y . count ( 0 ) ) + ' real val samples' )
894	plt . figure ( figsize = ( 10 , 8 ) ) sns . kdeplot ( interesting_features [ 'MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Approved)' ] . dropna ( ) , label = 'Approved' ) sns . kdeplot ( interesting_features [ 'MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Canceled)' ] . dropna ( ) , label = 'Canceled' ) plt . xlabel ( 'MEAN(previous.CNT_PAYMENT)' ) ; plt . ylabel ( 'Density' ) ; plt . title ( 'Average Term of Previous Credit' ) ;
728	plt . figure ( figsize = ( 8 , 8 ) ) sns . boxplot ( x = 'Target' , y = 'meaneduc' , hue = 'female-head' , data = final ) ; plt . title ( 'Average Education by Target and Female Head of Household' , size = 16 ) ;
547	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "bedroomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Bedroom Count ",title=" Bedroom Count Vs Log Error " )
899	from featuretools import selection feature_matrix2 = selection . remove_low_information_features ( feature_matrix ) print ( 'Removed %d features from training features' % ( feature_matrix . shape [ 1 ] - feature_matrix2 . shape [ 1 ] ) ) feature_matrix_test2 = selection . remove_low_information_features ( feature_matrix_test ) print ( 'Removed %d features from testing features' % ( feature_matrix_test . shape [ 1 ] - feature_matrix_test2 . shape [ 1 ] ) )
215	corr_matrix = features . corr ( ) . abs ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) ; threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style collinear_features = [ column for column in upper . columns if any ( upper [ column ] > threshold ) ] upper . style . applymap ( highlight )
596	import pandas as pd train = pd . read_csv ( "../input/train.csv" ) counts = train . target . value_counts ( ) print ( "Class distribution:" ) counts / counts . sum ( )
237	n = 18 commits_df . loc [ n , 'commit_num' ] = 22 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 216 commits_df . loc [ n , 'LB_score' ] = 0.25827
760	def lb_dist ( model ) : model . fit ( x_train , y_train ) print ( "Train Acc :: " , accuracy_score ( y_train , model . predict ( x_train ) ) ) print ( "Valid Acc :: " , accuracy_score ( y_val , model . predict ( x_val ) ) ) print ( "CV Accuracy :: " , cross_val_score ( rand , train , label [ 'surface' ] , cv = 5 ) . mean ( ) ) return model
841	import time time . sleep ( 600 ) app = app . merge ( credit_info , on = 'SK_ID_CURR' , how = 'left' ) del credit_info app . shape
1218	@ trainer . on ( Events . EPOCH_COMPLETED ) def compute_and_display_val_metrics ( engine ) : epoch = engine . state . epoch metrics = val_evaluator . run ( val_loader ) . metrics print ( "Validation Results - Epoch: {} Average Loss: {:.4f} | Accuracy: {:.4f} " . format ( engine . state . epoch , metrics [ 'loss' ] , metrics [ 'accuracy' ] ) )
246	train = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/train.json' , lines = True ) test = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/test.json' , lines = True ) sample_sub = pd . read_csv ( "/kaggle/input/stanford-covid-vaccine/sample_submission.csv" )
346	df_preds = pd . DataFrame ( predictions ) new_names = [ 'conf_1' , 'x_1' , 'y_1' , 'width_1' , 'height_1' , 'conf_2' , 'x_2' , 'y_2' , 'width_2' , 'height_2' ] df_preds . columns = new_names df_preds [ 'patientId' ] = df_test [ 'patientId' ] df_preds [ 'PredictionString' ] = 0
187	plt . figure ( figsize = ( 20 , 20 ) ) sns . boxplot ( x = 'price' , y = 'cat1' , data = train , orient = 'h' ) plt . title ( 'Prices of the first level of categories' , fontsize = 30 ) plt . ylabel ( 'First level categories' , fontsize = 20 ) plt . xlabel ( 'Price' , fontsize = 20 )
552	combined_aug = Compose ( transforms = [ GaussianTargetNoise ( p = 1.0 , gaus_std = 0.3 ) , TemporalFlip ( p = 1.0 ) ] )
1365	col = numeric_features [ 12 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
936	if use_selected : selected_aggregates_bureau_bal = sample_selected_aggregations ( bureau_bal , aggs_all_cat ) selected_aggregates_bureau = sample_selected_aggregations ( bureau , aggs_all_cat ) selected_aggregates_ins = sample_selected_aggregations ( ins , aggs_all_cat ) selected_aggregates_prev = sample_selected_aggregations ( prev , aggs_all_cat ) selected_aggregates_cred_card = sample_selected_aggregations ( cred_card_bal , aggs_all_cat ) selected_aggregates_pos_cash_bal = sample_selected_aggregations ( pos_cash_bal , aggs_all_cat ) else : aggs_cat , aggs_num = sample_aggregates ( aggs_medium_num , aggs_all_cat )
1417	from sklearn . linear_model import LogisticRegression lr = LogisticRegression ( penalty = 'l2' , C = 1000000 , class_weight = "balanced" ) lr . fit ( X_train , y_train ) y_pred = lr . predict ( X_test ) print ( classification_report ( y_pred , y_test ) )
561	imid = 'TCGA-G9-6362-01Z-00-DX1' image_path = df [ df . id == imid ] . path . values [ 0 ] image = cv2 . imread ( image_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( image ) plt . show ( )
757	train = pd . read_csv ( "../input/X_train.csv" ) test = pd . read_csv ( "../input/X_test.csv" ) label = pd . read_csv ( "../input/y_train.csv" ) sub = pd . read_csv ( "../input/sample_submission.csv" )
134	del train_df del y del test_df del transformed_x gc . collect ( )
1033	print ( "result_out keys:" , result_out . keys ( ) ) print ( "First 10 detection scores:" , result_out [ 'detection_scores' ] [ : 10 ] ) print ( ) print ( "Shape of image_out" , image_out . shape ) print ( "Type of image_out:" , type ( image_out ) )
176	print ( "Memory usage before optimization :" , str ( round ( total_before_opti / 1000000000 , 2 ) ) + 'GB' ) print ( "Memory usage after optimization :" , str ( round ( sum ( df . memory_usage ( ) ) / 1000000000 , 2 ) ) + 'GB' ) print ( "We reduced the dataframe size by" , str ( round ( ( ( total_before_opti - sum ( df . memory_usage ( ) ) ) / total_before_opti ) * 100 , 2 ) ) + '%' )
1093	fig , ax = plt . subplots ( nrows = 10 , ncols = 10 , figsize = ( 50 , 50 ) ) for i in range ( 10 ) : for j in range ( 10 ) : ids = i * 10 + j sns . scatterplot ( X [ 'var_' + str ( ids ) ] , shap_values [ : , ids ] , ax = ax [ i , j ] )
1394	col = numeric_features [ 43 ] plot_category_percent_of_target_for_numeric ( col )
976	def get_dicom_tag ( dicom , call ) : try : return dicom [ call ] except KeyError : print ( 'KeyError' ) get_dicom_tag ( first_dicom , 'ReferencedSOPClassUID' )
343	print ( df_train . shape ) print ( df_val . shape ) print ( df_test . shape )
188	plt . figure ( figsize = ( 17 , 10 ) ) sns . countplot ( y = train . brand_name , \ order = train . brand_name . value_counts ( ) . iloc [ : 10 ] . index , \ orient = 'v' ) plt . title ( 'Top 10 brands' , fontsize = 25 ) plt . ylabel ( 'Brand name' , fontsize = 20 ) plt . xlabel ( 'Number of product of the brand' , fontsize = 20 )
1350	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
1481	pred = learn . get_preds ( ds_type = DatasetType . Test ) y_pred = [ int ( np . argmax ( row ) ) for row in pred [ 0 ] ] test_csv [ 'AdoptionSpeed' ] = y_pred test_csv [ [ 'PetID' , 'AdoptionSpeed' ] ] . to_csv ( 'submission.csv' , index = False )
142	categorical = all_df . drop ( [ 'target' ] + continuous , axis = 1 ) . columns cat_cols_idx , cont_cols_idx = list ( ) , list ( ) for idx , column in enumerate ( all_df . drop ( 'target' , axis = 1 ) . columns ) : if column in categorical : cat_cols_idx . append ( idx ) elif column in continuous : cont_cols_idx . append ( idx )
500	def correlation_heatmap ( df ) : _ , ax = plt . subplots ( figsize = ( 20 , 15 ) ) colormap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) _ = sns . heatmap ( df . corr ( ) , cmap = colormap , square = True , ) plt . title ( 'Pearson Correlation of Features' ) correlation_heatmap ( application_train )
807	OUT_FILE = 'bayes_test.csv' of_connection = open ( OUT_FILE , 'w' ) writer = csv . writer ( of_connection ) ITERATION = 0 headers = [ 'loss' , 'hyperparameters' , 'iteration' , 'runtime' , 'score' ] writer . writerow ( headers ) of_connection . close ( )
514	def crop ( img , crop_list ) : x_coord = crop_list [ 0 ] y_coord = crop_list [ 1 ] width = crop_list [ 2 ] height = crop_list [ 3 ] cropped_img = img [ x_coord : x_coord + width , y_coord : y_coord + height ] return cropped_img
733	from sklearn . svm import LinearSVC from sklearn . naive_bayes import GaussianNB from sklearn . neural_network import MLPClassifier from sklearn . linear_model import LogisticRegressionCV , RidgeClassifierCV from sklearn . discriminant_analysis import LinearDiscriminantAnalysis from sklearn . neighbors import KNeighborsClassifier
1396	col = numeric_features [ 45 ] plot_category_percent_of_target_for_numeric ( col )
572	print ( f"First day entry:\t {df_covid['Date'].min()}" ) print ( f"Last day reported:\t {df_covid['Date'].max()}" ) print ( f"Total of tracked days:\t {df_covid['Date'].max() - df_covid['Date'].min()}" )
1179	files = folders = 0 path = "/kaggle/input/osic-pulmonary-fibrosis-progression/test" for _ , dirnames , filenames in os . walk ( path ) : files += len ( filenames ) folders += len ( dirnames ) print ( "{:,} files/images, {:,} folders/patients" . format ( files , folders ) )
622	fagg_model_accuracies = perform_feature_agglomeration ( X_train , y_train , X_test , y_test ) print ( fagg_model_accuracies )
530	aisles = pd . read_csv ( '../input/aisles.csv' ) departments = pd . read_csv ( '../input//departments.csv' ) orderProductsTrain = pd . read_csv ( '../input/order_products__train.csv' ) orders = pd . read_csv ( '../input/orders.csv' ) products = pd . read_csv ( '../input/products.csv' ) orderProductsPrior = pd . read_csv ( '../input/order_products__prior.csv' )
583	df_grouped_usa = get_df_country_cases ( df_covid , "US" ) df_usa_cases_by_day = df_grouped_usa [ df_grouped_usa . confirmed > 0 ] df_usa_cases_by_day = df_usa_cases_by_day . reset_index ( drop = True ) df_usa_cases_by_day [ 'day' ] = df_usa_cases_by_day . date . apply ( lambda x : ( x - df_usa_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_usa_cases_by_day = df_usa_cases_by_day [ reordered_columns ] df_usa_cases_by_day
509	def get_subject_labels ( infile , subject_id ) : df = pd . read_csv ( infile ) df [ 'Subject' ] , df [ 'Zone' ] = df [ 'Id' ] . str . split ( '_' , 1 ) . str df = df [ [ 'Subject' , 'Zone' , 'Probability' ] ] threat_list = df . loc [ df [ 'Subject' ] == subject_id ] return threat_list
1347	col1 = 'NONLIVINGAREA_MEDI' col2 = 'NONLIVINGAREA_MODE' multi_features_kde_plot ( col1 , col2 )
120	train_df [ 'FVC Difference' ] = train_df [ 'Expected FVC' ] - train_df [ 'FVC' ] plt . figure ( figsize = ( 10 , 5 ) ) sns . distplot ( train_df [ 'FVC Difference' ] , rug = True )
545	topFeatures = features [ "features" ] . tolist ( ) [ : 20 ] corrMatt = merged [ topFeatures ] . corr ( ) mask = np . array ( corrMatt ) mask [ np . tril_indices_from ( mask ) ] = False fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) sn . heatmap ( corrMatt , mask = mask , vmax = .8 , square = True )
1445	columns = [ 'ip' , 'click_time' , 'is_attributed' ] dtypes = { 'ip' : 'uint32' , 'is_attributed' : 'uint8' , } ips_df = pd . read_csv ( '../input/train.csv' , usecols = columns , dtype = dtypes )
698	households_leader = train . groupby ( 'idhogar' ) [ 'parentesco1' ] . sum ( ) households_no_head = train . loc [ train [ 'idhogar' ] . isin ( households_leader [ households_leader == 0 ] . index ) , : ] print ( 'There are {} households without a head.' . format ( households_no_head [ 'idhogar' ] . nunique ( ) ) )
1437	train_smp [ 'click_time' ] = ( train_smp [ 'click_time' ] . astype ( np . int64 ) // 10 ** 9 ) . astype ( np . int32 ) train_smp [ 'next_click' ] = ( train_smp . groupby ( [ 'ip' , 'app' , 'device' , 'os' ] ) . click_time . shift ( - 1 ) - train_smp . click_time ) . astype ( np . float32 ) train_smp [ 'next_click' ] = train_smp [ 'next_click' ] . fillna ( 360000000 ) . astype ( 'uint32' )
225	n = 6 commits_df . loc [ n , 'commit_num' ] = 8 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 224 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25868
689	dcm_data = pydicom . dcmread ( img_filepath ) print ( dcm_data )
555	SS = StandardScaler ( ) df_scale = SS . fit_transform ( df [ real_feature ] ) trn_real = df_scale [ : 182080 ] tst_real = df_scale [ 182080 : ]
1275	key_prev = 'SK_ID_PREV' payment_cols = [ 'AMT_PAYMENT' ] df_previous_app = pd . merge ( left = df_previous_app , right = df_installments [ [ key_prev ] + payment_cols ] . groupby ( key_prev ) . min ( ) , left_on = key_prev , right_index = True , how = 'left' ) df_previous_app [ [ key ] + [ key_prev ] + payment_cols ] [ df_previous_app . SK_ID_CURR == 365597 ]
483	vector = vectorizer . transform ( text ) print ( vector . shape ) print ( vector . toarray ( ) )
217	import os import sys import json import math import random import numpy as np import pandas as pd import gc from tqdm import tqdm import matplotlib . pyplot as plt import seaborn as sns import plotly . express as px import plotly . graph_objects as go from sklearn . model_selection import train_test_split , KFold , StratifiedKFold import tensorflow as tf import tensorflow_addons as tfa import tensorflow . keras . backend as K import tensorflow . keras . layers as L import warnings warnings . filterwarnings ( "ignore" )
467	def timer ( start_time = None ) : if not start_time : start_time = datetime . now ( ) return start_time elif start_time : thour , temp_sec = divmod ( ( datetime . now ( ) - start_time ) . total_seconds ( ) , 3600 ) tmin , tsec = divmod ( temp_sec , 60 ) print ( 'Time taken : %i hours %i minutes and %s seconds.' % ( thour , tmin , round ( tsec , 2 ) ) )
564	sort_idx = np . argsort ( classes ) . astype ( int ) sample_sub = pd . read_csv ( '../input/freesound-audio-tagging-2019/sample_submission.csv' ) test_Y_sort = test_Y [ : , sort_idx ] sample_sub . iloc [ : , 1 : ] = test_Y_sort sample_sub . to_csv ( 'submission.csv' , index = False ) t2 = time . time ( ) print ( 'Total time: ' , ( t2 - t1 ) )
1232	lgb_lv2_outcomes = cross_validate_lgb ( lgb_params , lv1_train_df , y_train , lv1_test_df , kf , [ ] , use_cat = False , verbose_eval = False , use_rank = True ) lgb_lv2_cv = xgb_lv2_outcomes [ 0 ] lgb_lv2_train_pred = lgb_lv2_outcomes [ 1 ] lgb_lv2_test_pred = lgb_lv2_outcomes [ 2 ]
592	positive_train = train [ train [ "sentiment" ] == "positive" ] negative_train = train [ train [ "sentiment" ] == "negative" ] neutral_train = train [ train [ "sentiment" ] == "neutral" ]
886	app_types = { } for col in app_train : if ( app_train [ col ] . dtype != 'object' ) and ( len ( app_train [ col ] . unique ( ) ) <= 2 ) : app_types [ col ] = ft . variable_types . Boolean print ( 'Number of boolean variables: ' , len ( app_types ) )
1558	stopwords = nltk . corpus . stopwords . words ( 'english' ) len ( stopwords ) first_text_list_cleaned = [ word for word in first_text_list if word . lower ( ) not in stopwords ] print ( first_text_list_cleaned ) print ( "=" * 90 ) print ( "Length of original list: {0} words\n" "Length of list after stopwords removal: {1} words" . format ( len ( first_text_list ) , len ( first_text_list_cleaned ) ) )
565	predict_iterator = SpeechDirectoryIterator ( directory = test_path , batch_size = batch_size , window_size = window_size , window_stride = window_stride , window_type = window_type , normalize = normalize , max_len = max_len , classes = None , shuffle = False )
157	import torch , torchvision print ( torch . __version__ , torch . cuda . is_available ( ) ) import mmdet print ( mmdet . __version__ ) from mmcv . ops import get_compiling_cuda_version , get_compiler_version print ( get_compiling_cuda_version ( ) ) print ( get_compiler_version ( ) )
145	filenames = os . listdir ( "../input/train/train" ) categories = [ ] for filename in filenames : category = filename . split ( '.' ) [ 0 ] if category == 'dog' : categories . append ( 1 ) else : categories . append ( 0 ) df = pd . DataFrame ( { 'filename' : filenames , 'category' : categories } )
648	model_checkpoint = ModelCheckpoint ( save_model_name , monitor = 'my_iou_metric' , mode = 'max' , save_best_only = True , verbose = 1 ) reduce_lr = ReduceLROnPlateau ( monitor = 'my_iou_metric' , mode = 'max' , factor = 0.5 , patience = 5 , min_lr = 0.0001 , verbose = 1 ) epochs = 10 batch_size = 32 history = model1 . fit ( x_train , y_train , validation_data = [ x_valid , y_valid ] , epochs = epochs , batch_size = batch_size , callbacks = [ model_checkpoint , reduce_lr ] , verbose = 2 )
390	print ( "Unique categories: " , len ( CATEGORY_NAMES_DF [ 'category_id' ] . unique ( ) ) ) print ( "Unique level 1 categories: " , len ( CATEGORY_NAMES_DF [ 'category_level1' ] . unique ( ) ) ) print ( "Unique level 2 categories: " , len ( CATEGORY_NAMES_DF [ 'category_level2' ] . unique ( ) ) ) print ( "Unique level 3 categories: " , len ( CATEGORY_NAMES_DF [ 'category_level3' ] . unique ( ) ) )
1392	col = numeric_features [ 41 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1378	col = numeric_features [ 25 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1286	train_kfold = kfold_data [ kfold_data . kfold < 4 ] val_kfold = kfold_data [ kfold_data . kfold == 4 ]
1017	fig = plt . figure ( 200 , figsize = ( 15 , 15 ) ) random_indicies = np . random . choice ( range ( len ( X_images ) ) , 9 , False ) subset = X_images [ random_indicies ] for i in range ( 9 ) : ax = fig . add_subplot ( 3 , 3 , i + 1 ) ax . imshow ( subset [ i ] ) plt . show ( )
222	n = 3 commits_df . loc [ n , 'commit_num' ] = 5 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 384 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25880
1305	cats = { } for c in cat_cols : df_train [ c ] = df_train [ c ] . astype ( "category" ) df_train [ c ] . cat . add_categories ( 'unknown' , inplace = True ) cats [ c ] = df_train [ c ] . cat . categories
1067	directory = '/kaggle/input/tensorflow2-question-answering/' test_path = directory + 'simplified-nq-test.jsonl' test = build_test ( test_path ) submission = pd . read_csv ( "../input/tensorflow2-question-answering/sample_submission.csv" ) test . head ( )
1128	shap . initjs ( ) explainer = shap . TreeExplainer ( model ) shap_values = explainer . shap_values ( data_for_prediction ) shap . force_plot ( explainer . expected_value [ 4 ] , shap_values [ 4 ] , data_for_prediction , link = 'logit' )
408	data_ids = train_df [ 'id' ] . values . tolist ( ) from image_dataset_viz import DatasetExporter de = DatasetExporter ( read_image , read_mask , blend_alpha = 0.3 , n_cols = 20 , max_output_img_size = ( 100 , 100 ) ) de . export ( data_ids , data_ids , "train_dataset_viz" )
517	train_flat [ 'totals.transactionRevenueLog' ] = np . log1p ( train_flat [ 'totals.transactionRevenue' ] ) train_flat [ 'totals.transactionRevenueLogNAN' ] = np . log1p ( train_flat [ 'totals.transactionRevenue' ] ) train_flat [ 'totals.transactionRevenueLogNAN' ] . replace ( 0 , np . nan , inplace = True )
716	train_heads = heads . loc [ heads [ 'Target' ] . notnull ( ) , : ] . copy ( ) pcorrs = pd . DataFrame ( train_heads . corr ( ) [ 'Target' ] . sort_values ( ) ) . rename ( columns = { 'Target' : 'pcorr' } ) . reset_index ( ) pcorrs = pcorrs . rename ( columns = { 'index' : 'feature' } ) print ( 'Most negatively correlated variables:' ) print ( pcorrs . head ( ) ) print ( '\nMost positively correlated variables:' ) print ( pcorrs . dropna ( ) . tail ( ) )
1039	preds_ls = [ ] for df , preds in [ ( public_df , public_preds ) , ( private_df , private_preds ) ] : for i , uid in enumerate ( df . id ) : single_pred = preds [ i ] single_df = pd . DataFrame ( single_pred , columns = pred_cols ) single_df [ 'id_seqpos' ] = [ f'{uid}_{x}' for x in range ( single_df . shape [ 0 ] ) ] preds_ls . append ( single_df ) preds_df = pd . concat ( preds_ls ) preds_df . head ( )
1522	th = np . array ( [ 0.50 , 0.56 , 0.50 , 0.50 , 0.50 , 0.47 , 0.39 , 0.56 , 0.36 , 0.45 , 0.41 , 0.48 , 0.45 , 0.44 , 0.63 , 0.50 , 0.38 , 0.33 , 0.45 , 0.41 , 0.33 , 0.49 , 0.43 , 0.52 , 0.51 , 0.48 , 0.43 , 0.1 ] ) print ( f1_score ( y , pred > 0.5 , average = 'macro' ) , f1_score ( y , pred > th , average = 'macro' ) )
164	concat_sub [ 'scalar_coupling_constant' ] = np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] > cutoff_lo , axis = 1 ) , concat_sub [ 'm_max' ] , np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] < cutoff_hi , axis = 1 ) , concat_sub [ 'm_min' ] , concat_sub [ 'm_median' ] ) ) concat_sub [ [ 'id' , 'scalar_coupling_constant' ] ] . to_csv ( 'stack_minmax_median.csv' , index = False , float_format = '%.6f' )
937	X_train = X [ X [ 'TARGET' ] . notnull ( ) ] X_test = X [ X [ 'TARGET' ] . isnull ( ) ] y_train = X_train . TARGET del X gc . collect ( ) good_features = [ x for x in X_train . columns if x not in [ 'TARGET' , 'SK_ID_CURR' , 'SK_ID_BUREAU' , 'SK_ID_PREV' , 'index' ] ]
146	sample = random . choice ( filenames ) image = load_img ( "../input/train/train/" + sample ) plt . imshow ( image )
576	def get_df_country_cases ( df : pd . DataFrame , country_name : str ) -> pd . DataFrame : df_grouped_country = df [ df [ 'country' ] == country_name ] . reset_index ( ) df_grouped_country_date = df_grouped_country . groupby ( 'date' ) [ 'date' , 'confirmed' , 'deaths' ] . sum ( ) . reset_index ( ) df_grouped_country_date [ "confirmed_marker" ] = df_grouped_country_date . shape [ 0 ] * [ 'Confirmed' ] df_grouped_country_date [ "deaths_marker" ] = df_grouped_country_date . shape [ 0 ] * [ 'Deaths' ] return df_grouped_country_date
348	def my_generator ( ) : for i in range ( 0 , 3 ) : yield print ( i ) my_gen = my_generator ( )
41	train = pd . read_csv ( '../input/google-quest-challenge/train.csv' ) . fillna ( ' ' ) test = pd . read_csv ( '../input/google-quest-challenge/test.csv' ) . fillna ( ' ' ) sample_submission = pd . read_csv ( '../input/google-quest-challenge/sample_submission.csv' )
1524	sample_df = pd . read_csv ( SAMPLE ) sample_list = list ( sample_df . Id ) pred_dic = dict ( ( key , value ) for ( key , value ) in zip ( learner . data . test_ds . fnames , pred_list ) ) pred_list_cor = [ pred_dic [ id ] for id in sample_list ] df = pd . DataFrame ( { 'Id' : sample_list , 'Predicted' : pred_list_cor } ) df . to_csv ( 'protein_classification.csv' , header = True , index = False )
959	X_train = pd . read_csv ( '../input/microsoft-malware-prediction/train.csv' , dtype = dtypes ) X_train = reduce_mem_usage ( X_train ) X_test = pd . read_csv ( '../input/microsoft-malware-prediction/test.csv' , dtype = dtypes ) X_test = reduce_mem_usage ( X_test ) avsig_timestamp = np . load ( '../input/timestamps/AvSigVersionTimestamps.npy' ) [ ( ) ] osver_timestamp = np . load ( '../input/timestamps/OSVersionTimestamps.npy' ) [ ( ) ] print ( 'data loaded.' )
1125	def change ( addr ) : if addr == 16.0 : return 1 elif addr == 65.0 : return 1 elif addr == np . nan : return np . nan else : return 0 df [ "Asia" ] = df [ "addr2" ] . map ( change )
1531	plt . figure ( figsize = [ 18 , 4 ] ) df_train [ 'kills' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of kills" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'kills' ] . value_counts ( ) )
664	print ( f"One-Hot encoding {len(OH_cols)} columns" ) OH_full = pd . get_dummies ( full_data , columns = OH_cols , drop_first = True , dummy_na = True , sparse = True , ) . sparse . to_coo ( )
1433	from sklearn . model_selection import GridSearchCV , RandomizedSearchCV , cross_val_score , train_test_split from sklearn . metrics import precision_score , roc_auc_score , recall_score , confusion_matrix , roc_curve , precision_recall_curve , accuracy_score , explained_variance_score import lightgbm as lgbm from scipy . stats import randint as sp_randint from scipy . stats import uniform as sp_uniform import warnings warnings . filterwarnings ( 'ignore' )
1359	col = numeric_features [ 6 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
484	text2 = [ "the the the times" ] vector = vectorizer . transform ( text2 ) print ( vector . toarray ( ) )
986	for f in train_clean_rob . columns : if train_clean_rob [ f ] . dtype == 'object' and test_clean_rob [ f ] . dtype == 'object' : lbl = preprocessing . LabelEncoder ( ) lbl . fit ( list ( train_clean_rob [ f ] . values ) + list ( test_clean_rob [ f ] . values ) ) train_clean_rob [ f ] = lbl . transform ( list ( train_clean_rob [ f ] . values ) ) test_clean_rob [ f ] = lbl . transform ( list ( test_clean_rob [ f ] . values ) )
1413	from keras . preprocessing . image import ImageDataGenerator train_datagen = ImageDataGenerator ( rescale = 1. / 255 , shear_range = 0.2 , zoom_range = 0.2 , horizontal_flip = True , rotation_range = 90 , featurewise_center = True , width_shift_range = 0.2 , height_shift_range = 0.2 ) train_datagen . fit ( X )
71	train = pd . read_csv ( filepath_or_buffer = '../input/train.csv' , index_col = 'id' , parse_dates = [ 'pickup_datetime' , 'dropoff_datetime' ] , infer_datetime_format = True ) test = pd . read_csv ( filepath_or_buffer = '../input/test.csv' , index_col = 'id' , parse_dates = [ 'pickup_datetime' ] , infer_datetime_format = True )
74	def seed_everything ( seed ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( 43 )
162	concat_sub [ 'scalar_coupling_constant' ] = np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] > cutoff_lo , axis = 1 ) , 1 , np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] < cutoff_hi , axis = 1 ) , 0 , concat_sub [ 'm_median' ] ) ) concat_sub [ [ 'id' , 'scalar_coupling_constant' ] ] . to_csv ( 'stack_pushout_median.csv' , index = False , float_format = '%.6f' )
1426	stats_df = pd . DataFrame ( stats ) stats_df . columns = [ 'country' , 'Confirmed-N' , 'Confirmed-a' , 'Confirmed-alpha' , 'Deaths-N' , 'Deaths-a' , 'Deaths-alpha' , 'Recorved-N' , 'Recorved-a' , 'Recorved-alpha' ] stats_df
481	params = { 'objective' : 'binary' , 'boosting' : 'gbdt' , 'learning_rate' : 0.2 , 'verbose' : 0 , 'num_leaves' : 100 , 'bagging_fraction' : 0.95 , 'bagging_freq' : 1 , 'bagging_seed' : 1 , 'feature_fraction' : 0.9 , 'feature_fraction_seed' : 1 , 'max_bin' : 256 , 'num_rounds' : 100 , 'metric' : 'auc' } lgbm_model = lgb . train ( params , train_set = lgb_train , valid_sets = lgb_val , verbose_eval = 5 )
150	test_gen = ImageDataGenerator ( rescale = 1. / 255 ) test_generator = test_gen . flow_from_dataframe ( test_df , "../input/test1/test1/" , x_col = 'filename' , y_col = None , class_mode = None , target_size = IMAGE_SIZE , batch_size = batch_size , shuffle = False )
38	images = [ ] for i , image_id in enumerate ( tqdm ( train [ 'image_name' ] . head ( 10 ) ) ) : im = Image . open ( f'../input/siim-isic-melanoma-classification/jpeg/train/{image_id}.jpg' ) im = im . resize ( ( 128 , ) * 2 , resample = Image . LANCZOS ) images . append ( im )
45	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train_df . target . values , bins = 100 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
549	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "roomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Room Count ",title=" Room Count Vs Log Error " )
645	print ( "Number of unique_label: {}" . format ( len ( set ( labels ) ) ) ) print ( "Number of unique_label(unicode_translation.csv): {}" . format ( unicode_trans . shape [ 0 ] ) ) print ( "diff: {}" . format ( abs ( len ( set ( labels ) ) - unicode_trans . shape [ 0 ] ) ) )
414	def compute_histogram ( img , hist_size = 100 ) : hist = cv2 . calcHist ( [ img ] , [ 0 ] , mask = None , histSize = [ hist_size ] , ranges = ( 0 , 255 ) ) hist = cv2 . normalize ( hist , dst = hist ) return hist
874	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns plt . rcParams [ 'font.size' ] = 18 plt . style . use ( 'fivethirtyeight' )
1451	sns . barplot ( 'click_hour' , 'is_attributed' , data = train_smp ) plt . title ( 'HOURLY CONVERSION RATIO' ) ; plt . ylabel ( 'Converted Ratio' ) ;
1390	col = numeric_features [ 39 ] plot_category_percent_of_target_for_numeric ( col )
662	map_ord1 = { 'Novice' : 1 , 'Contributor' : 2 , 'Expert' : 4 , 'Master' : 5 , 'Grandmaster' : 6 } full_data . ord_1 = full_data . ord_1 . map ( map_ord1 )
1444	df_converted = pd . DataFrame ( ) chunksize = 10 ** 6 for chunk in pd . read_csv ( '../input/train.csv' , chunksize = chunksize , dtype = dtypes ) : filtered = ( chunk [ ( np . where ( chunk [ 'is_attributed' ] == 1 , True , False ) ) ] ) df_converted = pd . concat ( [ df_converted , filtered ] , ignore_index = True , )
303	seed_random = 42 window_sizes = [ 10 , 50 ] lr_lgb = 0.05 num_leaves = 200 num_iterations = 1500 lr_wn = 0.0015 EPOCHS_wn = 15 SPLITS = 2 w_lgb = 0.5 w_wnet = 1 - w_lgb print ( w_wnet )
903	def target_corrs ( df ) : corrs = [ ] for col in df . columns : print ( col ) if col != 'TARGET' : corr = df [ 'TARGET' ] . corr ( df [ col ] ) corrs . append ( ( col , corr ) ) corrs = sorted ( corrs , key = lambda x : abs ( x [ 1 ] ) , reverse = True ) return corrs
61	products = train_transaction . ProductCD . unique ( ) . tolist ( ) col = 'D1' for prod in products : print ( "Product code:" , prod ) timehist1_2 ( col , prod )
1182	x_train , x_val , y_train , y_val = train_test_split ( x_train , y_train_multi , test_size = TRAIN_VAL_RATIO , random_state = 2020 )
723	ind [ 'inst/age' ] = ind [ 'inst' ] / ind [ 'age' ] ind [ 'tech' ] = ind [ 'v18q' ] + ind [ 'mobilephone' ] ind [ 'tech' ] . describe ( )
901	columns = [ 'SK_ID_CURR' ] for var in bureau_agg . columns . levels [ 0 ] : if var != 'SK_ID_CURR' : for stat in bureau_agg . columns . levels [ 1 ] [ : - 1 ] : columns . append ( 'bureau_%s_%s' % ( var , stat ) )
642	df_train = df_train [ df_train [ 'outliers' ] == 0 ] target = df_train [ 'target' ] del df_train [ 'target' ] features = [ c for c in df_train . columns if c not in [ 'card_id' , 'first_active_month' , 'outliers' ] ] categorical_feats = [ c for c in features if 'feature_' in c ]
59	train_transaction [ 'day' ] = train_transaction [ 'TransactionDT' ] / ( 3600 * 24 ) train_transaction [ 'D1minusday' ] = ( train_transaction [ 'D1' ] - train_transaction [ 'day' ] ) . replace ( np . nan , - 9999 ) . map ( int ) colsID = [ 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'D1minusday' , 'ProductCD' ] train_transaction = create_key ( train_transaction , colsID , 'cardID_D1' )
252	country_name = "Italy" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1357	col = numeric_features [ 4 ] plot_kde_hist_for_numeric ( col )
812	scores = pd . DataFrame ( { 'ROC AUC' : random_params [ 'score' ] , 'iteration' : random_params [ 'iteration' ] , 'search' : 'Random' } ) scores = scores . append ( pd . DataFrame ( { 'ROC AUC' : bayes_params [ 'score' ] , 'iteration' : bayes_params [ 'iteration' ] , 'search' : 'Bayesian' } ) ) scores [ 'ROC AUC' ] = scores [ 'ROC AUC' ] . astype ( np . float32 ) scores [ 'iteration' ] = scores [ 'iteration' ] . astype ( np . int32 ) scores . head ( )
1253	fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 8 , 4 ) ) plt . subplots_adjust ( wspace = 0.5 , hspace = 0.5 ) sns . distplot ( train . cod_prov [ train . cod_prov >= 0 ] , kde = False , ax = axes [ 0 ] , axlabel = 'train cod_prov' ) sns . distplot ( test . cod_prov [ test . cod_prov >= 0 ] , kde = False , ax = axes [ 1 ] , axlabel = 'test cod_prov' )
445	bold ( '**READINGS REALLY PEAKED FROM MAY TO OCTOBER**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( [ 'timestamp' , 'month' ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . lineplot ( data = temp_df , x = 'timestamp' , y = 'meter_reading' , color = 'teal' ) plt . xlabel ( 'Timestamp' , fontsize = 15 ) plt . ylabel ( 'Meter Reading' ) plt . show ( )
827	feature_importances = np . zeros ( train . shape [ 1 ] ) model = lgb . LGBMClassifier ( objective = 'binary' , boosting_type = 'goss' , n_estimators = 10000 , class_weight = 'balanced' )
580	df_china_cases_by_day = df_grouped_china [ df_grouped_china . confirmed > 0 ] df_china_cases_by_day = df_china_cases_by_day . reset_index ( drop = True ) df_china_cases_by_day [ 'day' ] = df_china_cases_by_day . date . apply ( lambda x : ( x - df_china_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_china_cases_by_day = df_china_cases_by_day [ reordered_columns ] df_china_cases_by_day
982	if len ( VALIDATION_MISMATCHES_IDS ) > 0 : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True ) dataset = dataset . filter ( lambda image , label , idnum : tf . reduce_sum ( tf . cast ( idnum == VALIDATION_MISMATCHES_IDS , tf . int32 ) ) > 0 ) dataset = dataset . map ( lambda image , label , idnum : [ image , label ] ) imgs = next ( iter ( dataset . batch ( len ( VALIDATION_MISMATCHES_IDS ) ) ) ) display_batch_of_images ( imgs )
617	def perform_rfc ( df_X , df_Y , test_df_X , test_Y ) : rfr_clf = RandomForestRegressor ( n_estimators = 100 , oob_score = True , max_features = "auto" ) rfr_clf . fit ( df_X , df_Y ) pred_Y = rfr_clf . predict ( test_df_X ) r2_score_rfc = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( rfr_clf . score ( df_X , df_Y ) * 100 , 2 ) returnval = { 'model' : 'RandomForestRegressor' , 'r2_score' : r2_score_rfc } return returnval
1525	import numpy as np import pandas as pd from sklearn import model_selection , preprocessing , metrics import matplotlib . pyplot as plt import seaborn as sns pd . set_option ( 'display.width' , 1000 ) pd . set_option ( 'display.max_rows' , 200 ) pd . set_option ( 'display.max_columns' , 200 ) df_train = pd . read_csv ( '../input/train.csv' ) df_test = pd . read_csv ( '../input/test.csv' )
1474	def select_plate_group ( pp_mult , idx ) : sub_test = test_csv . loc [ test_csv . experiment == all_test_exp [ idx ] , : ] assert len ( pp_mult ) == len ( sub_test ) mask = np . repeat ( plate_groups [ np . newaxis , : , exp_to_group [ idx ] ] , len ( pp_mult ) , axis = 0 ) != \ np . repeat ( sub_test . plate . values [ : , np . newaxis ] , 1108 , axis = 1 ) pp_mult [ mask ] = 0 return pp_mult
1343	for i in range ( 21 , 40 ) : temp_col = features_dtype_int [ i ] plot_count_percent_for_int ( application_train , temp_col )
1225	col_to_drop = train . columns [ train . columns . str . startswith ( 'ps_calc_' ) ] train . drop ( col_to_drop , axis = 1 , inplace = True ) test . drop ( col_to_drop , axis = 1 , inplace = True )
858	import altair as alt alt . renderers . enable ( 'notebook' )
862	model = lgb . LGBMClassifier ( n_estimators = len ( cv_results [ 'auc-mean' ] ) , ** hyperparameters ) model . fit ( train , train_labels ) preds = model . predict_proba ( test ) [ : , 1 ]
840	credit = pd . read_csv ( '../input/credit_card_balance.csv' ) . replace ( { 365243 : np . nan } ) credit = convert_types ( credit ) credit [ 'OVER_LIMIT' ] = credit [ 'AMT_BALANCE' ] > credit [ 'AMT_CREDIT_LIMIT_ACTUAL' ] credit [ 'BALANCE_CLEARED' ] = credit [ 'AMT_BALANCE' ] == 0.0 credit [ 'LOW_PAYMENT' ] = credit [ 'AMT_PAYMENT_CURRENT' ] < credit [ 'AMT_INST_MIN_REGULARITY' ] credit [ 'LATE' ] = credit [ 'SK_DPD' ] > 0.0
857	import ast grid_results [ 'hyperparameters' ] = grid_results [ 'hyperparameters' ] . map ( ast . literal_eval ) random_results [ 'hyperparameters' ] = random_results [ 'hyperparameters' ] . map ( ast . literal_eval )
1089	import numpy as np import pandas as pd import os from tqdm import tqdm_notebook from collections import Counter import warnings warnings . filterwarnings ( "ignore" ) import optuna import multiprocessing from joblib import Parallel , delayed from typing import Any import gc import re import random pd . set_option ( 'display.max_columns' , None ) target = 'accuracy_group_target'
2	features = [ f for f in train . names if f not in [ 'HasDetections' ] ] ftrl = Ftrl ( nepochs = 2 , interactions = True )
695	train . select_dtypes ( np . int64 ) . nunique ( ) . value_counts ( ) . sort_index ( ) . plot . bar ( color = 'blue' , figsize = ( 8 , 6 ) , edgecolor = 'k' , linewidth = 2 ) ; plt . xlabel ( 'Number of Unique Values' ) ; plt . ylabel ( 'Count' ) ; plt . title ( 'Count of Unique Values in Integer Columns' ) ;
487	from keras . preprocessing . text import text_to_word_sequence text = 'The quick brown fox jumped over the lazy dog.' result = text_to_word_sequence ( text ) print ( result )
1325	cols_with_only_one_value = [ ] for col in df_train . columns : if col == 'Target' : continue if df_train [ col ] . value_counts ( ) . shape [ 0 ] == 1 or df_test [ col ] . value_counts ( ) . shape [ 0 ] == 1 : print ( col ) cols_with_only_one_value . append ( col )
523	threshold = 2 y_decision_function_pred = ( y_decision_function_scores [ 6 ] > threshold ) y_decision_function_pred
684	train_all_zero_features = cr . index [ cr ] train . drop ( columns = train_all_zero_features , inplace = True ) count_of_binary_features = ( train . max ( ) == 1 ) . sum ( ) print ( 'Number of binary features: {}' . format ( count_of_binary_features ) )
1204	multi_model = Sequential ( ) multi_model . add ( LSTM ( 4 , input_shape = ( train_X . shape [ 1 ] , train_X . shape [ 2 ] ) ) ) multi_model . add ( Dense ( 1 ) ) multi_model . compile ( loss = 'mse' , optimizer = 'adam' ) multi_history = multi_model . fit ( train_X , train_y , epochs = 3 , batch_size = 100 , validation_data = ( test_X , test_y ) , verbose = 1 , shuffle = False )
577	df_grouped_china = get_df_country_cases ( df_covid , "China" ) df_grouped_china
1299	integer_cols = [ ] for c in num_cols : try : if df_train [ c ] . fillna ( - 1.0 ) . apply ( float . is_integer ) . all ( ) : integer_cols += [ c ] except Exception as e : print ( "error: " , c , e )
678	p_sample = particles . sample ( 8000 ) sns . pairplot ( p_sample , vars = [ 'particle_id' , 'vx' , 'vy' , 'vz' , 'px' , 'py' , 'pz' , 'nhits' ] , hue = 'nhits' , size = 8 ) plt . show ( )
701	def plot_value_counts ( df , col , heads_only = False ) : if heads_only : df = df . loc [ df [ 'parentesco1' ] == 1 ] . copy ( ) plt . figure ( figsize = ( 8 , 6 ) ) df [ col ] . value_counts ( ) . sort_index ( ) . plot . bar ( color = 'blue' , edgecolor = 'k' , linewidth = 2 ) plt . xlabel ( f'{col}' ) ; plt . title ( f'{col} Value Counts' ) ; plt . ylabel ( 'Count' ) plt . show ( ) ;
1071	task_num = np . random . randint ( 1 , 400 ) arc = ARC_solver ( task_num ) arc . plot_task ( ) arc . extract_object_pairs ( method = 1 ) arc . plot_identified_objects ( arc . input_objects [ 0 ] , title = 'input' ) arc . plot_identified_objects ( arc . output_objects [ 0 ] , title = 'output' )
1499	train [ 'created' ] = pd . to_datetime ( train [ 'created' ] ) train [ 'day_of_year' ] = train [ 'created' ] . dt . dayofyear plt . figure ( figsize = ( 13 , 10 ) ) train [ 'week_of_year' ] = train [ 'day_of_year' ] // 7 sns . boxplot ( train [ 'week_of_year' ] , train [ 'listing_id' ] , train [ 'interest_level' ] , hue_order = order ) plt . show ( )
880	fig = plt . figure ( figsize = ( 10 , 10 ) ) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( random_hyp [ 'learning_rate' ] , random_hyp [ 'n_estimators' ] , random_hyp [ 'score' ] , c = random_hyp [ 'score' ] , cmap = plt . cm . seismic_r , s = 40 ) ax . set_xlabel ( 'Learning Rate' ) ax . set_ylabel ( 'Number of Estimators' ) ax . set_zlabel ( 'Score' ) plt . title ( 'Score as Function of Learning Rate and Estimators' , size = 16 ) ;
121	plt . figure ( figsize = ( 10 , 10 ) ) sns . heatmap ( train_df . apply ( lambda x : pd . factorize ( x ) [ 0 ] if x . dtype == 'object' else x ) . corr ( method = 'pearson' ) , annot = True )
1377	col = numeric_features [ 24 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1135	import pandas as pd import numpy as np import matplotlib . pylab as plt pd . set_option ( 'display.max_rows' , 500 ) pd . get_option ( "display.max_columns" , 500 )
1091	params = { 'boosting_type' : 'gbdt' , 'objective' : 'binary' , 'metric' : 'auc' , 'num_leaves' : 13 , 'learning_rate' : 0.01 , 'feature_fraction' : 0.1 , 'bagging_fraction' : 0.3 , 'bagging_freq' : 5 , 'min_data_in_leaf' : 80 , 'min_sum_hessian_in_leaf' : 10.0 , 'num_boost_round' : 999999 , 'early_stopping_rounds' : 500 , 'random_state' : 2019 }
1107	def preprocess ( df ) : df [ "hour" ] = df [ "timestamp" ] . dt . hour df [ "weekend" ] = df [ "timestamp" ] . dt . weekday df [ "month" ] = df [ "timestamp" ] . dt . month df [ "dayofweek" ] = df [ "timestamp" ] . dt . dayofweek
508	COLORMAP = 'pink' APS_FILE_NAME = 'tsa_datasets/stage1/aps/00360f79fd6e02781457eda48f85da90.aps' BODY_ZONES = 'tsa_datasets/stage1/body_zones.png' THREAT_LABELS = 'tsa_datasets/stage1/stage1_labels.csv'
234	n = 15 commits_df . loc [ n , 'commit_num' ] = 19 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 384 commits_df . loc [ n , 'LB_score' ] = 0.25956
334	Xtrain , Xval , Ztrain , Zval = train_test_split ( trainb , targetb , test_size = 0.2 , random_state = 0 ) train_set = lgb . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgb . Dataset ( Xval , Zval , silent = False )
823	train = pd . get_dummies ( train ) test = pd . get_dummies ( test ) train , test = train . align ( test , join = 'inner' , axis = 1 ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
1026	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
533	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( color = " ax.set(xlabel='Hour Of The Day',title=" Reorder Count " )
707	heads = heads . drop ( columns = 'area2' ) heads . groupby ( 'area1' ) [ 'Target' ] . value_counts ( normalize = True )
575	df_grouped = df_covid . groupby ( 'date' ) [ 'date' , 'confirmed' , 'deaths' ] . sum ( ) . reset_index ( ) df_grouped
1556	plt . figure ( figsize = ( 16 , 13 ) ) wc = WordCloud ( background_color = "black" , max_words = 10000 , mask = hcmask3 , stopwords = STOPWORDS , max_font_size = 40 ) wc . generate ( " " . join ( hpl ) ) plt . title ( "HP Lovecraft (Cthulhu-Squidy)" , fontsize = 20 ) plt . imshow ( wc . recolor ( colormap = 'pink_r' , random_state = 17 ) , alpha = 0.98 ) plt . axis ( 'off' )
358	DATA_DIR = '../input/gplearn-data' submission = pd . read_csv ( os . path . join ( '../input/LANL-Earthquake-Prediction' , 'sample_submission.csv' ) , index_col = 'seg_id' ) scaled_train_X = pd . read_csv ( os . path . join ( DATA_DIR , 'scaled_train_X_AF0.csv' ) ) scaled_test_X = pd . read_csv ( os . path . join ( DATA_DIR , 'scaled_test_X_AF0.csv' ) ) train_y = pd . read_csv ( os . path . join ( DATA_DIR , 'train_y_AF0.csv' ) ) predictions = np . zeros ( len ( scaled_test_X ) ) print ( 'ok' )
1110	def preprocess ( df ) : df [ "hour" ] = df [ "timestamp" ] . dt . hour df [ "weekend" ] = df [ "timestamp" ] . dt . weekday df [ "month" ] = df [ "timestamp" ] . dt . month df [ "dayofweek" ] = df [ "timestamp" ] . dt . dayofweek
584	df_population = pd . read_csv ( "../input/countries-of-the-world/countries of the world.csv" ) df_population
602	import seaborn as sns pub_prv_diff = submissions [ "public_score" ] - submissions [ "private_score" ] _ = sns . distplot ( pub_prv_diff , hist = True ) _ = plt . xlabel ( "Public-Private Difference" ) _ = plt . ylabel ( "Density" ) pub_prv_diff . describe ( )
1103	def preprocess ( df ) : df [ "hour" ] = df [ "timestamp" ] . dt . hour df [ "weekend" ] = df [ "timestamp" ] . dt . weekday df [ "month" ] = df [ "timestamp" ] . dt . month df [ "dayofweek" ] = df [ "timestamp" ] . dt . dayofweek
1003	save_dir = '/kaggle/tmp/fake/' if not os . path . exists ( save_dir ) : os . makedirs ( save_dir )
15	train_pad_sequences = pad_sequences ( train_text , maxlen = max_len ) test_pad_sequences = pad_sequences ( test_text , maxlen = max_len ) all_pad_sequences = pad_sequences ( all_text , maxlen = max_len )
383	OUTPUT_DIR = r'd:\ DATA_DIR = r' d : \ SIG_LEN = 150000 NUM_SEG_PER_PROC = 4000 NUM_THREADS = 6 NY_FREQ_IDX = 75000 CUTOFF = 18000 MAX_FREQ_IDX = 20000 FREQ_STEP = 2500
97	df_variants_test = pd . read_csv ( '../input/test_variants' , usecols = [ 'ID' , 'Gene' , 'Variation' ] ) df_text_test = pd . read_csv ( '../input/test_text' , sep = '\|\|' , engine = 'python' , skiprows = 1 , names = [ 'ID' , 'Text' ] ) df_variants_test [ 'Text' ] = df_text_test [ 'Text' ] df_test = df_variants_test
372	decision_tree = DecisionTreeRegressor ( ) decision_tree . fit ( train , target ) acc_model ( 5 , decision_tree , train , test )
1052	from keras . optimizers import Adam custom_objects = custom_objects = { 'swish' : tf . nn . swish , 'FixedDropout' : FixedDropout , 'dice_coef' : dice_coef , 'bce_dice_loss' : bce_dice_loss , 'GroupNormalization' : GroupNormalization , 'AccumOptimizer' : Adam , 'dice_coef_rounded' : dice_coef_rounded } unet_model_path = '../input/severstal-u-net-with-efficientnetb4/model.h5' unet = load_model ( unet_model_path , custom_objects = custom_objects ) unet . summary ( )
1258	bert_tokenizer , bert_nq = get_pretrained_model ( FLAGS . model_name ) if not IS_KAGGLE : bert_nq . trainable_variables
1308	DATA_PATH = '../input/aptos2019-blindness-detection' TRAIN_IMG_PATH = os . path . join ( DATA_PATH , 'train_images' ) TEST_IMG_PATH = os . path . join ( DATA_PATH , 'test_images' ) TRAIN_LABEL_PATH = os . path . join ( DATA_PATH , 'train.csv' ) TEST_LABEL_PATH = os . path . join ( DATA_PATH , 'test.csv' ) train_df = pd . read_csv ( TRAIN_LABEL_PATH ) test_df = pd . read_csv ( TEST_LABEL_PATH ) train_df . head ( )
1318	for col in new_feats : df_train [ col ] . replace ( [ np . inf ] , np . nan , inplace = True ) df_train [ col ] . fillna ( 0 , inplace = True ) df_test [ col ] . replace ( [ np . inf ] , np . nan , inplace = True ) df_test [ col ] . fillna ( 0 , inplace = True )
1533	train_ = df_train def show_count_sum ( df , col , n = 10 ) : return df . groupby ( col ) . agg ( { 'winPlacePerc' : [ 'count' , 'mean' ] } ) . sort_values ( ( 'winPlacePerc' , 'count' ) , ascending = False ) . head ( n )
590	import numpy as np import pandas as pd pd . set_option ( 'display.max_colwidth' , - 1 ) import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
459	road_encoding = { 'Road' : 1 , 'Street' : 2 , 'Avenue' : 2 , 'Drive' : 3 , 'Broad' : 3 , 'Boulevard' : 4 }
1046	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB7 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( train_labels . shape [ 1 ] , activation = 'softmax' ) ] ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) model . summary ( )
1371	col = numeric_features [ 18 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1001	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB3 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . summary ( )
338	Ada_Boost = AdaBoostRegressor ( ) Ada_Boost . fit ( train , target ) acc_model ( 13 , Ada_Boost , train , test )
1168	import numpy as np import pandas as pd from tqdm import tqdm import copy import multiprocessing import nltk import re import gensim . models . word2vec as w2v import matplotlib . pyplot as plt
324	from sklearn . metrics import cohen_kappa_score cohen_kappa = cohen_kappa_score ( y_true , y_pred , weights = 'quadratic' ) cohen_kappa
635	def transpose_df ( df ) : df = df . drop ( [ 'Lat' , 'Long' ] , axis = 1 ) . groupby ( 'Country/Region' ) . sum ( ) . T df . index = pd . to_datetime ( df . index ) return df
680	import os import numpy as np import pandas as pd from keras . preprocessing import image import keras . applications . resnet50 as resnet50 import keras . applications . xception as xception import keras . applications . inception_v3 as inception_v3 import matplotlib . pyplot as plt import seaborn as sns from PIL import Image sns . set_style ( "whitegrid" ) sns . set_context ( "notebook" , font_scale = 1.5 , rc = { "lines.linewidth" : 2.5 } )
950	_ , new_merchant_catint_feats2 , new_merchant_num_feats2 = get_column_types ( new_merchant_ ) print ( '\nCategorical int features: {}' . format ( new_merchant_catint_feats2 ) ) print ( '\nNumerical features: {}' . format ( new_merchant_num_feats2 ) )
149	test_filenames = os . listdir ( "../input/test1/test1" ) test_df = pd . DataFrame ( { 'filename' : test_filenames } ) nb_samples = test_df . shape [ 0 ]
1006	checkpoint = ModelCheckpoint ( 'model.h5' , save_best_only = True ) train_history_step1 = model . fit_generator ( train_gen , validation_data = val_gen , steps_per_epoch = len ( train_gen ) , validation_steps = len ( val_gen ) , callbacks = [ checkpoint ] , epochs = 7 )
714	x = np . array ( [ 1 , 1 , 1 , 2 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 6 , 7 , 8 , 8 , 9 , 9 , 9 ] ) y = np . array ( [ 1 , 2 , 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 1 , 3 , 3 , 2 , 4 , 2 , 2 , 4 ] ) plot_corrs ( x , y )
697	all_equal = train . groupby ( 'idhogar' ) [ 'Target' ] . apply ( lambda x : x . nunique ( ) == 1 ) not_equal = all_equal [ all_equal != True ] print ( 'There are {} households where the family members do not all have the same target.' . format ( len ( not_equal ) ) )
999	print ( 'Session level CV score: ' , np . mean ( results . best_score_ ) ) print ( 'User level CV score: ' , RMSE_log_sum ( predictions_train [ 'predictedRevenue' ] , train_df ) ) print ( predictions_train . groupby ( 'fullVisitorId' ) . count ( ) . info ( ) )
1095	fig , axs = plt . subplots ( 5 , 1 , figsize = ( 10 , 15 ) ) axs . flatten ( ) sample = train [ train . SN_filter == 1 ] . sample ( 1 ) . iloc [ 0 ] for i , err_col in enumerate ( err_cols ) : axs [ i ] . plot ( sample [ err_col ] , color = 'red' , drawstyle = 'steps-mid' ) axs [ i ] . set_title ( err_col )
380	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
90	df_text = pd . read_csv ( '../input/training_text' , sep = '\|\|' , engine = 'python' , skiprows = 1 , names = [ 'ID' , 'Text' ] ) . set_index ( 'ID' ) df_text . head ( )
1084	with strategy . scope ( ) : transformer_layer = TFAutoModel . from_pretrained ( MODEL ) model = build_model_PT ( transformer_layer , max_len = MAX_LEN )
1183	def create_datagen ( ) : return ImageDataGenerator ( zoom_range = 0.15 , fill_mode = 'constant' , cval = 0. , horizontal_flip = True , vertical_flip = True , ) data_generator = create_datagen ( ) . flow ( x_train , y_train , batch_size = BATCH_SIZE , seed = 2019 )
756	pascal_voc_boxes = train_df [ train_df [ 'image_id' ] == image_id ] [ [ 'x_min' , 'y_min' , 'x_max' , 'y_max' ] ] . astype ( np . int32 ) . values coco_boxes = train_df [ train_df [ 'image_id' ] == image_id ] [ [ 'x_min' , 'y_min' , 'width' , 'height' ] ] . astype ( np . int32 ) . values assert ( len ( pascal_voc_boxes ) == len ( coco_boxes ) ) labels = np . ones ( ( len ( pascal_voc_boxes ) , ) )
810	import json with open ( 'trials.json' , 'w' ) as f : f . write ( json . dumps ( trials_dict ) )
11	train [ 'outliers' ] = 0 train . loc [ train [ 'target' ] < - 30 , 'outliers' ] = 1 train [ 'outliers' ] . value_counts ( )
1256	def jsonl_iterator ( jsonl_files , to_json = False ) : for file_path in jsonl_files : with open ( file_path , "r" , encoding = "UTF-8" ) as fp : for jsonl in fp : raw_example = jsonl if to_json : raw_example = json . loads ( jsonl ) yield raw_example creator = TFExampleCreator ( is_training = False ) nq_lines = jsonl_iterator ( [ FLAGS . predict_file ] ) creator . process_nq_lines ( nq_lines = nq_lines , output_tfrecord = FLAGS . test_tf_record , max_examples = 0 , collect_nq_features = False )
24	from sklearn . feature_extraction . text import CountVectorizer bow = CountVectorizer ( )
967	for key , value in dict_sigmoid . items ( ) : plot_curve_fit ( sigmoid , value , 'Confirmed' , key + ' - Logistic Growth Curve for ' , True , 'Logistic' ) plot_curve_fit ( sigmoid , value , 'Deaths' , key + ' - Logistic Growth Curve for ' , True , 'Logistic' )
192	wc = WordCloud ( background_color = "white" , max_words = 5000 , stopwords = STOPWORDS , max_font_size = 50 ) wc . generate ( " " . join ( str ( s ) for s in train . item_description . values ) ) plt . figure ( figsize = ( 20 , 12 ) ) plt . axis ( 'off' ) plt . imshow ( wc , interpolation = 'bilinear' )
543	import pylab import calendar import numpy as np import pandas as pd import seaborn as sn from scipy import stats import missingno as msno from datetime import datetime import matplotlib import matplotlib . pyplot as plt from scipy . stats import kendalltau import warnings matplotlib . style . use ( 'ggplot' ) pd . options . mode . chained_assignment = None warnings . filterwarnings ( "ignore" )
428	model = CatBoostRegressor ( iterations = 1000 , task_type = 'GPU' , verbose = 0 , loss_function = 'RMSE' , boosting_type = 'Plain' , depth = 8 , ) model . fit ( train_pool , eval_set = val_pool , plot = True ) del train_pool , val_pool gc . collect ( )
593	positive_train [ 'temp_list' ] = positive_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) positive_train [ 'temp_list' ] = positive_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) positive_top = Counter ( [ item for sublist in positive_train [ 'temp_list' ] for item in sublist ] ) positive_temp = pd . DataFrame ( positive_top . most_common ( 20 ) ) positive_temp . columns = [ 'Common_words' , 'count' ] positive_temp . style . background_gradient ( cmap = 'Blues' )
488	from keras . preprocessing . text import hashing_trick text = 'The quick brown fox jumped over the lazy dog.' words = set ( text_to_word_sequence ( text ) ) vocab_size = len ( words ) print ( vocab_size ) result = hashing_trick ( text , round ( vocab_size * 1.3 ) , hash_function = 'md5' ) print ( result )
613	plt . figure ( ) plt . plot ( hist . history [ 'loss' ] , lw = 2.0 , color = 'b' , label = 'train' ) plt . plot ( hist . history [ 'val_loss' ] , lw = 2.0 , color = 'r' , label = 'val' ) plt . title ( 'CNN sentiment' ) plt . xlabel ( 'Epochs' ) plt . ylabel ( 'Cross-Entropy Loss' ) plt . legend ( loc = 'upper right' ) plt . show ( )
119	train_df [ 'Expected FVC' ] = train_df [ 'FVC' ] * ( 100 / train_df [ 'Percent' ] ) plt . figure ( figsize = ( 10 , 5 ) ) sns . distplot ( train_df [ 'Expected FVC' ] , rug = True )
1320	for col1 in [ 'public' , 'planpri' , 'noelec' , 'coopele' ] : for col2 in [ 'energcocinar1' , 'energcocinar2' , 'energcocinar3' , 'energcocinar4' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
606	import numpy as np import pandas as pd import seaborn as sns import matplotlib as plt import tensorflow as tf from tensorflow . keras . preprocessing import text , sequence from tensorflow . keras . models import Sequential from tensorflow . keras . layers import Dense , Dropout , Activation from tensorflow . keras . layers import Embedding from tensorflow . keras . layers import Conv1D , GlobalMaxPooling1D , MaxPooling1D from sklearn . model_selection import train_test_split print ( tf . __version__ )
815	bars = alt . Chart ( random_params , width = 500 ) . mark_bar ( color = 'orange' ) . encode ( x = 'boosting_type' , y = alt . Y ( 'count()' , scale = alt . Scale ( domain = [ 0 , 400 ] ) ) ) text = bars . mark_text ( size = 20 , align = 'center' , baseline = 'bottom' ) . encode ( text = 'count()' ) bars + text
1428	fulltable_us = pd . read_csv ( '../input/us-counties-covid-19-dataset/us-counties.csv' ) fulltable_us = fulltable_us . drop ( [ 'fips' ] , axis = 1 ) . groupby ( [ 'date' , 'state' ] ) . sum ( ) . reset_index ( ) fulltable_us . columns = [ 'Date' , 'Province/State' , 'Confirmed' , 'Deaths' ] fulltable_us [ 'Date' ] = pd . to_datetime ( fulltable_us [ 'Date' ] ) fulltable_us
644	length = 5 labels = [ ] for label in df_train [ "labels" ] : if type ( label ) == str : split_label = label . split ( ) [ : : length ] labels += split_label
1085	del model from keras import backend as K import gc K . clear_session ( ) gc . collect ( )
325	import pandas as pd import numpy as np from sklearn . model_selection import train_test_split from keras . preprocessing . text import Tokenizer from keras . preprocessing . sequence import pad_sequences from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers import Embedding from keras . optimizers import Adam from keras . layers import BatchNormalization , Flatten , Conv1D , MaxPooling1D from keras . layers import Dropout from keras . callbacks import EarlyStopping , ModelCheckpoint , ReduceLROnPlateau import warnings warnings . filterwarnings ( 'ignore' )
266	etr = ExtraTreesRegressor ( ) etr . fit ( train , target ) acc_model ( 12 , etr , train , test )
273	n = 2 commits_df . loc [ n , 'commit_num' ] = 6 commits_df . loc [ n , 'Dropout_model' ] = 0.36 commits_df . loc [ n , 'FVC_weight' ] = 0.35 commits_df . loc [ n , 'LB_score' ] = - 6.8158
1479	emb_szs = data . get_emb_szs ( { } ) model = TabularModel ( emb_szs , len ( data . cont_names ) , out_sz = data . c , layers = [ 200 , 100 ] , ps = None , emb_drop = 0. , y_range = None , use_bn = True ) learn = Learner ( data , model , loss_func = KappaLoss ( ) , path = '/tmp' )
382	import os import time import warnings import traceback import numpy as np import pandas as pd from scipy import stats import scipy . signal as sg import multiprocessing as mp from scipy . signal import hann from scipy . signal import hilbert from scipy . signal import convolve from sklearn . linear_model import LinearRegression from sklearn . preprocessing import StandardScaler import xgboost as xgb import lightgbm as lgb from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error from sklearn . metrics import mean_absolute_error from tqdm import tqdm warnings . filterwarnings ( "ignore" )
1136	from tensorflow . keras . preprocessing . image import ImageDataGenerator Image_path = '/kaggle/input/siim-isic-melanoma-classification/jpeg/' train_csv = pd . read_csv ( '/kaggle/input/siim-isic-melanoma-classification/train.csv' , dtype = str ) test_csv = pd . read_csv ( '/kaggle/input/siim-isic-melanoma-classification/test.csv' , dtype = str )
1068	def compute_text_and_questions ( test , tokenizer ) : test_text = tokenizer . texts_to_sequences ( test . text . values ) test_questions = tokenizer . texts_to_sequences ( test . question . values ) test_text = sequence . pad_sequences ( test_text , maxlen = 300 ) test_questions = sequence . pad_sequences ( test_questions ) return test_text , test_questions
201	pix_resampled , spacing = resample ( first_patient_pixels , first_patient , [ 1 , 1 , 1 ] ) print ( "Shape before resampling\t" , first_patient_pixels . shape ) print ( "Shape after resampling\t" , pix_resampled . shape )
480	import sys import pandas as pd import numpy as np import lightgbm as lgb from sklearn . model_selection import train_test_split
147	learning_rate_reduction = ReduceLROnPlateau ( monitor = 'val_acc' , patience = 2 , verbose = 1 , factor = 0.5 , min_lr = 0.00001 )
685	plt . figure ( figsize = ( 13 , 5 ) ) ax = sns . distplot ( target , kde = False , norm_hist = False , bins = 200 ) ax . get_yaxis ( ) . set_major_formatter ( matplotlib . ticker . FuncFormatter ( lambda x , p : format ( int ( x ) , ',' ) ) ) ax . set_xlabel ( 'Transaction value' ) ax . set_ylabel ( 'Number of transactions' ) ax . set_title ( 'Distribution of target transaction values' ) ;
557	len_cat = trn_cat . shape [ 1 ] len_real = trn_real . shape [ 1 ] len_text = trn_text . shape [ 1 ] size_embedding = 5000
91	plt . figure ( ) ax = df [ 'Gene' ] . value_counts ( ) . plot ( kind = 'area' ) ax . get_xaxis ( ) . set_ticks ( [ ] ) ax . set_title ( 'Gene Frequency Plot' ) ax . set_xlabel ( 'Gene' ) ax . set_ylabel ( 'Frequency' ) plt . tight_layout ( ) plt . show ( )
1210	total = merchant . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( merchant . isnull ( ) . sum ( ) / merchant . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_data . head ( 20 )
287	n = 16 commits_df . loc [ n , 'commit_num' ] = 23 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8402
601	import matplotlib . pyplot as plt plt . figure ( figsize = ( 11 , 6 ) ) plt . plot ( submissions [ "n" ] , submissions [ "public_score" ] , label = "Public" ) plt . plot ( submissions [ "n" ] , submissions [ "private_score" ] , label = "Private" ) plt . xlabel ( "Samples spoiled" ) plt . ylabel ( "Gini Score" ) _ = plt . legend ( )
1316	continuous_features = [ col for col in df_train . columns if col not in binary_cat_features ] continuous_features = [ col for col in continuous_features if col not in features_object ] continuous_features = [ col for col in continuous_features if col not in [ 'Id' , 'Target' , 'idhogar' ] ]
99	import pandas as pd import keras import os import numpy as np from sklearn . metrics import log_loss from keras import Sequential from keras . layers import * from keras . optimizers import Adam from sklearn . model_selection import train_test_split import cv2 from mtcnn import MTCNN from tqdm . notebook import tqdm
1304	for c in cat_cols : df_train [ c ] = df_train [ c ] . fillna ( "missing" ) for c in cat_cols : df_test [ c ] = df_test [ c ] . fillna ( "missing" )
913	train_corrs_removed = train . drop ( columns = cols_to_remove ) test_corrs_removed = test . drop ( columns = cols_to_remove ) print ( 'Training Corrs Removed Shape: ' , train_corrs_removed . shape ) print ( 'Testing Corrs Removed Shape: ' , test_corrs_removed . shape )
1346	temp_col = 'EXT_SOURCE_3' sns . kdeplot ( application_train_float . loc [ application_train_float [ 'TARGET' ] == 0 , temp_col ] , label = 'repay(0)' , color = 'r' ) sns . kdeplot ( application_train_float . loc [ application_train_float [ 'TARGET' ] == 1 , temp_col ] , label = 'not repay(1)' , color = 'b' ) plt . title ( 'KDE for {} splitted by target' . format ( temp_col ) ) plt . show ( )
406	def stage_1b_PIL ( img_pil ) : img_pil = ImageOps . box_blur ( img_pil , radius = 1 ) img_pil = img_pil . transpose ( Image . FLIP_LEFT_RIGHT ) return np . asarray ( img_pil ) def stage_1b_cv2 ( img ) : img = cv2 . blur ( img , ksize = ( 3 , 3 ) ) img = cv2 . flip ( img , flipCode = 1 ) return img
1065	model . load_weights ( 'model.h5' ) y_test = model . predict_generator ( test_gen , steps = len ( test_gen ) , verbose = 1 )
744	def macro_f1_score ( labels , predictions ) : predictions = predictions . reshape ( len ( np . unique ( labels ) ) , - 1 ) . argmax ( axis = 0 ) metric_value = f1_score ( labels , predictions , average = 'macro' ) return 'macro_f1' , metric_value , True
683	cr = ( train . max ( ) == 0 ) & ( train . min ( ) == 0 ) train_all_zero_feature_count = cr . index [ cr ] . shape [ 0 ] cr2 = ( test . max ( ) == 0 ) & ( test . max ( ) == 0 ) test_all_zero_feature_count = cr2 . index [ cr2 ] . shape [ 0 ] print ( 'Number of training features with all 0 values:\t{}' . format ( train_all_zero_feature_count ) ) print ( 'Number of test features with all 0 values:\t{}' . format ( test_all_zero_feature_count ) )
368	linreg = LinearRegression ( ) linreg . fit ( train , target ) acc_model ( 0 , linreg , train , test )
19	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . target . values , bins = 200 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
1586	start = datetime ( 2012 , 1 , 1 , 0 , 0 , 0 ) . date ( ) market_train_df = market_train_df . loc [ market_train_df [ 'time' ] . dt . date >= start ] . reset_index ( drop = True ) news_train_df = news_train_df . loc [ news_train_df [ 'time' ] . dt . date >= start ] . reset_index ( drop = True ) del start gc . collect ( )
204	import os , random , re , math , time random . seed ( a = 42 ) import numpy as np import pandas as pd import tensorflow as tf import tensorflow . keras . backend as K import efficientnet . tfkeras as efn import PIL from kaggle_datasets import KaggleDatasets from tqdm import tqdm
878	random_hyp [ 'set' ] = 'Random Search' opt_hyp [ 'set' ] = 'Bayesian' hyp = random_hyp . append ( opt_hyp , ignore_index = True , sort = True ) hyp . head ( )
851	com = 1 for x in param_grid . values ( ) : com *= len ( x ) print ( 'There are {} combinations' . format ( com ) )
1585	from kaggle . competitions import twosigmanews env = twosigmanews . make_env ( ) print ( 'Data is loaded' )
1229	nb = BernoulliNB ( ) outcomes = cross_validate_sklearn ( nb , x_train , y_train , x_test , kf , scale = True , verbose = True ) nb_cv = outcomes [ 0 ] nb_train_pred = outcomes [ 1 ] nb_test_pred = outcomes [ 2 ] nb_train_pred_df = pd . DataFrame ( columns = [ 'prediction_probability' ] , data = nb_train_pred ) nb_test_pred_df = pd . DataFrame ( columns = [ 'prediction_probability' ] , data = nb_test_pred )
1138	def jpg_tag ( image_name ) : return image_name + '.jpg' train_csv [ 'image_name' ] = train_csv [ 'image_name' ] . apply ( jpg_tag ) test_csv [ 'image_name' ] = test_csv [ 'image_name' ] . apply ( jpg_tag )
1565	from scipy . signal import hilbert from scipy . signal import hann from scipy . signal import convolve
86	def calc_age_category ( x ) : if x < 3 : return 'young' if x < 5 : return 'young adult' if x < 10 : return 'adult' return 'old' animals [ 'AgeCategory' ] = animals . AgeInYears . apply ( calc_age_category )
801	boosting_type = { 'boosting_type' : hp . choice ( 'boosting_type' , [ { 'boosting_type' : 'gbdt' , 'subsample' : hp . uniform ( 'subsample' , 0.5 , 1 ) } , { 'boosting_type' : 'dart' , 'subsample' : hp . uniform ( 'subsample' , 0.5 , 1 ) } , { 'boosting_type' : 'goss' , 'subsample' : 1.0 } ] ) } hyperparams = sample ( boosting_type ) hyperparams
837	installments_info = agg_grandchild ( installments , previous , 'SK_ID_PREV' , 'SK_ID_CURR' , 'IN' ) del installments installments_info . shape
308	def plot_word_cloud ( x , col ) : corpus = [ ] for k in x [ col ] . str . split ( ) : for i in k : corpus . append ( i ) plt . figure ( figsize = ( 12 , 8 ) ) word_cloud = WordCloud ( background_color = 'black' , max_font_size = 80 ) . generate ( " " . join ( corpus [ : 50 ] ) ) plt . imshow ( word_cloud ) plt . axis ( 'off' ) plt . show ( ) return corpus [ : 50 ]
404	import os this_path = os . path . dirname ( '.' ) INPUT_PATH = os . path . abspath ( os . path . join ( this_path , '..' , 'input' ) ) TRAIN_DATA = os . path . join ( INPUT_PATH , "train" ) from glob import glob filenames = glob ( os . path . join ( TRAIN_DATA , "*.jpg" ) ) len ( filenames )
781	corrs = data . corr ( ) plt . figure ( figsize = ( 12 , 12 ) ) sns . heatmap ( corrs , annot = True , vmin = - 1 , vmax = 1 , fmt = '.3f' , cmap = plt . cm . PiYG_r ) ;
107	import matplotlib . pyplot as plt savePickleBZ ( 'before.pbz' , before ) savePickleBZ ( 'sets.pbz' , sets ) def showBefore ( before ) : before = np . array ( before , dtype = np . float ) before = before / ( before + np . transpose ( np . copy ( before ) ) + 1e-30 ) before *= 255 before = np . array ( before , dtype = np . uint8 ) plt . figure ( figsize = ( 16 , 16 ) ) plt . imshow ( before ) showBefore ( before )
1393	col = numeric_features [ 42 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
911	threshold = 0.8 above_threshold_vars = { } for col in corrs : above_threshold_vars [ col ] = list ( corrs . index [ corrs [ col ] > threshold ] )
1455	def format_prediction_string ( image_id , result ) : prediction_strings = [ ] for i in range ( len ( result [ 'detection_scores' ] ) ) : class_name = result [ 'detection_class_names' ] [ i ] . decode ( "utf-8" ) boxes = result [ 'detection_boxes' ] [ i ] score = result [ 'detection_scores' ] [ i ] prediction_strings . append ( f"{class_name} {score} " + " " . join ( map ( str , boxes ) ) ) prediction_string = " " . join ( prediction_strings ) return { "ImageID" : image_id , "PredictionString" : prediction_string }
342	df_train = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftrain.pickle' , 'rb' ) ) df_test = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftest.pickle' , 'rb' ) ) print ( df_train . shape ) print ( df_test . shape )
1434	data = All_df [ 0 : 91713 ] Test_data = All_df [ 91713 : 131021 ] y = np . array ( All_y [ 0 : 91713 ] . tolist ( ) ) random_state = 23 X_train , X_test , y_train , y_test = train_test_split ( data , y , test_size = 0.2 , random_state = random_state , stratify = y ) X_train = pd . DataFrame ( X_train , columns = data . columns ) X_test = pd . DataFrame ( X_test , columns = data . columns )
426	import os import gc import pandas as pd import numpy as np import matplotlib . pyplot as plt from datetime import datetime , timedelta from catboost import Pool , CatBoostRegressor pd . set_option ( 'display.max_columns' , None ) from catboost . utils import get_gpu_device_count from tqdm . notebook import tqdm print ( 'available GPU devices catboost:' , get_gpu_device_count ( ) )
1063	train_df [ 'isNan' ] = pd . isna ( train_df [ 'EncodedPixels' ] ) train_df [ 'ImageId' ] = train_df [ 'ImageId_ClassId' ] . apply ( lambda x : x . split ( '_' ) [ 0 ] ) train_df . head ( )
1506	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
401	train_df = pd . read_csv ( os . path . join ( DATA_DIR , 'train.csv' ) , dtype = { 'acoustic_data' : np . int16 , 'time_to_failure' : np . float32 } ) print ( train_df . shape ) print ( 'ok' )
955	from sklearn . model_selection import train_test_split tr_ids , valid_ids , tr_coverage , valid_coverage = train_test_split ( train_ids , train_df . coverage . values , test_size = 0.2 , stratify = train_df . coverage_class , random_state = 1234 )
1199	def create_dataset ( dataset , look_back = 1 ) : dataX , dataY = [ ] , [ ] for i in range ( len ( dataset ) - look_back ) : a = dataset [ i : ( i + look_back ) , 0 ] dataX . append ( a ) dataY . append ( dataset [ i + look_back , 0 ] ) print ( len ( dataY ) ) return np . array ( dataX ) , np . array ( dataY )
127	def lung_volume ( patient , masks ) : slice_thickness = patient [ 0 ] . SliceThickness pixel_spacing = patient [ 0 ] . PixelSpacing return np . round ( np . sum ( masks ) * slice_thickness * pixel_spacing [ 0 ] * pixel_spacing [ 1 ] , 3 )
802	subsample = hyperparams [ 'boosting_type' ] . get ( 'subsample' , 1.0 ) hyperparams [ 'boosting_type' ] = hyperparams [ 'boosting_type' ] [ 'boosting_type' ] hyperparams [ 'subsample' ] = subsample hyperparams
181	two_cell_indices = ndimage . find_objects ( labels ) [ 1 ] cell_mask = mask [ two_cell_indices ] cell_mask_opened = ndimage . binary_opening ( cell_mask , iterations = 8 )
1121	DC = animals [ [ 'OutcomeType' , 'Neutered' , 'AnimalType' ] ] . groupby ( [ 'AnimalType' , 'OutcomeType' , 'Neutered' ] ) . size ( ) . unstack ( ) . unstack ( ) DC . plot ( kind = 'bar' , stacked = False , figsize = ( 10 , 8 ) , rot = - 30 )
168	print ( 'Minimum number of clicks needed to download an app :' , df . freq_ip [ df [ 'is_attributed' ] == 1 ] . min ( ) ) print ( "How many IP do we have in each category ?\n" , IP_ready_to_merge [ 'clicker_type' ] . value_counts ( ) ) print ( "How many clicks, clickers of each caterogy have generate ?\n" , df [ 'clicker_type' ] . value_counts ( ) )
6	sns . set_style ( "whitegrid" ) ax = sns . violinplot ( x = train . target . values ) plt . show ( )
818	model = lgb . LGBMClassifier ( n_estimators = len ( cv_results [ 'auc-mean' ] ) , ** hyperparameters ) model . fit ( train , train_labels ) preds = model . predict_proba ( test ) [ : , 1 ] submission = pd . DataFrame ( { 'SK_ID_CURR' : test_ids , 'TARGET' : preds } ) submission . to_csv ( 'submission_random_search.csv' , index = False )
1468	sns . catplot ( x = "store_id" , y = "total_sales" , hue = "cat_id" , data = train_sales , kind = "bar" , height = 8 , aspect = 1 ) ;
1222	cat_cols = [ 'ps_ind_02_cat' , 'ps_car_04_cat' , 'ps_car_09_cat' , 'ps_ind_05_cat' , 'ps_car_01_cat' , 'ps_car_11_cat' ] train_freq , test_freq = freq_encoding ( cat_cols , train , test ) train = pd . concat ( [ train , train_freq ] , axis = 1 ) test = pd . concat ( [ test , test_freq ] , axis = 1 )
1551	melted = pd . melt ( train_df . iloc [ : , 194 : ] ) melted [ "value" ] = pd . to_numeric ( melted [ "value" ] )
727	ind_agg = ind_agg . drop ( columns = to_drop ) ind_feats = list ( ind_agg . columns ) final = heads . merge ( ind_agg , on = 'idhogar' , how = 'left' ) print ( 'Final features shape: ' , final . shape )
963	shap . dependence_plot ( "returnsClosePrevRaw10_lag_3_mean" , shap_values , X_importance )
1114	N = 10 scores = np . zeros ( N , ) for i in range ( N ) : p = i * 1. / N v = p * leak_df [ 'pred1' ] . values + ( 1. - p ) * leak_df [ 'pred3' ] . values vl1p = np . log1p ( v ) scores [ i ] = np . sqrt ( mean_squared_error ( vl1p , leak_df . meter_reading_l1p ) )
1559	from nltk . stem import WordNetLemmatizer lemm = WordNetLemmatizer ( ) print ( "The lemmatized form of leaves is: {}" . format ( lemm . lemmatize ( "leaves" ) ) )
1588	assetNameGB = market_train_df [ market_train_df [ 'assetName' ] == 'Unknown' ] . groupby ( 'assetCode' ) unknownAssets = assetNameGB . size ( ) . reset_index ( 'assetCode' ) print ( "There are {} unique assets without assetName in the training set" . format ( unknownAssets . shape [ 0 ] ) )
1535	def dist_matrix ( coords , i , penalize = False ) : begin = np . array ( [ df . X [ i ] , df . Y [ i ] ] ) [ : , np . newaxis ] mat = coords - begin if penalize : return np . linalg . norm ( mat , ord = 2 , axis = 0 ) * penalization else : return np . linalg . norm ( mat , ord = 2 , axis = 0 )
1160	from sklearn import preprocessing le = preprocessing . LabelEncoder ( ) le . fit ( train_df [ 'category_id' ] ) train_df [ 'category_id_le' ] = le . transform ( train_df [ 'category_id' ] ) class_map = dict ( sorted ( train_df [ [ 'category_id_le' , 'category_id' ] ] . values . tolist ( ) ) )
421	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yt , yt_pred ) print ( confusion )
1267	print ( f"{ckpt_manager._directory}" ) os . system ( f"ls -l {ckpt_manager._directory} > results.txt" ) with open ( "results.txt" , "r" , encoding = "UTF-8" ) as fp : for line in fp : print ( line . strip ( ) )
438	bold ( '**Preview of building data**' ) display ( building . head ( 3 ) ) bold ( '**Preview of Weather Train Data:**' ) display ( weather_train . head ( 3 ) ) bold ( '**Preview of Weather Test Data:**' ) display ( weather_test . head ( 3 ) ) bold ( '**Preview of Train Data:**' ) display ( train . head ( 3 ) ) bold ( '**Preview of Test Data:**' ) display ( test . head ( 3 ) )
582	df_grouped_iran = get_df_country_cases ( df_covid , "Iran" ) df_iran_cases_by_day = df_grouped_iran [ df_grouped_iran . confirmed > 0 ] df_iran_cases_by_day = df_iran_cases_by_day . reset_index ( drop = True ) df_iran_cases_by_day [ 'day' ] = df_iran_cases_by_day . date . apply ( lambda x : ( x - df_iran_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_iran_cases_by_day = df_iran_cases_by_day [ reordered_columns ] df_iran_cases_by_day
435	vectorizer = TfidfVectorizer ( min_df = 0.00009 , max_features = 200000 , tokenizer = lambda x : x . split ( ) , ngram_range = ( 1 , 3 ) ) X_train_multilabel = vectorizer . fit_transform ( X_train [ 'question' ] ) X_test_multilabel = vectorizer . transform ( X_test [ 'question' ] )
1322	for col1 in [ 'abastaguadentro' , 'abastaguafuera' , 'abastaguano' ] : for col2 in [ 'sanitario1' , 'sanitario2' , 'sanitario3' , 'sanitario5' , 'sanitario6' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
16	toxic_predictions_train = pd . DataFrame ( columns = list_classes , data = train_pred ) toxic_predictions_test = pd . DataFrame ( columns = list_classes , data = test_pred ) toxic_predictions_train [ 'question_text' ] = train [ 'question_text' ] . values toxic_predictions_test [ 'question_text' ] = test [ 'question_text' ] . values toxic_predictions_train [ 'qid' ] = train_qid toxic_predictions_test [ 'qid' ] = test_qid
1041	trials_df = pd . DataFrame ( [ parse_trial_state ( t ) for t in tuner . oracle . trials . values ( ) ] ) trials_df . to_csv ( 'trials_table.csv' , index = False ) trials_df
1309	model_path = '../input/aptos-2019-pretrained-models/' weight_file = 'weights-InceptionResNetV2.hdf5' model = load_model ( os . path . join ( model_path , weight_file ) )
446	bold ( '**MANUFACTURING REALLY BUCKED THE GENERAL TREND**' ) temp_df = train . groupby ( [ 'timestamp' , "primary_use" ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . FacetGrid ( temp_df , col = "primary_use" , col_wrap = 2 , height = 4 , aspect = 2 , sharey = False ) ax . map ( sns . lineplot , 'timestamp' , 'meter_reading' , color = "teal" ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
136	nunique_vals = list ( ) for column in all_df : nunique_vals . append ( all_df [ column ] . nunique ( ) ) pd . DataFrame ( { 'columns' : all_df . columns , 'num_of_unique' : nunique_vals } )
687	train_df [ [ 'ID' , 'Subtype' ] ] = train_df [ 'ID' ] . str . rsplit ( pat = '_' , n = 1 , expand = True ) print ( train_df . shape ) train_df . head ( )
587	data_time = df_target_country . day . values . astype ( np . float64 ) infected_individuals = df_target_country . confirmed . values / target_population dead_individuals = df_target_country . deaths . values / target_population
1337	temp_col = features_dtype_object [ 4 ] plot_count_percent_for_object ( application_train , temp_col ) plot_count_percent_for_object ( application_object_na_filled , temp_col )
279	n = 8 commits_df . loc [ n , 'commit_num' ] = 14 commits_df . loc [ n , 'Dropout_model' ] = 0.36 commits_df . loc [ n , 'FVC_weight' ] = 0.225 commits_df . loc [ n , 'LB_score' ] = - 6.8100
392	cat_level2_counts = CATEGORY_NAMES_DF . groupby ( 'category_level2' ) [ 'category_level2' ] . count ( ) print ( cat_level2_counts . describe ( ) ) print ( "Level 2 the most frequent category: " , cat_level2_counts . argmax ( ) )
1443	train [ 'click_rnd' ] = train [ 'click_time' ] . dt . round ( 'H' ) train [ [ 'click_rnd' , 'is_attributed' ] ] . groupby ( [ 'click_rnd' ] , as_index = True ) . count ( ) . plot ( ) plt . title ( 'HOURLY CLICK FREQUENCY' ) ; plt . ylabel ( 'Number of Clicks' ) ; train [ [ 'click_rnd' , 'is_attributed' ] ] . groupby ( [ 'click_rnd' ] , as_index = True ) . mean ( ) . plot ( ) plt . title ( 'HOURLY CONVERSION RATIO' ) ; plt . ylabel ( 'Converted Ratio' ) ;
1303	test_num_cols = list ( set ( num_cols ) - set ( [ 'isFraud' ] ) ) a = df_test [ test_num_cols ] . isnull ( ) . any ( ) test_null_num_cols = a [ a ] . index
943	cred_card_bal = pd . read_csv ( "../input/credit_card_balance.csv" ) cred_card_bal = cred_card_bal . drop ( [ 'SK_ID_PREV' ] , axis = 1 ) cred_card_bal_dfs = feature_aggregator_on_df ( cred_card_bal , aggs_cat , aggs_num , [ 'SK_ID_CURR' ] , 'cred_card_balance' , 'basic' , save = False )
1130	train = train . drop ( "diff_V109_V110" , axis = 1 ) test = test . drop ( "diff_V109_V110" , axis = 1 ) train = train . drop ( "diff_V329_V330" , axis = 1 ) test = test . drop ( "diff_V329_V330" , axis = 1 ) train = train . drop ( "diff_V316_V331" , axis = 1 ) test = test . drop ( "diff_V316_V331" , axis = 1 ) train = train . drop ( "diff_V4_V5" , axis = 1 ) test = test . drop ( "diff_V4_V5" , axis = 1 )
457	plt . figure ( figsize = ( 15 , 6 ) ) df_train . IntersectionId . value_counts ( ) [ : 50 ] . plot ( kind = 'bar' , color = 'teal' ) plt . xlabel ( "Intersection Number" , fontsize = 18 ) plt . ylabel ( "Count" , fontsize = 18 ) plt . title ( "TOP 50 most commmon IntersectionID's " , fontsize = 22 ) plt . show ( )
1576	train = pd . read_csv ( '../input/pku-autonomous-driving/train.csv' ) k = np . array ( [ [ 2304.5479 , 0 , 1686.2379 ] , [ 0 , 2305.8757 , 1354.9849 ] , [ 0 , 0 , 1 ] ] , dtype = np . float32 )
1422	stats = [ ] df = full_table [ full_table [ 'Country/Region' ] != 'China' ] [ [ 'Province/State' , 'Country/Region' , 'Date' , 'Confirmed' , 'Deaths' , 'Recovered' , 'Active' ] ] . groupby ( 'Date' ) . sum ( ) print ( 'World COVID-19 Prediction(Without China Data)' ) opt_display_model ( df , stats )
1454	history = [ ] resa2 = clustering ( hits , stds , filters , phik = 3.3 , nu = nu , truth = truth , history = history ) resa2 [ "event_id" ] = event_num score = score_event_fast ( truth , resa2 . rename ( index = str , columns = { "label" : "track_id" } ) ) print ( "Your score: " , score )
518	from sklearn . base import BaseEstimator import numpy as np class BaseClassifier ( BaseEstimator ) : def fit ( self , X , y = None ) : pass def predict ( self , X ) : return np . zeros ( ( len ( X ) , 1 ) , dtype = bool ) base_clf = BaseClassifier ( ) cross_val_score ( base_clf , X_train , y_train , cv = 10 , scoring = "accuracy" ) . mean ( )
912	cols_to_remove = [ ] cols_seen = [ ] cols_to_remove_pair = [ ] for key , value in above_threshold_vars . items ( ) : cols_seen . append ( key ) for x in value : if x == key : next else : if x not in cols_seen : cols_to_remove . append ( x ) cols_to_remove_pair . append ( key ) cols_to_remove = list ( set ( cols_to_remove ) ) print ( 'Number of columns to remove: ' , len ( cols_to_remove ) )
442	bold ( '**MONTHLY READINGS ARE HIGHEST CHANGES BASED ON BUILDING TYPE**' ) temp_df = train . groupby ( [ 'month' , 'primary_use' ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . FacetGrid ( temp_df , col = "primary_use" , col_wrap = 2 , height = 4 , aspect = 2 , sharey = False ) ax . map ( plt . plot , 'month' , 'meter_reading' , color = "teal" , linewidth = 3 ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
798	model = lgb . LGBMClassifier ( random_state = 50 ) train_set = lgb . Dataset ( train_features , label = train_labels ) test_set = lgb . Dataset ( test_features , label = test_labels )
249	def fa ( N , a , b , beta ) : fa = - beta * a * b return fa def fb ( N , a , b , beta , gamma ) : fb = beta * a * b - gamma * b return fb def fc ( N , b , gamma ) : fc = gamma * b return fc
556	df_text = df [ text_feature ] . fillna ( ' ' ) df_text [ 'full_text' ] = '' for f in text_feature : df_text [ 'full_text' ] = df_text [ 'full_text' ] + df_text [ f ]
1464	order = [ ] with open ( 'lk.sol' , 'r' ) as fp : lines = fp . readlines ( ) order = [ int ( v . split ( ' ' ) [ 0 ] ) for v in lines [ 1 : ] ] + [ 0 ]
985	train_clean [ 'TransactionAmt' ] = np . log ( train_clean [ 'TransactionAmt' ] + 1 ) train_clean [ 'dist1' ] = np . log ( train_clean [ 'dist1' ] + 1 ) train_clean [ 'dist2' ] = np . log ( train_clean [ 'dist2' ] + 1 ) test_clean [ 'TransactionAmt' ] = np . log ( test_clean [ 'TransactionAmt' ] + 1 ) test_clean [ 'dist1' ] = np . log ( test_clean [ 'dist1' ] + 1 ) test_clean [ 'dist2' ] = np . log ( test_clean [ 'dist2' ] + 1 )
375	Xtrain , Xval , Ztrain , Zval = train_test_split ( trainb , targetb , test_size = 0.2 , random_state = 0 ) train_set = lgb . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgb . Dataset ( Xval , Zval , silent = False )
671	expensive = df [ df . price > 1000000 ] . category_name . value_counts ( ) . map ( lambda x : '{:.2f}%' . format ( x / df . shape [ 0 ] * 100 ) ) print ( 'Categories of items > 1M \u20BD (top 10)' ) expensive . head ( 10 )
1500	import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt
632	df_hmp [ 'log1p_Demanda_uni_equil_sum' ] = np . log1p ( df_hmp . Demanda_uni_equil_sum ) g = sns . FacetGrid ( df_hmp , row = 'Semana' ) g = g . map ( sns . distplot , 'log1p_Demanda_uni_equil_sum' )
1361	col = numeric_features [ 8 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
455	i = 0 result = [ ] step_size = 50000 for j in tqdm ( range ( int ( np . ceil ( test . shape [ 0 ] / 50000 ) ) ) ) : result . append ( np . expm1 ( sum ( [ model . predict ( test . iloc [ i : i + step_size ] ) for model in models ] ) / folds ) ) i += step_size
1395	col = numeric_features [ 44 ] plot_category_percent_of_target_for_numeric ( col )
139	all_df [ 'ord_5_1' ] = all_df [ 'ord_5' ] . str [ 0 ] all_df [ 'ord_5_2' ] = all_df [ 'ord_5' ] . str [ 1 ] all_df = all_df . drop ( 'ord_5' , axis = 1 )
676	import os import numpy as np import pandas as pd from trackml . dataset import load_event from trackml . randomize import shuffle_hits from trackml . score import score_event import matplotlib . pyplot as plt from mpl_toolkits . mplot3d import Axes3D import seaborn as sns
241	n = 22 commits_df . loc [ n , 'commit_num' ] = 26 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 210 commits_df . loc [ n , 'LB_score' ] = 0.25844
195	def get_tsne ( X ) : X_reshape = X . reshape ( len ( X ) , - 1 ) tsne = TSNE ( n_components = 2 , init = 'pca' , n_iter = 2500 , random_state = 23 ) X_reshape_2D = tsne . fit_transform ( X_reshape ) return X_reshape_2D
1173	num_features = 200 min_word_count = 1 num_workers = multiprocessing . cpu_count ( ) context_size = 7 downsampling = 1e-3 seed = 1
355	lsvr = LinearSVR ( C = 0.05 , max_iter = 1000 ) . fit ( dfe , target_fe ) model = SelectFromModel ( lsvr , prefit = True ) X_new = model . transform ( dfe ) X_selected_df = pd . DataFrame ( X_new , columns = [ dfe . columns [ i ] for i in range ( len ( dfe . columns ) ) if model . get_support ( ) [ i ] ] ) X_selected_df . shape
759	train . fillna ( 0 , inplace = True ) train . replace ( - np . inf , 0 , inplace = True ) train . replace ( np . inf , 0 , inplace = True ) test . fillna ( 0 , inplace = True ) test . replace ( - np . inf , 0 , inplace = True ) test . replace ( np . inf , 0 , inplace = True )
85	def calc_age_in_years ( x ) : x = str ( x ) if x == 'nan' : return 0 age = int ( x . split ( ) [ 0 ] ) if x . find ( 'year' ) > - 1 : return age if x . find ( 'month' ) > - 1 : return age / 12. if x . find ( 'week' ) > - 1 : return age / 52. if x . find ( 'day' ) > - 1 : return age / 365. else : return 0
1264	bert_tokenizer , bert_nq = get_pretrained_model ( FLAGS . model_name ) if not IS_KAGGLE : bert_nq . trainable_variables
272	n = 1 commits_df . loc [ n , 'commit_num' ] = 4 commits_df . loc [ n , 'Dropout_model' ] = 0.36 commits_df . loc [ n , 'FVC_weight' ] = 0.25 commits_df . loc [ n , 'LB_score' ] = - 6.8105
625	ignored_feat2 = [ 'FEATURE_10' , 'FEATURE_257' , 'FEATURE_245' , 'FEATURE_201' , 'FEATURE_246' , 'FEATURE_247' , 'FEATURE_145' , 'FEATURE_202' , 'FEATURE_27' , 'FEATURE_157' , 'FEATURE_146' , 'FEATURE_156' , 'FEATURE_234' , 'FEATURE_242' , 'FEATURE_75' , 'FEATURE_231' , 'FEATURE_29' , 'FEATURE_2' , 'FEATURE_3' , 'FEATURE_5' , 'FEATURE_6' , 'FEATURE_15' , 'FEATURE_16' , 'FEATURE_17' , 'FEATURE_18' , 'FEATURE_19' , 'FEATURE_20' , 'FEATURE_22' , 'FEATURE_25' , 'FEATURE_28' , 'FEATURE_39' , 'FEATURE_40' , 'FEATURE_41' , 'FEATURE_139' , 'FEATURE_141' , 'FEATURE_144' , 'FEATURE_159' , 'FEATURE_229' , 'FEATURE_249' , 'FEATURE_256' ]
895	late_payment = ft . Feature ( es [ 'installments' ] [ 'installments_due_date' ] ) < ft . Feature ( es [ 'installments' ] [ 'installments_paid_date' ] ) late_payment = late_payment . rename ( "late_payment" ) seed_features , seed_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'percent_true' , 'mean' ] , trans_primitives = [ ] , seed_features = [ late_payment ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
854	random . seed ( 50 ) random_params = { k : random . sample ( v , 1 ) [ 0 ] for k , v in param_grid . items ( ) } random_params [ 'subsample' ] = 1.0 if random_params [ 'boosting_type' ] == 'goss' else random_params [ 'subsample' ] random_params
135	base_df = pd . read_csv ( "/kaggle/input/covid19-global-forecasting-week-2/train.csv" ) base_df = base_df . rename ( { "Province_State" : "state" , "Country_Region" : "country" } , axis = 1 ) base_df . loc [ base_df [ "state" ] . isna ( ) , "state" ] = "Unknown" scoring_dates = test [ "Date" ] . unique ( )
1200	look_back = 1 trainX , trainY = create_dataset ( V_train , look_back ) testX , testY = create_dataset ( V_test , look_back )
833	import gc def agg_child ( df , parent_var , df_name ) : df_agg = agg_numeric ( df , parent_var , df_name ) df_agg_cat = agg_categorical ( df , parent_var , df_name ) df_info = df_agg . merge ( df_agg_cat , on = parent_var , how = 'outer' ) _ , idx = np . unique ( df_info , axis = 1 , return_index = True ) df_info = df_info . iloc [ : , idx ] gc . enable ( ) del df_agg , df_agg_cat gc . collect ( ) return df_info
1133	df [ "android_browser" ] = df [ 'id_31' ] . str . contains ( 'android browser' ) * 1 df [ "android_browser" ] = df [ 'id_31' ] . str . contains ( 'android webview' ) * 1 df [ "android_browser" ] = df [ 'id_31' ] . str . contains ( 'Generic/Android' ) * 1 df [ "android_browser" ] = df [ 'id_31' ] . str . contains ( 'Generic/Android 7.0' ) * 1
980	first_dcm = dcmread ( dir + patient + '/1.dcm' ) img = first_dcm [ 'PixelData' ] . value img [ : 100 ]
264	ridge = RidgeCV ( cv = 5 ) ridge . fit ( train , target ) acc_model ( 10 , ridge , train , test )
1569	id_error_c = id_error . astype ( 'category' ) print ( id_error_c . value_counts ( ) ) print ( id_error_c . value_counts ( ) / id_error_c . value_counts ( ) . sum ( ) ) id_error_c . value_counts ( ) . plot ( kind = 'bar' )
436	clf = OneVsRestClassifier ( SGDClassifier ( loss = 'log' , alpha = 0.00001 , penalty = 'l2' ) ) clf . fit ( X_train_multilabel , y_train ) y_pred = clf . predict ( X_test_multilabel )
63	train_transaction [ ( train_transaction . isFraud == 1 ) & ( train_transaction . D1minusday == 78 ) ] [ [ 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'addr1' , 'addr2' , 'dist1' , 'dist2' , 'P_emaildomain' , 'R_emaildomain' , 'TransactionDTday' ] ]
424	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yt , yr_pred ) print ( confusion )
208	train = pd . DataFrame ( preprocessing . MinMaxScaler ( ) . fit_transform ( train ) , columns = train . columns , index = train . index )
429	from matplotlib import pyplot H1 = pyplot . hist ( x , bins = 200 , histtype = 'stepfilled' , alpha = 0.2 , normed = True ) H2 = pyplot . hist ( x , bins = bayesian_blocks ( x ) , color = 'black' , histtype = 'step' , normed = True )
1517	for i in range ( 1 , 5 ) : sns . set ( font_scale = 1 , style = "white" ) c = sns . color_palette ( 'spring_d' ) [ i ] sns_jointplot = sns . jointplot ( 'age' , 'meaneduc' , data = train [ train [ 'Target' ] == i ] , kind = 'kde' , color = c , size = 6 , stat_func = None )
1407	np . random . seed ( 1989 ) train = pd . read_csv ( "../input/train.csv" ) test = pd . read_csv ( "../input/test.csv" ) print ( "Train shape : " , train . shape ) print ( "Test shape : " , test . shape )
1083	test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv' ) test [ 'content' ] = test [ 'content' ] . apply ( lambda x : text_process ( x ) ) x_test = regular_encode ( test . content . values , tokenizer , maxlen = MAX_LEN ) lang_tag_test = np . array ( [ lang_embed ( row [ 'lang' ] , 'orig' ) for _ , row in test . iterrows ( ) ] )
450	plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) sns . kdeplot ( train [ 'air_temperature' ] . dropna ( ) , shade = True , color = 'gold' ) plt . xlabel ( 'Air Temperature' , fontsize = 15 ) plt . ylabel ( 'Density' , fontsize = 15 ) plt . show ( )
39	train [ 'sex' ] = ( train [ 'sex' ] . values == 'male' ) * 1 test [ 'sex' ] = ( test [ 'sex' ] . values == 'male' ) * 1 train . head ( )
1480	learn . fit ( 10 , 5e-2 ) pred = learn . get_preds ( ds_type = DatasetType . Train ) y_pred = [ int ( np . argmax ( row ) ) for row in pred [ 0 ] ] print ( 'QWK insample' , cohen_kappa_score ( y_pred , pred [ 1 ] , weights = 'quadratic' ) )
848	import matplotlib . pyplot as plt import seaborn as sns plt . hist ( param_grid [ 'learning_rate' ] , bins = 20 , color = 'r' , edgecolor = 'k' ) ; plt . xlabel ( 'Learning Rate' , size = 14 ) ; plt . ylabel ( 'Count' , size = 14 ) ; plt . title ( 'Learning Rate Distribution' , size = 18 ) ;
1056	X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 45 ) neigh = KNeighborsClassifier ( n_neighbors = 9 ) neigh . fit ( X_train , y_train )
247	ensemble_final = ensembles [ 0 ] . copy ( ) ensemble_final [ target_cols ] = 0 for ensemble in ensembles : ensemble_final [ target_cols ] += ensemble [ target_cols ] . values / len ( ensembles ) ensemble_final
1227	drop_cols = [ 'id' , 'target' ] y_train = train [ 'target' ] x_train = train . drop ( drop_cols , axis = 1 ) x_test = test . drop ( [ 'id' ] , axis = 1 )
843	features = list ( train . columns ) fi = pd . DataFrame ( { 'feature' : features , 'importance' : model . feature_importances_ } )
784	test = extract_dateinfo ( test , 'pickup_datetime' , drop = False , time = True , start_ref = data [ 'pickup_datetime' ] . min ( ) ) test . head ( )
1279	print ( "Check the number of records" ) print ( "Number of records: " , train . shape [ 0 ] , "\n" ) print ( "Null analysis" ) empty_sample = train [ train . isnull ( ) . any ( axis = 1 ) ] print ( "Number of records contain 1+ null: " , empty_sample . shape [ 0 ] , "\n" )
458	df_train [ 'Intersection' ] = df_train [ 'IntersectionId' ] . astype ( str ) + df_train [ 'City' ] df_test [ 'Intersection' ] = df_test [ 'IntersectionId' ] . astype ( str ) + df_test [ 'City' ] print ( df_train [ 'Intersection' ] . sample ( 6 ) . values )
969	train = pd . read_csv ( '../input/train/train.csv' ) test = pd . read_csv ( '../input/test/test.csv' ) sample_submission = pd . read_csv ( '../input/test/sample_submission.csv' )
993	import os filepath = '' def MakeFile ( file_name ) : temp_path = filepath + file_name with open ( file_name , 'w' ) as f : f . write ( ) MakeFile ( 'slicer_code.py' )
1061	filtered_mask = sub_df [ 'ImageId' ] . isin ( filtered_test_imgs [ "ImageId" ] . values ) filtered_sub_df = sub_df [ filtered_mask ] . copy ( ) null_sub_df = sub_df [ ~ filtered_mask ] . copy ( ) null_sub_df [ 'EncodedPixels' ] = null_sub_df [ 'EncodedPixels' ] . apply ( lambda x : ' ' ) filtered_sub_df . reset_index ( drop = True , inplace = True ) filtered_test_imgs . reset_index ( drop = True , inplace = True ) print ( filtered_sub_df . shape ) print ( null_sub_df . shape ) filtered_sub_df . head ( )
1029	n_steps = x_valid . shape [ 0 ] // BATCH_SIZE train_history_2 = model . fit ( valid_dataset . repeat ( ) , steps_per_epoch = n_steps , epochs = EPOCHS * 2 )
923	import matplotlib . pyplot as plt ed = application . groupby ( 'CNT_CHILDREN' ) . CNT_CHILDREN . count ( ) u_ch = application . CNT_CHILDREN . unique ( ) plt . figure ( figsize = ( 10 , 6 ) ) plt . bar ( u_ch , ed , bottom = None , color = 'green' , label = 'Children count' ) plt . legend ( ) plt . xlabel ( 'Children count' ) plt . plot ( )
1240	train [ 'Year' ] = train [ 'Date' ] . dt . year train [ 'Month' ] = train [ 'Date' ] . dt . month train [ 'Week' ] = train [ 'Date' ] . dt . week train [ 'Day' ] = train [ 'Date' ] . dt . day train [ 'n_days' ] = ( train [ 'Date' ] . dt . date - train [ 'Date' ] . dt . date . min ( ) ) . apply ( lambda x : x . days )
1288	corr_df = pd . DataFrame ( columns = [ 'feature' , 'pearson' , 'kendall' , 'spearman' ] ) corr = macro_df [ macro_columns ] . corr ( method = 'spearman' ) fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( corr , annot = True , linewidths = .5 , ax = ax )
1534	def sieve_eratosthenes ( n ) : primes = [ False , False ] + [ True for i in range ( n - 1 ) ] p = 2 while ( p * p <= n ) : if ( primes [ p ] == True ) : for i in range ( p * 2 , n + 1 , p ) : primes [ i ] = False p += 1 return primes
961	train_months = X_train . DateAvSigVersion . apply ( lambda x : '{}-{}' . format ( x . year , x . month ) ) test_months = X_test . DateAvSigVersion . apply ( lambda x : '{}-{}' . format ( x . year , x . month ) ) df_months = pd . DataFrame ( train_months . value_counts ( ) ) . reset_index ( ) df_months = df_months . merge ( pd . DataFrame ( test_months . value_counts ( ) ) . reset_index ( ) , how = 'left' , on = 'index' ) df_months . sort_values ( 'index' )
1167	def get_model ( ) : with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB3 ( input_shape = ( * IMAGE_SIZE , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ tf . keras . metrics . AUC ( ) ] , ) return model
610	filters = 250 kernel_size = 3 hidden_dims = 250
498	def group_by ( df , t1 = '' , t2 = '' ) : a1 = df . groupby ( [ t1 , t2 ] ) [ t2 ] . count ( ) return a1
197	def render_neato ( s , format = 'png' , dpi = 100 ) : p = subprocess . Popen ( [ 'neato' , '-T' , format , '-o' , '/dev/stdout' , '-Gdpi={}' . format ( dpi ) ] , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) image , _ = p . communicate ( bytes ( s , encoding = 'utf-8' ) ) return image
1572	train_day = train_flattened . groupby ( [ "month" , "day" ] ) [ 'Visits' ] . mean ( ) . reset_index ( ) train_day = train_day . pivot ( 'day' , 'month' , 'Visits' ) train_day . sort_index ( inplace = True ) train_day . dropna ( inplace = True )
1463	cities = pd . read_csv ( '../input/cities.csv' ) xy_int = ( cities [ [ 'X' , 'Y' ] ] * 1000 ) . astype ( np . int64 ) with open ( 'xy_int.csv' , 'w' ) as fp : print ( len ( xy_int ) , file = fp ) print ( xy_int . to_csv ( index = False , header = False , sep = ' ' ) , file = fp )
1295	acc = history . history [ 'acc' ] val_acc = history . history [ 'val_acc' ] plt . plot ( acc ) plt . plot ( val_acc ) plt . ylabel ( 'Accuracy' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Val' ] , loc = 'upper left' ) plt . show ( )
1366	col = numeric_features [ 13 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
194	plt . figure ( figsize = ( 20 , 20 ) ) sns . regplot ( x = 'coms_length' , y = 'price' , data = train , scatter_kws = { 's' : 2 } ) plt . title ( 'Description length VS price' , fontsize = 20 ) plt . xlabel ( 'Description length' , fontsize = 20 ) plt . ylabel ( 'Price' , fontsize = 20 )
1206	rooms = train [ [ "num_room" , "price_doc" ] ] . groupby ( "num_room" ) . aggregate ( np . mean ) . reset_index ( ) mplt . scatter ( x = rooms . num_room , y = rooms . price_doc ) mplt . xlabel ( "Num rooms" ) mplt . ylabel ( 'Mean Price' )
842	app . reset_index ( inplace = True ) train , test = app [ app [ 'TARGET' ] . notnull ( ) ] . copy ( ) , app [ app [ 'TARGET' ] . isnull ( ) ] . copy ( ) gc . enable ( ) del app gc . collect ( )
563	img2 = cv2 . bitwise_and ( image [ 400 : 600 , 200 : 400 ] , image [ 400 : 600 , 200 : 400 ] , mask = total_mask [ 400 : 600 , 200 : 400 ] . astype ( np . uint8 ) ) plt . figure ( figsize = ( 8 , 8 ) ) total_mask [ total_mask > 1 ] = 0 plt . title ( 'Masks over image' ) plt . imshow ( img2 ) plt . show ( )
1355	col = numeric_features [ 2 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1058	plt . plot ( range ( 3 , 40 , 2 ) , loglossbig ) plt . ylabel ( 'logloss' ) plt . xlabel ( 'k value' ) plt . title ( 'KNN logloss on longitude and latitude' )
1230	xgb_lv2_outcomes = cross_validate_xgb ( xgb_params , lv1_train_df , y_train , lv1_test_df , kf , verbose = True , verbose_eval = False , use_rank = False ) xgb_lv2_cv = xgb_lv2_outcomes [ 0 ] xgb_lv2_train_pred = xgb_lv2_outcomes [ 1 ] xgb_lv2_test_pred = xgb_lv2_outcomes [ 2 ]
437	import numpy as np import pandas as pd from scipy import stats import os , gc import matplotlib . pyplot as plt import seaborn as sns sns . set_style ( "whitegrid" ) import plotly . offline as py from plotly . offline import iplot , init_notebook_mode import plotly . graph_objs as go init_notebook_mode ( connected = True ) from IPython . display import Markdown def bold ( string ) : display ( Markdown ( string ) )
1040	train = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/train.json' , lines = True ) test = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/test.json' , lines = True ) sample_df = pd . read_csv ( '/kaggle/input/stanford-covid-vaccine/sample_submission.csv' )
37	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train [ 'age_approx' ] . values , bins = 200 ) plt . title ( 'Histogram age_approx counts in train' ) plt . xlabel ( 'Value' ) plt . ylabel ( 'Count' ) plt . show ( )
956	random_index = np . random . randint ( 0 , val_masks_stacked . shape [ 0 ] ) print ( 'Validation Index: {}' . format ( random_index ) ) fig , ax = plt . subplots ( 2 , 1 ) ax [ 0 ] . imshow ( val_masks_stacked [ random_index ] , cmap = 'seismic' ) ax [ 1 ] . imshow ( val_predictions_stacked [ random_index ] > 0.5 , cmap = 'seismic' )
1278	import numpy as np import pandas as pd from fbprophet import Prophet import matplotlib . pyplot as plt import math as math
1310	import lightgbm as lgb import numpy as np import pandas as pd import itertools import time import pprint from sklearn . metrics import mean_squared_error from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import StratifiedKFold , KFold , GroupKFold , train_test_split from sklearn . cluster import KMeans SEEDS = 42
1268	training_dataset = get_training_dataset ( dataset , do_aug = False , advanced_aug = True , repeat = 1 , with_labels = True ) start = datetime . datetime . now ( ) for i in tqdm . tqdm ( range ( n_iter ) ) : for image , labe in training_dataset : pass end = datetime . datetime . now ( ) elapsed = ( end - start ) . total_seconds ( ) average = elapsed / n_iter print ( "Average timing for 1 iteration = {}" . format ( average ) )
289	n = 18 commits_df . loc [ n , 'commit_num' ] = 27 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.21 commits_df . loc [ n , 'LB_score' ] = - 6.8091
516	train_flat [ 'totals.transactionRevenue' ] . fillna ( 0 , inplace = True ) def fillNan ( cols ) : for col in cols : train_flat [ col ] . fillna ( 0 , inplace = True ) test_flat [ col ] . fillna ( 0 , inplace = True ) cols = [ 'trafficSource.adwordsClickInfo.page' , 'trafficSource.isTrueDirect' , 'totals.newVisits' , 'totals.bounces' , 'trafficSource.adwordsClickInfo.isVideoAd' ] fillNan ( cols ) ;
290	n = 19 commits_df . loc [ n , 'commit_num' ] = 28 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8090
965	shap_sum = np . abs ( shap_values ) . mean ( axis = 0 ) importance_df = pd . DataFrame ( [ X_importance . columns . tolist ( ) , shap_sum . tolist ( ) ] ) . T importance_df . columns = [ 'column_name' , 'shap_importance' ] importance_df = importance_df . sort_values ( 'shap_importance' , ascending = False ) importance_df
724	range_ = lambda x : x . max ( ) - x . min ( ) range_ . __name__ = 'range_' ind_agg = ind . drop ( columns = 'Target' ) . groupby ( 'idhogar' ) . agg ( [ 'min' , 'max' , 'sum' , 'count' , 'std' , range_ ] ) ind_agg . head ( )
1233	rf_lv2 = RandomForestClassifier ( n_estimators = 200 , n_jobs = 6 , min_samples_split = 5 , max_depth = 7 , criterion = 'gini' , random_state = 0 ) rf_lv2_outcomes = cross_validate_sklearn ( rf_lv2 , lv1_train_df , y_train , lv1_test_df , kf , scale = True , verbose = True ) rf_lv2_cv = rf_lv2_outcomes [ 0 ] rf_lv2_train_pred = rf_lv2_outcomes [ 1 ] rf_lv2_test_pred = rf_lv2_outcomes [ 2 ]
598	from sklearn . metrics import roc_auc_score def gini ( y_target , y_score ) : return 2 * roc_auc_score ( y_target , y_score ) - 1 print ( f"Gini on perfect submission: {gini(target, perfect_sub):0.5f}" )
594	negative_train [ 'temp_list' ] = negative_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) negative_train [ 'temp_list' ] = negative_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) negative_top = Counter ( [ item for sublist in negative_train [ 'temp_list' ] for item in sublist ] ) negative_temp = pd . DataFrame ( negative_top . most_common ( 20 ) ) negative_temp . columns = [ 'Common_words' , 'count' ] negative_temp . style . background_gradient ( cmap = 'Blues' )
1348	print ( 'Applicatoin train shape before merge: ' , application_train . shape ) application_train = application_train . merge ( bureau . groupby ( 'SK_ID_CURR' ) . mean ( ) . reset_index ( ) , left_on = 'SK_ID_CURR' , right_on = 'SK_ID_CURR' , how = 'left' , validate = 'one_to_one' ) print ( 'Applicatoin train shape after merge: ' , application_train . shape )
178	from skimage . filters import threshold_otsu thresh_val = threshold_otsu ( im_gray ) mask = np . where ( im_gray > thresh_val , 1 , 0 ) if np . sum ( mask == 0 ) < np . sum ( mask == 1 ) : mask = np . where ( mask , 0 , 1 )
1520	from sklearn . metrics import classification_report Xgb_test = xgb . DMatrix ( X_test ) y_pred = model . predict ( Xgb_test , ntree_limit = model . best_ntree_limit ) print ( classification_report ( y_test , y_pred ) )
1074	pretrain_weights_path = [ '../input/kmnist-data/0_model.hdf5' , '../input/kmnist-data/1_model.hdf5' , '../input/kmnist-data/2_model.hdf5' , '../input/kmnist-data/3_model.hdf5' , '../input/kmnist-data/4_model.hdf5' , '../input/kmnist-data/5_model.hdf5' , '../input/kmnist-data/6_model.hdf5' , '../input/kmnist-data/7_model.hdf5' , ] uptrain = False submit = True num_classes = 10 num_features = ( 28 , 28 , 1 ) batch_size = 1024 lr = 3e-4 epochs = 30 k_fold_split = 8
184	plt . figure ( figsize = ( 17 , 10 ) ) sns . countplot ( y = train . category_name , \ order = train . category_name . value_counts ( ) . iloc [ : 10 ] . index , \ orient = 'v' ) plt . title ( 'Top 10 categories' , fontsize = 25 ) plt . ylabel ( 'Category name' , fontsize = 20 ) plt . xlabel ( 'Number of product in the category' , fontsize = 20 )
261	decision_tree = DecisionTreeRegressor ( ) decision_tree . fit ( train , target ) acc_model ( 5 , decision_tree , train , test )
255	country_name = "Andorra" shift = 0 day_start = 39 + shift dates_list2 = dates_list [ shift : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , shift )
778	train_mean = y_train . mean ( ) train_preds = [ train_mean for _ in range ( len ( y_train ) ) ] valid_preds = [ train_mean for _ in range ( len ( y_valid ) ) ] tr , vr , tm , vm = metrics ( train_preds , valid_preds , y_train , y_valid ) print ( f'Baseline Training: rmse = {round(tr, 2)} \t mape = {round(tm, 2)}' ) print ( f'Baseline Validation: rmse = {round(vr, 2)} \t mape = {round(vm, 2)}' )
226	n = 7 commits_df . loc [ n , 'commit_num' ] = 9 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 288 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25881
732	model . fit ( train_set , train_labels ) feature_importances = pd . DataFrame ( { 'feature' : features , 'importance' : model . feature_importances_ } ) feature_importances . head ( )
1367	col = numeric_features [ 14 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1521	def sigmoid_np ( x ) : return 1.0 / ( 1.0 + np . exp ( - x ) ) preds , y = learner . TTA ( ) preds = np . stack ( preds , axis = - 1 ) preds = sigmoid_np ( preds ) pred = preds . max ( axis = - 1 )
954	train_path = data_src + 'train' test_path = data_src train_ids = train_df . index . values test_ids = test_df . index . values
321	df_0 = df_train [ df_train [ 'binary_target' ] == 0 ] df_1 = df_train [ df_train [ 'binary_target' ] == 1 ] . sample ( len ( df_0 ) , random_state = 101 ) df_data = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_data = shuffle ( df_data ) print ( df_data . shape ) df_data . head ( )
825	train = train . drop ( columns = to_drop ) test = test . drop ( columns = to_drop ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
988	from pyvirtualdisplay import Display disp = Display ( ) . start ( ) import vtk disp . stop ( )
425	def im_convert ( tensor ) : image = tensor . cpu ( ) . clone ( ) . detach ( ) . numpy ( ) image = image . transpose ( 1 , 2 , 0 ) image = image * np . array ( ( 0.5 , 0.5 , 0.5 ) ) + np . array ( ( 0.5 , 0.5 , 0.5 ) ) image = image . clip ( 0 , 1 ) return image
1398	col = numeric_features [ 47 ] plot_category_percent_of_target_for_numeric ( col )
1277	clf = RandomForestClassifier ( n_estimators = 50 , criterion = 'gini' , max_depth = 5 , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = - 1 , random_state = 0 , verbose = 0 , warm_start = False , class_weight = 'balanced' )
497	total = bureau_balance . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( bureau_balance . isnull ( ) . sum ( ) / bureau_balance . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) ms = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) ms = ms [ ms [ "Percent" ] > 0 ] ms
212	building_df = pd . read_csv ( "../input/ashrae-energy-prediction/building_metadata.csv" ) weather_train = pd . read_csv ( "../input/ashrae-energy-prediction/weather_train.csv" ) train = pd . read_csv ( "../input/ashrae-energy-prediction/train.csv" ) train = train . merge ( building_df , left_on = "building_id" , right_on = "building_id" , how = "left" ) train = train . merge ( weather_train , left_on = [ "site_id" , "timestamp" ] , right_on = [ "site_id" , "timestamp" ] ) del weather_train train . head ( )
224	n = 5 commits_df . loc [ n , 'commit_num' ] = 7 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 192 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25877
159	import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt import os from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
634	confirmed = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv' ) . sort_values ( by = 'Country/Region' ) deaths = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv' ) recovered = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv' )
472	bayesian_tr_idx , bayesian_val_idx = train_test_split ( train_df , test_size = 0.3 , random_state = 42 , stratify = train_df [ target ] ) bayesian_tr_idx = bayesian_tr_idx . index bayesian_val_idx = bayesian_val_idx . index
3	import os print ( os . listdir ( "../input" ) )
939	submission_preds = oof_test . mean ( axis = 1 ) submission_df = X_test [ [ 'SK_ID_CURR' ] ] . copy ( ) submission_df [ 'TARGET' ] = submission_preds submission_df [ [ 'SK_ID_CURR' , 'TARGET' ] ] . to_csv ( 'submission.csv' , index = False )
285	n = 14 commits_df . loc [ n , 'commit_num' ] = 20 commits_df . loc [ n , 'Dropout_model' ] = 0.37 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8092
1226	def probability_to_rank ( prediction , scaler = 1 ) : pred_df = pd . DataFrame ( columns = [ 'probability' ] ) pred_df [ 'probability' ] = prediction pred_df [ 'rank' ] = pred_df [ 'probability' ] . rank ( ) / len ( prediction ) * scaler return pred_df [ 'rank' ] . values
1045	model = build_model ( input_shape = ( 300 , 300 , 3 ) , n_classes = train_target_df . shape [ 1 ] ) model . summary ( )
52	sns . set_style ( "whitegrid" ) ax = sns . violinplot ( x = np . log ( train_df [ columns_to_use ] . values . flatten ( ) + 1 ) ) plt . show ( )
777	lr . fit ( X_train [ [ 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train ) print ( 'Intercept' , round ( lr . intercept_ , 4 ) ) print ( 'abs_lat_diff coef: ' , round ( lr . coef_ [ 0 ] , 4 ) , '\tabs_lon_diff coef:' , round ( lr . coef_ [ 1 ] , 4 ) , '\tpassenger_count coef:' , round ( lr . coef_ [ 2 ] , 4 ) )
304	def MacroF1Metric ( preds , dtrain ) : labels = dtrain . get_label ( ) preds = np . round ( np . clip ( preds , 0 , 10 ) ) . astype ( int ) score = f1_score ( labels , preds , average = 'macro' ) return ( 'MacroF1Metric' , score , True )
94	t_id = 0 text = df . loc [ t_id , 'Text' ] word_scores = keywords ( text , words = 5 , scores = True , split = True , lemmatize = True ) word_scores = ', ' . join ( [ '{}-{:.2f}' . format ( k , s [ 0 ] ) for k , s in word_scores ] ) summary = summarize ( text , word_count = 100 ) print ( 'ID [{}]\nKeywords: [{}]\nSummary: [{}]' . format ( t_id , word_scores , summary ) )
1009	batch_size = 64 num_classes = 14 epochs = 30 val_split = 0.1 save_dir = os . path . join ( os . getcwd ( ) , 'models' ) model_name = 'keras_cnn_model.h5'
444	bold ( '**PLACES OF INDUSTRY HIGHEST READINGS ON WEEKDAYS**' ) ax = sns . FacetGrid ( train , col = "primary_use" , col_wrap = 4 , height = 4 , aspect = 1 , sharex = False ) ax . map ( sns . boxplot , 'meter_reading' , 'weekday_name' , color = "teal" , boxprops = dict ( alpha = .3 ) ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
942	bureau_bal_dfs = feature_aggregator_on_df ( bureau_bal , aggs_cat , aggs_num , [ 'SK_ID_BUREAU' ] , 'bureau_balance' , 'basic' , save = False ) bureau_bal_dfs [ 0 ]
148	example_df = train_df . sample ( n = 1 ) . reset_index ( drop = True ) example_generator = train_datagen . flow_from_dataframe ( example_df , "../input/train/train/" , x_col = 'filename' , y_col = 'category' , target_size = IMAGE_SIZE , class_mode = 'categorical' )
1362	col = numeric_features [ 9 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1317	family_size_features = [ 'adult' , 'hogar_adul' , 'hogar_mayor' , 'hogar_nin' , 'hogar_total' , 'r4h1' , 'r4h2' , 'r4h3' , 'r4m1' , 'r4m2' , 'r4m3' , 'r4t1' , 'r4t2' , 'r4t3' , 'hhsize' ] new_feats = [ ] for col in family_size_features : new_col_name = 'new_{}_per_{}' . format ( 'v2a1' , col ) new_feats . append ( new_col_name ) df_train [ new_col_name ] = df_train [ 'v2a1' ] / df_train [ col ] df_test [ new_col_name ] = df_test [ 'v2a1' ] / df_test [ col ]
373	random_forest = GridSearchCV ( estimator = RandomForestRegressor ( ) , param_grid = { 'n_estimators' : [ 100 , 1000 ] } , cv = 10 ) random_forest . fit ( train , target ) print ( random_forest . best_params_ ) acc_model ( 6 , random_forest , train , test )
513	def roi ( img , vertices ) : mask = np . zeros_like ( img ) cv2 . fillPoly ( mask , [ vertices ] , 255 ) masked = cv2 . bitwise_and ( img , mask ) return masked
589	has_to_plot_infection_peak = True if has_to_run_sir : crisis_day_sir = np . argmax ( I_predict_sir ) if has_to_run_sird : crisis_day_sird = np . argmax ( I_predict_sird ) if has_to_run_seir : crisis_day_seir = np . argmax ( I_predict_seir ) if has_to_run_seird : crisis_day_seird = np . argmax ( I_predict_seird ) if has_to_run_seirdq : crisis_day_seirdq = np . argmax ( I_predict_seirdq )
1248	data = pd . concat ( [ train [ 'Dept' ] , train [ 'Weekly_Sales' ] , train [ 'IsHoliday' ] ] , axis = 1 ) f , ax = plt . subplots ( figsize = ( 25 , 10 ) ) fig = sns . boxplot ( x = 'Dept' , y = 'Weekly_Sales' , data = data , showfliers = False , hue = "IsHoliday" )
718	corrs = pcorrs . merge ( scorrs , on = 'feature' ) corrs [ 'diff' ] = corrs [ 'pcorr' ] - corrs [ 'scorr' ] corrs . sort_values ( 'diff' ) . head ( )
329	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
199	def render_neato ( s , format = 'png' , dpi = 100 ) : p = subprocess . Popen ( [ 'neato' , '-T' , format , '-o' , '/dev/stdout' , '-Gdpi={}' . format ( dpi ) ] , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) image , _ = p . communicate ( bytes ( s , encoding = 'utf-8' ) ) return image
1375	col = numeric_features [ 22 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
454	from sklearn . preprocessing import LabelEncoder le = LabelEncoder ( ) train [ 'primary_use' ] = le . fit_transform ( train [ 'primary_use' ] ) test [ 'primary_use' ] = le . fit_transform ( test [ 'primary_use' ] )
627	date_agg_2 = train_agg . groupby ( level = 1 ) . sum ( ) date_agg_2 . columns = ( 'bookings' , 'total' ) date_agg_2 . index . name = 'Year' date_agg_2 . plot ( kind = 'bar' , stacked = 'True' )
1430	import pandas as pd import math import numpy as np import matplotlib . pyplot as plt import matplotlib . mlab as mlab import seaborn as sn from pandas . plotting import scatter_matrix import plotly . offline as py import plotly . graph_objs as go import plotly . tools as tls import plotly . figure_factory as ff
1419	full_table [ 'Active' ] = full_table [ 'Confirmed' ] - full_table [ 'Deaths' ] - full_table [ 'Recovered' ] full_table [ 'Country/Region' ] = full_table [ 'Country/Region' ] . replace ( 'Mainland China' , 'China' ) full_table [ [ 'Province/State' ] ] = full_table [ [ 'Province/State' ] ] . fillna ( '' ) full_table
790	lr = LinearRegression ( ) lr . fit ( X_train [ features ] , y_train ) evaluate ( lr , features , X_train , X_valid , y_train , y_valid )
570	import numpy as np import pandas as pd import pymc3 as pm from scipy . integrate import solve_ivp from scipy import optimize from numba import jit import theano import theano . tensor as t import matplotlib . pyplot as plt import altair as alt seed = 12345 np . random . seed ( seed ) plt . style . use ( 'seaborn-talk' ) THEANO_FLAGS = 'optimizer=fast_compile'
681	import numpy as np import pandas as pd import matplotlib . pyplot as plt import matplotlib import seaborn as sns import os train = pd . read_csv ( '../input/train.csv' , index_col = 'ID' ) test = pd . read_csv ( '../input/test.csv' , index_col = 'ID' ) train . head ( )
1112	sample_submission1 = pd . read_csv ( '../input/ashrae-kfold-lightgbm-without-leak-1-08/submission.csv' , index_col = 0 ) sample_submission2 = pd . read_csv ( '../input/ashrae-half-and-half/submission.csv' , index_col = 0 ) sample_submission3 = pd . read_csv ( '../input/ashrae-highway-kernel-route4/submission.csv' , index_col = 0 )
260	sgd = SGDRegressor ( ) sgd . fit ( train , target ) acc_model ( 4 , sgd , train , test )
725	new_col = [ ] for c in ind_agg . columns . levels [ 0 ] : for stat in ind_agg . columns . levels [ 1 ] : new_col . append ( f'{c}-{stat}' ) ind_agg . columns = new_col ind_agg . head ( )
1193	def preprocess_image ( image_path , desired_size = 224 ) : biopsy = openslide . OpenSlide ( image_path ) im = np . array ( biopsy . get_thumbnail ( size = ( desired_size , desired_size ) ) ) im = Image . fromarray ( im ) im = im . resize ( ( desired_size , desired_size ) ) im = np . array ( im ) / 255 return im
292	n = 21 commits_df . loc [ n , 'commit_num' ] = 31 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'GaussianNoise_stddev' ] = 0.21 commits_df . loc [ n , 'LB_score' ] = - 6.8093
381	models = pd . DataFrame ( { 'Model' : [ 'Linear Regression' , 'Support Vector Machines' , 'Linear SVR' , 'MLPRegressor' , 'Stochastic Gradient Decent' , 'Decision Tree Regressor' , 'Random Forest' , 'XGB' , 'LGBM' , 'GradientBoostingRegressor' , 'RidgeRegressor' , 'BaggingRegressor' , 'ExtraTreesRegressor' , 'AdaBoostRegressor' , 'VotingRegressor' ] , 'r2_train' : acc_train_r2 , 'r2_test' : acc_test_r2 , 'd_train' : acc_train_d , 'd_test' : acc_test_d , 'rmse_train' : acc_train_rmse , 'rmse_test' : acc_test_rmse } )
388	for item in TEST_DB : break print ( type ( item ) , list ( item . keys ( ) ) ) print ( item [ '_id' ] , len ( item [ 'imgs' ] ) )
1577	feature_cols = [ col for col in train . columns if col not in [ 'is_churn' , 'msno' ] ] train [ feature_cols ] = train [ feature_cols ] . applymap ( lambda x : np . nan if np . isinf ( x ) else x ) test [ feature_cols ] = test [ feature_cols ] . applymap ( lambda x : np . nan if np . isinf ( x ) else x )
80	def get_sex ( x ) : x = str ( x ) if x . find ( 'Male' ) >= 0 : return 'male' if x . find ( 'Female' ) >= 0 : return 'female' return 'unknown' def get_neutered ( x ) : x = str ( x ) if x . find ( 'Spayed' ) >= 0 : return 'neutered' if x . find ( 'Neutered' ) >= 0 : return 'neutered' if x . find ( 'Intact' ) >= 0 : return 'intact' return 'unknown'
319	def create_fname ( x ) : fname = str ( x ) + '.png' return fname df_train [ 'file_name' ] = df_train [ 'id_code' ] . apply ( create_fname )
968	plot_curve_fit ( gaussian , roc_by_date , 'Active' , 'China w/o Hubei' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , china_by_date , 'Active' , 'China' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , skorea_by_date , 'Active' , 'South Korea' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , italy_by_date , 'Active' , 'Italy' + ' - Curve for Cases ' , False , 'Gauss' )
569	from segmentation_models . backbones import get_preprocessing preprocess_input = get_preprocessing ( 'resnet34' ) training_generator = DataGenerator ( X [ : 400 ] , M [ : 400 ] , batch_size = 16 , dim = ( 384 , 384 ) , aug = aug , preprocess_input = preprocess_input ) valid_genarator = DataGenerator ( X [ 400 : ] , M [ 400 : ] , batch_size = 16 , dim = ( 384 , 384 ) , aug = aug_null , preprocess_input = preprocess_input , shuffle = False )
152	model = CatBoostClassifier ( iterations = 100 , learning_rate = 0.003 , depth = 10 , l2_leaf_reg = 0.01 , loss_function = 'MultiClass' )
476	train_df = train_transaction . merge ( train_identity , how = 'left' , left_index = True , right_index = True ) test_df = test_transaction . merge ( test_identity , how = 'left' , left_index = True , right_index = True ) print ( "Train shape : " + str ( train_df . shape ) ) print ( "Test shape : " + str ( test_df . shape ) )
1442	skiplines = np . random . choice ( np . arange ( 1 , lines ) , size = lines - 1 - 1000000 , replace = False ) skiplines = np . sort ( skiplines )
1037	fig = px . line ( history . history , y = [ 'loss' , 'val_loss' ] , labels = { 'index' : 'epoch' , 'value' : 'MCRMSE' } , title = 'Training History' ) fig . show ( )
1008	x_train = np . load ( '../input/reducing-image-sizes-to-32x32/X_train.npy' ) x_test = np . load ( '../input/reducing-image-sizes-to-32x32/X_test.npy' ) y_train = np . load ( '../input/reducing-image-sizes-to-32x32/y_train.npy' ) print ( 'x_train shape:' , x_train . shape ) print ( x_train . shape [ 0 ] , 'train samples' ) print ( x_test . shape [ 0 ] , 'test samples' )
1126	naive_vals = train . groupby ( 'Category' ) . count ( ) . iloc [ : , 0 ] / train . shape [ 0 ] n_rows = test . shape [ 0 ] submission = pd . DataFrame ( np . repeat ( np . array ( naive_vals ) , n_rows ) . reshape ( 39 , n_rows ) . transpose ( ) , columns = naive_vals . index )
1312	train_data = pd . read_csv ( '../input/augmented-dataset/aug_train_df_2.csv' ) test_data = pd . read_csv ( '../input/augmented-dataset/aug_test_df_2.csv' ) submission = pd . read_csv ( '../input/stanford-covid-vaccine/sample_submission.csv' )
1334	train_fullVisitorId = train_df [ 'fullVisitorId' ] train_sessionId = train_df [ 'sessionId' ] train_visitId = train_df [ 'visitId' ] test_fullVisitorId = test_df [ 'fullVisitorId' ] test_sessionId = test_df [ 'sessionId' ] test_visitId = test_df [ 'visitId' ] train_df . drop ( [ 'fullVisitorId' , 'sessionId' , 'visitId' ] , axis = 1 , inplace = True ) test_df . drop ( [ 'fullVisitorId' , 'sessionId' , 'visitId' ] , axis = 1 , inplace = True )
281	n = 10 commits_df . loc [ n , 'commit_num' ] = 16 commits_df . loc [ n , 'Dropout_model' ] = 0.25 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8093
1541	train_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . notnull ( ) ] . copy ( ) test_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . isnull ( ) ] . copy ( ) test_df . drop ( [ 'TARGET' ] , axis = 1 , inplace = True ) del feature_matrix , feature_defs , feature_matrix_enc gc . collect ( )
1306	training_sample = training_set [ - 100000 : ] y_train_sample = training_sample [ 'isFraud' ] X_train_sample = training_sample . drop ( 'isFraud' , axis = 1 )
750	plot_confusion_matrix ( cm , normalize = True , classes = [ 'Extreme' , 'Moderate' , 'Vulnerable' , 'Non-Vulnerable' ] , title = 'Poverty Confusion Matrix' )
434	X_train , X_test , y_train , y_test = train_test_split ( preprocessed_df , yx_multilabel , test_size = 0.2 , random_state = 42 ) print ( "Number of data points in training data :" , X_train . shape [ 0 ] ) print ( "Number of data points in test data :" , X_test . shape [ 0 ] )
1532	correlation = df_train . corr ( ) correlation = correlation [ 'winPlacePerc' ] . sort_values ( ascending = False ) print ( correlation . head ( 20 ) )
384	def des_bw_filter_lp ( cutoff = CUTOFF ) : b , a = sg . butter ( 4 , Wn = cutoff / NY_FREQ_IDX ) return b , a def des_bw_filter_hp ( cutoff = CUTOFF ) : b , a = sg . butter ( 4 , Wn = cutoff / NY_FREQ_IDX , btype = 'highpass' ) return b , a def des_bw_filter_bp ( low , high ) : b , a = sg . butter ( 4 , Wn = ( low / NY_FREQ_IDX , high / NY_FREQ_IDX ) , btype = 'bandpass' ) return b , a
1345	temp_col = 'EXT_SOURCE_2' sns . kdeplot ( application_train_float . loc [ application_train_float [ 'TARGET' ] == 0 , temp_col ] , label = 'repay(0)' , color = 'r' ) sns . kdeplot ( application_train_float . loc [ application_train_float [ 'TARGET' ] == 1 , temp_col ] , label = 'not repay(1)' , color = 'b' ) plt . title ( 'KDE for {} splitted by target' . format ( temp_col ) ) plt . show ( )
370	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
235	n = 16 commits_df . loc [ n , 'commit_num' ] = 20 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 128 commits_df . loc [ n , 'hidden_dim_third' ] = 248 commits_df . loc [ n , 'LB_score' ] = 0.25891
138	month_temperature = all_df . groupby ( [ 'month' , 'ord_2' ] ) [ 'ord_2' ] . count ( ) . to_frame ( ) month_temperature = month_temperature . rename ( columns = { 'ord_2' : 'num_of_days' } ) month_temperature = month_temperature . reset_index ( )
914	import lightgbm as lgb from sklearn . model_selection import KFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import LabelEncoder import gc import matplotlib . pyplot as plt
76	def F1 ( y_pred , y ) : y_pred = y_pred . softmax ( dim = 1 ) y_pred = y_pred . argmax ( dim = 1 ) return torch . tensor ( f1_score ( y . cpu ( ) , y_pred . cpu ( ) , labels = list ( range ( 10 ) ) , average = 'weighted' ) , device = 'cuda:0' )
1351	def group_battery ( x ) : x = x . lower ( ) if 'li' in x : return 1 else : return 0 train_small [ 'Census_InternalBatteryType' ] = train_small [ 'Census_InternalBatteryType' ] . apply ( group_battery )
1432	h1_col = [ s for s in col if "h1_" in s ] d1_col = [ s for s in col if "d1_" in s ] h1d1 = list ( pd . Series ( h1_col ) . str . replace ( "h1_" , "" ) ) for c in h1d1 : All_df [ 'diff_' + c ] = All_df [ 'd1_' + c ] - All_df [ 'h1_' + c ]
190	plt . figure ( figsize = ( 10 , 10 ) ) sns . boxplot ( x = train . shipping , y = train . price , showfliers = False , orient = 'v' ) plt . title ( 'Does shipping depend of prices ?' , fontsize = 25 ) plt . xlabel ( 'Shipping fee paid by seller (1) or by buyer (0)' , fontsize = 20 ) plt . ylabel ( 'Price without outliers' , fontsize = 20 )
20	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train [ 'muggy-smalt-axolotl-pembus' ] . values , bins = 200 ) plt . title ( 'Histogram muggy-smalt-axolotl-pembus counts' ) plt . xlabel ( 'Value' ) plt . ylabel ( 'Count' ) plt . show ( )
1219	lr_scheduler = ExponentialLR ( optimizer , gamma = 0.95 ) @ trainer . on ( Events . EPOCH_COMPLETED ) def update_lr_scheduler ( engine ) : lr_scheduler . step ( ) lr = float ( optimizer . param_groups [ 0 ] [ 'lr' ] ) print ( "Learning rate: {}" . format ( lr ) )
729	from sklearn . ensemble import RandomForestClassifier from sklearn . metrics import f1_score , make_scorer from sklearn . model_selection import cross_val_score from sklearn . preprocessing import Imputer from sklearn . preprocessing import MinMaxScaler from sklearn . pipeline import Pipeline scorer = make_scorer ( f1_score , greater_is_better = True , average = 'macro' )
156	clear_output ( wait = True ) print ( 'Done!' )
496	def type_features ( data ) : categorical_features = data . select_dtypes ( include = [ "object" ] ) . columns numerical_features = data . select_dtypes ( exclude = [ "object" ] ) . columns print ( "categorical_features :" , categorical_features ) print ( '-----' * 40 ) print ( "numerical_features:" , numerical_features )
1078	tta_aug = albu . Compose ( [ albu . ShiftScaleRotate ( scale_limit = 0.1 , rotate_limit = 10 , shift_limit = 0.10 , p = 1.0 , border_mode = 0 , value = 0 ) ] )
900	train_labels = feature_matrix [ 'TARGET' ] feature_matrix , feature_matrix_test = feature_matrix2 . align ( feature_matrix_test2 , join = 'inner' , axis = 1 ) feature_matrix [ 'TARGET' ] = train_labels print ( 'Final training shape: ' , feature_matrix . shape ) print ( 'Final testing shape: ' , feature_matrix_test . shape )
400	DATA_DIR = r'../input' TEST_DIR = r'../input/test' print ( 'ok' )
1563	lda = LatentDirichletAllocation ( n_components = 11 , max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( tf ) lda . components_
1314	def replace_edjefe ( x ) : if x == 'yes' : return 1 elif x == 'no' : return 0 else : return x df_train [ 'edjefe' ] = df_train [ 'edjefe' ] . apply ( replace_edjefe ) . astype ( float ) df_test [ 'edjefe' ] = df_test [ 'edjefe' ] . apply ( replace_edjefe ) . astype ( float )
1201	model = Sequential ( ) model . add ( LSTM ( 4 , input_shape = ( trainX . shape [ 1 ] , trainX . shape [ 2 ] ) ) ) model . add ( Dense ( 1 ) ) model . compile ( loss = 'mse' , optimizer = 'adam' ) history = model . fit ( trainX , trainY , epochs = 3 , batch_size = 100 , validation_data = ( testX , testY ) , verbose = 1 , shuffle = False )
503	val_p = [ 'AMT_ANNUITY' , 'AMT_CREDIT' , 'AMT_GOODS_PRICE' , 'HOUR_APPR_PROCESS_START' ] for i in val_p : plt . figure ( figsize = ( 5 , 5 ) ) sns . distplot ( application_train [ i ] . dropna ( ) , kde = True , color = 'g' ) plt . title ( i ) plt . xticks ( rotation = - 90 ) plt . show ( )
356	model = RandomForestRegressor ( max_depth = 2 , random_state = 0 , n_estimators = 100 ) embeded_rf_selector = SelectFromModel ( model , threshold = '1.25*median' ) embeded_rf_selector . fit ( dfe , target_fe )
1293	import pandas as pd import numpy as np import warnings import time warnings . filterwarnings ( "ignore" ) import lightgbm as lgb from bayes_opt import BayesianOptimization from sklearn . metrics import roc_auc_score from sklearn . model_selection import train_test_split
571	df_covid = pd . read_csv ( "../input/corona-virus-report/covid_19_clean_complete.csv" , parse_dates = [ 'Date' ] ) df_covid . info ( )
1447	variables = [ 'ip' , 'app' , 'device' , 'os' , 'channel' ] for v in variables : train [ v ] = train [ v ] . astype ( 'category' ) test [ v ] = test [ v ] . astype ( 'category' )
110	def build_lrfn ( lr_start = 0.00001 , lr_max = 0.000075 , lr_min = 0.000001 , lr_rampup_epochs = 20 , lr_sustain_epochs = 0 , lr_exp_decay = .8 ) : lr_max = lr_max * strategy . num_replicas_in_sync def lrfn ( epoch ) : if epoch < lr_rampup_epochs : lr = ( lr_max - lr_start ) / lr_rampup_epochs * epoch + lr_start elif epoch < lr_rampup_epochs + lr_sustain_epochs : lr = lr_max else : lr = ( lr_max - lr_min ) * lr_exp_decay ** ( epoch - lr_rampup_epochs - lr_sustain_epochs ) + lr_min return lr return lrfn
934	y_pred_valid = model . predict ( X_val ) y_pred_test = model . predict ( X_test ) del X_tr , X_val , X_test gc . collect ( )
17	preds_age = np . load ( '../input/trends-h2o-automl-age/preds_age.npy' ) preds_domain1_var1 = np . load ( '../input/trends-h2o-automl-domain1-var1/preds_domain1_var1.npy' ) preds_domain1_var2 = np . load ( '../input/trends-h2o-automl-domain1-var2/preds_domain1_var2.npy' ) preds_domain2_var1 = np . load ( '../input/trends-h2o-automl-domain2-var1/preds_domain2_var1.npy' ) preds_domain2_var2 = np . load ( '../input/trends-h2o-automl-domain2-var2/preds_domain2_var2.npy' )
394	plt . figure ( figsize = ( 12 , 6 ) ) plt . title ( 'Category_count vs Image_count' ) bin_size = 25 plt . hist ( train_categories_count , bins = range ( 0 , int ( 1e4 ) , bin_size ) ) plt . xlabel ( 'Amount of available images' ) _ = plt . ylabel ( 'Number of classes' )
314	from sklearn . metrics import classification_report y_pred_binary = predictions . argmax ( axis = 1 ) report = classification_report ( y_true , y_pred_binary , target_names = cm_plot_labels ) print ( report )
78	learn . unfreeze ( ) learn . lr_find ( ) learn . recorder . plot ( suggestion = True )
873	train = feature_matrix2 [ feature_matrix2 [ 'set' ] == 'train' ] test = feature_matrix2 [ feature_matrix2 [ 'set' ] == 'test' ] train = pd . get_dummies ( train ) test = pd . get_dummies ( test ) train , test = train . align ( test , join = 'inner' , axis = 1 ) test = test . drop ( columns = [ 'TARGET' ] ) print ( 'Final Training Shape: ' , train . shape ) print ( 'Final Testing Shape: ' , test . shape )
1581	train = pd . read_csv ( '/kaggle/input/3d-object-detection-for-autonomous-vehicles/train.csv' ) sub = pd . read_csv ( '/kaggle/input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv' ) print ( train . shape ) train . head ( )
1495	def program_desc ( program ) : desc = [ x . __name__ for x in program ] return ( ' >> ' . join ( desc ) ) program = [ splitH , groupByColor , negative , intersect ] print ( program_desc ( program ) )
53	train_nz = np . log ( train_df [ columns_to_use ] . values . flatten ( ) + 1 ) train_nz = train_nz [ np . nonzero ( train_nz ) ] plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train_nz , bins = 50 ) plt . title ( 'Log Histogram nonzero train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Log value' ) plt . show ( )
924	ed = application . groupby ( [ 'TARGET' , 'CNT_CHILDREN' ] ) [ 'TARGET' ] . count ( ) . unstack ( 'TARGET' ) . fillna ( 0 ) ed . plot ( kind = 'bar' , stacked = True ) print ( ed )
1478	train_csv = pd . read_csv ( '../input/train/train.csv' , low_memory = False ) test_csv = pd . read_csv ( '../input/test/test.csv' , low_memory = False ) def preprocess ( csv ) : csv [ 'Description_len' ] = [ len ( str ( tt ) ) for tt in csv [ 'Description' ] ] csv [ 'Name_len' ] = [ len ( str ( tt ) ) for tt in csv [ 'Name' ] ] return csv train_csv = preprocess ( train_csv ) test_csv = preprocess ( test_csv )
506	target1samplecols = [ str ( i ) for i in list ( target1df . iloc [ : nSamples ] . signal_id ) ] target1sampledata = pq . read_pandas ( TRAIN_DATA_FILE_PATH , columns = target1samplecols ) . to_pandas ( ) target1sampledata . plot ( title = "Target 1" , figsize = ( 15 , 10 ) )
1028	n_steps = x_train . shape [ 0 ] // BATCH_SIZE train_history = model . fit ( train_dataset , steps_per_epoch = n_steps , validation_data = valid_dataset , epochs = EPOCHS )
1032	print ( image_string_placeholder ) print ( decoded_image ) print ( decoded_image_float ) print ( image_tensor )
1482	patientId = df [ 'patientId' ] [ 3 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 1 - Normal Image" ) draw ( parsed [ patientId ] )
1000	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
267	Ada_Boost = AdaBoostRegressor ( ) Ada_Boost . fit ( train , target ) acc_model ( 13 , Ada_Boost , train , test )
1327	data_dir = '/kaggle/input/stanford-covid-vaccine/' train = pd . read_json ( data_dir + 'train.json' , lines = True ) test = pd . read_json ( data_dir + 'test.json' , lines = True ) sample_df = pd . read_csv ( data_dir + 'sample_submission.csv' )
1247	data = pd . concat ( [ train [ 'Dept' ] , train [ 'Weekly_Sales' ] , train [ 'Type' ] ] , axis = 1 ) f , ax = plt . subplots ( figsize = ( 25 , 10 ) ) fig = sns . boxplot ( x = 'Dept' , y = 'Weekly_Sales' , data = data , showfliers = False )
539	fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 13 , 8 ) sn . countplot ( x = "bedrooms" , data = data , ax = ax1 ) data1 = data . groupby ( [ 'bedrooms' , 'interest_level' ] ) [ 'bedrooms' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) data1 [ [ 'low' , 'medium' , "high" ] ] . plot ( kind = 'bar' , stacked = True , ax = ax2 )
1016	clf = xgb . XGBClassifier ( ** grid . best_params_ ) clf . fit ( X_train , y_train ) sample_submission [ 'isFraud' ] = clf . predict_proba ( X_test ) [ : , 1 ] sample_submission . to_csv ( 'simple_xgboost.csv' )
1042	pickle . dump ( best_hp , open ( 'best_hp.pickle' , 'wb' ) ) best_model = tuner . get_best_models ( 1 ) [ 0 ] best_model . save ( 'best_model.h5' )
970	labels_breed = pd . read_csv ( '../input/breed_labels.csv' ) labels_state = pd . read_csv ( '../input/color_labels.csv' ) labels_color = pd . read_csv ( '../input/state_labels.csv' )
1149	epoch_datetime = pd . datetime ( 1900 , 1 , 1 ) trf_var_68_s = ( train_df [ 'var_68' ] * 10000 - 7000 + epoch_datetime . toordinal ( ) ) . astype ( int ) date_s = trf_var_68_s . map ( datetime . fromordinal ) train_df [ 'date' ] = date_s sorted_train_df = train_df . drop ( 'var_68' , axis = 1 ) . sort_values ( 'date' )
1057	predVal = neigh . predict ( X_test ) mat = [ predVal , y_test ] df = pd . DataFrame ( mat ) . transpose ( ) df . columns = ( 'h0' , 'y' ) df [ 'diff' ] = np . where ( df . h0 == df . y , 1 , 0 ) print ( '% correct =' , sum ( df [ 'diff' ] ) / len ( df [ 'diff' ] ) * 100 )
93	df . drop ( [ 'Gene' , 'Variation' ] , axis = 1 , inplace = True ) df = df [ df [ 'Text' ] != 'null' ]
1466	import os import sys sys . path . append ( "../input/pystacknet/repository/h2oai-pystacknet-af571e0" ) import pystacknet
536	import librosa y , sr = librosa . load ( "/kaggle/input/birdsong-recognition/train_audio/purfin/XC195200.mp3" , duration = 10.0 ) onset_env = librosa . onset . onset_strength ( y = y , sr = sr ) print ( onset_env )
908	bureau_by_loan = bureau_balance_agg . merge ( bureau_balance_counts , right_index = True , left_on = 'SK_ID_BUREAU' , how = 'outer' ) bureau_by_loan = bureau [ [ 'SK_ID_BUREAU' , 'SK_ID_CURR' ] ] . merge ( bureau_by_loan , on = 'SK_ID_BUREAU' , how = 'left' ) bureau_balance_by_client = agg_numeric ( bureau_by_loan . drop ( columns = [ 'SK_ID_BUREAU' ] ) , group_var = 'SK_ID_CURR' , df_name = 'client' )
1223	cat_cols = [ 'ps_ind_02_cat' , 'ps_car_04_cat' , 'ps_car_09_cat' , 'ps_ind_05_cat' , 'ps_car_01_cat' ] train , test = binary_encoding ( train , test , 'ps_ind_02_cat' ) train , test = binary_encoding ( train , test , 'ps_car_04_cat' ) train , test = binary_encoding ( train , test , 'ps_car_09_cat' ) train , test = binary_encoding ( train , test , 'ps_ind_05_cat' ) train , test = binary_encoding ( train , test , 'ps_car_01_cat' )
202	MIN_BOUND = - 1000.0 MAX_BOUND = 400.0 def normalize ( image ) : image = ( image - MIN_BOUND ) / ( MAX_BOUND - MIN_BOUND ) image [ image > 1 ] = 1. image [ image < 0 ] = 0. return image
1583	lidar_data = [ ] image_data = [ ] for record in data_json : if record [ 'fileformat' ] == 'jpeg' : image_data . append ( record ) else : lidar_data . append ( record )
1411	def oneHotEncode_dataframe ( df , features ) : for feature in features : temp_onehot_encoded = pd . get_dummies ( df [ feature ] ) column_names = [ "{}_{}" . format ( feature , x ) for x in temp_onehot_encoded . columns ] temp_onehot_encoded . columns = column_names df = df . drop ( feature , axis = 1 ) df = pd . concat ( [ df , temp_onehot_encoded ] , axis = 1 ) return df
1116	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak_df [ 'meter_reading' ] = leak_df . meter_reading_scraped leak_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak_df . fillna ( 0 , inplace = True ) leak_df . loc [ leak_df . meter_reading < 0 , 'meter_reading' ] = 0 leak_df = leak_df [ leak_df . timestamp . dt . year > 2016 ] print ( len ( leak_df ) )
298	tourney_win_result [ 'Seed_diff' ] = tourney_win_result [ 'Seed1' ] - tourney_win_result [ 'Seed2' ] tourney_win_result [ 'ScoreT_diff' ] = tourney_win_result [ 'ScoreT1' ] - tourney_win_result [ 'ScoreT2' ] tourney_lose_result [ 'Seed_diff' ] = tourney_lose_result [ 'Seed1' ] - tourney_lose_result [ 'Seed2' ] tourney_lose_result [ 'ScoreT_diff' ] = tourney_lose_result [ 'ScoreT1' ] - tourney_lose_result [ 'ScoreT2' ]
471	train_df = train_transaction . merge ( train_identity , how = 'left' , left_index = True , right_index = True ) test_df = test_transaction . merge ( test_identity , how = 'left' , left_index = True , right_index = True ) print ( "Train shape : " + str ( train_df . shape ) ) print ( "Test shape : " + str ( test_df . shape ) )
1505	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) fasttext_embeddings = load_fasttext ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings , fasttext_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings , fasttext_embeddings gc . collect ( ) np . shape ( embedding_matrix )
1209	temp = ht [ "authorized_flag" ] . value_counts ( ) df = pd . DataFrame ( { 'labels' : temp . index , 'values' : temp . values } ) plt . figure ( figsize = ( 6 , 6 ) ) plt . title ( 'authorized_flag - Y or N' ) sns . set_color_codes ( "pastel" ) sns . barplot ( x = 'labels' , y = "values" , data = df ) locs , labels = plt . xticks ( ) plt . show ( )
1300	int8columns = stats [ stats [ 'max' ] < 256 ] . index print ( int8columns . shape ) print ( int8columns ) int16columns = stats [ ( stats [ 'max' ] >= 256 ) & ( stats [ 'max' ] <= 32767 ) ] . index print ( int16columns . shape ) print ( int16columns )
1446	dtypes = { 'ip' : 'uint32' , 'app' : 'uint16' , 'device' : 'uint16' , 'os' : 'uint16' , 'channel' : 'uint16' , 'is_attributed' : 'uint8' } train = dd . read_csv ( "../input/train.csv" , dtype = dtypes , parse_dates = [ 'click_time' , 'attributed_time' ] ) train . head ( )
468	import pandas as pd import numpy as np import warnings warnings . filterwarnings ( 'ignore' ) from datetime import datetime from sklearn . model_selection import RandomizedSearchCV , GridSearchCV from sklearn import metrics from sklearn . metrics import roc_auc_score from sklearn . model_selection import StratifiedKFold from xgboost import XGBClassifier pd . set_option ( 'display.max_columns' , 200 )
354	corr_matrix = features . corr ( ) . abs ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) ; threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style collinear_features = [ column for column in upper . columns if any ( upper [ column ] > threshold ) ] upper . style . applymap ( highlight )
1101	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
674	train_image_labels = pd . read_csv ( '../input/avito-images-recognized/train_image_labels.csv' , index_col = 'image_id' ) test_image_labels = pd . read_csv ( '../input/avito-images-recognized/test_image_labels.csv' , index_col = 'image_id' ) all_image_labels = pd . concat ( [ train_image_labels , test_image_labels ] , axis = 0 )
699	for household in not_equal . index : true_target = int ( train [ ( train [ 'idhogar' ] == household ) & ( train [ 'parentesco1' ] == 1.0 ) ] [ 'Target' ] ) train . loc [ train [ 'idhogar' ] == household , 'Target' ] = true_target all_equal = train . groupby ( 'idhogar' ) [ 'Target' ] . apply ( lambda x : x . nunique ( ) == 1 ) not_equal = all_equal [ all_equal != True ] print ( 'There are {} households where the family members do not all have the same target.' . format ( len ( not_equal ) ) )
832	pca_df = pd . DataFrame ( { 'pc_1' : train_pca [ : , 0 ] , 'pc_2' : train_pca [ : , 1 ] , 'target' : train_labels } ) sns . lmplot ( 'pc_1' , 'pc_2' , data = pca_df , hue = 'target' , fit_reg = False , size = 10 ) plt . title ( 'PC2 vs PC1 by Target' ) ;
925	application [ 'income_bins' ] = pd . cut ( application [ 'AMT_INCOME_TOTAL' ] , range ( 0 , 1000000 , 10000 ) ) ed = application . groupby ( [ 'TARGET' , 'income_bins' ] ) [ 'TARGET' ] . count ( ) . unstack ( 'TARGET' ) . fillna ( 0 ) ed . plot ( kind = 'bar' , stacked = True , figsize = ( 50 , 15 ) ) print ( ed )
866	feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'app' , trans_primitives = default_trans_primitives , agg_primitives = default_agg_primitives , max_depth = 2 , features_only = False , verbose = True ) pd . options . display . max_columns = 1700 feature_matrix . head ( 10 )
679	import zipfile NUM_IMAGES_TO_EXTRACT = 1000 with zipfile . ZipFile ( '../input/avito-demand-prediction/train_jpg.zip' , 'r' ) as train_zip : files_in_zip = sorted ( train_zip . namelist ( ) ) for idx , file in enumerate ( files_in_zip [ : NUM_IMAGES_TO_EXTRACT ] ) : if file . endswith ( '.jpg' ) : train_zip . extract ( file , path = file . split ( '/' ) [ 3 ] )
616	def perform_svc ( train_X , train_Y , test_X , test_Y ) : svr_clf = SVR ( ) svr_clf . fit ( X = train_X , y = train_Y ) pred_Y = svr_clf . predict ( test_X ) r2_score_svc = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( svr_clf . score ( train_X , train_Y ) * 100 , 2 ) returnval = { 'model' : 'SVR' , 'r2_score' : r2_score_svc } return returnval
269	models = pd . DataFrame ( { 'Model' : [ 'Linear Regression' , 'Support Vector Machines' , 'Linear SVR' , 'MLPRegressor' , 'Stochastic Gradient Decent' , 'Decision Tree Regressor' , 'Random Forest' , 'XGB' , 'LGBM' , 'GradientBoostingRegressor' , 'RidgeRegressor' , 'BaggingRegressor' , 'ExtraTreesRegressor' , 'AdaBoostRegressor' , 'VotingRegressor' ] , 'r2_train' : acc_train_r2 , 'r2_test' : acc_test_r2 , 'd_train' : acc_train_d , 'd_test' : acc_test_d , 'rmse_train' : acc_train_rmse , 'rmse_test' : acc_test_rmse } )
1339	temp_col = features_dtype_object [ 12 ] plot_count_percent_for_object ( application_train , temp_col ) plot_count_percent_for_object ( application_object_na_filled , temp_col )
1544	token = Tokenizer ( ) token . fit_on_texts ( [ "Let us learn on a example" ] ) print ( token . texts_to_sequences ( [ "Let us learn on a example" ] ) ) print ( token . texts_to_sequences ( [ "Let us hopefully learn on a example" ] ) )
470	import numpy as np import pandas as pd from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import train_test_split , StratifiedKFold , KFold from bayes_opt import BayesianOptimization from datetime import datetime from sklearn . metrics import precision_score , recall_score , confusion_matrix , accuracy_score , roc_auc_score , f1_score , roc_curve , auc , precision_recall_curve from sklearn import metrics from sklearn import preprocessing import lightgbm as lgb import warnings warnings . filterwarnings ( "ignore" ) import itertools from scipy import interp import seaborn as sns import matplotlib . pyplot as plt from matplotlib import rcParams
1002	original_fake_paths = [ ] for dirname , _ , filenames in tqdm ( os . walk ( '/kaggle/input/1-million-fake-faces/' ) ) : for filename in filenames : original_fake_paths . append ( [ os . path . join ( dirname , filename ) , filename ] )
797	import pandas as pd import numpy as np import lightgbm as lgb from sklearn . model_selection import KFold , train_test_split from sklearn . metrics import roc_auc_score import matplotlib . pyplot as plt import seaborn as sns plt . rcParams [ 'font.size' ] = 18 N_FOLDS = 5 MAX_EVALS = 5
228	n = 9 commits_df . loc [ n , 'commit_num' ] = 11 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 240 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25868
1399	col = numeric_features [ 48 ] plot_category_percent_of_target_for_numeric ( col )
31	Sum_of_squared_distances = [ ] K = range ( 1 , 15 ) for k in K : km = KMeans ( n_clusters = k ) km = km . fit ( train_test ) Sum_of_squared_distances . append ( km . inertia_ )
1397	col = numeric_features [ 46 ] plot_category_percent_of_target_for_numeric ( col )
395	TRAIN_MASKS_CSV [ 'id' ] = TRAIN_MASKS_CSV [ 'img' ] . apply ( lambda x : x [ : - 7 ] ) len ( TRAIN_MASKS_CSV [ 'id' ] . unique ( ) ) , len ( TRAIN_MASKS_CSV [ 'id' ] . unique ( ) ) * 16
240	n = 21 commits_df . loc [ n , 'commit_num' ] = 25 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 212 commits_df . loc [ n , 'LB_score' ] = 0.25823
350	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) import matplotlib . pyplot as plt import featuretools as ft from featuretools . primitives import * from featuretools . variable_types import Numeric from sklearn . preprocessing import LabelEncoder , MinMaxScaler from sklearn . svm import LinearSVR from sklearn . feature_selection import SelectFromModel from sklearn . ensemble import RandomForestRegressor import warnings warnings . filterwarnings ( "ignore" )
220	n = 1 commits_df . loc [ n , 'commit_num' ] = 3 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 256 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25855
1015	def create_title_mode ( train_labels ) : titles = train_labels . title . unique ( ) title2mode = { } for title in titles : mode = ( train_labels [ train_labels . title == title ] . accuracy_group . value_counts ( ) . index [ 0 ] ) title2mode [ title ] = mode return title2mode def add_title_mode ( labels , title2mode ) : labels [ 'title_mode' ] = labels . title . apply ( lambda title : title2mode [ title ] ) return labels
826	train_labels = train [ "TARGET" ] train_ids = train [ 'SK_ID_CURR' ] test_ids = test [ 'SK_ID_CURR' ] train = pd . get_dummies ( train . drop ( columns = all_missing ) ) test = pd . get_dummies ( test . drop ( columns = all_missing ) ) train , test = train . align ( test , join = 'inner' , axis = 1 ) print ( 'Training set full shape: ' , train . shape ) print ( 'Testing set full shape: ' , test . shape )
295	N = len ( subs ) sub = subs [ 0 ] . copy ( ) sub [ "FVC" ] = 0 sub [ "Confidence" ] = 0 for i in range ( N ) : sub [ "FVC" ] += subs [ 0 ] [ "FVC" ] * ( 1 / N ) sub [ "Confidence" ] += subs [ 0 ] [ "Confidence" ] * ( 1 / N )
1391	col = numeric_features [ 40 ] plot_category_percent_of_target_for_numeric ( col )
957	test_predictions = [ ] for image in tqdm . tqdm ( test_loader ) : image = image [ 0 ] . type ( torch . float ) . to ( device ) y_pred = model ( image ) . cpu ( ) . detach ( ) . numpy ( ) test_predictions . append ( y_pred ) test_predictions_stacked = np . vstack ( test_predictions ) [ : , 0 , : , : ] test_predictions_stacked = test_predictions_stacked [ : , y_min_pad : - y_max_pad , x_min_pad : - x_max_pad ] print ( test_predictions_stacked . shape )
44	embeddings_train = [ ] for i in tqdm ( range ( 25 ) ) : embeddings = embed ( train_text [ i ] ) embeddings_train . append ( embeddings )
739	def submit ( model , train , train_labels , test , test_ids ) : model . fit ( train , train_labels ) predictions = model . predict ( test ) predictions = pd . DataFrame ( { 'idhogar' : test_ids , 'Target' : predictions } ) submission = submission_base . merge ( predictions , on = 'idhogar' , how = 'left' ) . drop ( columns = [ 'idhogar' ] ) submission [ 'Target' ] = submission [ 'Target' ] . fillna ( 4 ) . astype ( np . int8 ) return submission
534	orderCount = orders [ orders [ "eval_set" ] == "prior" ] . groupby ( by = [ "user_id" ] ) [ "order_id" ] . count ( ) . to_frame ( ) fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( color = " ax.set(xlabel='Order Count',title=" Order Count " )
13	embed_size = 300 max_features = 130000 max_len = 220 list_classes = [ "toxic" , "severe_toxic" , "obscene" , "threat" , "insult" , "identity_hate" ] train_text = train_text . str . lower ( ) test_text = test_text . str . lower ( ) all_text = all_text . str . lower ( )
995	preds = test_preds . argmax ( axis = 1 ) . astype ( np . int8 ) print ( f'predicted accuracy_group distribution:\n\n{pd.Series(preds).value_counts(normalize=True)} \n\n' ) submission [ 'accuracy_group' ] = preds submission . to_csv ( 'submission.csv' , index = False )
1292	test [ 'Min_week' ] = test . groupby ( 'Patient' ) [ 'Weeks' ] . transform ( 'min' ) base = test . loc [ test . Weeks == test . Min_week ] base = base [ [ 'Patient' , 'FVC' ] ] . copy ( ) base . columns = [ 'Patient' , 'Base_FVC' ] test = test . merge ( base , on = 'Patient' , how = 'left' ) test [ 'Base_week' ] = test [ 'Weeks' ] - test [ 'Min_week' ]
935	use_selected = True data_src = '../input/' aggs_all_num = [ 'mean' , 'median' , 'min' , 'max' , 'count' , 'std' , 'sem' , 'sum' , 'mad' ] aggs_medium_num = [ 'mean' , 'median' , 'min' , 'max' , 'count' , 'std' , 'sem' , 'sum' ] aggs_all_cat = [ 'mean' , 'std' , 'sum' , 'median' , 'count' ]
978	IPython . OutputArea . prototype . _should_scroll = function ( lines ) { return false ; }
930	model = [ ] for i in range ( 0 , 6 ) : m = MLPClassifier ( solver = 'adam' , hidden_layer_sizes = ( 30 , 30 , 30 ) , random_state = 1 ) model . append ( m ) print ( model )
1510	def create_video ( image_list , out_file ) : height , width = image_list [ 0 ] . shape fourcc = cv2 . VideoWriter_fourcc ( * 'X264' ) fps = 30.0 video = cv2 . VideoWriter ( out_file , fourcc , fps , ( width , height ) , False ) for im in image_list : video . write ( im . astype ( np . uint8 ) ) cv2 . destroyAllWindows ( ) video . release ( )
691	def process_det ( index , outputs , score_threshold = 0.5 ) : boxes = outputs [ index ] [ 'boxes' ] . data . cpu ( ) . numpy ( ) scores = outputs [ index ] [ 'scores' ] . data . cpu ( ) . numpy ( ) boxes = ( boxes ) . clip ( min = 0 , max = 1023 ) . astype ( int ) indexes = np . where ( scores > score_threshold ) boxes = boxes [ indexes ] scores = scores [ indexes ] return boxes , scores
558	masks = pd . read_csv ( os . path . join ( '../input/' , 'train_ship_segmentations_v2.csv' ) ) print ( masks . shape [ 0 ] , 'masks found' ) print ( masks [ 'ImageId' ] . value_counts ( ) . shape [ 0 ] ) masks . head ( )
638	import numpy as np import pandas as pd import os import tensorflow as tf import cv2 import skimage . io import imgaug as ia from imgaug import augmenters as iaa from PIL import Image import keras . backend as K K . set_image_data_format ( 'channels_last' ) K . set_learning_phase ( 1 ) import seaborn as sns import matplotlib . pyplot as plt
1176	plt . figure ( figsize = ( 10 , 8 ) ) sns . heatmap ( link_count ) plt . show ( )
1019	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) train2 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv" ) train2 . toxic = train2 . toxic . round ( ) . astype ( int ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
1387	col = numeric_features [ 36 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1587	volumeByAssets = market_train_df . groupby ( market_train_df [ 'assetCode' ] ) [ 'volume' ] . sum ( ) highestVolumes = volumeByAssets . sort_values ( ascending = False ) [ 0 : 10 ] trace1 = go . Pie ( labels = highestVolumes . index , values = highestVolumes . values ) layout = dict ( title = "Highest trading volumes" ) data = [ trace1 ] py . iplot ( dict ( data = data , layout = layout ) , filename = 'basic-line' )
297	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns import eli5 from sklearn . linear_model import LogisticRegression , LinearRegression from sklearn . preprocessing import StandardScaler from sklearn . utils import shuffle from sklearn . model_selection import GridSearchCV , KFold , train_test_split from sklearn import preprocessing from sklearn . metrics import confusion_matrix import lightgbm as lgb import xgboost as xgb import gc import warnings warnings . filterwarnings ( "ignore" )
527	dtypestrain = { } dtypestrain [ 'ID_code' ] = 'category' dtypestrain [ 'target' ] = 'int8' for i in range ( 0 , 200 ) : dtypestrain [ 'var_' + str ( i ) ] = 'float32' dtypestest = { } dtypestest [ 'ID_code' ] = 'category' for i in range ( 0 , 200 ) : dtypestest [ 'var_' + str ( i ) ] = 'float32'
35	import numpy as np import pandas as pd import os from PIL import Image from tqdm import tqdm from sklearn import preprocessing from sklearn . model_selection import StratifiedKFold , cross_val_score from sklearn . metrics import roc_auc_score import lightgbm as lgb
1166	sub = pd . read_csv ( '/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv' ) TRAINING_FILENAMES = tf . io . gfile . glob ( GCS_PATH + '/tfrecords/train*.tfrec' ) TRAINING_FILENAMES_LIST = [ TRAINING_FILENAMES [ : 4 ] , TRAINING_FILENAMES [ 4 : 8 ] , TRAINING_FILENAMES [ 8 : 12 ] , TRAINING_FILENAMES [ 12 : ] ] print ( TRAINING_FILENAMES_LIST )
374	xgb_clf = xgb . XGBRegressor ( { 'objective' : 'reg:squarederror' } ) parameters = { 'n_estimators' : [ 200 , 300 ] , 'learning_rate' : [ 0.01 , 0.02 , 0.03 ] , 'max_depth' : [ 10 , 12 ] } xgb_reg = GridSearchCV ( estimator = xgb_clf , param_grid = parameters , cv = 5 , n_jobs = - 1 ) . fit ( trainb , targetb ) print ( "Best score: %0.3f" % xgb_reg . best_score_ ) print ( "Best parameters set:" , xgb_reg . best_params_ ) acc_boosting_model ( 7 , xgb_reg , trainb , testb )
1243	data = pd . concat ( [ stores [ 'Type' ] , stores [ 'Size' ] ] , axis = 1 ) f , ax = plt . subplots ( figsize = ( 8 , 6 ) ) fig = sns . boxplot ( x = 'Type' , y = 'Size' , data = data )
1188	N = sub_df . shape [ 0 ] x_sub = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( sub_df [ 'Patient' ] ) ) : x_sub [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
1035	data_dir = '/kaggle/input/stanford-covid-vaccine/' train = pd . read_json ( data_dir + 'train.json' , lines = True ) test = pd . read_json ( data_dir + 'test.json' , lines = True ) sample_df = pd . read_csv ( data_dir + 'sample_submission.csv' )
805	from hyperopt import tpe tpe_algorithm = tpe . suggest
849	a = 0 b = 0 for x in param_grid [ 'learning_rate' ] : if x >= 0.005 and x < 0.05 : a += 1 elif x >= 0.05 and x < 0.5 : b += 1 print ( 'There are {} values between 0.005 and 0.05' . format ( a ) ) print ( 'There are {} values between 0.05 and 0.5' . format ( b ) )
1330	train_df . head ( ) percent = ( 100 * train_df . isnull ( ) . sum ( ) / train_df . shape [ 0 ] ) . sort_values ( ascending = False ) percent [ : 10 ]
1547	with open ( "../input/glove-wiki-twitter2550/glove.twitter.27B.50d.txt" ) as f : lines = f . readlines ( ) lines = [ line . rstrip ( ) . split ( ) for line in lines ] print ( len ( lines ) ) print ( len ( lines [ 0 ] ) ) print ( lines [ 99 ] [ 0 ] ) print ( lines [ 99 ] [ 1 : ] ) print ( len ( lines [ 99 ] [ 1 : ] ) )
740	rf_submission = submit ( RandomForestClassifier ( n_estimators = 100 , random_state = 10 , n_jobs = - 1 ) , train_set , train_labels , test_set , test_ids ) rf_submission . to_csv ( 'rf_submission.csv' , index = False )
345	test_gen = \ test_generator ( df_test , test_batch_size , num_rows , num_cols ) model . load_weights ( filepath = 'model.h5' ) predictions = model . predict_generator ( test_gen , steps = num_test_batches , max_queue_size = 1 , workers = 1 , use_multiprocessing = False , verbose = 1 )
1406	import numpy as np import pandas as pd import matplotlib . pyplot as plt import matplotlib import seaborn as sns import missingno as msno import xgboost as xgb import warnings sns . set ( style = 'white' , context = 'notebook' , palette = 'deep' )
618	def perform_knn ( df_X , df_Y , test_df_X , test_Y ) : knn = KNeighborsRegressor ( n_neighbors = 5 ) knn . fit ( df_X , df_Y ) pred_Y = knn . predict ( test_df_X ) r2_score_knn = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( knn . score ( df_X , df_Y ) * 100 , 2 ) returnval = { 'model' : 'KNeighborsRegressor' , 'r2_score' : r2_score_knn } return returnval
75	sz = 128 bs = 32 tfms = get_transforms ( do_flip = False , flip_vert = False , max_rotate = 20 , max_zoom = 1.5 , max_lighting = 0.2 )
1349	def overdue ( x ) : if x < 30 : return 'A' elif x < 60 : return 'B' elif x < 90 : return 'C' elif x < 180 : return 'D' elif x < 365 : return 'E' else : return 'F'
915	top_100 = list ( fi_raw_sorted [ 'feature' ] ) [ : 100 ] new_features = [ x for x in top_100 if x not in list ( fi [ 'feature' ] ) ] print ( '%% of Top 100 Features created from the bureau data = %d.00' % len ( new_features ) )
1335	application_train = pd . read_csv ( '../input/application_train.csv' ) bureau_balance = pd . read_csv ( '../input/bureau_balance.csv' ) previous_application = pd . read_csv ( '../input/previous_application.csv' )
433	i = np . arange ( 20 ) tag_df_sorted . head ( 20 ) . plot ( kind = 'bar' ) plt . title ( 'Frequency of top 20 tags' ) plt . xticks ( i , tag_df_sorted [ 'Tags' ] ) plt . xlabel ( 'Tags' ) plt . ylabel ( 'Counts' ) plt . show ( )
1461	for i in tqdm ( range ( len ( df_test ) ) ) : if df_test [ 'sentiment' ] . iloc [ i ] == 'neutral' : df_test [ 'selected_text' ] . iloc [ i ] = df_test [ 'text' ] . iloc [ i ] else : pass
448	train [ 'square_feet' ] = np . log1p ( train [ 'square_feet' ] ) test [ 'square_feet' ] = np . log1p ( test [ 'square_feet' ] ) bold ( '**Distribution after log tranformation**' ) distplot ( train [ 'square_feet' ] , 'darkgreen' )
1302	for k , v in nas . items ( ) : df_test [ f'{k}_isna' ] = df_test [ k ] . isnull ( ) df_test [ k ] . fillna ( v , inplace = True )
721	plt . figure ( figsize = ( 10 , 8 ) ) sns . violinplot ( x = 'Target' , y = 'inst' , data = ind ) ; plt . title ( 'Education Distribution by Target' ) ;
1213	data_dir = '../input/alaska2-image-steganalysis' folder_names = [ 'JMiPOD/' , 'JUNIWARD/' , 'UERD/' ] class_names = [ 'Normal' , 'JMiPOD_75' , 'JMiPOD_90' , 'JMiPOD_95' , 'JUNIWARD_75' , 'JUNIWARD_90' , 'JUNIWARD_95' , 'UERD_75' , 'UERD_90' , 'UERD_95' ] class_labels = { name : i for i , name in enumerate ( class_names ) }
407	f = filenames [ 0 ] r1 = stage_2_PIL ( f ) r2 = stage_2_cv2 ( f ) plt . figure ( figsize = ( 16 , 16 ) ) plt . subplot ( 131 ) plt . imshow ( r1 ) plt . subplot ( 132 ) plt . imshow ( r2 ) plt . subplot ( 133 ) plt . imshow ( np . abs ( r1 - r2 ) )
673	print ( 'Coefficient of variation (CV) for prices in different categories (category_name).' ) dffd = dff . groupby ( 'category_name' ) [ 'price' ] . apply ( lambda x : np . std ( x ) / np . mean ( x ) ) . sort_values ( ascending = False ) dffd
1269	input_layer = tf . keras . layers . InputLayer ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , name = 'input_layer' ) data_augmentation_layer = Data_Augmentation_Dummy ( ) with strategy . scope ( ) : model = tf . keras . Sequential ( [ input_layer , data_augmentation_layer ] )
1369	col = numeric_features [ 16 ] plot_category_percent_of_target_for_numeric ( col )
1208	fig , ( ( ax1 , ax2 ) ) = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 14 , 8 ) ) sns . boxplot ( x = "feature_1" , y = "target" , hue = "feature_2" , data = train , palette = "Set3" , ax = ax1 ) sns . boxplot ( x = "feature_2" , y = "target" , hue = "feature_1" , data = train , palette = "Set3" , ax = ax2 )
1557	first_text = train . text . values [ 0 ] first_text_list = nltk . word_tokenize ( first_text ) print ( first_text_list )
1192	train_df = pd . read_csv ( '../input/prostate-cancer-grade-assessment/train.csv' ) test_df = pd . read_csv ( '../input/prostate-cancer-grade-assessment/test.csv' ) print ( train_df . shape ) print ( test_df . shape ) train_df . head ( )
1389	col = numeric_features [ 38 ] plot_category_percent_of_target_for_numeric ( col )
1094	def calc_snr ( sample ) : ratios = np . zeros ( 5 ) for i , ( deg , err ) in enumerate ( zip ( mes_cols , err_cols ) ) : ratio = ( np . array ( sample [ deg ] ) / np . array ( sample [ err ] ) ) . mean ( ) ratios [ i ] = ratio return ratios . mean ( )
709	heads [ 'walls+roof+floor' ] = heads [ 'walls' ] + heads [ 'roof' ] + heads [ 'floor' ] plot_categoricals ( 'walls+roof+floor' , 'Target' , heads , annotate = False )
396	test_metadata_csv . loc [ test_metadata_csv [ 'trim1' ] . isnull ( ) , 'trim1' ] = '-' test_gb_year_make_model_trim1 = test_metadata_csv . groupby ( [ 'year' , 'make' , 'model' , 'trim1' ] ) len ( test_gb_year_make_model_trim1 . groups )
785	sns . lmplot ( 'pickup_Elapsed' , 'fare_amount' , hue = 'pickup_Year' , palette = palette , size = 8 , scatter_kws = { 'alpha' : 0.05 } , markers = '.' , fit_reg = False , data = data . sample ( 1000000 , random_state = RSEED ) ) ; plt . title ( 'Fare Amount versus Time Since Start of Records' ) ;
117	Xmas_date = [ pd . datetime ( 2011 , 12 , 25 ) , pd . datetime ( 2012 , 12 , 25 ) , pd . datetime ( 2013 , 12 , 25 ) , pd . datetime ( 2014 , 12 , 25 ) , pd . datetime ( 2015 , 12 , 25 ) ] state_group . drop ( Xmas_date , inplace = True )
1523	th_t = np . array ( [ 0.565 , 0.39 , 0.55 , 0.345 , 0.33 , 0.39 , 0.33 , 0.45 , 0.38 , 0.39 , 0.34 , 0.42 , 0.31 , 0.38 , 0.49 , 0.50 , 0.38 , 0.43 , 0.46 , 0.40 , 0.39 , 0.505 , 0.37 , 0.47 , 0.41 , 0.545 , 0.32 , 0.1 ] ) ( pred_t > th_t ) . mean ( axis = 0 )
1497	def product_less ( a , b ) : a = np . array ( a ) b = np . array ( b ) return ( np . array ( a ) < np . array ( b ) ) . all ( )
1100	for task , prediction in tqdm ( zip ( train_tasks , train_predictions ) ) : if input_output_shape_is_same ( task ) : for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
896	def most_recent ( y , x ) : df = pd . DataFrame ( { "x" : x , "y" : y } ) . dropna ( ) if df . shape [ 0 ] < 1 : return np . nan df = df . sort_values ( 'x' , ascending = False ) . reset_index ( ) return df . iloc [ 0 ] [ 'y' ] MostRecent = make_agg_primitive ( function = most_recent , input_types = [ Discrete , Datetime ] , return_type = Discrete )
1415	import seaborn as sns sns . set ( ) g = sns . FacetGrid ( pd . melt ( df [ [ 'bone_length' , 'rotting_flesh' , 'hair_length' , 'has_soul' , 'type' ] ] , id_vars = 'type' ) , col = 'type' ) g . map ( sns . boxplot , 'value' , 'variable' )
1329	import pandas as pd import numpy as np import time import lightgbm as lgb from sklearn . metrics import mean_squared_error from sklearn . model_selection import KFold import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'seaborn' ) sns . set ( font_scale = 2 ) import warnings warnings . filterwarnings ( 'ignore' )
154	model . save_model ( "cbmodel.cbm" , format = "cbm" , export_parameters = None , pool = None )
364	img_1 = get_image_data ( '1023' , 'Type_1' ) img_2 = get_image_data ( '531' , 'Type_1' ) img_3 = get_image_data ( '596' , 'Type_1' ) img_4 = get_image_data ( '1061' , 'Type_1' ) img_5 = get_image_data ( '1365' , 'Type_2' )
112	model . compile ( optimizer = 'adam' , loss = 'mean_squared_error' , metrics = [ 'mae' ] ) model . fit ( x , Y , batch_size = 64 , epochs = 100 ) model . fit ( x , Y , batch_size = 128 , epochs = 50 ) model . fit ( x , Y , batch_size = 256 , epochs = 50 )
738	model_results = cv_model ( train_set , train_labels , RandomForestClassifier ( 100 , random_state = 10 ) , 'RF' , model_results )
105	from six . moves import cPickle as pickle import bz2 def loadPickleBZ ( pickle_file ) : with bz2 . BZ2File ( pickle_file , 'r' ) as f : loadedData = pickle . load ( f ) return loadedData def savePickleBZ ( pickle_file , data ) : with bz2 . BZ2File ( pickle_file , 'w' ) as f : pickle . dump ( data , f , pickle . HIGHEST_PROTOCOL ) return
209	linreg = LinearRegression ( ) linreg . fit ( train , target ) coeff_linreg = pd . DataFrame ( train . columns . delete ( 0 ) ) coeff_linreg . columns = [ 'feature' ] coeff_linreg [ "score_linreg" ] = pd . Series ( linreg . coef_ ) coeff_linreg . sort_values ( by = 'score_linreg' , ascending = False )
1420	country = 'China' df = get_time_series ( country ) if len ( df ) > 1 and df . iloc [ - 2 , 0 ] >= df . iloc [ - 1 , 0 ] : df . drop ( df . tail ( 1 ) . index , inplace = True ) df . tail ( 10 )
1123	import datetime START_DATE = '2017-12-01' startdate = datetime . datetime . strptime ( START_DATE , '%Y-%m-%d' ) df [ 'TransactionDT' ] = df [ 'TransactionDT' ] . apply ( lambda x : ( startdate + datetime . timedelta ( seconds = x ) ) ) df [ 'hour' ] = df [ 'TransactionDT' ] . dt . hour
754	model = RandomForestClassifier ( max_depth = None , n_estimators = 10 ) model . fit ( train_selected , train_labels ) estimator_nonlimited = model . estimators_ [ 5 ] export_graphviz ( estimator_nonlimited , out_file = 'tree_nonlimited.dot' , feature_names = train_selected . columns , class_names = [ 'extreme' , 'moderate' , 'vulnerable' , 'non-vulnerable' ] , rounded = True , proportion = False , precision = 2 )
166	print ( "Number of different values :" ) print ( "IP :" , len ( df [ 'ip' ] . unique ( ) ) ) print ( "App :" , len ( df [ 'app' ] . unique ( ) ) ) print ( "Device :" , len ( df [ 'device' ] . unique ( ) ) ) print ( "OS :" , len ( df [ 'os' ] . unique ( ) ) ) print ( "Channel :" , len ( df [ 'channel' ] . unique ( ) ) )
611	print ( 'loading word embeddings...' ) embeddings_index = { } f = codecs . open ( '../input/fasttext/wiki.simple.vec' , encoding = 'utf-8' ) for line in tqdm ( f ) : values = line . rstrip ( ) . rsplit ( ' ' ) word = values [ 0 ] coefs = np . asarray ( values [ 1 : ] , dtype = 'float32' ) embeddings_index [ word ] = coefs f . close ( ) print ( 'found %s word vectors' % len ( embeddings_index ) )
670	cheap = df [ df . price < 10 ] . category_name . value_counts ( ) . map ( lambda x : '{:.2f}%' . format ( x / df . shape [ 0 ] * 100 ) ) print ( 'Categories of items < 10 \u20BD (top 10)' ) cheap . head ( 10 )
1252	le = LabelEncoder ( ) train_mask = ~ train [ 'sexo' ] . isnull ( ) train . loc [ train_mask , 'sexo' ] = le . fit_transform ( train [ 'sexo' ] [ train_mask ] )
629	date_agg_4 = train_agg . groupby ( level = [ 1 , 2 , 3 ] ) . sum ( ) date_agg_4 . columns = ( 'bookings' , 'total' ) date_agg_4 . reset_index ( inplace = True ) date_agg_4 [ 'dt' ] = pd . to_datetime ( date_agg_4 . year * 10000 + date_agg_4 . month * 100 + date_agg_4 . day , format = '%Y%m%d' ) date_agg_4 . head ( )
944	labels_breed = pd . read_csv ( '../input/breed_labels.csv' ) labels_state = pd . read_csv ( '../input/color_labels.csv' ) labels_color = pd . read_csv ( '../input/state_labels.csv' )
25	pred_test = ( test_pred_stack > thresh ) . astype ( np . int ) submission = pd . DataFrame . from_dict ( { 'qid' : test [ 'qid' ] } ) submission [ 'prediction' ] = pred_test submission . to_csv ( 'submission.csv' , index = False )
505	target0df = metadata_train [ metadata_train [ 'target' ] == 0 ] target1df = metadata_train [ metadata_train [ 'target' ] == 1 ] print ( "target0data shape:" , target0df . shape ) print ( "target1data shape:" , target1df . shape )
706	corr_matrix = heads . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
1024	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) tokenizer . save_pretrained ( '.' ) fast_tokenizer = BertWordPieceTokenizer ( 'vocab.txt' , lowercase = False ) fast_tokenizer
1143	for col in df . select_dtypes ( 'object' ) : print ( df [ col ] . sample ( 5 ) ) print ( f"{df[col].nunique()} unique values for {col}, which has {len(df[col])} rows." )
795	model . n_jobs = - 1 model . fit ( X_train [ features ] , y_train ) evaluate ( model , features , X_train , X_valid , y_train , y_valid )
1132	df [ "diff_V319_V320" ] = np . zeros ( df . shape [ 0 ] ) df [ "diff_V320_V321" ] = np . zeros ( df . shape [ 0 ] ) df [ "diff_V319_V321" ] = np . zeros ( df . shape [ 0 ] )
141	train = all_df [ : train . shape [ 0 ] ] test = all_df [ train . shape [ 0 ] : ]
1271	def get_training_dataset_raw ( ) : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True , ordered = False ) return dataset raw_training_dataset = get_training_dataset_raw ( ) label_counter = Counter ( ) for images , labels in raw_training_dataset : label_counter . update ( [ labels . numpy ( ) ] ) del raw_training_dataset label_counting_sorted = label_counter . most_common ( ) NUM_TRAINING_IMAGES = sum ( [ x [ 1 ] for x in label_counting_sorted ] ) print ( "number of examples in the original training dataset: {}" . format ( NUM_TRAINING_IMAGES ) ) print ( "labels in the original training dataset, sorted by occurrence" ) label_counting_sorted
1154	store_to_trend_dict = ( pd . concat ( dfs ) . loc [ lambda df : df [ "ds" ] == TRAIN_END_DATE , [ "store_id" , "trend" ] ] . set_index ( "store_id" ) [ "trend" ] . to_dict ( ) )
1217	metrics = { 'loss' : Loss ( criterion ) , 'accuracy' : Accuracy ( ) , } trainer = create_supervised_trainer ( model , optimizer , criterion , device = device ) val_evaluator = create_supervised_evaluator ( model , metrics = metrics , device = device )
1491	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 13 - Unclear Abnormality" ) draw ( parsed [ df [ 'patientId' ] [ 106 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 106 ] ] )
302	params = { 'num_leaves' : 35 , 'max_depth' : 5 , 'subsample' : 0.4 , 'min_child_samples' : 10 , 'learning_rate' : 0.01 , 'num_iterations' : 500 , 'random_state' : 12 }
113	train_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv" ) calendar_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/calendar.csv" ) price_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv" ) sample = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv" )
1553	import base64 import numpy as np import pandas as pd import plotly . offline as py py . init_notebook_mode ( connected = True ) import plotly . graph_objs as go import plotly . tools as tls from collections import Counter from scipy . misc import imread from sklearn . feature_extraction . text import TfidfVectorizer , CountVectorizer from sklearn . decomposition import NMF , LatentDirichletAllocation from matplotlib import pyplot as plt
1313	total = df_train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = 100 * ( df_train . isnull ( ) . sum ( ) / df_train . isnull ( ) . count ( ) ) . sort_values ( ascending = False ) missing_df = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_df . head ( 20 )
1509	tst_leak = pd . read_csv ( '../input/breaking-lb-fresh-start-with-lag-selection/test_leak.csv' ) test [ 'leak' ] = tst_leak [ 'compiled_leak' ] test [ 'log_leak' ] = np . log1p ( tst_leak [ 'compiled_leak' ] )
653	rf1 = RandomForestRegressor ( n_jobs = - 1 , verbose = 1 ) rf1 . fit ( test [ 'x' ] [ 0 ] . ix [ : , range ( 34 ) ] , test [ 'y' ] [ 0 ] ) rf1 . score ( test [ 'x' ] [ 0 ] . ix [ : , range ( 34 ) ] , test [ 'y' ] [ 0 ] )
416	ax = np . log ( df_grouped_state . pivot ( index = 'date' , columns = 'state' , values = 'unit_sales' ) ) . plot ( figsize = ( 14 , 7 ) , colormap = cmap , linewidth = 2 ) lgd = plt . legend ( bbox_to_anchor = ( 1.05 , 1 ) , loc = 2 , borderaxespad = 0. , fontsize = 12 ) plt . title ( 'Sales evolution - 2017' , fontsize = 14 ) plt . ylabel ( 'Total units sold' ) plt . grid ( which = 'minor' ) plt . show ( )
1503	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
463	bold ( '**Updated train data for modelling:**' ) display ( df_train . head ( 3 ) ) bold ( '**Updated test data for modelling:**' ) display ( df_test . head ( 3 ) )
98	df_labels_test = pd . read_csv ( '../input/stage1_solution_filtered.csv' ) df_labels_test [ 'Class' ] = pd . to_numeric ( df_labels_test . drop ( 'ID' , axis = 1 ) . idxmax ( axis = 1 ) . str [ 5 : ] ) df_test = df_test . merge ( df_labels_test [ [ 'ID' , 'Class' ] ] , on = 'ID' , how = 'left' ) . drop ( 'ID' , axis = 1 ) df_test = df_test [ df_test [ 'Class' ] . notnull ( ) ] df_stage_2_train = pd . concat ( [ df_train , df_test ] )
313	from sklearn . metrics import roc_auc_score roc_auc_score ( y_true , y_pred )
58	import pandas as pd import numpy as np import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) train_transaction = pd . read_csv ( '../input/ieee-fraud-detection/train_transaction.csv' ) train_identity = pd . read_csv ( '../input/ieee-fraud-detection/train_identity.csv' ) train_transaction = train_transaction . merge ( train_identity , how = 'left' , left_on = 'TransactionID' , right_on = 'TransactionID' ) del train_identity
742	from sklearn . feature_selection import RFECV estimator = RandomForestClassifier ( random_state = 10 , n_estimators = 100 , n_jobs = - 1 ) selector = RFECV ( estimator , step = 1 , cv = 3 , scoring = scorer , n_jobs = - 1 )
1231	xgb_lv2_outcomes = cross_validate_xgb ( xgb_params , lv1_train_df , y_train , lv1_test_df , kf , verbose = True , verbose_eval = False , use_rank = True ) xgb_lv2_cv = xgb_lv2_outcomes [ 0 ] xgb_lv2_train_pred = xgb_lv2_outcomes [ 1 ] xgb_lv2_test_pred = xgb_lv2_outcomes [ 2 ]
1174	print ( '[!]Adding \'PAD\' to each sequence...' ) for i in tqdm ( range ( len ( clean_ ) ) ) : sentence = clean_ [ i ] [ : : - 1 ] for _ in range ( maxlen - len ( sentence ) ) : sentence . append ( 'PAD' ) clean_ [ i ] = sentence [ : : - 1 ] print ( ) PAD = np . zeros ( word2vec_ [ 'guy' ] . shape )
1012	train_resized_imgs = [ ] test_resized_imgs = [ ] for image_path in label_df [ 'Image' ] : train_resized_imgs . append ( pad_and_resize ( image_path , 'train' ) ) for image_path in submission_df [ 'Image' ] : test_resized_imgs . append ( pad_and_resize ( image_path , 'test' ) )
1436	sns . boxplot ( train_smp . is_attributed , train_smp [ 'min' ] ) plt . title ( 'Boxplot of Minute distribution' ) plt . show ( ) sns . violinplot ( train_smp . is_attributed , train_smp [ 'min' ] ) plt . title ( 'Violinplot of Minute distribution' ) plt . show ( )
1344	temp_col = 'DAYS_BIRTH' sns . kdeplot ( ( application_train_int . loc [ application_train_int [ 'TARGET' ] == 0 , temp_col ] / 365 ) . abs ( ) , label = 'repay(0)' , color = 'r' ) sns . kdeplot ( ( application_train_int . loc [ application_train_int [ 'TARGET' ] == 1 , temp_col ] / 365 ) . abs ( ) , label = 'not repay(1)' , color = 'b' ) plt . xlabel ( 'Age(years)' ) plt . title ( 'KDE for {} splitted by target' . format ( temp_col ) ) plt . show ( )
1326	binary_cat_features = [ col for col in train . columns if train [ col ] . value_counts ( ) . shape [ 0 ] == 2 ] object_features = [ 'edjefe' , 'edjefa' ] categorical_feats = binary_cat_features + object_features
789	time_features = [ 'pickup_frac_day' , 'pickup_frac_week' , 'pickup_frac_year' , 'pickup_Elapsed' ] features = [ 'abs_lat_diff' , 'abs_lon_diff' , 'haversine' , 'passenger_count' , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ] + time_features
550	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "numberofstories" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" No Of Storeys ",title=" No Of Storeys Vs Log Error " )
907	import gc gc . enable ( ) del train , bureau , bureau_balance , bureau_agg , bureau_agg_new , bureau_balance_agg , bureau_balance_counts , bureau_by_loan , bureau_balance_by_client , bureau_counts gc . collect ( )
238	n = 19 commits_df . loc [ n , 'commit_num' ] = 23 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 184 commits_df . loc [ n , 'LB_score' ] = 0.25898
46	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( np . log ( 1 + train_df . target . values ) , bins = 100 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Log 1+Target' ) plt . show ( )
1528	plt . figure ( figsize = [ 10 , 6 ] ) df_train [ 'DBNOs' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of DBNOs" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'DBNOs' ] . value_counts ( ) )
283	n = 12 commits_df . loc [ n , 'commit_num' ] = 18 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8087
173	df . set_index ( pd . to_datetime ( df [ 'click_time' ] ) , inplace = True ) by_hour = df . resample ( 'H' ) . ip . count ( ) plt . figure ( figsize = ( 10 , 5 ) ) by_hour . plot ( ) plt . title ( 'Number of clicks over the day' , fontsize = 15 ) plt . xlabel ( 'Time' ) plt . ylabel ( 'Number of clicks' ) del by_hour gc . collect ( )
909	test = pd . read_csv ( '../input/application_test.csv' ) test = test . merge ( bureau_counts , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( bureau_agg , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( bureau_balance_by_client , on = 'SK_ID_CURR' , how = 'left' )
715	x = np . array ( range ( - 19 , 20 ) ) y = 2 * np . sin ( x ) plot_corrs ( x , y )
1081	def display_blurry_samples ( df , img_id_list , columns = 4 , rows = 3 ) : fig = plt . figure ( figsize = ( 5 * columns , 4 * rows ) ) for i in range ( columns * rows ) : img = preprocess_image ( f'../input/train_images/{img_id_list[i]}.png' ) fig . add_subplot ( rows , columns , i + 1 ) plt . title ( f'index:{i} isclear:{isClear(img)}' ) plt . imshow ( img ) plt . tight_layout ( ) display_blurry_samples ( train_df , blur_list_id )
207	data_tr = xgb . DMatrix ( Xtrain , label = Ztrain ) data_cv = xgb . DMatrix ( Xval , label = Zval ) evallist = [ ( data_tr , 'train' ) , ( data_cv , 'valid' ) ]
1250	start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_mixup ( images , labels , PROBABILITY = 1.0 ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_mixup: {timing}" )
1106	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak0_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak0_df [ 'meter_reading' ] = leak0_df . meter_reading_scraped leak0_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak0_df . fillna ( 0 , inplace = True ) leak0_df . loc [ leak0_df . meter_reading < 0 , 'meter_reading' ] = 0 leak0_df = leak0_df [ leak0_df . timestamp . dt . year > 2016 ] print ( len ( leak0_df ) )
615	def check_missing_values ( df ) : if df . isnull ( ) . any ( ) . any ( ) : print ( "There are missing values in the dataframe" ) else : print ( "There are no missing values in the dataframe" ) check_missing_values ( train_df ) check_missing_values ( test_df )
1425	stats = [ ] for country in sorted ( full_table [ 'Country/Region' ] . unique ( ) ) : df = get_time_series ( country ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 1000 ) : continue print ( '{} COVID-19 Prediction' . format ( country ) ) opt_display_model ( df , stats )
659	plt . figure ( figsize = ( 10 , 7 ) ) corr_0 = raw_train [ num_cols ] [ raw_train . target == 0 ] . corr ( ) sns . heatmap ( corr_0 , xticklabels = corr_0 . columns . values , yticklabels = corr_0 . columns . values )
494	from keras . models import Model from keras . layers import Input from keras . layers import Dense visible = Input ( shape = ( 2 , ) ) hidden = Dense ( 2 ) ( visible ) model = Model ( inputs = visible , outputs = hidden )
230	n = 11 commits_df . loc [ n , 'commit_num' ] = 13 commits_df . loc [ n , 'dropout_model' ] = 0.3 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25879
876	plt . figure ( figsize = ( 10 , 6 ) ) sns . kdeplot ( opt [ 'score' ] , label = 'Bayesian Opt' ) sns . kdeplot ( random [ 'score' ] , label = 'Random Search' ) plt . xlabel ( 'Score (5 Fold Validation ROC AUC)' ) ; plt . ylabel ( 'Density' ) ; plt . title ( 'Random Search and Bayesian Optimization Results' ) ;
1070	task_num = np . random . randint ( 1 , 400 ) arc = ARC_solver ( task_num ) arc . plot_task ( ) image = np . array ( train_tasks [ task_num ] [ 'train' ] [ 0 ] [ 'input' ] ) arc . identify_object ( image , method = 2 ) arc . plot_identified_objects ( arc . identified_objects )
819	hyperparameters = dict ( ** bayes_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS ) print ( 'The cross validation score on the full dataset for Bayesian optimization = {:.5f} with std: {:.5f}.' . format ( cv_results [ 'auc-mean' ] [ - 1 ] , cv_results [ 'auc-stdv' ] [ - 1 ] ) ) print ( 'Number of estimators = {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
1020	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
242	n = 23 commits_df . loc [ n , 'commit_num' ] = 27 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 64 commits_df . loc [ n , 'hidden_dim_second' ] = 124 commits_df . loc [ n , 'hidden_dim_third' ] = 108 commits_df . loc [ n , 'LB_score' ] = 0.25963
1412	y_log = np . log1p ( y ) y_categorized = pd . cut ( y_log , bins = range ( 0 , 25 , 3 ) , include_lowest = True , right = False , labels = range ( 0 , 24 , 3 ) )
1358	col = numeric_features [ 5 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1514	sns . set ( style = 'darkgrid' ) sns_plot = sns . palplot ( sns . color_palette ( 'Accent' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Accent_d' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'CMRmap' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Set1' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Set3' ) )
1502	df_train = pd . read_csv ( "../input/train.csv" ) df_test = pd . read_csv ( "../input/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
131	def clean_special_chars ( text ) : for s in specail_signs : text = text . replace ( s , specail_signs [ s ] ) for p in punct : text = text . replace ( p , f' {p} ' ) return text
856	import csv out_file = 'random_search_trials.csv' of_connection = open ( out_file , 'w' ) writer = csv . writer ( of_connection ) headers = [ 'score' , 'hyperparameters' , 'iteration' ] writer . writerow ( headers ) of_connection . close ( )
708	heads [ 'walls' ] = np . argmax ( np . array ( heads [ [ 'epared1' , 'epared2' , 'epared3' ] ] ) , axis = 1 ) plot_categoricals ( 'walls' , 'Target' , heads )
169	print ( DL_by_IP . describe ( ) , '\n Quantile 99% :' , DL_by_IP . quantile ( 0.99 ) , \ '\n Quantile 99,9% :' , DL_by_IP . quantile ( 0.999 ) , \ '\n Quantile 99,999% :' , DL_by_IP . quantile ( 0.9999 ) )
947	input_dir = '../input/' input_files = sorted ( glob . glob ( input_dir + '*' ) ) input_files
410	test_duplicated_mask = md5_df [ md5_df [ 'is_train' ] == False ] [ 'hash' ] . duplicated ( ) test_duplicates = md5_df [ ( md5_df [ 'is_train' ] == False ) & test_duplicated_mask ] test_duplicates [ 'hash' ] . unique ( ) , test_duplicates [ 'id' ] . values [ : 3 ] , len ( test_duplicates [ 'id' ] )
1545	df_train = pd . read_csv ( "../input/quora-insincere-questions-classification/train.csv" ) df_test = pd . read_csv ( "../input/quora-insincere-questions-classification/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
1124	def change ( addr ) : if addr == 60.0 : return 1 elif addr == 96.0 : return 1 elif addr == np . nan : return np . nan else : return 0 df [ "Europe" ] = df [ "addr2" ] . map ( change )
675	print ( 'Coefficient of variation (CV) for prices in different recognized image categories.' ) dfl = dff . merge ( all_image_labels , left_on = 'image' , right_index = True , how = 'left' ) dfd = dfl . groupby ( 'image_label' ) [ 'price' ] . apply ( lambda x : np . std ( x ) / np . mean ( x ) ) . sort_values ( ascending = False ) dfd . head ( 10 )
906	bureau_by_loan = bureau_balance_agg . merge ( bureau_balance_counts , right_index = True , left_on = 'SK_ID_BUREAU' , how = 'outer' ) bureau_by_loan = bureau_by_loan . merge ( bureau [ [ 'SK_ID_BUREAU' , 'SK_ID_CURR' ] ] , on = 'SK_ID_BUREAU' , how = 'left' ) bureau_by_loan . head ( )
418	best_n_clusters = 5 norm_feat = MinMaxScaler ( ) . fit_transform ( test_feat ) clt = KMeans ( n_clusters = best_n_clusters , random_state = RANDOM_SEED ) . fit ( norm_feat ) test_clusters = pd . DataFrame ( clt . labels_ , columns = [ 'cluster' ] , index = test_meta [ 'signal_id' ] ) stats_df = test_clusters . reset_index ( ) . groupby ( 'cluster' ) . count ( ) stats_df . columns = [ 'count' ] display ( stats_df )
559	images_with_ship = masks . ImageId [ masks . EncodedPixels . isnull ( ) == False ] images_with_ship = np . unique ( images_with_ship . values ) print ( 'There are ' + str ( len ( images_with_ship ) ) + ' image files with masks' )
793	valid_preds = random_forest . predict ( X_valid [ features ] ) plt . figure ( figsize = ( 10 , 6 ) ) sns . kdeplot ( y_valid , label = 'Actual' ) sns . kdeplot ( valid_preds , label = 'Predicted' ) plt . legend ( prop = { 'size' : 30 } ) plt . title ( "Distribution of Validation Fares" ) ;
1566	submission [ 'time_to_failure' ] = ( prediction_lgb + prediction_xgb ) / 2 print ( submission . head ( ) ) submission . to_csv ( 'submission.csv' )
129	train_df = train_df [ [ "target" , "comment_text" ] ] mem_usg = train_df . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( "Memory usage is: " , mem_usg , " MB" )
758	plt . figure ( figsize = ( 15 , 5 ) ) sns . countplot ( label [ 'surface' ] , order = label . surface . value_counts ( ) . index ) plt . show ( )
1475	import glob import cv2 from PIL import Image import pandas as pd import numpy as np
973	try : first_dicom [ 'Patient Name' ] except ValueError : print ( 'ValueError' )
786	plt . figure ( figsize = ( 10 , 8 ) ) for h , grouped in data . groupby ( 'pickup_Hour' ) : sns . kdeplot ( grouped [ 'fare_amount' ] , label = f'{h} hour' ) ; plt . title ( 'Fare Amount by Hour of Day' ) ;
465	tour_results = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MNCAATourneyDetailedResults.csv' ) season_results = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MRegularSeasonDetailedResults.csv' ) season_results . head ( )
466	def get_image_file_path ( image_file_name ) : return '../input/test/' + image_file_name def get_images ( n ) : all_image_files = os . listdir ( "../input/test/" ) image_paths = list ( map ( get_image_file_path , all_image_files ) ) image_paths = image_paths [ : n ] return image_paths def get_image_id_from_path ( image_path ) : return image_path . split ( '../input/test/' ) [ 1 ] . split ( '.jpg' ) [ 0 ]
186	train [ 'cat1' ] = train . category_name . str . extract ( '([^/]+)/[^/]+/[^/]+' ) train [ 'cat2' ] = train . category_name . str . extract ( '([^/]+/[^/]+)/[^/]+' ) plt . figure ( figsize = ( 10 , 12 ) ) train . name . groupby ( train . cat1 ) . count ( ) . plot ( kind = 'pie' ) plt . title ( 'First levels of categories' , fontsize = 20 ) plt . axis ( 'equal' ) plt . ylabel ( '' )
10	numeric_dtypes = [ 'float64' ] numerics = [ ] for i in train . columns : if train [ i ] . dtype in numeric_dtypes : numerics . append ( i )
1383	col = numeric_features [ 30 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1487	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 7 - Pleural Effusion" ) draw ( parsed [ df [ 'patientId' ] [ 125 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 125 ] ] )
402	ld = os . listdir ( TEST_DIR ) sizes = np . zeros ( len ( ld ) ) for i , f in enumerate ( ld ) : df = pd . read_csv ( os . path . join ( TEST_DIR , f ) ) sizes [ i ] = df . shape [ 0 ] print ( np . mean ( sizes ) ) print ( np . min ( sizes ) ) print ( np . max ( sizes ) ) print ( 'ok' )
568	from sklearn . feature_selection import VarianceThreshold y = train [ 'open_channels' ] . values X = train [ selected_columns ] . values vt = VarianceThreshold ( 0.5 ) vt . fit ( X , y ) top_idx = np . argpartition ( vt . variances_ , - 15 ) [ - 15 : ] SELECTED_FEATURES = [ selected_columns [ i ] for i in top_idx ] print ( SELECTED_FEATURES )
1	from sklearn . metrics import log_loss , roc_auc_score from datetime import datetime import datatable as dt from datatable . models import Ftrl
1549	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
1050	np . random . seed ( 1749 ) sample_files = np . random . choice ( os . listdir ( BASE_PATH + TRAIN_DIR ) , 4000 ) sample_df = train_df [ train_df . filename . apply ( lambda x : x . replace ( '.png' , '.dcm' ) ) . isin ( sample_files ) ]
1156	def seed_to_int ( seed ) : s_int = int ( seed [ 1 : 3 ] ) return ( s_int ) df_seeds [ 'seed_int' ] = df_seeds . Seed . apply ( seed_to_int ) df_seeds . drop ( labels = [ 'Seed' ] , inplace = True , axis = 1 )
726	corr_matrix = ind_agg . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] print ( f'There are {len(to_drop)} correlated columns to remove.' )
275	n = 4 commits_df . loc [ n , 'commit_num' ] = 9 commits_df . loc [ n , 'Dropout_model' ] = 0.35 commits_df . loc [ n , 'FVC_weight' ] = 0.3 commits_df . loc [ n , 'LB_score' ] = - 6.8125
1097	ds = pd . concat ( [ train , test ] ) ds [ 'sample_struc' ] = ds [ 'id' ] . apply ( sample_struc ) shape = ds . query ( 'structure == sample_struc' ) . shape
703	data . loc [ ( ( data [ 'age' ] > 19 ) | ( data [ 'age' ] < 7 ) ) & ( data [ 'rez_esc' ] . isnull ( ) ) , 'rez_esc' ] = 0 data [ 'rez_esc-missing' ] = data [ 'rez_esc' ] . isnull ( )
639	- - train_csv_path = . . / input / landmark - retrieval - 2020 / train . csv \ - - train_clean_csv_path = . . / input / train - clean - sample / train_clean_sample . csv \ - - train_directory = . . / input / landmark - retrieval - 2020 / train / * / * / * / \ - - output_directory = / tmp / data \ - - num_shards = 12 \ - - generate_train_validation_splits \ - - validation_split_size = 0.2
650	m , n = df . shape miss_count = [ ] for col in df . columns : x = df [ col ] . isnull ( ) . sum ( ) miss_count . append ( x ) miss_count_rate = np . array ( miss_count ) / m
835	previous = pd . read_csv ( '../input/previous_application.csv' ) . replace ( { 365243 : np . nan } ) previous = convert_types ( previous ) previous [ 'LOAN_RATE' ] = previous [ 'AMT_ANNUITY' ] / previous [ 'AMT_CREDIT' ] previous [ "AMT_DIFFERENCE" ] = previous [ 'AMT_CREDIT' ] - previous [ 'AMT_APPLICATION' ]
1117	def preprocess ( df ) : df [ "hour" ] = df [ "timestamp" ] . dt . hour df [ "weekend" ] = df [ "timestamp" ] . dt . weekday df [ "month" ] = df [ "timestamp" ] . dt . month df [ "dayofweek" ] = df [ "timestamp" ] . dt . dayofweek
72	n_train_rows , n_train_cols = train . shape n_test_rows , n_test_cols = test . shape print ( '- Training data has {:9,} rows and {:2,} columns.' . format ( * train . shape ) ) print ( '- Testing data has {:9,} rows and {:2,} columns.' . format ( * test . shape ) ) print ( '- There are {:.1f} times more ( ' testing data examples . ' . format ( n_train_rows / n_test_rows , n_train_rows - n_test_rows ) ) print ( "- There are %i missing values in the training data." % train . isnull ( ) . sum ( ) . sum ( ) ) print ( "- There are %i missing values in the testing data." % test . isnull ( ) . sum ( ) . sum ( ) )
1216	path_data = '../input' device = 'cuda' batch_size = 32 torch . manual_seed ( 0 )
1512	import seaborn as sns import numpy as np import pandas as pd from mpl_toolkits . mplot3d import Axes3D import matplotlib . pyplot as plt sns . set ( style = 'darkgrid' ) import os print ( os . listdir ( "../input" ) ) from pylab import rcParams rcParams [ 'figure.figsize' ] = 25 , 12.5 train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
1202	yhat = model . predict ( testX ) yhat_inverse = scaler . inverse_transform ( yhat . reshape ( - 1 , 1 ) ) testY_inverse = scaler . inverse_transform ( testY . reshape ( - 1 , 1 ) )
1177	filename = "/kaggle/input/osic-pulmonary-fibrosis-progression/train/ID00123637202217151272140/137.dcm" ds = pydicom . dcmread ( filename ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone )
163	concat_sub [ 'scalar_coupling_constant' ] = np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] > cutoff_lo , axis = 1 ) , concat_sub [ 'm_max' ] , np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] < cutoff_hi , axis = 1 ) , concat_sub [ 'm_min' ] , concat_sub [ 'm_mean' ] ) ) concat_sub [ [ 'id' , 'scalar_coupling_constant' ] ] . to_csv ( 'stack_minmax_mean.csv' , index = False , float_format = '%.6f' )
767	xs , ys = ecdf ( np . logspace ( 0 , 2 ) ) plt . plot ( xs , ys , '.' ) ; plt . ylabel ( 'Percentile' ) ; plt . title ( 'ECDF' ) ;
1014	def compute_game_time_stats ( group , col ) : return group [ [ 'installation_id' , col , 'event_count' , 'game_time' ] ] . groupby ( [ 'installation_id' , col ] ) . agg ( [ np . mean , np . sum , np . std ] ) . reset_index ( ) . pivot ( columns = col , index = 'installation_id' )
705	heads = data . loc [ data [ 'parentesco1' ] == 1 , : ] heads = heads [ id_ + hh_bool + hh_cont + hh_ordered ] heads . shape
1403	df [ 'MA_7MA' ] = df [ 'close' ] . rolling ( window = 7 ) . mean ( ) df [ 'MA_15MA' ] = df [ 'close' ] . rolling ( window = 15 ) . mean ( ) df [ 'MA_30MA' ] = df [ 'close' ] . rolling ( window = 30 ) . mean ( ) df [ 'MA_60MA' ] = df [ 'close' ] . rolling ( window = 60 ) . mean ( )
1043	public_df = test . query ( "seq_length == 107" ) . copy ( ) private_df = test . query ( "seq_length == 130" ) . copy ( ) public_inputs = preprocess_inputs ( public_df , token2int ) private_inputs = preprocess_inputs ( private_df , token2int )
451	plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) sns . kdeplot ( train [ 'dew_temperature' ] . dropna ( ) , shade = True , color = 'indigo' ) plt . xlabel ( 'Dew Temperature' , fontsize = 15 ) plt . ylabel ( 'Density' , fontsize = 15 ) plt . show ( )
185	mean_price_2 = pd . DataFrame ( group . price . mean ( ) ) mean_price_2 . reset_index ( level = 0 , inplace = True ) plt . figure ( figsize = ( 12 , 7 ) ) sns . kdeplot ( mean_price_2 . price , shade = True ) plt . title ( 'Mean price by category distribution' , fontsize = 20 ) plt . xlabel ( 'Mean price of each category' , fontsize = 16 )
229	n = 10 commits_df . loc [ n , 'commit_num' ] = 12 commits_df . loc [ n , 'dropout_model' ] = 0.35 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25884
641	import numpy as np import pandas as pd import time import lightgbm as lgb from sklearn . model_selection import StratifiedKFold , KFold from sklearn . metrics import mean_squared_error from sklearn . metrics import log_loss
1580	def find_all ( input_str , search_str ) : l1 = [ ] length = len ( input_str ) index = 0 while index < length : i = input_str . find ( search_str , index ) if i == - 1 : return l1 l1 . append ( i ) index = i + 1 return l1
111	X = train . Image . values del train [ 'Image' ] Y = train . values
927	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) result = test [ [ 'id' ] ] . copy ( ) print ( train . head ( 3 ) )
1021	with strategy . scope ( ) : transformer_layer = TFAutoModel . from_pretrained ( MODEL ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
218	dropout_model = 0.36 hidden_dim_first = 128 hidden_dim_second = 248 hidden_dim_third = 212
4	train = pd . read_csv ( "../input/train.csv" , parse_dates = [ "first_active_month" ] ) test = pd . read_csv ( "../input/test.csv" , parse_dates = [ "first_active_month" ] ) print ( "{} observations and {} features in train set." . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( "{} observations and {} features in test set." . format ( test . shape [ 0 ] , test . shape [ 1 ] ) )
977	import numpy as np SeriesUIDs = [ ] for file_dicom in os . listdir ( first_patient ) : SeriesUIDs . append ( dcmread ( first_patient + file_dicom ) [ 'SeriesInstanceUID' ] . value ) np . unique ( np . array ( SeriesUIDs ) )
612	batch_size = 256 num_epochs = 8 num_filters = 64 embed_dim = 300 weight_decay = 1e-4
604	np . random . seed ( 123 ) correct = 172560 index = np . random . choice ( range ( length ) , size = ( length - correct ) , replace = False ) submission286 = perfect_sub . copy ( ) spoiled_samples = np . random . rand ( length - correct ) submission286 [ index ] = spoiled_samples public , private = evaluate ( submission286 ) print ( f"Public score: {public:0.4f}\nPrivate score: {private:0.4f}" )
317	num_test_images = 57458 model . load_weights ( 'model.h5' ) predictions = model . predict_generator ( test_gen , steps = num_test_images , verbose = 1 )
1404	df [ 'close_26EMA' ] = ewma ( df [ "close" ] , span = 26 ) . mean ( ) df [ 'close_12EMA' ] = ewma ( df [ "close" ] , span = 12 ) . mean ( ) df [ 'MACD' ] = df [ 'close_12EMA' ] - df [ 'close_26EMA' ]
1402	import pandas as pd import numpy as np import time import lightgbm as lgb from sklearn . metrics import mean_squared_error from sklearn . model_selection import KFold import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'seaborn' ) sns . set ( font_scale = 2 ) import warnings warnings . filterwarnings ( 'ignore' ) import os
872	from featuretools import selection feature_matrix2 = selection . remove_low_information_features ( feature_matrix ) print ( 'Removed %d features' % ( feature_matrix . shape [ 1 ] - feature_matrix2 . shape [ 1 ] ) )
1244	data = pd . concat ( [ train [ 'Type' ] , train [ 'Weekly_Sales' ] ] , axis = 1 ) f , ax = plt . subplots ( figsize = ( 8 , 6 ) ) fig = sns . boxplot ( x = 'Type' , y = 'Weekly_Sales' , data = data , showfliers = False )
88	t = time . time ( ) for i in range ( 1000 ) : score_path ( ) print ( time . time ( ) - t )
57	print ( 'Total error' , np . sqrt ( mean_squared_error ( y , 0.7 * ( 0.75 * ( 1.4 * ( 1.6 * y_oof_7 - 0.6 * y_oof_6 ) - 0.4 * y_oof_5 ) + 0.25 * ( - 0.5 * y_oof - 0.5 * y_oof_2 - y_oof_3 + 3 * y_oof_4 ) ) + 0.3 * y_oof_8 ) ) )
1554	train = pd . read_csv ( "../input/train.csv" ) train . head ( )
54	test_nz = np . log ( test_df [ columns_to_use ] . values . flatten ( ) + 1 ) test_nz = test_nz [ np . nonzero ( test_nz ) ] plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( test_nz , bins = 50 ) plt . title ( 'Log Histogram nonzero test counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Log value' ) plt . show ( )
423	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yT , yR_pred ) print ( confusion )
1352	null_cols_to_remove = [ 'DefaultBrowsersIdentifier' , 'PuaMode' , 'Census_IsFlightingInternal' , 'Census_InternalBatteryType' ] train . drop ( null_cols_to_remove , axis = 1 , inplace = True ) test . drop ( null_cols_to_remove , axis = 1 , inplace = True )
983	X_test = Parallel ( n_jobs = - 3 , verbose = 1 ) ( delayed ( processor . prepareSample ) ( test_root , sample_submission . iloc [ f , : ] , processor . createMel , CONFIG , TRAINING_CONFIG , test_mode = True , proc_mode = 'resize' , ) for f in range ( 100 ) ) X_test = np . array ( X_test ) print ( X_test . shape )
126	plt . hist ( imgs . flatten ( ) , bins = 50 , color = 'c' ) plt . xlabel ( "Hounsfield Units (HU)" ) plt . ylabel ( "Frequency" ) plt . show ( )
315	shutil . rmtree ( 'base_dir' )
239	n = 20 commits_df . loc [ n , 'commit_num' ] = 24 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 208 commits_df . loc [ n , 'LB_score' ] = 0.25868
1221	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) sample_submission = pd . read_csv ( '../input/sample_submission.csv' )
214	es = ft . EntitySet ( id = 'ashrae_energy_data' ) es = es . entity_from_dataframe ( entity_id = 'dfe' , dataframe = dfe , variable_types = { 'air_temperature' : ft . variable_types . Numeric , 'cloud_coverage' : ft . variable_types . Numeric , 'dew_temperature' : ft . variable_types . Numeric , 'precip_depth_1_hr' : ft . variable_types . Numeric } , index = 'index' )
1195	print ( "Number of records where toxicity_annotator_count is 1: {}" . format ( len ( j_df [ ( j_df [ 'toxicity_annotator_count' ] == 1 ) ] ) ) ) print ( "Most common print(j_df['toxicity_annotator_count'].value_counts().head()) print(" Most annotators : " , max ( j_df [ 'toxicity_annotator_count' ] ) )
254	country_name = "Albania" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
654	df_ = df [ df [ 'timestamp' ] < 100 ] X2 = df_ . drop ( [ 'y' , 'id' , 'timestamp' ] , axis = 1 ) y2 = df_ [ 'y' ] rf4 = RandomForestRegressor ( ) rf4 . fit ( X2 , y2 ) rf4 . score ( X2 , y2 )
520	logloss_logreg = cross_val_score ( clf_logreg , X_train , y_train , cv = cv , scoring = 'neg_log_loss' ) . mean ( ) logloss_rfc = cross_val_score ( clf_rfc , X_train , y_train , cv = cv , scoring = 'neg_log_loss' ) . mean ( ) from sklearn . calibration import CalibratedClassifierCV new_clf_SGD = CalibratedClassifierCV ( clf_SGD ) new_clf_SGD . fit ( X_train , y_train ) logloss_SGD = cross_val_score ( new_clf_SGD , X_train , y_train , cv = cv , scoring = 'neg_log_loss' ) . mean ( ) logloss_logreg , logloss_SGD , logloss_rfc
1448	train [ 'click_time' ] = pd . to_datetime ( train [ 'click_time' ] ) train [ 'attributed_time' ] = pd . to_datetime ( train [ 'attributed_time' ] ) test [ 'click_time' ] = pd . to_datetime ( test [ 'click_time' ] ) train [ 'is_attributed' ] = train [ 'is_attributed' ] . astype ( 'category' )
541	TARGET_SR = 32_000 PP_NORMALIZE = True MIN_SEC = 1 DEVICE = 'cuda' BATCH_SIZE = 64 SIGMOID_THRESH = 0.3 MAX_BIRDS = None
1087	import sys , os , multiprocessing , csv from urllib import request , error from PIL import Image from io import BytesIO
243	n = 24 commits_df . loc [ n , 'commit_num' ] = 28 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 108 commits_df . loc [ n , 'LB_score' ] = 0.25860
902	new_corrs = [ ] for col in columns : corr = train [ 'TARGET' ] . corr ( train [ col ] ) new_corrs . append ( ( col , corr ) )
1498	program = build_model ( task [ 'train' ] , verbose = True ) print ( ) if program is None : print ( "No program was found" ) else : print ( "Found program:" , program_desc ( program ) )
1504	x_train = np . load ( "x_train.npy" ) x_test = np . load ( "x_test.npy" ) y_train = np . load ( "y_train.npy" ) features = np . load ( "features.npy" ) test_features = np . load ( "test_features.npy" ) word_index = np . load ( "word_index.npy" ) . item ( )
1560	sentence = [ "I study Math" , "I enjoy doing Math" ] vectorizer = CountVectorizer ( min_df = 0 ) sentence_transform = vectorizer . fit_transform ( sentence )
581	df_grouped_spain = get_df_country_cases ( df_covid , "Spain" ) df_spain_cases_by_day = df_grouped_spain [ df_grouped_spain . confirmed > 0 ] df_spain_cases_by_day = df_spain_cases_by_day . reset_index ( drop = True ) df_spain_cases_by_day [ 'day' ] = df_spain_cases_by_day . date . apply ( lambda x : ( x - df_spain_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_spain_cases_by_day = df_spain_cases_by_day [ reordered_columns ] df_spain_cases_by_day
1236	xgb_lv3_params = { "booster" : "gbtree" , "objective" : "binary:logistic" , "tree_method" : "hist" , "eval_metric" : "auc" , "eta" : 0.1 , "max_depth" : 2 , "min_child_weight" : 10 , "gamma" : 0.70 , "subsample" : 0.76 , "colsample_bytree" : 0.95 , "nthread" : 6 , "seed" : 0 , 'silent' : 1 } xgb_lv3_outcomes = cross_validate_xgb ( xgb_lv3_params , lv2_train , y_train , lv2_test , kf , verbose = True , verbose_eval = False , use_rank = True ) xgb_lv3_cv = xgb_lv3_outcomes [ 0 ] xgb_lv3_train_pred = xgb_lv3_outcomes [ 1 ] xgb_lv3_test_pred = xgb_lv3_outcomes [ 2 ]
809	MAX_EVALS = 10 best = fmin ( fn = objective , space = space , algo = tpe . suggest , trials = trials , max_evals = MAX_EVALS )
306	MAX_LEN = 96 PATH = '../input/tf-roberta/' tokenizer = tokenizers . ByteLevelBPETokenizer ( vocab_file = PATH + 'vocab-roberta-base.json' , merges_file = PATH + 'merges-roberta-base.txt' , lowercase = True , add_prefix_space = True ) EPOCHS = 3 BATCH_SIZE = 32 PAD_ID = 1 SEED = 88888 LABEL_SMOOTHING = 0.1 tf . random . set_seed ( SEED ) np . random . seed ( SEED ) sentiment_id = { 'positive' : 1313 , 'negative' : 2430 , 'neutral' : 7974 } train = pd . read_csv ( '../input/tweet-sentiment-extraction/train.csv' ) . fillna ( '' ) train . head ( )
1242	print ( stores . head ( ) ) grouped = stores . groupby ( 'Type' ) print ( grouped . describe ( ) [ 'Size' ] . round ( 2 ) )
987	workdir = '/kaggle/working/patients/' os . makedirs ( workdir , exist_ok = True ) datadir = "/kaggle/input/osic-pulmonary-fibrosis-progression/train/" patients = os . listdir ( datadir ) patients . sort ( ) patient = patients [ 17 ] reader = vtk . vtkDICOMImageReader ( ) reader . SetDirectoryName ( datadir + patient ) reader . Update ( )
1079	fig = plt . figure ( figsize = ( 8 , 8 ) ) image_path = train_df . loc [ 8 , 'id_code' ] image_id = train_df . loc [ 8 , 'diagnosis' ] img = preprocess_image ( f'../input/train_images/{image_path}.png' ) plt . title ( f'diagnosis:{image_id} index:{8}' ) plt . imshow ( img ) plt . tight_layout ( )
210	feature_score = pd . DataFrame ( preprocessing . MinMaxScaler ( ) . fit_transform ( feature_score ) , columns = feature_score . columns , index = feature_score . index ) feature_score [ 'mean' ] = feature_score . mean ( axis = 1 ) feature_score . sort_values ( 'mean' , ascending = False ) . plot ( kind = 'bar' , figsize = ( 20 , 10 ) )
89	tokenizer = RegexpTokenizer ( r'\w+' ) clean_train_comments = pd . read_csv ( "train_clean_data.csv" ) clean_train_comments [ 'comment_text' ] = clean_train_comments [ 'comment_text' ] . astype ( 'str' ) clean_train_comments . dtypes clean_train_comments [ "tokens" ] = clean_train_comments [ "comment_text" ] . apply ( tokenizer . tokenize ) clean_train_comments [ "tokens" ] = clean_train_comments [ "tokens" ] . apply ( lambda vec : [ word for word in vec if word not in stop_words ] ) clean_train_comments . head ( )
128	def hist_analysis ( segmented , display = False ) : values = segmented . flatten ( ) values = values [ values >= - 1000 ] if display : plt . hist ( values , bins = 50 ) summary_statistics = describe ( values ) return summary_statistics
462	from sklearn . preprocessing import StandardScaler scaler = StandardScaler ( ) lat_long = [ 'Latitude' , 'Longitude' ] for col in lat_long : df_train [ col ] = ( scaler . fit_transform ( df_train [ col ] . values . reshape ( - 1 , 1 ) ) ) df_test [ col ] = ( scaler . fit_transform ( df_test [ col ] . values . reshape ( - 1 , 1 ) ) )
1163	label_map = dict ( labels [ [ 'attribute_id' , 'attribute_name' ] ] . values . tolist ( ) ) not_in_train_labels = set ( labels [ 'attribute_id' ] . astype ( str ) . values ) - set ( list ( cls_counts ) ) for _id in not_in_train_labels : label = label_map [ int ( _id ) ] print ( f'attribute_id: {_id} attribute_name: {label}' )
1400	col = numeric_features [ 49 ] plot_category_percent_of_target_for_numeric ( col )
1092	feature_importance = pd . DataFrame ( columns = [ 'feature' , 'importance' ] ) feature_importance . feature = X . columns . values feature_importance . importance = model . feature_importance ( ) feature_importance . sort_values ( by = 'importance' , ascending = False , inplace = True ) plt . figure ( figsize = ( 10 , 50 ) ) sns . barplot ( 'importance' , 'feature' , data = feature_importance )
1574	from fbprophet import Prophet sns . set ( font_scale = 1 ) df_date_index = times_series_means [ [ 'date' , 'Visits' ] ] df_date_index = df_date_index . set_index ( 'date' ) df_prophet = df_date_index . copy ( ) df_prophet . reset_index ( drop = False , inplace = True ) df_prophet . columns = [ 'ds' , 'y' ] m = Prophet ( ) m . fit ( df_prophet ) future = m . make_future_dataframe ( periods = 30 , freq = 'D' ) forecast = m . predict ( future ) fig = m . plot ( forecast )
1239	print ( "the structure of train data is " , train . shape ) print ( "the structure of test data is " , test . shape ) print ( "the ratio of train data : test data is " , ( round ( train . shape [ 0 ] * 100 / ( train . shape [ 0 ] + test . shape [ 0 ] ) ) , 100 - round ( train . shape [ 0 ] * 100 / ( train . shape [ 0 ] + test . shape [ 0 ] ) ) ) )
1034	sample_submission_df = pd . read_csv ( '../input/sample_submission.csv' ) image_ids = sample_submission_df [ 'ImageId' ] predictions = [ ] for image_id in tqdm ( image_ids ) : image_path = f'../input/test/{image_id}.jpg' with tf . gfile . Open ( image_path , "rb" ) as binfile : image_string = binfile . read ( ) result_out = sess . run ( detector_output , feed_dict = { image_string_placeholder : image_string } ) predictions . append ( format_prediction_string ( image_id , result_out ) ) sess . close ( )
811	bayes_results = pd . read_csv ( '../input/home-credit-model-tuning/bayesian_trials_1000.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) random_results = pd . read_csv ( '../input/home-credit-model-tuning/random_search_trials_1000.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) random_results [ 'loss' ] = 1 - random_results [ 'score' ] bayes_params = evaluate ( bayes_results , name = 'Bayesian' ) random_params = evaluate ( random_results , name = 'random' )
905	def count_categorical ( df , group_var , df_name ) : categorical = pd . get_dummies ( df . select_dtypes ( 'object' ) ) categorical [ group_var ] = df [ group_var ] categorical = categorical . groupby ( group_var ) . agg ( [ 'sum' , 'mean' ] ) column_names = [ ] for var in categorical . columns . levels [ 0 ] : for stat in [ 'count' , 'count_norm' ] : column_names . append ( '%s_%s_%s' % ( df_name , var , stat ) ) categorical . columns = column_names return categorical
772	test = pd . read_csv ( '../input/test.csv' , parse_dates = [ 'pickup_datetime' ] ) test [ 'abs_lat_diff' ] = ( test [ 'dropoff_latitude' ] - test [ 'pickup_latitude' ] ) . abs ( ) test [ 'abs_lon_diff' ] = ( test [ 'dropoff_longitude' ] - test [ 'pickup_longitude' ] ) . abs ( ) test_id = list ( test . pop ( 'key' ) ) test . describe ( )
889	bureau [ 'bureau_credit_application_date' ] = start_date + bureau [ 'DAYS_CREDIT' ] bureau [ 'bureau_credit_end_date' ] = start_date + bureau [ 'DAYS_CREDIT_ENDDATE' ] bureau [ 'bureau_credit_close_date' ] = start_date + bureau [ 'DAYS_ENDDATE_FACT' ] bureau [ 'bureau_credit_update_date' ] = start_date + bureau [ 'DAYS_CREDIT_UPDATE' ]
1459	pos_train = df_train . query ( 'sentiment=="positive"' ) neg_train = df_train . query ( 'sentiment=="negative"' ) neu_train = df_train . query ( 'sentiment=="neutral"' ) pos_val = df_train . query ( 'sentiment=="positive"' ) neg_val = df_train . query ( 'sentiment=="negative"' ) neu_val = df_train . query ( 'sentiment=="neutral"' ) pos_test = df_test . query ( 'sentiment=="positive"' ) neg_test = df_test . query ( 'sentiment=="negative"' ) neu_test = df_test . query ( 'sentiment=="neutral"' )
1096	all_var = np . array ( train . query ( 'SN_filter == 1' ) [ err_cols ] . applymap ( lambda c : np . array ( c ) ** 2 ) . values . tolist ( ) ) mse = all_var . mean ( 2 ) . mean ( 0 ) mcrmse = np . sqrt ( mse ) . mean ( ) mcrmse
155	clear_output ( wait = True ) print ( 'Done!' )
1141	import torch from effdet import get_efficientdet_config , EfficientDet , DetBenchTrain from effdet . efficientdet import HeadNet def get_train_efficientdet ( ) : config = get_efficientdet_config ( 'tf_efficientdet_d5' ) net = EfficientDet ( config , pretrained_backbone = False ) checkpoint = torch . load ( '../input/efficientdet/efficientdet_d5-ef44aea8.pth' ) net . load_state_dict ( checkpoint ) config . num_classes = 1 config . image_size = IMG_SIZE net . class_net = HeadNet ( config , num_outputs = config . num_classes , norm_kwargs = dict ( eps = .001 , momentum = .01 ) ) return DetBenchTrain ( net , config )
108	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Device:' , tpu . master ( ) ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) except : strategy = tf . distribute . get_strategy ( ) print ( 'Number of replicas:' , strategy . num_replicas_in_sync )
972	from pydicom import dcmread import os dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/train/' first_patient = dir + os . listdir ( dir ) [ 0 ] + '/' first_dicom = dcmread ( first_patient + os . listdir ( first_patient ) [ 0 ] ) first_dicom
81	def get_mix ( x ) : x = str ( x ) if x . find ( 'Mix' ) >= 0 : return 'mix' return 'not' animals [ 'Mix' ] = animals . Breed . apply ( get_mix ) sns . countplot ( animals . Mix , palette = 'Set3' )
1372	col = numeric_features [ 19 ] plot_category_percent_of_target_for_numeric ( col )
1127	model = LGBMClassifier ( ** params ) . fit ( X , y , categorical_feature = [ 'PdDistrict' ] ) pdp_Pd = pdp . pdp_isolate ( model = model , dataset = X , model_features = X . columns . tolist ( ) , feature = 'Hour' , n_jobs = - 1 ) pdp . pdp_plot ( pdp_Pd , 'Hour' , ncols = 3 ) plt . show ( )
1297	plt . figure ( figsize = ( 12 , 6 ) ) sns . countplot ( submission [ "diagnosis" ] ) plt . title ( "Number of data per each diagnosis" ) plt . show ( )
1215	test_filenames = sorted ( glob ( f"{data_dir}/Test/*.jpg" ) ) test_df = pd . DataFrame ( { 'ImageFileName' : list ( test_filenames ) } , columns = [ 'ImageFileName' ] ) batch_size = 16 num_workers = 4 test_dataset = Alaska2Dataset ( test_df , augmentations = AUGMENTATIONS_TEST , test = True ) test_loader = torch . utils . data . DataLoader ( test_dataset , batch_size = batch_size , num_workers = num_workers , shuffle = False , drop_last = False )
578	df_grouped_italy = get_df_country_cases ( df_covid , "Italy" ) df_grouped_italy
250	all_data [ all_data [ 'Country_Region' ] == 'Spain' ] . iloc [ 40 : 50 ] [ [ 'Id' , 'Province_State' , 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' , 'ForecastId' , 'Day_num' , 'ConfirmedCases_1' , 'ConfirmedCases_2' , 'ConfirmedCases_3' , 'Fatalities_1' , 'Fatalities_2' , 'Fatalities_3' ] ]
1145	from fastai . vision import open_mask_rle mask_shape = ( img . px . shape [ 1 ] , img . px . shape [ 2 ] ) mask = open_mask_rle ( mask_rle , shape = mask_shape ) mask
337	etr = ExtraTreesRegressor ( ) etr . fit ( train , target ) acc_model ( 12 , etr , train , test )
605	tries , fix = 30 , 0 found = False np . random . seed ( 10 ) while not found : fix += 5 print ( f"Fixing {fix} samples" ) for t in range ( tries ) : new_submission = submission286 . copy ( ) improve_index = np . random . choice ( index , size = fix , replace = False ) new_submission [ improve_index ] = perfect_sub [ improve_index ] public , _ = evaluate ( new_submission ) if public >= 0.287 : print ( "0.287 reached!" ) found = True break
36	oof_c = pd . read_csv ( '../input/triple-stratified-kfold-with-tfrecords/oof.csv' ) submission_c = pd . read_csv ( '../input/triple-stratified-kfold-with-tfrecords/submission.csv' ) oof_c . head ( )
865	default_agg_primitives = [ "sum" , "std" , "max" , "skew" , "min" , "mean" , "count" , "percent_true" , "num_unique" , "mode" ] default_trans_primitives = [ "day" , "year" , "month" , "weekday" , "haversine" , "numwords" , "characters" ] feature_names = ft . dfs ( entityset = es , target_entity = 'app' , trans_primitives = default_trans_primitives , agg_primitives = default_agg_primitives , max_depth = 2 , features_only = True ) print ( '%d Total Features' % len ( feature_names ) )
1102	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak0_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak0_df [ 'meter_reading' ] = leak0_df . meter_reading_scraped leak0_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak0_df . fillna ( 0 , inplace = True ) leak0_df . loc [ leak0_df . meter_reading < 0 , 'meter_reading' ] = 0 leak0_df = leak0_df [ leak0_df . timestamp . dt . year > 2016 ] print ( len ( leak0_df ) )
42	def spearman_corr ( y_true , y_pred ) : if np . ndim ( y_pred ) == 2 : corr = np . mean ( [ stats . spearmanr ( y_true [ : , i ] , y_pred [ : , i ] ) [ 0 ] for i in range ( y_true . shape [ 1 ] ) ] ) else : corr = stats . spearmanr ( y_true , y_pred ) [ 0 ] return corr
262	random_forest = GridSearchCV ( estimator = RandomForestRegressor ( ) , param_grid = { 'n_estimators' : [ 100 , 1000 ] } , cv = 5 ) random_forest . fit ( train , target ) print ( random_forest . best_params_ ) acc_model ( 6 , random_forest , train , test )
328	svr = SVR ( ) svr . fit ( train , target ) acc_model ( 1 , svr , train , test )
478	import pandas as pd import numpy as np from sklearn . model_selection import StratifiedKFold import lightgbm as lgb from sklearn import metrics import gc pd . set_option ( 'display.max_columns' , 200 )
1137	train_augmenter = ImageDataGenerator ( rescale = 1. / 255 , horizontal_flip = True , width_shift_range = 0.1 , rotation_range = 45 , fill_mode = 'nearest' , zoom_range = 0.10 , ) test_augmenter = ImageDataGenerator ( rescale = 1. / 255 )
753	from sklearn . tree import export_graphviz export_graphviz ( estimator_limited , out_file = 'tree_limited.dot' , feature_names = train_selected . columns , class_names = [ 'extreme' , 'moderate' , 'vulnerable' , 'non-vulnerable' ] , rounded = True , proportion = False , precision = 2 , filled = True )
1172	total_tokens_l = [ ] for s in clean_lower : total_tokens_l . extend ( s ) unk_tokens_l = list ( set ( total_tokens_l ) ) print ( "[!]Total number of tokens:" , len ( total_tokens_l ) ) print ( "[!]Total number of unique tokens:" , len ( unk_tokens_l ) )
257	linreg = LinearRegression ( ) linreg . fit ( train , target ) acc_model ( 0 , linreg , train , test )
1341	temp_col = features_dtype_object [ 14 ] plot_count_percent_for_object ( application_train , temp_col ) plot_count_percent_for_object ( application_object_na_filled , temp_col )
686	i = test [ test [ 'key_id' ] == 9000052667981386 ] . iloc [ 0 ] [ 'drawing' ] img = draw_cv2 ( ast . literal_eval ( i ) , img_size = 256 ) plt . imshow ( img )
771	plt . figure ( figsize = ( 10 , 6 ) ) for p , grouped in data . groupby ( 'passenger_count' ) : sns . kdeplot ( grouped [ 'fare_amount' ] , label = f'{p} passengers' , color = list ( grouped [ 'color' ] ) [ 0 ] ) ; plt . xlabel ( 'Fare Amount' ) ; plt . ylabel ( 'Density' ) plt . title ( 'Distribution of Fare Amount by Number of Passengers' ) ;
1265	decay_var_list = [ ] for i in range ( len ( bert_nq . trainable_variables ) ) : name = bert_nq . trainable_variables [ i ] . name if any ( x in name for x in [ "LayerNorm" , "layer_norm" , "bias" ] ) : decay_var_list . append ( name ) decay_var_list
122	fig = px . line ( train_df , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'Sex' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines+markers' )
193	train [ 'coms_length' ] = train [ 'item_description' ] . str . len ( ) pd . options . display . float_format = '{:.2f}' . format train [ 'coms_length' ] . describe ( )
1484	patientId = df [ 'patientId' ] [ 2 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 3 - Lung Nodules and Masses" ) draw ( parsed [ patientId ] )
327	linreg = LinearRegression ( ) linreg . fit ( train , target ) acc_model ( 0 , linreg , train , test )
822	train_labels = train_bureau [ 'TARGET' ] previous_features . append ( 'SK_ID_CURR' ) train_ids = train_bureau [ 'SK_ID_CURR' ] test_ids = test_bureau [ 'SK_ID_CURR' ] train = train_bureau . merge ( train_previous [ previous_features ] , on = 'SK_ID_CURR' ) test = test_bureau . merge ( test_previous [ previous_features ] , on = 'SK_ID_CURR' ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
140	for feat in tqdm ( features ) : lbl_enc = preprocessing . LabelEncoder ( ) all_df [ feat ] = lbl_enc . fit_transform ( all_df [ feat ] . \ fillna ( '-1' ) . \ astype ( str ) . values ) all_df [ 'target' ] = all_df [ 'target' ] . fillna ( - 1 ) all_df [ continuous ] = all_df [ continuous ] . fillna ( - 2 )
1274	key = 'SK_ID_CURR' bureau_cols = [ 'DAYS_CREDIT' , 'DAYS_CREDIT_ENDDATE' , 'DAYS_ENDDATE_FACT' ] bureau_cols_max = [ 'BUREAU_MAX_' + c for c in bureau_cols ] df = pd . merge ( left = df , right = df_bureau [ [ key ] + bureau_cols ] . groupby ( key ) . max ( ) . rename ( columns = dict ( zip ( bureau_cols , bureau_cols_max ) ) ) , left_on = key , right_index = True , how = 'left' ) df [ [ key ] + bureau_cols_max ] . sample ( 3 )
276	n = 5 commits_df . loc [ n , 'commit_num' ] = 10 commits_df . loc [ n , 'Dropout_model' ] = 0.36 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8089
161	sub_path = "../input/ieee-blend" all_files = os . listdir ( sub_path ) all_files
366	def compute_histogram ( img , hist_size = 100 ) : hist = cv2 . calcHist ( [ img ] , [ 0 ] , mask = None , histSize = [ hist_size ] , ranges = ( 0 , 255 ) ) hist = cv2 . normalize ( hist , dst = hist ) return hist
537	y , sr = librosa . load ( "/kaggle/input/birdsong-recognition/train_audio/purfin/XC195200.mp3" ) pitches , magnitudes = librosa . piptrack ( y = y , sr = sr ) print ( pitches )
1543	plt . figure ( figsize = ( 20 , 5 ) ) plt . plot ( - plot1 . compute ( ) , plot2 . compute ( ) ) ; plt . xlabel ( "- Quaketime" ) plt . ylabel ( "Signal" ) plt . title ( "PLOT 0" ) ;
1440	dtypes = { 'ip' : 'uint32' , 'app' : 'uint16' , 'device' : 'uint16' , 'os' : 'uint16' , 'channel' : 'uint16' , 'is_attributed' : 'uint8' , } train = pd . read_csv ( '../input/train_sample.csv' , dtype = dtypes ) train . info ( )
379	Ada_Boost = AdaBoostRegressor ( ) Ada_Boost . fit ( train , target ) acc_model ( 13 , Ada_Boost , train , test )
456	bold ( '**Preview of Train Data:**' ) display ( df_train . head ( ) ) bold ( '**Preview of Test Data:**' ) display ( df_test . head ( ) )
1048	def build_new_df ( df , extension = 'jpeg' ) : new_df = pd . concat ( [ df , df ] ) new_df [ 'filename' ] = pd . concat ( [ df [ 'id_code' ] . apply ( lambda string : string + f'_s1.{extension}' ) , df [ 'id_code' ] . apply ( lambda string : string + f'_s2.{extension}' ) ] ) return new_df new_train = build_new_df ( train_df ) new_test = build_new_df ( test_df ) new_train . to_csv ( 'new_train.csv' , index = False ) new_test . to_csv ( 'new_test.csv' , index = False )
409	train_duplicated_mask = md5_df [ md5_df [ 'is_train' ] == True ] [ 'hash' ] . duplicated ( ) train_duplicates = md5_df [ ( md5_df [ 'is_train' ] == True ) & train_duplicated_mask ] train_duplicates [ 'hash' ] . unique ( ) , train_duplicates [ 'id' ] . values [ : 3 ] , len ( train_duplicates [ 'id' ] )
109	def augmentation_pipeline ( image , label ) : image = tf . image . random_flip_left_right ( image ) return image , label
115	print ( "*" * 30 , "store_id" , "*" * 30 ) print ( "store_id unique value counts:{}" . format ( len ( price [ "store_id" ] . unique ( ) ) ) ) print ( price [ "store_id" ] . unique ( ) ) print ( "*" * 30 , "item_id" , "*" * 30 ) print ( "item_id unique value counts:{}" . format ( len ( price [ "item_id" ] . unique ( ) ) ) ) print ( price [ "item_id" ] . unique ( ) )
1501	def seed_everything ( seed = 1029 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( )
652	q_high = df . y . quantile ( 0.75 ) q_low = df . y . quantile ( 0.25 ) iqr = ( q_high - q_low ) * 1.5 high = q_high + iqr low = q_low - iqr df = df . drop ( df [ df . y > high ] . index ) df = df . drop ( df [ df . y < low ] . index )
574	df_covid [ 'country' ] = df_covid [ 'country' ] . replace ( 'Mainland China' , 'China' ) df_covid
885	train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test_ids = list ( test [ 'SK_ID_CURR' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] )
824	threshold = 0.9 corr_matrix = train . corr ( ) . abs ( ) corr_matrix . head ( )
0	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train_df . Target . values , bins = 4 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
548	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "bathroomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Bathroom Count ",title=" Bathroom Count Vs Log Error " )
1111	category_cols = [ 'building_id' , 'site_id' , 'primary_use' ] feature_cols = [ 'square_feet' , 'year_built' ] + [ 'hour' , 'weekend' , 'building_median' ] + [ 'air_temperature' , 'cloud_coverage' , 'dew_temperature' , 'precip_depth_1_hr' , 'sea_level_pressure' , 'wind_direction' , 'wind_speed' , ]
23	train_text = train [ 'question_text' ] test_text = test [ 'question_text' ] all_text = pd . concat ( [ train_text , test_text ] ) word_vectorizer = TfidfVectorizer ( sublinear_tf = True , strip_accents = 'unicode' , analyzer = 'word' , token_pattern = r'\w{1,}' , stop_words = 'english' , ngram_range = ( 1 , 1 ) , max_features = 5000 ) word_vectorizer . fit ( all_text ) train_word_features = word_vectorizer . transform ( train_text ) test_word_features = word_vectorizer . transform ( test_text )
1363	col = numeric_features [ 10 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1281	def extract_series ( df , row_num , start_idx ) : y = df . iloc [ row_num , start_idx : ] df = pd . DataFrame ( { 'ds' : y . index , 'y' : y . values } ) return df
991	ren . AddActor ( cylinderActor ) ren . SetBackground ( colors . GetColor3d ( "BkgColor" ) ) ren . ResetCamera ( ) ren . GetActiveCamera ( ) . Zoom ( 1.5 )
529	model_conv = Model ( inputs = input_img , outputs = out ) model_conv . compile ( optimizer = 'Adam' , loss = logloss , metrics = [ weighted_loss ] ) model_conv . summary ( ) history_conv = model_conv . fit_generator ( dataGenerator , steps_per_epoch = 500 , epochs = 20 , validation_data = ( x_val , y_val ) , verbose = True )
1171	clean_lower = [ ] for i in tqdm ( range ( len ( total_ ) ) ) : clean_lower . append ( sentence_to_wordlist ( total_ [ i ] ) )
1294	convert_path = './convert_dir' if not os . path . isdir ( convert_path ) : os . mkdir ( convert_path ) else : pass for f in os . listdir ( sample_path ) : if f [ - 3 : ] == 'dcm' : ds = pydicom . read_file ( sample_path + '/' + f ) img = ds . pixel_array cv2 . imwrite ( convert_path + '/' + f . replace ( '.dcm' , '.png' ) , img ) os . listdir ( convert_path )
761	folds = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 20 ) predicted = np . zeros ( ( test . shape [ 0 ] , 9 ) ) measured = np . zeros ( ( train . shape [ 0 ] ) ) score = 0
1187	N = test_df . shape [ 0 ] x_test = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( test_df [ 'Patient' ] ) ) : x_test [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
1342	temp_col = features_dtype_object [ 15 ] plot_count_percent_for_object ( application_train , temp_col ) plot_count_percent_for_object ( application_object_na_filled , temp_col )
1529	plt . figure ( figsize = [ 10 , 6 ] ) df_train [ 'headshotKills' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of headshotKills" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'headshotKills' ] . value_counts ( ) )
244	n = 25 commits_df . loc [ n , 'commit_num' ] = 29 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 244 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25846
1555	all_words = train [ 'text' ] . str . split ( expand = True ) . unstack ( ) . value_counts ( ) all_words . head ( )
990	cylinderActor = vtk . vtkActor ( ) cylinderActor . SetMapper ( cylinderMapper ) cylinderActor . GetProperty ( ) . SetColor ( colors . GetColor3d ( "Tomato" ) ) cylinderActor . RotateX ( 30.0 ) cylinderActor . RotateY ( - 45.0 )
265	bagging = BaggingRegressor ( ) bagging . fit ( train , target ) acc_model ( 11 , bagging , train , test )
305	EPOCHS = EPOCHS_wn NNBATCHSIZE = 16 GROUP_BATCH_SIZE = 4000 SEED = 321 LR = lr_wn
1272	TARGET_MIN_COUNTING = 100 def get_num_of_repetition_for_class ( class_id ) : counting = label_counter [ class_id ] if counting >= TARGET_MIN_COUNTING : return 1.0 num_to_repeat = TARGET_MIN_COUNTING / counting return num_to_repeat numbers_of_repetition_for_classes = { class_id : get_num_of_repetition_for_class ( class_id ) for class_id in range ( 104 ) } print ( "number of repetitions for each class (if > 1)" ) { k : v for k , v in sorted ( numbers_of_repetition_for_classes . items ( ) , key = lambda item : item [ 1 ] , reverse = True ) if v > 1 }
1467	train_sales [ 'total_sales' ] = train_sales . sum ( axis = 1 ) sns . catplot ( x = "cat_id" , y = "total_sales" , hue = "state_id" , data = train_sales , kind = "bar" , height = 8 , aspect = 1 ) ;
595	neutral_train [ 'temp_list' ] = neutral_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) neutral_train [ 'temp_list' ] = neutral_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) neutral_top = Counter ( [ item for sublist in neutral_train [ 'temp_list' ] for item in sublist ] ) neutral_temp = pd . DataFrame ( neutral_top . most_common ( 20 ) ) neutral_temp . columns = [ 'Common_words' , 'count' ] neutral_temp . style . background_gradient ( cmap = 'Blues' )
579	df_brazil_cases_by_day = df_grouped_brazil [ df_grouped_brazil . confirmed > 0 ] df_brazil_cases_by_day = df_brazil_cases_by_day . reset_index ( drop = True ) df_brazil_cases_by_day [ 'day' ] = df_brazil_cases_by_day . date . apply ( lambda x : ( x - df_brazil_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_brazil_cases_by_day = df_brazil_cases_by_day [ reordered_columns ] df_brazil_cases_by_day
417	train_meta = pd . read_csv ( '../input/metadata_train.csv' ) train_meta = train_meta . loc [ train_meta [ 'phase' ] == 0 ] train_sig_ids = train_meta [ 'signal_id' ] . values train_feat = Parallel ( n_jobs = N_THREADS , verbose = 2 ) ( delayed ( cluster_features ) ( s , 'train' ) for s in np . array_split ( train_sig_ids , 3 * N_THREADS ) ) train_feat = np . concatenate ( train_feat , axis = 0 )
227	n = 8 commits_df . loc [ n , 'commit_num' ] = 10 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25850
920	model = UNet ( ) model_path = 'model_1.pt' state = torch . load ( str ( model_path ) ) state = { key . replace ( 'module.' , '' ) : value for key , value in state [ 'model' ] . items ( ) } model . load_state_dict ( state ) if torch . cuda . is_available ( ) : model . cuda ( ) model . eval ( )
656	import warnings warnings . filterwarnings ( "ignore" ) import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns from sklearn . impute import SimpleImputer import scipy from sklearn . linear_model import LogisticRegression from sklearn . metrics import auc , roc_curve from sklearn . model_selection import StratifiedKFold , GridSearchCV from tqdm import tqdm_notebook
667	model = LogisticRegression ( C = 0.03 , max_iter = 300 ) model . fit ( encoded_train , raw_train . target ) test_pred = model . predict_proba ( encoded_test ) [ : , 1 ]
274	n = 3 commits_df . loc [ n , 'commit_num' ] = 8 commits_df . loc [ n , 'Dropout_model' ] = 0.35 commits_df . loc [ n , 'FVC_weight' ] = 0.25 commits_df . loc [ n , 'LB_score' ] = - 6.8107
1211	total = new_merchant . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( new_merchant . isnull ( ) . sum ( ) / new_merchant . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_data . head ( 20 )
821	train_bureau = pd . read_csv ( '../input/home-credit-manual-engineered-features/train_bureau_raw.csv' , nrows = 1000 ) test_bureau = pd . read_csv ( '../input/home-credit-manual-engineered-features/test_bureau_raw.csv' , nrows = 1000 ) train_previous = pd . read_csv ( '../input/home-credit-manual-engineered-features/train_previous_raw.csv' , nrows = 1000 ) test_previous = pd . read_csv ( '../input/home-credit-manual-engineered-features/test_previous_raw.csv' , nrows = 1000 ) bureau_columns = list ( train_bureau . columns ) previous_columns = list ( train_previous . columns )
312	train_path = 'base_dir/train_dir' valid_path = 'base_dir/val_dir' test_path = '../input/test' num_train_samples = len ( df_train ) num_val_samples = len ( df_val ) train_batch_size = 10 val_batch_size = 10 train_steps = np . ceil ( num_train_samples / train_batch_size ) val_steps = np . ceil ( num_val_samples / val_batch_size )
871	original_features = list ( pd . get_dummies ( app ) . columns ) created_features = [ ] for feature in fi [ 'feature' ] [ : 100 ] : if feature not in original_features : created_features . append ( feature ) print ( '%d of the top 100 features were made by featuretools' % len ( created_features ) )
1228	logit = LogisticRegression ( random_state = 0 , C = 0.5 ) outcomes = cross_validate_sklearn ( logit , x_train , y_train , x_test , kf , scale = True , verbose = True ) logit_cv = outcomes [ 0 ] logit_train_pred = outcomes [ 1 ] logit_test_pred = outcomes [ 2 ] logit_train_pred_df = pd . DataFrame ( columns = [ 'prediction_probability' ] , data = logit_train_pred ) logit_test_pred_df = pd . DataFrame ( columns = [ 'prediction_probability' ] , data = logit_test_pred )
116	print ( "Whole data avarage:{}" . format ( price [ "sell_price" ] . mean ( ) ) ) print ( "Whole data standard deviation:{}" . format ( price [ "sell_price" ] . std ( ) ) ) plt . figure ( figsize = ( 10 , 6 ) ) sns . distplot ( price [ "sell_price" ] ) plt . title ( "Price data distribution of whole data" ) plt . ylabel ( "Frequency" ) ;
870	fi = pd . read_csv ( '../input/home-credit-default-risk-feature-tools/spec_feature_importances_ohe.csv' , index_col = 0 ) fi = fi . sort_values ( 'importance' , ascending = False ) fi . head ( 15 )
765	data [ 'fare-bin' ] = pd . cut ( data [ 'fare_amount' ] , bins = list ( range ( 0 , 50 , 5 ) ) ) . astype ( str ) data . loc [ data [ 'fare-bin' ] == 'nan' , 'fare-bin' ] = '[45+]' data . loc [ data [ 'fare-bin' ] == '(5, 10]' , 'fare-bin' ] = '(05, 10]' data [ 'fare-bin' ] . value_counts ( ) . sort_index ( ) . plot . bar ( color = 'b' , edgecolor = 'k' ) ; plt . title ( 'Fare Binned' ) ;
755	image_id = 'c14c1e300' image = cv2 . imread ( os . path . join ( BASE_DIR , 'train' , f'{image_id}.jpg' ) , cv2 . IMREAD_COLOR ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) . astype ( np . float32 ) image /= 255.0 plt . figure ( figsize = ( 10 , 10 ) ) plt . imshow ( image ) plt . show ( )
619	def perform_linear_regression ( df_X , df_Y , test_X , test_Y ) : regr = LinearRegression ( ) regr . fit ( df_X , df_Y ) pred_Y = regr . predict ( test_X ) r2_score_lr = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( regr . score ( df_X , df_Y ) * 100 , 2 ) returnval = { 'model' : 'LinearRegressor' , 'r2_score' : r2_score_lr } return returnval
1181	def preprocess_image ( image_path , desired_size = 224 ) : biopsy = openslide . OpenSlide ( image_path ) im = np . array ( biopsy . get_thumbnail ( size = ( desired_size , desired_size ) ) ) im = Image . fromarray ( im ) im = im . resize ( ( desired_size , desired_size ) ) im = np . array ( im ) return im
919	from sklearn . model_selection import train_test_split unique_img_ids = masks . groupby ( 'ImageId' ) . size ( ) . reset_index ( name = 'counts' ) train_ids , valid_ids = train_test_split ( unique_img_ids , test_size = 0.05 , stratify = unique_img_ids [ 'counts' ] , random_state = 42 ) train_df = pd . merge ( masks , train_ids ) valid_df = pd . merge ( masks , valid_ids ) print ( train_df . shape [ 0 ] , 'training masks' ) print ( valid_df . shape [ 0 ] , 'validation masks' )
546	fig , ax1 = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) merged [ "yearbuilt" ] = merged [ "yearbuilt" ] . map ( lambda x : str ( x ) . split ( "." ) [ 0 ] ) yearMerged = merged . groupby ( [ 'yearbuilt' , 'numberofstories' ] ) [ "parcelid" ] . count ( ) . unstack ( 'numberofstories' ) . fillna ( 0 ) yearMerged . plot ( kind = 'bar' , stacked = True , ax = ax1 )
293	n = 22 commits_df . loc [ n , 'commit_num' ] = 32 commits_df . loc [ n , 'Dropout_model' ] = 0.24 commits_df . loc [ n , 'FVC_weight' ] = 0.14 commits_df . loc [ n , 'GaussianNoise_stddev' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8106
1220	model . eval ( ) with torch . no_grad ( ) : preds = np . empty ( 0 ) for x , _ in tqdm_notebook ( tloader ) : x = x . to ( device ) output = model ( x ) idx = output . max ( dim = - 1 ) [ 1 ] . cpu ( ) . numpy ( ) preds = np . append ( preds , idx , axis = 0 )
1315	def replace_edjefa ( x ) : if x == 'yes' : return 1 elif x == 'no' : return 0 else : return x df_train [ 'edjefa' ] = df_train [ 'edjefa' ] . apply ( replace_edjefa ) . astype ( float ) df_test [ 'edjefa' ] = df_test [ 'edjefa' ] . apply ( replace_edjefa ) . astype ( float )
288	n = 17 commits_df . loc [ n , 'commit_num' ] = 26 commits_df . loc [ n , 'Dropout_model' ] = 0.385 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8092
852	grid_results = grid_search ( param_grid ) print ( 'The best validation score was {:.5f}' . format ( grid_results . loc [ 0 , 'score' ] ) ) print ( '\nThe best hyperparameters were:' ) import pprint pprint . pprint ( grid_results . loc [ 0 , 'params' ] )
83	f , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 ) ) sns . countplot ( data = animals , x = 'OutcomeType' , hue = 'Neutered' , ax = ax1 ) sns . countplot ( data = animals , x = 'Neutered' , hue = 'OutcomeType' , ax = ax2 )
773	test [ 'manhattan' ] = minkowski_distance ( test [ 'pickup_longitude' ] , test [ 'dropoff_longitude' ] , test [ 'pickup_latitude' ] , test [ 'dropoff_latitude' ] , 1 ) test [ 'euclidean' ] = minkowski_distance ( test [ 'pickup_longitude' ] , test [ 'dropoff_longitude' ] , test [ 'pickup_latitude' ] , test [ 'dropoff_latitude' ] , 2 )
489	from keras . preprocessing . text import Tokenizer docs = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( docs )
291	n = 20 commits_df . loc [ n , 'commit_num' ] = 29 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'GaussianNoise_stddev' ] = 0.15 commits_df . loc [ n , 'LB_score' ] = - 6.8092
1385	col = numeric_features [ 32 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1257	validation_dataset = get_dataset ( os . path . join ( NQ_DIR , valid_tf_record ) , seq_length = FLAGS . max_seq_length , batch_size = FLAGS . predict_batch_size , is_training = False ) validation_dataset_with_labels = get_dataset ( os . path . join ( NQ_DIR , valid_tf_record_with_labels ) , seq_length = FLAGS . max_seq_length , batch_size = FLAGS . predict_batch_size , is_training = True ) test_dataset = get_dataset ( FLAGS . test_tf_record , seq_length = FLAGS . max_seq_length , batch_size = FLAGS . predict_batch_size , is_training = False )
1457	def seed_everything ( seed = 1234 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True
585	df_target_country = df_italy_cases_by_day S0 , E0 , I0 , R0 , D0 = target_population , 5 * float ( df_target_country . confirmed [ 0 ] ) , float ( df_target_country . confirmed [ 0 ] ) , 0. , 0. y0_sir = S0 / target_population , I0 / target_population , R0 y0_sird = S0 / target_population , I0 / target_population , R0 , D0 y0_seir = S0 / target_population , E0 / target_population , I0 / target_population , R0 y0_seird = S0 / target_population , E0 / target_population , I0 / target_population , R0 , D0
665	imputer = SimpleImputer ( strategy = 'mean' ) retain_full = pd . DataFrame ( imputer . fit_transform ( full_data [ retain_cols ] ) , columns = retain_cols ) retain_full = retain_full / retain_full . max ( )
278	n = 7 commits_df . loc [ n , 'commit_num' ] = 13 commits_df . loc [ n , 'Dropout_model' ] = 0.36 commits_df . loc [ n , 'FVC_weight' ] = 0.175 commits_df . loc [ n , 'LB_score' ] = - 6.8096
352	dfe = train . sample ( n = 10000 , replace = True , random_state = 1 ) dfe = dfe . drop_duplicates ( keep = False ) ; dfe [ 'index' ] = dfe . index . tolist ( ) dfe = dfe . fillna ( 0 ) ; dfe . info ( )
1456	import numpy as np import pandas as pd from pathlib import Path import os import random from sklearn . model_selection import train_test_split import torch import torch . nn as nn import torch . optim as optim from torch . utils . data import Dataset , DataLoader from collections import Counter from transformers import BertForQuestionAnswering from transformers import BertTokenizer import pytorch_lightning as pl from tqdm import tqdm_notebook as tqdm
946	optR = OptimizedRounder ( ) optR . fit ( oof_train , X_train [ 'AdoptionSpeed' ] . values ) coefficients = optR . coefficients ( ) pred_test_y_k = optR . predict ( oof_train , coefficients ) print ( "\nValid Counts = " , Counter ( X_train [ 'AdoptionSpeed' ] . values ) ) print ( "Predicted Counts = " , Counter ( pred_test_y_k ) ) print ( "Coefficients = " , coefficients ) qwk = quadratic_weighted_kappa ( X_train [ 'AdoptionSpeed' ] . values , pred_test_y_k ) print ( "QWK = " , qwk )
1062	final_submission_df = pd . concat ( [ test_df , null_sub_df ] ) print ( final_submission_df . shape ) final_submission_df . head ( )
479	sub_df = pd . DataFrame ( { "ID_code" : test_df . ID_code . values } ) sub_df [ "target" ] = predictions sub_df [ : 10 ]
1290	from sklearn . metrics import mean_squared_error watchlist = [ ( dtrain , 'train' ) ] num_round = 600 bst = xgb . train ( dict ( xgb_params , silent = 0 ) , dtrain , num_boost_round = num_round ) preds = bst . predict ( dtest ) err = ( mean_squared_error ( test [ target ] . values , preds ) ) print ( 'MSE ={}' . format ( err ) )
887	app_types [ 'REGION_RATING_CLIENT' ] = ft . variable_types . Ordinal app_types [ 'REGION_RATING_CLIENT_W_CITY' ] = ft . variable_types . Ordinal app_test_types = app_types . copy ( ) del app_test_types [ 'TARGET' ]
490	model = Sequential ( ) model . add ( Dense ( 5 , input_dim = 2 ) ) model . add ( Activation ( 'relu' ) ) model . add ( Dense ( 1 ) ) model . add ( Activation ( 'sigmoid' ) )
165	df = pd . read_csv ( '../input/train.csv' , skiprows = 9308568 , nrows = 59633310 ) header = pd . read_csv ( '../input/train.csv' , nrows = 0 ) df . columns = header . columns df del header gc . collect ( ) print ( "The created dataframe contains" , df . shape [ 0 ] , "rows." )
746	submission , gbm_fi , valid_scores = model_gbm ( train_set , train_labels , test_set , test_ids , return_preds = False ) submission . to_csv ( 'gbm_baseline.csv' )
1331	def add_new_category ( x ) : x = str ( x ) . lower ( ) if x == 'nan' : return 'nan' x = '' . join ( x . split ( ) ) if 'youtube' in x or 'you' in x or 'yo' in x or 'tub' in x : return 'youtube' elif 'google' in x or 'goo' in x or 'gle' in x : return 'google' else : return 'other'
829	threshold = 0.95 features_to_keep = list ( norm_feature_importances [ norm_feature_importances [ 'cumulative_importance' ] < threshold ] [ 'feature' ] ) train_small = train [ features_to_keep ] test_small = test [ features_to_keep ]
369	svr = SVR ( ) svr . fit ( train , target ) acc_model ( 1 , svr , train , test )
748	import json with open ( 'trials.json' , 'w' ) as f : f . write ( json . dumps ( str ( trials ) ) )
560	bboxes_df = pd . DataFrame ( [ bboxes_dict ] ) bboxes_df = bboxes_df . transpose ( ) bboxes_df . columns = [ 'bbox_list' ] bboxes_df . head ( )
1053	def create_test_gen ( batch_size = 64 ) : return ImageDataGenerator ( rescale = 1 / 255. ) . flow_from_dataframe ( test_imgs , directory = '../input/severstal-steel-defect-detection/test_images' , x_col = 'ImageId' , class_mode = None , target_size = ( 256 , 256 ) , batch_size = batch_size , shuffle = False )
621	def perform_ridge_regression ( df_X , df_Y , test_X , test_Y ) : clf = Ridge ( alpha = 1.0 ) clf . fit ( df_X , df_Y ) pred_Y = clf . predict ( test_X ) r2_score_rr = r2_score ( test_Y , pred_Y ) accuracy = round ( clf . score ( df_X , df_Y ) * 100 , 3 ) returnval = { 'model' : 'RidgeRegression' , 'r2_score' : r2_score_rr } return returnval
1060	test_missing_pred = remove_model . predict_generator ( test_gen , steps = len ( test_gen ) , verbose = 1 ) test_imgs [ 'allMissing' ] = test_missing_pred test_imgs . head ( )
452	plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) sns . kdeplot ( train [ 'wind_speed' ] . dropna ( ) , shade = True , color = 'peru' ) plt . xlabel ( 'Wind Speed' , fontsize = 15 ) plt . ylabel ( 'Density' , fontsize = 15 ) plt . show ( )
984	import numpy as np import pandas as pd import time from sklearn . preprocessing import Imputer from sklearn . preprocessing import RobustScaler import seaborn as sns import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( "ignore" ) import xgboost as xgb from sklearn import preprocessing import os print ( os . listdir ( "../input" ) )
79	preds = learn . get_preds ( ds_type = DatasetType . Test ) preds = np . argmax ( preds [ 0 ] . numpy ( ) , axis = 1 ) categories = sorted ( train . genres . unique ( ) . astype ( 'str' ) ) final_preds = [ ] for idx in preds : final_preds . append ( categories [ idx ] ) final_submit = pd . read_csv ( '../input/clabscvcomp/data/sample_submission.csv' ) final_submit . genres = final_preds final_submit . head ( ) final_submit . to_csv ( 'submission.csv' , index = False )
492	from keras . layers import Input visible = Input ( shape = ( 2 , ) )
649	def rle_encode ( im ) : pixels = im . flatten ( order = 'F' ) pixels = np . concatenate ( [ [ 0 ] , pixels , [ 0 ] ] ) runs = np . where ( pixels [ 1 : ] != pixels [ : - 1 ] ) [ 0 ] + 1 runs [ 1 : : 2 ] -= runs [ : : 2 ] return ' ' . join ( str ( x ) for x in runs )
633	dftrain = pd . read_csv ( '../input/covid19-global-forecasting-week-2/train.csv' , parse_dates = [ 'Date' ] ) . sort_values ( by = [ 'Country_Region' , 'Date' ] ) dftest = pd . read_csv ( '../input/covid19-global-forecasting-week-2/test.csv' , parse_dates = [ 'Date' ] ) . sort_values ( by = [ 'Country_Region' , 'Date' ] ) dftrain . head ( )
344	import matplotlib . pyplot as plt loss = history . history [ 'loss' ] val_loss = history . history [ 'val_loss' ] epochs = range ( 1 , len ( loss ) + 1 ) plt . legend ( ) plt . plot ( epochs , loss , 'bo' , label = 'Training loss' ) plt . plot ( epochs , val_loss , 'b' , label = 'Validation loss' ) plt . title ( 'Training and validation loss' ) plt . legend ( ) plt . show ( )
567	PATH = '/kaggle/input/data-without-drift/' train = pd . read_csv ( PATH + 'train_clean.csv' ) test = pd . read_csv ( PATH + 'test_clean.csv' ) train . head ( )
367	from PIL import Image import seaborn as sns def _get_image_data_pil ( image_id , image_type , return_exif_md = False ) : fname = get_filename ( image_id , image_type ) try : img_pil = Image . open ( fname ) except Exception as e : assert False , "Failed to read image : %s, %s. Error message: %s" % ( image_id , image_type , e ) img = np . asarray ( img_pil ) assert isinstance ( img , np . ndarray ) , "Open image is not an ndarray. Image id/type : %s, %s" % ( image_id , image_type ) if not return_exif_md : return img else : return img , img_pil . _getexif ( )
1196	print ( "Number of annotators:" ) print ( j_df [ ( j_df [ 'target' ] != 0 ) & ( j_df [ 'target' ] == 1.0 ) ] [ 'toxicity_annotator_count' ] . iloc [ 16 ] ) print ( "Comment:" ) print ( j_df [ ( j_df [ 'target' ] != 0 ) & ( j_df [ 'target' ] == 1.0 ) ] [ 'comment_text' ] . iloc [ 16 ] )
554	for i in cat_feature : df [ i ] = pd . factorize ( df [ i ] ) [ 0 ] trn_cat = df [ cat_feature ] . values [ : 182080 ] tst_cat = df [ cat_feature ] . values [ 182080 : ]
916	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' ) plt . style . use ( 'fivethirtyeight' ) import gc
357	import os import shutil import copy import numpy as np import pandas as pd from sklearn . metrics import mean_absolute_error from gplearn . functions import make_function from gplearn . genetic import SymbolicRegressor from sklearn . model_selection import KFold print ( 'ok' )
1578	from sklearn . metrics import ( confusion_matrix , precision_recall_curve , auc , roc_curve , recall_score , classification_report , f1_score , precision_recall_fscore_support )
33	vect_word = TfidfVectorizer ( max_features = 10000 , lowercase = True , analyzer = 'word' , stop_words = 'english' , ngram_range = ( 1 , 2 ) , dtype = np . float32 ) vect_char = TfidfVectorizer ( max_features = 30000 , lowercase = True , analyzer = 'char' , stop_words = 'english' , ngram_range = ( 1 , 6 ) , dtype = np . float32 )
1162	from collections import Counter cls_counts = Counter ( cls for classes in train [ 'attribute_ids' ] . str . split ( ) for cls in classes ) print ( len ( cls_counts ) )
1261	if FLAGS . do_predict : test_features = ( tf . train . Example . FromString ( r . numpy ( ) ) for r in tf . data . TFRecordDataset ( FLAGS . test_tf_record ) ) predictions_json = get_prediction_json ( mode = 'test' , max_nb_pos_logits = FLAGS . n_best_size )
713	heads [ 'phones-per-capita' ] = heads [ 'qmobilephone' ] / heads [ 'tamviv' ] heads [ 'tablets-per-capita' ] = heads [ 'v18q1' ] / heads [ 'tamviv' ] heads [ 'rooms-per-capita' ] = heads [ 'rooms' ] / heads [ 'tamviv' ] heads [ 'rent-per-capita' ] = heads [ 'v2a1' ] / heads [ 'tamviv' ]
1150	test_df = pd . read_csv ( '../input/test.csv' ) epoch_datetime = pd . datetime ( 1900 , 1 , 1 ) s = ( test_df [ 'var_68' ] * 10000 - 7000 + epoch_datetime . toordinal ( ) ) . astype ( int ) test_df [ 'date' ] = s . map ( datetime . fromordinal ) sorted_test_df = test_df . drop ( 'var_68' , axis = 1 ) . sort_values ( 'date' )
301	eps = 1e-8 dense_game_features = train_dense . columns [ train_dense [ : 22 ] . std ( ) <= eps ] dense_player_features = train_dense . columns [ train_dense [ : 22 ] . std ( ) > eps ] cat_game_features = train_cat . columns [ train_cat [ : 22 ] . std ( ) <= eps ] cat_player_features = train_cat . columns [ train_cat [ : 22 ] . std ( ) > eps ]
175	df = pd . read_csv ( '../input/train.csv' , skiprows = 9308568 , nrows = 59633310 ) header = pd . read_csv ( '../input/train.csv' , nrows = 0 ) df . columns = header . columns df del header gc . collect ( ) print ( "The created dataframe contains" , df . shape [ 0 ] , "rows." )
397	METADATA_CSV [ 'in_train' ] = False METADATA_CSV [ 'in_test' ] = False METADATA_CSV . loc [ test_dataset_ids , 'in_test' ] = True METADATA_CSV . loc [ TRAIN_MASKS_CSV [ 'id' ] . unique ( ) , 'in_train' ] = True
1550	import numpy as np import pandas as pd import matplotlib . pyplot as plt import os import datetime import warnings warnings . filterwarnings ( "ignore" ) import seaborn as sns import scipy
1473	def create_model ( input_shape , n_out ) : input_tensor = Input ( shape = input_shape ) base_model = DenseNet121 ( include_top = False , weights = None , input_tensor = input_tensor ) x = GlobalAveragePooling2D ( ) ( base_model . output ) x = Dense ( 1024 , activation = 'relu' ) ( x ) final_output = Dense ( n_out , activation = 'softmax' , name = 'final_output' ) ( x ) model = Model ( input_tensor , final_output ) return model
1134	import numpy as np import pandas as pd import efficientnet . tfkeras as efn from tensorflow . keras import backend as K import tensorflow_addons as tfa import tensorflow . keras . layers as layers import tensorflow as tf
1120	M_F = { 'Neutered Male' : 'Male' , 'Spayed Female' : 'Female' , 'Intact Male' : 'Male' , 'Intact Female' : 'Female' , 'Unknown' : 'Unknown' } N_T = { 'Neutered Male' : 'Neutered' , 'Spayed Female' : 'Neutered' , 'Intact Male' : 'Intact' , 'Intact Female' : 'Intact' , 'Unknown' : 'Unknown' } animals [ 'Sex' ] = animals . SexuponOutcome . map ( M_F ) animals [ 'Neutered' ] = animals . SexuponOutcome . map ( N_T )
95	whole_text_freq = class_corpus . sum ( ) fig , ax = plt . subplots ( ) label , repetition = zip ( * whole_text_freq . most_common ( 25 ) ) ax . barh ( range ( len ( label ) ) , repetition , align = 'center' ) ax . set_yticks ( np . arange ( len ( label ) ) ) ax . set_yticklabels ( label ) ax . invert_yaxis ( ) ax . set_title ( 'Word Distribution Over Whole Text' ) ax . set_xlabel ( ' ax.set_ylabel(' Word ' ) plt . tight_layout ( ) plt . show ( )
863	app_train [ 'set' ] = 'train' app_test [ 'set' ] = 'test' app_test [ "TARGET" ] = np . nan app = app_train . append ( app_test , ignore_index = True )
1439	temp = pd . read_csv ( '../input/train_sample.csv' ) temp [ 'os' ] = temp [ 'os' ] . astype ( 'str' )
22	train_target = train [ 'target' ] . values np . unique ( train_target )
1530	plt . figure ( figsize = [ 18 , 4 ] ) df_train [ 'killPlace' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of killPlace" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'killPlace' ] . value_counts ( ) )
892	plt . figure ( figsize = ( 8 , 6 ) ) plt . hist ( time_features [ 'TREND(bureau.AMT_CREDIT_SUM, bureau_credit_application_date)' ] . dropna ( ) , edgecolor = 'k' ) ; plt . xlabel ( 'TREND(bureau.AMT_CREDIT_SUM, bureau_credit_application_date)' ) ; plt . ylabel ( 'Counts' ) ; plt . title ( 'Distribution of Trends in Credit Sum' ) ;
1301	test_transaction_df = pd . read_csv ( '/kaggle/input/test_transaction.csv' ) test_identity_df = pd . read_csv ( '/kaggle/input/test_identity.csv' ) df_test = test_transaction_df . merge ( test_identity_df , on = 'TransactionID' , how = 'left' )
69	def distance ( tour , data , pen = 9 ) : xy , z = np . hsplit ( data [ tour ] , [ 2 ] ) dist = np . hypot ( * ( xy [ : - 1 ] - xy [ 1 : ] ) . T ) dist [ pen : : 10 ] *= z [ : - 1 ] [ pen : : 10 ] . flat return dist dist = distance ( tour , data ) dist
1023	n_steps = x_valid . shape [ 0 ] // BATCH_SIZE train_history_2 = model . fit ( valid_dataset . repeat ( ) , steps_per_epoch = n_steps , epochs = EPOCHS )
775	from sklearn . linear_model import LinearRegression from sklearn . model_selection import train_test_split lr = LinearRegression ( )
349	def my_generator ( ) : while True : for i in range ( 0 , 4 ) : yield i infinity_gen = my_generator ( )
1370	col = numeric_features [ 17 ] plot_category_percent_of_target_for_numeric ( col )
1356	col = numeric_features [ 3 ] plot_kde_hist_for_numeric ( col )
682	print ( '\trows\tcolumns' ) print ( 'Train:\t{:>6,}\t{:>6,}' . format ( * train . shape ) ) print ( 'Test:\t{:>6,}\t{:>6,}' . format ( * test . shape ) )
660	plt . figure ( figsize = ( 22 , 6 ) ) plt . title ( 'Day distribution' ) ax = sns . countplot ( raw_train . day , hue = raw_train . target ) for p in ax . patches : ax . text ( p . get_x ( ) + p . get_width ( ) / 2. , p . get_height ( ) + 1000 , f'{100*p.get_height()/height:.2f} %' , ha = 'center' ) plt . show ( )
259	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
1526	plt . hist ( df_train [ 'winPlacePerc' ] ) plt . xlabel ( "winPlacePerc" ) plt . ylabel ( "count" ) plt . title ( 'Distribution of winPlacePerc' )
831	from sklearn . decomposition import PCA from sklearn . preprocessing import Imputer from sklearn . pipeline import Pipeline train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) pipeline = Pipeline ( steps = [ ( 'imputer' , Imputer ( strategy = 'median' ) ) , ( 'pca' , PCA ( ) ) ] ) train_pca = pipeline . fit_transform ( train ) test_pca = pipeline . transform ( test )
34	col = 'identity_hate' print ( "Column:" , col ) pred = lr . predict ( X ) print ( '\nConfusion matrix\n' , confusion_matrix ( y [ col ] , pred ) ) print ( classification_report ( y [ col ] , pred ) )
1368	col = numeric_features [ 15 ] plot_category_percent_of_target_for_numeric ( col )
787	plt . figure ( figsize = ( 10 , 8 ) ) for d , grouped in data . groupby ( 'pickup_Dayofweek' ) : sns . kdeplot ( grouped [ 'fare_amount' ] , label = f'{d}' ) plt . title ( 'Fare Amount by Day of Week' ) ;
1575	df_dl = times_series_means [ [ 'date' , 'Visits' ] ] train_size = int ( len ( df_dl ) * 0.80 ) test_size = len ( df_dl ) - train_size train , test = df_dl . iloc [ 0 : train_size , : ] , df_dl . iloc [ train_size : len ( df_dl ) , : ] print ( len ( train ) , len ( test ) )
9	nulls = np . sum ( train . isnull ( ) ) nullcols = nulls . loc [ ( nulls != 0 ) ] dtypes = train . dtypes dtypes2 = dtypes . loc [ ( nulls != 0 ) ] info = pd . concat ( [ nullcols , dtypes2 ] , axis = 1 ) . sort_values ( by = 0 , ascending = False )
1266	from tensorflow . keras . optimizers import Adam from adamw_optimizer import AdamW optimizer = AdamW ( weight_decay = FLAGS . init_weight_decay_rate , learning_rate = learning_rate , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-6 , decay_var_list = decay_var_list )
84	f , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 ) ) sns . countplot ( data = animals , x = 'OutcomeType' , hue = 'Mix' , ax = ax1 ) sns . countplot ( data = animals , x = 'Mix' , hue = 'OutcomeType' , ax = ax2 )
1423	stats = [ ] for Province in [ 'Hong Kong' , 'Hubei' ] : df = get_time_series_province ( Province ) print ( '{} COVID-19 Prediction' . format ( Province ) ) opt_display_model ( df , stats )
1536	prev_app_df [ 'DAYS_LAST_DUE' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_TERMINATION' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_FIRST_DRAWING' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_FIRST_DUE' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_LAST_DUE_1ST_VERSION' ] . replace ( 365243 , np . nan , inplace = True )
839	cash_info = agg_grandchild ( cash , previous , 'SK_ID_PREV' , 'SK_ID_CURR' , 'CASH' ) del cash cash_info . shape
405	f = filenames [ 0 ] r1 = stage_1_PIL ( f ) r2 = stage_1_cv2 ( f ) plt . figure ( figsize = ( 16 , 16 ) ) plt . subplot ( 131 ) plt . imshow ( r1 ) plt . subplot ( 132 ) plt . imshow ( r2 ) plt . subplot ( 133 ) plt . imshow ( np . abs ( r1 - r2 ) )
1515	mappy = { 4 : "NonVulnerable" , 3 : "Moderate Poverty" , 2 : "Vulnerable" , 1 : "Extereme Poverty" } target_values [ 'Household_type' ] = target_values . Household_type . map ( mappy ) target_values
1379	col = numeric_features [ 26 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
658	plt . figure ( figsize = ( 10 , 7 ) ) num_cols = raw_train . select_dtypes ( exclude = [ 'object' ] ) . columns corr = raw_train [ num_cols ] . corr ( ) sns . heatmap ( corr , xticklabels = corr . columns . values , yticklabels = corr . columns . values )
731	model = RandomForestClassifier ( n_estimators = 100 , random_state = 10 , n_jobs = - 1 ) cv_score = cross_val_score ( model , train_set , train_labels , cv = 10 , scoring = scorer ) print ( f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}' )
277	n = 6 commits_df . loc [ n , 'commit_num' ] = 11 commits_df . loc [ n , 'Dropout_model' ] = 0.36 commits_df . loc [ n , 'FVC_weight' ] = 0.15 commits_df . loc [ n , 'LB_score' ] = - 6.8100
1207	train . loc [ ( train . state < 1 ) ^ ( train . state > 4 ) , "state" ] = np . nan inv_counts = train [ train . product_type == "Investment" ] [ "state" ] . value_counts ( ) own_counts = train [ train . product_type == "OwnerOccupier" ] [ "state" ] . value_counts ( ) product_category = pd . DataFrame ( [ inv_counts , own_counts ] ) product_category . index = [ "Investment" , "OwnerOccupier" ] product_category . plot ( kind = "bar" , stacked = True )
172	not_missing = df [ df [ 'attributed_time' ] . isna ( ) == False ] not_missing [ 'gap' ] = pd . to_datetime ( not_missing [ 'attributed_time' ] ) . sub ( pd . to_datetime ( not_missing [ 'click_time' ] ) ) for i in range ( 0 , 11 ) : y = i / 10 print ( "{0:.0f}" . format ( y * 100 ) , "quantile :" , not_missing [ 'gap' ] . quantile ( y ) )
624	public_df = test . query ( "seq_length == 107" ) . copy ( ) private_df = test . query ( "seq_length == 130" ) . copy ( ) public_inputs , public_adj = preprocess_inputs ( public_df ) private_inputs , private_adj = preprocess_inputs ( private_df ) public_inputs = torch . tensor ( public_inputs , dtype = torch . long ) private_inputs = torch . tensor ( private_inputs , dtype = torch . long ) public_adj = torch . tensor ( public_adj , dtype = torch . long ) private_adj = torch . tensor ( private_adj , dtype = torch . long )
256	train0 = train0 . drop ( [ 'Id' , '3' , '4' , '5' , '6' , '7' ] , axis = 1 ) train0 = train0 . dropna ( ) train0 . info ( )
1147	with_masks_df = ( train_df . groupby ( 'img_id' ) [ 'EncodedPixels' ] . count ( ) . reset_index ( ) . rename ( columns = { "EncodedPixels" : "n_masks" } ) ) with_masks_df = with_masks_df . loc [ lambda df : df [ "n_masks" ] > 0 , : ]
253	country_name = "Germany" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1589	num_cols = [ 'volume' , 'close' , 'open' , 'returnsClosePrevRaw1' , 'returnsOpenPrevRaw1' , 'returnsClosePrevMktres1' , 'returnsOpenPrevMktres1' , 'returnsClosePrevRaw10' , 'returnsOpenPrevRaw10' , 'returnsClosePrevMktres10' , 'returnsOpenPrevMktres10' ]
1260	if FLAGS . do_valid : if FLAGS . smaller_valid_dataset : predict_file = FLAGS . validation_predict_file_small else : predict_file = FLAGS . validation_predict_file f1 , long_f1 , short_f1 = compute_f1_scores ( valid_predictions_json , predict_file ) print ( f" valid f1: {f1}\n valid long_f1: {long_f1}\nvalid short_f1: {short_f1}" )
526	import statsmodels . api as sm X_train_2 = sm . add_constant ( X_train ) est = sm . OLS ( y_train , X_train_2 ) est2 = est . fit ( ) print ( "summary()\n" , est2 . summary ( ) )
846	def objective ( hyperparameters , iteration ) : if 'n_estimators' in hyperparameters . keys ( ) : del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , nfold = N_FOLDS , early_stopping_rounds = 100 , metrics = 'auc' , seed = 42 ) score = cv_results [ 'auc-mean' ] [ - 1 ] estimators = len ( cv_results [ 'auc-mean' ] ) hyperparameters [ 'n_estimators' ] = estimators return [ score , hyperparameters , iteration ]
1080	import time blur_list = [ ] blur_list_id = [ ] start_time = time . time ( ) ; for i , image_id in enumerate ( tqdm ( train_df [ 'id_code' ] ) ) : img = preprocess_image ( f'../input/train_images/{image_id}.png' ) if ( not isClear ( img ) ) : blur_list . append ( i ) blur_list_id . append ( image_id ) train_df = train_df . drop ( blur_list ) print ( f'Cost: {time.time() - start_time}:.3% seconds' ) ;
280	n = 9 commits_df . loc [ n , 'commit_num' ] = 15 commits_df . loc [ n , 'Dropout_model' ] = 0.32 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8092
1090	reduce_valid = pd . DataFrame ( ) for i , row in reduce_train_org . groupby ( 'installation_id' , sort = False ) : reduce_valid = reduce_valid . append ( row . sample ( 1 ) ) reduce_train = reduce_train_org . drop ( reduce_valid . index )
320	def binary_target ( x ) : if x != 0 : return 1 else : return x df_train [ 'binary_target' ] = df_train [ 'diagnosis' ] . apply ( binary_target )
651	cate0 = range ( 36 , 44 ) col = df . columns for i in cate0 : df . ix [ : , i ] = np . where ( df . ix [ : , i ] < - 1 , - 2 , df . ix [ : , i ] ) df . ix [ : , i ] = np . where ( df . ix [ : , i ] >= - 1 , 0 , df . ix [ : , i ] )
734	model_results = cv_model ( train_set , train_labels , MLPClassifier ( hidden_layer_sizes = ( 32 , 64 , 128 , 64 , 32 ) ) , 'MLP' , model_results )
692	from itertools import product tta_transforms = [ ] for tta_combination in product ( [ TTAHorizontalFlip ( ) , None ] , [ TTAVerticalFlip ( ) , None ] , [ TTARotate90 ( ) , None ] ) : tta_transforms . append ( TTACompose ( [ tta_transform for tta_transform in tta_combination if tta_transform ] ) )
1561	lemm = WordNetLemmatizer ( ) class LemmaCountVectorizer ( CountVectorizer ) : def build_analyzer ( self ) : analyzer = super ( LemmaCountVectorizer , self ) . build_analyzer ( ) return lambda doc : ( lemm . lemmatize ( w ) for w in analyzer ( doc ) )
1408	print ( 'Id is unique.' ) if train . id . nunique ( ) == train . shape [ 0 ] else print ( 'Oh no' ) print ( 'Train and test sets are distinct.' ) if len ( np . intersect1d ( train . id . values , test . id . values ) ) == 0 else print ( 'Oh no' ) print ( 'We do not need to worry about missing values.' ) if train . count ( ) . min ( ) == train . shape [ 0 ] else print ( 'Oh no' )
992	img = vtk_show ( ren ) disp . stop ( ) Image ( img )
183	train . shape train . info ( ) train . isnull ( ) . sum ( ) train . head ( 5 )
764	plt . figure ( figsize = ( 10 , 6 ) ) sns . distplot ( data [ 'fare_amount' ] ) ; plt . title ( 'Distribution of Fare' ) ;
668	n = 10 from collections import Counter label_mapping = pd . Series . from_csv ( '../input/label_names.csv' , header = 0 ) . to_dict ( ) top_n = Counter ( [ item for sublist in labels for item in sublist ] ) . most_common ( n ) top_n_labels = [ int ( i [ 0 ] ) for i in top_n ] top_n_label_names = [ label_mapping [ x ] for x in top_n_labels ] top_n_label_names
104	if blaze_bboxes == [ ] : print ( 'BlazeFace is unable to detect face in this frame.' ) if mtcnn_bboxes == [ ] : print ( 'MTCNN is unable to detect face in this frame.' ) if mobilenet_bboxes == [ ] : print ( 'mobilenet is unable to detect face in this frame.' ) if yolo_bboxes == [ ] : print ( 'mobilenet is unable to detect face in this frame.' )
1018	train = pd . read_csv ( "../input/liverpool-ion-switching/train.csv" ) test = pd . read_csv ( "../input/liverpool-ion-switching/test.csv" ) sub = pd . read_csv ( "../input/liverpool-ion-switching/sample_submission.csv" , dtype = dict ( time = str ) )
1548	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) fasttext_embeddings = load_fasttext ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings , fasttext_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings , fasttext_embeddings gc . collect ( ) np . shape ( embedding_matrix )
1054	filtered_mask = sub_df [ 'ImageId' ] . isin ( filtered_test_imgs [ "ImageId" ] . values ) filtered_sub_df = sub_df [ filtered_mask ] . copy ( ) null_sub_df = sub_df [ ~ filtered_mask ] . copy ( ) null_sub_df [ 'EncodedPixels' ] = null_sub_df [ 'EncodedPixels' ] . apply ( lambda x : ' ' ) filtered_sub_df . reset_index ( drop = True , inplace = True ) filtered_test_imgs . reset_index ( drop = True , inplace = True ) print ( filtered_sub_df . shape ) print ( null_sub_df . shape ) filtered_sub_df . head ( )
1011	img = cv2 . imread ( f'../input/train/{label_df.loc[0,"Image"]}' ) pad_width = get_pad_width ( img , max ( img . shape ) ) padded = np . pad ( img , pad_width = pad_width , mode = 'constant' , constant_values = 0 ) resized = cv2 . resize ( padded , ( 224 , 224 ) ) plt . imshow ( resized )
1311	train = pd . read_json ( '' ) test = pd . read_json ( '' )
447	plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) sns . heatmap ( train . corr ( ) , vmin = - 1 , vmax = 1 , center = 0 , square = True , cmap = sns . diverging_palette ( 20 , 220 , n = 200 ) ) plt . show ( )
474	MAX_TREE_DEPTH = 8 TREE_METHOD = 'gpu_hist' ITERATIONS = 1000 SUBSAMPLE = 0.6 REGULARIZATION = 0.1 GAMMA = 0.3 POS_WEIGHT = 1 EARLY_STOP = 10 params = { 'tree_method' : TREE_METHOD , 'max_depth' : MAX_TREE_DEPTH , 'alpha' : REGULARIZATION , 'gamma' : GAMMA , 'subsample' : SUBSAMPLE , 'scale_pos_weight' : POS_WEIGHT , 'learning_rate' : 0.05 , 'silent' : 1 , 'objective' : 'binary:logistic' , 'eval_metric' : 'auc' , 'n_gpus' : 1 }
1238	submission = sample_submission . copy ( ) submission [ 'target' ] = logit_lv3_test_pred * 0.5 + xgb_lv3_test_pred * 0.5 filename = 'stacking_demonstration.csv.gz' submission . to_csv ( filename , compression = 'gzip' , index = False )
1049	train_resized_imgs = [ ] test_resized_imgs = [ ] for image_id in label_df [ 'id' ] : train_resized_imgs . append ( pad_and_resize ( image_id , 'train' ) ) for image_id in submission_df [ 'Id' ] : test_resized_imgs . append ( pad_and_resize ( image_id , 'test' ) )
1516	train [ 'v2a11' ] = train . v2a1 . apply ( lambda x : np . log ( x + 1 ) ) sns . set ( font_scale = 1 , style = "darkgrid" ) c = sns . color_palette ( 'spring_d' ) [ 4 ] sns_jointplot = sns . jointplot ( 'age' , 'meaneduc' , data = train , kind = 'kde' , color = c , size = 6 )
1076	train_x = train_x . reshape ( - 1 , 28 , 28 , 1 ) train_y = tf . keras . utils . to_categorical ( train_y , num_classes ) test_x = test_x . reshape ( - 1 , 28 , 28 , 1 )
893	es [ 'previous' ] [ 'NAME_CONTRACT_STATUS' ] . interesting_values = [ 'Approved' , 'Refused' , 'Canceled' ] interesting_features , interesting_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , max_depth = 1 , where_primitives = [ 'mean' , 'mode' ] , trans_primitives = [ ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
1051	pivot_df = sample_df [ [ 'Label' , 'filename' , 'type' ] ] . drop_duplicates ( ) . pivot ( index = 'filename' , columns = 'type' , values = 'Label' ) . reset_index ( ) print ( pivot_df . shape ) pivot_df . head ( )
804	OUT_FILE = 'bayes_test.csv' of_connection = open ( OUT_FILE , 'w' ) writer = csv . writer ( of_connection ) ITERATION = 0 headers = [ 'loss' , 'hyperparameters' , 'iteration' , 'runtime' , 'score' ] writer . writerow ( headers ) of_connection . close ( ) results = objective ( sample ( space ) ) print ( 'The cross validation loss = {:.5f}.' . format ( results [ 'loss' ] ) ) print ( 'The optimal number of estimators was {}.' . format ( results [ 'hyperparameters' ] [ 'n_estimators' ] ) )
1161	X_COL = "var_81" Y_COL = "var_68" Z_COL = "var_108" HUE_COL = "target" N_SAMPLES = 10000 df = train_df . sample ( N_SAMPLES )
307	Dropout_new = 0.15 n_split = 5 lr = 3e-5
702	data . loc [ ( data [ 'tipovivi1' ] == 1 ) , 'v2a1' ] = 0 data [ 'v2a1-missing' ] = data [ 'v2a1' ] . isnull ( ) data [ 'v2a1-missing' ] . value_counts ( )
30	sample_submission = pd . read_csv ( '../input/sample_submission.csv' ) sample_submission [ 'target' ] = preds sample_submission . to_csv ( 'submission.csv' , index = False )
1571	plt . figure ( figsize = ( 50 , 8 ) ) mean_group = train_flattened [ [ 'Page' , 'date' , 'Visits' ] ] . groupby ( [ 'date' ] ) [ 'Visits' ] . mean ( ) plt . plot ( mean_group ) plt . title ( 'Time Series - Average' ) plt . show ( )
1152	import datetime from datetime import timedelta import matplotlib . pylab as plt import numpy as np import pandas as pd import tensorflow as tf from tensorflow . keras . layers import ( Activation , Add , Concatenate , Conv1D , Dense , Dropout , Input , Lambda , Multiply , Embedding , Flatten , concatenate , TimeDistributed , Reshape ) from sklearn . preprocessing import StandardScaler from tensorflow . keras . models import Model from tensorflow . keras . optimizers import Adam from tqdm import tqdm from pathlib import Path import gc
521	def evaluate_threshold ( tpr , fpr , clf_threshold , threshold ) : print ( 'Sensitivity:' , tpr [ clf_threshold > threshold ] [ - 1 ] ) print ( 'Specificity:' , 1 - fpr [ clf_threshold > threshold ] [ - 1 ] )
333	xgb_clf = xgb . XGBRegressor ( { 'objective' : 'reg:squarederror' } ) parameters = { 'n_estimators' : [ 200 , 300 ] , 'learning_rate' : [ 0.01 , 0.02 , 0.03 ] , 'max_depth' : [ 10 , 12 ] } xgb_reg = GridSearchCV ( estimator = xgb_clf , param_grid = parameters , cv = 5 , n_jobs = - 1 ) . fit ( trainb , targetb ) print ( "Best score: %0.3f" % xgb_reg . best_score_ ) print ( "Best parameters set:" , xgb_reg . best_params_ ) acc_boosting_model ( 7 , xgb_reg , trainb , testb )
114	train = train_df . copy ( ) price = price_df . copy ( ) calendar = calendar_df . copy ( )
1518	from sklearn . manifold import TSNE from sklearn . preprocessing import StandardScaler scaler1 = StandardScaler ( ) X_scaled = scaler1 . fit_transform ( train [ numerical ] )
182	def rle_encoding ( x ) : dots = np . where ( x . T . flatten ( ) == 1 ) [ 0 ] run_lengths = [ ] prev = - 2 for b in dots : if ( b > prev + 1 ) : run_lengths . extend ( ( b + 1 , 0 ) ) run_lengths [ - 1 ] += 1 prev = b return " " . join ( [ str ( i ) for i in run_lengths ] ) print ( 'RLE Encoding for the current mask is: {}' . format ( rle_encoding ( label_mask ) ) )
420	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yT , y_pred ) print ( confusion )
1072	import os import pandas as pd import numpy as np import matplotlib . pyplot as plt
932	salt_parser . initialize_data ( ) X_train , y_train , X_test = salt_parser . load_data ( ) train_df = salt_parser . compute_coverage ( ) padding_pixels = salt_parser . return_padding_borders ( )
1382	col = numeric_features [ 29 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1245	plt . style . use ( 'ggplot' ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) ax . scatter ( train [ 'Size' ] , train [ 'Weekly_Sales' ] , alpha = 0.5 ) plt . show ( )
544	dataTypeDf = pd . DataFrame ( merged . dtypes . value_counts ( ) ) . reset_index ( ) . rename ( columns = { "index" : "variableType" , 0 : "count" } ) fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . barplot ( data = dataTypeDf , x = "variableType" , y = "count" , ax = ax , color = " ax.set(xlabel='Variable Type', ylabel='Count',title=" Variables Count Across Datatype " )
1591	news_agg_cols = [ f for f in news_train_df . columns if 'novelty' in f or 'volume' in f or 'sentiment' in f or 'bodySize' in f or 'Count' in f or 'marketCommentary' in f or 'tf_score' in f or 'rise_fall' in f or 'relevance' in f ] news_agg_dict = { } for col in news_agg_cols : news_agg_dict [ col ] = [ 'mean' , 'sum' , 'max' , 'min' ] news_agg_dict [ 'urgency' ] = [ 'min' , 'count' ] news_agg_dict [ 'takeSequence' ] = [ 'max' ]
340	models = pd . DataFrame ( { 'Model' : [ 'Linear Regression' , 'Support Vector Machines' , 'Linear SVR' , 'MLPRegressor' , 'Stochastic Gradient Decent' , 'Decision Tree Regressor' , 'Random Forest' , 'XGB' , 'LGBM' , 'GradientBoostingRegressor' , 'RidgeRegressor' , 'BaggingRegressor' , 'ExtraTreesRegressor' , 'AdaBoostRegressor' , 'VotingRegressor' ] , 'r2_train' : acc_train_r2 , 'r2_test' : acc_test_r2 , 'd_train' : acc_train_d , 'd_test' : acc_test_d , 'rmse_train' : acc_train_rmse , 'rmse_test' : acc_test_rmse } )
1197	my_string = j_df [ ( j_df [ 'target' ] != 0 ) & ( j_df [ 'target' ] == 1.0 ) ] [ 'comment_text' ] . iloc [ 16 ] def my_str_compare ( x ) : return fuzz . token_sort_ratio ( x , my_string ) jdf0 = j_df [ j_df [ 'target' ] == 0 ] jdf0 [ 'distance2mys1' ] = jdf0 [ 'comment_text' ] . apply ( my_str_compare ) jdf0 . sort_values ( by = [ 'distance2mys1' ] , ascending = False ) . head ( )
282	n = 11 commits_df . loc [ n , 'commit_num' ] = 17 commits_df . loc [ n , 'Dropout_model' ] = 0.25 commits_df . loc [ n , 'FVC_weight' ] = 0.5 commits_df . loc [ n , 'LB_score' ] = - 6.8283
830	train [ 'TARGET' ] = train_labels train [ 'SK_ID_CURR' ] = train_ids test [ 'SK_ID_CURR' ] = test_ids submission , feature_importances , metrics = model ( train , test )
1494	def lift ( fct ) : def lifted_function ( xs ) : list_of_results = [ fct ( x ) for x in xs ] return list ( itertools . chain ( * list_of_results ) ) import re lifted_function . __name__ = re . sub ( '_unlifted$' , '_lifted' , fct . __name__ ) return lifted_function cropToContent = lift ( cropToContent_unlifted ) groupByColor = lift ( groupByColor_unlifted ) splitH = lift ( splitH_unlifted ) negative = lift ( negative_unlifted )
730	features = list ( train_set . columns ) pipeline = Pipeline ( [ ( 'imputer' , Imputer ( strategy = 'median' ) ) , ( 'scaler' , MinMaxScaler ( ) ) ] ) train_set = pipeline . fit_transform ( train_set ) test_set = pipeline . transform ( test_set )
867	feature_matrix_spec , feature_names_spec = ft . dfs ( entityset = es , target_entity = 'app' , agg_primitives = [ 'sum' , 'count' , 'min' , 'max' , 'mean' , 'mode' ] , max_depth = 2 , features_only = False , verbose = True )
803	x = sample ( space ) subsample = x [ 'boosting_type' ] . get ( 'subsample' , 1.0 ) x [ 'boosting_type' ] = x [ 'boosting_type' ] [ 'boosting_type' ] x [ 'subsample' ] = subsample x
926	import time import numpy as np import pandas as pd import re import nltk from nltk . corpus import stopwords from gensim . models import word2vec from sklearn . neural_network import MLPClassifier
28	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train [ '0' ] . values , bins = 20 ) plt . title ( 'Histogram 0 train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
1438	import numpy as np import pandas as pd import datetime import os import time import matplotlib . pyplot as plt import seaborn as sns import gc
690	def get_patient_data ( filepath ) : if filepath != 'DNE' : dcm_data = pydicom . dcmread ( filepath , stop_before_pixels = True ) return dcm_data . PatientID , dcm_data . StudyInstanceUID , dcm_data . SeriesInstanceUID
460	directions = { 'N' : 0 , 'NE' : 1 / 4 , 'E' : 1 / 2 , 'SE' : 3 / 4 , 'S' : 1 , 'SW' : 5 / 4 , 'W' : 3 / 2 , 'NW' : 7 / 4 }
232	n = 13 commits_df . loc [ n , 'commit_num' ] = 17 commits_df . loc [ n , 'dropout_model' ] = 0.37 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25885
477	cd LightGBM rm - r build mkdir build cd build cmake - DUSE_GPU = 1 - DOpenCL_LIBRARY = / usr / local / cuda / lib64 / libOpenCL . so - DOpenCL_INCLUDE_DIR = / usr / local / cuda / include / . . make - j $ ( nproc )
741	train_set = pd . DataFrame ( train_set , columns = features ) corr_matrix = train_set . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
966	for key , value in dict . items ( ) : if key in [ "China" , 'Rest of China w/o Hubei' ] : pass else : growth_rate_over_time ( exp , value , 'Confirmed' , key + ' - Growth Rate Percentage for ' , )
198	bg , = bulge_graph . BulgeGraph . from_fasta_text ( '>seq\n' + Sequence + '\n' + structure ) plt . figure ( figsize = ( 20 , 10 ) ) fvm . plot_rna ( bg , text_kwargs = { "fontweight" : "black" } , lighten = 0.8 , backbone_kwargs = { "linewidth" : 3 } ) plt . show ( )
412	data_id = "d4d34af4f7" img = read_image ( data_id ) mask = read_mask ( data_id ) plt . figure ( figsize = ( 7 , 7 ) ) plt . subplot ( 121 ) plt . title ( "Depth = {}" . format ( depth_df . loc [ data_id , 'z' ] ) ) plt . imshow ( img ) plt . subplot ( 122 ) plt . imshow ( mask )
1283	import glob def read_from_folder ( path ) : filenames = glob . glob ( path + "/*.csv" ) dfs = [ ] for filename in filenames : dfs . append ( pd . read_csv ( filename , index_col = 0 ) ) frame = pd . concat ( dfs ) return frame . sort_index ( )
1527	plt . figure ( figsize = [ 10 , 6 ] ) df_train [ 'assists' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of assists" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'assists' ] . value_counts ( ) )
948	print ( 'Train NaN:\n\n{}\n' . format ( np . sum ( pd . isnull ( train ) ) ) ) print ( 'Test NaN:\n\n{}\n' . format ( np . sum ( pd . isnull ( test ) ) ) ) print ( 'Merchant NaN:\n\n{}\n' . format ( np . sum ( pd . isnull ( merchant ) ) ) ) print ( 'New Merchant NaN:\n\n{}\n' . format ( np . sum ( pd . isnull ( new_merchant ) ) ) )
882	plt . figure ( figsize = ( 8 , 7 ) ) plt . plot ( opt_hyp [ 'learning_rate' ] , opt_hyp [ 'n_estimators' ] , 'ro' ) plt . xlabel ( 'Learning Rate' ) ; plt . ylabel ( 'N Estimators' ) ; plt . title ( 'Number of Estimators vs Learning Rate' ) ;
219	n = 0 commits_df . loc [ n , 'commit_num' ] = 0 commits_df . loc [ n , 'dropout_model' ] = 0.4 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 128 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25883
1511	if __name__ == '__main__' : p = patients [ 1 ] print ( 'Create video for {}' . format ( p ) ) image_list = create_set_of_png_for_patient ( p ) create_video ( image_list , "output.avi" )
300	params_xgb = { 'max_depth' : xgb_max_depth , 'objective' : 'binary:logistic' , 'min_child_weight' : xgb_min_child_weight , 'learning_rate' : xgb_lr , 'eta' : 0.3 , 'subsample' : 0.8 , 'eval_metric' : 'logloss' , 'colsample_bylevel' : 1 }
270	Dropout_model = 0.25 FVC_weight = 0.5 Confidence_weight = 0.5 GaussianNoise_stddev = 0.2
1537	app_both [ 'LOAN_INCOME_RATIO' ] = app_both [ 'AMT_CREDIT' ] / app_both [ 'AMT_INCOME_TOTAL' ] app_both [ 'ANNUITY_INCOME_RATIO' ] = app_both [ 'AMT_ANNUITY' ] / app_both [ 'AMT_INCOME_TOTAL' ] app_both [ 'ANNUITY LENGTH' ] = app_both [ 'AMT_CREDIT' ] / app_both [ 'AMT_ANNUITY' ] app_both [ 'WORKING_LIFE_RATIO' ] = app_both [ 'DAYS_EMPLOYED' ] / app_both [ 'DAYS_BIRTH' ] app_both [ 'INCOME_PER_FAM' ] = app_both [ 'AMT_INCOME_TOTAL' ] / app_both [ 'CNT_FAM_MEMBERS' ] app_both [ 'CHILDREN_RATIO' ] = app_both [ 'CNT_CHILDREN' ] / app_both [ 'CNT_FAM_MEMBERS' ]
1234	logit_lv2 = LogisticRegression ( random_state = 0 , C = 0.5 ) logit_lv2_outcomes = cross_validate_sklearn ( logit_lv2 , lv1_train_df , y_train , lv1_test_df , kf , scale = True , verbose = True ) logit_lv2_cv = logit_lv2_outcomes [ 0 ] logit_lv2_train_pred = logit_lv2_outcomes [ 1 ] logit_lv2_test_pred = logit_lv2_outcomes [ 2 ]
1025	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) train2 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv" ) train2 . toxic = train2 . toxic . round ( ) . astype ( int ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
1259	if FLAGS . do_valid : validation_features = ( tf . train . Example . FromString ( r . numpy ( ) ) for r in tf . data . TFRecordDataset ( valid_tf_record ) ) valid_predictions_json = get_prediction_json ( mode = 'valid' , max_nb_pos_logits = FLAGS . n_best_size )
855	random_search_params = random_results . loc [ 0 , 'params' ] model = lgb . LGBMClassifier ( ** random_search_params , random_state = 42 ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] print ( 'The best model from random search scores {:.5f} ROC AUC on the test set.' . format ( roc_auc_score ( test_labels , preds ) ) )
877	random [ 'set' ] = 'random' scores = random [ [ 'score' , 'iteration' , 'set' ] ] opt [ 'set' ] = 'opt' scores = scores . append ( opt [ [ 'set' , 'iteration' , 'score' ] ] , sort = True ) scores . head ( )
415	ix = 3 test_image = X_test [ ix ] . astype ( float ) imshow ( test_image ) plt . show ( )
422	from sklearn . ensemble import RandomForestClassifier rfc = RandomForestClassifier ( n_estimators = 10 , n_jobs = 2 , random_state = 0 ) rfc . fit ( x , y )
68	def initial ( ) : df = pd . read_csv ( '../input/traveling-santa-2018-prime-paths/cities.csv' ) df [ 'Z' ] = 1 + .1 * ~ df [ 'CityId' ] . apply ( sp . isprime ) data = df [ [ 'X' , 'Y' , 'Z' ] ] . values tour = np . loadtxt ( '../input/traveling-santa-lkh-solution/pure1502650.csv' , skiprows = 1 , dtype = int ) return tour , data tour , data = initial ( )
663	time_cols = [ 'day' , 'month' ] for col in time_cols : full_data [ col + '_sin' ] = np . sin ( 2 * np . pi * full_data [ col ] / 7 ) full_data [ col + '_cos' ] = np . cos ( 2 * np . pi * full_data [ col ] / 12 ) full_data = full_data . drop ( columns = time_cols )
1119	sexType = animals [ 'SexuponOutcome' ] . unique ( ) print ( sexType )
783	preds = random_forest . predict ( test [ [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] ) sub = pd . DataFrame ( { 'key' : test_id , 'fare_amount' : preds } ) sub . to_csv ( 'sub_rf_simple.csv' , index = False ) sns . distplot ( sub [ 'fare_amount' ] ) plt . title ( 'Distribution of Random Forest Predicted Fare Amount' ) ;
1031	image_with_boxes = draw_boxes ( np . array ( image_out ) , result_out [ "detection_boxes" ] , result_out [ "detection_class_entities" ] , result_out [ "detection_scores" ] ) display_image ( image_with_boxes )
511	def convert_to_grayscale ( img ) : base_range = np . amax ( img ) - np . amin ( img ) rescaled_range = 255 - 0 img_rescaled = ( ( ( img - np . amin ( img ) ) * rescaled_range ) / base_range ) return np . uint8 ( img_rescaled )
608	max_features = 20000 max_text_length = 400
816	train = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_train.csv' ) test = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_test.csv' ) test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
669	n = 100 from collections import Counter top_n = Counter ( [ item for sublist in train . ingredients for item in sublist ] ) . most_common ( n ) top_n
1005	densenet = DenseNet121 ( weights = '/kaggle/input/densenet-keras/DenseNet-BC-121-32-no-top.h5' , include_top = False , input_shape = ( 224 , 224 , 3 ) ) for layer in densenet . layers : layer . trainable = False
1441	import subprocess def file_len ( fname ) : p = subprocess . Popen ( [ 'wc' , '-l' , fname ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) result , err = p . communicate ( ) if p . returncode != 0 : raise IOError ( err ) return int ( result . strip ( ) . split ( ) [ 0 ] ) lines = file_len ( '../input/train.csv' ) print ( 'Number of lines in "train.csv" is:' , lines )
177	print ( 'Original image shape: {}' . format ( im . shape ) ) from skimage . color import rgb2gray im_gray = rgb2gray ( im ) print ( 'New image shape: {}' . format ( im_gray . shape ) )
1185	train_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) test_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' ) sub_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' ) print ( train_df . shape ) print ( test_df . shape ) train_df . head ( )
67	import time import pandas as pd import numpy as np import sympy as sp import matplotlib . pyplot as plt import matplotlib . collections as mcoll from itertools import chain from itertools import combinations from itertools import permutations from itertools import product from sklearn . neighbors import NearestNeighbors
770	sns . lmplot ( 'abs_lat_diff' , 'abs_lon_diff' , hue = 'fare-bin' , size = 8 , palette = palette , fit_reg = False , data = data . sample ( 10000 , random_state = RSEED ) ) ; plt . title ( 'Absolute latitude difference vs Absolute longitude difference' ) ;
1353	categorical_features = [ 'ProductName' , 'EngineVersion' , 'AppVersion' , 'AvSigVersion' , 'Platform' , 'Processor' , 'OsVer' , 'OsPlatformSubRelease' , 'OsBuildLab' , 'SkuEdition' , 'SmartScreen' , 'Census_MDC2FormFactor' , 'Census_DeviceFamily' , 'Census_PrimaryDiskTypeName' , 'Census_ChassisTypeName' , 'Census_PowerPlatformRoleName' , 'Census_OSVersion' , 'Census_OSArchitecture' , 'Census_OSBranch' , 'Census_OSEdition' , 'Census_OSSkuName' , 'Census_OSInstallTypeName' , 'Census_OSWUAutoUpdateOptionsName' , 'Census_GenuineStateName' , 'Census_ActivationChannel' , 'Census_FlightRing' , ]
318	submission = pd . DataFrame ( { 'id' : image_id , 'label' : y_pred , } ) . set_index ( 'id' ) submission . to_csv ( 'patch_preds.csv' , columns = [ 'label' ] )
51	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( np . log ( train_df [ columns_to_use ] . values . flatten ( ) + 1 ) , bins = 50 ) plt . title ( 'Log Histogram all train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Log value' ) plt . show ( )
56	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train_zeros . Percentile . values , bins = 50 ) plt . title ( 'Histogram percentage zeros train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Value' ) plt . show ( )
1113	leak_df [ 'mean_pred' ] = np . mean ( leak_df [ [ 'pred1' , 'pred2' , 'pred3' ] ] . values , axis = 1 ) leak_df [ 'mean_pred_l1p' ] = np . log1p ( leak_df . mean_pred ) leak_score = np . sqrt ( mean_squared_error ( leak_df . mean_pred_l1p , leak_df . meter_reading_l1p ) ) sns . distplot ( leak_df . mean_pred_l1p ) sns . distplot ( leak_df . meter_reading_l1p ) print ( 'mean score=' , leak_score )
1364	col = numeric_features [ 11 ] plot_kde_hist_for_numeric ( col )
160	sns . set ( ) plt . hist ( sub1 [ 'isFraud' ] , bins = 100 ) plt . show ( )
688	def id_to_filepath ( img_id , img_dir = TRAIN_DIR ) : filepath = f'{img_dir}/{img_id}.dcm' if os . path . exists ( filepath ) : return filepath else : return 'DNE'
1036	public_df = test . query ( "seq_length == 107" ) private_df = test . query ( "seq_length == 130" ) public_inputs = preprocess_inputs ( public_df , token2int ) private_inputs = preprocess_inputs ( private_df , token2int )
836	installments = pd . read_csv ( '../input/installments_payments.csv' ) . replace ( { 365243 : np . nan } ) installments = convert_types ( installments ) installments [ 'LATE' ] = installments [ 'DAYS_ENTRY_PAYMENT' ] > installments [ 'DAYS_INSTALMENT' ] installments [ 'LOW_PAYMENT' ] = installments [ 'AMT_PAYMENT' ] < installments [ 'AMT_INSTALMENT' ]
106	try : before = loadPickleBZ ( data_path + 'one-more-approach-to-sort-columns-and-rows/before.pbz' ) beforeM = before - np . transpose ( before ) print ( 'before matrix loaded' ) except : before = np . zeros ( ( values . shape [ 1 ] , values . shape [ 1 ] ) , dtype = np . int32 ) print ( 'before matrix NOT loaded!' ) sets = [ [ set ( ) ] for i in range ( values . shape [ 0 ] ) ]
1066	BATCH_SIZE = 8 train_idx , val_idx = train_test_split ( non_missing_train_idx . index , random_state = 2019 , test_size = 0.2 ) train_generator = DataGenerator ( train_idx , reshape = ( 256 , 512 ) , df = mask_count_df , target_df = train_df , augment = True , batch_size = BATCH_SIZE , n_classes = 4 ) val_generator = DataGenerator ( val_idx , reshape = ( 256 , 512 ) , df = mask_count_df , target_df = train_df , augment = False , batch_size = BATCH_SIZE , n_classes = 4 )
532	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) ordersDay = orders [ [ "order_dow" ] ] . replace ( { 0 : "Sunday" , 1 : "Monday" , 2 : "Tuesday" , 3 : "Wednesday" , 4 : "Thursday" , 5 : "Friday" , 6 : "Saturday" } ) sn . countplot ( color = " ax.set(xlabel='Day Of The Week',title=" Order Count Across Days Of The Week " )
211	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) import matplotlib . pyplot as plt import featuretools as ft from featuretools . primitives import * from featuretools . variable_types import Numeric from sklearn . preprocessing import LabelEncoder , MinMaxScaler from sklearn . svm import LinearSVR from sklearn . feature_selection import SelectFromModel import warnings warnings . filterwarnings ( "ignore" )
1099	for task , prediction , solved in tqdm ( zip ( evaluation_tasks , evaluation_predictions , evaluation_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
1191	x_train , x_val , y_train , y_val = train_test_split ( x_train_full , best_confidence , test_size = TRAIN_VAL_RATIO , random_state = 2020 )
1360	col = numeric_features [ 7 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1241	print ( "the shape of stores data set is" , stores . shape ) print ( "the unique value of store is" , stores [ 'Store' ] . unique ( ) ) print ( "the unique value of Type is" , stores [ 'Type' ] . unique ( ) )
551	class GaussianTargetNoise ( object ) : def __init__ ( self , p : float = 0.5 , gaus_std : float = 1.0 , ) : self . p = p self . gaus_std = gaus_std def __call__ ( self , x_arr , y_arr , atten_arr ) : if np . random . binomial ( n = 1 , p = self . p ) : y_arr = y_arr + np . random . normal ( scale = self . gaus_std , size = y_arr . shape ) return x_arr , y_arr , atten_arr
522	report_logreg = metrics . classification_report ( y_train , y_pred_class_logreg ) report_SGD = metrics . classification_report ( y_train , y_pred_class_SGD ) report_rfc = metrics . classification_report ( y_train , y_pred_class_rfc ) print ( "report_logreg " + "\n" + report_logreg , "report_SGD " + "\n" + report_SGD , "report_rfc " + "\n" + report_rfc , sep = "\n" )
1450	proportion = train [ [ 'device' , 'is_attributed' ] ] . groupby ( 'device' , as_index = False ) . mean ( ) . sort_values ( 'is_attributed' , ascending = False ) counts = train [ [ 'device' , 'is_attributed' ] ] . groupby ( 'device' , as_index = False ) . count ( ) . sort_values ( 'is_attributed' , ascending = False ) merge = counts . merge ( proportion , on = 'device' , how = 'left' ) merge . columns = [ 'device' , 'click_count' , 'prop_downloaded' ] print ( 'Count of clicks and proportion of downloads by device:' ) print ( merge )
1410	etc_ordianal_features = [ 'ps_ind_01' , 'ps_ind_03' , 'ps_ind_14' , 'ps_ind_15' , 'ps_reg_01' , 'ps_reg_02' , 'ps_car_11' , 'ps_calc_01' , 'ps_calc_02' , 'ps_calc_03' , 'ps_calc_04' , 'ps_calc_05' , 'ps_calc_06' , 'ps_calc_07' , 'ps_calc_08' , 'ps_calc_09' , 'ps_calc_10' , 'ps_calc_11' , 'ps_calc_12' , 'ps_calc_13' , 'ps_calc_14' ] etc_continuous_features = [ 'ps_reg_03' , 'ps_car_12' , 'ps_car_13' , 'ps_car_14' , 'ps_car_15' ] train_null_columns = train_null . columns test_null_columns = test_null . columns
1328	pred_test_y = 0.33 * pred_glove_test_y + 0.33 * pred_fasttext_test_y + 0.34 * pred_paragram_test_y pred_test_y = ( pred_test_y > 0.35 ) . astype ( int ) out_df = pd . DataFrame ( { "qid" : test_df [ "qid" ] . values } ) out_df [ 'prediction' ] = pred_test_y out_df . to_csv ( "submission.csv" , index = False )
1465	train = train . sort_values ( by = 'visitStartTime' ) train [ 'previous_visitStartTime' ] = train [ 'visitStartTime' ] . astype ( str ) . groupby ( train [ 'fullVisitorId' ] ) . shift ( 1 ) . fillna ( '-1' ) . astype ( np . int64 ) train = train . sort_index ( )
588	if has_to_run_sir : num_of_parameters_to_fit_sir = 1 bounds_sir = num_of_parameters_to_fit_sir * [ ( 0 , 1 ) ] result_sir = optimize . differential_evolution ( sir_least_squares_error_ode , bounds = bounds_sir , args = ( data_time , infected_individuals , sir_ode_solver , y0_sir ) , popsize = 300 , strategy = 'best1bin' , tol = 1e-2 , recombination = 0.5 , maxiter = 100 , disp = True , seed = seed , callback = callback_de , workers = - 1 ) print ( result_sir )
1564	first_topic = lda . components_ [ 0 ] second_topic = lda . components_ [ 1 ] third_topic = lda . components_ [ 2 ] fourth_topic = lda . components_ [ 3 ]
191	train [ 'no_descrip' ] = 0 train . loc [ train . item_description == 'No description yet' , 'no_descrip' ] = 1 i = str ( round ( train [ 'no_descrip' ] . value_counts ( normalize = True ) . iloc [ 1 ] * 100 , 2 ) ) + '%' print ( i , 'of the items have no a description.' )
123	fig = px . line ( train_df , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'SmokingStatus' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines+markers' )
353	es = ft . EntitySet ( id = 'ashrae_energy_data' ) es = es . entity_from_dataframe ( entity_id = 'dfe' , dataframe = dfe , variable_types = { 'air_temperature' : ft . variable_types . Numeric , 'cloud_coverage' : ft . variable_types . Numeric , 'dew_temperature' : ft . variable_types . Numeric , 'precip_depth_1_hr' : ft . variable_types . Numeric } , index = 'index' )
49	columns_to_use = test_df . columns . tolist ( ) del columns_to_use [ 0 ] columns_to_use = [ x for x in columns_to_use if x not in constant_train ] len ( columns_to_use )
1122	import pandas as pd import numpy as np import matplotlib . pylab as plt pd . set_option ( 'display.max_rows' , 500 ) pd . get_option ( "display.max_columns" , 500 )
386	split_raw_data ( ) build_rnd_idxs ( ) run_mp_build ( ) join_mp_build ( ) build_test_fields ( ) scale_fields ( )
92	plt . figure ( ) ax = df [ 'Class' ] . value_counts ( ) . plot ( kind = 'bar' ) ax . set_title ( 'Class Distribution Over Entries' ) ax . set_xlabel ( 'Class' ) ax . set_ylabel ( 'Frequency' ) plt . tight_layout ( ) plt . show ( )
507	reducedtarget0sampleDF = pd . DataFrame ( ) for col in range ( target0sampledata . shape [ 1 ] ) : tmp_pdSeries = reduce_sample ( target0sampledata . iloc [ : , col ] ) reducedtarget0sampleDF [ str ( col ) ] = tmp_pdSeries reducedtarget0sampleDF . shape
1483	patientId = df [ 'patientId' ] [ 8 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 2 - Lung Opacity" ) draw ( parsed [ patientId ] )
27	import os print ( os . listdir ( "../input" ) )
1255	PRETRAINED_MODELS = { "BERT" : [ 'bert-base-uncased' , 'bert-large-uncased' , 'bert-base-cased' , 'bert-large-cased' , 'bert-base-multilingual-uncased' , 'bert-base-multilingual-cased' , 'bert-base-chinese' , 'bert-base-german-cased' , 'bert-large-uncased-whole-word-masking' , 'bert-large-cased-whole-word-masking' , 'bert-large-uncased-whole-word-masking-finetuned-squad' , 'bert-large-cased-whole-word-masking-finetuned-squad' , 'bert-base-cased-finetuned-mrpc' ] , "DISTILBERT" : [ 'distilbert-base-uncased' , 'distilbert-base-uncased-distilled-squad' ] }
573	df_covid [ 'active' ] = df_covid [ 'confirmed' ] - df_covid [ 'deaths' ] - df_covid [ 'recovered' ] df_covid
1319	for col1 in [ 'epared1' , 'epared2' , 'epared3' ] : for col2 in [ 'etecho1' , 'etecho2' , 'etecho3' ] : for col3 in [ 'eviv1' , 'eviv2' , 'eviv3' ] : new_col_name = 'new_{}_x_{}_x_{}' . format ( col1 , col2 , col3 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] * df_train [ col3 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ] * df_train [ col3 ]
875	import pprint import ast keys = [ ] for key , value in ast . literal_eval ( random . loc [ 0 , 'hyperparameters' ] ) . items ( ) : print ( f'{key}: {value}' ) keys . append ( key )
929	num_features = 300 min_word_count = 40 num_workers = 4 context = 10 downsampling = 1e-3 model = word2vec . Word2Vec ( sentences_train , workers = num_workers , size = num_features , min_count = min_word_count , window = context , sample = downsampling ) model . init_sims ( replace = True )
951	train_ = train . set_index ( 'card_id' ) . join ( new_merchant_card_id_cat , how = 'left' ) train_ = train_ . join ( new_merchant_card_id_num , how = 'left' ) test_ = test . set_index ( 'card_id' ) . join ( new_merchant_card_id_cat , how = 'left' ) test_ = test_ . join ( new_merchant_card_id_num , how = 'left' ) del train , test gc . collect ( )
1307	model = RandomForestRegressor ( n_estimators = 400 , max_features = 0.3 , min_samples_leaf = 20 , n_jobs = - 1 , verbose = 1 )
179	from scipy import ndimage labels , nlabels = ndimage . label ( mask ) label_arrays = [ ] for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) label_arrays . append ( label_mask ) print ( 'There are {} separate components / objects detected.' . format ( nlabels ) )
1262	import os import sys import json import tensorflow as tf import numpy as np import pandas as pd import matplotlib . pyplot as plt import absl import datetime
231	n = 12 commits_df . loc [ n , 'commit_num' ] = 15 commits_df . loc [ n , 'dropout_model' ] = 0.45 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25920
351	building_df = pd . read_csv ( "../input/ashrae-energy-prediction/building_metadata.csv" ) weather_train = pd . read_csv ( "../input/ashrae-energy-prediction/weather_train.csv" ) train = pd . read_csv ( "../input/ashrae-energy-prediction/train.csv" ) train = train . merge ( building_df , left_on = "building_id" , right_on = "building_id" , how = "left" ) train = train . merge ( weather_train , left_on = [ "site_id" , "timestamp" ] , right_on = [ "site_id" , "timestamp" ] ) del weather_train train . head ( )
859	bars = alt . Chart ( random_hyp , width = 400 ) . mark_bar ( ) . encode ( x = 'boosting_type' , y = alt . Y ( 'count()' , scale = alt . Scale ( domain = [ 0 , 400 ] ) ) ) bars . title = 'Boosting Type for Random Search' text = bars . mark_text ( align = 'center' , baseline = 'bottom' , size = 20 ) . encode ( text = 'count()' ) bars + text
431	duplicate_pairs = df . sort_values ( 'Title' , ascending = False ) . duplicated ( 'Title' ) print ( "Total number of duplicate questions : " , duplicate_pairs . sum ( ) ) df = df [ ~ duplicate_pairs ] print ( "Dataframe shape after duplicate removal : " , df . shape )
1321	for col1 in [ 'sanitario1' , 'sanitario2' , 'sanitario3' , 'sanitario5' , 'sanitario6' ] : for col2 in [ 'elimbasu1' , 'elimbasu2' , 'elimbasu3' , 'elimbasu4' , 'elimbasu5' , 'elimbasu6' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
917	cash = pd . read_csv ( '../input/POS_CASH_balance.csv' ) cash = convert_types ( cash , print_info = True ) cash . head ( )
964	shap . dependence_plot ( ( "returnsClosePrevRaw10_lag_3_mean" , "returnsOpenPrevMktres10" ) , shap_interaction_values , X_interaction )
768	data = data . loc [ data [ 'pickup_latitude' ] . between ( 40 , 42 ) ] data = data . loc [ data [ 'pickup_longitude' ] . between ( - 75 , - 72 ) ] data = data . loc [ data [ 'dropoff_latitude' ] . between ( 40 , 42 ) ] data = data . loc [ data [ 'dropoff_longitude' ] . between ( - 75 , - 72 ) ] print ( f'New number of observations: {data.shape[0]}' )
393	from tqdm import tqdm_notebook num_dicts = 7069896 prod_to_category = [ None ] * num_dicts with tqdm_notebook ( total = num_dicts ) as bar : TRAIN_DB = bson . decode_file_iter ( open ( os . path . join ( INPUT_PATH , 'train.bson' ) , 'rb' ) ) for i , item in enumerate ( TRAIN_DB ) : bar . update ( ) prod_to_category [ i ] = ( item [ '_id' ] , item [ 'category_id' ] )
371	sgd = SGDRegressor ( ) sgd . fit ( train , target ) acc_model ( 4 , sgd , train , test )
562	mask_dir = df [ df . id == imid ] . masks . values [ 0 ] masks = os . listdir ( mask_dir ) masks [ : 10 ]
389	item = get_item ( 1234 ) mask = CATEGORY_NAMES_DF [ 'category_id' ] == item [ 'category_id' ] cat_levels = CATEGORY_NAMES_DF [ mask ] [ level_tags ] . values . tolist ( ) [ 0 ] cat_levels = [ c [ : 25 ] for c in cat_levels ] title = str ( item [ 'category_id' ] ) + '\n' title += '\n' . join ( cat_levels ) plt . title ( title ) plt . imshow ( decode_images ( item [ 'imgs' ] ) ) _ = plt . axis ( 'off' )
47	sns . set_style ( "whitegrid" ) ax = sns . violinplot ( x = np . log ( 1 + train_df . target . values ) ) plt . show ( )
1044	preds_ls = [ ] for df , preds in [ ( public_df , public_preds ) , ( private_df , private_preds ) ] : for i , uid in enumerate ( df . id ) : single_pred = preds [ i ] single_df = pd . DataFrame ( single_pred , columns = pred_cols ) single_df [ 'id_seqpos' ] = [ f'{uid}_{x}' for x in range ( single_df . shape [ 0 ] ) ] preds_ls . append ( single_df ) preds_df = pd . concat ( preds_ls )
1249	n_iter = 1000 start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_cutmix ( images , labels , PROBABILITY = 1.0 ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_cutmix: {timing}" )
1401	col = numeric_features [ 50 ] plot_category_percent_of_target_for_numeric ( col )
994	plt . figure ( figsize = ( 5 , 5 ) ) dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/train/ID00009637202177434476278/' import pydicom as dcm plt . imshow ( dcm . dcmread ( dir + os . listdir ( dir ) [ 0 ] ) . pixel_array )
845	model = lgb . LGBMClassifier ( ) default_params = model . get_params ( ) del default_params [ 'n_estimators' ] cv_results = lgb . cv ( default_params , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS , seed = 42 )
776	X_train , X_valid , y_train , y_valid = train_test_split ( data , np . array ( data [ 'fare_amount' ] ) , stratify = data [ 'fare-bin' ] , random_state = RSEED , test_size = 1_000_000 )
646	length = 5 split_labels = df_train [ "labels" ] [ 0 ] . split ( ) for idx in range ( len ( split_labels ) // length ) : start_idx = idx * length print ( split_labels [ start_idx : start_idx + length ] ) if idx == 4 : break
763	data = pd . read_csv ( '../input/train.csv' , nrows = 5_000_000 , parse_dates = [ 'pickup_datetime' ] ) . drop ( columns = 'key' ) data = data . dropna ( ) data . head ( )
299	params_lgb = { 'num_leaves' : lgb_num_leaves_max , 'min_data_in_leaf' : lgb_in_leaf , 'objective' : 'binary' , 'max_depth' : - 1 , 'learning_rate' : lgb_lr , "boosting_type" : "gbdt" , "bagging_seed" : lgb_bagging , "metric" : 'logloss' , "verbosity" : - 1 , 'random_state' : 42 , }
1270	training_dataset = get_training_dataset ( dataset , do_aug = False , advanced_aug = False , repeat = 1 , with_labels = False , drop_remainder = True ) start = datetime . datetime . now ( ) for i in tqdm . tqdm ( range ( n_iter ) ) : model . predict ( training_dataset ) end = datetime . datetime . now ( ) elapsed = ( end - start ) . total_seconds ( ) average = elapsed / n_iter print ( "Average timing for 1 iteration = {}" . format ( average ) )
1131	for f in X_train . columns : if X_train [ f ] . dtype == 'object' or X_test [ f ] . dtype == 'object' : lbl = preprocessing . LabelEncoder ( ) lbl . fit ( list ( X_train [ f ] . values ) + list ( X_test [ f ] . values ) ) X_train [ f ] = lbl . transform ( list ( X_train [ f ] . values ) ) X_test [ f ] = lbl . transform ( list ( X_test [ f ] . values ) )
1427	stats = [ ] for Province in sorted ( full_table [ 'Province/State' ] . unique ( ) ) : if ( Province == '' ) : continue df = get_time_series_province ( Province ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 500 ) : continue print ( '{} COVID-19 Prediction' . format ( Province ) ) opt_display_model ( df , stats )
693	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'fivethirtyeight' ) plt . rcParams [ 'font.size' ] = 18 plt . rcParams [ 'patch.edgecolor' ] = 'k'
1273	oversampled_training_dataset = get_training_dataset_with_oversample ( repeat_dataset = False , oversample = True , augumentation = False ) label_counter_2 = Counter ( ) for images , labels in oversampled_training_dataset : label_counter_2 . update ( labels . numpy ( ) ) del oversampled_training_dataset label_counting_sorted_2 = label_counter_2 . most_common ( ) NUM_TRAINING_IMAGES_OVERSAMPLED = sum ( [ x [ 1 ] for x in label_counting_sorted_2 ] ) print ( "number of examples in the oversampled training dataset: {}" . format ( NUM_TRAINING_IMAGES_OVERSAMPLED ) ) print ( "labels in the oversampled training dataset, sorted by occurrence" ) label_counting_sorted_2
1470	from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM from keras . layers import Flatten from keras . layers import TimeDistributed from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D
1105	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
904	categorical = pd . get_dummies ( bureau . select_dtypes ( 'object' ) ) categorical [ 'SK_ID_CURR' ] = bureau [ 'SK_ID_CURR' ] categorical . head ( )
975	import matplotlib . pyplot as plt first_img = first_dicom . pixel_array plt . imshow ( first_img ) print ( first_img . shape )
125	patient_dir = '../input/osic-pulmonary-fibrosis-progression/train/ID00007637202177411956430/' scans = glob ( patient_dir + '/*.dcm' ) scans [ : 5 ]
1059	def load_img ( code , base , resize = True ) : path = f'{base}/{code}' img = cv2 . imread ( path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) if resize : img = cv2 . resize ( img , ( 256 , 256 ) ) return img def validate_path ( path ) : if not os . path . exists ( path ) : os . makedirs ( path )
271	n = 0 commits_df . loc [ n , 'commit_num' ] = 1 commits_df . loc [ n , 'Dropout_model' ] = 0.4 commits_df . loc [ n , 'FVC_weight' ] = 0.25 commits_df . loc [ n , 'LB_score' ] = - 6.8111
1490	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 12 - Unclear Abnormality" ) draw ( parsed [ df [ 'patientId' ] [ 40 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 40 ] ] )
1486	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 4 - Ground-Glass Opacities" ) draw ( parsed [ df [ 'patientId' ] [ 25 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 25 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 5 - Consolidations" ) draw ( parsed [ df [ 'patientId' ] [ 28 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 28 ] ] )
171	plt . figure ( figsize = ( 6 , 4 ) ) sns . boxplot ( ip_level [ 'DL_by_click_ratio' ] , ip_level [ 'clicker_type' ] ) plt . title ( 'Ratio : Download by click' , fontsize = 15 ) plt . xlabel ( 'Download by click' ) plt . ylabel ( 'Category of clicker' ) del ip_level gc . collect ( )
103	import scipy print ( model_pred . clip ( 0.35 , 0.65 ) . mean ( ) ) print ( scipy . stats . median_absolute_deviation ( model_pred . clip ( 0.35 , 0.65 ) ) [ 0 ] )
1488	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 3 - Lung Nodules and Masses" ) draw ( parsed [ df [ 'patientId' ] [ 2 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 2 ] ] )
1435	unique_count_features = [ 'uq_app_per_ip' , 'uq_os_per_ip_app' , 'uq_device_per_ip' , 'uq_channels_per_app' , 'uq_channel_per_ip' ] other_count_features = [ 'ip_per_day_count' , 'ip_app_count' , 'ip_app_os_count' , 'ip_channel_per_hour' , 'cumcount_ip_dev_os' ]
898	feature_matrix_test , feature_names_test = ft . dfs ( entityset = es , target_entity = 'app_test' , agg_primitives = [ 'mean' , 'max' , 'min' , 'trend' , 'mode' , 'count' , 'sum' , 'percent_true' , NormalizedModeCount , MostRecent , LongestSeq ] , trans_primitives = [ 'diff' , 'cum_sum' , 'cum_mean' , 'percentile' ] , where_primitives = [ 'mean' , 'sum' ] , seed_features = [ late_payment , past_due ] , max_depth = 2 , features_only = False , verbose = True , chunk_size = len ( app_test ) , ignore_entities = [ 'app_train' ] )
376	ridge = RidgeCV ( cv = 10 ) ridge . fit ( train , target ) acc_model ( 10 , ridge , train , test )
1590	from sklearn . feature_extraction . text import CountVectorizer from sklearn . feature_extraction . text import TfidfTransformer from nltk . corpus import stopwords
32	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in the train data set:' , train . shape ) print ( 'Number of rows and columns in the test data set:' , test . shape )
491	from keras import optimizers model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) algorithm = optimizers . SGD ( lr = 0.1 , momentum = 0.3 ) model . compile ( optimizer = algorithm , loss = 'mean_squared_error' )
813	import altair as alt alt . renderers . enable ( 'notebook' ) c = alt . Chart ( scores ) . mark_circle ( ) . encode ( x = 'iteration' , y = alt . Y ( 'ROC AUC' , scale = alt . Scale ( domain = [ 0.64 , 0.74 ] ) ) , color = 'search' ) c . title = 'Validation ROC AUC vs Iteration' c
1077	permutation = np . random . RandomState ( 2019 ) . permutation ( len ( train_x ) ) train_x = train_x [ permutation ] train_y = train_y [ permutation ]
628	date_agg_3 = train_agg . groupby ( level = [ 1 , 2 ] ) . sum ( ) date_agg_3 . columns = ( 'bookings' , 'total' ) date_agg_3 . plot ( kind = 'bar' , stacked = 'True' , figsize = ( 16 , 10 ) )
542	probs = np . concatenate ( probs ) result_df = pd . DataFrame ( { 'row_id' : names , 'birds' : [ probs [ i ] for i in range ( probs . shape [ 0 ] ) ] } ) result_df = result_df . groupby ( 'row_id' ) [ 'birds' ] . apply ( lambda x : np . stack ( x ) . max ( 0 ) ) . reset_index ( ) result_df
203	PIXEL_MEAN = 0.25 def zero_center ( image ) : image = image - PIXEL_MEAN return image
719	variables = [ 'Target' , 'dependency' , 'warning' , 'walls+roof+floor' , 'meaneduc' , 'floor' , 'r4m1' , 'overcrowding' ] corr_mat = train_heads [ variables ] . corr ( ) . round ( 2 ) plt . rcParams [ 'font.size' ] = 18 plt . figure ( figsize = ( 12 , 12 ) ) sns . heatmap ( corr_mat , vmin = - 0.5 , vmax = 0.8 , center = 0 , cmap = plt . cm . RdYlGn_r , annot = True ) ;
1507	leak = pd . read_csv ( '../input/breaking-lb-fresh-start-with-lag-selection/train_leak.csv' ) data [ 'leak' ] = leak [ 'compiled_leak' ] . values data [ 'log_leak' ] = np . log1p ( leak [ 'compiled_leak' ] . values )
958	submission = pd . DataFrame . from_dict ( all_masks , orient = 'index' ) submission . index . names = [ 'id' ] submission . columns = [ 'rle_mask' ] submission . to_csv ( 'submission.csv' )
921	train_imgs , val_imgs = train_test_split ( selected_imgs , test_size = 0.15 , stratify = selected_imgs [ 'has_ship' ] , random_state = 69278 ) train_fnames = train_imgs [ 'ImageId' ] . values val_fnames = val_imgs [ 'ImageId' ] . values
1552	corr = train_df . iloc [ : , 190 : ] . corr ( ) sns_plot2 = sns . heatmap ( corr , xticklabels = corr . columns , yticklabels = corr . columns )
1539	def process_dataframe ( input_df , encoder_dict = None ) : print ( 'Label encoding categorical features...' ) categorical_feats = input_df . columns [ input_df . dtypes == 'object' ] for feat in categorical_feats : encoder = LabelEncoder ( ) input_df [ feat ] = encoder . fit_transform ( input_df [ feat ] . fillna ( 'NULL' ) ) print ( 'Label encoding complete.' ) return input_df , categorical_feats . tolist ( ) , encoder_dict
614	train_file = '../input/train.csv' test_file = '../input/test.csv' def read_training_data ( filepath ) : train_df = pd . read_csv ( filepath ) return train_df def read_test_data ( filepath ) : test_df = pd . read_csv ( filepath ) return test_df train_df = read_training_data ( train_file ) test_df = read_test_data ( test_file )
998	site_4 = pd . read_csv ( '../input/ucb-data-leakage-site-4/site4.csv' , parse_dates = [ 'timestamp' ] ) site_4 . columns = [ 'building_id' , 'timestamp' , 'meter_reading' ] site_4 [ 'meter' ] = 0 site_4 [ 'timestamp' ] = pd . DatetimeIndex ( site_4 [ 'timestamp' ] ) + timedelta ( hours = - 8 ) site_4 = site_4 [ site_4 [ 'timestamp' ] . dt . year > 2016 ]
469	y_test = random_search . predict_proba ( test_df [ predictors ] ) y_test . shape
1055	trainDF = pd . read_json ( '../input/train.json' ) testDF = pd . read_json ( '../input/test.json' ) print ( 'Training data dimensions:' , trainDF . shape ) print ( 'Testing data dimensions:' , testDF . shape )
310	df_data = pd . read_csv ( '../input/train_labels.csv' ) df_data [ df_data [ 'id' ] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' ] df_data [ df_data [ 'id' ] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' ] print ( df_data . shape )
774	corrs = data . corr ( ) corrs [ 'fare_amount' ] . plot . bar ( color = 'b' ) ; plt . title ( 'Correlation with Fare Amount' ) ;
1384	col = numeric_features [ 31 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
170	ip_level [ 'DL_by_click_ratio' ] = ip_level [ 'is_attributed' ] / ip_level [ 'freq_ip' ] print ( ip_level [ 'DL_by_click_ratio' ] . describe ( ) ) plt . figure ( figsize = ( 6 , 4 ) ) sns . violinplot ( ip_level [ 'DL_by_click_ratio' ] ) plt . title ( 'Ratio : Download by click' , fontsize = 15 ) plt . xlabel ( 'Download by click' )
528	params = { 'bagging_fraction' : 0.7982116702024386 , 'feature_fraction' : 0.1785051643813966 , 'max_depth' : int ( 49.17611603427576 ) , 'min_child_weight' : 3.2852905549011155 , 'min_data_in_leaf' : int ( 31.03480802715621 ) , 'n_estimators' : 5000 , 'num_leaves' : int ( 52.851307790411965 ) , 'reg_alpha' : 0.45963319421692145 , 'reg_lambda' : 0.6591286807489907 , 'metric' : 'auc' , 'boosting_type' : 'gbdt' , 'colsample_bytree' : .8 , 'subsample' : .9 , 'min_split_gain' : .01 , 'max_bin' : 127 , 'bagging_freq' : 5 , 'learning_rate' : 0.01 , 'early_stopping_rounds' : 100 }
29	AUCs = [ ] Ginis = [ ] for i in range ( 300 ) : AUC = roc_auc_score ( train . target . values , train [ str ( i ) ] . values ) AUCs . append ( AUC ) Gini = 2 * AUC - 1 Ginis . append ( Gini )
609	model = Sequential ( ) model . add ( Embedding ( max_features , embedding_dim , embeddings_initializer = tf . keras . initializers . Constant ( embedding_matrix ) , trainable = False ) ) model . add ( Dropout ( 0.2 ) )
251	dates_list = [ '2020-03-01' , '2020-03-02' , '2020-03-03' , '2020-03-04' , '2020-03-05' , '2020-03-06' , '2020-03-07' , '2020-03-08' , '2020-03-09' , '2020-03-10' , '2020-03-11' , '2020-03-12' , '2020-03-13' , '2020-03-14' , '2020-03-15' , '2020-03-16' , '2020-03-17' , '2020-03-18' , '2020-03-19' , '2020-03-20' , '2020-03-21' , '2020-03-22' , '2020-03-23' , '2020-03-24' , '2020-03-25' , '2020-03-26' , '2020-03-27' , '2020-03-28' , '2020-03-29' , '2020-03-30' , '2020-03-31' ]
1069	lwk = metrics . cohen_kappa_score ( y_true , y_pred , weights = 'linear' ) qwk = metrics . cohen_kappa_score ( y_true , y_pred , weights = 'quadratic' ) \ print ( "Linear Weighted Kappa Score:" , lwk ) print ( "Quadratic Weighted Kappa Score:" , qwk )
1158	logregModel = LogisticRegression ( ) params = { 'C' : np . logspace ( start = - 5 , stop = 3 , num = 9 ) } clf = GridSearchCV ( logregModel , params , scoring = 'neg_log_loss' , refit = True ) clf . fit ( X_train , y_train )
1178	files = folders = 0 path = "/kaggle/input/osic-pulmonary-fibrosis-progression/train" for _ , dirnames , filenames in os . walk ( path ) : files += len ( filenames ) folders += len ( dirnames ) print ( "{:,} files/images, {:,} folders/patients" . format ( files , folders ) )
1276	base_cols = [ 'EXT_SOURCE_1' , 'EXT_SOURCE_2' , 'EXT_SOURCE_3' , 'DAYS_BIRTH' , 'AMT_CREDIT' , 'AMT_ANNUITY' , 'DAYS_EMPLOYED' , 'AMT_GOODS_PRICE' , 'DAYS_ID_PUBLISH' , 'OWN_CAR_AGE' ] feature_cols = base_cols + bureau_cols_max + prev_agg_cols y = df . TARGET X = df [ feature_cols ] X = X . fillna ( value = X . mean ( ) ) X [ df . SK_ID_CURR == 365597 ] . transpose ( )
1170	train_sentences = data_train [ 'comment_text' ] . values . tolist ( ) test_sentences = data_test [ 'comment_text' ] . values . tolist ( ) total_ = copy . deepcopy ( train_sentences ) total_ . extend ( test_sentences ) print ( '[*]Training Sentences:' , len ( train_sentences ) ) print ( '[*]Test Sentences:' , len ( test_sentences ) ) print ( '[*]Total Sentences:' , len ( total_ ) ) for i in tqdm ( range ( len ( total_ ) ) ) : total_ [ i ] = str ( total_ [ i ] ) . lower ( )
540	corrMatt = data [ [ "bedrooms" , "bathrooms" , "price" ] ] . corr ( ) mask = np . array ( corrMatt ) mask [ np . tril_indices_from ( mask ) ] = False fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) sn . heatmap ( corrMatt , mask = mask , vmax = .8 , square = True , annot = True )
1086	of [ 'toxic' ] = mybest . toxic . values * 0.8 + of . toxic . values * 0.2 score1 = roc_auc_score ( mybest . toxic . round ( ) . astype ( int ) , of . toxic . values ) score2 = roc_auc_score ( of . toxic . round ( ) . astype ( int ) , mybest . toxic . values ) print ( '%2.4f\t%2.4f' % ( 100 * score1 , 100 * score2 ) ) print ( of . head ( ) ) of . to_csv ( 'submission.csv' , index = False )
1471	import pandas as pd import glob import os import subprocess as sp import tqdm . notebook as tqdm from collections import defaultdict import json
1155	import numpy as np import pandas as pd import os from sklearn . linear_model import LogisticRegression import matplotlib . pyplot as plt from sklearn . utils import shuffle from sklearn . model_selection import GridSearchCV from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
640	inds_to_shuffle = np . random . choice ( range ( len ( df_gt_simulated ) ) , int ( len ( df_gt_simulated ) * 0.6 ) ) df_pred_simulated [ inds_to_shuffle ] = np . random . permutation ( df_pred_simulated [ inds_to_shuffle ] ) cohen_kappa_score ( df_gt_simulated , df_pred_simulated , weights = 'quadratic' ) , accuracy_score ( df_gt_simulated , df_pred_simulated )
485	from sklearn . feature_extraction . text import TfidfVectorizer text = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] vectorizer = TfidfVectorizer ( ) vectorizer . fit ( text ) print ( sorted ( vectorizer . vocabulary_ ) ) vector = vectorizer . transform ( [ text [ 0 ] ] )
1109	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
799	model . n_estimators = len ( cv_results [ 'auc-mean' ] ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] baseline_auc = roc_auc_score ( test_labels , preds ) print ( 'The baseline model scores {:.5f} ROC AUC on the test set.' . format ( baseline_auc ) )
607	train_df = pd . read_csv ( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip' ) . fillna ( ' ' ) test_df = pd . read_csv ( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip' ) . fillna ( ' ' ) train_df . sample ( 10 )
70	t0 = time . time ( ) kopt ( opt = 2 , ext = 0 , fil = False ) print ( f'Time:\t{time.time()-t0:.2f}s' )
1159	preds = clf . predict_proba ( X_test ) [ : , 1 ] clipped_preds = np . clip ( preds , 0.05 , 0.95 ) df_sample_sub . Pred = clipped_preds
806	from hyperopt import Trials trials = Trials ( )
502	print ( 'Applicatoin train shape before merge: ' , ap_train . shape ) ap_train = ap_train . merge ( br_data , left_on = 'SK_ID_CURR' , right_on = 'SK_ID_CURR' , how = 'inner' ) print ( 'Applicatoin train shape after merge: ' , ap_train . shape )
118	print ( 'Number of data points: ' + str ( len ( train_df ) ) ) print ( '----------------------' ) for col in train_df . columns : print ( '{} : {} unique values, {} missing.' . format ( col , str ( len ( train_df [ col ] . unique ( ) ) ) , str ( train_df [ col ] . isna ( ) . sum ( ) ) ) )
647	from pathlib import Path if Path ( previous_model_name ) . is_file ( ) : print ( "Using previous sucessful run's model" ) model2 = load_model ( previous_model_name , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } ) else : print ( "Using stored trained model" ) model2 = load_model ( stored_trained_model , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } )
475	sub_df = pd . DataFrame ( { "ID_code" : test_df . ID_code . values } ) sub_df [ "target" ] = predictions sub_df [ : 10 ]
1157	df_wins = pd . DataFrame ( ) df_wins [ 'SeedDiff' ] = df_concat [ 'SeedDiff' ] df_wins [ 'result' ] = 1 df_losses = pd . DataFrame ( ) df_losses [ 'SeedDiff' ] = - df_concat [ 'SeedDiff' ] df_losses [ 'result' ] = 0 df_for_predictions = pd . concat ( ( df_wins , df_losses ) )
1030	def format_prediction_string ( image_id , result ) : prediction_strings = [ ] for i in range ( len ( result [ 'detection_scores' ] ) ) : class_name = result [ 'detection_class_names' ] [ i ] . decode ( "utf-8" ) YMin , XMin , YMax , XMax = result [ 'detection_boxes' ] [ i ] score = result [ 'detection_scores' ] [ i ] prediction_strings . append ( f"{class_name} {score} {XMin} {YMin} {XMax} {YMax}" ) prediction_string = " " . join ( prediction_strings ) return { "ImageID" : image_id , "PredictionString" : prediction_string }
960	X_test_pub = X_test . loc [ X_test . DateAvSigVersion <= '2018-10-25' , : ] X_test_priv = X_test . loc [ X_test . DateAvSigVersion > '2018-10-25' , : ] print ( 'public test shape: {}' . format ( X_test_pub . shape ) ) print ( 'private test shape: {}' . format ( X_test_priv . shape ) ) print ( 'fraction of private test split: {:.3f}' . format ( X_test_priv . shape [ 0 ] / X_test . shape [ 0 ] ) )
196	bg , = bulge_graph . BulgeGraph . from_fasta_text ( '>seq\n' + Sequence + '\n' + structure ) plt . figure ( figsize = ( 10 , 10 ) ) fvm . plot_rna ( bg , text_kwargs = { "fontweight" : "black" } , lighten = 0.7 , backbone_kwargs = { "linewidth" : 3 } ) plt . show ( )
1489	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 8 - Increased Vascular Markings + Enlarged Heart" ) draw ( parsed [ df [ 'patientId' ] [ 38 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 38 ] ] )
1263	PRETRAINED_MODELS = { "BERT" : [ 'bert-base-uncased' , 'bert-large-uncased' , 'bert-base-cased' , 'bert-large-cased' , 'bert-base-multilingual-uncased' , 'bert-base-multilingual-cased' , 'bert-base-chinese' , 'bert-base-german-cased' , 'bert-large-uncased-whole-word-masking' , 'bert-large-cased-whole-word-masking' , 'bert-large-uncased-whole-word-masking-finetuned-squad' , 'bert-large-cased-whole-word-masking-finetuned-squad' , 'bert-base-cased-finetuned-mrpc' ] , "DISTILBERT" : [ 'distilbert-base-uncased' , 'distilbert-base-uncased-distilled-squad' ] }
1374	col = numeric_features [ 21 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
792	features = list ( data . columns ) for f in [ 'pickup_datetime' , 'fare_amount' , 'fare-bin' , 'color' ] : features . remove ( f ) len ( features )
1449	temp = train [ 'ip' ] . value_counts ( ) . reset_index ( name = 'counts' ) temp . columns = [ 'ip' , 'counts' ] temp [ : 10 ]
18	train = pd . read_csv ( "../input/train.csv" ) test = pd . read_csv ( "../input/test.csv" ) print ( "{} observations and {} features in train set." . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( "{} observations and {} features in test set." . format ( test . shape [ 0 ] , test . shape [ 1 ] ) )
205	columns = [ i for i in data . columns ] dummies = pd . get_dummies ( data , columns = columns , drop_first = True , sparse = True ) del data
1175	link_count = np . zeros ( [ len ( title_dic ) , len ( title_dic ) ] , dtype = np . int ) node_count = np . zeros ( [ len ( title_dic ) ] ) for i in tqdm ( train . index ) : link_count [ train . loc [ i , 'previous_title' ] ] [ train . loc [ i , 'title' ] ] += 1 node_count [ train . loc [ i , 'title' ] ] += 1
711	plt . figure ( figsize = ( 10 , 6 ) ) sns . violinplot ( x = 'warning' , y = 'Target' , data = heads ) ; plt . title ( 'Target vs Warning Variable' ) ;
316	test_path = 'test_dir' test_gen = datagen . flow_from_directory ( test_path , target_size = ( IMAGE_SIZE , IMAGE_SIZE ) , batch_size = 1 , class_mode = 'categorical' , shuffle = False )
636	dftrainall = dftrain . join ( pop , on = 'Country_Region' ) dftrainall [ 'Lockdown' ] = dftrainlockdown [ 'Lockdown' ] dftrainall = dftrainall . join ( flights , on = 'Country_Region' ) dftrainall [ 'Mortality' ] = dftrainall [ 'Fatalities' ] / dftrainall [ 'ConfirmedCases' ] dftrainall [ 'ConfirmedCases_by_pop' ] = dftrainall [ 'ConfirmedCases' ] / dftrainall [ 'Population (2020)' ] dftrainall [ 'ConfirmedCases_by_Km' ] = dftrainall [ 'ConfirmedCases' ] / dftrainall [ 'Land Area (Km)' ] dftrainall . tail ( )
1323	for col1 in [ 'area1' , 'area2' ] : for col2 in [ 'instlevel1' , 'instlevel2' , 'instlevel3' , 'instlevel4' , 'instlevel5' , 'instlevel6' , 'instlevel7' , 'instlevel8' , 'instlevel9' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
449	plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( 'year_built' ) . building_id . sum ( ) . reset_index ( ) ax = sns . lineplot ( data = temp_df , x = 'year_built' , y = 'building_id' , color = 'black' , linewidth = 3.5 ) plt . xlabel ( 'Year Built' , fontsize = 15 ) plt . ylabel ( 'Building_ID' , fontsize = 15 ) plt . show ( )
403	ttf_diff = train_df [ 'time_to_failure' ] . diff ( ) ttf_diff = ttf_diff . loc [ ttf_diff > 0 ] print ( ttf_diff . index )
868	correlations = pd . read_csv ( '../input/home-credit-default-risk-feature-tools/correlations_spec.csv' , index_col = 0 ) correlations . index . name = 'Variable' correlations . head ( )
747	trials = Trials ( ) OUT_FILE = 'optimization.csv' of_connection = open ( OUT_FILE , 'w' ) writer = csv . writer ( of_connection ) MAX_EVALS = 100 PROGRESS = 10 N_FOLDS = 5 ITERATION = 0 headers = [ 'loss' , 'hyperparameters' , 'iteration' , 'runtime' , 'score' , 'std' ] writer . writerow ( headers ) of_connection . close ( )
989	colors = vtk . vtkNamedColors ( ) bkg = map ( lambda x : x / 255.0 , [ 26 , 51 , 102 , 255 ] ) colors . SetColor ( "BkgColor" , * bkg )
1098	for task , prediction , solved in tqdm ( zip ( train_tasks , train_predictions , train_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
782	from sklearn . ensemble import RandomForestRegressor random_forest = RandomForestRegressor ( n_estimators = 20 , max_depth = 20 , max_features = None , oob_score = True , bootstrap = True , verbose = 1 , n_jobs = - 1 ) random_forest . fit ( X_train [ [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train )
910	train_labels = train [ 'TARGET' ] train , test = train . align ( test , join = 'inner' , axis = 1 ) train [ 'TARGET' ] = train_labels
1284	def model_to_use ( median , holiday_log , yearly_log ) : result = median if ( median * 1 > yearly_log ) : result = yearly_log elif ( median * 1 > holiday_log ) : result = holiday_log return result model_score = valid_score_data . apply ( lambda x : model_to_use ( x [ 'median14' ] , x [ 'holiday_log' ] , x [ 'yearly_log' ] ) , axis = 1 ) print ( "Validation score for a proposed model is: %.6f" % validation_score ( model_score ) ) model_score . plot . hist ( bins = 40 )
751	from umap import UMAP from sklearn . decomposition import PCA , FastICA from sklearn . manifold import TSNE n_components = 3 umap = UMAP ( n_components = n_components ) pca = PCA ( n_components = n_components ) ica = FastICA ( n_components = n_components ) tsne = TSNE ( n_components = n_components )
1115	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
1139	import matplotlib . pyplot as plt def plotImages ( images_arr ) : fig , axes = plt . subplots ( 1 , 5 , figsize = ( 20 , 20 ) ) axes = axes . flatten ( ) for img , ax in zip ( images_arr , axes ) : ax . imshow ( img ) plt . tight_layout ( ) plt . show ( ) augmented_images = [ train_generator [ 0 ] [ 0 ] [ 0 ] for i in range ( 5 ) ] plotImages ( augmented_images )
1291	for c in test_df . columns : if c == 'mo_ye' : continue if test_df [ c ] . dtype == 'object' : lbl = preprocessing . LabelEncoder ( ) lbl . fit ( list ( test_df [ c ] . values ) ) test_df [ c ] = lbl . transform ( list ( test_df [ c ] . values ) ) test_df [ 'mo_ye' ] = test_df [ 'mo_ye' ] . apply ( lambda x : 100 * pd . to_datetime ( x ) . year + pd . to_datetime ( x ) . month )
1108	category_cols = [ 'building_id' , 'site_id' , 'primary_use' ] feature_cols = [ 'square_feet' , 'year_built' ] + [ 'hour' , 'weekend' , 'building_median' ] + [ 'air_temperature' , 'cloud_coverage' , 'dew_temperature' , 'precip_depth_1_hr' , 'sea_level_pressure' , 'wind_direction' , 'wind_speed' , ]
453	train [ 'year_built' ] = np . uint8 ( train [ 'year_built' ] - 1900 , inplace = True ) test [ 'year_built' ] = np . uint8 ( test [ 'year_built' ] - 1900 , inplace = True )
1493	import numpy as np import pandas as pd import itertools import random import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' training_tasks = sorted ( os . listdir ( training_path ) )
1472	plate_groups = np . zeros ( ( 1108 , 4 ) , int ) for sirna in range ( 1108 ) : grp = train_csv . loc [ train_csv . sirna == sirna , : ] . plate . value_counts ( ) . index . values assert len ( grp ) == 3 plate_groups [ sirna , 0 : 3 ] = grp plate_groups [ sirna , 3 ] = 10 - grp . sum ( ) plate_groups [ : 10 , : ]
861	train_set = lgb . Dataset ( train , label = train_labels ) hyperparameters = dict ( ** random_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS )
248	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from sklearn import preprocessing import time from datetime import datetime from scipy import integrate , optimize import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm as lgb import xgboost as xgb from xgboost import plot_importance , plot_tree from sklearn . model_selection import RandomizedSearchCV , GridSearchCV from sklearn import linear_model from sklearn . metrics import mean_squared_error from sklearn . linear_model import Ridge , RidgeCV
96	df_variants_train = pd . read_csv ( '../input/training_variants' , usecols = [ 'Gene' , 'Variation' , 'Class' ] ) df_text_train = pd . read_csv ( '../input/training_text' , sep = '\|\|' , engine = 'python' , skiprows = 1 , names = [ 'ID' , 'Text' ] ) df_variants_train [ 'Text' ] = df_text_train [ 'Text' ] df_train = df_variants_train
949	merchant_card_id_cat = merchant . groupby ( [ 'merchant_id' ] ) [ merchant_cat_feats ] . agg ( aggs_cat_basic ) merchant_card_id_num = merchant . groupby ( [ 'merchant_id' ] ) [ merchant_num_feats ] . agg ( aggs_num_basic ) merchant_card_id_cat = rename_columns ( merchant_card_id_cat ) merchant_card_id_num = rename_columns ( merchant_card_id_num )
1280	import re def breakdown_topic ( str ) : m = re . search ( '(.*)\_(.*).wikipedia.org\_(.*)\_(.*)' , str ) if m is not None : return m . group ( 1 ) , m . group ( 2 ) , m . group ( 3 ) , m . group ( 4 ) else : return "" , "" , "" , "" print ( breakdown_topic ( ",___ru.wikipedia.org_all-access_spider" ) ) print ( breakdown_topic ( "_zh.wikipedia.org_all-access_spider" ) ) print ( breakdown_topic ( "File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents" ) )
1144	CATEGORICAL_COLS = [ "card_id" , "category_3" , "merchant_id" ] for col in [ "card_id" , "category_3" , "merchant_id" ] : df [ col ] = df [ col ] . astype ( "category" )
1376	col = numeric_features [ 23 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1038	model_public = build_model ( seq_len = 107 , pred_len = 107 , embed_size = len ( token2int ) ) model_private = build_model ( seq_len = 130 , pred_len = 130 , embed_size = len ( token2int ) ) model_public . load_weights ( 'model.h5' ) model_private . load_weights ( 'model.h5' )
362	print ( 'ok' )
1224	col_to_drop = train . columns [ train . columns . str . startswith ( 'ps_calc_' ) ] train . drop ( col_to_drop , axis = 1 , inplace = True ) test . drop ( col_to_drop , axis = 1 , inplace = True )
677	hits_sample = hits . sample ( 8000 ) sns . pairplot ( hits_sample , hue = 'volume_id' , size = 8 ) plt . show ( )
1333	len_train = train_df . shape [ 0 ] df_all = pd . concat ( [ train_df , test_df ] )
1453	import pickle def load_obj ( filename ) : with open ( filename , 'rb' ) as f : return pickle . load ( f ) df_train = load_obj ( '../input/trackml-validation-data-for-ml-pandas-df/df_train_v1.pkl' ) df_test = load_obj ( '../input/trackml-validation-data-for-ml-pandas-df/df_test_v1.pkl' ) y_train = df_train . target . values y_test = df_test . target . values print ( "The dataframe with all features:" ) display ( df_train . head ( ) ) print ( "Features for each track:" , df_train . columns . values )
672	parent_category = dff . parent_category_name category = dff . category_name price_log = np . log ( dff . price ) plt . figure ( figsize = ( 20 , 8 ) ) ax = sns . violinplot ( x = parent_category , y = price_log , scale = 'width' , palette = 'Set3' ) plt . ylim ( [ 1 , 15 ] ) plt . xticks ( rotation = 40 , fontsize = 14 ) plt . title ( 'Price variance within parent categories' ) plt . ylabel ( 'Log(price) \u20BD' ) plt . show ( )
1386	col = numeric_features [ 34 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
847	import random random . seed ( 50 ) boosting_type = random . sample ( param_grid [ 'boosting_type' ] , 1 ) [ 0 ] subsample = 1.0 if boosting_type == 'goss' else random . sample ( param_grid [ 'subsample' ] , 1 ) [ 0 ] print ( 'Boosting type: ' , boosting_type ) print ( 'Subsample ratio: ' , subsample )
440	bold ( '**SUNDAYS HAVE THE LOWEST READINGS**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) ax = sns . boxplot ( data = train , x = 'weekday_name' , y = 'meter_reading' , color = 'teal' , boxprops = dict ( alpha = .3 ) ) ax . set_ylabel ( 'Log(Meter Reading)' , fontsize = 20 ) ax . set_xlabel ( 'weekdays' , fontsize = 20 ) plt . show ( )
1088	[ x1 , x2 , x3 , x4 ] = sess . run ( [ num_frames , video_matrix , batch_labels , batch_frames ] ) [ z1 , z2 ] = sess . run ( [ labels , num_frames ] ) vid_byte = sess . run ( batch_video_ids ) vid = vid_byte [ 0 ] . decode ( ) print ( 'vid = %s' % vid )
1573	lagged_basic = lagged [ [ 'date' , 'Visits' , 'weekday' ] ] lagged_basic_tr = lagged_basic [ lagged_basic [ 'date' ] < last_date ] lagged_basic_pred = lagged_basic [ lagged_basic [ 'date' ] >= last_date ] lagged_basic_pred . drop ( 'Visits' , inplace = True , axis = 1 )
1104	category_cols = [ 'building_id' , 'site_id' , 'primary_use' ] feature_cols = [ 'square_feet' , 'year_built' ] + [ 'hour' , 'weekend' , 'building_median' ] + [ 'air_temperature' , 'cloud_coverage' , 'dew_temperature' , 'precip_depth_1_hr' , 'sea_level_pressure' , 'wind_direction' , 'wind_speed' , ]
745	plt . rcParams [ 'font.size' ] = 18 g = sns . FacetGrid ( predictions , row = 'fold' , hue = 'Target' , size = 3 , aspect = 4 ) g . map ( sns . kdeplot , 'confidence' ) ; g . add_legend ( ) ; plt . suptitle ( 'Distribution of Confidence by Fold and Target' , y = 1.05 ) ;
1022	n_steps = x_train . shape [ 0 ] // BATCH_SIZE train_history = model . fit ( train_dataset , steps_per_epoch = n_steps , validation_data = valid_dataset , epochs = EPOCHS )
43	x = np . unique ( train [ 'question_asker_intent_understanding' ] . values , return_counts = True ) [ 0 ] y = np . unique ( train [ 'question_asker_intent_understanding' ] . values , return_counts = True ) [ 1 ] plt . bar ( x , y , align = 'center' , width = 0.05 )
722	ind [ 'escolari/age' ] = ind [ 'escolari' ] / ind [ 'age' ] plt . figure ( figsize = ( 10 , 8 ) ) sns . violinplot ( 'Target' , 'escolari/age' , data = ind ) ;
158	import numpy as np import pandas as pd import os print ( os . listdir ( "../input" ) )
365	plt_st ( 20 , 20 ) plt . imshow ( complete_images [ 0 ] ) plt . title ( "Training dataset of type %i" % ( 0 ) )
864	primitives = ft . list_primitives ( ) pd . options . display . max_colwidth = 100 primitives [ primitives [ 'type' ] == 'aggregation' ] . head ( 10 )
1567	train_data , train_labels = get_data_labels ( DATAPATH / 'train.csv' , 'label' ) test_data , test_labels = get_data_labels ( DATAPATH / 'test.csv' , 'id' ) other_data , other_labels = get_data_labels ( DATAPATH / 'Dig-MNIST.csv' , 'label' )
704	x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_ from collections import Counter print ( 'There are no repeats: ' , np . all ( np . array ( list ( Counter ( x ) . values ( ) ) ) == 1 ) ) print ( 'We covered every variable: ' , len ( x ) == data . shape [ 1 ] )
820	import pandas as pd import numpy as np import featuretools as ft import matplotlib . pyplot as plt plt . rcParams [ 'font.size' ] = 22 import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm as lgb from sklearn . model_selection import train_test_split from sklearn . model_selection import KFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import LabelEncoder import gc
1542	plot1 = train1 [ 0 : 150000 ] . time_to_failure plot2 = train1 [ 0 : 150000 ] . acoustic_data
1064	def load_img ( code , base , resize = True ) : path = f'{base}/{code}' img = cv2 . imread ( path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) if resize : img = cv2 . resize ( img , ( 256 , 256 ) ) return img def validate_path ( path ) : if not os . path . exists ( path ) : os . makedirs ( path )
430	from sklearn . preprocessing import LabelEncoder features = [ c for c in train_df . columns if c not in [ "ID_code" , "target" ] ] lbl_enc = LabelEncoder ( ) lbl_enc . fit ( train_df [ features ] ) df_Train_cat = lbl_enc . transform ( train_df [ features ] )
780	lr . fit ( X_train [ [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train ) evaluate ( lr , [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] , X_train , X_valid , y_train , y_valid )
1189	x_train_full2 = np . square ( x_train_full ) x_test_full2 = np . square ( x_test_full ) x_sub_full2 = np . square ( x_sub_full )
1010	if not os . path . isdir ( save_dir ) : os . makedirs ( save_dir ) model_path = os . path . join ( save_dir , model_name ) model . save ( model_path ) print ( 'Saved trained model at %s ' % model_path )
413	subm_datagen = DataGenOsic ( df = subm , tab_cols = feat_cols , batch_size = BATCH_SIZE_PRED , mode = 'predict' , shuffle = False , aug = None , resize = RESIZE , masked = MASKED , cutoff = CUTOFF , seq_len = SEQ_LEN , img_size = IMG_SIZE )
1190	def md_learning_rate ( val ) : if val == 0 : return 10 elif val == 1 : return 8 else : return 5 / np . log ( val )
971	plt . rcParams [ 'figure.figsize' ] = ( 10 , 10 ) X_tr_temp , y_tr_temp = tr_datagen . __getitem__ ( 0 ) X_valid_temp , y_valid_temp = valid_datagen . __getitem__ ( 0 ) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . imshow ( X_tr_temp [ 0 ] ) ax [ 0 ] . set_title ( 'train:' ) ax [ 1 ] . imshow ( X_valid_temp [ 0 ] ) ax [ 1 ] . set_title ( 'valid:' )
504	PARENT_DATA_DIR_PATH = '../input' METADATA_TRAIN_FILE_PATH = os . path . join ( PARENT_DATA_DIR_PATH , "metadata_train.csv" ) TRAIN_DATA_FILE_PATH = os . path . join ( PARENT_DATA_DIR_PATH , "train.parquet" )
12	train = pd . read_csv ( '../input/quora-insincere-questions-classification/train.csv' ) . fillna ( ' ' ) test = pd . read_csv ( '../input/quora-insincere-questions-classification/test.csv' ) . fillna ( ' ' ) test_qid = test [ 'qid' ] train_qid = train [ 'qid' ] train_target = train [ 'target' ] . values train_text = train [ 'question_text' ] test_text = test [ 'question_text' ] all_text = pd . concat ( [ train_text , test_text ] )
8	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) hist_trans = pd . read_csv ( '../input/historical_transactions.csv' ) new_merchant_trans = pd . read_csv ( '../input/new_merchant_transactions.csv' )
387	for item in TRAIN_DB : break print ( type ( item ) , list ( item . keys ( ) ) ) print ( item [ '_id' ] , len ( item [ 'imgs' ] ) , item [ 'category_id' ] , )
766	def ecdf ( x ) : x = np . sort ( x ) n = len ( x ) y = np . arange ( 1 , n + 1 , 1 ) / n return x , y
631	df = ( pd . merge ( agg . reset_index ( ) , products , on = 'Producto_ID' , how = 'left' ) . groupby ( 'short_name' ) [ 'Demanda_uni_equil_sum' , 'Venta_uni_hoy_sum' , 'Dev_uni_proxima_sum' , 'Dev_uni_proxima_count' ] . sum ( ) . sort_values ( by = 'Demanda_uni_equil_sum' , ascending = False ) )
286	n = 15 commits_df . loc [ n , 'commit_num' ] = 21 commits_df . loc [ n , 'Dropout_model' ] = 0.38 commits_df . loc [ n , 'FVC_weight' ] = 0.19 commits_df . loc [ n , 'LB_score' ] = - 6.8093
398	import sys import scipy import matplotlib import numpy as np import pandas as pd print ( sys . version ) print ( 'pandas:' , pd . __version__ ) print ( 'numpy:' , np . __version__ ) print ( 'scipy:' , scipy . __version__ ) print ( 'matplotlib:' , matplotlib . __version__ ) print ( 'ok' )
1570	import pandas as pd import numpy as np import warnings import scipy from datetime import timedelta from pylab import rcParams import statsmodels . api as sm from statsmodels . tsa . stattools import adfuller from statsmodels . tsa . tsatools import lagmat from sklearn . linear_model import LinearRegression , RidgeCV from sklearn . ensemble import RandomForestRegressor from sklearn . metrics import r2_score import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'fivethirtyeight' ) warnings . filterwarnings ( 'ignore' )
339	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
1184	import numpy as np import pandas as pd import os
501	corrmat = application_train . corr ( ) top_corr_features = corrmat . index [ abs ( corrmat [ "TARGET" ] ) >= 0.03 ] plt . figure ( figsize = ( 20 , 10 ) ) g = sns . heatmap ( application_train [ top_corr_features ] . corr ( ) , annot = True , cmap = "Oranges" )
881	plt . figure ( figsize = ( 8 , 7 ) ) plt . plot ( random_hyp [ 'learning_rate' ] , random_hyp [ 'n_estimators' ] , 'ro' ) plt . xlabel ( 'Learning Rate' ) ; plt . ylabel ( 'N Estimators' ) ; plt . title ( 'Number of Estimators vs Learning Rate' ) ;
938	gbm_params = { 'objective' : 'binary' , 'boosting_type' : 'gbdt' , 'nthread' : 6 , 'learning_rate' : 0.05 , 'num_leaves' : 20 , 'colsample_bytree' : 0.9497036 , 'subsample' : 0.8715623 , 'subsample_freq' : 1 , 'max_depth' : 8 , 'reg_alpha' : 0.041545473 , 'reg_lambda' : 0.0735294 , 'min_split_gain' : 0.0222415 , 'min_child_weight' : 60 , 'seed' : 0 , 'verbose' : - 1 , 'metric' : 'auc' , } oof_train , oof_test = run_kfold_lgbm ( X_train , y_train , X_test , gbm_params )
788	X_train , X_valid , y_train , y_valid = train_test_split ( data , np . array ( data [ 'fare_amount' ] ) , stratify = data [ 'fare-bin' ] , random_state = RSEED , test_size = 1_000_000 )
1492	import numpy as np import pandas as pd import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors import numpy as np
931	def rle_encode ( im ) : pixels = im . flatten ( order = 'F' ) pixels = np . concatenate ( [ [ 0 ] , pixels , [ 0 ] ] ) runs = np . where ( pixels [ 1 : ] != pixels [ : - 1 ] ) [ 0 ] + 1 runs [ 1 : : 2 ] -= runs [ : : 2 ] return ' ' . join ( str ( x ) for x in runs )
828	train = train . drop ( columns = zero_features ) test = test . drop ( columns = zero_features ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
124	import pydicom from glob import glob import scipy . ndimage from skimage import morphology from skimage import measure from skimage . filters import threshold_otsu , median from scipy . ndimage import binary_fill_holes from skimage . segmentation import clear_border from scipy . stats import describe
1075	train_label = train_df [ 'label' ] test_indices = test_df [ 'id' ] train_df = train_df . drop ( [ 'label' ] , axis = 1 ) test_df = test_df . drop ( [ 'id' ] , axis = 1 ) train_x = train_df . values train_y = train_label . values test_x = test_df . values print ( "shape of train_x :" , train_x . shape ) print ( "shape of train_y :" , train_y . shape ) print ( "shape of test_x :" , test_x . shape )
180	for label_ind , label_coords in enumerate ( ndimage . find_objects ( labels ) ) : cell = im_gray [ label_coords ] if np . product ( cell . shape ) < 10 : print ( 'Label {} is too small! Setting to 0.' . format ( label_ind ) ) mask = np . where ( labels == label_ind + 1 , 0 , mask ) labels , nlabels = ndimage . label ( mask ) print ( 'There are now {} separate components / objects detected.' . format ( nlabels ) )
626	date_agg_1 = train_agg . groupby ( level = 0 ) . agg ( [ 'sum' ] ) date_agg_1 . columns = ( 'bookings' , 'total' ) date_agg_1 . head ( )
5	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . target . values , bins = 200 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
7	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . feature_1 . values , bins = 200 ) plt . title ( 'Histogram feature_1 counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
710	heads [ 'warning' ] = 1 * ( heads [ 'sanitario1' ] + ( heads [ 'elec' ] == 0 ) + heads [ 'pisonotiene' ] + heads [ 'abastaguano' ] + ( heads [ 'cielorazo' ] == 0 ) )
515	def normalize ( image ) : MIN_BOUND = 0.0 MAX_BOUND = 255.0 image = ( image - MIN_BOUND ) / ( MAX_BOUND - MIN_BOUND ) image [ image > 1 ] = 1. image [ image < 0 ] = 0. return image
1562	text = list ( train . text . values ) tf_vectorizer = LemmaCountVectorizer ( max_df = 0.95 , min_df = 2 , stop_words = 'english' , decode_error = 'ignore' ) tf = tf_vectorizer . fit_transform ( text ) print ( tf_vectorizer . get_feature_names ( ) )
64	tsne = TSNE ( n_components = 2 ) train_2D = tsne . fit_transform ( df [ features ] . values )
712	heads [ 'bonus' ] = 1 * ( heads [ 'refrig' ] + heads [ 'computer' ] + ( heads [ 'v18q1' ] > 0 ) + heads [ 'television' ] ) sns . violinplot ( 'bonus' , 'Target' , data = heads , figsize = ( 10 , 6 ) ) ; plt . title ( 'Target vs Bonus Variable' ) ;
891	time_features , time_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , trans_primitives = [ 'cum_sum' , 'time_since_previous' ] , max_depth = 2 , agg_primitives = [ 'trend' ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
378	etr = ExtraTreesRegressor ( ) etr . fit ( train , target ) acc_model ( 12 , etr , train , test )
1214	class Net ( nn . Module ) : def __init__ ( self , num_classes ) : super ( ) . __init__ ( ) self . model = EfficientNet . from_name ( 'efficientnet-b0' ) self . dense_output = nn . Linear ( 1280 , num_classes ) def forward ( self , x ) : feat = self . model . extract_features ( x ) feat = F . avg_pool2d ( feat , feat . size ( ) [ 2 : ] ) . reshape ( - 1 , 1280 ) return self . dense_output ( feat )
1405	df [ 'VMA_7MA' ] = df [ 'volume' ] . rolling ( window = 7 ) . mean ( ) df [ 'VMA_15MA' ] = df [ 'volume' ] . rolling ( window = 15 ) . mean ( ) df [ 'VMA_30MA' ] = df [ 'volume' ] . rolling ( window = 30 ) . mean ( ) df [ 'VMA_60MA' ] = df [ 'volume' ] . rolling ( window = 60 ) . mean ( )
637	def create_lags ( df , maxshift ) : shifts = np . arange ( 1 , maxshift + 1 ) many_shifts = { 'lag_{}' . format ( ii ) : df . shift ( ii ) for ii in shifts } many_shifts = pd . DataFrame ( many_shifts ) . fillna ( 0. ) return many_shifts
1540	all_data_na = ( feature_matrix_enc . isnull ( ) . sum ( ) / len ( feature_matrix_enc ) ) * 100 all_data_na = all_data_na . drop ( all_data_na [ all_data_na == 0 ] . index ) . sort_values ( ascending = False ) [ : 30 ] missing_data = pd . DataFrame ( { 'Missing Ratio' : all_data_na } ) missing_data . head ( 20 )
427	CALENDAR_DTYPES = { 'date' : 'str' , 'wm_yr_wk' : 'int16' , 'weekday' : 'object' , 'wday' : 'int16' , 'month' : 'int16' , 'year' : 'int16' , 'd' : 'object' , 'event_name_1' : 'object' , 'event_type_1' : 'object' , 'event_name_2' : 'object' , 'event_type_2' : 'object' , 'snap_CA' : 'int16' , 'snap_TX' : 'int16' , 'snap_WI' : 'int16' } PARSE_DATES = [ 'date' ] SPRICES_DTYPES = { 'store_id' : 'object' , 'item_id' : 'object' , 'wm_yr_wk' : 'int16' , 'sell_price' : 'float32' }
1477	seed = 42 random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . backends . cudnn . deterministic = True if torch . cuda . is_available ( ) : torch . cuda . manual_seed_all ( seed )
1462	model = make_yolov3_model ( ) weight_reader = WeightReader ( '../input/yoloweight/yolov3.weights' ) weight_reader . load_weights ( model ) model . save ( 'model.h5' )
55	train_zeros = pd . DataFrame ( { 'Percentile' : ( ( train_df [ columns_to_use ] . values ) == 0 ) . mean ( axis = 0 ) , 'Column' : columns_to_use } ) train_zeros . head ( )
879	fig = plt . figure ( figsize = ( 10 , 10 ) ) ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( random_hyp [ 'reg_alpha' ] , random_hyp [ 'reg_lambda' ] , random_hyp [ 'score' ] , c = random_hyp [ 'score' ] , cmap = plt . cm . seismic_r , s = 40 ) ax . set_xlabel ( 'Reg Alpha' ) ax . set_ylabel ( 'Reg Lambda' ) ax . set_zlabel ( 'Score' ) plt . title ( 'Score as Function of Reg Lambda and Alpha' ) ;
268	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
597	import numpy as np test = pd . read_csv ( "../input/train.csv" ) length = len ( test ) np . random . seed ( 287 ) perfect_sub = np . random . rand ( length ) target = ( perfect_sub > 0.963552 ) . astype ( dtype = int ) print ( "Perfect submission looks like: " , perfect_sub ) print ( "Target vector looks like: " , target ) print ( "Target vector class distibution: " ) counts = pd . Series ( target ) . value_counts ( ) counts / counts . sum ( )
223	n = 4 commits_df . loc [ n , 'commit_num' ] = 6 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 384 commits_df . loc [ n , 'hidden_dim_second' ] = 128 commits_df . loc [ n , 'hidden_dim_third' ] = 128 commits_df . loc [ n , 'LB_score' ] = 0.25989
743	plt . plot ( selector . grid_scores_ ) ; plt . xlabel ( 'Number of Features' ) ; plt . ylabel ( 'Macro F1 Score' ) ; plt . title ( 'Feature Selection Scores' ) ; selector . n_features_
258	svr = SVR ( ) svr . fit ( train , target ) acc_model ( 1 , svr , train , test )
940	aggs1 = [ 'mean' , 'median' , 'min' , 'max' , 'count' , 'std' , 'sem' , 'sum' , 'mad' ] aggs2 = [ 'mean' , 'median' , 'min' , 'max' , 'count' , 'std' , 'sem' , 'sum' ] aggs_num_basic = [ 'mean' , 'min' , 'max' , 'std' , 'sem' , 'sum' ] aggs_cat_basic = [ 'mean' , 'std' , 'sum' ] aggs_num = aggs_num_basic aggs_cat = aggs_cat_basic
296	finish_data = 2014 lgb_num_leaves_max = 200 lgb_in_leaf = 10 lgb_lr = 0.001 lgb_bagging = 7 xgb_max_depth = 7 xgb_min_child_weight = 75 xgb_lr = 0.0004 xgb_num_boost_round_max = 3000 w_lgb = 0.4 w_xgb = 0.5 w_logreg = 1 - w_lgb - w_xgb w_logreg
933	X_tr , X_val , y_tr , y_val , cov_train , cov_test , depth_train , depth_test = train_test_split ( X_train , y_train , train_df . coverage . values , train_df . z . values , test_size = 0.2 , stratify = train_df . coverage_class , random_state = 1234 ) del train_df gc . collect ( ) del X_train , y_train gc . collect ( )
600	first30percent = int ( length * 0.3 ) target_pub = target [ : first30percent ] target_prv = target [ first30percent : ] def evaluate ( submission ) : return gini ( target_pub , submission [ : first30percent ] ) , \ gini ( target_prv , submission [ first30percent : ] )
1373	col = numeric_features [ 20 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
1546	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
130	def count_words_from ( series ) : sentences = series . str . split ( ) vocab = { } for sentence in tqdm ( sentences ) : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
1519	from matplotlib import pyplot from mpl_toolkits . mplot3d import Axes3D rcParams [ 'figure.figsize' ] = 30 , 20 fig = pyplot . figure ( ) ax = Axes3D ( fig ) ax . scatter ( tsne_representation3d . loc [ : , 'First_col' ] , tsne_representation3d . loc [ : , 'Second_col' ] , tsne_representation3d . loc [ : , 'Third_col' ] , s = 29 , c = tsne_representation3d . loc [ : , 'Target' ] , edgecolors = 'black' ) ax . set_title ( 't-SNE visualization in 3 dimensions' , size = 20 ) pyplot . show ( )
800	learning_rate_dist = [ ] for _ in range ( 10000 ) : learning_rate_dist . append ( sample ( learning_rate ) [ 'learning_rate' ] ) plt . figure ( figsize = ( 8 , 6 ) ) sns . kdeplot ( learning_rate_dist , color = 'red' , linewidth = 2 , shade = True ) ; plt . title ( 'Learning Rate Distribution' , size = 18 ) ; plt . xlabel ( 'Learning Rate' , size = 16 ) ; plt . ylabel ( 'Density' , size = 16 ) ;
499	val_p = [ 'APARTMENTS_AVG' , 'BASEMENTAREA_AVG' , 'YEARS_BEGINEXPLUATATION_AVG' , 'YEARS_BUILD_AVG' , 'COMMONAREA_AVG' , 'ELEVATORS_AVG' , 'ENTRANCES_AVG' , 'FLOORSMAX_AVG' , 'FLOORSMIN_AVG' ] for i in val_p : plt . figure ( figsize = ( 5 , 5 ) ) sns . distplot ( application_train [ i ] . dropna ( ) , kde = True , color = 'g' ) plt . title ( i ) plt . xticks ( rotation = - 45 ) plt . show ( )
869	features_sample = pd . read_csv ( '../input/home-credit-default-risk-feature-tools/feature_matrix.csv' , nrows = 20000 ) features_sample = features_sample [ features_sample [ 'set' ] == 'train' ] features_sample . head ( )
661	nom_cols = [ f'nom_{i}' for i in range ( 5 , 10 ) ] fig , ax = plt . subplots ( 5 , 1 , figsize = ( 22 , 17 ) ) for i , col in enumerate ( nom_cols ) : plt . subplot ( 5 , 1 , i + 1 ) sns . countplot ( raw_train [ col ] ) plt . show ( )
1584	image_df [ 'host' ] = image_df [ 'filename' ] . apply ( lambda st : st . strip ( 'images/host-' ) . split ( '_' ) [ 0 ] ) image_df [ 'cam' ] = image_df [ 'filename' ] . apply ( lambda st : st . split ( '_' ) [ 1 ] ) image_df [ 'timestamp' ] = image_df [ 'filename' ] . apply ( lambda st : st . split ( '_' ) [ 2 ] . strip ( '.jpeg' ) )
167	IP = df [ 'ip' ] . value_counts ( ) plt . figure ( figsize = [ 10 , 5 ] ) sns . boxplot ( IP ) plt . title ( 'Number of click by IP' , fontsize = 15 )
461	df_train = pd . concat ( [ df_train , pd . get_dummies ( df_train [ 'City' ] , drop_first = False ) ] , axis = 1 ) df_test = pd . concat ( [ df_test , pd . get_dummies ( df_test [ 'City' ] , drop_first = False ) ] , axis = 1 )
361	sample_wts = np . sqrt ( np . array ( [ x - 10.0 if x > 10.0 else 0 for x in y ] ) + 1.0 ) print ( y [ 0 : 16 ] ) print ( sample_wts [ - 8 : ] ) print ( 'ok' )
928	train [ 'len' ] = train [ 'comment_text' ] . str . len ( ) print ( 'Average comment length: %d' % train [ 'len' ] . mean ( ) ) print ( 'Median comment length: %d' % train [ 'len' ] . quantile ( .5 ) ) print ( '90th percentile comment length: %d' % train [ 'len' ] . quantile ( .9 ) )
1251	n_iter = 100 start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_grid_mask ( images ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_grid_mask: {timing}" )
538	fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 13 , 8 ) sn . countplot ( x = "bathrooms" , data = data , ax = ax1 ) data1 = data . groupby ( [ 'bathrooms' , 'interest_level' ] ) [ 'bathrooms' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) data1 [ [ 'low' , 'medium' , "high" ] ] . plot ( kind = 'bar' , stacked = True , ax = ax2 )
769	BB_zoom = ( - 74.1 , - 73.7 , 40.6 , 40.85 ) nyc_map_zoom = plt . imread ( 'https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/images/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true' )
735	model_results = cv_model ( train_set , train_labels , LinearDiscriminantAnalysis ( ) , 'LDA' , model_results )
945	column_types = X . dtypes int_cols = column_types [ column_types == 'int' ] float_cols = column_types [ column_types == 'float' ] cat_cols = column_types [ column_types == 'object' ] print ( '\tinteger columns:\n{}' . format ( int_cols ) ) print ( '\n\tfloat columns:\n{}' . format ( float_cols ) ) print ( '\n\tto encode categorical columns:\n{}' . format ( cat_cols ) )
553	train = pd . read_csv ( '../input/train.csv' , low_memory = False , index_col = 'id' ) test = pd . read_csv ( '../input/test.csv' , low_memory = False , index_col = 'id' ) res = pd . read_csv ( '../input/resources.csv' , low_memory = False , index_col = 'id' )
510	def get_single_image ( infile , nth_image ) : img = read_data ( infile ) img = img . transpose ( ) return np . flipud ( img [ nth_image ] )
996	site_0 = pd . read_csv ( '../input/new-ucf-starter-kernel/submission_ucf_replaced.csv' , index_col = 0 ) submission . loc [ test [ test [ 'site_id' ] == 0 ] . index , 'meter_reading' ] = site_0 [ 'meter_reading' ] del site_0 gc . collect ( )
341	def IoU ( y_true , y_pred ) : iou = tf . py_func ( calculate_iou , [ y_true , y_pred ] , tf . float32 ) return iou
102	import random real = [ ] fake = [ ] for m , n in zip ( paths , y ) : if n == 0 : real . append ( m ) else : fake . append ( m ) fake = random . sample ( fake , len ( real ) ) paths , y = [ ] , [ ] for x in real : paths . append ( x ) y . append ( 0 ) for x in fake : paths . append ( x ) y . append ( 1 )
1332	def add_new_category ( x ) : x = str ( x ) . lower ( ) if 'google' in x : return 'google' elif 'baidu' in x : return 'baidu' elif 'facebook' in x : return 'facebook' elif 'reddit' in x : return 'reddit' elif 'yahoo' in x : return 'yahoo' elif 'bing' in x : return 'bing' elif 'yandex' in x : return 'yandex' else : return 'other'
82	f , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 ) ) sns . countplot ( data = animals , x = 'OutcomeType' , hue = 'Sex' , ax = ax1 ) sns . countplot ( data = animals , x = 'Sex' , hue = 'OutcomeType' , ax = ax2 )
1047	for folder in [ 'train' , 'test' ] : os . makedirs ( folder )
1568	xs = pq . read_table ( '../input/train.parquet' , columns = [ str ( i ) for i in range ( 999 ) ] ) . to_pandas ( ) print ( ( xs . shape ) ) xs . head ( 2 )
1429	stats = [ ] for Province in sorted ( fulltable_us [ 'Province/State' ] . unique ( ) ) : if ( Province == '' ) : continue df = get_time_series_province_us ( Province ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 500 ) : continue print ( '{} of United States COVID-19 Prediction' . format ( Province ) ) opt_display_model_us ( df , stats )
311	df_0 = df_data [ df_data [ 'label' ] == 0 ] . sample ( SAMPLE_SIZE , random_state = 101 ) df_1 = df_data [ df_data [ 'label' ] == 1 ] . sample ( SAMPLE_SIZE , random_state = 101 ) df_data = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_data = shuffle ( df_data ) df_data [ 'label' ] . value_counts ( )
40	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , features ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 20 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 100 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1458	df_train [ 'start_position' ] = start_position_candidates df_train [ 'end_position' ] = end_position_candidates df_test [ 'start_position' ] = - 1 df_test [ 'end_position' ] = - 1
332	random_forest = GridSearchCV ( estimator = RandomForestRegressor ( ) , param_grid = { 'n_estimators' : [ 100 , 1000 ] } , cv = 10 ) random_forest . fit ( train , target ) print ( random_forest . best_params_ ) acc_model ( 6 , random_forest , train , test )
336	bagging = BaggingRegressor ( ) bagging . fit ( train , target ) acc_model ( 11 , bagging , train , test )
1082	yhat = [ 1 if y > 0.5 else 0 for y in yhat ] test_df [ 'label' ] = yhat label_map = dict ( ( v , k ) for k , v in train_generator . class_indices . items ( ) ) test_df [ 'label' ] = test_df [ 'label' ] . replace ( label_map ) test_df [ 'label' ] = test_df [ 'label' ] . replace ( { 'dog' : 1 , 'cat' : 0 } ) test_df . to_csv ( 'submission.csv' , index = False )
206	import numpy as np import pandas as pd import matplotlib . pyplot as plt import eli5 import lightgbm as lgbm import xgboost as xgb from sklearn . linear_model import LogisticRegression , LinearRegression from sklearn . model_selection import train_test_split from sklearn import preprocessing import warnings warnings . filterwarnings ( "ignore" )
817	train_set = lgb . Dataset ( train , label = train_labels ) hyperparameters = dict ( ** random_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS ) print ( 'The cross validation score on the full dataset for Random Search= {:.5f} with std: {:.5f}.' . format ( cv_results [ 'auc-mean' ] [ - 1 ] , cv_results [ 'auc-stdv' ] [ - 1 ] ) ) print ( 'Number of estimators = {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
1582	with open ( BASE_PATH + '/train_data/sample_data.json' ) as f : data_json = json . load ( f ) print ( "There are" , len ( data_json ) , "records in sample_data.json" ) print ( "\nBelow is a record containing lidar data:" ) pprint ( data_json [ 0 ] ) print ( '\n This one contains information about image data:' ) pprint ( data_json [ 2 ] )
566	test_path_audio = os . path . join ( test_path , 'audio' ) test_filenames = os . listdir ( test_path_audio ) test_filenames = np . sort ( test_filenames ) list ( test_filenames ) [ : 10 ]
100	import random real = [ ] fake = [ ] for m , n in zip ( X , y ) : if n == 0 : real . append ( m ) else : fake . append ( m ) fake = random . sample ( fake , len ( real ) ) X , y = [ ] , [ ] for x in real : X . append ( x ) y . append ( 0 ) for x in fake : X . append ( x ) y . append ( 1 )
1205	mode_by_own = train . loc [ train . product_type == "OwnerOccupier" , "build_year" ] . mode ( ) [ 0 ] mode_by_invest = train . loc [ train . product_type == "Investment" , "build_year" ] . mode ( ) [ 0 ] ( mode_by_own , mode_by_invest )
1148	train_df = pd . read_csv ( "../input/train.csv" ) test_df = pd . read_csv ( "../input/test.csv" ) structures_df = pd . read_csv ( '../input/structures.csv' ) test_df [ 'scalar_coupling_constant' ] = np . nan df = pd . concat ( [ train_df , test_df ] ) del train_df del test_df
385	def run_mp_build ( ) : t0 = time . time ( ) num_proc = NUM_THREADS pool = mp . Pool ( processes = num_proc ) results = [ pool . apply_async ( build_fields , args = ( pid , ) ) for pid in range ( NUM_THREADS ) ] output = [ p . get ( ) for p in results ] num_built = sum ( output ) pool . close ( ) pool . join ( ) print ( num_built ) print ( 'Run time: %.2f' % ( time . time ( ) - t0 ) )
796	pred = np . array ( model . predict ( test [ features ] ) ) . reshape ( ( - 1 ) ) sub = pd . DataFrame ( { 'key' : test_id , 'fare_amount' : pred } ) sub . to_csv ( 'sub_rf_tuned.csv' , index = False ) sub [ 'fare_amount' ] . plot . hist ( ) ; plt . title ( 'Predicted Test Fare Distribution' ) ;
997	with open ( '../usr/lib/ucl_data_leakage_episode_2/site1.pkl' , 'rb' ) as f : site_1 = pickle . load ( f ) site_1 = site_1 [ site_1 [ 'timestamp' ] . dt . year > 2016 ]
953	print ( 'Initialize.' ) train_df = pd . read_csv ( '{}train.csv' . format ( data_src ) , usecols = [ 0 ] , index_col = 'id' ) depths_df = pd . read_csv ( '{}depths.csv' . format ( data_src ) , index_col = 'id' ) train_df = train_df . join ( depths_df ) test_df = depths_df [ ~ depths_df . index . isin ( train_df . index ) ]
1409	import missingno as msno train_null = train train_null = train_null . replace ( - 1 , np . NaN ) msno . matrix ( df = train_null . iloc [ : , : ] , figsize = ( 20 , 14 ) , color = ( 0.8 , 0.5 , 0.2 ) )
377	bagging = BaggingRegressor ( ) bagging . fit ( train , target ) acc_model ( 11 , bagging , train , test )
309	print ( len ( os . listdir ( '../input/train' ) ) ) print ( len ( os . listdir ( '../input/test' ) ) )
153	from sklearn import metrics fbeta_sklearn = metrics . fbeta_score ( valid_labels , preds , 2 , average = 'samples' ) print ( fbeta_sklearn )
443	bold ( '**UTILITIES AND HEALTHCARE HAVE THE HIGHEST READINGS**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 15 ) ax = sns . boxplot ( data = train , y = 'primary_use' , x = 'meter_reading' , color = 'teal' , boxprops = dict ( alpha = .3 ) ) ax . set_xlabel ( 'Log(Meter Reading)' , fontsize = 20 ) ax . set_ylabel ( 'primary_use' , fontsize = 20 ) plt . show ( )
1513	categorical = [ ] numerical = [ ] for feature in test . columns : if test [ feature ] . dtype == object : categorical . append ( feature ) else : numerical . append ( feature ) train [ categorical ] . head ( )
952	y = train_ . target X = train_ . drop ( [ 'target' ] , axis = 1 ) X_test = test_ . copy ( ) features_to_remove = [ 'first_active_month' ] X = X . drop ( features_to_remove , axis = 1 ) X_test = X_test . drop ( features_to_remove , axis = 1 ) assert np . all ( X . columns == X_test . columns ) del train_ , test_ gc . collect ( )
1424	stats = [ ] for country in [ 'US' , 'United Kingdom' , 'Russia' , 'Singapore' , 'New Zealand' ] : df = get_time_series ( country ) print ( '{} COVID-19 Prediction' . format ( country ) ) opt_display_model ( df , stats )
486	from sklearn . feature_extraction . text import HashingVectorizer text = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] vectorizer = HashingVectorizer ( n_features = 6 ) vector = vectorizer . transform ( text ) print ( vector . shape ) print ( vector . toarray ( ) )
1287	import numpy as np import pandas as pd from numpy . polynomial . chebyshev import * import matplotlib . pyplot as plt import seaborn as sns pd . options . mode . chained_assignment = None pd . set_option ( 'display.max_columns' , 500 ) from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
331	decision_tree = DecisionTreeRegressor ( ) decision_tree . fit ( train , target ) acc_model ( 5 , decision_tree , train , test )
62	import seaborn as sns print ( 'Blue: Frauds, Orange: Non-Fraud' ) sns . distplot ( train_transaction [ ( train_transaction . isFraud == 1 ) & ( train_transaction . ProductCD == 'S' ) ] [ 'D1minusday' ] , hist = False , rug = False ) ; sns . distplot ( train_transaction [ ( train_transaction . isFraud == 0 ) & ( train_transaction . ProductCD == 'S' ) ] [ 'D1minusday' ] , hist = False , rug = False ) ;
363	dup_diff_target = dup [ ( dup [ 'mean' ] != 0.0 ) & ( dup [ 'mean' ] != 1.0 ) ] len_dup_diff_target = len ( dup_diff_target ) print ( 'NUmber of duplicate clicks with different target values in train data: ' , len_dup_diff_target )
918	credit = pd . read_csv ( '../input/credit_card_balance.csv' ) credit = convert_types ( credit , print_info = True ) credit . head ( )
1164	for item in sorted ( cls_counts . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) [ : 20 ] : _id , count = item [ 0 ] , item [ 1 ] label = label_map [ int ( _id ) ] print ( f'attribute_name: {label} count: {count}' )
519	acc_logreg = cross_val_score ( clf_logreg , X_train , y_train , cv = cv , scoring = 'accuracy' ) . mean ( ) acc_SGD = cross_val_score ( clf_SGD , X_train , y_train , cv = cv , scoring = 'accuracy' ) . mean ( ) acc_rfc = cross_val_score ( clf_rfc , X_train , y_train , cv = cv , scoring = 'accuracy' ) . mean ( ) acc_logreg , acc_SGD , acc_rfc
391	gb = CATEGORY_NAMES_DF . groupby ( 'category_level3' ) cnt = gb . count ( ) cnt [ cnt [ 'category_id' ] > 1 ]
236	n = 17 commits_df . loc [ n , 'commit_num' ] = 21 commits_df . loc [ n , 'dropout_model' ] = 0.36 commits_df . loc [ n , 'hidden_dim_first' ] = 128 commits_df . loc [ n , 'hidden_dim_second' ] = 248 commits_df . loc [ n , 'hidden_dim_third' ] = 240 commits_df . loc [ n , 'LB_score' ] = 0.25831
1198	train_size = int ( len ( scaled ) * 0.7 ) test_size = len ( scaled ) - train_size V_train , V_test = scaled [ 0 : train_size , : ] , scaled [ train_size : len ( scaled ) , : ] print ( len ( V_train ) , len ( V_test ) )
322	df_train , df_val = train_test_split ( df_data , test_size = 0.1 , random_state = 101 ) print ( df_train . shape ) print ( df_val . shape )
808	global ITERATION ITERATION = 0 best = fmin ( fn = objective , space = space , algo = tpe . suggest , trials = trials , max_evals = MAX_EVALS ) best
1592	def remove_cols ( X ) : del_cols = [ f for f in X . columns if X [ f ] . dtype == 'object' ] for f in del_cols : del X [ f ]
897	feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'mean' , 'max' , 'min' , 'trend' , 'mode' , 'count' , 'sum' , 'percent_true' , NormalizedModeCount , MostRecent , LongestSeq ] , trans_primitives = [ 'diff' , 'cum_sum' , 'cum_mean' , 'percentile' ] , where_primitives = [ 'mean' , 'sum' ] , seed_features = [ late_payment , past_due ] , max_depth = 2 , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
335	ridge = RidgeCV ( cv = 10 ) ridge . fit ( train , target ) acc_model ( 10 , ridge , train , test )
50	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train_df [ columns_to_use ] . values . flatten ( ) , bins = 50 ) plt . title ( 'Histogram all train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Value' ) plt . show ( )
1282	def plot_prediction_and_actual ( model , forecast , actual , xlim = None , ylim = None , figSize = None , title = None ) : fig , ax = plt . subplots ( 1 , 1 , figsize = figSize ) ax . set_ylim ( ylim ) ax . plot ( pd . to_datetime ( actual . ds ) , actual . y , 'r.' ) model . plot ( forecast , ax = ax ) ; ax . set_title ( title ) plt . show ( )
1496	def evaluate ( program : [ ] , input_image : np . array ) : input_image = np . array ( input_image ) assert type ( input_image ) == np . ndarray image_list = [ input_image ] for fct in program : image_list = fct ( image_list ) image_list = [ img for img in image_list if img . shape [ 0 ] > 0 and img . shape [ 1 ] > 0 ] if image_list == [ ] : return [ ] return image_list
1388	col = numeric_features [ 37 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
473	import pandas as pd import numpy as np from sklearn . model_selection import StratifiedKFold from sklearn import metrics import gc import xgboost as xgb pd . set_option ( 'display.max_columns' , 200 )
1460	df_test . index = df_test [ 'textID' ] df_test [ 'selected_text' ] = '' df_test . loc [ ds_pos_test . hash_index [ : BATCH_SIZE if DEBUG_MODE else len ( df_test ) ] , 'start_position' : 'end_position' ] = pos_pred . values df_test . loc [ ds_neg_test . hash_index [ : BATCH_SIZE if DEBUG_MODE else len ( df_test ) ] , 'start_position' : 'end_position' ] = neg_pred . values df_test
1485	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 2 - Lung Opacity" ) draw ( parsed [ df [ 'patientId' ] [ 8 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 3 - Lung Nodules and Masses" ) draw ( parsed [ df [ 'patientId' ] [ 2 ] ] )
736	for n in [ 5 , 10 , 20 ] : print ( f'\nKNN with {n} neighbors\n' ) model_results = cv_model ( train_set , train_labels , KNeighborsClassifier ( n_neighbors = n ) , f'knn-{n}' , model_results )
884	plt . figure ( figsize = ( 12 , 12 ) ) sns . heatmap ( opt_hyp . corr ( ) . round ( 2 ) , cmap = plt . cm . gist_heat_r , vmin = - 1.0 , annot = True , vmax = 1.0 ) plt . title ( 'Correlation Heatmap' ) ;
1004	real_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' eval_partition = pd . read_csv ( '/kaggle/input/celeba-dataset/list_eval_partition.csv' ) eval_partition [ 'filename' ] = eval_partition . image_id . apply ( lambda st : real_dir + st ) eval_partition [ 'class' ] = 'REAL'
1380	col = numeric_features [ 27 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
834	app = app . set_index ( 'SK_ID_CURR' ) app = app . merge ( bureau_info , on = 'SK_ID_CURR' , how = 'left' ) del bureau_info app . shape
974	idx = 0 for key in keyword_dict : if idx < 5 : print ( key ) print ( keyword_dict [ key ] ) idx += 1
216	lsvr = LinearSVR ( C = 0.05 , max_iter = 1000 ) . fit ( dfe , target_fe ) model = SelectFromModel ( lsvr , prefit = True ) X_new = model . transform ( dfe ) X_selected_df = pd . DataFrame ( X_new , columns = [ dfe . columns [ i ] for i in range ( len ( dfe . columns ) ) if model . get_support ( ) [ i ] ] ) X_selected_df . shape
482	import numpy as np import librosa . display import tensorflow as tf import tensorflow . contrib . eager as tfe tfe . enable_eager_execution ( ) from scipy . io import wavfile import matplotlib . pyplot as plt
60	L = liste_existstrun G = nx . Graph ( ) G . add_nodes_from ( sum ( L , [ ] ) ) q = [ [ ( s [ i ] , s [ i + 1 ] ) for i in range ( len ( s ) - 1 ) ] for s in L ] for i in q : G . add_edges_from ( i ) group_list = [ list ( i ) for i in nx . connected_components ( G ) ] group_list
1129	import numpy as np import pandas as pd import os print ( os . listdir ( "../input" ) )
1579	plt . plot ( history [ 'loss' ] ) plt . plot ( history [ 'val_loss' ] ) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ( [ 'train' , 'test' ] , loc = 'upper right' ) ;
14	tk = Tokenizer ( num_words = max_features , lower = True ) tk . fit_on_texts ( all_text ) all_text = tk . texts_to_sequences ( all_text ) train_text = tk . texts_to_sequences ( train_text ) test_text = tk . texts_to_sequences ( test_text )
1285	from typing import List def list_squared ( input_list : List [ int ] ) -> List [ int ] : return [ element ** 2 for element in input_list ] list_squared ( [ 2 , 4 , 6 , 8 ] )
850	random_results = pd . DataFrame ( columns = [ 'score' , 'params' , 'iteration' ] , index = list ( range ( MAX_EVALS ) ) ) grid_results = pd . DataFrame ( columns = [ 'score' , 'params' , 'iteration' ] , index = list ( range ( MAX_EVALS ) ) )
603	_ = sns . distplot ( abs ( pub_prv_diff ) , bins = 20 , hist_kws = dict ( cumulative = - 1 ) , kde_kws = dict ( cumulative = True ) , kde = False , norm_hist = True ) _ = plt . xlabel ( "Public-Private Absolute Difference" ) _ = plt . ylabel ( "Frequency" )
1289	from sklearn . model_selection import train_test_split params = { 'n_estimators' : 200 , 'max_depth' : 5 , 'min_child_weight' : 100 , 'subsample' : .9 , 'gamma' : 1 , 'objective' : 'reg:linear' , 'colsample_bytree' : .8 , 'nthread' : 3 , 'silent' : 1 , 'seed' : 27 } train , test = train_test_split ( x_train , test_size = 0.2 ) predictors = df [ 'feature' ] [ df [ 'fscore' ] > 0.5 ] . tolist ( )
1027	with strategy . scope ( ) : transformer_layer = ( transformers . TFDistilBertModel . from_pretrained ( 'distilbert-base-multilingual-cased' ) ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
347	ID = df_preds [ 'patientId' ] preds = df_preds [ 'PredictionString' ] submission = pd . DataFrame ( { 'patientId' : ID , 'PredictionString' : preds , } ) . set_index ( 'patientId' ) submission . to_csv ( 'pneu_keras_model.csv' , columns = [ 'PredictionString' ] )
65	train_data = observation . train train_data = train_data . set_index ( [ 'id' , 'timestamp' ] ) . sort_index ( ) train_data
696	mapping = { "yes" : 1 , "no" : 0 } for df in [ train , test ] : df [ 'dependency' ] = df [ 'dependency' ] . replace ( mapping ) . astype ( np . float64 ) df [ 'edjefa' ] = df [ 'edjefa' ] . replace ( mapping ) . astype ( np . float64 ) df [ 'edjefe' ] = df [ 'edjefe' ] . replace ( mapping ) . astype ( np . float64 ) train [ [ 'dependency' , 'edjefa' , 'edjefe' ] ] . describe ( )
1538	feature_matrix , feature_defs = ft . dfs ( entityset = es , target_entity = 'applications' , drop_contains = [ 'SK_ID_PREV' ] , max_depth = 2 , verbose = True )
432	tag_to_count_map tupl = dict ( tag_to_count_map . items ( ) ) word_cloud = WordCloud ( width = 1600 , height = 800 , ) . generate_from_frequencies ( tupl ) plt . figure ( figsize = ( 12 , 8 ) ) plt . imshow ( word_cloud ) plt . axis ( 'off' ) plt . tight_layout ( pad = 0 )
144	cat_dim = [ int ( all_df [ col ] . nunique ( ) ) for col in categorical ] cat_dim = [ [ x , min ( 200 , ( x + 1 ) // 2 ) ] for x in cat_dim ] for el in cat_dim : if el [ 0 ] < 10 : el [ 1 ] = el [ 0 ] cat_dim
888	import re def replace_day_outliers ( df ) : for col in df . columns : if "DAYS" in col : df [ col ] = df [ col ] . replace ( { 365243 : np . nan } ) return df app_train = replace_day_outliers ( app_train ) app_test = replace_day_outliers ( app_test ) bureau = replace_day_outliers ( bureau ) bureau_balance = replace_day_outliers ( bureau_balance ) credit = replace_day_outliers ( credit ) cash = replace_day_outliers ( cash ) previous = replace_day_outliers ( previous ) installments = replace_day_outliers ( installments )
794	tune_data = data . sample ( 100_000 , random_state = RSEED ) time_features = [ 'pickup_frac_day' , 'pickup_frac_week' , 'pickup_frac_year' , 'pickup_Elapsed' ] features = [ 'abs_lat_diff' , 'abs_lon_diff' , 'haversine' , 'passenger_count' , 'pickup_latitude' , 'pickup_longitude' , 'dropoff_latitude' , 'dropoff_longitude' ] + time_features rs . fit ( tune_data [ features ] , np . array ( tune_data [ 'fare_amount' ] ) )
752	model = RandomForestClassifier ( max_depth = 3 , n_estimators = 10 ) model . fit ( train_selected , train_labels ) estimator_limited = model . estimators_ [ 5 ] estimator_limited
922	x , y , w , h = result [ 0 ] [ 'box' ] right_eye = result [ 0 ] [ 'keypoints' ] [ 'right_eye' ] nose = result [ 0 ] [ 'keypoints' ] [ 'nose' ] mouth_left = result [ 0 ] [ 'keypoints' ] [ 'mouth_left' ] mouth_right = result [ 0 ] [ 'keypoints' ] [ 'mouth_right' ]
493	from keras . layers import Input from keras . layers import Dense visible = Input ( shape = ( 2 , ) ) hidden = Dense ( 2 ) ( visible )
844	features = pd . read_csv ( '../input/home-credit-default-risk/application_train.csv' ) features = features . sample ( n = 16000 , random_state = 42 ) features = features . select_dtypes ( 'number' ) labels = np . array ( features [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) features = features . drop ( columns = [ 'TARGET' , 'SK_ID_CURR' ] ) train_features , test_features , train_labels , test_labels = train_test_split ( features , labels , test_size = 6000 , random_state = 50 )
531	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( data = orders , x = "order_hour_of_day" , ax = ax , color = " ax.set(xlabel='Hour Of The Day',title=" Order Count Across Hour Of The Day " )
419	from sklearn . tree import DecisionTreeClassifier dt = DecisionTreeClassifier ( ) dt = dt . fit ( xT , yT )
620	def perform_linear_lasso ( df_X , df_Y , test_X , test_Y ) : clf = Lasso ( alpha = 1.0 ) clf . fit ( df_X , df_Y ) pred_Y = clf . predict ( test_X ) r2_score_ll = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( clf . score ( df_X , df_Y ) * 100 , 2 ) returnval = { 'model' : 'Lasso' , 'r2_score' : r2_score_ll } return returnval
838	cash = pd . read_csv ( '../input/POS_CASH_balance.csv' ) . replace ( { 365243 : np . nan } ) cash = convert_types ( cash ) cash [ 'LATE_PAYMENT' ] = cash [ 'SK_DPD' ] > 0.0 cash [ 'INSTALLMENTS_PAID' ] = cash [ 'CNT_INSTALMENT' ] - cash [ 'CNT_INSTALMENT_FUTURE' ]
284	n = 13 commits_df . loc [ n , 'commit_num' ] = 19 commits_df . loc [ n , 'Dropout_model' ] = 0.39 commits_df . loc [ n , 'FVC_weight' ] = 0.2 commits_df . loc [ n , 'LB_score' ] = - 6.8090
1013	def apply_convolution ( sig , window ) : conv = np . repeat ( [ 0. , 1. , 0. ] , window ) filtered = signal . convolve ( sig , conv , mode = 'same' ) / window return filtered
762	sub [ 'surface' ] = le . inverse_transform ( predicted . argmax ( axis = 1 ) ) sub . to_csv ( 'rand_sub_10.csv' , index = False ) sub . head ( )
1142	from pytorch_lightning import Trainer , seed_everything , loggers seed_everything ( 314 ) model = WheatModel ( processed_train_labels_df , fold = 0 ) logger = loggers . TensorBoardLogger ( "logs" , name = "effdet-b5" , version = "fold_0" ) trainer = Trainer ( gpus = 1 , logger = logger , fast_dev_run = True ) trainer . fit ( model ) torch . save ( model . model . state_dict ( ) , "wheatdet.pth" )
73	from fastai import * from fastai . vision import * from sklearn . metrics import f1_score
591	def generate_word_cloud ( text ) : wordcloud = WordCloud ( width = 3000 , height = 2000 , background_color = 'black' ) . generate ( str ( text ) ) fig = plt . figure ( figsize = ( 40 , 30 ) , facecolor = 'k' , edgecolor = 'k' ) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . tight_layout ( pad = 0 ) plt . show ( )
1151	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 8 ) ) sorted_train_df . groupby ( 'date' ) [ 'var_91' ] . count ( ) . plot ( ax = ax , label = "train" ) sorted_test_df . groupby ( 'date' ) [ 'var_91' ] . count ( ) . plot ( ax = ax , label = "test" ) ax . legend ( )
853	grid_search_params = grid_results . loc [ 0 , 'params' ] model = lgb . LGBMClassifier ( ** grid_search_params , random_state = 42 ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] print ( 'The best model from grid search scores {:.5f} ROC AUC on the test set.' . format ( roc_auc_score ( test_labels , preds ) ) )
1118	category_cols = [ 'building_id' , 'site_id' , 'primary_use' ] feature_cols = [ 'square_feet' , 'year_built' ] + [ 'hour' , 'weekend' , 'building_median' ] + [ 'air_temperature' , 'cloud_coverage' , 'dew_temperature' , 'precip_depth_1_hr' , 'sea_level_pressure' , 'wind_direction' , 'wind_speed' , ]
245	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'best' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmin ( ) , 'best' ] = 1
630	pv_agg = train_agg . reset_index ( ) pv_agg [ 'dt' ] = pd . to_datetime ( pv_agg . year * 10000 + pv_agg . month * 100 + pv_agg . day , format = '%Y%m%d' ) pv_agg = pv_agg . pivot ( index = 'dt' , columns = 'hotel_cluster' , values = 'bookings' ) pv_agg . columns = [ str ( i ) for i in pv_agg . columns ] pv_agg [ 'dt' ] = pv_agg . index pv_agg [ 'dow' ] = pv_agg . dt . dt . weekday pv_agg . head ( )
326	X = df_train_padded X_test = df_test_padded y_toxic = df_train [ 'toxic' ] y_severe_toxic = df_train [ 'severe_toxic' ] y_obscene = df_train [ 'obscene' ] y_threat = df_train [ 'threat' ] y_insult = df_train [ 'insult' ] y_identity_hate = df_train [ 'identity_hate' ]
1354	col = numeric_features [ 1 ] plot_kde_hist_for_numeric ( col ) plot_category_percent_of_target_for_numeric ( col )
524	from sklearn . metrics import precision_score , recall_score y_pred_90 = ( y_pred_prob_logreg_class1 > 0.32 ) precisionScore = precision_score ( y_train , y_pred_90 ) recallScore = recall_score ( y_train , y_pred_90 ) precisionScore , recallScore
586	has_to_run_sir = True has_to_run_sird = False has_to_run_seir = True has_to_run_seird = False has_to_run_seirdq = True
1340	temp_col = features_dtype_object [ 13 ] plot_count_percent_for_object ( application_train , temp_col ) plot_count_percent_for_object ( application_object_na_filled , temp_col )
1073	import tensorflow as tf from sklearn . metrics import confusion_matrix , accuracy_score , classification_report import seaborn as sn import albumentations as albu from sklearn . model_selection import train_test_split , KFold from tqdm import tqdm_notebook import gc import os import warnings warnings . filterwarnings ( 'ignore' ) main_dir = '../input/Kannada-MNIST/' tf . keras . __version__
189	price_of_zero = train . loc [ train . price == 0 ] plt . figure ( figsize = ( 17 , 10 ) ) sns . countplot ( y = price_of_zero . category_name , \ order = price_of_zero . category_name . value_counts ( ) . iloc [ : 10 ] . index , \ orient = 'v' ) plt . title ( 'Top 10 categories of items with a price of 0' , fontsize = 25 ) plt . ylabel ( 'Category name' , fontsize = 20 ) plt . xlabel ( 'Number of product in the category' , fontsize = 20 )
1246	data = pd . concat ( [ train [ 'Store' ] , train [ 'Weekly_Sales' ] , train [ 'IsHoliday' ] ] , axis = 1 ) f , ax = plt . subplots ( figsize = ( 25 , 8 ) ) fig = sns . boxplot ( x = 'Store' , y = 'Weekly_Sales' , data = data , showfliers = False , hue = "IsHoliday" )
860	train = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_train.csv' ) test = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_test.csv' ) test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
720	corr_matrix = ind . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
941	train = pd . read_csv ( "../input/application_train.csv" ) test = pd . read_csv ( "../input/application_test.csv" ) bureau = pd . read_csv ( "../input/bureau.csv" ) bureau_bal = pd . read_csv ( '../input/bureau_balance.csv' )
655	model . save ( 'UNET.h5' , include_optimizer = False ) f = open ( "preprocess.dill" , "wb" ) dill . dump ( preprocess , f ) f . close classify_model . save ( 'classify_model.h5' , include_optimizer = False ) f = open ( "classify_preprocess.dill" , "wb" ) dill . dump ( classify_preprocess , f ) f . close if mode != 'train' :
464	mteams = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MTeams.csv' ) mseasons = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MSeasons.csv' ) mtourney_seed = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MNCAATourneySeeds.csv' ) mseason_results = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MRegularSeasonCompactResults.csv' ) mtourney_results = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MNCAATourneyCompactResults.csv' ) conference = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/Conferences.csv' ) team_conference = pd . read_csv ( '../input/march-madness-analytics-2020/2020DataFiles/2020-Mens-Data/MDataFiles_Stage1/MTeamConferences.csv' )
883	plt . figure ( figsize = ( 12 , 12 ) ) sns . heatmap ( random_hyp . corr ( ) . round ( 2 ) , cmap = plt . cm . gist_heat_r , vmin = - 1.0 , annot = True , vmax = 1.0 ) plt . title ( 'Correlation Heatmap' ) ;
1165	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
1381	col = numeric_features [ 28 ] plot_category_percent_of_target_for_numeric ( col )
1237	logit_lv3 = LogisticRegression ( random_state = 0 , C = 0.5 ) logit_lv3_outcomes = cross_validate_sklearn ( logit_lv3 , lv2_train , y_train , lv2_test , kf , scale = True , verbose = True ) logit_lv3_cv = logit_lv3_outcomes [ 0 ] logit_lv3_train_pred = logit_lv3_outcomes [ 1 ] logit_lv3_test_pred = logit_lv3_outcomes [ 2 ]
979	np . random . seed ( 0 ) patients = os . listdir ( '/kaggle/input/osic-pulmonary-fibrosis-progression/train' ) patients . sort ( ) n_patients = len ( patients ) patient = patients [ np . random . randint ( 0 , n_patients ) ] patient
1418	import pandas as pd , numpy as np from matplotlib import pyplot as plt import scipy . stats as stats pd . options . display . max_columns = 50
1153	def compute_rolling_mean_per_store_df ( df , period = 30 ) : return ( df . set_index ( "date" ) . groupby ( "store_id" ) . rolling ( period ) . mean ( ) . reset_index ( ) )
1336	def random_color_generator ( number_of_colors ) : color = [ " for i in range ( number_of_colors ) ] return color
657	raw_train = pd . read_csv ( '../input/cat-in-the-dat-ii/train.csv' , index_col = 'id' ) raw_test = pd . read_csv ( '../input/cat-in-the-dat-ii/test.csv' , index_col = 'id' ) print ( raw_train . shape , raw_test . shape )
666	encoded_full = scipy . sparse . hstack ( [ OH_full , retain_full , retain_full ** 2 ] ) . tocsr ( ) print ( encoded_full . shape ) encoded_train = encoded_full [ : len ( raw_train ) ] encoded_test = encoded_full [ len ( raw_train ) : ]
1194	x_train , x_val , y_train , y_val = train_test_split ( x_train , y_train_multi , test_size = 0.40 , random_state = 2020 )
779	preds = lr . predict ( test [ [ 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] ) sub = pd . DataFrame ( { 'key' : test_id , 'fare_amount' : preds } ) sub . to_csv ( 'sub_lr_simple.csv' , index = False )
1235	lv2_columns = [ 'rf_lf2' , 'logit_lv2' , 'xgb_lv2' , 'lgb_lv2' ] train_lv2_pred_list = [ rf_lv2_train_pred , logit_lv2_train_pred , xgb_lv2_train_pred , lgb_lv2_train_pred ] test_lv2_pred_list = [ rf_lv2_test_pred , logit_lv2_test_pred , xgb_lv2_test_pred , lgb_lv2_test_pred ] lv2_train = pd . DataFrame ( columns = lv2_columns ) lv2_test = pd . DataFrame ( columns = lv2_columns ) for i in range ( 0 , len ( lv2_columns ) ) : lv2_train [ lv2_columns [ i ] ] = train_lv2_pred_list [ i ] lv2_test [ lv2_columns [ i ] ] = test_lv2_pred_list [ i ]
1508	good_features = report . loc [ report [ 'rmse' ] <= 0.7955 ] . index rmses = report . loc [ report [ 'rmse' ] <= 0.7955 , 'rmse' ] . values good_features
643	target = df_train [ 'outliers' ] del df_train [ 'outliers' ] del df_train [ 'target' ]
