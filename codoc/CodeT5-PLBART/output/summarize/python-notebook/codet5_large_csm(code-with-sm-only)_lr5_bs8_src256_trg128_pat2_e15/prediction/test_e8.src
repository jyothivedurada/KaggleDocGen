283	range_ = lambda x : x . max ( ) - x . min ( ) range_ . __name__ = 'range_' ind_agg = ind . drop ( columns = 'Target' ) . groupby ( 'idhogar' ) . agg ( [ 'min' , 'max' , 'sum' , 'count' , 'std' , range_ ] ) ind_agg . head ( )
466	link_count = np . zeros ( [ len ( title_dic ) , len ( title_dic ) ] , dtype = np . int ) node_count = np . zeros ( [ len ( title_dic ) ] ) for i in tqdm ( train . index ) : link_count [ train . loc [ i , 'previous_title' ] ] [ train . loc [ i , 'title' ] ] += 1 node_count [ train . loc [ i , 'title' ] ] += 1
154	bold ( '**READINGS REALLY PEAKED FROM MAY TO OCTOBER**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( [ 'timestamp' , 'month' ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . lineplot ( data = temp_df , x = 'timestamp' , y = 'meter_reading' , color = 'teal' ) plt . xlabel ( 'Timestamp' , fontsize = 15 ) plt . ylabel ( 'Meter Reading' ) plt . show ( )
151	bold ( '**SUNDAYS HAVE THE LOWEST READINGS**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) ax = sns . boxplot ( data = train , x = 'weekday_name' , y = 'meter_reading' , color = 'teal' , boxprops = dict ( alpha = .3 ) ) ax . set_ylabel ( 'Log(Meter Reading)' , fontsize = 20 ) ax . set_xlabel ( 'weekdays' , fontsize = 20 ) plt . show ( )
85	tourney_win_result [ 'Seed_diff' ] = tourney_win_result [ 'Seed1' ] - tourney_win_result [ 'Seed2' ] tourney_win_result [ 'ScoreT_diff' ] = tourney_win_result [ 'ScoreT1' ] - tourney_win_result [ 'ScoreT2' ] tourney_lose_result [ 'Seed_diff' ] = tourney_lose_result [ 'Seed1' ] - tourney_lose_result [ 'Seed2' ] tourney_lose_result [ 'ScoreT_diff' ] = tourney_lose_result [ 'ScoreT1' ] - tourney_lose_result [ 'ScoreT2' ]
49	model . save_model ( "cbmodel.cbm" , format = "cbm" , export_parameters = None , pool = None )
42	sample = random . choice ( filenames ) image = load_img ( "../input/train/train/" + sample ) plt . imshow ( image )
474	N = test_df . shape [ 0 ] x_test = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( test_df [ 'Patient' ] ) ) : x_test [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
219	df_china_cases_by_day = df_grouped_china [ df_grouped_china . confirmed > 0 ] df_china_cases_by_day = df_china_cases_by_day . reset_index ( drop = True ) df_china_cases_by_day [ 'day' ] = df_china_cases_by_day . date . apply ( lambda x : ( x - df_china_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_china_cases_by_day = df_china_cases_by_day [ reordered_columns ] df_china_cases_by_day
520	for col1 in [ 'lugar1' , 'lugar2' , 'lugar3' , 'lugar4' , 'lugar5' , 'lugar6' ] : for col2 in [ 'instlevel1' , 'instlevel2' , 'instlevel3' , 'instlevel4' , 'instlevel5' , 'instlevel6' , 'instlevel7' , 'instlevel8' , 'instlevel9' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
74	columns = [ i for i in data . columns ] dummies = pd . get_dummies ( data , columns = columns , drop_first = True , sparse = True ) del data
213	mask_dir = df [ df . id == imid ] . masks . values [ 0 ] masks = os . listdir ( mask_dir ) masks [ : 10 ]
419	import tensorflow as tf from sklearn . metrics import confusion_matrix , accuracy_score , classification_report import seaborn as sn import albumentations as albu from sklearn . model_selection import train_test_split , KFold from tqdm import tqdm_notebook import gc import os import warnings warnings . filterwarnings ( 'ignore' ) main_dir = '../input/Kannada-MNIST/' tf . keras . __version__
440	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
510	from sklearn . metrics import mean_squared_error watchlist = [ ( dtrain , 'train' ) ] num_round = 600 bst = xgb . train ( dict ( xgb_params , silent = 0 ) , dtrain , num_boost_round = num_round ) preds = bst . predict ( dtest ) err = ( mean_squared_error ( test [ target ] . values , preds ) ) print ( 'MSE ={}' . format ( err ) )
107	df_train = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftrain.pickle' , 'rb' ) ) df_test = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftest.pickle' , 'rb' ) ) print ( df_train . shape ) print ( df_test . shape )
58	plt . figure ( figsize = ( 10 , 5 ) ) df . resample ( 'H' ) . is_attributed . mean ( ) . plot ( ) plt . title ( 'Download rate evolution over the day' , fontsize = 15 ) plt . xlabel ( 'Time' ) plt . ylabel ( 'Download rate' )
567	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
86	eps = 1e-8 dense_game_features = train_dense . columns [ train_dense [ : 22 ] . std ( ) <= eps ] dense_player_features = train_dense . columns [ train_dense [ : 22 ] . std ( ) > eps ] cat_game_features = train_cat . columns [ train_cat [ : 22 ] . std ( ) <= eps ] cat_player_features = train_cat . columns [ train_cat [ : 22 ] . std ( ) > eps ]
276	households_leader = train . groupby ( 'idhogar' ) [ 'parentesco1' ] . sum ( ) households_no_head = train . loc [ train [ 'idhogar' ] . isin ( households_leader [ households_leader == 0 ] . index ) , : ] print ( 'There are {} households without a head.' . format ( households_no_head [ 'idhogar' ] . nunique ( ) ) )
158	from sklearn . preprocessing import LabelEncoder le = LabelEncoder ( ) train [ 'primary_use' ] = le . fit_transform ( train [ 'primary_use' ] ) test [ 'primary_use' ] = le . fit_transform ( test [ 'primary_use' ] )
66	price_of_zero = train . loc [ train . price == 0 ] plt . figure ( figsize = ( 17 , 10 ) ) sns . countplot ( y = price_of_zero . category_name , \ order = price_of_zero . category_name . value_counts ( ) . iloc [ : 10 ] . index , \ orient = 'v' ) plt . title ( 'Top 10 categories of items with a price of 0' , fontsize = 25 ) plt . ylabel ( 'Category name' , fontsize = 20 ) plt . xlabel ( 'Number of product in the category' , fontsize = 20 )
416	def compute_text_and_questions ( test , tokenizer ) : test_text = tokenizer . texts_to_sequences ( test . text . values ) test_questions = tokenizer . texts_to_sequences ( test . question . values ) test_text = sequence . pad_sequences ( test_text , maxlen = 300 ) test_questions = sequence . pad_sequences ( test_questions ) return test_text , test_questions
371	if len ( VALIDATION_MISMATCHES_IDS ) > 0 : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True ) dataset = dataset . filter ( lambda image , label , idnum : tf . reduce_sum ( tf . cast ( idnum == VALIDATION_MISMATCHES_IDS , tf . int32 ) ) > 0 ) dataset = dataset . map ( lambda image , label , idnum : [ image , label ] ) imgs = next ( iter ( dataset . batch ( len ( VALIDATION_MISMATCHES_IDS ) ) ) ) display_batch_of_images ( imgs )
104	X = df_train_padded X_test = df_test_padded y_toxic = df_train [ 'toxic' ] y_severe_toxic = df_train [ 'severe_toxic' ] y_obscene = df_train [ 'obscene' ] y_threat = df_train [ 'threat' ] y_insult = df_train [ 'insult' ] y_identity_hate = df_train [ 'identity_hate' ]
464	train_sentences = data_train [ 'comment_text' ] . values . tolist ( ) test_sentences = data_test [ 'comment_text' ] . values . tolist ( ) total_ = copy . deepcopy ( train_sentences ) total_ . extend ( test_sentences ) print ( '[*]Training Sentences:' , len ( train_sentences ) ) print ( '[*]Test Sentences:' , len ( test_sentences ) ) print ( '[*]Total Sentences:' , len ( total_ ) ) for i in tqdm ( range ( len ( total_ ) ) ) : total_ [ i ] = str ( total_ [ i ] ) . lower ( )
583	plt . figure ( figsize = ( 20 , 5 ) ) plt . plot ( - plot1 . compute ( ) , plot2 . compute ( ) ) ; plt . xlabel ( "- Quaketime" ) plt . ylabel ( "Signal" ) plt . title ( "PLOT 0" ) ;
134	ix = 3 test_image = X_test [ ix ] . astype ( float ) imshow ( test_image ) plt . show ( )
578	app_both [ 'LOAN_INCOME_RATIO' ] = app_both [ 'AMT_CREDIT' ] / app_both [ 'AMT_INCOME_TOTAL' ] app_both [ 'ANNUITY_INCOME_RATIO' ] = app_both [ 'AMT_ANNUITY' ] / app_both [ 'AMT_INCOME_TOTAL' ] app_both [ 'ANNUITY LENGTH' ] = app_both [ 'AMT_CREDIT' ] / app_both [ 'AMT_ANNUITY' ] app_both [ 'WORKING_LIFE_RATIO' ] = app_both [ 'DAYS_EMPLOYED' ] / app_both [ 'DAYS_BIRTH' ] app_both [ 'INCOME_PER_FAM' ] = app_both [ 'AMT_INCOME_TOTAL' ] / app_both [ 'CNT_FAM_MEMBERS' ] app_both [ 'CHILDREN_RATIO' ] = app_both [ 'CNT_CHILDREN' ] / app_both [ 'CNT_FAM_MEMBERS' ]
584	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
345	from sklearn . model_selection import train_test_split unique_img_ids = masks . groupby ( 'ImageId' ) . size ( ) . reset_index ( name = 'counts' ) train_ids , valid_ids = train_test_split ( unique_img_ids , test_size = 0.05 , stratify = unique_img_ids [ 'counts' ] , random_state = 42 ) train_df = pd . merge ( masks , train_ids ) valid_df = pd . merge ( masks , valid_ids ) print ( train_df . shape [ 0 ] , 'training masks' ) print ( valid_df . shape [ 0 ] , 'validation masks' )
372	X_test = Parallel ( n_jobs = - 3 , verbose = 1 ) ( delayed ( processor . prepareSample ) ( test_root , sample_submission . iloc [ f , : ] , processor . createMel , CONFIG , TRAINING_CONFIG , test_mode = True , proc_mode = 'resize' , ) for f in range ( 100 ) ) X_test = np . array ( X_test ) print ( X_test . shape )
326	bars = alt . Chart ( random_hyp , width = 400 ) . mark_bar ( ) . encode ( x = 'boosting_type' , y = alt . Y ( 'count()' , scale = alt . Scale ( domain = [ 0 , 400 ] ) ) ) bars . title = 'Boosting Type for Random Search' text = bars . mark_text ( align = 'center' , baseline = 'bottom' , size = 20 ) . encode ( text = 'count()' ) bars + text
130	DATA_DIR = r'../input' TEST_DIR = r'../input/test' print ( 'ok' )
229	positive_train [ 'temp_list' ] = positive_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) positive_train [ 'temp_list' ] = positive_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) positive_top = Counter ( [ item for sublist in positive_train [ 'temp_list' ] for item in sublist ] ) positive_temp = pd . DataFrame ( positive_top . most_common ( 20 ) ) positive_temp . columns = [ 'Common_words' , 'count' ] positive_temp . style . background_gradient ( cmap = 'Blues' )
390	def create_title_mode ( train_labels ) : titles = train_labels . title . unique ( ) title2mode = { } for title in titles : mode = ( train_labels [ train_labels . title == title ] . accuracy_group . value_counts ( ) . index [ 0 ] ) title2mode [ title ] = mode return title2mode def add_title_mode ( labels , title2mode ) : labels [ 'title_mode' ] = labels . title . apply ( lambda title : title2mode [ title ] ) return labels
570	def create_video ( image_list , out_file ) : height , width = image_list [ 0 ] . shape fourcc = cv2 . VideoWriter_fourcc ( * 'X264' ) fps = 30.0 video = cv2 . VideoWriter ( out_file , fourcc , fps , ( width , height ) , False ) for im in image_list : video . write ( im . astype ( np . uint8 ) ) cv2 . destroyAllWindows ( ) video . release ( )
516	total = df_train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = 100 * ( df_train . isnull ( ) . sum ( ) / df_train . isnull ( ) . count ( ) ) . sort_values ( ascending = False ) missing_df = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_df . head ( 20 )
498	PRETRAINED_MODELS = { "BERT" : [ 'bert-base-uncased' , 'bert-large-uncased' , 'bert-base-cased' , 'bert-large-cased' , 'bert-base-multilingual-uncased' , 'bert-base-multilingual-cased' , 'bert-base-chinese' , 'bert-base-german-cased' , 'bert-large-uncased-whole-word-masking' , 'bert-large-cased-whole-word-masking' , 'bert-large-uncased-whole-word-masking-finetuned-squad' , 'bert-large-cased-whole-word-masking-finetuned-squad' , 'bert-base-cased-finetuned-mrpc' ] , "DISTILBERT" : [ 'distilbert-base-uncased' , 'distilbert-base-uncased-distilled-squad' ] }
145	tag_to_count_map tupl = dict ( tag_to_count_map . items ( ) ) word_cloud = WordCloud ( width = 1600 , height = 800 , ) . generate_from_frequencies ( tupl ) plt . figure ( figsize = ( 12 , 8 ) ) plt . imshow ( word_cloud ) plt . axis ( 'off' ) plt . tight_layout ( pad = 0 )
531	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
91	df_data = pd . read_csv ( '../input/train_labels.csv' ) df_data [ df_data [ 'id' ] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' ] df_data [ df_data [ 'id' ] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' ] print ( df_data . shape )
192	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) ordersDay = orders [ [ "order_dow" ] ] . replace ( { 0 : "Sunday" , 1 : "Monday" , 2 : "Tuesday" , 3 : "Wednesday" , 4 : "Thursday" , 5 : "Friday" , 6 : "Saturday" } ) sn . countplot ( color = " ax.set(xlabel='Day Of The Week',title=" Order Count Across Days Of The Week " )
455	logregModel = LogisticRegression ( ) params = { 'C' : np . logspace ( start = - 5 , stop = 3 , num = 9 ) } clf = GridSearchCV ( logregModel , params , scoring = 'neg_log_loss' , refit = True ) clf . fit ( X_train , y_train )
22	import random real = [ ] fake = [ ] for m , n in zip ( X , y ) : if n == 0 : real . append ( m ) else : fake . append ( m ) fake = random . sample ( fake , len ( real ) ) X , y = [ ] , [ ] for x in real : X . append ( x ) y . append ( 0 ) for x in fake : X . append ( x ) y . append ( 1 )
302	X_train , X_valid , y_train , y_valid = train_test_split ( data , np . array ( data [ 'fare_amount' ] ) , stratify = data [ 'fare-bin' ] , random_state = RSEED , test_size = 1_000_000 )
549	cities = pd . read_csv ( '../input/cities.csv' ) xy_int = ( cities [ [ 'X' , 'Y' ] ] * 1000 ) . astype ( np . int64 ) with open ( 'xy_int.csv' , 'w' ) as fp : print ( len ( xy_int ) , file = fp ) print ( xy_int . to_csv ( index = False , header = False , sep = ' ' ) , file = fp )
569	tst_leak = pd . read_csv ( '../input/breaking-lb-fresh-start-with-lag-selection/test_leak.csv' ) test [ 'leak' ] = tst_leak [ 'compiled_leak' ] test [ 'log_leak' ] = np . log1p ( tst_leak [ 'compiled_leak' ] )
152	bold ( '**READINGS HIGHEST DURING THE MIDDLE OF THE DAY**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( 'hour' ) . meter_reading . sum ( ) temp_df . plot ( linewidth = 5 , color = 'teal' ) plt . xlabel ( 'Reading Hour' , fontsize = 15 ) plt . ylabel ( 'Meter Reading' ) plt . show ( )
312	import pandas as pd import numpy as np import featuretools as ft import matplotlib . pyplot as plt plt . rcParams [ 'font.size' ] = 22 import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm as lgb from sklearn . model_selection import train_test_split from sklearn . model_selection import KFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import LabelEncoder import gc
165	import numpy as np import pandas as pd from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import train_test_split , StratifiedKFold , KFold from bayes_opt import BayesianOptimization from datetime import datetime from sklearn . metrics import precision_score , recall_score , confusion_matrix , accuracy_score , roc_auc_score , f1_score , roc_curve , auc , precision_recall_curve from sklearn import metrics from sklearn import preprocessing import lightgbm as lgb import warnings warnings . filterwarnings ( "ignore" ) import itertools from scipy import interp import seaborn as sns import matplotlib . pyplot as plt from matplotlib import rcParams
499	bert_tokenizer , bert_nq = get_pretrained_model ( FLAGS . model_name ) if not IS_KAGGLE : bert_nq . trainable_variables
392	fig = plt . figure ( 200 , figsize = ( 15 , 15 ) ) random_indicies = np . random . choice ( range ( len ( X_images ) ) , 9 , False ) subset = X_images [ random_indicies ] for i in range ( 9 ) : ax = fig . add_subplot ( 3 , 3 , i + 1 ) ax . imshow ( subset [ i ] ) plt . show ( )
270	train_df [ [ 'ID' , 'Subtype' ] ] = train_df [ 'ID' ] . str . rsplit ( pat = '_' , n = 1 , expand = True ) print ( train_df . shape ) train_df . head ( )
300	sns . lmplot ( 'pickup_Elapsed' , 'fare_amount' , hue = 'pickup_Year' , palette = palette , size = 8 , scatter_kws = { 'alpha' : 0.05 } , markers = '.' , fit_reg = False , data = data . sample ( 1000000 , random_state = RSEED ) ) ; plt . title ( 'Fare Amount versus Time Since Start of Records' ) ;
415	directory = '/kaggle/input/tensorflow2-question-answering/' test_path = directory + 'simplified-nq-test.jsonl' test = build_test ( test_path ) submission = pd . read_csv ( "../input/tensorflow2-question-answering/sample_submission.csv" ) test . head ( )
446	def change ( addr ) : if addr == 60.0 : return 1 elif addr == 96.0 : return 1 elif addr == np . nan : return np . nan else : return 0 df [ "Europe" ] = df [ "addr2" ] . map ( change )
443	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak_df [ 'meter_reading' ] = leak_df . meter_reading_scraped leak_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak_df . fillna ( 0 , inplace = True ) leak_df . loc [ leak_df . meter_reading < 0 , 'meter_reading' ] = 0 leak_df = leak_df [ leak_df . timestamp . dt . year > 2016 ] print ( len ( leak_df ) )
299	lr . fit ( X_train [ [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train ) evaluate ( lr , [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] , X_train , X_valid , y_train , y_valid )
380	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB3 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . summary ( )
114	def my_generator ( ) : while True : for i in range ( 0 , 4 ) : yield i infinity_gen = my_generator ( )
546	history = [ ] resa2 = clustering ( hits , stds , filters , phik = 3.3 , nu = nu , truth = truth , history = history ) resa2 [ "event_id" ] = event_num score = score_event_fast ( truth , resa2 . rename ( index = str , columns = { "label" : "track_id" } ) ) print ( "Your score: " , score )
436	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
215	test_path_audio = os . path . join ( test_path , 'audio' ) test_filenames = os . listdir ( test_path_audio ) test_filenames = np . sort ( test_filenames ) list ( test_filenames ) [ : 10 ]
291	train . fillna ( 0 , inplace = True ) train . replace ( - np . inf , 0 , inplace = True ) train . replace ( np . inf , 0 , inplace = True ) test . fillna ( 0 , inplace = True ) test . replace ( - np . inf , 0 , inplace = True ) test . replace ( np . inf , 0 , inplace = True )
447	def change ( addr ) : if addr == 16.0 : return 1 elif addr == 65.0 : return 1 elif addr == np . nan : return np . nan else : return 0 df [ "Asia" ] = df [ "addr2" ] . map ( change )
15	from fastai import * from fastai . vision import * from sklearn . metrics import f1_score
249	def transpose_df ( df ) : df = df . drop ( [ 'Lat' , 'Long' ] , axis = 1 ) . groupby ( 'Country/Region' ) . sum ( ) . T df . index = pd . to_datetime ( df . index ) return df
505	oversampled_training_dataset = get_training_dataset_with_oversample ( repeat_dataset = False , oversample = True , augumentation = False ) label_counter_2 = Counter ( ) for images , labels in oversampled_training_dataset : label_counter_2 . update ( labels . numpy ( ) ) del oversampled_training_dataset label_counting_sorted_2 = label_counter_2 . most_common ( ) NUM_TRAINING_IMAGES_OVERSAMPLED = sum ( [ x [ 1 ] for x in label_counting_sorted_2 ] ) print ( "number of examples in the oversampled training dataset: {}" . format ( NUM_TRAINING_IMAGES_OVERSAMPLED ) ) print ( "labels in the oversampled training dataset, sorted by occurrence" ) label_counting_sorted_2
408	for folder in [ 'train' , 'test' ] : os . makedirs ( folder )
87	MAX_LEN = 96 PATH = '../input/tf-roberta/' tokenizer = tokenizers . ByteLevelBPETokenizer ( vocab_file = PATH + 'vocab-roberta-base.json' , merges_file = PATH + 'merges-roberta-base.txt' , lowercase = True , add_prefix_space = True ) EPOCHS = 3 BATCH_SIZE = 32 PAD_ID = 1 SEED = 88888 LABEL_SMOOTHING = 0.1 tf . random . set_seed ( SEED ) np . random . seed ( SEED ) sentiment_id = { 'positive' : 1313 , 'negative' : 2430 , 'neutral' : 7974 } train = pd . read_csv ( '../input/tweet-sentiment-extraction/train.csv' ) . fillna ( '' ) train . head ( )
317	app = app . set_index ( 'SK_ID_CURR' ) app = app . merge ( bureau_info , on = 'SK_ID_CURR' , how = 'left' ) del bureau_info app . shape
196	fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 13 , 8 ) sn . countplot ( x = "bedrooms" , data = data , ax = ax1 ) data1 = data . groupby ( [ 'bedrooms' , 'interest_level' ] ) [ 'bedrooms' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) data1 [ [ 'low' , 'medium' , "high" ] ] . plot ( kind = 'bar' , stacked = True , ax = ax2 )
287	model_results = cv_model ( train_set , train_labels , RandomForestClassifier ( 100 , random_state = 10 ) , 'RF' , model_results )
178	def group_by ( df , t1 = '' , t2 = '' ) : a1 = df . groupby ( [ t1 , t2 ] ) [ t2 ] . count ( ) return a1
483	use_cols = [ col for col in train . columns if col not in [ 'card_id' , 'first_active_month' ] ] train = train [ use_cols ] test = test [ use_cols ] features = list ( train [ use_cols ] . columns ) categorical_feats = [ col for col in features if 'feature_' in col ]
35	def count_words_from ( series ) : sentences = series . str . split ( ) vocab = { } for sentence in tqdm ( sentences ) : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
482	rooms = train [ [ "num_room" , "price_doc" ] ] . groupby ( "num_room" ) . aggregate ( np . mean ) . reset_index ( ) mplt . scatter ( x = rooms . num_room , y = rooms . price_doc ) mplt . xlabel ( "Num rooms" ) mplt . ylabel ( 'Mean Price' )
73	import os , random , re , math , time random . seed ( a = 42 ) import numpy as np import pandas as pd import tensorflow as tf import tensorflow . keras . backend as K import efficientnet . tfkeras as efn import PIL from kaggle_datasets import KaggleDatasets from tqdm import tqdm
200	fig , ax1 = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) merged [ "yearbuilt" ] = merged [ "yearbuilt" ] . map ( lambda x : str ( x ) . split ( "." ) [ 0 ] ) yearMerged = merged . groupby ( [ 'yearbuilt' , 'numberofstories' ] ) [ "parcelid" ] . count ( ) . unstack ( 'numberofstories' ) . fillna ( 0 ) yearMerged . plot ( kind = 'bar' , stacked = True , ax = ax1 )
72	def render_neato ( s , format = 'png' , dpi = 100 ) : p = subprocess . Popen ( [ 'neato' , '-T' , format , '-o' , '/dev/stdout' , '-Gdpi={}' . format ( dpi ) ] , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) image , _ = p . communicate ( bytes ( s , encoding = 'utf-8' ) ) return image
492	PRETRAINED_MODELS = { "BERT" : [ 'bert-base-uncased' , 'bert-large-uncased' , 'bert-base-cased' , 'bert-large-cased' , 'bert-base-multilingual-uncased' , 'bert-base-multilingual-cased' , 'bert-base-chinese' , 'bert-base-german-cased' , 'bert-large-uncased-whole-word-masking' , 'bert-large-cased-whole-word-masking' , 'bert-large-uncased-whole-word-masking-finetuned-squad' , 'bert-large-cased-whole-word-masking-finetuned-squad' , 'bert-base-cased-finetuned-mrpc' ] , "DISTILBERT" : [ 'distilbert-base-uncased' , 'distilbert-base-uncased-distilled-squad' ] }
75	feature_score = pd . DataFrame ( preprocessing . MinMaxScaler ( ) . fit_transform ( feature_score ) , columns = feature_score . columns , index = feature_score . index ) feature_score [ 'mean' ] = feature_score . mean ( axis = 1 ) feature_score . sort_values ( 'mean' , ascending = False ) . plot ( kind = 'bar' , figsize = ( 20 , 10 ) )
50	clear_output ( wait = True ) print ( 'Done!' )
465	print ( '[!]Adding \'PAD\' to each sequence...' ) for i in tqdm ( range ( len ( clean_ ) ) ) : sentence = clean_ [ i ] [ : : - 1 ] for _ in range ( maxlen - len ( sentence ) ) : sentence . append ( 'PAD' ) clean_ [ i ] = sentence [ : : - 1 ] print ( ) PAD = np . zeros ( word2vec_ [ 'guy' ] . shape )
24	import random real = [ ] fake = [ ] for m , n in zip ( paths , y ) : if n == 0 : real . append ( m ) else : fake . append ( m ) fake = random . sample ( fake , len ( real ) ) paths , y = [ ] , [ ] for x in real : paths . append ( x ) y . append ( 0 ) for x in fake : paths . append ( x ) y . append ( 1 )
119	DATA_DIR = '../input/gplearn-data' submission = pd . read_csv ( os . path . join ( '../input/LANL-Earthquake-Prediction' , 'sample_submission.csv' ) , index_col = 'seg_id' ) scaled_train_X = pd . read_csv ( os . path . join ( DATA_DIR , 'scaled_train_X_AF0.csv' ) ) scaled_test_X = pd . read_csv ( os . path . join ( DATA_DIR , 'scaled_test_X_AF0.csv' ) ) train_y = pd . read_csv ( os . path . join ( DATA_DIR , 'train_y_AF0.csv' ) ) predictions = np . zeros ( len ( scaled_test_X ) ) print ( 'ok' )
172	from sklearn . feature_extraction . text import HashingVectorizer text = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] vectorizer = HashingVectorizer ( n_features = 6 ) vector = vectorizer . transform ( text ) print ( vector . shape ) print ( vector . toarray ( ) )
181	print ( 'Applicatoin train shape before merge: ' , ap_train . shape ) ap_train = ap_train . merge ( br_data , left_on = 'SK_ID_CURR' , right_on = 'SK_ID_CURR' , how = 'inner' ) print ( 'Applicatoin train shape after merge: ' , ap_train . shape )
529	def oneHotEncode_dataframe ( df , features ) : for feature in features : temp_onehot_encoded = pd . get_dummies ( df [ feature ] ) column_names = [ "{}_{}" . format ( feature , x ) for x in temp_onehot_encoded . columns ] temp_onehot_encoded . columns = column_names df = df . drop ( feature , axis = 1 ) df = pd . concat ( [ df , temp_onehot_encoded ] , axis = 1 ) return df
70	train [ 'coms_length' ] = train [ 'item_description' ] . str . len ( ) pd . options . display . float_format = '{:.2f}' . format train [ 'coms_length' ] . describe ( )
456	preds = clf . predict_proba ( X_test ) [ : , 1 ] clipped_preds = np . clip ( preds , 0.05 , 0.95 ) df_sample_sub . Pred = clipped_preds
210	masks = pd . read_csv ( os . path . join ( '../input/' , 'train_ship_segmentations_v2.csv' ) ) print ( masks . shape [ 0 ] , 'masks found' ) print ( masks [ 'ImageId' ] . value_counts ( ) . shape [ 0 ] ) masks . head ( )
316	import gc def agg_child ( df , parent_var , df_name ) : df_agg = agg_numeric ( df , parent_var , df_name ) df_agg_cat = agg_categorical ( df , parent_var , df_name ) df_info = df_agg . merge ( df_agg_cat , on = parent_var , how = 'outer' ) _ , idx = np . unique ( df_info , axis = 1 , return_index = True ) df_info = df_info . iloc [ : , idx ] gc . enable ( ) del df_agg , df_agg_cat gc . collect ( ) return df_info
332	train = feature_matrix2 [ feature_matrix2 [ 'set' ] == 'train' ] test = feature_matrix2 [ feature_matrix2 [ 'set' ] == 'test' ] train = pd . get_dummies ( train ) test = pd . get_dummies ( test ) train , test = train . align ( test , join = 'inner' , axis = 1 ) test = test . drop ( columns = [ 'TARGET' ] ) print ( 'Final Training Shape: ' , train . shape ) print ( 'Final Testing Shape: ' , test . shape )
413	model . load_weights ( 'model.h5' ) y_test = model . predict_generator ( test_gen , steps = len ( test_gen ) , verbose = 1 )
17	sz = 128 bs = 32 tfms = get_transforms ( do_flip = False , flip_vert = False , max_rotate = 20 , max_zoom = 1.5 , max_lighting = 0.2 )
121	print ( 'ok' )
102	df_train , df_val = train_test_split ( df_data , test_size = 0.1 , random_state = 101 ) print ( df_train . shape ) print ( df_val . shape )
133	def compute_histogram ( img , hist_size = 100 ) : hist = cv2 . calcHist ( [ img ] , [ 0 ] , mask = None , histSize = [ hist_size ] , ranges = ( 0 , 255 ) ) hist = cv2 . normalize ( hist , dst = hist ) return hist
67	plt . figure ( figsize = ( 10 , 10 ) ) sns . boxplot ( x = train . shipping , y = train . price , showfliers = False , orient = 'v' ) plt . title ( 'Does shipping depend of prices ?' , fontsize = 25 ) plt . xlabel ( 'Shipping fee paid by seller (1) or by buyer (0)' , fontsize = 20 ) plt . ylabel ( 'Price without outliers' , fontsize = 20 )
575	plt . hist ( df_train [ 'winPlacePerc' ] ) plt . xlabel ( "winPlacePerc" ) plt . ylabel ( "count" ) plt . title ( 'Distribution of winPlacePerc' )
526	df [ 'MA_7MA' ] = df [ 'close' ] . rolling ( window = 7 ) . mean ( ) df [ 'MA_15MA' ] = df [ 'close' ] . rolling ( window = 15 ) . mean ( ) df [ 'MA_30MA' ] = df [ 'close' ] . rolling ( window = 30 ) . mean ( ) df [ 'MA_60MA' ] = df [ 'close' ] . rolling ( window = 60 ) . mean ( )
193	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( color = " ax.set(xlabel='Hour Of The Day',title=" Reorder Count " )
224	has_to_run_sir = True has_to_run_sird = False has_to_run_seir = True has_to_run_seird = False has_to_run_seirdq = True
126	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
40	cat_dim = [ int ( all_df [ col ] . nunique ( ) ) for col in categorical ] cat_dim = [ [ x , min ( 200 , ( x + 1 ) // 2 ) ] for x in cat_dim ] for el in cat_dim : if el [ 0 ] < 10 : el [ 1 ] = el [ 0 ] cat_dim
68	train [ 'no_descrip' ] = 0 train . loc [ train . item_description == 'No description yet' , 'no_descrip' ] = 1 i = str ( round ( train [ 'no_descrip' ] . value_counts ( normalize = True ) . iloc [ 1 ] * 100 , 2 ) ) + '%' print ( i , 'of the items have no a description.' )
160	bold ( '**Preview of Train Data:**' ) display ( df_train . head ( ) ) bold ( '**Preview of Test Data:**' ) display ( df_test . head ( ) )
337	feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'mean' , 'max' , 'min' , 'trend' , 'mode' , 'count' , 'sum' , 'percent_true' , NormalizedModeCount , MostRecent , LongestSeq ] , trans_primitives = [ 'diff' , 'cum_sum' , 'cum_mean' , 'percentile' ] , where_primitives = [ 'mean' , 'sum' ] , seed_features = [ late_payment , past_due ] , max_depth = 2 , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
177	def type_features ( data ) : categorical_features = data . select_dtypes ( include = [ "object" ] ) . columns numerical_features = data . select_dtypes ( exclude = [ "object" ] ) . columns print ( "categorical_features :" , categorical_features ) print ( '-----' * 40 ) print ( "numerical_features:" , numerical_features )
76	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) import matplotlib . pyplot as plt import featuretools as ft from featuretools . primitives import * from featuretools . variable_types import Numeric from sklearn . preprocessing import LabelEncoder , MinMaxScaler from sklearn . svm import LinearSVR from sklearn . feature_selection import SelectFromModel import warnings warnings . filterwarnings ( "ignore" )
558	import numpy as np import pandas as pd import itertools import random import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' training_tasks = sorted ( os . listdir ( training_path ) )
268	train_all_zero_features = cr . index [ cr ] train . drop ( columns = train_all_zero_features , inplace = True ) count_of_binary_features = ( train . max ( ) == 1 ) . sum ( ) print ( 'Number of binary features: {}' . format ( count_of_binary_features ) )
37	def clean_up_text_with_all_process ( text ) : text = text . lower ( ) text = clean_contractions ( text ) text = clean_special_chars ( text ) text = clean_small_caps ( text ) return text
539	stats = [ ] for Province in sorted ( full_table [ 'Province/State' ] . unique ( ) ) : if ( Province == '' ) : continue df = get_time_series_province ( Province ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 500 ) : continue print ( '{} COVID-19 Prediction' . format ( Province ) ) opt_display_model ( df , stats )
327	train = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_train.csv' ) test = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_test.csv' ) test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
46	train_data , eval_data , train_label , eval_label = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( train_data . shape , eval_data . shape , train_label . shape , eval_label . shape )
475	N = sub_df . shape [ 0 ] x_sub = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( sub_df [ 'Patient' ] ) ) : x_sub [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
517	continuous_features = [ col for col in df_train . columns if col not in binary_cat_features ] continuous_features = [ col for col in continuous_features if col not in features_object ] continuous_features = [ col for col in continuous_features if col not in [ 'Id' , 'Target' , 'idhogar' ] ]
314	threshold = 0.9 corr_matrix = train . corr ( ) . abs ( ) corr_matrix . head ( )
292	sub [ 'surface' ] = le . inverse_transform ( predicted . argmax ( axis = 1 ) ) sub . to_csv ( 'rand_sub_10.csv' , index = False ) sub . head ( )
161	road_encoding = { 'Road' : 1 , 'Street' : 2 , 'Avenue' : 2 , 'Drive' : 3 , 'Broad' : 3 , 'Boulevard' : 4 }
359	y = train_ . target X = train_ . drop ( [ 'target' ] , axis = 1 ) X_test = test_ . copy ( ) features_to_remove = [ 'first_active_month' ] X = X . drop ( features_to_remove , axis = 1 ) X_test = X_test . drop ( features_to_remove , axis = 1 ) assert np . all ( X . columns == X_test . columns ) del train_ , test_ gc . collect ( )
398	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) train2 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv" ) train2 . toxic = train2 . toxic . round ( ) . astype ( int ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
377	plt . figure ( figsize = ( 5 , 5 ) ) dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/train/ID00009637202177434476278/' import pydicom as dcm plt . imshow ( dcm . dcmread ( dir + os . listdir ( dir ) [ 0 ] ) . pixel_array )
141	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yt , yr_pred ) print ( confusion )
330	features_sample = pd . read_csv ( '../input/home-credit-default-risk-feature-tools/feature_matrix.csv' , nrows = 20000 ) features_sample = features_sample [ features_sample [ 'set' ] == 'train' ] features_sample . head ( )
251	df_train = df_train [ df_train [ 'outliers' ] == 0 ] target = df_train [ 'target' ] del df_train [ 'target' ] features = [ c for c in df_train . columns if c not in [ 'card_id' , 'first_active_month' , 'outliers' ] ] categorical_feats = [ c for c in features if 'feature_' in c ]
13	train = pd . read_csv ( filepath_or_buffer = '../input/train.csv' , index_col = 'id' , parse_dates = [ 'pickup_datetime' , 'dropoff_datetime' ] , infer_datetime_format = True ) test = pd . read_csv ( filepath_or_buffer = '../input/test.csv' , index_col = 'id' , parse_dates = [ 'pickup_datetime' ] , infer_datetime_format = True )
261	nom_cols = [ f'nom_{i}' for i in range ( 5 , 10 ) ] fig , ax = plt . subplots ( 5 , 1 , figsize = ( 22 , 17 ) ) for i , col in enumerate ( nom_cols ) : plt . subplot ( 5 , 1 , i + 1 ) sns . countplot ( raw_train [ col ] ) plt . show ( )
379	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
349	application [ 'income_bins' ] = pd . cut ( application [ 'AMT_INCOME_TOTAL' ] , range ( 0 , 1000000 , 10000 ) ) ed = application . groupby ( [ 'TARGET' , 'income_bins' ] ) [ 'TARGET' ] . count ( ) . unstack ( 'TARGET' ) . fillna ( 0 ) ed . plot ( kind = 'bar' , stacked = True , figsize = ( 50 , 15 ) ) print ( ed )
346	model = UNet ( ) model_path = 'model_1.pt' state = torch . load ( str ( model_path ) ) state = { key . replace ( 'module.' , '' ) : value for key , value in state [ 'model' ] . items ( ) } model . load_state_dict ( state ) if torch . cuda . is_available ( ) : model . cuda ( ) model . eval ( )
478	def create_dataset ( dataset , look_back = 1 ) : dataX , dataY = [ ] , [ ] for i in range ( len ( dataset ) - look_back ) : a = dataset [ i : ( i + look_back ) , 0 ] dataX . append ( a ) dataY . append ( dataset [ i + look_back , 0 ] ) print ( len ( dataY ) ) return np . array ( dataX ) , np . array ( dataY )
209	df_text = df [ text_feature ] . fillna ( ' ' ) df_text [ 'full_text' ] = '' for f in text_feature : df_text [ 'full_text' ] = df_text [ 'full_text' ] + df_text [ f ]
191	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( data = orders , x = "order_hour_of_day" , ax = ax , color = " ax.set(xlabel='Hour Of The Day',title=" Order Count Across Hour Of The Day " )
341	bureau_by_loan = bureau_balance_agg . merge ( bureau_balance_counts , right_index = True , left_on = 'SK_ID_BUREAU' , how = 'outer' ) bureau_by_loan = bureau [ [ 'SK_ID_BUREAU' , 'SK_ID_CURR' ] ] . merge ( bureau_by_loan , on = 'SK_ID_BUREAU' , how = 'left' ) bureau_balance_by_client = agg_numeric ( bureau_by_loan . drop ( columns = [ 'SK_ID_BUREAU' ] ) , group_var = 'SK_ID_CURR' , df_name = 'client' )
559	def lift ( fct ) : def lifted_function ( xs ) : list_of_results = [ fct ( x ) for x in xs ] return list ( itertools . chain ( * list_of_results ) ) import re lifted_function . __name__ = re . sub ( '_unlifted$' , '_lifted' , fct . __name__ ) return lifted_function cropToContent = lift ( cropToContent_unlifted ) groupByColor = lift ( groupByColor_unlifted ) splitH = lift ( splitH_unlifted ) negative = lift ( negative_unlifted )
9	embeddings_train = [ ] for i in tqdm ( range ( 25 ) ) : embeddings = embed ( train_text [ i ] ) embeddings_train . append ( embeddings )
198	TARGET_SR = 32_000 PP_NORMALIZE = True MIN_SEC = 1 DEVICE = 'cuda' BATCH_SIZE = 64 SIGMOID_THRESH = 0.3 MAX_BIRDS = None
355	train = pd . read_csv ( "../input/application_train.csv" ) test = pd . read_csv ( "../input/application_test.csv" ) bureau = pd . read_csv ( "../input/bureau.csv" ) bureau_bal = pd . read_csv ( '../input/bureau_balance.csv' )
47	model = CatBoostClassifier ( iterations = 100 , learning_rate = 0.003 , depth = 10 , l2_leaf_reg = 0.01 , loss_function = 'MultiClass' )
431	fig , ax = plt . subplots ( nrows = 10 , ncols = 10 , figsize = ( 50 , 50 ) ) for i in range ( 10 ) : for j in range ( 10 ) : ids = i * 10 + j sns . scatterplot ( X [ 'var_' + str ( ids ) ] , shap_values [ : , ids ] , ax = ax [ i , j ] )
296	corrs = data . corr ( ) corrs [ 'fare_amount' ] . plot . bar ( color = 'b' ) ; plt . title ( 'Correlation with Fare Amount' ) ;
242	date_agg_2 = train_agg . groupby ( level = 1 ) . sum ( ) date_agg_2 . columns = ( 'bookings' , 'total' ) date_agg_2 . index . name = 'Year' date_agg_2 . plot ( kind = 'bar' , stacked = 'True' )
458	from collections import Counter cls_counts = Counter ( cls for classes in train [ 'attribute_ids' ] . str . split ( ) for cls in classes ) print ( len ( cls_counts ) )
101	df_0 = df_train [ df_train [ 'binary_target' ] == 0 ] df_1 = df_train [ df_train [ 'binary_target' ] == 1 ] . sample ( len ( df_0 ) , random_state = 101 ) df_data = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_data = shuffle ( df_data ) print ( df_data . shape ) df_data . head ( )
396	with strategy . scope ( ) : transformer_layer = TFAutoModel . from_pretrained ( MODEL ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
203	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "roomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Room Count ",title=" Room Count Vs Log Error " )
562	def seed_everything ( seed = 1029 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( )
221	df_grouped_iran = get_df_country_cases ( df_covid , "Iran" ) df_iran_cases_by_day = df_grouped_iran [ df_grouped_iran . confirmed > 0 ] df_iran_cases_by_day = df_iran_cases_by_day . reset_index ( drop = True ) df_iran_cases_by_day [ 'day' ] = df_iran_cases_by_day . date . apply ( lambda x : ( x - df_iran_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_iran_cases_by_day = df_iran_cases_by_day [ reordered_columns ] df_iran_cases_by_day
582	train_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . notnull ( ) ] . copy ( ) test_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . isnull ( ) ] . copy ( ) test_df . drop ( [ 'TARGET' ] , axis = 1 , inplace = True ) del feature_matrix , feature_defs , feature_matrix_enc gc . collect ( )
125	from PIL import Image import seaborn as sns def _get_image_data_pil ( image_id , image_type , return_exif_md = False ) : fname = get_filename ( image_id , image_type ) try : img_pil = Image . open ( fname ) except Exception as e : assert False , "Failed to read image : %s, %s. Error message: %s" % ( image_id , image_type , e ) img = np . asarray ( img_pil ) assert isinstance ( img , np . ndarray ) , "Open image is not an ndarray. Image id/type : %s, %s" % ( image_id , image_type ) if not return_exif_md : return img else : return img , img_pil . _getexif ( )
468	filename = "/kaggle/input/osic-pulmonary-fibrosis-progression/train/ID00123637202217151272140/137.dcm" ds = pydicom . dcmread ( filename ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone )
350	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) result = test [ [ 'id' ] ] . copy ( ) print ( train . head ( 3 ) )
387	img = cv2 . imread ( f'../input/train/{label_df.loc[0,"Image"]}' ) pad_width = get_pad_width ( img , max ( img . shape ) ) padded = np . pad ( img , pad_width = pad_width , mode = 'constant' , constant_values = 0 ) resized = cv2 . resize ( padded , ( 224 , 224 ) ) plt . imshow ( resized )
309	train = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_train.csv' ) test = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_test.csv' ) test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
55	IP = df [ 'ip' ] . value_counts ( ) plt . figure ( figsize = [ 10 , 5 ] ) sns . boxplot ( IP ) plt . title ( 'Number of click by IP' , fontsize = 15 )
222	df_grouped_usa = get_df_country_cases ( df_covid , "US" ) df_usa_cases_by_day = df_grouped_usa [ df_grouped_usa . confirmed > 0 ] df_usa_cases_by_day = df_usa_cases_by_day . reset_index ( drop = True ) df_usa_cases_by_day [ 'day' ] = df_usa_cases_by_day . date . apply ( lambda x : ( x - df_usa_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_usa_cases_by_day = df_usa_cases_by_day [ reordered_columns ] df_usa_cases_by_day
297	X_train , X_valid , y_train , y_valid = train_test_split ( data , np . array ( data [ 'fare_amount' ] ) , stratify = data [ 'fare-bin' ] , random_state = RSEED , test_size = 1_000_000 )
378	preds = test_preds . argmax ( axis = 1 ) . astype ( np . int8 ) print ( f'predicted accuracy_group distribution:\n\n{pd.Series(preds).value_counts(normalize=True)} \n\n' ) submission [ 'accuracy_group' ] = preds submission . to_csv ( 'submission.csv' , index = False )
507	import re def breakdown_topic ( str ) : m = re . search ( '(.*)\_(.*).wikipedia.org\_(.*)\_(.*)' , str ) if m is not None : return m . group ( 1 ) , m . group ( 2 ) , m . group ( 3 ) , m . group ( 4 ) else : return "" , "" , "" , "" print ( breakdown_topic ( "Рудова,_Наталья_Александровна_ru.wikipedia.org_all-access_spider" ) ) print ( breakdown_topic ( "台灣災難列表_zh.wikipedia.org_all-access_spider" ) ) print ( breakdown_topic ( "File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents" ) )
422	def display_blurry_samples ( df , img_id_list , columns = 4 , rows = 3 ) : fig = plt . figure ( figsize = ( 5 * columns , 4 * rows ) ) for i in range ( columns * rows ) : img = preprocess_image ( f'../input/train_images/{img_id_list[i]}.png' ) fig . add_subplot ( rows , columns , i + 1 ) plt . title ( f'index:{i} isclear:{isClear(img)}' ) plt . imshow ( img ) plt . tight_layout ( ) display_blurry_samples ( train_df , blur_list_id )
556	patientId = df [ 'patientId' ] [ 8 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 2 - Lung Opacity" ) draw ( parsed [ patientId ] )
382	save_dir = '/kaggle/tmp/fake/' if not os . path . exists ( save_dir ) : os . makedirs ( save_dir )
336	app_types = { } for col in app_train : if ( app_train [ col ] . dtype != 'object' ) and ( len ( app_train [ col ] . unique ( ) ) <= 2 ) : app_types [ col ] = ft . variable_types . Boolean print ( 'Number of boolean variables: ' , len ( app_types ) )
501	print ( f"{ckpt_manager._directory}" ) os . system ( f"ls -l {ckpt_manager._directory} > results.txt" ) with open ( "results.txt" , "r" , encoding = "UTF-8" ) as fp : for line in fp : print ( line . strip ( ) )
65	plt . figure ( figsize = ( 20 , 20 ) ) sns . boxplot ( x = 'price' , y = 'cat1' , data = train , orient = 'h' ) plt . title ( 'Prices of the first level of categories' , fontsize = 30 ) plt . ylabel ( 'First level categories' , fontsize = 20 ) plt . xlabel ( 'Price' , fontsize = 20 )
248	confirmed = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv' ) . sort_values ( by = 'Country/Region' ) deaths = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv' ) recovered = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv' )
417	lwk = metrics . cohen_kappa_score ( y_true , y_pred , weights = 'linear' ) qwk = metrics . cohen_kappa_score ( y_true , y_pred , weights = 'quadratic' ) \ print ( "Linear Weighted Kappa Score:" , lwk ) print ( "Quadratic Weighted Kappa Score:" , qwk )
266	print ( 'Coefficient of variation (CV) for prices in different recognized image categories.' ) dfl = dff . merge ( all_image_labels , left_on = 'image' , right_index = True , how = 'left' ) dfd = dfl . groupby ( 'image_label' ) [ 'price' ] . apply ( lambda x : np . std ( x ) / np . mean ( x ) ) . sort_values ( ascending = False ) dfd . head ( 10 )
462	def get_model ( ) : with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB3 ( input_shape = ( * IMAGE_SIZE , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ tf . keras . metrics . AUC ( ) ] , ) return model
319	installments = pd . read_csv ( '../input/installments_payments.csv' ) . replace ( { 365243 : np . nan } ) installments = convert_types ( installments ) installments [ 'LATE' ] = installments [ 'DAYS_ENTRY_PAYMENT' ] > installments [ 'DAYS_INSTALMENT' ] installments [ 'LOW_PAYMENT' ] = installments [ 'AMT_PAYMENT' ] < installments [ 'AMT_INSTALMENT' ]
587	text = list ( train . text . values ) tf_vectorizer = LemmaCountVectorizer ( max_df = 0.95 , min_df = 2 , stop_words = 'english' , decode_error = 'ignore' ) tf = tf_vectorizer . fit_transform ( text ) print ( tf_vectorizer . get_feature_names ( ) )
153	bold ( '**MONTHLY READINGS ARE HIGHEST CHANGES BASED ON BUILDING TYPE**' ) temp_df = train . groupby ( [ 'month' , 'primary_use' ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . FacetGrid ( temp_df , col = "primary_use" , col_wrap = 2 , height = 4 , aspect = 2 , sharey = False ) ax . map ( plt . plot , 'month' , 'meter_reading' , color = "teal" , linewidth = 3 ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
61	print ( 'Original image shape: {}' . format ( im . shape ) ) from skimage . color import rgb2gray im_gray = rgb2gray ( im ) print ( 'New image shape: {}' . format ( im_gray . shape ) )
547	pos_train = df_train . query ( 'sentiment=="positive"' ) neg_train = df_train . query ( 'sentiment=="negative"' ) neu_train = df_train . query ( 'sentiment=="neutral"' ) pos_val = df_train . query ( 'sentiment=="positive"' ) neg_val = df_train . query ( 'sentiment=="negative"' ) neu_val = df_train . query ( 'sentiment=="neutral"' ) pos_test = df_test . query ( 'sentiment=="positive"' ) neg_test = df_test . query ( 'sentiment=="negative"' ) neu_test = df_test . query ( 'sentiment=="neutral"' )
340	import gc gc . enable ( ) del train , bureau , bureau_balance , bureau_agg , bureau_agg_new , bureau_balance_agg , bureau_balance_counts , bureau_by_loan , bureau_balance_by_client , bureau_counts gc . collect ( )
548	for i in tqdm ( range ( len ( df_test ) ) ) : if df_test [ 'sentiment' ] . iloc [ i ] == 'neutral' : df_test [ 'selected_text' ] . iloc [ i ] = df_test [ 'text' ] . iloc [ i ] else : pass
381	original_fake_paths = [ ] for dirname , _ , filenames in tqdm ( os . walk ( '/kaggle/input/1-million-fake-faces/' ) ) : for filename in filenames : original_fake_paths . append ( [ os . path . join ( dirname , filename ) , filename ] )
493	def jsonl_iterator ( jsonl_files , to_json = False ) : for file_path in jsonl_files : with open ( file_path , "r" , encoding = "UTF-8" ) as fp : for jsonl in fp : raw_example = jsonl if to_json : raw_example = json . loads ( jsonl ) yield raw_example creator = TFExampleCreator ( is_training = False ) nq_lines = jsonl_iterator ( [ FLAGS . predict_file ] ) creator . process_nq_lines ( nq_lines = nq_lines , output_tfrecord = FLAGS . test_tf_record , max_examples = 0 , collect_nq_features = False )
131	train_df = pd . read_csv ( os . path . join ( DATA_DIR , 'train.csv' ) , dtype = { 'acoustic_data' : np . int16 , 'time_to_failure' : np . float32 } ) print ( train_df . shape ) print ( 'ok' )
284	new_col = [ ] for c in ind_agg . columns . levels [ 0 ] : for stat in ind_agg . columns . levels [ 1 ] : new_col . append ( f'{c}-{stat}' ) ind_agg . columns = new_col ind_agg . head ( )
156	train [ 'square_feet' ] = np . log1p ( train [ 'square_feet' ] ) test [ 'square_feet' ] = np . log1p ( test [ 'square_feet' ] ) bold ( '**Distribution after log tranformation**' ) distplot ( train [ 'square_feet' ] , 'darkgreen' )
521	cols_with_only_one_value = [ ] for col in df_train . columns : if col == 'Target' : continue if df_train [ col ] . value_counts ( ) . shape [ 0 ] == 1 or df_test [ col ] . value_counts ( ) . shape [ 0 ] == 1 : print ( col ) cols_with_only_one_value . append ( col )
241	date_agg_1 = train_agg . groupby ( level = 0 ) . agg ( [ 'sum' ] ) date_agg_1 . columns = ( 'bookings' , 'total' ) date_agg_1 . head ( )
328	default_agg_primitives = [ "sum" , "std" , "max" , "skew" , "min" , "mean" , "count" , "percent_true" , "num_unique" , "mode" ] default_trans_primitives = [ "day" , "year" , "month" , "weekday" , "haversine" , "numwords" , "characters" ] feature_names = ft . dfs ( entityset = es , target_entity = 'app' , trans_primitives = default_trans_primitives , agg_primitives = default_agg_primitives , max_depth = 2 , features_only = True ) print ( '%d Total Features' % len ( feature_names ) )
540	import plotly . express as px fig = px . histogram ( df [ [ 'age' , 'gender' , 'hospital_death' , 'bmi' ] ] . dropna ( ) , x = "age" , y = "hospital_death" , color = "gender" , marginal = "box" , hover_data = df [ [ 'age' , 'gender' , 'hospital_death' , 'bmi' ] ] . columns ) fig . show ( )
36	def clean_special_chars ( text ) : for s in specail_signs : text = text . replace ( s , specail_signs [ s ] ) for p in punct : text = text . replace ( p , f' {p} ' ) return text
352	def rle_encode ( im ) : pixels = im . flatten ( order = 'F' ) pixels = np . concatenate ( [ [ 0 ] , pixels , [ 0 ] ] ) runs = np . where ( pixels [ 1 : ] != pixels [ : - 1 ] ) [ 0 ] + 1 runs [ 1 : : 2 ] -= runs [ : : 2 ] return ' ' . join ( str ( x ) for x in runs )
463	import numpy as np import pandas as pd from tqdm import tqdm import copy import multiprocessing import nltk import re import gensim . models . word2vec as w2v import matplotlib . pyplot as plt
78	lsvr = LinearSVR ( C = 0.05 , max_iter = 1000 ) . fit ( dfe , target_fe ) model = SelectFromModel ( lsvr , prefit = True ) X_new = model . transform ( dfe ) X_selected_df = pd . DataFrame ( X_new , columns = [ dfe . columns [ i ] for i in range ( len ( dfe . columns ) ) if model . get_support ( ) [ i ] ] ) X_selected_df . shape
519	for col1 in [ 'epared1' , 'epared2' , 'epared3' ] : for col2 in [ 'etecho1' , 'etecho2' , 'etecho3' ] : for col3 in [ 'eviv1' , 'eviv2' , 'eviv3' ] : new_col_name = 'new_{}_x_{}_x_{}' . format ( col1 , col2 , col3 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] * df_train [ col3 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ] * df_train [ col3 ]
260	import warnings warnings . filterwarnings ( "ignore" ) import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns from sklearn . impute import SimpleImputer import scipy from sklearn . linear_model import LogisticRegression from sklearn . metrics import auc , roc_curve from sklearn . model_selection import StratifiedKFold , GridSearchCV from tqdm import tqdm_notebook
220	df_grouped_spain = get_df_country_cases ( df_covid , "Spain" ) df_spain_cases_by_day = df_grouped_spain [ df_grouped_spain . confirmed > 0 ] df_spain_cases_by_day = df_spain_cases_by_day . reset_index ( drop = True ) df_spain_cases_by_day [ 'day' ] = df_spain_cases_by_day . date . apply ( lambda x : ( x - df_spain_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_spain_cases_by_day = df_spain_cases_by_day [ reordered_columns ] df_spain_cases_by_day
420	train_label = train_df [ 'label' ] test_indices = test_df [ 'id' ] train_df = train_df . drop ( [ 'label' ] , axis = 1 ) test_df = test_df . drop ( [ 'id' ] , axis = 1 ) train_x = train_df . values train_y = train_label . values test_x = test_df . values print ( "shape of train_x :" , train_x . shape ) print ( "shape of train_y :" , train_y . shape ) print ( "shape of test_x :" , test_x . shape )
106	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
273	from itertools import product tta_transforms = [ ] for tta_combination in product ( [ TTAHorizontalFlip ( ) , None ] , [ TTAVerticalFlip ( ) , None ] , [ TTARotate90 ( ) , None ] ) : tta_transforms . append ( TTACompose ( [ tta_transform for tta_transform in tta_combination if tta_transform ] ) )
62	from scipy import ndimage labels , nlabels = ndimage . label ( mask ) label_arrays = [ ] for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) label_arrays . append ( label_mask ) print ( 'There are {} separate components / objects detected.' . format ( nlabels ) )
139	from sklearn . ensemble import RandomForestClassifier rfc = RandomForestClassifier ( n_estimators = 10 , n_jobs = 2 , random_state = 0 ) rfc . fit ( x , y )
59	df = pd . read_csv ( '../input/train.csv' , skiprows = 9308568 , nrows = 59633310 ) header = pd . read_csv ( '../input/train.csv' , nrows = 0 ) df . columns = header . columns df del header gc . collect ( ) print ( "The created dataframe contains" , df . shape [ 0 ] , "rows." )
374	train_clean [ 'TransactionAmt' ] = np . log ( train_clean [ 'TransactionAmt' ] + 1 ) train_clean [ 'dist1' ] = np . log ( train_clean [ 'dist1' ] + 1 ) train_clean [ 'dist2' ] = np . log ( train_clean [ 'dist2' ] + 1 ) test_clean [ 'TransactionAmt' ] = np . log ( test_clean [ 'TransactionAmt' ] + 1 ) test_clean [ 'dist1' ] = np . log ( test_clean [ 'dist1' ] + 1 ) test_clean [ 'dist2' ] = np . log ( test_clean [ 'dist2' ] + 1 )
39	for feat in tqdm ( features ) : lbl_enc = preprocessing . LabelEncoder ( ) all_df [ feat ] = lbl_enc . fit_transform ( all_df [ feat ] . \ fillna ( '-1' ) . \ astype ( str ) . values ) all_df [ 'target' ] = all_df [ 'target' ] . fillna ( - 1 ) all_df [ continuous ] = all_df [ continuous ] . fillna ( - 2 )
227	def generate_word_cloud ( text ) : wordcloud = WordCloud ( width = 3000 , height = 2000 , background_color = 'black' ) . generate ( str ( text ) ) fig = plt . figure ( figsize = ( 40 , 30 ) , facecolor = 'k' , edgecolor = 'k' ) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . tight_layout ( pad = 0 ) plt . show ( )
472	train_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) test_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' ) sub_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' ) print ( train_df . shape ) print ( test_df . shape ) train_df . head ( )
246	df_hmp [ 'log1p_Demanda_uni_equil_sum' ] = np . log1p ( df_hmp . Demanda_uni_equil_sum ) g = sns . FacetGrid ( df_hmp , row = 'Semana' ) g = g . map ( sns . distplot , 'log1p_Demanda_uni_equil_sum' )
264	cheap = df [ df . price < 10 ] . category_name . value_counts ( ) . map ( lambda x : '{:.2f}%' . format ( x / df . shape [ 0 ] * 100 ) ) print ( 'Categories of items < 10 \u20BD (top 10)' ) cheap . head ( 10 )
553	seed = 42 random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . backends . cudnn . deterministic = True if torch . cuda . is_available ( ) : torch . cuda . manual_seed_all ( seed )
188	dtypestrain = { } dtypestrain [ 'ID_code' ] = 'category' dtypestrain [ 'target' ] = 'int8' for i in range ( 0 , 200 ) : dtypestrain [ 'var_' + str ( i ) ] = 'float32' dtypestest = { } dtypestest [ 'ID_code' ] = 'category' for i in range ( 0 , 200 ) : dtypestest [ 'var_' + str ( i ) ] = 'float32'
393	train = pd . read_csv ( "../input/liverpool-ion-switching/train.csv" ) test = pd . read_csv ( "../input/liverpool-ion-switching/test.csv" ) sub = pd . read_csv ( "../input/liverpool-ion-switching/sample_submission.csv" , dtype = dict ( time = str ) )
19	preds = learn . get_preds ( ds_type = DatasetType . Test ) preds = np . argmax ( preds [ 0 ] . numpy ( ) , axis = 1 ) categories = sorted ( train . genres . unique ( ) . astype ( 'str' ) ) final_preds = [ ] for idx in preds : final_preds . append ( categories [ idx ] ) final_submit = pd . read_csv ( '../input/clabscvcomp/data/sample_submission.csv' ) final_submit . genres = final_preds final_submit . head ( ) final_submit . to_csv ( 'submission.csv' , index = False )
111	df_preds = pd . DataFrame ( predictions ) new_names = [ 'conf_1' , 'x_1' , 'y_1' , 'width_1' , 'height_1' , 'conf_2' , 'x_2' , 'y_2' , 'width_2' , 'height_2' ] df_preds . columns = new_names df_preds [ 'patientId' ] = df_test [ 'patientId' ] df_preds [ 'PredictionString' ] = 0
551	import os import sys sys . path . append ( "../input/pystacknet/repository/h2oai-pystacknet-af571e0" ) import pystacknet
369	labels_breed = pd . read_csv ( '../input/breed_labels.csv' ) labels_state = pd . read_csv ( '../input/color_labels.csv' ) labels_color = pd . read_csv ( '../input/state_labels.csv' )
537	stats = [ ] for country in [ 'US' , 'United Kingdom' , 'Russia' , 'Singapore' , 'New Zealand' ] : df = get_time_series ( country ) print ( '{} COVID-19 Prediction' . format ( country ) ) opt_display_model ( df , stats )
100	def binary_target ( x ) : if x != 0 : return 1 else : return x df_train [ 'binary_target' ] = df_train [ 'diagnosis' ] . apply ( binary_target )
233	import numpy as np import pandas as pd import seaborn as sns import matplotlib as plt import tensorflow as tf from tensorflow . keras . preprocessing import text , sequence from tensorflow . keras . models import Sequential from tensorflow . keras . layers import Dense , Dropout , Activation from tensorflow . keras . layers import Embedding from tensorflow . keras . layers import Conv1D , GlobalMaxPooling1D , MaxPooling1D from sklearn . model_selection import train_test_split print ( tf . __version__ )
579	feature_matrix , feature_defs = ft . dfs ( entityset = es , target_entity = 'applications' , drop_contains = [ 'SK_ID_PREV' ] , max_depth = 2 , verbose = True )
254	from pathlib import Path if Path ( previous_model_name ) . is_file ( ) : print ( "Using previous sucessful run's model" ) model2 = load_model ( previous_model_name , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } ) else : print ( "Using stored trained model" ) model2 = load_model ( stored_trained_model , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } )
394	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) train2 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv" ) train2 . toxic = train2 . toxic . round ( ) . astype ( int ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
228	positive_train = train [ train [ "sentiment" ] == "positive" ] negative_train = train [ train [ "sentiment" ] == "negative" ] neutral_train = train [ train [ "sentiment" ] == "neutral" ]
69	wc = WordCloud ( background_color = "white" , max_words = 5000 , stopwords = STOPWORDS , max_font_size = 50 ) wc . generate ( " " . join ( str ( s ) for s in train . item_description . values ) ) plt . figure ( figsize = ( 20 , 12 ) ) plt . axis ( 'off' ) plt . imshow ( wc , interpolation = 'bilinear' )
168	train_df = train_transaction . merge ( train_identity , how = 'left' , left_index = True , right_index = True ) test_df = test_transaction . merge ( test_identity , how = 'left' , left_index = True , right_index = True ) print ( "Train shape : " + str ( train_df . shape ) ) print ( "Test shape : " + str ( test_df . shape ) )
320	cash = pd . read_csv ( '../input/POS_CASH_balance.csv' ) . replace ( { 365243 : np . nan } ) cash = convert_types ( cash ) cash [ 'LATE_PAYMENT' ] = cash [ 'SK_DPD' ] > 0.0 cash [ 'INSTALLMENTS_PAID' ] = cash [ 'CNT_INSTALMENT' ] - cash [ 'CNT_INSTALMENT_FUTURE' ]
421	import time blur_list = [ ] blur_list_id = [ ] start_time = time . time ( ) ; for i , image_id in enumerate ( tqdm ( train_df [ 'id_code' ] ) ) : img = preprocess_image ( f'../input/train_images/{image_id}.png' ) if ( not isClear ( img ) ) : blur_list . append ( i ) blur_list_id . append ( image_id ) train_df = train_df . drop ( blur_list ) print ( f'Cost: {time.time() - start_time}:.3% seconds' ) ;
497	if FLAGS . do_predict : test_features = ( tf . train . Example . FromString ( r . numpy ( ) ) for r in tf . data . TFRecordDataset ( FLAGS . test_tf_record ) ) predictions_json = get_prediction_json ( mode = 'test' , max_nb_pos_logits = FLAGS . n_best_size )
479	look_back = 1 trainX , trainY = create_dataset ( V_train , look_back ) testX , testY = create_dataset ( V_test , look_back )
484	data_dir = '../input/alaska2-image-steganalysis' folder_names = [ 'JMiPOD/' , 'JUNIWARD/' , 'UERD/' ] class_names = [ 'Normal' , 'JMiPOD_75' , 'JMiPOD_90' , 'JMiPOD_95' , 'JUNIWARD_75' , 'JUNIWARD_90' , 'JUNIWARD_95' , 'UERD_75' , 'UERD_90' , 'UERD_95' ] class_labels = { name : i for i , name in enumerate ( class_names ) }
541	h1_col = [ s for s in col if "h1_" in s ] d1_col = [ s for s in col if "d1_" in s ] h1d1 = list ( pd . Series ( h1_col ) . str . replace ( "h1_" , "" ) ) for c in h1d1 : All_df [ 'diff_' + c ] = All_df [ 'd1_' + c ] - All_df [ 'h1_' + c ]
333	random [ 'set' ] = 'random' scores = random [ [ 'score' , 'iteration' , 'set' ] ] opt [ 'set' ] = 'opt' scores = scores . append ( opt [ [ 'set' , 'iteration' , 'score' ] ] , sort = True ) scores . head ( )
495	if FLAGS . do_valid : validation_features = ( tf . train . Example . FromString ( r . numpy ( ) ) for r in tf . data . TFRecordDataset ( valid_tf_record ) ) valid_predictions_json = get_prediction_json ( mode = 'valid' , max_nb_pos_logits = FLAGS . n_best_size )
44	test_filenames = os . listdir ( "../input/test1/test1" ) test_df = pd . DataFrame ( { 'filename' : test_filenames } ) nb_samples = test_df . shape [ 0 ]
324	com = 1 for x in param_grid . values ( ) : com *= len ( x ) print ( 'There are {} combinations' . format ( com ) )
244	date_agg_4 = train_agg . groupby ( level = [ 1 , 2 , 3 ] ) . sum ( ) date_agg_4 . columns = ( 'bookings' , 'total' ) date_agg_4 . reset_index ( inplace = True ) date_agg_4 [ 'dt' ] = pd . to_datetime ( date_agg_4 . year * 10000 + date_agg_4 . month * 100 + date_agg_4 . day , format = '%Y%m%d' ) date_agg_4 . head ( )
115	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) import matplotlib . pyplot as plt import featuretools as ft from featuretools . primitives import * from featuretools . variable_types import Numeric from sklearn . preprocessing import LabelEncoder , MinMaxScaler from sklearn . svm import LinearSVR from sklearn . feature_selection import SelectFromModel from sklearn . ensemble import RandomForestRegressor import warnings warnings . filterwarnings ( "ignore" )
211	images_with_ship = masks . ImageId [ masks . EncodedPixels . isnull ( ) == False ] images_with_ship = np . unique ( images_with_ship . values ) print ( 'There are ' + str ( len ( images_with_ship ) ) + ' image files with masks' )
265	train_image_labels = pd . read_csv ( '../input/avito-images-recognized/train_image_labels.csv' , index_col = 'image_id' ) test_image_labels = pd . read_csv ( '../input/avito-images-recognized/test_image_labels.csv' , index_col = 'image_id' ) all_image_labels = pd . concat ( [ train_image_labels , test_image_labels ] , axis = 0 )
293	plt . figure ( figsize = ( 10 , 6 ) ) sns . distplot ( data [ 'fare_amount' ] ) ; plt . title ( 'Distribution of Fare' ) ;
543	temp = train [ 'ip' ] . value_counts ( ) . reset_index ( name = 'counts' ) temp . columns = [ 'ip' , 'counts' ] temp [ : 10 ]
269	i = test [ test [ 'key_id' ] == 9000052667981386 ] . iloc [ 0 ] [ 'drawing' ] img = draw_cv2 ( ast . literal_eval ( i ) , img_size = 256 ) plt . imshow ( img )
163	bold ( '**Updated train data for modelling:**' ) display ( df_train . head ( 3 ) ) bold ( '**Updated test data for modelling:**' ) display ( df_test . head ( 3 ) )
18	learn . unfreeze ( ) learn . lr_find ( ) learn . recorder . plot ( suggestion = True )
253	length = 5 labels = [ ] for label in df_train [ "labels" ] : if type ( label ) == str : split_label = label . split ( ) [ : : length ] labels += split_label
564	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
555	patientId = df [ 'patientId' ] [ 3 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 1 - Normal Image" ) draw ( parsed [ patientId ] )
29	X = train . Image . values del train [ 'Image' ] Y = train . values
169	import pandas as pd import numpy as np from sklearn . model_selection import StratifiedKFold import lightgbm as lgb from sklearn import metrics import gc pd . set_option ( 'display.max_columns' , 200 )
250	dftrainall = dftrain . join ( pop , on = 'Country_Region' ) dftrainall [ 'Lockdown' ] = dftrainlockdown [ 'Lockdown' ] dftrainall = dftrainall . join ( flights , on = 'Country_Region' ) dftrainall [ 'Mortality' ] = dftrainall [ 'Fatalities' ] / dftrainall [ 'ConfirmedCases' ] dftrainall [ 'ConfirmedCases_by_pop' ] = dftrainall [ 'ConfirmedCases' ] / dftrainall [ 'Population (2020)' ] dftrainall [ 'ConfirmedCases_by_Km²' ] = dftrainall [ 'ConfirmedCases' ] / dftrainall [ 'Land Area (Km²)' ] dftrainall . tail ( )
236	filters = 250 kernel_size = 3 hidden_dims = 250
461	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
459	label_map = dict ( labels [ [ 'attribute_id' , 'attribute_name' ] ] . values . tolist ( ) ) not_in_train_labels = set ( labels [ 'attribute_id' ] . astype ( str ) . values ) - set ( list ( cls_counts ) ) for _id in not_in_train_labels : label = label_map [ int ( _id ) ] print ( f'attribute_id: {_id} attribute_name: {label}' )
391	clf = xgb . XGBClassifier ( ** grid . best_params_ ) clf . fit ( X_train , y_train ) sample_submission [ 'isFraud' ] = clf . predict_proba ( X_test ) [ : , 1 ] sample_submission . to_csv ( 'simple_xgboost.csv' )
450	import numpy as np import pandas as pd import efficientnet . tfkeras as efn from tensorflow . keras import backend as K import tensorflow_addons as tfa import tensorflow . keras . layers as layers import tensorflow as tf
313	train_labels = train_bureau [ 'TARGET' ] previous_features . append ( 'SK_ID_CURR' ) train_ids = train_bureau [ 'SK_ID_CURR' ] test_ids = test_bureau [ 'SK_ID_CURR' ] train = train_bureau . merge ( train_previous [ previous_features ] , on = 'SK_ID_CURR' ) test = test_bureau . merge ( test_previous [ previous_features ] , on = 'SK_ID_CURR' ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
79	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'best' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmin ( ) , 'best' ] = 1
452	epoch_datetime = pd . datetime ( 1900 , 1 , 1 ) trf_var_68_s = ( train_df [ 'var_68' ] * 10000 - 7000 + epoch_datetime . toordinal ( ) ) . astype ( int ) date_s = trf_var_68_s . map ( datetime . fromordinal ) train_df [ 'date' ] = date_s sorted_train_df = train_df . drop ( 'var_68' , axis = 1 ) . sort_values ( 'date' )
2	numeric_dtypes = [ 'float64' ] numerics = [ ] for i in train . columns : if train [ i ] . dtype in numeric_dtypes : numerics . append ( i )
357	labels_breed = pd . read_csv ( '../input/breed_labels.csv' ) labels_state = pd . read_csv ( '../input/color_labels.csv' ) labels_color = pd . read_csv ( '../input/state_labels.csv' )
358	column_types = X . dtypes int_cols = column_types [ column_types == 'int' ] float_cols = column_types [ column_types == 'float' ] cat_cols = column_types [ column_types == 'object' ] print ( '\tinteger columns:\n{}' . format ( int_cols ) ) print ( '\n\tfloat columns:\n{}' . format ( float_cols ) ) print ( '\n\tto encode categorical columns:\n{}' . format ( cat_cols ) )
127	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
8	col = 'identity_hate' print ( "Column:" , col ) pred = lr . predict ( X ) print ( '\nConfusion matrix\n' , confusion_matrix ( y [ col ] , pred ) ) print ( classification_report ( y [ col ] , pred ) )
524	train_df . head ( ) percent = ( 100 * train_df . isnull ( ) . sum ( ) / train_df . shape [ 0 ] ) . sort_values ( ascending = False ) percent [ : 10 ]
137	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yT , y_pred ) print ( confusion )
280	x = np . array ( range ( - 19 , 20 ) ) y = 2 * np . sin ( x ) plot_corrs ( x , y )
593	lidar_data = [ ] image_data = [ ] for record in data_json : if record [ 'fileformat' ] == 'jpeg' : image_data . append ( record ) else : lidar_data . append ( record )
138	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yt , yt_pred ) print ( confusion )
405	trials_df = pd . DataFrame ( [ parse_trial_state ( t ) for t in tuner . oracle . trials . values ( ) ] ) trials_df . to_csv ( 'trials_table.csv' , index = False ) trials_df
238	def perform_rfc ( df_X , df_Y , test_df_X , test_Y ) : rfr_clf = RandomForestRegressor ( n_estimators = 100 , oob_score = True , max_features = "auto" ) rfr_clf . fit ( df_X , df_Y ) pred_Y = rfr_clf . predict ( test_df_X ) r2_score_rfc = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( rfr_clf . score ( df_X , df_Y ) * 100 , 2 ) returnval = { 'model' : 'RandomForestRegressor' , 'r2_score' : r2_score_rfc } return returnval
214	img2 = cv2 . bitwise_and ( image [ 400 : 600 , 200 : 400 ] , image [ 400 : 600 , 200 : 400 ] , mask = total_mask [ 400 : 600 , 200 : 400 ] . astype ( np . uint8 ) ) plt . figure ( figsize = ( 8 , 8 ) ) total_mask [ total_mask > 1 ] = 0 plt . title ( 'Masks over image' ) plt . imshow ( img2 ) plt . show ( )
167	import pandas as pd import numpy as np from sklearn . model_selection import StratifiedKFold from sklearn import metrics import gc import xgboost as xgb pd . set_option ( 'display.max_columns' , 200 )
437	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak0_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak0_df [ 'meter_reading' ] = leak0_df . meter_reading_scraped leak0_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak0_df . fillna ( 0 , inplace = True ) leak0_df . loc [ leak0_df . meter_reading < 0 , 'meter_reading' ] = 0 leak0_df = leak0_df [ leak0_df . timestamp . dt . year > 2016 ] print ( len ( leak0_df ) )
432	fig , axs = plt . subplots ( 5 , 1 , figsize = ( 10 , 15 ) ) axs . flatten ( ) sample = train [ train . SN_filter == 1 ] . sample ( 1 ) . iloc [ 0 ] for i , err_col in enumerate ( err_cols ) : axs [ i ] . plot ( sample [ err_col ] , color = 'red' , drawstyle = 'steps-mid' ) axs [ i ] . set_title ( err_col )
142	def im_convert ( tensor ) : image = tensor . cpu ( ) . clone ( ) . detach ( ) . numpy ( ) image = image . transpose ( 1 , 2 , 0 ) image = image * np . array ( ( 0.5 , 0.5 , 0.5 ) ) + np . array ( ( 0.5 , 0.5 , 0.5 ) ) image = image . clip ( 0 , 1 ) return image
523	data_dir = '/kaggle/input/stanford-covid-vaccine/' train = pd . read_json ( data_dir + 'train.json' , lines = True ) test = pd . read_json ( data_dir + 'test.json' , lines = True ) sample_df = pd . read_csv ( data_dir + 'sample_submission.csv' )
288	from sklearn . feature_selection import RFECV estimator = RandomForestClassifier ( random_state = 10 , n_estimators = 100 , n_jobs = - 1 ) selector = RFECV ( estimator , step = 1 , cv = 3 , scoring = scorer , n_jobs = - 1 )
239	fagg_model_accuracies = perform_feature_agglomeration ( X_train , y_train , X_test , y_test ) print ( fagg_model_accuracies )
427	of [ 'toxic' ] = mybest . toxic . values * 0.8 + of . toxic . values * 0.2 score1 = roc_auc_score ( mybest . toxic . round ( ) . astype ( int ) , of . toxic . values ) score2 = roc_auc_score ( of . toxic . round ( ) . astype ( int ) , mybest . toxic . values ) print ( '%2.4f\t%2.4f' % ( 100 * score1 , 100 * score2 ) ) print ( of . head ( ) ) of . to_csv ( 'submission.csv' , index = False )
235	model = Sequential ( ) model . add ( Embedding ( max_features , embedding_dim , embeddings_initializer = tf . keras . initializers . Constant ( embedding_matrix ) , trainable = False ) ) model . add ( Dropout ( 0.2 ) )
231	neutral_train [ 'temp_list' ] = neutral_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) neutral_train [ 'temp_list' ] = neutral_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) neutral_top = Counter ( [ item for sublist in neutral_train [ 'temp_list' ] for item in sublist ] ) neutral_temp = pd . DataFrame ( neutral_top . most_common ( 20 ) ) neutral_temp . columns = [ 'Common_words' , 'count' ] neutral_temp . style . background_gradient ( cmap = 'Blues' )
410	train_resized_imgs = [ ] test_resized_imgs = [ ] for image_id in label_df [ 'id' ] : train_resized_imgs . append ( pad_and_resize ( image_id , 'train' ) ) for image_id in submission_df [ 'Id' ] : test_resized_imgs . append ( pad_and_resize ( image_id , 'test' ) )
376	ren . AddActor ( cylinderActor ) ren . SetBackground ( colors . GetColor3d ( "BkgColor" ) ) ren . ResetCamera ( ) ren . GetActiveCamera ( ) . Zoom ( 1.5 )
576	plt . figure ( figsize = [ 10 , 6 ] ) df_train [ 'DBNOs' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of DBNOs" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'DBNOs' ] . value_counts ( ) )
306	model . n_estimators = len ( cv_results [ 'auc-mean' ] ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] baseline_auc = roc_auc_score ( test_labels , preds ) print ( 'The baseline model scores {:.5f} ROC AUC on the test set.' . format ( baseline_auc ) )
500	decay_var_list = [ ] for i in range ( len ( bert_nq . trainable_variables ) ) : name = bert_nq . trainable_variables [ i ] . name if any ( x in name for x in [ "LayerNorm" , "layer_norm" , "bias" ] ) : decay_var_list . append ( name ) decay_var_list
95	from sklearn . metrics import classification_report y_pred_binary = predictions . argmax ( axis = 1 ) report = classification_report ( y_true , y_pred_binary , target_names = cm_plot_labels ) print ( report )
470	files = folders = 0 path = "/kaggle/input/osic-pulmonary-fibrosis-progression/test" for _ , dirnames , filenames in os . walk ( path ) : files += len ( filenames ) folders += len ( dirnames ) print ( "{:,} files/images, {:,} folders/patients" . format ( files , folders ) )
93	train_path = 'base_dir/train_dir' valid_path = 'base_dir/val_dir' test_path = '../input/test' num_train_samples = len ( df_train ) num_val_samples = len ( df_val ) train_batch_size = 10 val_batch_size = 10 train_steps = np . ceil ( num_train_samples / train_batch_size ) val_steps = np . ceil ( num_val_samples / val_batch_size )
3	train [ 'outliers' ] = 0 train . loc [ train [ 'target' ] < - 30 , 'outliers' ] = 1 train [ 'outliers' ] . value_counts ( )
325	import altair as alt alt . renderers . enable ( 'notebook' )
439	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak0_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak0_df [ 'meter_reading' ] = leak0_df . meter_reading_scraped leak0_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak0_df . fillna ( 0 , inplace = True ) leak0_df . loc [ leak0_df . meter_reading < 0 , 'meter_reading' ] = 0 leak0_df = leak0_df [ leak0_df . timestamp . dt . year > 2016 ] print ( len ( leak0_df ) )
409	def build_new_df ( df , extension = 'jpeg' ) : new_df = pd . concat ( [ df , df ] ) new_df [ 'filename' ] = pd . concat ( [ df [ 'id_code' ] . apply ( lambda string : string + f'_s1.{extension}' ) , df [ 'id_code' ] . apply ( lambda string : string + f'_s2.{extension}' ) ] ) return new_df new_train = build_new_df ( train_df ) new_test = build_new_df ( test_df ) new_train . to_csv ( 'new_train.csv' , index = False ) new_test . to_csv ( 'new_test.csv' , index = False )
207	train = pd . read_csv ( '../input/train.csv' , low_memory = False , index_col = 'id' ) test = pd . read_csv ( '../input/test.csv' , low_memory = False , index_col = 'id' ) res = pd . read_csv ( '../input/resources.csv' , low_memory = False , index_col = 'id' )
423	yhat = [ 1 if y > 0.5 else 0 for y in yhat ] test_df [ 'label' ] = yhat label_map = dict ( ( v , k ) for k , v in train_generator . class_indices . items ( ) ) test_df [ 'label' ] = test_df [ 'label' ] . replace ( label_map ) test_df [ 'label' ] = test_df [ 'label' ] . replace ( { 'dog' : 1 , 'cat' : 0 } ) test_df . to_csv ( 'submission.csv' , index = False )
272	def process_det ( index , outputs , score_threshold = 0.5 ) : boxes = outputs [ index ] [ 'boxes' ] . data . cpu ( ) . numpy ( ) scores = outputs [ index ] [ 'scores' ] . data . cpu ( ) . numpy ( ) boxes = ( boxes ) . clip ( min = 0 , max = 1023 ) . astype ( int ) indexes = np . where ( scores > score_threshold ) boxes = boxes [ indexes ] scores = scores [ indexes ] return boxes , scores
425	with strategy . scope ( ) : transformer_layer = TFAutoModel . from_pretrained ( MODEL ) model = build_model_PT ( transformer_layer , max_len = MAX_LEN )
230	negative_train [ 'temp_list' ] = negative_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) negative_train [ 'temp_list' ] = negative_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) negative_top = Counter ( [ item for sublist in negative_train [ 'temp_list' ] for item in sublist ] ) negative_temp = pd . DataFrame ( negative_top . most_common ( 20 ) ) negative_temp . columns = [ 'Common_words' , 'count' ] negative_temp . style . background_gradient ( cmap = 'Blues' )
383	real_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' eval_partition = pd . read_csv ( '/kaggle/input/celeba-dataset/list_eval_partition.csv' ) eval_partition [ 'filename' ] = eval_partition . image_id . apply ( lambda st : real_dir + st ) eval_partition [ 'class' ] = 'REAL'
503	def get_training_dataset_raw ( ) : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True , ordered = False ) return dataset raw_training_dataset = get_training_dataset_raw ( ) label_counter = Counter ( ) for images , labels in raw_training_dataset : label_counter . update ( [ labels . numpy ( ) ] ) del raw_training_dataset label_counting_sorted = label_counter . most_common ( ) NUM_TRAINING_IMAGES = sum ( [ x [ 1 ] for x in label_counting_sorted ] ) print ( "number of examples in the original training dataset: {}" . format ( NUM_TRAINING_IMAGES ) ) print ( "labels in the original training dataset, sorted by occurrence" ) label_counting_sorted
71	plt . figure ( figsize = ( 20 , 20 ) ) sns . regplot ( x = 'coms_length' , y = 'price' , data = train , scatter_kws = { 's' : 2 } ) plt . title ( 'Description length VS price' , fontsize = 20 ) plt . xlabel ( 'Description length' , fontsize = 20 ) plt . ylabel ( 'Price' , fontsize = 20 )
140	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yT , yR_pred ) print ( confusion )
467	plt . figure ( figsize = ( 10 , 8 ) ) sns . heatmap ( link_count ) plt . show ( )
97	test_path = 'test_dir' test_gen = datagen . flow_from_directory ( test_path , target_size = ( IMAGE_SIZE , IMAGE_SIZE ) , batch_size = 1 , class_mode = 'categorical' , shuffle = False )
518	for col in new_feats : df_train [ col ] . replace ( [ np . inf ] , np . nan , inplace = True ) df_train [ col ] . fillna ( 0 , inplace = True ) df_test [ col ] . replace ( [ np . inf ] , np . nan , inplace = True ) df_test [ col ] . fillna ( 0 , inplace = True )
310	train_set = lgb . Dataset ( train , label = train_labels ) hyperparameters = dict ( ** random_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS ) print ( 'The cross validation score on the full dataset for Random Search= {:.5f} with std: {:.5f}.' . format ( cv_results [ 'auc-mean' ] [ - 1 ] , cv_results [ 'auc-stdv' ] [ - 1 ] ) ) print ( 'Number of estimators = {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
258	df_ = df [ df [ 'timestamp' ] < 100 ] X2 = df_ . drop ( [ 'y' , 'id' , 'timestamp' ] , axis = 1 ) y2 = df_ [ 'y' ] rf4 = RandomForestRegressor ( ) rf4 . fit ( X2 , y2 ) rf4 . score ( X2 , y2 )
189	params = { 'bagging_fraction' : 0.7982116702024386 , 'feature_fraction' : 0.1785051643813966 , 'max_depth' : int ( 49.17611603427576 ) , 'min_child_weight' : 3.2852905549011155 , 'min_data_in_leaf' : int ( 31.03480802715621 ) , 'n_estimators' : 5000 , 'num_leaves' : int ( 52.851307790411965 ) , 'reg_alpha' : 0.45963319421692145 , 'reg_lambda' : 0.6591286807489907 , 'metric' : 'auc' , 'boosting_type' : 'gbdt' , 'colsample_bytree' : .8 , 'subsample' : .9 , 'min_split_gain' : .01 , 'max_bin' : 127 , 'bagging_freq' : 5 , 'learning_rate' : 0.01 , 'early_stopping_rounds' : 100 }
25	import scipy print ( model_pred . clip ( 0.35 , 0.65 ) . mean ( ) ) print ( scipy . stats . median_absolute_deviation ( model_pred . clip ( 0.35 , 0.65 ) ) [ 0 ] )
84	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'max' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmax ( ) , 'max' ] = 1
411	def create_test_gen ( batch_size = 64 ) : return ImageDataGenerator ( rescale = 1 / 255. ) . flow_from_dataframe ( test_imgs , directory = '../input/severstal-steel-defect-detection/test_images' , x_col = 'ImageId' , class_mode = None , target_size = ( 256 , 256 ) , batch_size = batch_size , shuffle = False )
237	def check_missing_values ( df ) : if df . isnull ( ) . any ( ) . any ( ) : print ( "There are missing values in the dataframe" ) else : print ( "There are no missing values in the dataframe" ) check_missing_values ( train_df ) check_missing_values ( test_df )
574	import numpy as np import pandas as pd from sklearn import model_selection , preprocessing , metrics import matplotlib . pyplot as plt import seaborn as sns pd . set_option ( 'display.width' , 1000 ) pd . set_option ( 'display.max_rows' , 200 ) pd . set_option ( 'display.max_columns' , 200 ) df_train = pd . read_csv ( '../input/train.csv' ) df_test = pd . read_csv ( '../input/test.csv' )
496	if FLAGS . do_valid : if FLAGS . smaller_valid_dataset : predict_file = FLAGS . validation_predict_file_small else : predict_file = FLAGS . validation_predict_file f1 , long_f1 , short_f1 = compute_f1_scores ( valid_predictions_json , predict_file ) print ( f" valid f1: {f1}\n valid long_f1: {long_f1}\nvalid short_f1: {short_f1}" )
274	pd . options . display . max_columns = 150 train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
370	plt . rcParams [ 'figure.figsize' ] = ( 10 , 10 ) X_tr_temp , y_tr_temp = tr_datagen . __getitem__ ( 0 ) X_valid_temp , y_valid_temp = valid_datagen . __getitem__ ( 0 ) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . imshow ( X_tr_temp [ 0 ] ) ax [ 0 ] . set_title ( 'train:' ) ax [ 1 ] . imshow ( X_valid_temp [ 0 ] ) ax [ 1 ] . set_title ( 'valid:' )
491	n_iter = 100 start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_grid_mask ( images ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_grid_mask: {timing}" )
435	for task , prediction in tqdm ( zip ( train_tasks , train_predictions ) ) : if input_output_shape_is_same ( task ) : for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
444	import pandas as pd import numpy as np import matplotlib . pylab as plt pd . set_option ( 'display.max_rows' , 500 ) pd . get_option ( "display.max_columns" , 500 )
149	bold ( '**Preview of building data**' ) display ( building . head ( 3 ) ) bold ( '**Preview of Weather Train Data:**' ) display ( weather_train . head ( 3 ) ) bold ( '**Preview of Weather Test Data:**' ) display ( weather_test . head ( 3 ) ) bold ( '**Preview of Train Data:**' ) display ( train . head ( 3 ) ) bold ( '**Preview of Test Data:**' ) display ( test . head ( 3 ) )
402	data_dir = '/kaggle/input/stanford-covid-vaccine/' train = pd . read_json ( data_dir + 'train.json' , lines = True ) test = pd . read_json ( data_dir + 'test.json' , lines = True ) sample_df = pd . read_csv ( data_dir + 'sample_submission.csv' )
368	plot_curve_fit ( gaussian , roc_by_date , 'Active' , 'China w/o Hubei' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , china_by_date , 'Active' , 'China' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , skorea_by_date , 'Active' , 'South Korea' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , italy_by_date , 'Active' , 'Italy' + ' - Curve for Cases ' , False , 'Gauss' )
128	def run_mp_build ( ) : t0 = time . time ( ) num_proc = NUM_THREADS pool = mp . Pool ( processes = num_proc ) results = [ pool . apply_async ( build_fields , args = ( pid , ) ) for pid in range ( NUM_THREADS ) ] output = [ p . get ( ) for p in results ] num_built = sum ( output ) pool . close ( ) pool . join ( ) print ( num_built ) print ( 'Run time: %.2f' % ( time . time ( ) - t0 ) )
96	shutil . rmtree ( 'base_dir' )
331	from featuretools import selection feature_matrix2 = selection . remove_low_information_features ( feature_matrix ) print ( 'Removed %d features' % ( feature_matrix . shape [ 1 ] - feature_matrix2 . shape [ 1 ] ) )
460	for item in sorted ( cls_counts . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) [ : 20 ] : _id , count = item [ 0 ] , item [ 1 ] label = label_map [ int ( _id ) ] print ( f'attribute_name: {label} count: {count}' )
57	not_missing = df [ df [ 'attributed_time' ] . isna ( ) == False ] not_missing [ 'gap' ] = pd . to_datetime ( not_missing [ 'attributed_time' ] ) . sub ( pd . to_datetime ( not_missing [ 'click_time' ] ) ) for i in range ( 0 , 11 ) : y = i / 10 print ( "{0:.0f}" . format ( y * 100 ) , "quantile :" , not_missing [ 'gap' ] . quantile ( y ) )
404	train = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/train.json' , lines = True ) test = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/test.json' , lines = True ) sample_df = pd . read_csv ( '/kaggle/input/stanford-covid-vaccine/sample_submission.csv' )
490	start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_mixup ( images , labels , PROBABILITY = 1.0 ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_mixup: {timing}" )
335	train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test_ids = list ( test [ 'SK_ID_CURR' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] )
51	import torch , torchvision print ( torch . __version__ , torch . cuda . is_available ( ) ) import mmdet print ( mmdet . __version__ ) from mmcv . ops import get_compiling_cuda_version , get_compiler_version print ( get_compiling_cuda_version ( ) ) print ( get_compiler_version ( ) )
132	ld = os . listdir ( TEST_DIR ) sizes = np . zeros ( len ( ld ) ) for i , f in enumerate ( ld ) : df = pd . read_csv ( os . path . join ( TEST_DIR , f ) ) sizes [ i ] = df . shape [ 0 ] print ( np . mean ( sizes ) ) print ( np . min ( sizes ) ) print ( np . max ( sizes ) ) print ( 'ok' )
6	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in the train data set:' , train . shape ) print ( 'Number of rows and columns in the test data set:' , test . shape )
430	reduce_valid = pd . DataFrame ( ) for i , row in reduce_train_org . groupby ( 'installation_id' , sort = False ) : reduce_valid = reduce_valid . append ( row . sample ( 1 ) ) reduce_train = reduce_train_org . drop ( reduce_valid . index )
527	import missingno as msno train_null = train train_null = train_null . replace ( - 1 , np . NaN ) msno . matrix ( df = train_null . iloc [ : , : ] , figsize = ( 20 , 14 ) , color = ( 0.8 , 0.5 , 0.2 ) )
1	nulls = np . sum ( train . isnull ( ) ) nullcols = nulls . loc [ ( nulls != 0 ) ] dtypes = train . dtypes dtypes2 = dtypes . loc [ ( nulls != 0 ) ] info = pd . concat ( [ nullcols , dtypes2 ] , axis = 1 ) . sort_values ( by = 0 , ascending = False )
186	def convert_to_grayscale ( img ) : base_range = np . amax ( img ) - np . amin ( img ) rescaled_range = 255 - 0 img_rescaled = ( ( ( img - np . amin ( img ) ) * rescaled_range ) / base_range ) return np . uint8 ( img_rescaled )
34	fig = px . line ( train_df , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'SmokingStatus' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines+markers' )
82	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
386	if not os . path . isdir ( save_dir ) : os . makedirs ( save_dir ) model_path = os . path . join ( save_dir , model_name ) model . save ( model_path ) print ( 'Saved trained model at %s ' % model_path )
7	vect_word = TfidfVectorizer ( max_features = 10000 , lowercase = True , analyzer = 'word' , stop_words = 'english' , ngram_range = ( 1 , 2 ) , dtype = np . float32 ) vect_char = TfidfVectorizer ( max_features = 30000 , lowercase = True , analyzer = 'char' , stop_words = 'english' , ngram_range = ( 1 , 6 ) , dtype = np . float32 )
80	ensemble_final = ensembles [ 0 ] . copy ( ) ensemble_final [ target_cols ] = 0 for ensemble in ensembles : ensemble_final [ target_cols ] += ensemble [ target_cols ] . values / len ( ensembles ) ensemble_final
429	[ x1 , x2 , x3 , x4 ] = sess . run ( [ num_frames , video_matrix , batch_labels , batch_frames ] ) [ z1 , z2 ] = sess . run ( [ labels , num_frames ] ) vid_byte = sess . run ( batch_video_ids ) vid = vid_byte [ 0 ] . decode ( ) print ( 'vid = %s' % vid )
385	batch_size = 64 num_classes = 14 epochs = 30 val_split = 0.1 save_dir = os . path . join ( os . getcwd ( ) , 'models' ) model_name = 'keras_cnn_model.h5'
23	print ( 'There are ' + str ( y . count ( 1 ) ) + ' fake train samples' ) print ( 'There are ' + str ( y . count ( 0 ) ) + ' real train samples' ) print ( 'There are ' + str ( val_y . count ( 1 ) ) + ' fake val samples' ) print ( 'There are ' + str ( val_y . count ( 0 ) ) + ' real val samples' )
278	heads [ 'walls' ] = np . argmax ( np . array ( heads [ [ 'epared1' , 'epared2' , 'epared3' ] ] ) , axis = 1 ) plot_categoricals ( 'walls' , 'Target' , heads )
373	import numpy as np import pandas as pd import time from sklearn . preprocessing import Imputer from sklearn . preprocessing import RobustScaler import seaborn as sns import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( "ignore" ) import xgboost as xgb from sklearn import preprocessing import os print ( os . listdir ( "../input" ) )
99	submission = pd . DataFrame ( { 'id' : image_id , 'label' : y_pred , } ) . set_index ( 'id' ) submission . to_csv ( 'patch_preds.csv' , columns = [ 'label' ] )
205	class GaussianTargetNoise ( object ) : def __init__ ( self , p : float = 0.5 , gaus_std : float = 1.0 , ) : self . p = p self . gaus_std = gaus_std def __call__ ( self , x_arr , y_arr , atten_arr ) : if np . random . binomial ( n = 1 , p = self . p ) : y_arr = y_arr + np . random . normal ( scale = self . gaus_std , size = y_arr . shape ) return x_arr , y_arr , atten_arr
585	import numpy as np import pandas as pd import matplotlib . pyplot as plt import os import datetime import warnings warnings . filterwarnings ( "ignore" ) import seaborn as sns import scipy
448	model = LGBMClassifier ( ** params ) . fit ( X , y , categorical_feature = [ 'PdDistrict' ] ) pdp_Pd = pdp . pdp_isolate ( model = model , dataset = X , model_features = X . columns . tolist ( ) , feature = 'Hour' , n_jobs = - 1 ) pdp . pdp_plot ( pdp_Pd , 'Hour' , ncols = 3 ) plt . show ( )
88	Dropout_new = 0.15 n_split = 5 lr = 3e-5
52	sns . set ( ) plt . hist ( sub1 [ 'isFraud' ] , bins = 100 ) plt . show ( )
442	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
504	TARGET_MIN_COUNTING = 100 def get_num_of_repetition_for_class ( class_id ) : counting = label_counter [ class_id ] if counting >= TARGET_MIN_COUNTING : return 1.0 num_to_repeat = TARGET_MIN_COUNTING / counting return num_to_repeat numbers_of_repetition_for_classes = { class_id : get_num_of_repetition_for_class ( class_id ) for class_id in range ( 104 ) } print ( "number of repetitions for each class (if > 1)" ) { k : v for k , v in sorted ( numbers_of_repetition_for_classes . items ( ) , key = lambda item : item [ 1 ] , reverse = True ) if v > 1 }
259	model . save ( 'UNET.h5' , include_optimizer = False ) f = open ( "preprocess.dill" , "wb" ) dill . dump ( preprocess , f ) f . close classify_model . save ( 'classify_model.h5' , include_optimizer = False ) f = open ( "classify_preprocess.dill" , "wb" ) dill . dump ( classify_preprocess , f ) f . close if mode != 'train' :
184	target0df = metadata_train [ metadata_train [ 'target' ] == 0 ] target1df = metadata_train [ metadata_train [ 'target' ] == 1 ] print ( "target0data shape:" , target0df . shape ) print ( "target1data shape:" , target1df . shape )
588	xs = pq . read_table ( '../input/train.parquet' , columns = [ str ( i ) for i in range ( 999 ) ] ) . to_pandas ( ) print ( ( xs . shape ) ) xs . head ( 2 )
592	train = pd . read_csv ( '/kaggle/input/3d-object-detection-for-autonomous-vehicles/train.csv' ) sub = pd . read_csv ( '/kaggle/input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv' ) print ( train . shape ) train . head ( )
342	test = pd . read_csv ( '../input/application_test.csv' ) test = test . merge ( bureau_counts , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( bureau_agg , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( bureau_balance_by_client , on = 'SK_ID_CURR' , how = 'left' )
182	val_p = [ 'AMT_ANNUITY' , 'AMT_CREDIT' , 'AMT_GOODS_PRICE' , 'HOUR_APPR_PROCESS_START' ] for i in val_p : plt . figure ( figsize = ( 5 , 5 ) ) sns . distplot ( application_train [ i ] . dropna ( ) , kde = True , color = 'g' ) plt . title ( i ) plt . xticks ( rotation = - 90 ) plt . show ( )
116	corr_matrix = features . corr ( ) . abs ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) ; threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style collinear_features = [ column for column in upper . columns if any ( upper [ column ] > threshold ) ] upper . style . applymap ( highlight )
171	from sklearn . feature_extraction . text import TfidfVectorizer text = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] vectorizer = TfidfVectorizer ( ) vectorizer . fit ( text ) print ( sorted ( vectorizer . vocabulary_ ) ) vector = vectorizer . transform ( [ text [ 0 ] ] )
113	def my_generator ( ) : for i in range ( 0 , 3 ) : yield print ( i ) my_gen = my_generator ( )
308	bayes_results = pd . read_csv ( '../input/home-credit-model-tuning/bayesian_trials_1000.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) random_results = pd . read_csv ( '../input/home-credit-model-tuning/random_search_trials_1000.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) random_results [ 'loss' ] = 1 - random_results [ 'score' ] bayes_params = evaluate ( bayes_results , name = 'Bayesian' ) random_params = evaluate ( random_results , name = 'random' )
197	corrMatt = data [ [ "bedrooms" , "bathrooms" , "price" ] ] . corr ( ) mask = np . array ( corrMatt ) mask [ np . tril_indices_from ( mask ) ] = False fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) sn . heatmap ( corrMatt , mask = mask , vmax = .8 , square = True , annot = True )
146	vectorizer = TfidfVectorizer ( min_df = 0.00009 , max_features = 200000 , tokenizer = lambda x : x . split ( ) , ngram_range = ( 1 , 3 ) ) X_train_multilabel = vectorizer . fit_transform ( X_train [ 'question' ] ) X_test_multilabel = vectorizer . transform ( X_test [ 'question' ] )
190	aisles = pd . read_csv ( '../input/aisles.csv' ) departments = pd . read_csv ( '../input//departments.csv' ) orderProductsTrain = pd . read_csv ( '../input/order_products__train.csv' ) orders = pd . read_csv ( '../input/orders.csv' ) products = pd . read_csv ( '../input/products.csv' ) orderProductsPrior = pd . read_csv ( '../input/order_products__prior.csv' )
202	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "bathroomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Bathroom Count ",title=" Bathroom Count Vs Log Error " )
375	colors = vtk . vtkNamedColors ( ) bkg = map ( lambda x : x / 255.0 , [ 26 , 51 , 102 , 255 ] ) colors . SetColor ( "BkgColor" , * bkg )
412	def load_img ( code , base , resize = True ) : path = f'{base}/{code}' img = cv2 . imread ( path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) if resize : img = cv2 . resize ( img , ( 256 , 256 ) ) return img def validate_path ( path ) : if not os . path . exists ( path ) : os . makedirs ( path )
318	previous = pd . read_csv ( '../input/previous_application.csv' ) . replace ( { 365243 : np . nan } ) previous = convert_types ( previous ) previous [ 'LOAN_RATE' ] = previous [ 'AMT_ANNUITY' ] / previous [ 'AMT_CREDIT' ] previous [ "AMT_DIFFERENCE" ] = previous [ 'AMT_CREDIT' ] - previous [ 'AMT_APPLICATION' ]
395	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
469	files = folders = 0 path = "/kaggle/input/osic-pulmonary-fibrosis-progression/train" for _ , dirnames , filenames in os . walk ( path ) : files += len ( filenames ) folders += len ( dirnames ) print ( "{:,} files/images, {:,} folders/patients" . format ( files , folders ) )
217	df_grouped_italy = get_df_country_cases ( df_covid , "Italy" ) df_grouped_italy
550	order = [ ] with open ( 'lk.sol' , 'r' ) as fp : lines = fp . readlines ( ) order = [ int ( v . split ( ' ' ) [ 0 ] ) for v in lines [ 1 : ] ] + [ 0 ]
216	df_covid [ 'country' ] = df_covid [ 'country' ] . replace ( 'Mainland China' , 'China' ) df_covid
388	train_resized_imgs = [ ] test_resized_imgs = [ ] for image_path in label_df [ 'Image' ] : train_resized_imgs . append ( pad_and_resize ( image_path , 'train' ) ) for image_path in submission_df [ 'Image' ] : test_resized_imgs . append ( pad_and_resize ( image_path , 'test' ) )
477	train_size = int ( len ( scaled ) * 0.7 ) test_size = len ( scaled ) - train_size V_train , V_test = scaled [ 0 : train_size , : ] , scaled [ train_size : len ( scaled ) , : ] print ( len ( V_train ) , len ( V_test ) )
453	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 8 ) ) sorted_train_df . groupby ( 'date' ) [ 'var_91' ] . count ( ) . plot ( ax = ax , label = "train" ) sorted_test_df . groupby ( 'date' ) [ 'var_91' ] . count ( ) . plot ( ax = ax , label = "test" ) ax . legend ( )
571	import seaborn as sns import numpy as np import pandas as pd from mpl_toolkits . mplot3d import Axes3D import matplotlib . pyplot as plt sns . set ( style = 'darkgrid' ) import os print ( os . listdir ( "../input" ) ) from pylab import rcParams rcParams [ 'figure.figsize' ] = 25 , 12.5 train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
43	example_df = train_df . sample ( n = 1 ) . reset_index ( drop = True ) example_generator = train_datagen . flow_from_dataframe ( example_df , "../input/train/train/" , x_col = 'filename' , y_col = 'category' , target_size = IMAGE_SIZE , class_mode = 'categorical' )
433	for task , prediction , solved in tqdm ( zip ( train_tasks , train_predictions , train_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
185	reducedtarget0sampleDF = pd . DataFrame ( ) for col in range ( target0sampledata . shape [ 1 ] ) : tmp_pdSeries = reduce_sample ( target0sampledata . iloc [ : , col ] ) reducedtarget0sampleDF [ str ( col ) ] = tmp_pdSeries reducedtarget0sampleDF . shape
528	etc_ordianal_features = [ 'ps_ind_01' , 'ps_ind_03' , 'ps_ind_14' , 'ps_ind_15' , 'ps_reg_01' , 'ps_reg_02' , 'ps_car_11' , 'ps_calc_01' , 'ps_calc_02' , 'ps_calc_03' , 'ps_calc_04' , 'ps_calc_05' , 'ps_calc_06' , 'ps_calc_07' , 'ps_calc_08' , 'ps_calc_09' , 'ps_calc_10' , 'ps_calc_11' , 'ps_calc_12' , 'ps_calc_13' , 'ps_calc_14' ] etc_continuous_features = [ 'ps_reg_03' , 'ps_car_12' , 'ps_car_13' , 'ps_car_14' , 'ps_car_15' ] train_null_columns = train_null . columns test_null_columns = test_null . columns
247	dftrain = pd . read_csv ( '../input/covid19-global-forecasting-week-2/train.csv' , parse_dates = [ 'Date' ] ) . sort_values ( by = [ 'Country_Region' , 'Date' ] ) dftest = pd . read_csv ( '../input/covid19-global-forecasting-week-2/test.csv' , parse_dates = [ 'Date' ] ) . sort_values ( by = [ 'Country_Region' , 'Date' ] ) dftrain . head ( )
170	vector = vectorizer . transform ( text ) print ( vector . shape ) print ( vector . toarray ( ) )
511	for c in test_df . columns : if c == 'mo_ye' : continue if test_df [ c ] . dtype == 'object' : lbl = preprocessing . LabelEncoder ( ) lbl . fit ( list ( test_df [ c ] . values ) ) test_df [ c ] = lbl . transform ( list ( test_df [ c ] . values ) ) test_df [ 'mo_ye' ] = test_df [ 'mo_ye' ] . apply ( lambda x : 100 * pd . to_datetime ( x ) . year + pd . to_datetime ( x ) . month )
175	from keras . models import Model from keras . layers import Input from keras . layers import Dense visible = Input ( shape = ( 2 , ) ) hidden = Dense ( 2 ) ( visible ) model = Model ( inputs = visible , outputs = hidden )
445	import datetime START_DATE = '2017-12-01' startdate = datetime . datetime . strptime ( START_DATE , '%Y-%m-%d' ) df [ 'TransactionDT' ] = df [ 'TransactionDT' ] . apply ( lambda x : ( startdate + datetime . timedelta ( seconds = x ) ) ) df [ 'hour' ] = df [ 'TransactionDT' ] . dt . hour
488	model . eval ( ) with torch . no_grad ( ) : preds = np . empty ( 0 ) for x , _ in tqdm_notebook ( tloader ) : x = x . to ( device ) output = model ( x ) idx = output . max ( dim = - 1 ) [ 1 ] . cpu ( ) . numpy ( ) preds = np . append ( preds , idx , axis = 0 )
5	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train [ 'wheezy-copper-turtle-magic' ] . values , bins = 1000 ) plt . title ( 'Histogram muggy-smalt-axolotl-pembus counts' ) plt . xlabel ( 'Value' ) plt . ylabel ( 'Count' ) plt . show ( )
334	random_hyp [ 'set' ] = 'Random Search' opt_hyp [ 'set' ] = 'Bayesian' hyp = random_hyp . append ( opt_hyp , ignore_index = True , sort = True ) hyp . head ( )
20	df_text = pd . read_csv ( '../input/training_text' , sep = '\|\|' , engine = 'python' , skiprows = 1 , names = [ 'ID' , 'Text' ] ) . set_index ( 'ID' ) df_text . head ( )
10	train_log_target = train_df [ [ 'target' ] ] train_log_target [ 'target' ] = np . log ( 1 + train_df [ 'target' ] . values ) train_log_target . describe ( )
476	x_train_full2 = np . square ( x_train_full ) x_test_full2 = np . square ( x_test_full ) x_sub_full2 = np . square ( x_sub_full )
294	def ecdf ( x ) : x = np . sort ( x ) n = len ( x ) y = np . arange ( 1 , n + 1 , 1 ) / n return x , y
53	df = pd . read_csv ( '../input/train.csv' , skiprows = 9308568 , nrows = 59633310 ) header = pd . read_csv ( '../input/train.csv' , nrows = 0 ) df . columns = header . columns df del header gc . collect ( ) print ( "The created dataframe contains" , df . shape [ 0 ] , "rows." )
245	df = ( pd . merge ( agg . reset_index ( ) , products , on = 'Producto_ID' , how = 'left' ) . groupby ( 'short_name' ) [ 'Demanda_uni_equil_sum' , 'Venta_uni_hoy_sum' , 'Dev_uni_proxima_sum' , 'Dev_uni_proxima_count' ] . sum ( ) . sort_values ( by = 'Demanda_uni_equil_sum' , ascending = False ) )
256	m , n = df . shape miss_count = [ ] for col in df . columns : x = df [ col ] . isnull ( ) . sum ( ) miss_count . append ( x ) miss_count_rate = np . array ( miss_count ) / m
157	train [ 'year_built' ] = np . uint8 ( train [ 'year_built' ] - 1900 , inplace = True ) test [ 'year_built' ] = np . uint8 ( test [ 'year_built' ] - 1900 , inplace = True )
108	print ( df_train . shape ) print ( df_val . shape ) print ( df_test . shape )
148	import numpy as np import pandas as pd from scipy import stats import os , gc import matplotlib . pyplot as plt import seaborn as sns sns . set_style ( "whitegrid" ) import plotly . offline as py from plotly . offline import iplot , init_notebook_mode import plotly . graph_objs as go init_notebook_mode ( connected = True ) from IPython . display import Markdown def bold ( string ) : display ( Markdown ( string ) )
428	import sys , os , multiprocessing , csv from urllib import request , error from PIL import Image from io import BytesIO
323	a = 0 b = 0 for x in param_grid [ 'learning_rate' ] : if x >= 0.005 and x < 0.05 : a += 1 elif x >= 0.05 and x < 0.5 : b += 1 print ( 'There are {} values between 0.005 and 0.05' . format ( a ) ) print ( 'There are {} values between 0.05 and 0.5' . format ( b ) )
449	df [ "diff_V319_V320" ] = np . zeros ( df . shape [ 0 ] ) df [ "diff_V320_V321" ] = np . zeros ( df . shape [ 0 ] ) df [ "diff_V319_V321" ] = np . zeros ( df . shape [ 0 ] )
304	valid_preds = random_forest . predict ( X_valid [ features ] ) plt . figure ( figsize = ( 10 , 6 ) ) sns . kdeplot ( y_valid , label = 'Actual' ) sns . kdeplot ( valid_preds , label = 'Predicted' ) plt . legend ( prop = { 'size' : 30 } ) plt . title ( "Distribution of Validation Fares" ) ;
360	print ( 'Initialize.' ) train_df = pd . read_csv ( '{}train.csv' . format ( data_src ) , usecols = [ 0 ] , index_col = 'id' ) depths_df = pd . read_csv ( '{}depths.csv' . format ( data_src ) , index_col = 'id' ) train_df = train_df . join ( depths_df ) test_df = depths_df [ ~ depths_df . index . isin ( train_df . index ) ]
275	all_equal = train . groupby ( 'idhogar' ) [ 'Target' ] . apply ( lambda x : x . nunique ( ) == 1 ) not_equal = all_equal [ all_equal != True ] print ( 'There are {} households where the family members do not all have the same target.' . format ( len ( not_equal ) ) )
434	for task , prediction , solved in tqdm ( zip ( evaluation_tasks , evaluation_predictions , evaluation_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
384	densenet = DenseNet121 ( weights = '/kaggle/input/densenet-keras/DenseNet-BC-121-32-no-top.h5' , include_top = False , input_shape = ( 224 , 224 , 3 ) ) for layer in densenet . layers : layer . trainable = False
485	class Net ( nn . Module ) : def __init__ ( self , num_classes ) : super ( ) . __init__ ( ) self . model = EfficientNet . from_name ( 'efficientnet-b0' ) self . dense_output = nn . Linear ( 1280 , num_classes ) def forward ( self , x ) : feat = self . model . extract_features ( x ) feat = F . avg_pool2d ( feat , feat . size ( ) [ 2 : ] ) . reshape ( - 1 , 1280 ) return self . dense_output ( feat )
321	credit = pd . read_csv ( '../input/credit_card_balance.csv' ) . replace ( { 365243 : np . nan } ) credit = convert_types ( credit ) credit [ 'OVER_LIMIT' ] = credit [ 'AMT_BALANCE' ] > credit [ 'AMT_CREDIT_LIMIT_ACTUAL' ] credit [ 'BALANCE_CLEARED' ] = credit [ 'AMT_BALANCE' ] == 0.0 credit [ 'LOW_PAYMENT' ] = credit [ 'AMT_PAYMENT_CURRENT' ] < credit [ 'AMT_INST_MIN_REGULARITY' ] credit [ 'LATE' ] = credit [ 'SK_DPD' ] > 0.0
223	df_population = pd . read_csv ( "../input/countries-of-the-world/countries of the world.csv" ) df_population
473	N = train_df . shape [ 0 ] x_train = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( train_df [ 'Patient' ] ) ) : x_train [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
64	mean_price_2 = pd . DataFrame ( group . price . mean ( ) ) mean_price_2 . reset_index ( level = 0 , inplace = True ) plt . figure ( figsize = ( 12 , 7 ) ) sns . kdeplot ( mean_price_2 . price , shade = True ) plt . title ( 'Mean price by category distribution' , fontsize = 20 ) plt . xlabel ( 'Mean price of each category' , fontsize = 16 )
451	import matplotlib . pyplot as plt def plotImages ( images_arr ) : fig , axes = plt . subplots ( 1 , 5 , figsize = ( 20 , 20 ) ) axes = axes . flatten ( ) for img , ax in zip ( images_arr , axes ) : ax . imshow ( img ) plt . tight_layout ( ) plt . show ( ) augmented_images = [ train_generator [ 0 ] [ 0 ] [ 0 ] for i in range ( 5 ) ] plotImages ( augmented_images )
494	bert_tokenizer , bert_nq = get_pretrained_model ( FLAGS . model_name ) if not IS_KAGGLE : bert_nq . trainable_variables
122	dup_diff_target = dup [ ( dup [ 'mean' ] != 0.0 ) & ( dup [ 'mean' ] != 1.0 ) ] len_dup_diff_target = len ( dup_diff_target ) print ( 'NUmber of duplicate clicks with different target values in train data: ' , len_dup_diff_target )
589	feature_cols = [ col for col in train . columns if col not in [ 'is_churn' , 'msno' ] ] train [ feature_cols ] = train [ feature_cols ] . applymap ( lambda x : np . nan if np . isinf ( x ) else x ) test [ feature_cols ] = test [ feature_cols ] . applymap ( lambda x : np . nan if np . isinf ( x ) else x )
426	del model from keras import backend as K import gc K . clear_session ( ) gc . collect ( )
173	from keras . preprocessing . text import hashing_trick text = 'The quick brown fox jumped over the lazy dog.' words = set ( text_to_word_sequence ( text ) ) vocab_size = len ( words ) print ( vocab_size ) result = hashing_trick ( text , round ( vocab_size * 1.3 ) , hash_function = 'md5' ) print ( result )
533	from sklearn . linear_model import LogisticRegression lr = LogisticRegression ( penalty = 'l2' , C = 1000000 , class_weight = "balanced" ) lr . fit ( X_train , y_train ) y_pred = lr . predict ( X_test ) print ( classification_report ( y_pred , y_test ) )
103	train_path = 'base_dir/train_dir' val_path = 'base_dir/val_dir' num_train_samples = len ( df_train ) num_val_samples = len ( df_val ) train_batch_size = 5 val_batch_size = 5 train_steps = np . ceil ( num_train_samples / train_batch_size ) val_steps = np . ceil ( num_val_samples / val_batch_size )
573	sample_df = pd . read_csv ( SAMPLE ) sample_list = list ( sample_df . Id ) pred_dic = dict ( ( key , value ) for ( key , value ) in zip ( learner . data . test_ds . fnames , pred_list ) ) pred_list_cor = [ pred_dic [ id ] for id in sample_list ] df = pd . DataFrame ( { 'Id' : sample_list , 'Predicted' : pred_list_cor } ) df . to_csv ( 'protein_classification.csv' , header = True , index = False )
257	q_high = df . y . quantile ( 0.75 ) q_low = df . y . quantile ( 0.25 ) iqr = ( q_high - q_low ) * 1.5 high = q_high + iqr low = q_low - iqr df = df . drop ( df [ df . y > high ] . index ) df = df . drop ( df [ df . y < low ] . index )
480	yhat = model . predict ( testX ) yhat_inverse = scaler . inverse_transform ( yhat . reshape ( - 1 , 1 ) ) testY_inverse = scaler . inverse_transform ( testY . reshape ( - 1 , 1 ) )
424	test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv' ) test [ 'content' ] = test [ 'content' ] . apply ( lambda x : text_process ( x ) ) x_test = regular_encode ( test . content . values , tokenizer , maxlen = MAX_LEN ) lang_tag_test = np . array ( [ lang_embed ( row [ 'lang' ] , 'orig' ) for _ , row in test . iterrows ( ) ] )
560	def evaluate ( program : [ ] , input_image : np . array ) : input_image = np . array ( input_image ) assert type ( input_image ) == np . ndarray image_list = [ input_image ] for fct in program : image_list = fct ( image_list ) image_list = [ img for img in image_list if img . shape [ 0 ] > 0 and img . shape [ 1 ] > 0 ] if image_list == [ ] : return [ ] return image_list
554	train_csv = pd . read_csv ( '../input/train/train.csv' , low_memory = False ) test_csv = pd . read_csv ( '../input/test/test.csv' , low_memory = False ) def preprocess ( csv ) : csv [ 'Description_len' ] = [ len ( str ( tt ) ) for tt in csv [ 'Description' ] ] csv [ 'Name_len' ] = [ len ( str ( tt ) ) for tt in csv [ 'Name' ] ] return csv train_csv = preprocess ( train_csv ) test_csv = preprocess ( test_csv )
159	i = 0 result = [ ] step_size = 50000 for j in tqdm ( range ( int ( np . ceil ( test . shape [ 0 ] / 50000 ) ) ) ) : result . append ( np . expm1 ( sum ( [ model . predict ( test . iloc [ i : i + step_size ] ) for model in models ] ) / folds ) ) i += step_size
263	model = LogisticRegression ( C = 0.03 , max_iter = 300 ) model . fit ( encoded_train , raw_train . target ) test_pred = model . predict_proba ( encoded_test ) [ : , 1 ]
187	def evaluate_threshold ( tpr , fpr , clf_threshold , threshold ) : print ( 'Sensitivity:' , tpr [ clf_threshold > threshold ] [ - 1 ] ) print ( 'Specificity:' , 1 - fpr [ clf_threshold > threshold ] [ - 1 ] )
351	train [ 'len' ] = train [ 'comment_text' ] . str . len ( ) print ( 'Average comment length: %d' % train [ 'len' ] . mean ( ) ) print ( 'Median comment length: %d' % train [ 'len' ] . quantile ( .5 ) ) print ( '90th percentile comment length: %d' % train [ 'len' ] . quantile ( .9 ) )
305	model . n_jobs = - 1 model . fit ( X_train [ features ] , y_train ) evaluate ( model , features , X_train , X_valid , y_train , y_valid )
112	ID = df_preds [ 'patientId' ] preds = df_preds [ 'PredictionString' ] submission = pd . DataFrame ( { 'patientId' : ID , 'PredictionString' : preds , } ) . set_index ( 'patientId' ) submission . to_csv ( 'pneu_keras_model.csv' , columns = [ 'PredictionString' ] )
183	PARENT_DATA_DIR_PATH = '../input' METADATA_TRAIN_FILE_PATH = os . path . join ( PARENT_DATA_DIR_PATH , "metadata_train.csv" ) TRAIN_DATA_FILE_PATH = os . path . join ( PARENT_DATA_DIR_PATH , "train.parquet" )
195	fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 13 , 8 ) sn . countplot ( x = "bathrooms" , data = data , ax = ax1 ) data1 = data . groupby ( [ 'bathrooms' , 'interest_level' ] ) [ 'bathrooms' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) data1 [ [ 'low' , 'medium' , "high" ] ] . plot ( kind = 'bar' , stacked = True , ax = ax2 )
90	print ( len ( os . listdir ( '../input/train' ) ) ) print ( len ( os . listdir ( '../input/test' ) ) )
348	ed = application . groupby ( [ 'TARGET' , 'CNT_CHILDREN' ] ) [ 'TARGET' ] . count ( ) . unstack ( 'TARGET' ) . fillna ( 0 ) ed . plot ( kind = 'bar' , stacked = True ) print ( ed )
290	plt . figure ( figsize = ( 15 , 5 ) ) sns . countplot ( label [ 'surface' ] , order = label . surface . value_counts ( ) . index ) plt . show ( )
362	random_index = np . random . randint ( 0 , val_masks_stacked . shape [ 0 ] ) print ( 'Validation Index: {}' . format ( random_index ) ) fig , ax = plt . subplots ( 2 , 1 ) ax [ 0 ] . imshow ( val_masks_stacked [ random_index ] , cmap = 'seismic' ) ax [ 1 ] . imshow ( val_predictions_stacked [ random_index ] > 0.5 , cmap = 'seismic' )
180	def correlation_heatmap ( df ) : _ , ax = plt . subplots ( figsize = ( 20 , 15 ) ) colormap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) _ = sns . heatmap ( df . corr ( ) , cmap = colormap , square = True , ) plt . title ( 'Pearson Correlation of Features' ) correlation_heatmap ( application_train )
406	pickle . dump ( best_hp , open ( 'best_hp.pickle' , 'wb' ) ) best_model = tuner . get_best_models ( 1 ) [ 0 ] best_model . save ( 'best_model.h5' )
545	def calc_extra ( df_timeseries ) : gp = df_timeseries . groupby ( 'object_id' ) dfe = ( gp [ 'mjd' ] . max ( ) - gp [ 'mjd' ] . min ( ) ) . rename ( 'dmjd' ) . reset_index ( ) dfe [ 'dmjd' ] = dfe [ 'dmjd' ] / 1000 dfe [ 'std_flux' ] = gp . flux . std ( ) . reset_index ( ) . flux / 1000 return dfe
81	def fa ( N , a , b , beta ) : fa = - beta * a * b return fa def fb ( N , a , b , beta , gamma ) : fb = beta * a * b - gamma * b return fb def fc ( N , b , gamma ) : fc = gamma * b return fc
367	for key , value in dict . items ( ) : if key in [ "China" , 'Rest of China w/o Hubei' ] : pass else : growth_rate_over_time ( exp , value , 'Confirmed' , key + ' - Growth Rate Percentage for ' , )
486	test_filenames = sorted ( glob ( f"{data_dir}/Test/*.jpg" ) ) test_df = pd . DataFrame ( { 'ImageFileName' : list ( test_filenames ) } , columns = [ 'ImageFileName' ] ) batch_size = 16 num_workers = 4 test_dataset = Alaska2Dataset ( test_df , augmentations = AUGMENTATIONS_TEST , test = True ) test_loader = torch . utils . data . DataLoader ( test_dataset , batch_size = batch_size , num_workers = num_workers , shuffle = False , drop_last = False )
407	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB7 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( train_labels . shape [ 1 ] , activation = 'softmax' ) ] ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) model . summary ( )
89	def plot_word_cloud ( x , col ) : corpus = [ ] for k in x [ col ] . str . split ( ) : for i in k : corpus . append ( i ) plt . figure ( figsize = ( 12 , 8 ) ) word_cloud = WordCloud ( background_color = 'black' , max_font_size = 80 ) . generate ( " " . join ( corpus [ : 50 ] ) ) plt . imshow ( word_cloud ) plt . axis ( 'off' ) plt . show ( ) return corpus [ : 50 ]
530	y_log = np . log1p ( y ) y_categorized = pd . cut ( y_log , bins = range ( 0 , 25 , 3 ) , include_lowest = True , right = False , labels = range ( 0 , 24 , 3 ) )
150	bold ( '**ELECTRICITY THE MOST FREQUENT METER TYPE MEASURED**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) ax = sns . countplot ( data = train , x = 'meter' , palette = 'CMRmap' , alpha = 0.5 ) ax . set_ylabel ( 'Count' , fontsize = 20 ) ax . set_xlabel ( 'Meter Type' , fontsize = 20 ) plt . show ( )
204	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "numberofstories" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" No Of Storeys ",title=" No Of Storeys Vs Log Error " )
94	from sklearn . metrics import roc_auc_score roc_auc_score ( y_true , y_pred )
271	def id_to_filepath ( img_id , img_dir = TRAIN_DIR ) : filepath = f'{img_dir}/{img_id}.dcm' if os . path . exists ( filepath ) : return filepath else : return 'DNE'
508	corr_df = pd . DataFrame ( columns = [ 'feature' , 'pearson' , 'kendall' , 'spearman' ] ) corr = macro_df [ macro_columns ] . corr ( method = 'spearman' ) fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( corr , annot = True , linewidths = .5 , ax = ax )
538	stats = [ ] for country in sorted ( full_table [ 'Country/Region' ] . unique ( ) ) : df = get_time_series ( country ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 1000 ) : continue print ( '{} COVID-19 Prediction' . format ( country ) ) opt_display_model ( df , stats )
329	feature_matrix_spec , feature_names_spec = ft . dfs ( entityset = es , target_entity = 'app' , agg_primitives = [ 'sum' , 'count' , 'min' , 'max' , 'mean' , 'mode' ] , max_depth = 2 , features_only = False , verbose = True )
577	def sieve_eratosthenes ( n ) : primes = [ False , False ] + [ True for i in range ( n - 1 ) ] p = 2 while ( p * p <= n ) : if ( primes [ p ] == True ) : for i in range ( p * 2 , n + 1 , p ) : primes [ i ] = False p += 1 return primes
27	from six . moves import cPickle as pickle import bz2 def loadPickleBZ ( pickle_file ) : with bz2 . BZ2File ( pickle_file , 'r' ) as f : loadedData = pickle . load ( f ) return loadedData def savePickleBZ ( pickle_file , data ) : with bz2 . BZ2File ( pickle_file , 'w' ) as f : pickle . dump ( data , f , pickle . HIGHEST_PROTOCOL ) return
542	data = All_df [ 0 : 91713 ] Test_data = All_df [ 91713 : 131021 ] y = np . array ( All_y [ 0 : 91713 ] . tolist ( ) ) random_state = 23 X_train , X_test , y_train , y_test = train_test_split ( data , y , test_size = 0.2 , random_state = random_state , stratify = y ) X_train = pd . DataFrame ( X_train , columns = data . columns ) X_test = pd . DataFrame ( X_test , columns = data . columns )
56	print ( DL_by_IP . describe ( ) , '\n Quantile 99% :' , DL_by_IP . quantile ( 0.99 ) , \ '\n Quantile 99,9% :' , DL_by_IP . quantile ( 0.999 ) , \ '\n Quantile 99,999% :' , DL_by_IP . quantile ( 0.9999 ) )
16	def seed_everything ( seed ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( 43 )
397	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) tokenizer . save_pretrained ( '.' ) fast_tokenizer = BertWordPieceTokenizer ( 'vocab.txt' , lowercase = False ) fast_tokenizer
502	input_layer = tf . keras . layers . InputLayer ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , name = 'input_layer' ) data_augmentation_layer = Data_Augmentation_Dummy ( ) with strategy . scope ( ) : model = tf . keras . Sequential ( [ input_layer , data_augmentation_layer ] )
32	print ( "*" * 30 , "store_id" , "*" * 30 ) print ( "store_id unique value counts:{}" . format ( len ( price [ "store_id" ] . unique ( ) ) ) ) print ( price [ "store_id" ] . unique ( ) ) print ( "*" * 30 , "item_id" , "*" * 30 ) print ( "item_id unique value counts:{}" . format ( len ( price [ "item_id" ] . unique ( ) ) ) ) print ( price [ "item_id" ] . unique ( ) )
118	model = RandomForestRegressor ( max_depth = 2 , random_state = 0 , n_estimators = 100 ) embeded_rf_selector = SelectFromModel ( model , threshold = '1.25*median' ) embeded_rf_selector . fit ( dfe , target_fe )
234	train_df = pd . read_csv ( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip' ) . fillna ( ' ' ) test_df = pd . read_csv ( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip' ) . fillna ( ' ' ) train_df . sample ( 10 )
343	cash = pd . read_csv ( '../input/POS_CASH_balance.csv' ) cash = convert_types ( cash , print_info = True ) cash . head ( )
285	from sklearn . ensemble import RandomForestClassifier from sklearn . metrics import f1_score , make_scorer from sklearn . model_selection import cross_val_score from sklearn . preprocessing import Imputer from sklearn . preprocessing import MinMaxScaler from sklearn . pipeline import Pipeline scorer = make_scorer ( f1_score , greater_is_better = True , average = 'macro' )
438	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
565	x_train = np . load ( "x_train.npy" ) x_test = np . load ( "x_test.npy" ) y_train = np . load ( "y_train.npy" ) features = np . load ( "features.npy" ) test_features = np . load ( "test_features.npy" ) word_index = np . load ( "word_index.npy" ) . item ( )
365	X_interaction = X_importance . iloc [ : 500 , : ] shap_interaction_values = shap . TreeExplainer ( lgb_model ) . shap_interaction_values ( X_interaction )
481	train = train . sort_values ( 'visit_date' ) target_train = np . log1p ( train [ 'visitors' ] . values ) col = [ c for c in train if c not in [ 'id' , 'air_store_id' , 'visitors' ] ] train = train [ col ] train . set_index ( 'visit_date' , inplace = True ) train . head ( )
109	import matplotlib . pyplot as plt loss = history . history [ 'loss' ] val_loss = history . history [ 'val_loss' ] epochs = range ( 1 , len ( loss ) + 1 ) plt . legend ( ) plt . plot ( epochs , loss , 'bo' , label = 'Training loss' ) plt . plot ( epochs , val_loss , 'b' , label = 'Validation loss' ) plt . title ( 'Training and validation loss' ) plt . legend ( ) plt . show ( )
120	predictions = np . zeros ( len ( scaled_test_X ) ) n_fold = 5 folds = KFold ( n_splits = n_fold , shuffle = True , random_state = 42 ) fold_importance_df = pd . DataFrame ( ) fold_importance_df [ "Feature" ] = scaled_train_X . columns print ( 'ok' )
514	plt . figure ( figsize = ( 12 , 6 ) ) sns . countplot ( submission [ "diagnosis" ] ) plt . title ( "Number of data per each diagnosis" ) plt . show ( )
590	from sklearn . metrics import ( confusion_matrix , precision_recall_curve , auc , roc_curve , recall_score , classification_report , f1_score , precision_recall_fscore_support )
401	sample_submission_df = pd . read_csv ( '../input/sample_submission.csv' ) image_ids = sample_submission_df [ 'ImageId' ] predictions = [ ] for image_id in tqdm ( image_ids ) : image_path = f'../input/test/{image_id}.jpg' with tf . gfile . Open ( image_path , "rb" ) as binfile : image_string = binfile . read ( ) result_out = sess . run ( detector_output , feed_dict = { image_string_placeholder : image_string } ) predictions . append ( format_prediction_string ( image_id , result_out ) ) sess . close ( )
105	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
513	convert_path = './convert_dir' if not os . path . isdir ( convert_path ) : os . mkdir ( convert_path ) else : pass for f in os . listdir ( sample_path ) : if f [ - 3 : ] == 'dcm' : ds = pydicom . read_file ( sample_path + '/' + f ) img = ds . pixel_array cv2 . imwrite ( convert_path + '/' + f . replace ( '.dcm' , '.png' ) , img ) os . listdir ( convert_path )
301	plt . figure ( figsize = ( 10 , 8 ) ) for d , grouped in data . groupby ( 'pickup_Dayofweek' ) : sns . kdeplot ( grouped [ 'fare_amount' ] , label = f'{d}' ) plt . title ( 'Fare Amount by Day of Week' ) ;
174	from keras . preprocessing . text import Tokenizer docs = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( docs )
535	full_table [ 'Active' ] = full_table [ 'Confirmed' ] - full_table [ 'Deaths' ] - full_table [ 'Recovered' ] full_table [ 'Country/Region' ] = full_table [ 'Country/Region' ] . replace ( 'Mainland China' , 'China' ) full_table [ [ 'Province/State' ] ] = full_table [ [ 'Province/State' ] ] . fillna ( '' ) full_table
544	proportion = train [ [ 'device' , 'is_attributed' ] ] . groupby ( 'device' , as_index = False ) . mean ( ) . sort_values ( 'is_attributed' , ascending = False ) counts = train [ [ 'device' , 'is_attributed' ] ] . groupby ( 'device' , as_index = False ) . count ( ) . sort_values ( 'is_attributed' , ascending = False ) merge = counts . merge ( proportion , on = 'device' , how = 'left' ) merge . columns = [ 'device' , 'click_count' , 'prop_downloaded' ] print ( 'Count of clicks and proportion of downloads by device:' ) print ( merge )
363	X_test_pub = X_test . loc [ X_test . DateAvSigVersion <= '2018-10-25' , : ] X_test_priv = X_test . loc [ X_test . DateAvSigVersion > '2018-10-25' , : ] print ( 'public test shape: {}' . format ( X_test_pub . shape ) ) print ( 'private test shape: {}' . format ( X_test_priv . shape ) ) print ( 'fraction of private test split: {:.3f}' . format ( X_test_priv . shape [ 0 ] / X_test . shape [ 0 ] ) )
403	fig = px . line ( history . history , y = [ 'loss' , 'val_loss' ] , labels = { 'index' : 'epoch' , 'value' : 'MCRMSE' } , title = 'Training History' ) fig . show ( )
77	corr_matrix = features . corr ( ) . abs ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) ; threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style collinear_features = [ column for column in upper . columns if any ( upper [ column ] > threshold ) ] upper . style . applymap ( highlight )
240	public_df = test . query ( "seq_length == 107" ) . copy ( ) private_df = test . query ( "seq_length == 130" ) . copy ( ) public_inputs , public_adj = preprocess_inputs ( public_df ) private_inputs , private_adj = preprocess_inputs ( private_df ) public_inputs = torch . tensor ( public_inputs , dtype = torch . long ) private_inputs = torch . tensor ( private_inputs , dtype = torch . long ) public_adj = torch . tensor ( public_adj , dtype = torch . long ) private_adj = torch . tensor ( private_adj , dtype = torch . long )
255	def rle_encode ( im ) : pixels = im . flatten ( order = 'F' ) pixels = np . concatenate ( [ [ 0 ] , pixels , [ 0 ] ] ) runs = np . where ( pixels [ 1 : ] != pixels [ : - 1 ] ) [ 0 ] + 1 runs [ 1 : : 2 ] -= runs [ : : 2 ] return ' ' . join ( str ( x ) for x in runs )
279	heads [ 'phones-per-capita' ] = heads [ 'qmobilephone' ] / heads [ 'tamviv' ] heads [ 'tablets-per-capita' ] = heads [ 'v18q1' ] / heads [ 'tamviv' ] heads [ 'rooms-per-capita' ] = heads [ 'rooms' ] / heads [ 'tamviv' ] heads [ 'rent-per-capita' ] = heads [ 'v2a1' ] / heads [ 'tamviv' ]
166	train_df = train_transaction . merge ( train_identity , how = 'left' , left_index = True , right_index = True ) test_df = test_transaction . merge ( test_identity , how = 'left' , left_index = True , right_index = True ) print ( "Train shape : " + str ( train_df . shape ) ) print ( "Test shape : " + str ( test_df . shape ) )
534	import pandas as pd , numpy as np from matplotlib import pyplot as plt import scipy . stats as stats pd . options . display . max_columns = 50
515	DATA_PATH = '../input/aptos2019-blindness-detection' TRAIN_IMG_PATH = os . path . join ( DATA_PATH , 'train_images' ) TEST_IMG_PATH = os . path . join ( DATA_PATH , 'test_images' ) TRAIN_LABEL_PATH = os . path . join ( DATA_PATH , 'train.csv' ) TEST_LABEL_PATH = os . path . join ( DATA_PATH , 'test.csv' ) train_df = pd . read_csv ( TRAIN_LABEL_PATH ) test_df = pd . read_csv ( TEST_LABEL_PATH ) train_df . head ( )
339	def count_categorical ( df , group_var , df_name ) : categorical = pd . get_dummies ( df . select_dtypes ( 'object' ) ) categorical [ group_var ] = df [ group_var ] categorical = categorical . groupby ( group_var ) . agg ( [ 'sum' , 'mean' ] ) column_names = [ ] for var in categorical . columns . levels [ 0 ] : for stat in [ 'count' , 'count_norm' ] : column_names . append ( '%s_%s_%s' % ( df_name , var , stat ) ) categorical . columns = column_names return categorical
194	orderCount = orders [ orders [ "eval_set" ] == "prior" ] . groupby ( by = [ "user_id" ] ) [ "order_id" ] . count ( ) . to_frame ( ) fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( color = " ax.set(xlabel='Order Count',title=" Order Count " )
123	img_1 = get_image_data ( '1023' , 'Type_1' ) img_2 = get_image_data ( '531' , 'Type_1' ) img_3 = get_image_data ( '596' , 'Type_1' ) img_4 = get_image_data ( '1061' , 'Type_1' ) img_5 = get_image_data ( '1365' , 'Type_2' )
295	BB_zoom = ( - 74.1 , - 73.7 , 40.6 , 40.85 ) nyc_map_zoom = plt . imread ( 'https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/images/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true' )
506	clf = RandomForestClassifier ( n_estimators = 50 , criterion = 'gini' , max_depth = 5 , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = - 1 , random_state = 0 , verbose = 0 , warm_start = False , class_weight = 'balanced' )
586	train = pd . read_csv ( "../input/train.csv" ) train . head ( )
389	def compute_game_time_stats ( group , col ) : return group [ [ 'installation_id' , col , 'event_count' , 'game_time' ] ] . groupby ( [ 'installation_id' , col ] ) . agg ( [ np . mean , np . sum , np . std ] ) . reset_index ( ) . pivot ( columns = col , index = 'installation_id' )
28	import matplotlib . pyplot as plt savePickleBZ ( 'before.pbz' , before ) savePickleBZ ( 'sets.pbz' , sets ) def showBefore ( before ) : before = np . array ( before , dtype = np . float ) before = before / ( before + np . transpose ( np . copy ( before ) ) + 1e-30 ) before *= 255 before = np . array ( before , dtype = np . uint8 ) plt . figure ( figsize = ( 16 , 16 ) ) plt . imshow ( before ) showBefore ( before )
561	program = build_model ( task [ 'train' ] , verbose = True ) print ( ) if program is None : print ( "No program was found" ) else : print ( "Found program:" , program_desc ( program ) )
147	clf = OneVsRestClassifier ( SGDClassifier ( loss = 'log' , alpha = 0.00001 , penalty = 'l2' ) ) clf . fit ( X_train_multilabel , y_train ) y_pred = clf . predict ( X_test_multilabel )
199	probs = np . concatenate ( probs ) result_df = pd . DataFrame ( { 'row_id' : names , 'birds' : [ probs [ i ] for i in range ( probs . shape [ 0 ] ) ] } ) result_df = result_df . groupby ( 'row_id' ) [ 'birds' ] . apply ( lambda x : np . stack ( x ) . max ( 0 ) ) . reset_index ( ) result_df
591	plt . plot ( history [ 'loss' ] ) plt . plot ( history [ 'val_loss' ] ) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ( [ 'train' , 'test' ] , loc = 'upper right' ) ;
54	print ( "Number of different values :" ) print ( "IP :" , len ( df [ 'ip' ] . unique ( ) ) ) print ( "App :" , len ( df [ 'app' ] . unique ( ) ) ) print ( "Device :" , len ( df [ 'device' ] . unique ( ) ) ) print ( "OS :" , len ( df [ 'os' ] . unique ( ) ) ) print ( "Channel :" , len ( df [ 'channel' ] . unique ( ) ) )
457	X_COL = "var_81" Y_COL = "var_68" Z_COL = "var_108" HUE_COL = "target" N_SAMPLES = 10000 df = train_df . sample ( N_SAMPLES )
4	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . target . values , bins = 200 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
311	hyperparameters = dict ( ** bayes_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS ) print ( 'The cross validation score on the full dataset for Bayesian optimization = {:.5f} with std: {:.5f}.' . format ( cv_results [ 'auc-mean' ] [ - 1 ] , cv_results [ 'auc-stdv' ] [ - 1 ] ) ) print ( 'Number of estimators = {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
155	bold ( '**MANUFACTURING REALLY BUCKED THE GENERAL TREND**' ) temp_df = train . groupby ( [ 'timestamp' , "primary_use" ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . FacetGrid ( temp_df , col = "primary_use" , col_wrap = 2 , height = 4 , aspect = 2 , sharey = False ) ax . map ( sns . lineplot , 'timestamp' , 'meter_reading' , color = "teal" ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
206	combined_aug = Compose ( transforms = [ GaussianTargetNoise ( p = 1.0 , gaus_std = 0.3 ) , TemporalFlip ( p = 1.0 ) ] )
282	ind [ 'escolari/age' ] = ind [ 'escolari' ] / ind [ 'age' ] plt . figure ( figsize = ( 10 , 8 ) ) sns . violinplot ( 'Target' , 'escolari/age' , data = ind ) ;
110	test_gen = \ test_generator ( df_test , test_batch_size , num_rows , num_cols ) model . load_weights ( filepath = 'model.h5' ) predictions = model . predict_generator ( test_gen , steps = num_test_batches , max_queue_size = 1 , workers = 1 , use_multiprocessing = False , verbose = 1 )
208	for i in cat_feature : df [ i ] = pd . factorize ( df [ i ] ) [ 0 ] trn_cat = df [ cat_feature ] . values [ : 182080 ] tst_cat = df [ cat_feature ] . values [ 182080 : ]
117	lsvr = LinearSVR ( C = 0.05 , max_iter = 1000 ) . fit ( dfe , target_fe ) model = SelectFromModel ( lsvr , prefit = True ) X_new = model . transform ( dfe ) X_selected_df = pd . DataFrame ( X_new , columns = [ dfe . columns [ i ] for i in range ( len ( dfe . columns ) ) if model . get_support ( ) [ i ] ] ) X_selected_df . shape
563	df_train = pd . read_csv ( "../input/train.csv" ) df_test = pd . read_csv ( "../input/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
164	import pandas as pd import numpy as np import warnings warnings . filterwarnings ( 'ignore' ) from datetime import datetime from sklearn . model_selection import RandomizedSearchCV , GridSearchCV from sklearn import metrics from sklearn . metrics import roc_auc_score from sklearn . model_selection import StratifiedKFold from xgboost import XGBClassifier pd . set_option ( 'display.max_columns' , 200 )
129	TRAIN_MASKS_CSV [ 'id' ] = TRAIN_MASKS_CSV [ 'img' ] . apply ( lambda x : x [ : - 7 ] ) len ( TRAIN_MASKS_CSV [ 'id' ] . unique ( ) ) , len ( TRAIN_MASKS_CSV [ 'id' ] . unique ( ) ) * 16
201	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "bedroomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Bedroom Count ",title=" Bedroom Count Vs Log Error " )
162	directions = { 'N' : 0 , 'NE' : 1 / 4 , 'E' : 1 / 2 , 'SE' : 3 / 4 , 'S' : 1 , 'SW' : 5 / 4 , 'W' : 3 / 2 , 'NW' : 7 / 4 }
344	credit = pd . read_csv ( '../input/credit_card_balance.csv' ) credit = convert_types ( credit , print_info = True ) credit . head ( )
83	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
218	df_brazil_cases_by_day = df_grouped_brazil [ df_grouped_brazil . confirmed > 0 ] df_brazil_cases_by_day = df_brazil_cases_by_day . reset_index ( drop = True ) df_brazil_cases_by_day [ 'day' ] = df_brazil_cases_by_day . date . apply ( lambda x : ( x - df_brazil_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_brazil_cases_by_day = df_brazil_cases_by_day [ reordered_columns ] df_brazil_cases_by_day
322	def objective ( hyperparameters , iteration ) : if 'n_estimators' in hyperparameters . keys ( ) : del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , nfold = N_FOLDS , early_stopping_rounds = 100 , metrics = 'auc' , seed = 42 ) score = cv_results [ 'auc-mean' ] [ - 1 ] estimators = len ( cv_results [ 'auc-mean' ] ) hyperparameters [ 'n_estimators' ] = estimators return [ score , hyperparameters , iteration ]
487	path_data = '../input' device = 'cuda' batch_size = 32 torch . manual_seed ( 0 )
581	all_data_na = ( feature_matrix_enc . isnull ( ) . sum ( ) / len ( feature_matrix_enc ) ) * 100 all_data_na = all_data_na . drop ( all_data_na [ all_data_na == 0 ] . index ) . sort_values ( ascending = False ) [ : 30 ] missing_data = pd . DataFrame ( { 'Missing Ratio' : all_data_na } ) missing_data . head ( 20 )
21	plt . figure ( ) ax = df [ 'Class' ] . value_counts ( ) . plot ( kind = 'bar' ) ax . set_title ( 'Class Distribution Over Entries' ) ax . set_xlabel ( 'Class' ) ax . set_ylabel ( 'Frequency' ) plt . tight_layout ( ) plt . show ( )
361	train_path = data_src + 'train' test_path = data_src train_ids = train_df . index . values test_ids = test_df . index . values
489	n_iter = 1000 start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_cutmix ( images , labels , PROBABILITY = 1.0 ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_cutmix: {timing}" )
566	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) fasttext_embeddings = load_fasttext ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings , fasttext_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings , fasttext_embeddings gc . collect ( ) np . shape ( embedding_matrix )
418	task_num = np . random . randint ( 1 , 400 ) arc = ARC_solver ( task_num ) arc . plot_task ( ) image = np . array ( train_tasks [ task_num ] [ 'train' ] [ 0 ] [ 'input' ] ) arc . identify_object ( image , method = 2 ) arc . plot_identified_objects ( arc . identified_objects )
356	cred_card_bal = pd . read_csv ( "../input/credit_card_balance.csv" ) cred_card_bal = cred_card_bal . drop ( [ 'SK_ID_PREV' ] , axis = 1 ) cred_card_bal_dfs = feature_aggregator_on_df ( cred_card_bal , aggs_cat , aggs_num , [ 'SK_ID_CURR' ] , 'cred_card_balance' , 'basic' , save = False )
441	N = 10 scores = np . zeros ( N , ) for i in range ( N ) : p = i * 1. / N v = p * leak_df [ 'pred1' ] . values + ( 1. - p ) * leak_df [ 'pred3' ] . values vl1p = np . log1p ( v ) scores [ i ] = np . sqrt ( mean_squared_error ( vl1p , leak_df . meter_reading_l1p ) )
12	train_data = observation . train train_data = train_data . set_index ( [ 'id' , 'timestamp' ] ) . sort_index ( ) train_data
572	sns . set ( style = 'darkgrid' ) sns_plot = sns . palplot ( sns . color_palette ( 'Accent' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Accent_d' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'CMRmap' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Set1' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Set3' ) )
509	from sklearn . model_selection import train_test_split params = { 'n_estimators' : 200 , 'max_depth' : 5 , 'min_child_weight' : 100 , 'subsample' : .9 , 'gamma' : 1 , 'objective' : 'reg:linear' , 'colsample_bytree' : .8 , 'nthread' : 3 , 'silent' : 1 , 'seed' : 27 } train , test = train_test_split ( x_train , test_size = 0.2 ) predictors = df [ 'feature' ] [ df [ 'fscore' ] > 0.5 ] . tolist ( )
92	df_0 = df_data [ df_data [ 'label' ] == 0 ] . sample ( SAMPLE_SIZE , random_state = 101 ) df_1 = df_data [ df_data [ 'label' ] == 1 ] . sample ( SAMPLE_SIZE , random_state = 101 ) df_data = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_data = shuffle ( df_data ) df_data [ 'label' ] . value_counts ( )
454	def compute_rolling_mean_per_store_df ( df , period = 30 ) : return ( df . set_index ( "date" ) . groupby ( "store_id" ) . rolling ( period ) . mean ( ) . reset_index ( ) )
277	corr_matrix = heads . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
286	model = RandomForestClassifier ( n_estimators = 100 , random_state = 10 , n_jobs = - 1 ) cv_score = cross_val_score ( model , train_set , train_labels , cv = 10 , scoring = scorer ) print ( f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}' )
41	filenames = os . listdir ( "../input/train/train" ) categories = [ ] for filename in filenames : category = filename . split ( '.' ) [ 0 ] if category == 'dog' : categories . append ( 1 ) else : categories . append ( 0 ) df = pd . DataFrame ( { 'filename' : filenames , 'category' : categories } )
307	x = sample ( space ) subsample = x [ 'boosting_type' ] . get ( 'subsample' , 1.0 ) x [ 'boosting_type' ] = x [ 'boosting_type' ] [ 'boosting_type' ] x [ 'subsample' ] = subsample x
347	train_imgs , val_imgs = train_test_split ( selected_imgs , test_size = 0.15 , stratify = selected_imgs [ 'has_ship' ] , random_state = 69278 ) train_fnames = train_imgs [ 'ImageId' ] . values val_fnames = val_imgs [ 'ImageId' ] . values
26	if blaze_bboxes == [ ] : print ( '⚠️BlazeFace is unable to detect face in this frame.' ) if mtcnn_bboxes == [ ] : print ( '⚠️MTCNN is unable to detect face in this frame.' ) if mobilenet_bboxes == [ ] : print ( '⚠️mobilenet is unable to detect face in this frame.' ) if yolo_bboxes == [ ] : print ( '⚠️mobilenet is unable to detect face in this frame.' )
243	date_agg_3 = train_agg . groupby ( level = [ 1 , 2 ] ) . sum ( ) date_agg_3 . columns = ( 'bookings' , 'total' ) date_agg_3 . plot ( kind = 'bar' , stacked = 'True' , figsize = ( 16 , 10 ) )
0	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . target . values , bins = 200 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
143	CALENDAR_DTYPES = { 'date' : 'str' , 'wm_yr_wk' : 'int16' , 'weekday' : 'object' , 'wday' : 'int16' , 'month' : 'int16' , 'year' : 'int16' , 'd' : 'object' , 'event_name_1' : 'object' , 'event_type_1' : 'object' , 'event_name_2' : 'object' , 'event_type_2' : 'object' , 'snap_CA' : 'int16' , 'snap_TX' : 'int16' , 'snap_WI' : 'int16' } PARSE_DATES = [ 'date' ] SPRICES_DTYPES = { 'store_id' : 'object' , 'item_id' : 'object' , 'wm_yr_wk' : 'int16' , 'sell_price' : 'float32' }
338	new_corrs = [ ] for col in columns : corr = train [ 'TARGET' ] . corr ( train [ col ] ) new_corrs . append ( ( col , corr ) )
176	application_train = pd . read_csv ( '../input/application_train.csv' ) application_test = pd . read_csv ( '../input/application_test.csv' ) bureau = pd . read_csv ( '../input/bureau.csv' ) bureau_balance = pd . read_csv ( '../input/bureau_balance.csv' ) POS_CASH_balance = pd . read_csv ( '../input/POS_CASH_balance.csv' ) credit_card_balance = pd . read_csv ( '../input/credit_card_balance.csv' ) previous_application = pd . read_csv ( '../input/previous_application.csv' ) installments_payments = pd . read_csv ( '../input/installments_payments.csv' )
45	test_gen = ImageDataGenerator ( rescale = 1. / 255 ) test_generator = test_gen . flow_from_dataframe ( test_df , "../input/test1/test1/" , x_col = 'filename' , y_col = None , class_mode = None , target_size = IMAGE_SIZE , batch_size = batch_size , shuffle = False )
525	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
144	model = CatBoostRegressor ( iterations = 1000 , task_type = 'GPU' , verbose = 0 , loss_function = 'RMSE' , boosting_type = 'Plain' , depth = 8 , ) model . fit ( train_pool , eval_set = val_pool , plot = True ) del train_pool , val_pool gc . collect ( )
225	has_to_plot_infection_peak = True if has_to_run_sir : crisis_day_sir = np . argmax ( I_predict_sir ) if has_to_run_sird : crisis_day_sird = np . argmax ( I_predict_sird ) if has_to_run_seir : crisis_day_seir = np . argmax ( I_predict_seir ) if has_to_run_seird : crisis_day_seird = np . argmax ( I_predict_seird ) if has_to_run_seirdq : crisis_day_seirdq = np . argmax ( I_predict_seirdq )
31	train_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv" ) calendar_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/calendar.csv" ) price_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv" ) sample = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv" )
364	train_months = X_train . DateAvSigVersion . apply ( lambda x : '{}-{}' . format ( x . year , x . month ) ) test_months = X_test . DateAvSigVersion . apply ( lambda x : '{}-{}' . format ( x . year , x . month ) ) df_months = pd . DataFrame ( train_months . value_counts ( ) ) . reset_index ( ) df_months = df_months . merge ( pd . DataFrame ( test_months . value_counts ( ) ) . reset_index ( ) , how = 'left' , on = 'index' ) df_months . sort_values ( 'index' )
63	def rle_encoding ( x ) : dots = np . where ( x . T . flatten ( ) == 1 ) [ 0 ] run_lengths = [ ] prev = - 2 for b in dots : if ( b > prev + 1 ) : run_lengths . extend ( ( b + 1 , 0 ) ) run_lengths [ - 1 ] += 1 prev = b return " " . join ( [ str ( i ) for i in run_lengths ] ) print ( 'RLE Encoding for the current mask is: {}' . format ( rle_encoding ( label_mask ) ) )
267	hits_sample = hits . sample ( 8000 ) sns . pairplot ( hits_sample , hue = 'volume_id' , size = 8 ) plt . show ( )
354	gbm_params = { 'objective' : 'binary' , 'boosting_type' : 'gbdt' , 'nthread' : 6 , 'learning_rate' : 0.05 , 'num_leaves' : 20 , 'colsample_bytree' : 0.9497036 , 'subsample' : 0.8715623 , 'subsample_freq' : 1 , 'max_depth' : 8 , 'reg_alpha' : 0.041545473 , 'reg_lambda' : 0.0735294 , 'min_split_gain' : 0.0222415 , 'min_child_weight' : 60 , 'seed' : 0 , 'verbose' : - 1 , 'metric' : 'auc' , } oof_train , oof_test = run_kfold_lgbm ( X_train , y_train , X_test , gbm_params )
399	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
226	import numpy as np import pandas as pd pd . set_option ( 'display.max_colwidth' , - 1 ) import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
136	from sklearn . tree import DecisionTreeClassifier dt = DecisionTreeClassifier ( ) dt = dt . fit ( xT , yT )
60	print ( "Memory usage before optimization :" , str ( round ( total_before_opti / 1000000000 , 2 ) ) + 'GB' ) print ( "Memory usage after optimization :" , str ( round ( sum ( df . memory_usage ( ) ) / 1000000000 , 2 ) ) + 'GB' ) print ( "We reduced the dataframe size by" , str ( round ( ( ( total_before_opti - sum ( df . memory_usage ( ) ) ) / total_before_opti ) * 100 , 2 ) ) + '%' )
212	imid = 'TCGA-G9-6362-01Z-00-DX1' image_path = df [ df . id == imid ] . path . values [ 0 ] image = cv2 . imread ( image_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( image ) plt . show ( )
315	train = train . drop ( columns = to_drop ) test = test . drop ( columns = to_drop ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
471	def create_datagen ( ) : return ImageDataGenerator ( zoom_range = 0.15 , fill_mode = 'constant' , cval = 0. , horizontal_flip = True , vertical_flip = True , ) data_generator = create_datagen ( ) . flow ( x_train , y_train , batch_size = BATCH_SIZE , seed = 2019 )
252	target = df_train [ 'outliers' ] del df_train [ 'outliers' ] del df_train [ 'target' ]
414	BATCH_SIZE = 8 train_idx , val_idx = train_test_split ( non_missing_train_idx . index , random_state = 2019 , test_size = 0.2 ) train_generator = DataGenerator ( train_idx , reshape = ( 256 , 512 ) , df = mask_count_df , target_df = train_df , augment = True , batch_size = BATCH_SIZE , n_classes = 4 ) val_generator = DataGenerator ( val_idx , reshape = ( 256 , 512 ) , df = mask_count_df , target_df = train_df , augment = False , batch_size = BATCH_SIZE , n_classes = 4 )
30	model . compile ( optimizer = 'adam' , loss = 'mean_squared_error' , metrics = [ 'mae' ] ) model . fit ( x , Y , batch_size = 64 , epochs = 100 ) model . fit ( x , Y , batch_size = 128 , epochs = 50 ) model . fit ( x , Y , batch_size = 256 , epochs = 50 )
532	import seaborn as sns sns . set ( ) g = sns . FacetGrid ( pd . melt ( df [ [ 'bone_length' , 'rotting_flesh' , 'hair_length' , 'has_soul' , 'type' ] ] , id_vars = 'type' ) , col = 'type' ) g . map ( sns . boxplot , 'value' , 'variable' )
289	model = RandomForestClassifier ( max_depth = None , n_estimators = 10 ) model . fit ( train_selected , train_labels ) estimator_nonlimited = model . estimators_ [ 5 ] export_graphviz ( estimator_nonlimited , out_file = 'tree_nonlimited.dot' , feature_names = train_selected . columns , class_names = [ 'extreme' , 'moderate' , 'vulnerable' , 'non-vulnerable' ] , rounded = True , proportion = False , precision = 2 )
512	test [ 'Min_week' ] = test . groupby ( 'Patient' ) [ 'Weeks' ] . transform ( 'min' ) base = test . loc [ test . Weeks == test . Min_week ] base = base [ [ 'Patient' , 'FVC' ] ] . copy ( ) base . columns = [ 'Patient' , 'Base_FVC' ] test = test . merge ( base , on = 'Patient' , how = 'left' ) test [ 'Base_week' ] = test [ 'Weeks' ] - test [ 'Min_week' ]
353	y_pred_valid = model . predict ( X_val ) y_pred_test = model . predict ( X_test ) del X_tr , X_val , X_test gc . collect ( )
552	sns . catplot ( x = "store_id" , y = "total_sales" , hue = "cat_id" , data = train_sales , kind = "bar" , height = 8 , aspect = 1 ) ;
124	plt_st ( 20 , 20 ) plt . imshow ( complete_images [ 0 ] ) plt . title ( "Training dataset of type %i" % ( 0 ) )
536	stats = [ ] for Province in [ 'Hong Kong' , 'Hubei' ] : df = get_time_series_province ( Province ) print ( '{} COVID-19 Prediction' . format ( Province ) ) opt_display_model ( df , stats )
14	n_train_rows , n_train_cols = train . shape n_test_rows , n_test_cols = test . shape print ( '- Training data has {:9,} rows and {:2,} columns.' . format ( * train . shape ) ) print ( '- Testing data has {:9,} rows and {:2,} columns.' . format ( * test . shape ) ) print ( '- There are {:.1f} times more ( ' testing data examples . ' . format ( n_train_rows / n_test_rows , n_train_rows - n_test_rows ) ) print ( "- There are %i missing values in the training data." % train . isnull ( ) . sum ( ) . sum ( ) ) print ( "- There are %i missing values in the testing data." % test . isnull ( ) . sum ( ) . sum ( ) )
262	map_ord1 = { 'Novice' : 1 , 'Contributor' : 2 , 'Expert' : 4 , 'Master' : 5 , 'Grandmaster' : 6 } full_data . ord_1 = full_data . ord_1 . map ( map_ord1 )
179	val_p = [ 'APARTMENTS_AVG' , 'BASEMENTAREA_AVG' , 'YEARS_BEGINEXPLUATATION_AVG' , 'YEARS_BUILD_AVG' , 'COMMONAREA_AVG' , 'ELEVATORS_AVG' , 'ENTRANCES_AVG' , 'FLOORSMAX_AVG' , 'FLOORSMIN_AVG' ] for i in val_p : plt . figure ( figsize = ( 5 , 5 ) ) sns . distplot ( application_train [ i ] . dropna ( ) , kde = True , color = 'g' ) plt . title ( i ) plt . xticks ( rotation = - 45 ) plt . show ( )
303	features = list ( data . columns ) for f in [ 'pickup_datetime' , 'fare_amount' , 'fare-bin' , 'color' ] : features . remove ( f ) len ( features )
568	leak = pd . read_csv ( '../input/breaking-lb-fresh-start-with-lag-selection/train_leak.csv' ) data [ 'leak' ] = leak [ 'compiled_leak' ] . values data [ 'log_leak' ] = np . log1p ( leak [ 'compiled_leak' ] . values )
400	with strategy . scope ( ) : transformer_layer = ( transformers . TFDistilBertModel . from_pretrained ( 'distilbert-base-multilingual-cased' ) ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
135	best_n_clusters = 5 norm_feat = MinMaxScaler ( ) . fit_transform ( test_feat ) clt = KMeans ( n_clusters = best_n_clusters , random_state = RANDOM_SEED ) . fit ( norm_feat ) test_clusters = pd . DataFrame ( clt . labels_ , columns = [ 'cluster' ] , index = test_meta [ 'signal_id' ] ) stats_df = test_clusters . reset_index ( ) . groupby ( 'cluster' ) . count ( ) stats_df . columns = [ 'count' ] display ( stats_df )
38	base_df = pd . read_csv ( "/kaggle/input/covid19-global-forecasting-week-2/train.csv" ) base_df = base_df . rename ( { "Province_State" : "state" , "Country_Region" : "country" } , axis = 1 ) base_df . loc [ base_df [ "state" ] . isna ( ) , "state" ] = "Unknown" scoring_dates = test [ "Date" ] . unique ( )
33	fig = px . line ( train_df , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'Sex' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines+markers' )
298	lr . fit ( X_train [ [ 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train ) print ( 'Intercept' , round ( lr . intercept_ , 4 ) ) print ( 'abs_lat_diff coef: ' , round ( lr . coef_ [ 0 ] , 4 ) , '\tabs_lon_diff coef:' , round ( lr . coef_ [ 1 ] , 4 ) , '\tpassenger_count coef:' , round ( lr . coef_ [ 2 ] , 4 ) )
366	shap_sum = np . abs ( shap_values ) . mean ( axis = 0 ) importance_df = pd . DataFrame ( [ X_importance . columns . tolist ( ) , shap_sum . tolist ( ) ] ) . T importance_df . columns = [ 'column_name' , 'shap_importance' ] importance_df = importance_df . sort_values ( 'shap_importance' , ascending = False ) importance_df
232	import numpy as np test = pd . read_csv ( "../input/train.csv" ) length = len ( test ) np . random . seed ( 287 ) perfect_sub = np . random . rand ( length ) target = ( perfect_sub > 0.963552 ) . astype ( dtype = int ) print ( "Perfect submission looks like: " , perfect_sub ) print ( "Target vector looks like: " , target ) print ( "Target vector class distibution: " ) counts = pd . Series ( target ) . value_counts ( ) counts / counts . sum ( )
557	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 7 - Pleural Effusion" ) draw ( parsed [ df [ 'patientId' ] [ 125 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 125 ] ] )
48	from sklearn import metrics fbeta_sklearn = metrics . fbeta_score ( valid_labels , preds , 2 , average = 'samples' ) print ( fbeta_sklearn )
522	binary_cat_features = [ col for col in train . columns if train [ col ] . value_counts ( ) . shape [ 0 ] == 2 ] object_features = [ 'edjefe' , 'edjefa' ] categorical_feats = binary_cat_features + object_features
580	def process_dataframe ( input_df , encoder_dict = None ) : print ( 'Label encoding categorical features...' ) categorical_feats = input_df . columns [ input_df . dtypes == 'object' ] for feat in categorical_feats : encoder = LabelEncoder ( ) input_df [ feat ] = encoder . fit_transform ( input_df [ feat ] . fillna ( 'NULL' ) ) print ( 'Label encoding complete.' ) return input_df , categorical_feats . tolist ( ) , encoder_dict
281	corr_matrix = ind . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
98	num_test_images = 57458 model . load_weights ( 'model.h5' ) predictions = model . predict_generator ( test_gen , steps = num_test_images , verbose = 1 )
11	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( np . log ( train_df [ columns_to_use ] . values . flatten ( ) + 1 ) , bins = 50 ) plt . title ( 'Log Histogram all train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Log value' ) plt . show ( )
