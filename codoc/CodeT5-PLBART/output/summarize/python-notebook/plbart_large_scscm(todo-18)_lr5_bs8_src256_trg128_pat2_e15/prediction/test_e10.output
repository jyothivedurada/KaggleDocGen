143	Compile and fit model
1735	set color palette
756	Distribution for target
217	MinMax scale all features
1581	need to convert columns into categorical
1485	Use the TFRecords generated by the generator
970	Join the train and test sets
649	create the model
175	Load the image data
852	Fare Amount versus Time since Start of Records
343	create a generator that iterate over the Sequence
1015	Import the datasets
174	Load the image from image name
1575	Reading the datasets
1570	Convert the DICOM images to PNG
481	Tokenize the text
1367	Shape the districts of every day
1110	Extract processed data and format them as DFs
1119	Distribution inspection of original target and predicted train and test
200	Implementation for the plotting part
745	build a dict to convert surface names into numbers
1658	Tokenizing the selected text
1245	Convert floats to integers
1375	Difference Variations
245	Filter Andorra , run the Linear Regression workflow
1699	Now we will convert our sample input and output to array
1704	delete best candidate
1502	Order does not matter since we will be shuffling the data anyway
246	Set the dataframe where we will update the predictions
1212	Applying Quadratic Spline
1342	Calculates ratios of each message
1214	Order does not matter since we will be shuffling the data anyway
330	Visualizing the images
1207	load train and test images
1807	Read the data
353	fold means per fold
513	Conference Tourney Games
786	Plot normalized importance
201	Some functions to render neato images
1707	Run the program
432	Apply each model on test set and output the predictions
1819	Join market and news
282	Data Prepparation
1690	Sort by length
1564	Inverting the object and year , month
1334	Expansion and Depthwise Convolution
57	Seeding everything for reproducible results
960	What are the features we will work on
431	Extract target variable
663	Barplot on bookings and year
762	Create a scatterplot of the counts
891	Drop unwanted columns
820	Non Limitable Classifier
719	save preprocessed model and classify
1157	numpy and matplotlib defaults
784	Now lets check what our model looks like
1071	You can access the actual face itself like this
215	XGBOOST Sparse Feature Storage
1269	If the image is empty return the unique colors
1720	SAVE DATASET TO DISK
410	OneVsRestClassifier with SGDClassifier
176	Visualize the images
291	predictions image by image
873	Write column names
618	Calculate the areas of each contour
1639	Click Rnd in each feature
1451	inverse transform
355	Target difference comparision
749	This is the primary method this needs to be defined
1426	Number of smaller words
309	create an embedding matrix
1780	The wordcloud of the raven for Edgar Allen Poe
1427	Set values for various parameters
86	Raw data analysis
462	Load libs and funcs
835	BanglaLekha new observations
1309	Ready frame
1327	Convolutions like TensorFlow , for a fixed image size
871	Run the objective
1385	suppose all instances are not crowd
1598	checking missing data
398	Calculate the confusion matrix
1583	Multiply new features by lugar features
1659	for Neutral tweets keep things same
1154	Diff the datasets
1178	take a look of .dcm extension
1656	written by Nanashi
963	Featurize feature sets
943	ROC AUC from train and test data
352	Load the train and test images
62	Exploring the categorical variables
30	Load the train and test data sets
194	find and remove noise
868	Sample out data
1292	Instantiating the model
155	Downsize evolution over the day
967	There might be a more efficient method to accomplish this
1446	Order does not matter since we will be shuffling the data anyway
983	Bureau date features
886	Reading all data into respective dataframes
461	Run random search
83	Read the Text Data
400	Convert images from memory to gpu
1130	define the dataloader
422	Distribution of meter reading over hour
667	Maybe it is still somehow related to other distributions
772	Exploring the curve function
1587	create categorical and object features
588	Room Count Vs Log Error
1045	Count unique patients in each city
13	Load train and test data
1030	There might be a more efficient method to accomplish this
793	Create a selector
1010	Add the columns info
710	PRINT CV AUC
448	Updated train and test data for modelling
28	MODEL WITH SUPPORT VECTOR MACHINE
1039	Previous counts categorical
1742	SCALE target variable
954	Set the categorical variables
1728	This enables operations which are only applied during training like dropout
1759	Feature matrix and feature definition
654	Read the test data
186	Are there outliers or not
123	Process text for RNNs
1601	checking missing data
1481	Histogram of codprovide values
1677	Drawing some images
31	create the vectors for each feature
1425	total number of unique tokens
670	Create a function to transpose the data by country
1589	to truncate it
372	Accuracy of VotingReg
1751	Creating an entity from dataframe
1091	Create submission dataframe
1667	Stacking of Test predictions
1755	Relationship between bureau and rureau
231	Merge train and test , exclude overlap
1216	creating a function to aggregate the stats per installation id
484	example of plotting a model
1383	split train and validation sets
1313	Light GBM Results
1359	Fast data loading
1517	Get raw training data
682	Creating a DataFrame out of the original labels
375	Run the build process
1619	checking missing data
1800	plot baseline and predictions
1199	plot best predictions
558	If we have a list of parameters , convert them into lists
1447	Create submission file
751	Settings for pretty nice plots
645	try random samples
393	What about the clusters in the test set
1815	Load the data
1480	batch grid mask
717	Random Forest Regressor
778	drop high correlation columns
1364	Address change function
1546	Get month and year from date
617	Split back to train and test sets
1600	Lets look at some of the correlations
120	Function for gettinging words from series
1799	shift test predictions for plotting
254	Gradient Boosting model
192	Show original image and grayscale
334	Looking some informations of our data
1812	extract important features
903	get score on best score
716	We now have something we can pass to our model
364	We scale the data
1097	Check if train and test indices overlap
589	Number of Storeys Vs Log Error
1765	We can look at the correlation between two plots
1109	Unique IDs from train and test
541	Run Grid Search
472	Precision and Recall
1473	MAKE CUTMIX LABEL
644	Perfect submission and target vector
66	load and shuffle filenames
247	Apply exponential transf
286	Load the data
1340	Remove Data with Signal to noise
230	Implementing the SIR model
429	Create year column
1727	Shuffling happens when splitting for kfolds
1724	text version of squash , slight modified
1556	some config values
1706	choose best candidate
1683	Draw greyscale images
1525	Span logits minus the cls logits seems to be close to the best
277	reorder the input data
1197	Prediction of new features
1082	A single set of features of the dataset
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
652	Charts with seaborn
1693	lifted function to remove unneeded parameters
1547	inplace input data
840	Manhattan Distance by Fare Amount
1787	Index of the error we are trying to predict
540	Standardize the classifiers
623	Add China as an additional feature
817	Applies method on train and test
1252	Load Model Weights
579	Calculate logmel spectrogram using pytorch
1487	restore the latest checkpoint
22	Impute any values will significantly affect the RMSE score for test set
1424	function for cleaning lower case words
516	add team conffences
451	Only the classes that are true for each sample will be filled in
1183	We need to convert the weightage into a list
439	Top most commmon IntersectionId s
877	Evaluate Bayesian Results
878	Calculate scores from random and bayesian parameters
1314	Plot the variables
728	Read in the labels
1746	Replace some missing values
1115	Check if columns between the two DFs are the same
68	if augment then horizontal flip half the time
573	Bathrooms with respect to bedrooms
870	Write column names
499	handle .ahi files
1432	calculate the link count and the node count for each title
1370	Label Encoding for categorical features
54	Examine the duration of the taxi trip
1739	distribution of winPlacePerc in each city
1469	splitted features into a single bin feature
1049	one hot encoding
1676	Lung Opacity
1679	get some sessions information
1341	Draw the measured and un measured lines
550	Standardize the classifiers
647	Import the libraries
1305	Submission with best value
831	Now our training and testing data are same as above
1709	Importing sklearn libraries
337	Train the model
1234	Model initialization and fitting on train sets
494	Distribution of values in application train set
678	using outliers column as labels instead of target column
1354	Fast data loading
900	check for encoding
209	FIND ORIGIN PIXEL VALUES
547	Run Grid Search
874	ROC AUC from train and test data
250	Linear SVR model
466	Plot ROC Curve
1266	Extracting the test set and making submission
373	Compute the STA and the LTA
1121	Extract target variable
324	load train and test dataloader
1251	Split train and validation sets by CATEGORY
138	Compiling GPU and the compiler
1048	Credit card balance
1284	READING OUR GIVEN DATA INTO MODEL
287	Load the model and evaluate the model
1417	Load the model into the TPU
1567	Pinball loss for multiple quantiles
575	Correlations with Bathrooms and prices
1320	convert column names to ints
1625	filling missing values
626	Sort by day
972	random search and bayesian optimization
261	Look at parameters and LB score visualization
1504	LIST DESTINATION PIXEL INDICES
610	Reads absolute path and returns classes , filenames
1218	We can look at the stats by game time
1582	Factorize categorical features
1777	plot the heatmap
1654	Exploring the correlation values
810	store performance stats
196	Show original image and grayscale
1430	make test features
574	bedrooms and bedrooms with different interest level
1795	fit the model
1253	Train the model with a checkpoint
198	find and remove noise
182	Categories with highest price
423	Monthly readings bases on buildings
742	Clean up the ID some times
1330	Encodes a list of BlockArgs to a list of strings
1646	retype model to get the right model
1280	Run the ARC solver
1407	Train the model
1542	Fixing the spurious classes
939	Create dataframe to display scores and parameters
283	How many data in each label
1307	Calculate the quantized range of the feature
325	load train and test dataloader
1463	CNN Model for multiclass classification
41	Overview of Missing Values
826	Read the image from image id
279	Print inertia for each K
1545	Create new column name
781	need to convert everything to numeric
738	draw the image we picked up earlier
205	Load relevant Libraries
744	Convert DNE to PNG via openCV
834	There is an outlier in the prediction
1592	some config values
1306	Importing relevant Libraries
1139	Plot the dependence plot
1026	Train and predict
1227	Read the data
1410	Set the parameters for the model
354	Check if everything is ok
1387	transforms image and boxes
914	Joining the aggregated data with the main dataframe
1486	Load the pretrained model
1633	Age distribution with seaborn
1346	the predictions associated with the train tasks
1438	Artificially increase training set
296	Sample the binary target
447	Encoding the Regions
363	Drop nuisance columns
430	Encode Categorical Data
79	Resize image to desired size
532	Standardize the classifiers
1395	Draw the graph on top of the molecule
937	If you like it , Please upvote
1692	lifted function to convert a sequence of arguments
116	Defining the parameter grid for the model
686	Use the dataframe to define train and test generators
759	households without head
992	Relationship the new features to the main dataframe
1060	Split train and validation sets
1168	Transforming the lat and lon
821	Non Limitable Classifier
1584	Checking columns with only one value
761	Look at Missing Values
1736	Visualizing the Target feature pairs
433	Ignore the warnings
1397	Exploratory Data Analysis
1602	Different Time Series Features
1401	Explore the series values
880	Load and Preparation
1275	Applyes background to the image
800	Lightplot on best scores
1675	Visualize patient entry
1528	Join examples with features and raw results
915	unique values in each column
689	functions to show an image
1000	do feature engineering with our custom features
522	Add the modellosers to the dataframe
504	Applying the lineplot on the whole dataset
1749	Create an entity from installments dataframe
361	Get image data PIL
156	Creating a dataframe using the csv data
884	Loading the data
395	Calculating Confusion Matrix
875	dict存储数转化的数据 df
774	that have not been eliminated yet
1595	Pad the sentences
239	Filter Italy , run the Linear Regression workflow
782	need to convert everything to numeric
163	RLE Encoding for Mask
507	Split the train X , y , and the rest X
1689	Sort by weight
743	pivot to have one row per type
976	Setting for random search
907	Get the size of the data
114	Joining the month and state ids
563	Descriptive of Items
752	Setting the column width
303	Load the data
1353	iterate through all the columns of a dataframe and modify the data type
449	Setting the target variable
1278	Identify by fold
238	Filter Italy , run the Linear Regression workflow
1204	create fake folder
691	Computes gradient of the Lovasz extension minus the sorted values
1100	Get the average fold AUC
609	if save to dir
128	Prepare Traning Data
1336	Skip connection and drop connect
1445	we need to predict and calculate the scoreup
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1399	transform to series
780	Range of values in each feature
1701	Create a list of candidates from the candidates list
289	Ekush Classification Report
1345	The first prediction is always empty
602	Load the datasets
559	Create ordered dictionary
55	find number of clusters
298	Read in image and resize
202	Determine current pixel spacing
240	Filter Germany , run the Linear Regression workflow
1453	drop rows with NaN values
326	Initialize patient entry into parsed
1123	Create the training and validation sets
73	create network and compiler
1411	Create the layout
392	Resize fixed images
1406	Get just the digits from the seeding
272	configurations for the training part
1147	Unique IDs from train and test
1259	Using original generator
627	Groping the spain cases by day
1329	Encodes a block to a string
918	Some new features from train.csv
1318	build the train and test data
397	using random forests
235	Clean Id columns and keep ForecastId as index
906	Cumulative variance explained with PCA
919	Brand new features
631	calculate I , N , and D
1265	create train and validation generators
1680	the time spent in the app so far
266	There are some missing values in Display Images
916	Merge BureauInfo with main dataframe
594	Adding the text features onto the full text
1781	Calling our overwritten Count vectorizer
536	Standardize the classifiers
614	Weight of the class
799	Train and predict
867	Create our own hyperparameters
1052	Train and predict
958	List of bounding boxes in each feature
64	Distribution of continuous variables
371	We now have something we can pass to our model
32	Identity Hate Function
1210	Pad the image to be in square space
746	calculate the most frequent series
1083	Load the data
1328	Gets a block through a string notation of arguments
986	Start with some new features
1196	Get fold results
953	Importing the librarys and datasets
162	find two cell indices and create mask
360	Plot the complete images
150	Which category is attributed most often
1771	text version of squash , slight modified
1522	FIND ORIGIN PIXEL VALUES
1571	load best model
1572	Number of data per each diagnosis
91	Augmentations and data analysis
1202	Load Model into TPU
1141	Plot the dependence of the game
946	altair is a very nice plotting library by the way
1373	We can visualize the model as a heatmap
188	Word Cloud for each item
284	Defining the target variable
1660	Read the cities and convert to integers
1496	Some MODELS ENCODE LIST
794	Get the selected features
435	Markdown is a very nice library by the way
346	Fancing Generator
856	Freeze the features
643	Data augmentation and manipulation
1450	create the look back datasets
1594	Tokenize the sentences
1098	Create the training and validation sets
1326	Round number of filters based on depth multiplier
107	Number of stores and item
797	Convert data to numpy array
1400	scale series to one hot encoding
1191	Generate predictions for submission
243	Filter Albania , run the Linear Regression workflow
525	Run Grid Search
1089	Resize test predictions
1497	Load the pretrained model
207	LIST DESTINATION PIXEL INDICES
1145	Curve for each day
1457	checking missing data
17	Now extract the data from the new transactions
416	load train data
1530	Previous app data
378	Mean absolute error
1442	prepare submission data
1289	Initialize the input shapes
812	Write column names
885	Importing the librarys and datasets
1804	Plotting ROC Curve
1638	Show the boxplot of Minute distribution
926	The objective function is borrowed from
997	seed features after each training round
305	Load the model and evaluate the model
832	Observation Bounding Boxes
1714	cross validation and metrics
1767	Tokenize the sentences
1792	Web Traffic Months crossdays
1414	sorted by label
213	creating dummies columns
381	Load item from train.bson file
1563	import xgboost as xgb
1371	create the LightGBM dataset
362	Load image by name
1573	set of categorical columns
1392	Save the dataframe to the parquet file
1721	LOAD DATASET FROM DISK
1051	Hyperparameters search for LGBMClassifier
59	Unfreezing the model and checking the best lr for another cycle
1186	count of each type
1616	Computes gradient of the Lovasz extension minus the sorted values
489	Function for group by
1519	Number of repetitions for each class
386	unpack an item offset and length
628	Sort by day
226	Pickup model based on CNN
480	Tokenize the text
384	to reduce memory usage
1120	function to rename the columns
1124	predict oof val
1190	Event code distribution
684	Creating a DataFrame for the count of each label
994	What is the most common client type where Contract was approved
339	create test generator
702	ADD PSEUDO LABELED DATA
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1436	Number of Patients and Images in Training Images Folder
1810	Importing my classifiers and scoring metrics
685	Creating a DataFrame out of the original labels
1554	Train model on selected parameters and score
1055	Calculate metrics dictionary
159	split connected objects into separate masks
1260	Combine the filename column with the variable column
1174	Named Color visualization
1041	Clean up memory
1290	Squeeze and Excitation block
533	Run Grid Search
1632	Seting the df according to the province
1068	Print CV scores , as well as score on the test test set
1606	I will use the calc features instead of the individual features
766	Draw a legend and annotate each
285	Defining the target variable
1726	for numerical features
320	Gradient Boosting model
1518	Get raw training data
127	Get the categorical variables
1243	to truncate it
1106	Load metadata file
806	Convert hyperparameters to int
708	ADD PSEUDO LABELED DATA
791	Train and predict
1381	split the dataset in train and test set
690	obtain one batch of training images
576	some config values
1134	Stacking the validation masks and predictions
256	Accuracy of VotingReg
122	Featurize special chars
1090	Encodes predicted masks based on best threshold
703	STRATIFIED K FOLD
1239	Run the model
379	Print RMSE for validation set
348	highlight columns with correlations
1762	missing data in feature matrix
1687	Mask if all the masks are the same
473	Loading the data
1013	Clean up memory
1038	Previous applications categorical features
1793	Web Traffic Months cross days
1223	simple xgboost
698	Applying CRF seems to have smoothed their prediction
931	Calculates number of combinations
1014	Clean up memory
1509	LIST DESTINATION PIXEL INDICES
1257	build new train and test files
1198	Create submission file
1169	Print dimensions of dataframe
1431	Save the data
629	Sort by day
290	Cleaning the data
911	Create unique categorical variable
514	Summary of Wins and Losse
1484	Read and process the examples
84	Class Distribution Over Entries
1324	Change namedtuple defaults
4	Remove Unused Columns
750	Combinations of TTA
1642	Most frequent IPs
1070	Convert to RGB
616	Add some random data
1036	Resume Majority of var
184	Brand name price
1192	fill the missing values with zero
971	Get score on random search and reset indexes
1019	Merge with Bureau data
863	Building model on train and predicting on test set
551	Run Grid Search
1507	Now do the training data iteration
935	sort by score
1416	Detect hardware , return appropriate distribution strategy
299	Read in image and resize
770	Bonus vs Target
1312	Bring in the validation set for each installation
1472	size and spacing
1774	Shuffling happens when splitting for kfolds
1607	one hot encode the categorical features
1061	Depth first conv layer
212	calculate average score for each label
1084	Read the image from the source image
300	create sub folder if missing
658	perform model on given parameters and calculate accuracy
1768	shuffling the data
252	Decision Tree Regression
1232	Load Train , Validation and Test data
1339	Final linear layer
764	Markerize the Squares
210	Impute any columns with values not in test set
468	Loading the data
819	Add the value of start to the dataframe
642	Preprocess neutral words
81	Pooling and final linear layer
712	ADD PSEUDO LABELED DATA
693	Using previous model
1816	Read the train and sub files
651	write a submission
454	Run the detection on the image
35	create an array of embeddings from train text
633	Running all models
656	COMBINE TRAIN AND TEST DATA
467	Precision and Recall
80	Passes through the embedded masks and merges them into a single convolutional layer
683	Creating a DataFrame out of the original labels
1267	Here we take the test data and generate sequences from the test data
837	Set legend handles
1023	one hot encoding
1464	Load test dataset
1059	Visualize the expected mask
301	move an image to sub folder
491	Pearson Correlation heatmap
1650	Clustering for Logit
587	Bathroom Count Vs Log Error
1112	extract different column types
635	Deterministic model
1419	Import libraries and data
981	Replace day outliers
1182	return the prediction rate
1356	meter split based
23	Detect and Correct Outliers
1818	CHECK FOR EACH CATEGORY VARIABLE
102	grid mask augmentation
1665	fill in mean for floats
383	Setting up the environment
1009	remove variables with values not present in test set
557	Number of times each feature is interacted
665	Aggregate by month
662	First , we need to agg first
1233	Converting data into Tensordata for TPU processing
923	Cumulative importance plot
1053	get score on best score
549	Classification Report
131	Prepare Testing Data
755	Slicing the mapping values
1201	Detect hardware , return appropriate distribution strategy
1302	Load the model into the TFA
1733	Importing the datasets
1734	Fill in missing values
146	Number of different values
1103	Import the datasets
1532	Choose and initialize a model
809	Train and predict
1477	batch to avoid memory overhead
895	Find all features with zero importance
1696	Here I write a helper function to evaluate the images
1741	HANDLE MISSING VALUES
861	Split into training and testing data
1797	Plot rolling statistics
1006	Remove low features from train and testing data
1761	Label encode categoricals
1095	make predictions on test set
653	Read the data
1081	Train the model
1513	Order does not matter since we will be shuffling the data anyway
603	Binary representation of labels
1666	StackNetClassifier with GPU
811	Create a file and open a connection
913	Joining parent and child df
2	Add new Features
479	From Strings to Vectors
1325	Calculate and round number of filters based on depth multiplier
117	Preparing the submit data
1789	Forceasting with decompasable model
130	Look at how data generator augment the data
1369	Draw the centroids of the districts
157	Print final result
815	Optimal hyperparameters are
581	ceil and divide by the number of steps
1102	Preparing final file
1177	Setting the color used for visualization
1465	Define dataset and model
1293	Load dataset info
1344	Solve some tasks
1743	EXTRACT DEVELOPTMENT TEST
342	Save predictions to file
611	if save to dir
394	Decision Tree Classifier
767	drop columns with correlations
10	Merge Weather Data
1535	Loading the required datasets
1698	test if all the images are empty
726	PhotoId column contains names of photos
1631	Load the data and see some stats
599	Load an image
219	Import Libraries and Data
1672	Initialize patient entry into parsed
720	Prepare for data analysis
1489	Read candidates from csv file
896	There might be a more efficient method to accomplish this
169	Batch normalization and building model
1813	summarize history for loss
463	Data processing , metrics and modeling
456	Load libraries and data
1428	Add PAD to each sequence
889	align the rows of train and test data
33	prophet expects folllwing label names
795	delete the hyperparameters from the hyp dict
941	Evaluate the objective function
630	Numba model to be continued
1405	rolling mean for each store
356	Read an image from image id
1135	Load the timestamps
1460	checking missing data
739	Compute the ratio of x and y
1035	Create unique categorical variable
676	Store the data for further analysis
893	Get missing values
409	Features generated from TfidfVectorizer
1066	First dense layer
1151	Load train set shapes
1024	check for encoding
385	check test files
148	Number of click by IP
531	Classification Report
1008	Correlations of the Target Variable
1748	Creating an entity from dataframe
1520	LIST DESTINATION PIXEL INDICES
1300	Process text for RNNs
1610	Logcut the target variable
1294	warm up model
905	Calculate metrics dictionary
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
359	Segm RGB image
1288	Load dataset info
1368	Display the map
399	Calculate the confusion matrix
546	Standardize the classifiers
1279	Identify by both
1758	Add Relationship to es
862	Standard deviation of best score
48	Normalize colors based on the value of the column
483	Define the model
132	Create Testing Generator
763	current y value
621	Calculate the areas of each contour
7	declare target , categorical and numeric columns
1468	Max and min of a feature
1488	Eval data available for a single example
921	Calculate the normalized importance values
1062	Here are the main phases of the encoder
1163	Order does not matter since we will be shuffling the data anyway
596	Downsize to one image and optimize
965	reset index and reset color
1398	Apply time blockation on series
1200	Print RMSE
60	Generate Test Predictions
1478	LIST DESTINATION PIXEL INDICES
537	Run Grid Search
1348	Fast data loading
1678	convert text into datetime
1238	Create a session and run
450	Create data set for model training
1146	load mapping dictionaries
966	Title and Label by Target Value
177	Most common level
646	For each type how many samples do we have
376	Mean absolute error
178	Calculate mean price for each category
1651	Clustering for Logit
1379	Watershed out image if border is present or not
578	Calculate spectrogram using pytorch
236	Filter Spain , run the Linear Regression workflow
1310	Get feature importances
1390	Use train dataset
344	create a generator that iterate over the Sequence
329	read in the images
1047	load cash data
1162	Order does not matter since we will be shuffling the data anyway
249	Accuracy of the model
1142	Feature importance with shapap
1099	predict and take mean
438	We will look at the dimensions of our train and test data
668	Load Train and Test Data
1702	Evaluating candidates
1323	Parameters for an individual model block
1515	Uncomment line below to run
515	Calculate the distribution of team preferences
1319	Read the target and the input data
1551	Average values for all months
405	calculate the log loss
1085	resize the image to original image size
1382	Stemming and Lemmatization
1058	Initialize and load the model
97	Befine the beforeM and afterM
21	Check for missing values in training set
845	Fill NaN values with mean
1189	Start generate data sets
369	Create Train and Valid Sets
328	read in the images
1033	parent ids , var
1064	Generate data based on the type
1111	Extract processed data and format them as DFs
697	Precision helper function
1042	Sort the table by percentage of missing descending
1578	replace inf values with
555	Defining the column types
968	Remove low features
1443	Square for both Train and Test Arrays
945	iteration score 两列
1455	inverse transform for regression
648	Load Train and Test Data
776	PairGrid between Target and Target Columns
389	Check for empty images
1752	Creating an entity from dataframe
1072	Draw the attribute on the image
991	EDA and Feature Engineering
1226	Plot some random images to check how cleaning works
134	Initializing a CatBoostClassifier
288	sklearn is only imported for splitting
1817	Lidar Data Preparation
706	MODEL AND MODEL WITH QDA
855	separate train and validation sets
1498	Decide the layers of the model
1067	split training and validation data
1027	get score on best score
1107	Load sentiment file
1615	show mask class example
436	Preview of Train and Test Data
517	Get the seeds as integers
1653	Exploratory Data Analysis
1716	FUNCTIONS TAKEN FROM
455	Draw bounding boxes on image
823	Custom Cutout augmentation with handling of bounding boxes
1050	check for encoding
147	Number of click by IP
962	Featurize feature sets
624	Groping the Ialy dataset
1492	Predict on valid set
1221	find best score
1219	Creating new features based on train labels
735	Classify an image with different models
191	Model Description Length Vs Price
1475	MAKE MIXUP IMAGE
995	most Common Client Type
1337	Update block input and output filters based on depth multiplier
1506	FIND ORIGIN PIXEL VALUES
95	For getting the indices of the selected columns
927	the score , parameters and iteration
1236	Draw the text boxes with padding from the display string
1794	Train the model with zero predictions
478	It was decided to use HashingVectorizer
276	sort the validation data
788	Draw the threshold lines
1725	The method for training is borrowed from
982	Converting columns into timedelta object
639	Segregating the sentiment data
1184	set the variables to zero
802	Creating the targets and the confidence columns
160	Looking at Nuclei cells
377	RMSEs and RMSEs
1391	Draw bounding boxes on the image
166	Analyze number of masks and create dataframe
105	load the data
1808	How many transactions are there in the data
773	Correlation between all the heads
1321	configurations for training
76	load and shuffle filenames
841	uclidean Distance by Fare Amount
52	Normalize colors based on the browser used
1017	Sort the table by percentage of missing descending
1056	Splitting data into train and validation sets
597	load mask list
263	We will need some functions to calculate the logreg coefficient
1627	Plot country of interest
612	Listen to the test data
306	Combining all the datasets into one DataFrame
1389	StratifiedKFold On Labels
1688	Check if all the masks are the same
529	Run Grid Search
1001	Returns the Most recent value of a column
1108	Load image file
1641	Most frequent IPs
704	MODEL AND PREDICT WITH QDA
1125	Cast the output from the decoder
1712	Since the labels are textual , so we encode them categorically
280	use kmeans to cluster the labels
1783	Word cloud of First Topic
39	Get the next batch
1363	Some functions to change europe
74	cosine learning rate annealing
1423	function for transforming raw sentence to a list of words
203	For every slice we determine the largest solid structure
1063	We will use the most basic of all of them
464	Merge train and test dataframe
1002	Custom Features and Target Features
524	Standardize the classifiers
1561	Macro columns to a list
199	inpaint with original image and threshold image
1086	Check if the training set is ready
1776	Importing important libraries
503	scale pixel values to grayscale
1435	take a look of .dcm extension
99	intialize the model
669	Load all the data
267	Some Player College Names have missing values
894	Lighting the feature importances
237	Filter Spain , run the Linear Regression workflow
1596	Train vs Test Data
1230	Load the model into the TFA
141	histogram of Fraud vs
825	Set to instance variables to use this later
846	baseline train and validation predictions
351	Find Best Feature
977	Converting labels into numeric values
1274	Now check the unique colors in the true image
1069	Write the prediction to file for submission
366	Linear SVR model
1657	Sentiment Columns
1193	Maybe it is still somehow related to that day
50	Distribution of the variable count by hour
1170	Split the data back into the original data
1412	Number of unique classes
470	Merge train and test dataframe
836	Zooming nyc map zoom
145	Creating a dataframe using the csv data
190	Distribution of the description length
402	Month , Year , Day
333	read in the test images
519	Preparing the training and testing data
580	Logmel feature extractor
1355	iterate through all the columns of a dataframe and modify the data type
1181	conf mtx and conf mtx
1775	This enables operations which are only applied during training like dropout
736	Remove zero features
1361	Downsize of dataframe
1456	Number of Rooms and prices
1394	With Masks
265	Scaling the features
715	Remove the Outliers
20	Imputations and Data Transformation
426	Distribution after log transformation
1500	Print directory info
187	No Descriptions Section
380	unpack an item offset and length
1459	checking missing data
1662	Display some images
1574	Setting the Paths
1458	checking missing data
876	iteration score 两列
211	label encode categoricals
446	Encoding the Regions
865	Create hyperparameters with the values selected above
1022	remove columns found by this kernel
294	patch predictions after each epoch
225	Scatter plot of centers
1681	the accurace is the all time wins divided by the all time attempts
1483	Some MODELS ENCODE LIST
1040	Merge with previous counts
1129	Create train and test paths
292	Load and predict
1244	Load the data
220	highlight columns with correlations
26	visualization of Target values
1421	lets convert comment texts to lower case
90	fast less accurate
989	same installments as above
336	batch size for training and validation
482	We used the Functional API to create a model
920	Load data from csv file
492	Calculate Correlation Matrix
1057	We have to just move the mask into memory
161	make a mask based on the label
1773	for numerical features
1150	Resize cropped image to original image size
477	Vectorize the data using TfidfVectorizer
1437	Number of Patients and Images in Training Images Folder
258	Building Ridge model
1770	missing entries in the embedding are set using np.random.normal
930	Create random predictions
1203	List of Fake Images
1448	Scale Scaled Pinball
173	Find all unique calsses values
24	Detect and Correct Outliers
1738	Import and Read Data
1444	Square for both Train and Test Arrays
206	CONVERT DEGREES TO RADIANS
1159	LIST DESTINATION PIXEL INDICES
543	Classification Report
993	get interesting features
189	Shortest and longest coms
223	convert TFIDF to int
754	Distribution for target
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
1479	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
813	Which we can plot up ..
1360	Leak Data loading and concat
1187	Get the test set or create a new set
582	Calculates batch probabilities
98	Function for computing distance between sets
126	Prepare Categorical Features
1560	Heatmap for macro features
938	Write column names
82	make submission for model
779	assume age is in centimeters
112	Concatenate the three data frames into one DataFrame
998	Calculate the normalized mode values
488	function to get the categorical and numerical features
1074	Distribution of our application features
985	Example of balanced sales over time
262	Showing parameters and LB score visualization
1731	Read and write
18	impute missing values
909	parent ids , var
1295	Prepare Traning Data
92	A utility function to load pickle file
556	create lgbm feature list
1114	Remove missing target column from test
1332	Depthwise convolution phase
1343	Number of SNPEs with SNAP
844	Logistic Regression model
233	Create date columns
1007	Resume Majority of var
1171	Save images to a GIF file
1229	Converting data into Tensordata for TPU processing
584	Calculate row id with highest probability
321	We now have something we can pass to our model
1132	Make a prediction , then take the average
180	zoom on the second level of categories
1096	generate a submission
1138	SHAP interaction values
1576	checking missing data
1365	Download the Kaggle Data
1626	Optimize by Province
660	Computes and stores the average and current value
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
281	Folders of the dataset
1429	make train features
1087	Replace coverage class with confidence threshold
260	transforming to integers
85	fake data used for evaluation
1222	reapply grid to our parameters
949	Visualizing Learning Rate
1723	missing entries in the embedding are set using np.random.normal
1722	The mean of the two is used as the final embedding matrix
936	define a model with random search parameters
947	SETUP SOME COLUMNS
1415	Number of labels for each instance
1745	Some utility function to help out
1299	Embedding the data
790	Run model on full training set and predict on test set
124	pct change of group by columns
925	Fitting and predicting on train and test set
1695	Plot the tasks
1235	Draw the image on the image boundaries
51	Add columns to make calculations easier
1029	Calculate metrics dictionary
1211	so we have to pad the images
1694	Plot images and their prediction
1092	Applying CRF seems to have smoothed their prediction
1422	Deep copy the sentences
273	get lead and lags features
149	show some stats
694	remove activation layer and use my loss function
197	blackhat image processing
695	remove activation layer and use losvasz loss
8	merge with building info
1541	Argmax of the train and actual
154	Identify missing values
367	SGD model
47	Create features based on month
1011	Create dummy variables for categorical features
419	Glimpse of Data
1156	watch out for overfitting
996	Seed features and their distributions
1205	Load and Preparation
848	Create random Forest Object using the mentioned parameters
1476	MAKE CUTMIX LABEL
940	sort by score
498	read in header and get dimensions
1255	Load Model into TPU
713	STRATIFIED K FOLD
1	Resize image to desired size
1215	Only load those columns in order to save space
544	Building the Tourney Data
796	parameter value is copied from
1604	Check for Null values
1449	create the dataset that we need to predict
444	Latitude and Longitude
607	Return a normalized weight vector for the contributions of each class
1352	Leak Data loading and concat
561	just to check if it works as intended
1629	creating a list of all regions of the country
1668	Stores with missing sales
424	Meter reading by meter category
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
608	Reads absolute path and returns classes , filenames
1287	Bring some samples
1386	image scale
898	Selecting features with zero importance
170	Second component of main path
1078	Set values for various parameters
1471	Order does not matter since we will be shuffling the data anyway
67	split into train and validation filenames
1241	Load an image from image id
664	Average number of bookings over time
1452	Exploring the target variable
224	convert TFIDF to int
12	This block is SPPED UP
929	find parameters between 0.005 and 0.0
768	FACBuildings , walls , targets
1136	PUBLIC TEST AND PRIVATE TEST
185	Categories of items with a price of
1609	XGBOOST Sparse Feature Storage
1268	Linear Weighted Kappa
1540	check the train data
1756	create Relationship records for application
16	To plot pretty figures
933	sort by score
411	OneVsRestClassifier on multilabel features
1133	Stacking the val masks and masks
1043	Print some summary information
125	Load the base data
1291	Squeeze and Excitation block
144	get the data fields ready for stacking
897	Cumulative importance plot
183	Bands by brand name and number of item
950	Iterate over random hyperparameters
15	Common data processors
140	Make PyTorch deterministic
1750	Creating an entity from dataframe
572	Order distribution with respect to price
1175	Add the actor to the renace
69	add trailing channel dimension
29	Load train and test data
152	Categories of Clickers Download the app
530	Training and predicting
1510	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
332	read in the images
490	Distribution of values in application train set
1137	Joining with DateAvSigVersion
181	Price of the first level categories
1016	Bureau balance by loan
1802	Correlations between images
679	Split into training and test labels
382	Remove the noise
1744	FITTING THE MODEL
899	one hot encoding
1711	Read data from the CSV file
785	Calculate the normalized importance values
974	DataFrame for each hyperparameter
1732	Samples which have unique values are real the others are fake
100	code takesn from
486	Importing the Keras libraries and packages for LSTM
1647	Create a notebook in tqdm mode
604	Data Loading and Data explanation
908	Remove variables with too many missing values
227	Scatter plot of centers
1005	We will use the same metrics in our test set
838	Plot the binary features against dropoff location
1225	Make a picture format from flat vector
469	Data processing , metrics and modeling
1539	creating testing series
6	eliminate bad rows
1297	Load the test images
1644	Clicks with respect to time
1686	Get the pixel values for each color
453	Draw the image on the image boundaries
1655	Draw the heatmap using seaborn
1630	Investigation of Province and Province
1814	Copy predictions to submission file
692	Using previous model
1614	Split the train dataset into development and valid based on time
417	Preview of Building and Weather Data
1206	Define the densenet layer
1493	if do_valid is true
315	Accuracy of the model
1514	size and spacing
270	lgb weight distribution
1409	Set the parameters for the model
340	Making predictions dataframe
1246	Training History Show
1613	Split the train dataset into development and valid based on time
1778	Import Train csv file
733	Find an image and return key and descriptor
56	Modeling with Fastai Library
509	write a function to evaluate the threshold
1440	Preparing the train images
1674	Add boxes with random color if present
961	dfs to get a matrix of features
957	Relationship the new features to the main dataframe
523	Add Confidence Stadium
1122	create a baseline model with all features
1012	Add the column names
979	get the categorical variables
1663	kick off the animation
1534	This code is copied from here
1643	Calculate proportion of clicks and downloads by device
569	Hours of Order
1408	Create predictions on test data
1403	we need to predict at end
1127	Initialize the datasets
193	blackhat image processing
440	Top most commmon Paths
1358	iterate through all the columns of a dataframe and modify the data type
1046	Join the grouped data with the main dataframe
1593	fill up the missing values
1718	Tokenize the sentences
741	Define the local deform line
512	Season and Weekings
583	Sum the probabilities of each model in the batch
655	There are some missing values in the dataframe
1684	Load the training data
1533	Mean ROC Curve
9	fill test weather data
89	Check that the model has not overfit
475	Now we can build the vectorized on the given text
167	Only the classes that are true for each sample will be filled in
1608	Parameters for xgboost
888	Merge Previous Features
1790	import libraries for our model
1113	Subset text features
368	Decision Tree Regression
1224	select proper model parameters
956	EDA and Feature Engineering
857	Distribution of Validation Fares
476	the times in the text
135	Checking for Class Imbalance
1618	average the predictions from different folds
347	Import Libraries and Data
775	Draw the heatmap using seaborn
1635	Splitting the data
311	Target Exploration
427	first column only
718	Preprocess classifiers
401	obtain one batch of training images
988	Data is balanced on average cash accounts
568	Import the Data
622	Examples for usage and understanding
765	Marker for each image
420	Distribution of meter type
1270	Detect duplicate images
677	filtering outliers
528	Standardize the classifiers
1543	check the score on the train set
338	plot baseline and validation losses
1402	Extract first n exogen data
593	factorize categorical features
1263	Using original generator
1640	Load the Data
859	run randomized search
38	observation id , timestamp
110	Plotting sales accross each year
1131	create the dataloaders with the previous dataset
828	Read the image from image id
1164	Run this cell again for next set of images
688	draw box over image
808	Split up with indices
1317	configurations for training
1208	define parameters for model training
1282	get train and test dataframes
1388	Store all bounding boxes in a single image
58	you can play around
1301	load test data
1144	Growth Rate Percentage for each state
1806	Reading the train and test data
106	load the data
882	Create a LightGBM model and find appropriate parameters
1474	Cutmix on batches
1105	load mapping dictionaries
151	What is the most frequent category in the data
137	Clear output after each epoch
613	Load data and do data analysis
850	Get the data type
1021	Store the column name in a list
1757	recuring features based on application balance
760	Histogram of heads only
552	Training and predicting
1220	Drop nuisance columns and fill missing values
349	highlight the value with a threshold
1490	Read candidates with real multiple processes
1054	Clean up memory
1418	Load the needed libraries
1503	of the image to the TPU
1548	Add new Features
640	MosT common positive words
222	LinearSVR model on selected features
350	LinearSVR model on selected features
632	Load the population data
787	Cumulative importance plot
71	create numpy batch
542	Training and predicting
129	See sample image
510	process remaining batch
425	Distribution of the Target Variable
1673	Add box if opacity is present
638	Lets generate a word cloud image
1715	Ensure determinism in the results
1737	The competition metric relies only on the order of recods ignoring IDs
232	Double check that there are no informed confirmed cases and Fatalities after
661	get different test sets and process each
459	Run random search
769	find out the roof option
538	Training and predicting
1697	Helper function to convert a list of images into a list
980	storing the actual values in a dictionary
179	What is the mean price by category distribution
952	Preparing the data
1550	Calculate the average week of each year
1682	An optimizer for rounding thresholds
1557	Creation of the External Marker
275	Dropout parameters
34	Loading Train and Test Data
1378	Get random labels
229	Ensemble final class values
78	save dictionary as csv file
1148	the size of the image
1791	Create year , month and day columns
11	Compute the STA and the LTA
388	creating a histogram
641	For negative words
72	define iou or jaccard loss function
313	so we label encode categoricals
406	Exploring the variables
955	EDA and Feature Engineering
1076	Average length of comment
341	change column names
601	And there you have it
1649	Calculate extra features based on timeseries
295	Binary Target feature
1248	Load and preprocess data
566	Keras is only imported for feature extraction
1075	Getting the predictions
46	Distribution of the official variables in the dataset
1384	create numpy batch
234	Filter selected features
1553	Average of all day weeks over trip duration
1499	restore the latest checkpoint
727	Most frequent category
887	Most of the features are bureau and balance features
3	Reset Index for Fast Update
421	Distribution of meter reading over weekdays
310	Setting X and y
1004	Featurize feature sets
1705	Find the program that has the largest number of candidates
932	Evaluate the model
458	Parameters to use
1634	Difference between H1_ and D1_Columns
1624	Exploring the data
428	all other columns
1372	Creating the submission file
172	save model as json file
883	Fit the model
111	month by day
1803	Correlation between images
1347	iterate through all the columns of a dataframe and modify the data type
1393	Transforming the categorical features into cateogry
1380	Creating the xy values
707	QDA with CV
1020	List to keep track of columns to remove
1710	Keras Libraries for Neural Network
121	Checking the current coverage
271	Partial imports
987	Previous Loan Amounts
591	Combination of data augmentation and other
1467	realign the missing values to be
554	Add RUC metric
1591	load best weights and evaluate
548	Training and predicting
1623	Logistic Regression Logistic Regression
1544	inplace input data
842	Correlation with Fare Amount
675	Run a grid search
1636	BanglaLekha Click Time
434	Exploring the data
1272	Check for X and Y pairs
1754	create application and installment
158	The function to change the image shape
327	Add box if opacity is present
1298	make a submission
545	Select Percentile
322	Accuracy of VotingReg
805	Add subsample parameters
103	X and y are the important columns
1713	Building the model
758	Which are not equal in terms of households
1482	set the current working directory
1788	peaks in frequency value
590	Gaussian target noise
61	Get sex of the patients
902	Train and predict
77	retrieve x , y , height and width
560	Plot Gain importances
1664	Import stacknet
864	Fitting and predicting on train and test set
964	Looking at the feature matrix from train
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
860	Training and Evaluating the Model
732	Find an image and return key and descriptor
1585	fill all na as
721	Histogram of nominal variables
1031	Suppress warnings due to deprecation of methods used
88	create list of models and their weights
414	Charts with seaborn
63	Distribution of continuous variables
501	show the graphs
312	I will keep only the categorical columns in the train set
1254	Run predictions on test set
293	Extract the ID from file names
1140	Plot the dependence of the game
1264	Load and predict
1285	load the members of the model
14	visualization of Target values
119	Hover on the map
25	Load train and test data
866	choice of boosting type
1149	Load preprocessed data
96	Save before and after normalization
1166	create test dataset
1579	colunms new features
1621	What is the most interesting aspect of the data
1811	Plot the actual values vs predicted values
1612	Split the train dataset into development and valid based on time
42	Distribution of the official variables in the dataset
214	separate train and validation sets
1772	always call this before training for deterministic results
1562	parameters for LGBM model
892	Find the columns with more than
1333	Squeeze and Excitation layer , if desired
1172	Read the image from the kernel
1094	calculate the dimensions of the image
801	Creating the targets and the confidence columns
1079	vocaublary , add its feature vector to the model
973	we are going to use random
1779	Generate the Mask for EAP
1404	we can see there is no change in the train set
1491	Returns a dictionary of counts per example
314	Drop nuisance columns
1555	Make sure to apply mean encoding only to training set
851	Calculate elapsed time in each frame
672	List of models to be engineered
485	Now through the second convolutional layer
671	Joining all the columns together
139	mmcv is a very nice library for working
93	Set the values to what we had at the beginning
830	Lets start with the surface
307	Tokenize the sentences
171	Passes through the first convolutional layer
700	MODEL AND PREDICT WITH QDA
1303	Delete to reduce memory usage
1462	Create dataset for training and Validation
387	Load item from train.bson file
1801	k is camera instrinsic matrix
441	Latitude and Longitude
142	get the data fields ready for stacking
804	Get subsample and drop rate parameters
565	Show feature prediction
1685	Load the training data
1228	Load Train , Validation and Test data
218	MinMax scale all floats
816	Getting the prediction values in the correct format
1559	Applying all the methods
511	WINS and Los Angeles
1512	size and spacing
1185	update user samples
493	Applicatoin train data after merge
1798	shift train predictions for plotting
879	Iterate through parameters and their associated weights
1611	Specify the categorical features
553	Classification Report
1670	Fixing random state
297	Spliting the training and validation sets
910	Unique values of each column
1195	Make a prediction
115	Defining the parameter grid for the model
1044	drop missing columns
412	matplotlib and seaborn for plotting
944	DataFrame for each hyperparameter
843	separate train and validation sets
1566	same as above
374	Avoid division by zero by setting zero values to tiny float
520	Train the estimator
1470	scale the values
803	Understanding Confusion by Target
1377	Function to plot the augmented images
113	Join the month and store ids with the actual values
1753	Add Bureau Relationship
1161	FIND ORIGIN PIXEL VALUES
777	Draw the feature plots
783	Import all libraries we need
445	Extracting informations from street features
1073	Distribution of Target and its siblings
586	bedroom County Vs Log Error
251	SGD model
19	Imputations and Data Transformation
1764	Copy feature matrix enc
872	Create a file and open a connection
571	Hour of the Day when the Reorder occurs
1784	Compute the STA and the LTA
833	Distribution of Fare
45	Quad of the dataset
1331	Loads pretrained weights , and downloads if loading for the first time
408	Lets generate the wordcloud associated with each tag
403	We are gon na use CatBoostRegressor
257	Predict and Submit
44	Normalize each color by its max and min
1590	Load the data
94	Make a function to create a subset of values
1152	create the validation set
244	Filter Andorra , run the Linear Regression workflow
807	array to store the features and their scores
1645	Compute the correlation matrix
1538	actual is lagged here
317	SGD model
1433	Plot the heatmap
443	Latitude and Longitude
723	Sort ordinal features
221	highlight the value with a threshold
990	Amount loaned relative to salary
534	Training and predicting
1088	Remove padding from images
1661	Read in OOF Files
1396	Sort by date
365	Accuracy of the model
734	Classify image and return top matches
1256	create train , test and train
1527	Computes official answer key from raw logits
465	Bayesian Train Test Split
1413	attribute id , attribute name
722	Sort ordinal features
1691	lifting function for each atom type
681	Creating a dataframe for unicode and str labels
228	Ensemble final class values
396	Calculate the confusion matrix
168	initialize exp avg sqm
598	load list of images without ship
118	Pulmonary Condition by Sex
1180	assign the new conf to the training set
1537	Breakdown Topic Features
452	Lets import some libraries first
1637	do cumulative count
1338	The first block needs to take care of stride and filter size increase
822	Read the image from image id
1249	parse trials from oracle
255	We now have something we can pass to our model
1508	Now do the training data iteration
729	calculate coefficient of variation for each image
1362	Converting to Total Days , Month , Year
1805	Print current memory usage
917	Amount loaned relative to salary
725	Model and Model Evaluation
771	calculate households per capita
1032	Remove variables with too many missing values
1155	Create strategy from tpu
278	Word Cloud visualization
1700	Evaluating the fitness functions
1552	Calculate the average day of the year
1495	set the current working directory
1093	salt parameters are from the above mentioned tutorial
792	Merge with predictions
527	Classification Report
1809	Columns to be consolidated
43	Add new features
5	Encode Categorical Data
241	Filter Germany , run the Linear Regression workflow
1599	checking missing data
1374	Preparing X and y
323	Wrap the calculate iou value with the actual prediction
37	Lets take the natural log on each column
922	Plot normalized importance
829	Read the image from image id
696	Exclude background from the analysis
1708	Importing all libraries
1262	Load the image from image name
108	Sales volume per state
564	cast item description to int
253	Create Train and Valid Sets
1304	Delete to reduce memory usage
839	Set legend handles
659	Performing feature agglomeration
27	Histogram of the magic turtle
1080	Divide the result by the number of words
1622	Print the feature ranking
1597	checking missing data
1786	load a table
164	Reading the image
567	A simple Keras implementation that mimics that of
308	Padded Dataframes
1536	Check for Null values
259	transforming to integers
901	credits to for the parameters values
508	write a function to evaluate the threshold
1420	from googletrans import Translator
316	Linear SVR model
1349	Leak Data loading and concat
1763	Copy feature matrix enc
1729	Add train leak
1652	Import the Data
984	Days with Days
1165	Ready to start training
204	Remove other air pockets insided body
1116	Returns the counts of each type of rating that a rater made
1782	Calling our overwritten Count vectorizer
1524	Eval data available for a single example
413	OSIC Pydata Libraries
505	Joint plot for each user
1526	Default empty prediction
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
495	Read the data
562	Parameters from Tilii kernel
535	Classification Report
748	Load the trained weights
1065	Model Hyper Parameters
1516	Detect hardware , return appropriate distribution strategy
1167	Import the required libraries
1620	checking missing data
391	X test image
345	Fancing Generator
1529	Read candidates with real multiple processes
1237	Create a string image and convert to float
1143	Exploratory Data Analysis
1719	shuffling the data
65	save pneumonia location in dictionary
585	Building Year , Month , Number of Stories
1286	Here we check each image in the training set
1003	Featurize feature sets
1213	Create strategy from tpu
1441	Process Patient images
1617	remove activation layer and use losvasz loss
709	create QDA label slice
1028	Clean up memory
0	DICOM image ID
1258	so we have to pad the labels
606	Only the classes that are true for each sample will be filled in
620	Calculate the areas of each contour
1179	calculate the confusion matrix
1717	LOAD PROCESSED TRAINING DATA FROM DISK
1025	Hyperparameters search for LGBMClassifier
1316	make molecule index as
1037	Calculate the size of the data frame
1128	Calculate coverage class
636	Deterministic model
814	We will build a simple multilayer perceptron
924	Draw the threshold lines
370	Gradient Boosting model
849	Subset the feature importances
335	Looking some informations of our data
730	extract time calculation
592	Load the main train and test data
242	Filter Albania , run the Linear Regression workflow
216	MinMax scale all variables
1176	Save images to a GIF file
1018	Print some summary information
934	Fitting and predicting with grid search
1117	Compute QWK based on OOF train predictions
1669	gather input and output parts of the pattern
407	Convert each variable to its binary value
1104	Cred card balance
680	Creating list for each label
1194	Make a submission
890	Bivariate Linear Model
1335	Squeeze and Excitation
36	Log target feature
827	Read the image from image id
49	Distribution of Number of FVC in each Dataset
1569	Initialize Bayesian Optimization
600	Get the mask directory
1466	Run the model on the test set
248	Drop nuisance columns
1250	save best model
978	There might be a more efficient method to accomplish this
1366	Load the Data
1577	create continuous features
1276	get the inputs and targets of the task
724	Fillna with Simple Imputer
1247	to truncate it
404	initialize some empty arrays
570	Days of Orders
1501	Detect hardware , return appropriate distribution strategy
1730	Add leak to test
1240	Read in the submission files
518	Make a new dataframe with just the wins and losses
1158	size and spacing
615	An optimizer for rounding thresholds
1605	I will use the calc features instead of the individual features
526	Training and predicting
824	Applies the cutout on the given image
818	Add the column values in the test dataframe
714	Count the missing values in each column
1273	Count the number of objects in the image
390	Set some parameters
1511	Create the input and data augmentation layers
1796	Same for test stationarity
928	Get a random sample if you use goss
1281	from apex import amp
858	Show the correlation between predicted and actual validation
418	Preview of Building and Weather Data
1077	Word only remove stopwords
904	Clean up memory
40	load train and test data
1209	Save model and weights
942	sort by score
595	Create the model and train
854	Investigation of Fare Amount vs pickup fraction
539	Classification Report
460	Run random search
1760	Process categorical features
1357	Find Best Weight
975	iteration score 两列
1568	Pinball loss for multiple quantiles
474	Save submissions to file
53	Quad of the dataset
1703	Find the best candidates
1173	convert to HU
650	some config values
165	threshold the image to improve contrast
109	Plot the rolling average of price
1648	Generate real target and meta target variables
1740	Distribution of DBNOs
869	Create a file and open a connection
1766	cross validation and metrics
673	join the countries back onto main dataframe
1261	Create test generator
625	Sort by day
506	a function to plot boxplot between each feature and revenue
500	Separate the zone and subject id into a df
487	Reading all data into respective dataframes
331	read in the images
269	Merge Dense Players
75	create train and validation generators
1101	Training and Evaluating the Model
577	Get the sample
521	Merge the Teams with the Teams
1350	iterate through all the columns of a dataframe and modify the data type
959	Default Feature Names
1586	Correlation with Target
657	get train and test dataframes
1153	Let us now check the data
1188	Calculate average accuracy of each assessment
948	Boosting Type for Random Search
302	Read in image and resize
1461	Make a Baseline model
87	fake data sampling
740	add noise to y
853	Fare Amount by Day of Week
274	Setting the Paths
1217	Group by usage
415	Markdown is a very nice library by the way
358	skin like mask
912	Aparent function to create a parent and a child dataframe
1277	Identify by color
1549	Merge Weather Data
1454	inv means for each label
789	ignore all warnings
687	Get a sample from the dataset
674	Specify the training and validation sets
1558	Creation of the Watershed Marker
847	Evaluate without Standardization
1376	Deep Learning Libraries
101	load the image file using cv
747	Unfreeze backbone layers
1434	if unk is true and there is no previous word
1785	Avoid division by zero by setting zero values to tiny float
1439	Loading in the data
1494	create test features and create predictions
634	Calculate the maximum probability of each sir and seird
1351	Fast data loading
457	Loading important Libraries
319	Create Train and Valid Sets
442	Latitude and Longitude
1308	Ready frame
1523	oversampled training dataset
1322	Preparing the train and cv data
951	Load and Preparation
1671	Exploratory Data Analysis
1034	Unique values of each column
471	Plot ROC Curve
1271	check if all pairs are same or not
757	Create a bar chart
496	get the data
605	Pad the audio data to be divisible by the audio size
1747	Some new features
1769	SAVE DATASET TO DISK
304	Set class weights
1283	Drop unwanted columns
318	Decision Tree Regression
268	Find the columns where the distributions are very different
1531	Previous applications categorical features
357	get different image data types
133	Spliting the training and evaluation data
70	add trailing channel dimension
1311	Display current run and time used
153	Downsize by click
1126	Load the image and create a mask
1315	ATOMIC Numbering
881	Preparing the data
1118	Manually adjusted coefficients
1580	Multiply new columns by the train and test ones
195	inpaint with original image and threshold image
1242	Run the detector on the image string
637	About the data
1565	Data augmentation and manipulation
1628	Some place I should know ..
497	CreateDF for binary target
666	Here we merge all the products
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1603	Checking for missing values
753	Import Train and Test dataset
1820	Expand if missing
1588	predict labels and compute recall array
104	Compiling the model
619	Calculate the areas of each contour
437	Preview of Train and Test Data
798	Split up with indices
264	Prepare Training Data
731	Relationship between Volume id and pairplot
969	flip the feature matrix to train and test
1296	The params I use can be tweaked to your desire
136	save model to cbm
999	Returns the longest repetition from the elements
737	draw the image we picked up earlier
