0	import pandas as pd import numpy as np import matplotlib import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) import plotly . offline as py py . init_notebook_mode ( connected = True ) from plotly . offline import init_notebook_mode , iplot init_notebook_mode ( connected = True ) import plotly . graph_objs as go import plotly . offline as offline offline . init_notebook_mode ( ) import cufflinks as cf cf . go_offline ( )
1	train_labels = application_train_dummies [ 'TARGET' ] application_train_dummies , application_test_dummies = application_train_dummies . align ( application_test_dummies , join = 'inner' , axis = 1 ) application_train_dummies [ 'TARGET' ] = train_labels print ( 'Training Features shape: ' , application_train_dummies . shape ) print ( 'Testing Features shape: ' , application_test_dummies . shape )
2	application_train_dummies [ 'TARGET' ] = train_labels print ( 'Training Features shape: ' , application_train_dummies . shape ) print ( 'Testing Features shape: ' , application_test_dummies . shape )
3	from sklearn . impute import IterativeImputer from sklearn . ensemble import ExtraTreesRegressor from sklearn . linear_model import BayesianRidge import random
4	total = X_new . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( X_new . isnull ( ) . sum ( ) / X_new . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_application_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_application_train_data . head ( 20 )
5	X_new [ X_new . duplicated ( subset = columns_without_id , keep = False ) ] print ( 'The no of duplicates in the data:' , X_new [ X_new . duplicated ( subset = columns_without_id , keep = False ) ] . shape [ 0 ] )
6	import seaborn as sns color = sns . color_palette ( ) plt . figure ( figsize = ( 12 , 5 ) ) plt . title ( "Distribution of AMT_INCOME_TOTAL" ) ax = sns . distplot ( X_new [ "AMT_INCOME_TOTAL" ] )
7	from scipy . stats import boxcox from matplotlib import pyplot np . log ( application_train [ 'AMT_INCOME_TOTAL' ] ) . iplot ( kind = 'histogram' , bins = 100 , xTitle = 'log(INCOME_TOTAL)' , yTitle = 'Count corresponding to Incomes' , title = 'Distribution of log(AMT_INCOME_TOTAL)' )
8	import seaborn as sns color = sns . color_palette ( ) plt . figure ( figsize = ( 12 , 5 ) ) plt . title ( "Distribution of AMT_CREDIT" ) ax = sns . distplot ( application_train [ "AMT_CREDIT" ] )
9	original_train_data = pd . read_csv ( '../input/home-credit-default-risk/application_train.csv' ) contract_val = original_train_data [ 'NAME_CONTRACT_TYPE' ] . value_counts ( ) contract_df = pd . DataFrame ( { 'labels' : contract_val . index , 'values' : contract_val . values } ) contract_df . iplot ( kind = 'pie' , labels = 'labels' , values = 'values' , title = 'Types of Loan' )
10	original_train_data [ "NAME_TYPE_SUITE" ] . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Accompanying Person" , xTitle = 'People accompanying' , yTitle = 'Count' )
11	( original_train_data [ "DAYS_BIRTH" ] / - 365 ) . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Customer's Ages" , xTitle = 'Age of customer' , yTitle = 'Count' )
12	grp = bureau . groupby ( by = [ 'SK_ID_CURR' ] ) [ 'SK_ID_BUREAU' ] . count ( ) . reset_index ( ) . rename ( columns = { 'SK_ID_BUREAU' : 'BUREAU_LOAN_COUNT' } ) application_bureau = application_bureau . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau [ 'BUREAU_LOAN_COUNT' ] = application_bureau [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 ) application_bureau_test = application_bureau_test . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau_test [ 'BUREAU_LOAN_COUNT' ] = application_bureau_test [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 )
13	grp = bureau . groupby ( by = [ 'SK_ID_CURR' ] ) [ 'SK_ID_BUREAU' ] . count ( ) . reset_index ( ) . rename ( columns = { 'SK_ID_BUREAU' : 'BUREAU_LOAN_COUNT' } ) application_bureau = application_bureau . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau [ 'BUREAU_LOAN_COUNT' ] = application_bureau [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 ) application_bureau_test = application_bureau_test . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau_test [ 'BUREAU_LOAN_COUNT' ] = application_bureau_test [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 )
14	def isOneToOne ( df , col1 , col2 ) : first = df . drop_duplicates ( [ col1 , col2 ] ) . groupby ( col1 ) [ col2 ] . count ( ) . max ( ) second = df . drop_duplicates ( [ col1 , col2 ] ) . groupby ( col2 ) [ col1 ] . count ( ) . max ( ) return first + second == 2 isOneToOne ( previous_application , 'SK_ID_CURR' , 'SK_ID_PREV' )
15	grp . columns = prev_columns application_bureau_prev = application_bureau_prev . merge ( grp , on = [ 'SK_ID_CURR' ] , how = 'left' ) application_bureau_prev . update ( application_bureau_prev [ grp . columns ] . fillna ( 0 ) ) application_bureau_prev_test = application_bureau_prev_test . merge ( grp , on = [ 'SK_ID_CURR' ] , how = 'left' ) application_bureau_prev_test . update ( application_bureau_prev_test [ grp . columns ] . fillna ( 0 ) )
16	grp = pos_cash . drop ( 'SK_ID_PREV' , axis = 1 ) . groupby ( by = [ 'SK_ID_CURR' ] ) . mean ( ) . reset_index ( ) prev_columns = [ 'POS_' + column if column != 'SK_ID_CURR' else column for column in grp . columns ] grp . columns = prev_columns
17	pos_cash_categorical = pd . get_dummies ( pos_cash . select_dtypes ( 'object' ) ) pos_cash_categorical [ 'SK_ID_CURR' ] = pos_cash [ 'SK_ID_CURR' ] grp = pos_cash_categorical . groupby ( 'SK_ID_CURR' ) . mean ( ) . reset_index ( ) grp . columns = [ 'POS_' + column if column != 'SK_ID_CURR' else column for column in grp . columns ]
18	df_holdout = pd . read_csv ( '../input/home-credit-blending/holdout_res.csv' , index_col = 0 ) . clip ( 0 , None ) df_holdout . set_index ( 'SK_ID_CURR' , inplace = True , drop = True ) df_tst = pd . read_csv ( '../input/home-credit-blending/test_res.csv' , index_col = 0 ) . clip ( 0 , None ) df_tst . set_index ( 'SK_ID_CURR' , inplace = True , drop = True )
19	from sklearn . preprocessing import MinMaxScaler , StandardScaler , PolynomialFeatures def preprocessing ( X , degree ) : poly = PolynomialFeatures ( degree ) scaler = MinMaxScaler ( ) lin_scaler = StandardScaler ( ) poly_df = pd . DataFrame ( lin_scaler . fit_transform ( poly . fit_transform ( scaler . fit_transform ( X ) ) ) ) poly_df [ 'SK_ID_CURR' ] = X . index poly_df . set_index ( 'SK_ID_CURR' , inplace = True , drop = True ) return poly_df
20	pca_hold_test = pd . DataFrame ( pca . transform ( x_hold_test ) ) pca_hold_test . set_index ( x_hold_test . index , inplace = True ) ranks_pca = pd . concat ( [ ranked_hold_test , pca_hold_test ] , axis = 1 )
21	clf = LogisticRegression ( ) clf . fit ( ranks_pca_hold . values , y_hold . values . ravel ( ) ) ensemble_holdout = clf . predict_proba ( ranks_pca_hold . values ) [ : , 1 ]
22	sub_train = clf . predict_proba ( ranks_pca_test . values ) [ : , 1 ] sub_train = pd . DataFrame ( test_x . index ) sub_train [ 'TARGET' ] = ensemble_sub . values sub_train [ [ 'SK_ID_CURR' , 'TARGET' ] ] . to_csv ( 'sub_log_pca_rank.csv' , index = False )
23	clf = LogisticRegression ( ) clf . fit ( wf_hold . values , y_hold . values . ravel ( ) ) ensemble_holdout = clf . predict_proba ( wf_hold . values ) [ : , 1 ]
24	for train_idx , valid_idx in in_folds . split ( trn_x , trn_y ) : dtrain = lgb . Dataset ( data = trn_x [ train_idx ] , label = trn_y [ train_idx ] , free_raw_data = False , silent = True ) dvalid = lgb . Dataset ( data = trn_x [ valid_idx ] , label = trn_y [ valid_idx ] , free_raw_data = False , silent = True )
25	sub_train = model . predict ( ranks_pca_test . values ) sub_train = pd . DataFrame ( test_x . index ) sub_train [ 'TARGET' ] = ensemble_sub . values sub_train [ [ 'SK_ID_CURR' , 'TARGET' ] ] . to_csv ( 'sub_lgb_pca_rank.csv' , index = False )
26	model_a = kfold_lightgbm ( half_trn_x , half_trn_y ) model_b = kfold_lightgbm ( half_val_x , half_val_y ) model = kfold_lightgbm ( ranks_pca_hold . values , y_hold . values . ravel ( ) )
27	estimators_probas [ : , j * 3 ] = model_a . predict ( val_x ) estimators_probas [ : , j * 3 + 1 ] = model_b . predict ( val_x ) estimators_probas [ : , j * 3 + 2 ] = model . predict ( val_x )
28	test_probas [ : , j * 3 ] = model_a . predict ( ranks_pca_test ) test_probas [ : , j * 3 + 1 ] = model_b . predict ( ranks_pca_test ) test_probas [ : , j * 3 + 2 ] = model . predict ( ranks_pca_test )
29	sub_train = test_proba sub_train = pd . DataFrame ( test_x . index ) sub_train [ 'TARGET' ] = ensemble_sub . values sub_train [ [ 'SK_ID_CURR' , 'TARGET' ] ] . to_csv ( 'dragons.csv' , index = False )
30	from sklearn . preprocessing import StandardScaler x = X_all . values scaler = StandardScaler ( ) x_scaled = scaler . fit_transform ( x ) X_all = pd . DataFrame ( x_scaled ) . set_index ( X_all . index )
31	s = [ 0 ] * 200 m = [ 0 ] * 200 for i in range ( 200 ) : s [ i ] = np . std ( train [ 'var_' + str ( i ) ] ) m [ i ] = np . mean ( train [ 'var_' + str ( i ) ] )
32	def smooth ( x , st = 1 ) : for j in range ( st ) : x2 = np . ones ( len ( x ) ) * 0.1 for i in range ( len ( x ) - 2 ) : x2 [ i + 1 ] = 0.25 * x [ i ] + 0.5 * x [ i + 1 ] + 0.25 * x [ i + 2 ] x = x2 . copy ( ) return x
33	pr = 0.1 * np . ones ( ( 200 , res ) ) pr2 = pr . copy ( ) xr = np . zeros ( ( 200 , res ) ) xr2 = xr . copy ( ) ct2 = 0 for j in range ( 50 ) : if Picture : plt . figure ( figsize = ( 15 , 8 ) ) for v in range ( 4 ) : ct = 0
34	plt . subplot ( 2 , 4 , ct2 % 4 + 5 ) plt . plot ( xr [ v + 4 * j , : ] , pr2 [ v + 4 * j , : ] , '-' ) plt . title ( 'P( t=1 | var_' + str ( v + 4 * j ) + ' )' ) xx = plt . xlim ( )
35	with_holdout = isinstance ( holdout_x , pd . DataFrame ) if with_holdout : holdout_blend = pd . DataFrame ( holdout_x . index ) train_blend = pd . DataFrame ( train_x . index ) val_blend = pd . DataFrame ( train_x . index ) test_blend = pd . DataFrame ( test_x . index )
36	dataset_blend_train_j = np . zeros ( ( train_len , n_folds ) ) dataset_blend_test_j = np . zeros ( ( test_len , n_folds ) ) if with_holdout : dataset_blend_holdout_j = np . zeros ( ( holdout_x . shape [ 0 ] , n_folds ) )
37	for i , ( train , test ) in enumerate ( folds . split ( train_x , train_y ) ) : trn_x = train_x . iloc [ train , : ] trn_y = train_y . iloc [ train ] . values . ravel ( ) val_x = train_x . iloc [ test , : ] val_y = train_y . iloc [ test ] . values . ravel ( )
38	from sklearn . linear_model import Ridge import sklearn . linear_model def ridge ( trn_x , trn_y ) : clf = Ridge ( alpha = 20 , copy_X = True , fit_intercept = True , solver = 'auto' , max_iter = 10000 , normalize = False , random_state = 0 , tol = 0.0025 ) clf . fit ( trn_x , trn_y ) return clf
39	for train_idx , valid_idx in in_folds . split ( trn_x , trn_y ) : dtrain = lgb . Dataset ( data = trn_x . values [ train_idx ] , label = trn_y [ train_idx ] , free_raw_data = False , silent = True ) dvalid = lgb . Dataset ( data = trn_x . values [ valid_idx ] , label = trn_y [ valid_idx ] , free_raw_data = False , silent = True )
40	for n_fold , ( train_idx , valid_idx ) in enumerate ( folds . split ( trn_x , trn_y ) ) : dtrain = xgb . DMatrix ( trn_x . values [ train_idx ] , trn_y [ train_idx ] ) dvalid = xgb . DMatrix ( trn_x . values [ valid_idx ] , trn_y [ valid_idx ] )
41	for n_fold , ( train_idx , valid_idx ) in enumerate ( folds . split ( trn_x , trn_y ) ) : cat_X_train , cat_y_train = trn_x . values [ train_idx ] , trn_y [ train_idx ] cat_X_valid , cat_y_valid = trn_x . values [ valid_idx ] , trn_y [ valid_idx ]
42	y = df [ 'TARGET' ] X = df [ feats ] X = X . fillna ( X . mean ( ) ) . clip ( - 1e11 , 1e11 ) print ( "X shape: " , X . shape , " y shape:" , y . shape ) print ( "\nPreparing data..." ) training = y . notnull ( ) testing = y . isnull ( ) X_train = X . loc [ training , : ] X_test = X . loc [ testing , : ] y_train = y . loc [ training ]
43	df = pd . read_csv ( '../input/application_train.csv' , nrows = num_rows ) test_df = pd . read_csv ( '../input/application_test.csv' , nrows = num_rows ) print ( "Train samples: {}, test samples: {}" . format ( len ( df ) , len ( test_df ) ) ) df = df . append ( test_df ) . reset_index ( )
44	num_aggregations = { 'AMT_ANNUITY' : [ 'max' , 'mean' ] , 'AMT_APPLICATION' : [ 'max' , 'mean' ] , 'AMT_CREDIT' : [ 'max' , 'mean' ] , 'APP_CREDIT_PERC' : [ 'max' , 'mean' ] , 'AMT_DOWN_PAYMENT' : [ 'max' , 'mean' ] , 'AMT_GOODS_PRICE' : [ 'max' , 'mean' ] , 'HOUR_APPR_PROCESS_START' : [ 'max' , 'mean' ] , 'RATE_DOWN_PAYMENT' : [ 'max' , 'mean' ] , 'DAYS_DECISION' : [ 'max' , 'mean' ] , 'CNT_PAYMENT' : [ 'mean' , 'sum' ] , }
45	cat_aggregations = { } for cat in cat_cols : cat_aggregations [ cat ] = [ 'mean' ] prev_agg = prev . groupby ( 'SK_ID_CURR' ) . agg ( { ** num_aggregations , ** cat_aggregations } ) prev_agg . columns = pd . Index ( [ 'PREV_' + e [ 0 ] + "_" + e [ 1 ] . upper ( ) for e in prev_agg . columns . tolist ( ) ] )
46	pos_agg [ 'POS_COUNT' ] = pos . groupby ( 'SK_ID_CURR' ) . size ( ) del pos gc . collect ( ) return pos_agg
47	ins_agg [ 'INSTAL_COUNT' ] = ins . groupby ( 'SK_ID_CURR' ) . size ( ) del ins gc . collect ( ) return ins_agg
48	cc_agg [ 'CC_COUNT' ] = cc . groupby ( 'SK_ID_CURR' ) . size ( ) del cc gc . collect ( ) return cc_agg
49	data_pass = '/kaggle/input/m5-forecasting-accuracy/' sales = pd . read_csv ( data_pass + 'sales_train_validation.csv' ) calendar = pd . read_csv ( data_pass + 'calendar.csv' ) calendar = reduce_mem_usage ( calendar ) sell_prices = pd . read_csv ( data_pass + 'sell_prices.csv' ) sell_prices = reduce_mem_usage ( sell_prices )
50	dummies_list = [ sales . state_id , sales . store_id , sales . cat_id , sales . dept_id , sales . state_id + '_' + sales . cat_id , sales . state_id + '_' + sales . dept_id , sales . store_id + '_' + sales . cat_id , sales . store_id + '_' + sales . dept_id , sales . item_id , sales . state_id + '_' + sales . item_id , sales . id ]
51	W_df = pd . DataFrame ( W , index = roll_index , columns = [ 'w' ] ) data_pass = '/kaggle/input/original-weights/' W_original_df = pd . read_csv ( data_pass + 'weights_validation.csv' ) W_original_df = W_original_df . set_index ( W_df . index ) W_original_df [ 'Predicted' ] = W_df . w W_original_df [ 'diff' ] = W_original_df . Weight - W_original_df . Predicted m = W_original_df . Weight . values - W_df . w . values > 0.000001 W_original_df [ m ]
52	W_original_df = W_original_df . set_index ( W_df . index ) W_original_df [ 'Predicted' ] = W_df . w W_original_df [ 'diff' ] = W_original_df . Weight - W_original_df . Predicted
53	file_pass = '/kaggle/working/' sw_df = pd . read_pickle ( file_pass + 'sw_df.pkl' ) S = sw_df . s . values W = sw_df . w . values SW = sw_df . sw . values roll_mat_df = pd . read_pickle ( file_pass + 'roll_mat_df.pkl' ) roll_index = roll_mat_df . index roll_mat_csr = csr_matrix ( roll_mat_df . values ) del roll_mat_df
54	sw_df = pd . read_pickle ( file_pass + 'sw_df.pkl' ) S = sw_df . s . values W = sw_df . w . values SW = sw_df . sw . values
55	roll_mat_df = pd . read_pickle ( file_pass + 'roll_mat_df.pkl' ) roll_index = roll_mat_df . index roll_mat_csr = csr_matrix ( roll_mat_df . values ) del roll_mat_df
56	sub = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' ) sub = sub [ sub . id . str . endswith ( 'validation' ) ] sub . drop ( [ 'id' ] , axis = 1 , inplace = True ) DAYS_PRED = sub . shape [ 1 ] dayCols = [ "d_{}" . format ( i ) for i in range ( 1914 - DAYS_PRED , 1914 ) ] y_true = sales [ dayCols ]
57	def centroids ( X , lbls ) : centroids = np . zeros ( ( len ( np . unique ( lbls ) ) , 2 ) ) for l in np . unique ( lbls ) : mask = lbls == l centroids [ l ] = np . mean ( X [ mask ] , axis = 0 ) return centroids
58	def centroids ( X , lbls ) : centroids = np . zeros ( ( len ( np . unique ( lbls ) ) , 2 ) ) for l in np . unique ( lbls ) : mask = lbls == l centroids [ l ] = np . mean ( X [ mask ] , axis = 0 ) return centroids
59	db_centroids = centroids ( X , db . labels_ ) plt . scatter ( db_centroids [ cluster_mask ] [ : , 0 ] , db_centroids [ cluster_mask ] [ : , 1 ] , c = 'r' ) plt . scatter ( db_centroids [ ~ cluster_mask ] [ : , 0 ] , db_centroids [ ~ cluster_mask ] [ : , 1 ] , c = 'orange' )
60	def centroids ( X , lbls ) : cds_array = np . zeros ( ( len ( np . unique ( lbls ) ) , 2 ) ) for i , l in enumerate ( np . unique ( lbls ) ) : mask = lbls == l cds_array [ i ] = np . mean ( X [ mask ] , axis = 0 ) return cds_array
61	rng = np . max ( a , axis = 0 ) - np . min ( a , axis = 0 ) if rng [ 0 ] > rng [ 1 ] : a = a [ a [ : , 0 ] . argsort ( ) ] else : a = a [ a [ : , 1 ] . argsort ( ) ] return a [ - 1 ] - a [ 0 ]
62	dots_std_mask = groups [ 'group' ] . apply ( lambda x : x in grps [ std_mask ] ) fig , ax = plt . subplots ( figsize = ( 20 , 20 ) ) plt . scatter ( groups [ ~ dots_std_mask ] [ 'x' ] , groups [ ~ dots_std_mask ] [ 'y' ] , c = 'g' ) plt . scatter ( groups [ dots_std_mask ] [ 'x' ] , groups [ dots_std_mask ] [ 'y' ] , c = 'b' )
63	ts = ( ~ ( ts > 0 ) ) . astype ( int ) for i , val in enumerate ( ts ) : if val == 0 : continue else : ts [ i ] += ts [ i - 1 ] ts [ i - 1 ] = - 1 return ts def gap_counter ( ts ) :
64	itx = items . T . iloc [ : 5 ] . reset_index ( ) plt . figure ( ) ax = andrews_curves ( itx , class_column = 'index' , colormap = 'cubehelix' ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
65	plt . figure ( ) ax = autocorrelation_plot ( items . T . iloc [ 1 ] ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
66	plt . figure ( ) ax = lag_plot ( items . T . iloc [ 1 ] ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
67	norm1 = np . sort ( np . random . normal ( mu , sigma , 400000 ) ) norm2 = np . sort ( np . random . normal ( mu , sigma , 400000 ) ) norm3 = np . sort ( np . random . normal ( mu , sigma , 400000 ) )
68	fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 ) ) sns . distplot ( norm1 - norm2 , ax = ax [ 0 ] ) sns . distplot ( norm2 - norm3 , ax = ax [ 1 ] ) sns . distplot ( norm1 - norm3 , ax = ax [ 2 ] ) ax [ 2 ] . set_xlabel ( "norm1-norm2" ) ax [ 1 ] . set_xlabel ( "norm2-norm3" ) ax [ 0 ] . set_xlabel ( "norm1-norm3" )
69	train_file = '../input/train.csv' test_file = '../input/test.csv' train = pd . read_csv ( train_file , index_col = 'ID_code' ) X_test = pd . read_csv ( test_file , index_col = 'ID_code' )
70	s = [ 0 ] * 200 m = [ 0 ] * 200 for i in range ( 200 ) : s [ i ] = np . std ( train [ 'var_' + str ( i ) ] ) m [ i ] = np . mean ( train [ 'var_' + str ( i ) ] )
71	def smooth ( x , st = 1 ) : for j in range ( st ) : x2 = np . ones ( len ( x ) ) * 0.1 for i in range ( len ( x ) - 2 ) : x2 [ i + 1 ] = 0.25 * x [ i ] + 0.5 * x [ i + 1 ] + 0.25 * x [ i + 2 ] x = x2 . copy ( ) return x
72	pr = 0.1 * np . ones ( ( 200 , res ) ) pr2 = pr . copy ( ) xr = np . zeros ( ( 200 , res ) ) xr2 = xr . copy ( ) ct2 = 0 for j in tnrange ( 50 , desc = '1st loop' ) : if Picture : plt . figure ( figsize = ( 15 , 8 ) ) for v in tnrange ( 4 , desc = '1st loop' ) : ct = 0
73	plt . subplot ( 2 , 4 , ct2 % 4 + 5 ) plt . plot ( xr [ v + 4 * j , : ] , pr2 [ v + 4 * j , : ] , '-' ) plt . title ( 'P( t=1 | var_' + str ( v + 4 * j ) + ' )' ) xx = plt . xlim ( )
74	accum_add_prod = np . frompyfunc ( lambda x , y : int ( ( x + y ) * y ) , 2 , 1 ) sales_gaps [ : ] = accum_add_prod . accumulate ( df [ "gaps" ] , dtype = np . object ) . astype ( int ) sales_gaps [ sales_gaps < sales_gaps . shift ( - 1 ) ] = np . NaN sales_gaps = sales_gaps . fillna ( method = "bfill" ) . fillna ( method = 'ffill' ) s_list += [ sales_gaps ]
75	np . random . seed ( 19 ) depts = list ( grid_df . dept_id . unique ( ) ) prod_list = [ ] for d in depts : prod_by_dept = grid_df [ 'item_id' ] [ grid_df . dept_id == d ] . unique ( ) prod_list += list ( np . random . choice ( prod_by_dept , 5 ) ) m = grid_df . item_id . isin ( prod_list ) viz_df = grid_df [ m ] viz_df . head ( )
76	def convolves ( image_copy ) : for i in range ( 0 , 3 ) : image_copy [ : , : , i ] = convolve ( image_copy [ : , : , i ] , edgeexce ) image_copy [ : , : , i ] = convolve ( image_copy [ : , : , i ] , edge_kernel ) return image_copy
77	missing_df = pd . DataFrame ( data = lst , columns = [ 'Column_Name' , 'Missing_Values' ] ) fig = px . bar ( missing_df , x = 'Missing_Values' , y = 'Column_Name' , orientation = 'h' , text = 'Missing_Values' , title = 'Missing values in train dataset' ) fig . update_traces ( textposition = 'outside' ) fig . show ( )
78	missing = list ( test . isna ( ) . sum ( ) ) lst = [ ] i = 0 for col in test . columns : insert_lst = [ col , missing [ i ] ] lst . append ( insert_lst ) i += 1
79	missing_df = pd . DataFrame ( data = lst , columns = [ 'Column_Name' , 'Missing_Values' ] ) fig = px . bar ( missing_df , x = 'Missing_Values' , y = 'Column_Name' , orientation = 'h' , text = 'Missing_Values' , title = 'Missing values in test dataset' ) fig . update_traces ( textposition = 'outside' ) fig . show ( )
80	not_null_sex = train [ train [ 'sex' ] . notnull ( ) ] . reset_index ( drop = True ) nan_sex = train [ train [ 'sex' ] . isnull ( ) ] . reset_index ( drop = True )
81	def create_dist ( df , title ) : fig = plt . figure ( figsize = ( 15 , 6 ) ) x = df [ "age_approx" ] . value_counts ( normalize = True ) . to_frame ( ) x = x . reset_index ( ) ax = sns . barplot ( data = x , y = 'age_approx' , x = 'index' ) ax . set ( xlabel = 'Age' , ylabel = 'Percentage' ) ax . set ( title = title ) ;
82	train_images_dir = '../input/siim-isic-melanoma-classification/train/' train_images = listdir ( train_images_dir ) test_images_dir = '../input/siim-isic-melanoma-classification/test/' test_images = listdir ( test_images_dir )
83	def plot_images ( df ) : fig = plt . figure ( figsize = ( 15 , 6 ) ) for i in range ( 1 , 11 ) : image = df [ 'image_name' ] [ i ] ds = pydicom . dcmread ( train_images_dir + image + '.dcm' ) fig . add_subplot ( 2 , 5 , i ) plt . imshow ( ds . pixel_array )
84	random = train [ train [ 'benign_malignant' ] == 'benign' ] . sample ( n = 11 ) random = random . reset_index ( drop = True ) plot_images ( random )
85	random = train [ train [ 'benign_malignant' ] == 'malignant' ] . sample ( n = 11 ) random = random . reset_index ( drop = True ) plot_images ( random )
86	test_x = test [ [ 'image_name' ] ] test_x [ 'image_name' ] = test_x [ 'image_name' ] . apply ( lambda x : x + '.jpg' )
87	train = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/train.csv' ) submission = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/sample_submission.csv' ) test = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/test.csv' )
88	MAX_roll = 6 def movingaverage ( df ) : df [ 'cummax' ] = df [ 'signal' ] . cummax ( ) df [ 'cummin' ] = df [ 'signal' ] . cummin ( ) for i in range ( 2 , MAX_roll ) : df [ 'MA_{}' . format ( i ) ] = df [ 'signal' ] . rolling ( window = i ) . mean ( ) df . fillna ( - 999 , inplace = True ) df . reset_index ( drop = True , inplace = True ) return df
89	import pandas as pd import warnings warnings . filterwarnings ( 'ignore' ) train = pd . read_csv ( "../input/train.csv" ) train . head ( )
90	df = pd . concat ( [ pd . read_pickle ( BASE ) , pd . read_pickle ( PRICE ) . iloc [ : , 2 : ] , pd . read_pickle ( CALENDAR ) . iloc [ : , 2 : ] ] , axis = 1 )
91	START_TRAIN = 1200 END_TRAIN = 1941 START_PRED = 1942 P_HORIZON = 28 USE_AUX = False
92	plt . figure ( figsize = ( 10 , 8 ) ) sns . distplot ( np . log ( train . price_doc . values ) , bins = 60 , kde = True ) plt . xlabel ( 'Price' , fontsize = 12 ) plt . show ( )
93	import numpy as np import pandas as pd import matplotlib . pyplot as plt import os from datetime import datetime , timedelta import seaborn as sb print ( os . listdir ( "../input" ) )
94	features = [ 'returnsClosePrevRaw1' , 'returnsOpenPrevRaw1' , 'returnsClosePrevMktres1' , 'returnsOpenPrevMktres1' , 'returnsClosePrevRaw10' , 'returnsOpenPrevRaw10' , 'returnsClosePrevMktres10' , 'returnsOpenPrevMktres10' ] temp_show = market_data_no_outlier_scaled [ features ] temp_show [ 'target' ] = market_data_no_outlier_target [ 'returnsOpenNextMktres10' ] C_mat = temp_show . corr ( ) fig = plt . figure ( figsize = ( 15 , 15 ) ) sb . heatmap ( C_mat , vmax = 0.5 , square = True , annot = True ) plt . show ( ) del temp_show
95	optimizer = Adam ( lr = 0.001 ) model . compile ( optimizer = optimizer , loss = 'mean_squared_error' ) model . summary ( )
96	from keras . callbacks import ModelCheckpoint , EarlyStopping early_stopping = EarlyStopping ( monitor = 'val_loss' , patience = 3 , verbose = 1 , mode = 'auto' , restore_best_weights = True ) callbacks_list = [ early_stopping ]
97	def make_my_prediction ( x ) : my_pred = ( model . predict ( x ) ) . reshape ( 1 , - 1 ) [ 0 ] my_pred [ my_pred > 0 ] = 1 my_pred [ my_pred < 0 ] = - 1 return my_pred
98	for i in range ( len ( features ) ) : x_submission [ features [ i ] ] = x_submission [ features [ i ] ] . fillna ( x_submission [ features [ i ] ] . mean ( ) ) predictions_template_df [ 'confidenceValue' ] = make_my_prediction ( x_submission ) env . predict ( predictions_template_df ) del x_submission print ( 'Done!' )
99	path = Path ( '../input' ) train_df = pd . read_csv ( path / 'train.csv' ) var_names = [ col for col in train_df if 'var_' in col ]
100	def count_dist_peaks ( series , bins , prominence , width ) : count , division = np . histogram ( series , bins = bins ) peaks , props = find_peaks ( count , prominence = prominence , width = width ) return peaks
101	train_df_num_list = train_df_num . columns . tolist ( ) for i in train_df_num_list : make_histogram ( i )
102	img_name = "/kaggle/input/plant-pathology-2020-fgvc7/images/Test_0.jpg" ; predictions = gtf . Infer ( img_name = img_name ) ; from IPython . display import Image Image ( filename = img_name )
103	import pandas as pd from tqdm import tqdm_notebook as tqdm from scipy . special import softmax df = pd . read_csv ( "/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv" )
104	data_dir = "../input/" def load_data ( data_dir ) : train = pd . read_json ( data_dir + "train.json" ) test = pd . read_json ( data_dir + "test.json" )
105	def process_images ( df ) : X_band1 = np . array ( [ np . array ( band ) . astype ( np . float32 ) . reshape ( 75 , 75 ) for band in df [ "band_1" ] ] ) X_band2 = np . array ( [ np . array ( band ) . astype ( np . float32 ) . reshape ( 75 , 75 ) for band in df [ "band_2" ] ] )
106	gen = ImageDataGenerator ( horizontal_flip = True , vertical_flip = True , width_shift_range = 0.1 , height_shift_range = 0.1 , zoom_range = 0.1 , rotation_range = 40 )
107	model = simple_cnn ( ) model . fit_generator ( gen_flow , validation_data = ( [ X_valid , X_angle_valid ] , y_valid ) , steps_per_epoch = len ( X_train ) / batch_size , epochs = 20 )
108	test_img_file_path = train_dogs_filepaths [ 0 ] img_array = cv2 . imread ( test_img_file_path , cv2 . IMREAD_COLOR ) plt . imshow ( img_array ) plt . show ( )
109	ROW_DIMENSION = 60 COLUMN_DIMENSION = 60 CHANNELS = 3 new_array = cv2 . resize ( img_array_gray , ( ROW_DIMENSION , COLUMN_DIMENSION ) ) plt . imshow ( new_array , cmap = 'gray' ) plt . show ( )
110	from tensorflow import keras from keras . models import Sequential from keras . layers import Dense , Flatten , Conv2D , Dropout print ( "Import Successful" )
111	dvc_classifier . compile ( loss = keras . losses . binary_crossentropy , optimizer = 'adam' , metrics = [ 'accuracy' ] )
112	model_json = dvc_classifier . to_json ( ) with open ( "model.json" , "w" ) as json_file : json_file . write ( model_json )
113	json_file = open ( 'model.json' , 'r' ) loaded_model_json = json_file . read ( ) json_file . close ( ) loaded_model = model_from_json ( loaded_model_json )
114	for i in range ( 5 , 11 ) : if prediction_probabilities [ i , 0 ] >= 0.5 : print ( 'I am {:.2%} sure this is a Dog' . format ( prediction_probabilities [ i ] [ 0 ] ) ) else : print ( 'I am {:.2%} sure this is a Cat' . format ( 1 - prediction_probabilities [ i ] [ 0 ] ) ) plt . imshow ( arr_test [ i ] ) plt . show ( )
115	mydir = "/kaggle/working" try : shutil . rmtree ( mydir ) except OSError as e : print ( "Error: %s - %s." % ( e . filename , e . strerror ) )
116	def plot2x2Array ( image , mask ) : f , axarr = plt . subplots ( 1 , 2 ) axarr [ 0 ] . imshow ( image ) axarr [ 1 ] . imshow ( mask ) axarr [ 0 ] . grid ( ) axarr [ 1 ] . grid ( ) axarr [ 0 ] . set_title ( 'Image' ) axarr [ 1 ] . set_title ( 'Mask' )
117	def predict_chunk ( model , test ) : initial_idx = 0 chunk_size = 1000000 current_pred = np . zeros ( len ( test ) ) while initial_idx < test . shape [ 0 ] : final_idx = min ( initial_idx + chunk_size , test . shape [ 0 ] ) idx = range ( initial_idx , final_idx ) current_pred [ idx ] = model . predict ( test . iloc [ idx ] , num_iteration = model . best_iteration ) initial_idx = final_idx
118	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . map ( decode_image , num_parallel_calls = AUTO ) . map ( data_augment , num_parallel_calls = AUTO ) . repeat ( ) . shuffle ( 512 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) )
119	print ( "Read in libraries" ) import numpy as np import pandas as pd import matplotlib . pyplot as plt from scipy . optimize import curve_fit from statsmodels . tsa . statespace . sarimax import SARIMAX from statsmodels . tsa . arima_model import ARIMA from random import random
120	print ( "read in train file" ) df = pd . read_csv ( "/kaggle/input/covid19-global-forecasting-week-2/train.csv" , usecols = [ 'Province_State' , 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' ] )
121	submit_confirmed = [ ] submit_fatal = [ ] for i in df1 :
122	sns . set_style ( 'whitegrid' ) sns . set ( rc = { 'figure.figsize' : ( 11.7 , 8.27 ) } )
123	cols = train . columns . tolist ( ) print ( "Columns: " , cols ) columns = cols [ 1 : 11 ] + cols [ 56 : ] values = train [ columns ] labels = train [ 'Cover_Type' ] print ( "\nFeatures: " , columns )
124	start = time . time ( ) model_1 = ExtraTreesClassifier ( n_estimators = 375 ) model_1 . fit ( train [ columns ] , train [ 'Cover_Type' ] ) model_1_output = pd . DataFrame ( { "Id" : test [ 'Id' ] , "Cover_Type" : model_1 . predict ( test [ columns ] ) } ) print ( "Runtime ExtraTreesClassifier: " , time . time ( ) - start )
125	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . trip_duration . values , bins = 50 , kde = True ) plt . xlabel ( 'trip_duration' , fontsize = 12 ) plt . show ( )
126	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( np . log ( train_df . trip_duration . values ) , bins = 50 , kde = True ) plt . xlabel ( 'trip_duration' , fontsize = 12 ) plt . show ( )
127	lon1 , lat1 , lon2 , lat2 = map ( np . radians , [ lon1 , lat1 , lon2 , lat2 ] ) dlon = lon2 - lon1 dlat = lat2 - lat1 a = np . sin ( dlat / 2.0 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * np . sin ( dlon / 2.0 ) ** 2 c = 2 * np . arcsin ( np . sqrt ( a ) ) km = 6367 * c return km
128	test_df [ 'pickup_datetime' ] = pd . to_datetime ( test_df [ 'pickup_datetime' ] ) test_df [ 'pickup_hour' ] = test_df . pickup_datetime . dt . hour test_df [ 'day_week' ] = test_df . pickup_datetime . dt . weekday
129	def engineer_features ( data ) : data [ 'Direction_NS' ] = ( data . pickup_latitude > data . dropoff_latitude ) * 1 + 1 indices = data [ ( data . pickup_latitude == data . dropoff_latitude ) & ( data . pickup_latitude != 0 ) ] . index data . loc [ indices , 'Direction_NS' ] = 0
130	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
131	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "pickup_hour" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'pick up hour' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
132	grouped_df = train_df . groupby ( 'day_week' ) [ 'trip_duration' ] . aggregate ( np . median ) . reset_index ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . pointplot ( grouped_df . day_week . values , grouped_df . trip_duration . values , alpha = 0.8 , color = color [ 3 ] ) plt . ylabel ( 'trip duration median' , fontsize = 12 ) plt . xlabel ( 'week day' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
133	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "day_week" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Week day (0 - Monday)' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
134	import tensorflow as tf from tensorflow . python . data import Dataset import numpy as np import sklearn . metrics as metrics
135	N_TRAINING = 160000 N_VALIDATION = 100000 training_examples = train_data . head ( N_TRAINING ) [ [ my_feature_name ] ] . copy ( ) training_targets = train_data . head ( N_TRAINING ) [ [ my_target_name ] ] . copy ( ) validation_examples = train_data . tail ( N_VALIDATION ) [ [ my_feature_name ] ] . copy ( ) validation_targets = train_data . tail ( N_VALIDATION ) [ [ my_target_name ] ] . copy ( )
136	training_input_fn = lambda : my_input_fn ( training_examples , training_targets [ my_target_name ] , batch_size = batch_size )
137	predict_training_input_fn = lambda : my_input_fn ( training_examples , training_targets [ my_target_name ] , num_epochs = 1 , shuffle = False )
138	predict_validation_input_fn = lambda : my_input_fn ( validation_examples , validation_targets [ my_target_name ] , num_epochs = 1 , shuffle = False )
139	print ( "Training Loss: %0.2f" % training_log_loss ) print ( "Validation Loss: %0.2f" % validation_log_loss ) auc = metrics . auc
140	placeholder_label_vals = [ 0 for i in range ( 0 , 78035 ) ] test_labels = pd . DataFrame ( { "project_is_approved" : placeholder_label_vals } ) predict_test_input_fn = lambda : my_input_fn ( test_examples , test_labels , num_epochs = 1 , shuffle = False )
141	if filename in pneumonia_locations : pneumonia_locations [ filename ] . append ( location ) else : pneumonia_locations [ filename ] = [ location ]
142	folder = '../input/stage_2_train_images' filenames = os . listdir ( folder ) random . shuffle ( filenames )
143	n_valid_samples = 2560 train_filenames = filenames [ n_valid_samples : ] valid_filenames = filenames [ : n_valid_samples ] print ( 'n train samples' , len ( train_filenames ) ) print ( 'n valid samples' , len ( valid_filenames ) ) n_train_samples = len ( filenames ) - n_valid_samples
144	if self . augment and random . random ( ) > 0.5 : img = np . fliplr ( img ) msk = np . fliplr ( msk )
145	img = np . expand_dims ( img , - 1 ) msk = np . expand_dims ( msk , - 1 ) return img , msk def __loadpredict__ ( self , filename ) :
146	img = np . expand_dims ( img , - 1 ) return img def __getitem__ ( self , index ) :
147	imgs = np . array ( imgs ) msks = np . array ( msks ) return imgs , msks def on_epoch_end ( self ) : if self . shuffle : random . shuffle ( self . filenames ) def __len__ ( self ) : if self . predict :
148	def iou_loss ( y_true , y_pred ) : y_true = tf . reshape ( y_true , [ - 1 ] ) y_pred = tf . reshape ( y_pred , [ - 1 ] ) intersection = tf . reduce_sum ( y_true * y_pred ) score = ( intersection + 1. ) / ( tf . reduce_sum ( y_true ) + tf . reduce_sum ( y_pred ) - intersection + 1. ) return 1 - score
149	model = create_network ( input_size = IMAGE_SIZE , channels = 32 , n_blocks = 2 , depth = 4 ) model . compile ( optimizer = 'adam' , loss = iou_bce_loss , metrics = [ 'accuracy' , mean_iou ] )
150	def cosine_annealing ( x ) : lr = 0.001 epochs = 20 return lr * ( np . cos ( np . pi * x / epochs ) + 1. ) / 2 learning_rate = tf . keras . callbacks . LearningRateScheduler ( cosine_annealing )
151	folder = '../input/stage_2_train_images' train_gen = generator ( folder , train_filenames , pneumonia_locations , batch_size = BATCH_SIZE , image_size = IMAGE_SIZE , shuffle = True , augment = True , predict = False ) valid_gen = generator ( folder , valid_filenames , pneumonia_locations , batch_size = BATCH_SIZE , image_size = IMAGE_SIZE , shuffle = False , predict = False ) print ( model . summary ( ) )
152	folder = '../input/stage_2_test_images' test_filenames = os . listdir ( folder ) print ( 'n test samples:' , len ( test_filenames ) )
153	y , x , y2 , x2 = region . bbox height = y2 - y width = x2 - x
154	sub = pd . DataFrame . from_dict ( submission_dict , orient = 'index' ) sub . index . names = [ 'patientId' ] sub . columns = [ 'PredictionString' ] sub . to_csv ( 'submission.csv' )
155	def feature_importance ( forest , X_train ) : ranked_list = [ ] importances = forest . feature_importances_ indices = np . argsort ( importances ) [ : : - 1 ]
156	print ( "Feature ranking:" ) for f in range ( X_train . shape [ 1 ] ) : print ( "%d. feature %d (%f)" % ( f + 1 , indices [ f ] , importances [ indices [ f ] ] ) + " - " + X_train . columns [ indices [ f ] ] ) ranked_list . append ( X_train . columns [ indices [ f ] ] ) return ranked_list
157	aggs_num = { 'age' : [ 'min' , 'max' , 'mean' ] , 'escolari' : [ 'min' , 'max' , 'mean' ] } aggs_cat = { 'dis' : [ 'mean' ] } for s_ in [ 'estadocivil' , 'parentesco' , 'instlevel' ] : for f_ in [ f_ for f_ in df . columns if f_ . startswith ( s_ ) ] : aggs_cat [ f_ ] = [ 'mean' , 'count' ]
158	if 0 in sum_ohe : print ( 'The OHE in {} is incomplete. A new column will be added before label encoding' . format ( s_ ) )
159	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) test_ids = test . Id
160	return do_features ( df_ ) train = process_df ( train ) test = process_df ( test )
161	train [ 'edjefe' ] = train [ 'edjefe' ] . astype ( "int" ) train [ 'edjefa' ] = train [ 'edjefa' ] . astype ( "int" ) test [ 'edjefe' ] = test [ 'edjefe' ] . astype ( "int" ) test [ 'edjefa' ] = test [ 'edjefa' ] . astype ( "int" )
162	needless_cols = [ 'r4t3' , 'tamhog' , 'tamviv' , 'hhsize' , 'v18q' , 'v14a' , 'agesq' , 'mobilephone' , 'female' , ] instlevel_cols = [ s for s in train . columns . tolist ( ) if 'instlevel' in s ] needless_cols . extend ( instlevel_cols ) train = train . drop ( needless_cols , axis = 1 ) test = test . drop ( needless_cols , axis = 1 )
163	def split_data ( train , y , households , test_percentage = 0.20 , seed = None ) : train2 = train . copy ( ) cv_hhs = np . random . choice ( households , size = int ( len ( households ) * test_percentage ) , replace = False ) cv_idx = np . isin ( households , cv_hhs ) X_test = train2 [ cv_idx ] y_test = y [ cv_idx ] X_train = train2 [ ~ cv_idx ] y_train = y [ ~ cv_idx ] return X_train , y_train , X_test , y_test
164	cv_idx = np . isin ( households , cv_hhs ) X_test = train2 [ cv_idx ] y_test = y [ cv_idx ] X_train = train2 [ ~ cv_idx ] y_train = y [ ~ cv_idx ] return X_train , y_train , X_test , y_test
165	X_train = train2 y_train = y train_households = X_train . idhogar
166	if sample_weight is not None : estimator . fit ( X_train , y_train , sample_weight = sample_weight , ** fit_params ) else : estimator . fit ( X_train , y_train , ** fit_params ) return estimator class VotingClassifierLGBM ( VotingClassifier ) :
167	def feature_importance ( forest , X_train , display_results = True ) : ranked_list = [ ] zero_features = [ ] importances = forest . feature_importances_ indices = np . argsort ( importances ) [ : : - 1 ] if display_results :
168	print ( "Feature ranking:" ) for f in range ( X_train . shape [ 1 ] ) : if display_results : print ( "%d. feature %d (%f)" % ( f + 1 , indices [ f ] , importances [ indices [ f ] ] ) + " - " + X_train . columns [ indices [ f ] ] ) ranked_list . append ( X_train . columns [ indices [ f ] ] ) if importances [ indices [ f ] ] == 0.0 : zero_features . append ( X_train . columns [ indices [ f ] ] ) return ranked_list , zero_features
169	aggs_num = { 'age' : [ 'min' , 'max' , 'mean' ] , 'escolari' : [ 'min' , 'max' , 'mean' ] } aggs_cat = { 'dis' : [ 'mean' ] } for s_ in [ 'estadocivil' , 'parentesco' , 'instlevel' ] : for f_ in [ f_ for f_ in df . columns if f_ . startswith ( s_ ) ] : aggs_cat [ f_ ] = [ 'mean' , 'count' ]
170	if 0 in sum_ohe : print ( 'The OHE in {} is incomplete. A new column will be added before label encoding' . format ( s_ ) )
171	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) test_ids = test . Id
172	return do_features ( df_ ) train = process_df ( train ) test = process_df ( test )
173	train [ 'edjefe' ] = train [ 'edjefe' ] . astype ( "int" ) train [ 'edjefa' ] = train [ 'edjefa' ] . astype ( "int" ) test [ 'edjefe' ] = test [ 'edjefe' ] . astype ( "int" ) test [ 'edjefa' ] = test [ 'edjefa' ] . astype ( "int" )
174	needless_cols = [ 'r4t3' , 'tamhog' , 'tamviv' , 'hhsize' , 'v18q' , 'v14a' , 'agesq' , 'mobilephone' , 'female' , ] instlevel_cols = [ s for s in train . columns . tolist ( ) if 'instlevel' in s ] needless_cols . extend ( instlevel_cols ) train = train . drop ( needless_cols , axis = 1 ) test = test . drop ( needless_cols , axis = 1 )
175	cv_idx = np . isin ( households , cv_hhs ) X_test = train2 [ cv_idx ] y_test = y [ cv_idx ] X_train = train2 [ ~ cv_idx ] y_train = y [ ~ cv_idx ] if sample_weight is not None : y_train_weights = sample_weight [ ~ cv_idx ] return X_train , y_train , X_test , y_test , y_train_weights return X_train , y_train , X_test , y_test
176	X_train = train2 y_train = y train_households = X_train . idhogar
177	if sample_weight is not None : X_train , y_train , X_test , y_test , y_train_weight = split_data ( X , y , sample_weight , households = train_households ) else : X_train , y_train , X_test , y_test = split_data ( X , y , None , households = train_households )
178	else : print ( "Unacceptable!!! Trying again..." ) return _parallel_fit_estimator ( estimator1 , X , y , sample_weight = sample_weight , ** fit_params ) else : return estimator class VotingClassifierLGBM ( VotingClassifier ) :
179	useless_features = [ ] drop_features = set ( ) counter = 0 for est in vc . estimators_ : ranked_features , unused_features = feature_importance ( est , X_train . drop ( xgb_drop_cols , axis = 1 ) , display_results = False ) useless_features . append ( unused_features ) if counter == 0 : drop_features = set ( unused_features ) else : drop_features = drop_features . intersection ( set ( unused_features ) ) counter += 1 drop_features
180	ets = [ ] for i in range ( 10 ) : rf = RandomForestClassifier ( max_depth = None , random_state = 217 + i , n_jobs = 4 , n_estimators = 700 , min_impurity_decrease = 1e-3 , min_samples_leaf = 2 , verbose = 0 , class_weight = "balanced" ) ets . append ( ( 'rf{}' . format ( i ) , rf ) ) vc2 = VotingClassifierLGBM ( ets , voting = 'soft' ) _ = vc2 . fit ( X_train . drop ( et_drop_cols , axis = 1 ) , y_train , threshold = False )
181	useless_features = [ ] drop_features = set ( ) counter = 0 for est in vc2 . estimators_ : ranked_features , unused_features = feature_importance ( est , X_train . drop ( et_drop_cols , axis = 1 ) , display_results = False ) useless_features . append ( unused_features ) if counter == 0 : drop_features = set ( unused_features ) else : drop_features = drop_features . intersection ( set ( unused_features ) ) counter += 1 drop_features
182	vc . voting = "soft" vc1_probs = vc . predict_proba ( data . drop ( xgb_drop_cols , axis = 1 ) ) vc2 . voting = "soft" vc2_probs = vc2 . predict_proba ( data . drop ( et_drop_cols , axis = 1 ) ) final_vote = ( vc1_probs * weights [ 0 ] ) + ( vc2_probs * weights [ 1 ] ) predictions = np . argmax ( final_vote , axis = 1 ) return predictions
183	def clean ( X_train , X_test ) : X_train [ 'words' ] = [ re . sub ( "[^a-zA-Z]" , " " , data ) . lower ( ) . split ( ) for data in X_train [ 'text' ] ] X_test [ 'words' ] = [ re . sub ( "[^a-zA-Z]" , " " , data ) . lower ( ) . split ( ) for data in X_test [ 'text' ] ] return X_train , X_test X_train , X_test = clean ( X_train , X_test )
184	model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' ) return model def doNN_glove ( X_train , X_test , Y_train , xtrain_glove , xtest_glove ) :
185	from absl import logging import matplotlib . pyplot as plt import numpy as np from PIL import Image , ImageOps from scipy . spatial import cKDTree from skimage . feature import plot_matches from skimage . measure import ransac from skimage . transform import AffineTransform from six import BytesIO import tensorflow as tf import tensorflow_hub as hub from six . moves . urllib . request import urlopen
186	locations_2_to_use = np . array ( [ result2 [ 'locations' ] [ i , ] for i in range ( num_features_2 ) if indices [ i ] != num_features_1 ] ) locations_1_to_use = np . array ( [ result1 [ 'locations' ] [ indices [ i ] , ] for i in range ( num_features_2 ) if indices [ i ] != num_features_1 ] )
187	_ , inliers = ransac ( ( locations_1_to_use , locations_2_to_use ) , AffineTransform , min_samples = 3 , residual_threshold = 20 , max_trials = 1000 ) print ( 'Found %d inliers' % sum ( inliers ) )
188	locations_2_to_use = np . array ( [ result2 [ 'locations' ] [ i , ] for i in range ( num_features_2 ) if indices [ i ] != num_features_1 ] ) locations_1_to_use = np . array ( [ result1 [ 'locations' ] [ indices [ i ] , ] for i in range ( num_features_2 ) if indices [ i ] != num_features_1 ] )
189	_ , inliers = ransac ( ( locations_1_to_use , locations_2_to_use ) , AffineTransform , min_samples = 3 , residual_threshold = 20 , max_trials = 1000 )
190	import torch import time import numpy as np import pandas as pd from numba import njit torch . manual_seed ( 20191210 ) N_DAYS = 100 N_FAMILIES = 5000 MAX_OCCUPANCY = 300 MIN_OCCUPANCY = 125 INPUT_PATH = '/kaggle/input/santa-workshop-tour-2019/' OUTPUT_PATH = '' DEFAULT_DEVICE = torch . device ( 'cpu' )
191	MAX_ACTION = 4 SOFT_PENALTY_PER_PERSON = 1000 PENALTY_RAMP_TIME = 2000 BATCH_SIZE = 1000 N_BATCHES = 6000 LR = 0.025 GRADIENT_CLIP = 100.0 MAX_PREFERENCE = 8.5 USE_ADAM = True ADAM_BETA_M = 0.9 ADAM_BETA_V = 0.99 ADAM_EPSILON = 0.000001 MOMENTUM = 0.95
192	clf = LGBMClassifier ( n_estimators = 400 , learning_rate = 0.03 , num_leaves = 30 , colsample_bytree = .8 , subsample = .9 , max_depth = 7 , reg_alpha = .1 , reg_lambda = .1 , min_split_gain = .01 , min_child_weight = 2 , silent = - 1 , verbose = - 1 , ) clf . fit ( data_train , y_train , eval_set = [ ( data_train , y_train ) , ( data_valid , y_valid ) ] , eval_metric = 'auc' , verbose = 100 , early_stopping_rounds = 30 )
193	times = ( np . arange ( output_sr , dtype = np_dtype ) . reshape ( ( output_sr , 1 , 1 ) ) / output_sr - np . arange ( input_sr , dtype = np_dtype ) . reshape ( ( 1 , input_sr , 1 ) ) / input_sr - ( np . arange ( kernel_width , dtype = np_dtype ) . reshape ( ( 1 , 1 , kernel_width ) ) - blocks_per_side ) ) def hann_window ( a ) :
194	if filter == 'hann' : weights = ( np . sinc ( times * zeros_per_block ) * hann_window ( times / window_radius_in_blocks ) * zeros_per_block / input_sr ) else : weights = ( np . sinc ( times * zeros_per_block ) * kaiser_window ( times / window_radius_in_blocks , beta ) * zeros_per_block / input_sr ) self . input_sr = input_sr self . output_sr = output_sr
195	assert weights . shape == ( output_sr , input_sr , kernel_width ) if output_sr == 1 : self . resample_type = 'integer_downsample' self . padding = input_sr * blocks_per_side weights = torch . tensor ( weights , dtype = dtype , requires_grad = False ) self . weights = weights . transpose ( 1 , 2 ) . contiguous ( ) . view ( 1 , 1 , input_sr * kernel_width ) elif input_sr == 1 :
196	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
197	count_vectorizer = CountVectorizer ( min_df = 5 , stop_words = 'english' ) vect = count_vectorizer . fit ( train_df . question_text ) X_vect_train = vect . transform ( X_train ) X_vect_test = vect . transform ( X_test ) tf_train_transformer = TfidfTransformer ( use_idf = False ) . fit ( X_vect_train ) tf_test_transformer = TfidfTransformer ( use_idf = False ) . fit ( X_vect_test ) xtrain_tf = tf_train_transformer . transform ( X_vect_train ) xtest_tf = tf_test_transformer . transform ( X_vect_test )
198	fig , axes = plt . subplots ( 1 , 1 , figsize = ( 15 , 8 ) ) plt . ylim ( ( .5 , 1 ) ) axes . set_ylabel ( "Accuracy" ) axes . set_title ( "Traditional Classfieris Results for 67% Training and 23% Testing with Two Types of Embedding" ) results_df [ results_df . index != "RF" ] . plot ( kind = "bar" , ax = axes )
199	model . add ( Bidirectional ( CuDNNGRU ( 64 , return_sequences = True ) ) ) model . add ( GlobalMaxPool1D ( ) ) model . add ( Dense ( 16 , activation = "relu" ) ) model . add ( Dropout ( 0.1 ) ) model . add ( Dense ( 1 , activation = "sigmoid" ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )
200	market_train_full_df = env . get_training_data ( ) [ 0 ] sample_market_df = pd . read_csv ( "../input/marketdata_sample.csv" ) sample_news_df = pd . read_csv ( "../input/news_sample.csv" )
201	from wordcloud import WordCloud wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( " " . join ( market_train_full_df . assetName ) ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 20 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
202	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 20 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
203	fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Volume" ) axes . set_ylabel ( "volume" ) axes . set_xlabel ( "records" ) axes . plot ( market_train_full_df [ "volume" ] )
204	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
205	text = " " . join ( tmp_list ) . replace ( "'" , "" ) wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( text ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
206	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
207	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
208	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
209	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
210	import os import pandas as pd import numpy as np import json import matplotlib . pyplot as plt import datetime as datetime from datetime import timedelta , date import seaborn as sns import matplotlib . cm as CM import lightgbm as lgb from sklearn import preprocessing from sklearn . metrics import mean_squared_error from sklearn . model_selection import GridSearchCV , train_test_split
211	list_of_devices = train_data . device . apply ( json . loads ) . tolist ( ) keys = [ ] for devices_iter in list_of_devices : for list_element in list ( devices_iter . keys ( ) ) : if list_element not in keys : keys . append ( list_element )
212	revenue_datetime_df = train_data [ [ "revenue" , "date" ] ] . dropna ( ) revenue_datetime_df [ "revenue" ] = revenue_datetime_df . revenue . astype ( np . int64 ) revenue_datetime_df . head ( )
213	daily_revenue_df = revenue_datetime_df . groupby ( by = [ "date" ] , axis = 0 ) . sum ( ) import matplotlib . pyplot as plt fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Daily Revenue" ) axes . set_ylabel ( "Revenue" ) axes . set_xlabel ( "date" ) axes . plot ( daily_revenue_df [ "revenue" ] )
214	fig , axes = plt . subplots ( 1 , 2 , figsize = ( 15 , 10 ) ) traffic_source_df [ "keyword" ] . value_counts ( ) . head ( 10 ) . plot ( kind = "bar" , ax = axes [ 0 ] , title = "keywords (total)" , color = "orange" ) traffic_source_df [ traffic_source_df [ "keyword" ] != "(not provided)" ] [ "keyword" ] . value_counts ( ) . head ( 15 ) . plot ( kind = "bar" , ax = axes [ 1 ] , title = "keywords (dropping NA)" , color = "c" )
215	X_train = train . drop ( 'isFraud' , axis = 1 ) X_test = test . copy ( ) X_train = X_train . fillna ( - 999 ) X_test = X_test . fillna ( - 999 )
216	df_list = [ ] chunksize = 1_000_000 for df_chunk in pd . read_csv ( data_path , dtype = final_types , chunksize = chunksize ) : df_list . append ( df_chunk ) df = pd . concat ( df_list ) df = df [ ~ df . isin ( [ np . nan , np . inf , - np . inf ] ) . any ( 1 ) ] del df_list return df train_labels_df = create_efficient_df ( train_labels_path ) train_labels_df [ train_labels_df [ "Label" ] > 0 ] . head ( )
217	for hem_ix , hem_col in enumerate ( list ( hem_df ) [ 1 : ] ) : hem_df [ hem_col ] = list ( train_labels_df . iloc [ hem_ix : : len ( hem_types ) , 1 ] ) hem_df . info ( ) hem_df [ hem_df [ "type_5" ] > 0 ] . head ( )
218	print ( "\nProbability that a patient in the dataset has any of the 5 hemorrhage types:" ) print ( "Number of patients with hemorrhage: {}" . format ( hem_counts [ - 1 ] ) ) print ( "Total number of patients: {}" . format ( num_train ) ) print ( "p(hemorrhage): %.2f%%" % ( hem_counts [ - 1 ] / num_train * 100 ) )
219	print ( "\nNumber of each type of hemorrhage found in dataset and share of the raw total [p(hem_type)]: " ) print ( "%21s | %7s | %9s" % ( "hemorrhage type" , "count" , "portion" ) ) for hem_type in range ( 5 ) : print ( "%21s | %7d | %8.2f%%" % ( hem_types [ hem_type ] , hem_counts [ hem_type ] , hem_total_portions [ hem_type ] * 100 ) )
220	method = "sklearn_stratified" start = time . time ( ) train , valid = split ( dataset_path = DATASET_PATH , test_size = TEST_SIZE , stratification = method ) print ( f"Dataset split done for {time.time() - start} seconds" )
221	def get_rmse ( ytest_input = 'ytest' , pred_input = 'pred' ) : n , loss = 0 , 0 reader_ytest = open ( ytest_input , 'r' ) reader_pred = open ( pred_input , 'r' ) for label , pred in tqdm ( zip ( reader_ytest , reader_pred ) ) : n += 1 true_score = float ( label ) pred_score = float ( pred ) loss += np . square ( pred_score - true_score ) reader_ytest . close ( ) reader_pred . close ( ) return np . sqrt ( loss / n )
222	postproc_cols = [ col for col in stage2 . columns if col not in helpers_cols ] for col in postproc_cols : print ( 'Make postprocessing for {}' . format ( col ) ) add_custom_postproc ( stage2 , col , stage2_meta_df , optimal_triplets )
223	import gc import os import librosa import numpy as np import pandas as pd from glob import glob from pathlib import Path from librosa import display import matplotlib . pyplot as plt from scipy . io . wavfile import read from IPython . display import HTML , Audio , display_html from IPython . display import display as display_ipython pd . set_option ( 'display.max_colwidth' , 500 )
224	PATH = "../input/bert-base-uncased/" MAX_SEQUENCE_LENGTH = 128 TOKENIZER = BertWordPieceTokenizer ( f"{PATH}/vocab.txt" , lowercase = True )
225	tweet = tweet . decode ( 'utf-8' ) selected_text = selected_text . decode ( 'utf-8' ) sentiment = sentiment . decode ( 'utf-8' )
226	idx_start , idx_end = None , None for index in ( i for i , c in enumerate ( tweet ) if c == selected_text [ 0 ] ) : if tweet [ index : index + len ( selected_text ) ] == selected_text : idx_start = index idx_end = index + len ( selected_text ) break intersection = [ 0 ] * len ( tweet ) if idx_start != None and idx_end != None : for char_idx in range ( idx_start , idx_end ) : intersection [ char_idx ] = 1
227	diff = max ( len ( input_ids ) - ( MAX_SEQUENCE_LENGTH - 2 ) , 0 ) input_ids = input_ids [ 1 : - 1 ] [ : len ( input_ids ) - diff - 2 ] offsets = offsets [ : len ( offsets ) - diff ] target_start = list ( target_start [ : len ( target_start ) - diff ] ) target_end = list ( target_end [ : len ( target_end ) - diff ] )
228	pred_start , pred_end , text , selected_text , sentiment , offset = \ predict ( model , valid_dataset , loss_fn , optimizer ) selected_text_pred = decode_prediction ( pred_start , pred_end , text , offset , sentiment ) jaccards = [ ] for i in range ( len ( selected_text ) ) : jaccards . append ( jaccard ( selected_text [ i ] , selected_text_pred [ i ] ) ) score = np . mean ( jaccards ) print ( f"valid jaccard epoch {epoch_num+1:03d}: {score}" + " " * 15 ) if score > best_score : best_score = score
229	selected_text_pred = decode_prediction ( test_preds_start , test_preds_end , test_text , test_offset , test_sentiment ) submission_df . loc [ : , 'selected_text' ] = selected_text_pred submission_df . to_csv ( "submission.csv" , index = False )
230	PROJECT_ID = 'geultto' from google . cloud import bigquery client = bigquery . Client ( project = PROJECT_ID , location = "US" ) dataset = client . create_dataset ( 'bqml_example' , exists_ok = True ) from google . cloud . bigquery import magics from kaggle . gcp import KaggleKernelCredentials magics . context . credentials = KaggleKernelCredentials ( ) magics . context . project = PROJECT_ID
231	table = client . get_table ( "kaggle-competition-datasets.geotab_intersection_congestion.train" ) client . list_rows ( table , max_results = 5 ) . to_dataframe ( )
232	SELECT * FROM ML . TRAINING_INFO ( MODEL ` bqml_example . distance_p20 ` ) ORDER BY iteration
233	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
234	with strategy . scope ( ) : model = tf . keras . Sequential ( [ irv . InceptionResNetV2 ( include_top = False , weights = 'imagenet' , input_shape = ( 299 , 299 , 3 ) ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . summary ( )
235	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import glob import cv2 import os print ( os . listdir ( "../input" ) )
236	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( test_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
237	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( index_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
238	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( train_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
239	temp = pd . DataFrame ( train_data . landmark_id . value_counts ( ) . head ( 10 ) ) temp . reset_index ( inplace = True ) temp . columns = [ 'landmark_id' , 'count' ] temp
240	temp = pd . DataFrame ( train_data . landmark_id . value_counts ( ) . tail ( 10 ) ) temp . reset_index ( inplace = True ) temp . columns = [ 'landmark_id' , 'count' ] temp
241	img = cv2 . imread ( os . path . join ( dataset_path , img_name ) ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) kp , des = detector . detectAndCompute ( img , None ) return img , kp , des def draw_image_matches ( detector , img1_name , img2_name , nmatches = 50 ) :
242	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
243	from datetime import datetime from os import scandir def convert_date ( timestamp ) : d = datetime . utcfromtimestamp ( timestamp ) formated_date = d . strftime ( '%d %b %Y' ) return formated_date def get_files ( ) : dir_entries = scandir ( 'my_directory/' ) for entry in dir_entries : if entry . is_file ( ) : info = entry . stat ( ) print ( f'{entry.name}\t Last Modified: {convert_date(info.st_mtime)}' )
244	import openslide import skimage . io import random import seaborn as sns import cv2 import pandas as pd import numpy as np import matplotlib import matplotlib . pyplot as plt import PIL import os
245	import cv2 as cv import os import matplotlib . pylab as plt train_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/' fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 15 ) ) train_video_files = [ train_dir + x for x in os . listdir ( train_dir ) ]
246	print ( 'First subject anatomical nifti image (3D) is at: %s' % haxby_dataset . anat [ 0 ] ) print ( 'First subject functional nifti image (4D) is at: %s' % haxby_dataset . func [ 0 ] )
247	print ( 'First subject anatomical nifti image (3D) is at: %s' % haxby_dataset . anat [ 0 ] ) print ( 'First subject functional nifti image (4D) is at: %s' % haxby_dataset . func [ 0 ] ) haxby_anat_filename = haxby_dataset . anat [ 0 ] haxby_mask_filename = haxby_dataset . mask_vt [ 0 ] haxby_func_filename = haxby_dataset . func [ 0 ]
248	from nilearn import datasets rest_dataset = datasets . fetch_development_fmri ( n_subjects = 20 ) func_filenames = rest_dataset . func confounds = rest_dataset . confounds
249	dict_learn = DictLearning ( n_components = 8 , smoothing_fwhm = 6. , memory = "nilearn_cache" , memory_level = 2 , random_state = 0 )
250	from nilearn import plotting plotting . plot_prob_atlas ( components_img , view_type = 'filled_contours' , title = 'Dictionary Learning maps' )
251	import numpy as np mean_correlations = np . mean ( correlations , axis = 0 ) . reshape ( n_regions_extracted , n_regions_extracted )
252	regions_img = regions_extracted_img coords_connectome = plotting . find_probabilistic_atlas_cut_coords ( regions_img ) plotting . plot_connectome ( mean_correlations , coords_connectome , edge_threshold = '90%' , title = title )
253	colors = 'rgbcmyk' for each_index_of_map3 , color in zip ( regions_indices_of_map3 [ 0 ] , colors ) : display . add_overlay ( image . index_img ( regions_extracted_img , each_index_of_map3 ) , cmap = plotting . cm . alpha_cmap ( color ) ) plotting . show ( )
254	print ( 'First anatomical nifti image (3D) located is at: %s' % haxby_dataset . anat [ 0 ] ) print ( 'First functional nifti image (4D) is located at: %s' % haxby_dataset . func [ 0 ] )
255	import matplotlib . pyplot as plt plt . figure ( figsize = ( 7 , 5 ) ) plt . plot ( masked_data [ : 150 , : 2 ] ) plt . xlabel ( 'Time [TRs]' , fontsize = 16 ) plt . ylabel ( 'Intensity' , fontsize = 16 ) plt . xlim ( 0 , 150 ) plt . subplots_adjust ( bottom = .12 , top = .95 , right = .95 , left = .12 ) show ( )
256	plotting . plot_prob_atlas ( smith . bm10 , title = 'Smith2009 10 Brainmap (with' ' colorbar)' , colorbar = True ) print ( 'ready' ) plotting . show ( )
257	self . unigram_mnb . fit ( x , y ) return self def add_unigram_predictions ( self , text_series ) :
258	del unigram_predictions [ unigram_predictions . columns [ 0 ] ] df = df . merge ( unigram_predictions , left_index = True , right_index = True ) return df def transform ( self , text_series ) :
259	self . num_cores = cpu_count ( ) def part_of_speechiness ( self , pos_counts , part_of_speech ) : if eval ( part_of_speech ) in pos_counts : return pos_counts [ eval ( part_of_speech ) . numerator ] return 0 def add_pos_features ( self , df ) : text_series = df [ TEXT_COLUMN ]
260	lensig = signal . size symset = list ( set ( signal ) ) numsym = len ( symset ) propab = [ np . size ( signal [ signal == i ] ) / ( 1.0 * lensig ) for i in symset ] ent = np . sum ( [ p * np . log2 ( 1.0 / p ) for p in propab ] ) return ent
261	with open ( path / "label_descriptions.json" ) as f : label_descriptions = json . load ( f ) label_names = [ x [ 'name' ] for x in label_descriptions [ 'categories' ] ] print ( label_names )
262	img_file = fnames [ 500 ] img = open_image ( img_file ) img . show ( figsize = ( 5 , 5 ) )
263	images_df = pd . DataFrame ( images ) src = ( SegmentationItemList . from_df ( images_df , path_img ) . split_by_rand_pct ( ) . label_from_func ( get_y_fn , classes = codes ) ) data = ( src . transform ( get_transforms ( ) , size = size , tfm_y = True ) . databunch ( bs = bs ) . normalize ( imagenet_stats ) )
264	def acc_fashion ( input , target ) : target = target . squeeze ( 1 ) mask = target != category_num - 1 return ( input . argmax ( dim = 1 ) == target ) . float ( ) . mean ( )
265	import numpy as np import pandas as pd import random import seaborn as sns import matplotlib . pyplot as plt import gc import seaborn as sns sns . set ( style = 'whitegrid' , color_codes = True ) import scipy . stats as st import statsmodels . formula . api as smf from sklearn . ensemble import RandomForestRegressor from sklearn . cross_validation import train_test_split import xgboost as xgb import operator
266	import numpy as np import pandas as pd import random import seaborn as sns import matplotlib . pyplot as plt import gc import seaborn as sns sns . set ( style = 'whitegrid' , color_codes = True )
267	train_items = pd . merge ( df_train , item , how = 'inner' ) train_items1 = pd . merge ( df_train , item , how = 'inner' ) train_items2 = pd . merge ( df_train , item , how = 'inner' )
268	fig , ( axis1 ) = plt . subplots ( 1 , 1 , sharex = True , figsize = ( 15 , 8 ) ) ax1 = oil . plot ( legend = True , ax = axis1 , marker = 'o' , title = "Oil Price" )
269	if step < 200 : return CONFIG_MAX_SHIPS elif step < 300 : return CONFIG_MAX_SHIPS - 2 elif step < 350 : return CONFIG_MAX_SHIPS - 4 else : return CONFIG_MAX_SHIPS - 5 def set_turn_data ( board ) :
270	turn . num_ships = len ( me . ships ) turn . max_ships = compute_max_ships ( board . step ) turn . total_halite = me . halite
271	turn . taken = { } turn . last_episode = ( board . step == ( board . configuration . episode_steps - 2 ) ) def init ( obs , config ) :
272	global size global print if hasattr ( config , 'myval' ) and config . myval == 9 and not quiet :
273	print = print_none pprint . pprint = print_none size = config . size def limit ( x , a , b ) : if x < a : return a if x > b : return b return x def num_turns_to_mine ( C , H , rt_travel ) :
274	if action == ShipAction . NORTH : ret = pos + Point ( 0 , 1 ) if action == ShipAction . SOUTH : ret = pos + Point ( 0 , - 1 ) if action == ShipAction . EAST : ret = pos + Point ( 1 , 0 ) if action == ShipAction . WEST : ret = pos + Point ( - 1 , 0 ) if ret is None : ret = pos
275	if deltaX < 0 : deltaX += size elif deltaX > 0 : deltaX -= size if abs ( deltaY ) > size / 2 :
276	if deltaY < 0 : deltaY += size elif deltaY > 0 : deltaY -= size
277	action , step = dirs_to ( a , b , size = 21 ) return abs ( step [ 0 ] ) + abs ( step [ 1 ] ) def nearest_shipyard ( pos ) :
278	mn = 100 best_pos = None for sy in me . shipyards : d = dist ( pos , sy . position ) if d < mn : mn = d best_pos = sy . position return mn , best_pos def assign_targets ( board , ships ) :
279	old_target = copy . copy ( ship_target ) ship_target . clear ( ) if len ( ships ) == 0 : return halite_min = 50 pts1 = [ ] pts2 = [ ] for pt , c in board . cells . items ( ) : assert isinstance ( pt , Point ) if c . halite > halite_min : pts1 . append ( pt )
280	if d1 > 0 : v = my_halite / d1 else :
281	ret = [ ] for x in p : if x not in ret : ret . append ( x ) return ret def matrix_lookup ( matrix , pos ) : return matrix [ pos . y , pos . x ] def ship_converts ( board ) :
282	for ship in me . ships : if ship . next_action : continue
283	avoid = make_avoidance_matrix ( ship . halite ) z = [ matrix_lookup ( avoid , move ( ship . position , a ) ) for a in all_actions ] if np . all ( z ) and ship . halite > 500 : ship . next_action = ShipAction . CONVERT turn . taken [ ship . position ] = 1 turn . num_shipyards += 1 turn . total_halite -= 500 print ( 'ship id {} no escape converting' . format ( ship . id ) )
284	import os import numpy as np import pandas as pd import warnings from bayes_opt import BayesianOptimization from skopt import BayesSearchCV import matplotlib . pyplot as plt import seaborn as sns import lightgbm as lgb import xgboost as xgb from sklearn . model_selection import train_test_split , StratifiedKFold , cross_val_score import time import sys from sklearn . metrics import roc_auc_score , roc_curve import shap warnings . simplefilter ( action = 'ignore' , category = FutureWarning )
285	lgbBO . maximize ( init_points = init_round , n_iter = opt_round ) model_auc = [ ] for model in range ( len ( lgbBO . res ) ) : model_auc . append ( lgbBO . res [ model ] [ 'target' ] )
286	graph = lgb . create_tree_digraph ( clf , tree_index = 3 , name = 'Tree3' ) graph . graph_attr . update ( size = "110,110" ) graph
287	pubs_map = folium . Map ( location = [ 40.742459 , - 73.971765 ] , zoom_start = 12 ) data = [ [ x [ 0 ] , x [ 1 ] , 1 ] for x in np . array ( bars [ [ 'Latitude' , 'Longitude' ] ] ) ] HeatMap ( data , radius = 20 ) . add_to ( pubs_map ) pubs_map
288	train_df [ 'interest_level' ] = train_df [ 'interest_level' ] . apply ( lambda x : { 'high' : 0 , 'medium' : 1 , 'low' : 2 } [ x ] ) test_df [ 'interest_level' ] = - 1 train_index = train_df . index test_index = test_df . index data_df = pd . concat ( ( train_df , test_df ) , axis = 0 ) del train_df , test_df
289	data_df [ 'num_photos' ] = data_df [ 'photos' ] . apply ( len ) data_df [ 'num_features' ] = data_df [ 'features' ] . apply ( len ) data_df [ 'num_description' ] = data_df [ 'description' ] . apply ( lambda x : len ( x . split ( ' ' ) ) ) data_df . drop ( 'photos' , axis = 1 , inplace = True )
290	data_df [ 'room_difference' ] = data_df [ 'bedrooms' ] - data_df [ 'bathrooms' ] data_df [ 'total_rooms' ] = data_df [ 'bedrooms' ] + data_df [ 'bathrooms' ] data_df [ 'price_per_room' ] = data_df [ 'price' ] / ( data_df [ 'total_rooms' ] + 1 )
291	data_df [ 'created' ] = pd . to_datetime ( data_df [ 'created' ] ) data_df [ 'c_month' ] = data_df [ 'created' ] . dt . month data_df [ 'c_day' ] = data_df [ 'created' ] . dt . day data_df [ 'c_hour' ] = data_df [ 'created' ] . dt . hour data_df [ 'c_dayofyear' ] = data_df [ 'created' ] . dt . dayofyear data_df . drop ( 'created' , axis = 1 , inplace = True )
292	for col in [ 'display_address' , 'street_address' , 'manager_id' , 'building_id' ] : data_df [ col ] = LabelEncoder ( ) . fit_transform ( data_df [ col ] ) data_df . drop ( 'description' , axis = 1 , inplace = True )
293	t4_params = { 'boosting_type' : 'gbdt' , 'objective' : 'multiclass' , 'nthread' : - 1 , 'silent' : True , 'num_leaves' : 2 ** 4 , 'learning_rate' : 0.05 , 'max_depth' : - 1 , 'max_bin' : 255 , 'subsample_for_bin' : 50000 , 'subsample' : 0.8 , 'subsample_freq' : 1 , 'colsample_bytree' : 0.6 , 'reg_alpha' : 1 , 'reg_lambda' : 0 , 'min_split_gain' : 0.5 , 'min_child_weight' : 1 , 'min_child_samples' : 10 , 'scale_pos_weight' : 1 } t4 = lgbm . sklearn . LGBMClassifier ( n_estimators = 1000 , seed = 0 , ** t4_params )
294	plt . figure ( figsize = ( 12 , 6 ) ) plt . title ( 'Number of Team Members' ) tmp = dataset . groupby ( [ 'matchId' , 'groupId' ] ) [ 'Id' ] . agg ( 'count' ) sns . countplot ( tmp )
295	oof_preds = np . zeros ( train_df . shape [ 0 ] ) sub_preds = np . zeros ( test_df . shape [ 0 ] ) feature_importance_df = pd . DataFrame ( ) drop_features = [ 'Id' , 'groupId' , 'matchId' , 'matchType' , 'winPlacePerc' ] feats = [ f for f in train_df . columns if f not in drop_features ]
296	sub = pd . DataFrame ( ) sub [ 'Id' ] = test_df [ 'Id' ] sub [ 'winPlacePerc' ] = lgb_sub_preds sub [ 'winPlacePerc' ] [ sub [ 'winPlacePerc' ] > 1 ] = 1 sub . to_csv ( 'lgb_submission.csv' , index = False )
297	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = 'all'
298	input_path = '/kaggle/input/widsdatathon2020' for dirpath , dirname , filenames in os . walk ( input_path ) : for name in filenames : print ( os . path . join ( dirpath , name ) )
299	X_train , X_valid , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) X_train . shape X_valid . shape
300	output = pd . DataFrame ( { 'encounter_id' : imputed_X_test . encounter_id , 'hospital_death' : preds } , dtype = np . int32 ) output . to_csv ( 'submission.csv' , index = False )
301	df = pd . read_csv ( '../input/train_labels.csv' ) print ( 'Shape of DataFrame' , df . shape ) df . head ( )
302	df [ df [ 'id' ] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' ] df [ df [ 'id' ] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' ] df . head ( )
303	y = df_train [ 'label' ] df_train , df_val = train_test_split ( df_train , test_size = 0.1 , random_state = 0 , stratify = y )
304	if target == 0 : label = '0' elif target == 1 : label = '1'
305	if target == 0 : label = '0' elif target == 1 : label = '1'
306	model = Net . build ( width = 96 , height = 96 , depth = 3 , classes = 2 ) from keras . optimizers import SGD , Adam , Adagrad model . compile ( optimizer = Adam ( lr = 0.0001 ) , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] )
307	model . load_weights ( 'checkpoint.h5' ) val_loss , val_acc = \ model . evaluate_generator ( test_gen , steps = len ( df_val ) ) print ( 'val_loss:' , val_loss ) print ( 'val_acc:' , val_acc )
308	model . load_weights ( 'checkpoint.h5' ) val_loss , val_acc = \ model . evaluate_generator ( test_gen , steps = len ( df_val ) ) print ( 'val_loss:' , val_loss ) print ( 'val_acc:' , val_acc )
309	test_filenames = test_gen . filenames df_preds [ 'file_names' ] = test_filenames def extract_id ( x ) : a = x . split ( '/' ) b = a [ 1 ] . split ( '.' ) extracted_id = b [ 0 ] return extracted_id df_preds [ 'id' ] = df_preds [ 'file_names' ] . apply ( extract_id ) df_preds . head ( )
310	b = a [ 1 ] . split ( '.' ) extracted_id = b [ 0 ] return extracted_id df_preds [ 'id' ] = df_preds [ 'file_names' ] . apply ( extract_id ) df_preds . head ( )
311	submission = pd . DataFrame ( { 'id' : image_id , 'label' : y_pred , } ) . set_index ( 'id' ) submission . to_csv ( 'submission.csv' , columns = [ 'label' ] )
312	top_buildings = train [ 'building_id' ] . value_counts ( ) . nlargest ( 10 ) print ( top_buildings ) print ( len ( train [ 'building_id' ] . unique ( ) ) ) grouped_building = train . groupby ( [ 'building_id' , 'interest_level' ] ) [ 'building_id' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) grouped_building [ 'sum' ] = grouped_building . sum ( axis = 1 ) x = grouped_building [ ( grouped_building [ 'sum' ] > 50 ) & ( grouped_building [ 'high' ] > 10 ) ]
313	plt . subplots ( figsize = ( 13 , 6 ) ) plt . title ( 'Feature ranking' , fontsize = 18 ) plt . ylabel ( 'Importance degree' , fontsize = 13 )
314	from numpy import sort from xgboost import XGBClassifier from sklearn . model_selection import train_test_split from sklearn . metrics import accuracy_score from sklearn . feature_selection import SelectFromModel
315	y_pred = model . predict ( X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( "Accuracy: %.2f%%" % ( accuracy * 100.0 ) )
316	sns . countplot ( train [ 'bathrooms' ] , ax = plt . subplot ( 121 ) ) ; plt . xlabel ( 'NB of bathrooms' , fontsize = 13 ) ; plt . ylabel ( 'NB of listings' , fontsize = 13 ) ; sns . countplot ( train [ 'bedrooms' ] , ax = plt . subplot ( 122 ) ) ; plt . xlabel ( 'NB of bedrooms' , fontsize = 13 ) ; plt . ylabel ( 'NB of listings' , fontsize = 13 ) ;
317	svm_model = svm . SVC ( decision_function_shape = 'ovo' , tol = 0.00000001 ) svm_model = svm_model . fit ( X_train , y_train ) print_scores ( "Support Vector Machine" , svm_model . score ( X_train , y_train ) , accuracy_score ( y_test , svm_model . predict ( X_test ) ) )
318	def submit ( predictions ) : submit = pd . read_csv ( '../input/sample_submission.csv' ) submit [ "target" ] = predictions submit . to_csv ( "submission.csv" , index = False ) def fallback_auc ( y_true , y_pred ) : try : return metrics . roc_auc_score ( y_true , y_pred ) except : return 0.5 def auc ( y_true , y_pred ) : return tf . py_function ( fallback_auc , ( y_true , y_pred ) , tf . double )
319	import numpy as np import pandas as pd import os import time import datetime import gc from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import GroupKFold from sklearn . metrics import mean_absolute_error import matplotlib . pyplot as plt import seaborn as sns from tqdm import tqdm_notebook as tqdm from catboost import CatBoostRegressor , Pool import warnings warnings . filterwarnings ( "ignore" )
320	train = pd . read_csv ( '../input/train.csv' ) structures = pd . read_csv ( '../input/structures.csv' ) print ( 'Train dataset shape is -> rows: {} cols:{}' . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( 'Structures dataset shape is -> rows: {} cols:{}' . format ( structures . shape [ 0 ] , structures . shape [ 1 ] ) )
321	def catboost_fit ( model , X_train , y_train , X_val , y_val ) : train_pool = Pool ( X_train , y_train ) val_pool = Pool ( X_val , y_val ) model . fit ( train_pool , eval_set = val_pool ) return model model = CatBoostRegressor ( iterations = 20000 , max_depth = 9 , objective = 'MAE' , task_type = 'GPU' , verbose = False ) model = catboost_fit ( model , tr_X , tr_y , val_X , val_y )
322	forest [ 'Cover_Type' ] . replace ( { 1 : 'Spruce/Fir' , 2 : 'Lodgepole Pine' , 3 : 'Ponderosa Pine' , 4 : 'Cottonwood/Willow' , 5 : 'Aspen' , 6 : 'Douglas-fir' , 7 : 'Krummholz' } , inplace = True ) forest = forest . rename ( columns = { "Wilderness_Area1" : "Rawah_WA" , "Wilderness_Area2" : "Neota_WA" , "Wilderness_Area3" : "Comanche_Peak_WA" , "Wilderness_Area4" : "Cache_la_Poudre_WA" , "Horizontal_Distance_To_Hydrology" : "HD_Hydrology" , "Vertical_Distance_To_Hydrology" : "VD_Hydrology" , "Horizontal_Distance_To_Roadways" : "HD_Roadways" , "Horizontal_Distance_To_Fire_Points" : "HD_Fire_Points" } )
323	lst = [ ] for value in forest [ 'Soil types' ] : value = value . replace ( 'Soil_Type' , "ST" ) lst . append ( value ) forest [ 'Soil types' ] = lst
324	fig = px . bar ( temp , x = "Wild Areas" , y = "Elevation" , color = 'Cover_Type' , barmode = 'group' , height = 400 , width = 900 ) fig . show ( ) fig = px . treemap ( temp , path = [ 'Wild Areas' , 'Cover_Type' ] , values = 'Elevation' , height = 400 , width = 900 ) fig . show ( ) temp . style . background_gradient ( cmap = 'plasma' )
325	fig = px . histogram ( forest , x = "HD_Hydrology" , color = "Cover_Type" , marginal = 'rug' , title = "HD_Hydrology Histogram" , height = 500 , width = 800 ) fig . show ( )
326	fig = px . histogram ( forest , x = "VD_Hydrology" , color = "Cover_Type" , marginal = 'rug' , title = "VD_Hydrology Histogram" , height = 500 , width = 800 ) fig . show ( )
327	fig = px . treemap ( temp , path = [ 'Wild Areas' , 'Cover_Type' ] , values = 'VD_Hydrology' , height = 400 , width = 800 ) fig . show ( ) temp . style . background_gradient ( cmap = "BuPu" )
328	fig = px . histogram ( forest , x = "HD_Roadways" , color = "Cover_Type" , marginal = 'rug' , title = "HD_Roadways Histogram" , height = 500 , width = 800 ) fig . show ( )
329	temp = forest . groupby ( [ 'Cover_Type' ] , as_index = False ) [ [ 'HD_Roadways' ] ] . median ( ) fig = px . bar ( temp . sort_values ( by = "HD_Roadways" , ascending = False ) , x = "HD_Roadways" , y = "Cover_Type" , color = 'Cover_Type' , orientation = 'h' , height = 300 , width = 900 ) fig . show ( )
330	temp = forest . groupby ( [ 'Cover_Type' ] , as_index = False ) [ [ 'HD_Roadways' ] ] . median ( ) fig = px . bar ( temp . sort_values ( by = "HD_Roadways" , ascending = False ) , x = "HD_Roadways" , y = "Cover_Type" , color = 'Cover_Type' , orientation = 'h' , height = 300 , width = 900 ) fig . show ( )
331	fig = px . histogram ( forest , x = "HD_Fire_Points" , color = "Cover_Type" , marginal = 'rug' , title = "HD Fire Points Histogram" , height = 500 , width = 800 ) fig . show ( )
332	fig = px . histogram ( forest , x = "Hillshade_Noon" , color = "Cover_Type" , marginal = 'box' , title = "Hillshade at Noon Histogram" , height = 500 , width = 800 ) fig . show ( )
333	import numpy as np , pandas as pd from sklearn . model_selection import StratifiedKFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import StandardScaler from sklearn . feature_selection import VarianceThreshold from sklearn . discriminant_analysis import QuadraticDiscriminantAnalysis from tqdm import tqdm_notebook import warnings warnings . filterwarnings ( 'ignore' )
334	batch_size = 16 lrG = 0.001 lrD = 0.001 beta1 = 0.5 epochs = 300 real_label = 0.5 fake_label = 0 nz = 128 device = torch . device ( "cuda" if torch . cuda . is_available ( ) else "cpu" )
335	x = next ( iter ( train_loader ) ) fig = plt . figure ( figsize = ( 25 , 16 ) ) for ii , img in enumerate ( x ) : ax = fig . add_subplot ( 4 , 8 , ii + 1 , xticks = [ ] , yticks = [ ] ) img = img . numpy ( ) . transpose ( 1 , 2 , 0 ) plt . imshow ( ( img + 1 ) / 2 )
336	MAGIC_N = 42 train_subset = train [ train [ 'wheezy-copper-turtle-magic' ] == MAGIC_N ] test_subset = test [ test [ 'wheezy-copper-turtle-magic' ] == MAGIC_N ] concated = pd . concat ( [ train_subset , test_subset ] ) a = train_subset . std ( ) > 1.2 cols = [ idx for idx in a . index if a [ idx ] ] concated = concated [ cols + [ 'target' ] ]
337	train = pd . read_csv ( BASE_FOLDER + "train.csv" ) test = pd . read_csv ( BASE_FOLDER + "test.csv" ) sub = pd . read_csv ( BASE_FOLDER + "sample_submission.csv" )
338	example = openslide . OpenSlide ( os . path . join ( BASE_FOLDER + "train_images" , '005e66f06bce9c2e49142536caf2f6ee.tiff' ) ) patch = example . read_region ( ( 17800 , 19500 ) , 0 , ( 256 , 256 ) ) display ( patch ) example . close ( )
339	pen_marked_images = [ 'fd6fe1a3985b17d067f2cb4d5bc1e6e1' , 'ebb6a080d72e09f6481721ef9f88c472' , 'ebb6d5ca45942536f78beb451ee43cc4' , 'ea9d52d65500acc9b9d89eb6b82cdcdf' , 'e726a8eac36c3d91c3c4f9edba8ba713' , 'e90abe191f61b6fed6d6781c8305fe4b' , 'fd0bb45eba479a7f7d953f41d574bf9f' , 'ff10f937c3d52eff6ad4dd733f2bc3ac' , 'feee2e895355a921f2b75b54debad328' , ] overlay_mask_on_slide ( pen_marked_images )
340	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt import torch from torch import nn , optim import torch . nn . functional as F from torchvision import datasets , transforms from torchvision . utils import save_image from torch . utils . data import Dataset , DataLoader from torch . autograd import Variable from PIL import Image from tqdm import tqdm_notebook as tqdm
341	x = next ( iter ( train_loader ) ) fig = plt . figure ( figsize = ( 25 , 16 ) ) for ii , img in enumerate ( x ) : ax = fig . add_subplot ( 4 , 8 , ii + 1 , xticks = [ ] , yticks = [ ] ) img = img . numpy ( ) . transpose ( 1 , 2 , 0 ) plt . imshow ( ( img + 1. ) / 2. )
342	from sklearn . model_selection import StratifiedKFold from sklearn . metrics import f1_score from torch . optim . optimizer import Optimizer from unidecode import unidecode
343	def seed_everything ( seed = 1029 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( )
344	def load_glove ( word_index ) : EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt' def get_coefs ( word , * arr ) : return word , np . asarray ( arr , dtype = 'float32' ) [ : 300 ] embeddings_index = dict ( get_coefs ( * o . split ( " " ) ) for o in open ( EMBEDDING_FILE ) ) all_embs = np . stack ( embeddings_index . values ( ) ) emb_mean , emb_std = - 0.005838499 , 0.48782197 embed_size = all_embs . shape [ 1 ]
345	df_train = pd . read_csv ( "../input/train.csv" ) df_test = pd . read_csv ( "../input/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
346	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( train_X ) ) train_X = tokenizer . texts_to_sequences ( train_X ) test_X = tokenizer . texts_to_sequences ( test_X )
347	np . random . seed ( SEED ) trn_idx = np . random . permutation ( len ( train_X ) ) train_X = train_X [ trn_idx ] train_y = train_y [ trn_idx ] features = features [ trn_idx ] return train_X , test_X , train_y , features , test_features , tokenizer . word_index
348	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
349	x_train = np . load ( "x_train.npy" ) x_test = np . load ( "x_test.npy" ) y_train = np . load ( "y_train.npy" ) features = np . load ( "features.npy" ) test_features = np . load ( "test_features.npy" ) word_index = np . load ( "word_index.npy" ) . item ( )
350	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings gc . collect ( ) np . shape ( embedding_matrix )
351	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings ] , axis = 0 )
352	def squash ( self , x , axis = - 1 ) : s_squared_norm = ( x ** 2 ) . sum ( axis , keepdim = True ) scale = t . sqrt ( s_squared_norm + T_epsilon ) return x / scale class Capsule_Main ( nn . Module ) : def __init__ ( self , embedding_matrix = None , vocab_size = None ) : super ( Capsule_Main , self ) . __init__ ( ) self . embed_layer = Embed_Layer ( embedding_matrix , vocab_size ) self . gru_layer = GRU_Layer ( )
353	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
354	loss_fn = torch . nn . BCEWithLogitsLoss ( reduction = 'sum' ) step_size = 300 base_lr , max_lr = 0.001 , 0.003 optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ( ) ) , lr = max_lr )
355	train_loader = torch . utils . data . DataLoader ( train , batch_size = batch_size , shuffle = True ) valid_loader = torch . utils . data . DataLoader ( valid , batch_size = batch_size , shuffle = False ) print ( f'Fold {i + 1}' ) for epoch in range ( n_epochs ) :
356	start_time = time . time ( ) model . train ( ) avg_loss = 0. for i , ( x_batch , y_batch , index ) in enumerate ( train_loader ) :
357	def __init__ ( self ) : self . reset ( ) def reset ( self ) : self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def update ( self , val , n = 1 ) : self . val = val self . sum += val * n self . count += n self . avg = self . sum / self . count
358	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) sub = pd . read_csv ( '../input/sample_submission.csv' )
359	print ( 'Train data: \nRows: {}\nCols: {}' . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( train . columns ) print ( '\nTest data: \nRows: {}\nCols: {}' . format ( test . shape [ 0 ] , test . shape [ 1 ] ) ) print ( test . columns ) print ( '\nSubmission data: \nRows: {}\nCols: {}' . format ( sub . shape [ 0 ] , sub . shape [ 1 ] ) ) print ( sub . columns )
360	wordcloud = WordCloud ( background_color = 'black' , stopwords = stopwords , max_words = max_words , max_font_size = max_font_size , random_state = 42 , width = 800 , height = 400 , mask = mask ) wordcloud . generate ( str ( text ) )
361	plt . figure ( figsize = figure_size ) if image_color : image_colors = ImageColorGenerator ( mask ) ; plt . imshow ( wordcloud . recolor ( color_func = image_colors ) , interpolation = "bilinear" ) ; plt . title ( title , fontdict = { 'size' : title_size , 'verticalalignment' : 'bottom' } ) else : plt . imshow ( wordcloud ) ; plt . title ( title , fontdict = { 'size' : title_size , 'color' : 'black' , 'verticalalignment' : 'bottom' } ) plt . axis ( 'off' ) ; plt . tight_layout ( )
362	embeddings_index = { } f = open ( '../input/embeddings/glove.840B.300d/glove.840B.300d.txt' ) for line in tqdm ( f ) : values = line . split ( " " ) word = values [ 0 ] coefs = np . asarray ( values [ 1 : ] , dtype = 'float32' ) embeddings_index [ word ] = coefs f . close ( ) print ( 'Found %s word vectors.' % len ( embeddings_index ) )
363	def text_to_array ( text ) : empyt_emb = np . zeros ( 300 ) text = text [ : - 1 ] . split ( ) [ : 30 ] embeds = [ embeddings_index . get ( x , empyt_emb ) for x in text ] embeds += [ empyt_emb ] * ( 30 - len ( embeds ) ) return np . array ( embeds )
364	model = Sequential ( ) model . add ( Bidirectional ( CuDNNLSTM ( 64 , return_sequences = True ) , input_shape = ( 30 , 300 ) ) ) model . add ( Bidirectional ( CuDNNLSTM ( 64 ) ) ) model . add ( Dense ( 1 , activation = "sigmoid" ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )
365	model . fit_generator ( mg , epochs = 10 , steps_per_epoch = 1000 , validation_data = ( val_vects , val_y ) , verbose = True )
366	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os merchants = pd . read_csv ( "../input/merchants.csv" )
367	merchants [ 'category_2' ] = merchants [ 'category_2' ] . fillna ( 0 ) . astype ( int ) merchants . loc [ merchants [ 'city_id' ] == - 1 , 'city_id' ] = 0 merchants . loc [ merchants [ 'state_id' ] == - 1 , 'state_id' ] = 0
368	def rating ( x ) : if np . isfinite ( x ) and x > 0 : x = ( 1 / x ) - 1 if x > 1 : r = 1 elif x <= 1 and x > 0 : r = 2 elif x == 0 : r = 3 elif x < 0 and x >= - 1 : r = 4 else : r = 5 else : r = 5 return r
369	merchants [ 'most_recent_sales_range' ] = merchants [ 'most_recent_sales_range' ] \ . map ( { 'A' : 1 , 'B' : 2 , 'C' : 3 , 'D' : 4 , 'E' : 5 } ) merchants [ 'most_recent_purchases_range' ] = merchants [ 'most_recent_purchases_range' ] \ . map ( { 'A' : 1 , 'B' : 2 , 'C' : 3 , 'D' : 4 , 'E' : 5 } )
370	a_merchant_id_group_m = marchant_id_groups_m . get_group ( 'M_ID_9b0ef314cf_400_7_231_9_1' ) merchants = merchants . drop ( indices_to_drop , axis = 0 ) merchants = merchants . reset_index ( drop = True ) print ( "%d observations for %d merchant ids. %d duplicates deleted" % ( total , len ( list ( marchant_id_groups_m . groups . keys ( ) ) ) , len ( indices_to_drop ) ) ) merchants = merchants . drop ( [ 'merchant_group_id' ] , axis = 1 ) merchants . head ( 10 )
371	train_ = train [ 'comment_text' ] test_ = test [ 'comment_text' ] alldata = pd . concat ( [ train_ , test_ ] , axis = 0 ) alldata = pd . DataFrame ( alldata )
372	params = { 'objective' : 'binary' , 'learning_rate' : 0.02 , 'num_leaves' : 76 , 'feature_fraction' : 0.64 , 'bagging_fraction' : 0.8 , 'bagging_freq' : 1 , 'boosting_type' : 'gbdt' , 'metric' : 'binary_logloss' }
373	print ( 'predicting for :' + j ) preds [ : , i ] = bst . predict ( countvec_df_test_ ) print ( 'Fininshed Training' )
374	col = [ 'toxic' , 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate' ] only_col = [ 'toxic' ] preds = np . zeros ( ( test_new . shape [ 0 ] , len ( col ) ) )
375	train_words = all_words [ : len ( train_new ) ] test_words = all_words [ len ( train_new ) : ] train_chars = all_chars [ : len ( train_new ) ] test_chars = all_chars [ len ( train_new ) : ]
376	for i , j in enumerate ( col ) : print ( '===Fit ' + j ) model = LogisticRegression ( C = 4.0 , solver = 'sag' ) print ( 'Fitting model' ) model . fit ( train_feats , train_new [ j ] ) print ( 'Predicting on test' ) preds [ : , i ] = model . predict_proba ( test_feats ) [ : , 1 ]
377	subm = pd . read_csv ( '../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv' ) submid = pd . DataFrame ( { 'id' : subm [ "id" ] } ) submission = pd . concat ( [ submid , pd . DataFrame ( preds , columns = col ) ] , axis = 1 ) submission . to_csv ( 'feat_lr_2cols.csv' , index = False )
378	from sklearn . preprocessing import LabelEncoder cat_features = [ 'Sex' , 'SmokingStatus' ] encoder = LabelEncoder ( ) encoded = data [ cat_features ] . apply ( encoder . fit_transform )
379	import matplotlib . pyplot as plt import seaborn as seabornInstance from sklearn . model_selection import train_test_split from sklearn . linear_model import LinearRegression from sklearn import metrics
380	test = pd . read_csv ( '../input/nomad2018-predict-transparent-conductors/test.csv' ) test_id = test . id train = pd . read_csv ( '../input/nomad2018-predict-transparent-conductors/train.csv' )
381	def get_prop_list ( path_to_element_data ) : return [ f [ : - 4 ] for f in os . listdir ( path_to_element_data ) ] path_to_element_data = '../input/elemental-properties/' properties = get_prop_list ( path_to_element_data ) print ( sorted ( properties ) )
382	for prop in properties : all_data [ '_' . join ( [ 'avg' , prop ] ) ] = avg_prop ( all_data [ 'x_Al' ] , all_data [ 'x_Ga' ] , all_data [ 'x_In' ] , prop )
383	lattice_angles = [ 'alpha' , 'beta' , 'gamma' ] for lang in lattice_angles : all_data [ '_' . join ( [ lang , 'r' ] ) ] = np . pi * all_data [ lang ] / 180
384	for col in [ 'x_Al' , 'x_Ga' , 'x_In' , 'a' , 'b' , 'c' , 'vol' , 'atomic_density' ] : for x in all_data . sg . unique ( ) : sns . distplot ( all_data [ all_data [ 'sg' ] == x ] [ col ] ) plt . title ( col ) plt . show ( )
385	for col in [ 'E' , 'Eg' ] : sns . distplot ( ( train [ col ] ) ) plt . title ( col ) plt . show ( )
386	for col in [ 'E' , 'Eg' ] : sns . distplot ( ( train [ col ] ) ) plt . title ( col ) plt . show ( )
387	features = [ 'x_Al' , 'x_Ga' , 'x_In' , 'a' , 'b' , 'c' , 'alpha' , 'beta' , 'gamma' , 'vol' , 'atomic_density' , 'x_Al_avg' , 'x_Ga_avg' , 'x_In_avg' , 'a_avg' , 'b_avg' , 'c_avg' , 'vol_avg' , 'atomic_density_avg' , 'pca_abc' , 'pca_AlGaInDensity' , 'O_0_0' , 'O_1_0' , 'O_2_0' , 'O_3_0' , 'O_4_0' , 'O_5_0' , 'Al_0_0' , 'Al_1_0' , 'Al_2_0' , 'Al_3_0' , 'Al_4_0' , 'Al_5_0' , 'Ga_0_0' , 'Ga_1_0' , 'Ga_2_0' , 'Ga_3_0' , 'Ga_4_0' , 'Ga_5_0' , 'In_0_0' , 'In_1_0' , 'In_2_0' , 'In_3_0' , 'In_4_0' , 'In_5_0' , ]
388	pca = PCA ( ) pca . fit ( vector1 ) all_data [ 'pca_abc' ] = pca . transform ( vector1 ) [ : , 0 ] pca = PCA ( ) pca . fit ( vector2 ) all_data [ 'pca_AlGaInDensity' ] = pca . transform ( vector2 ) [ : , 0 ]
389	def rmsle ( h , y ) : return np . sqrt ( np . square ( np . log ( h + 1 ) - np . log ( y + 1 ) ) . mean ( ) )
390	import keras import tensorflow as tf import keras . backend as K from keras . regularizers import l2 from keras . optimizers import Adam from keras . models import Model from keras . models import Sequential from keras . layers import Dense , Dropout , BatchNormalization , Input
391	def rmsle_K ( y , y0 ) : return K . sqrt ( K . mean ( K . square ( tf . log1p ( tf . expm1 ( y ) ) - tf . log1p ( tf . expm1 ( y0 ) ) ) ) ) def compile_model ( shape , lr = 0.001 ) : model = get_model ( shape ) optimizer = Adam ( lr = lr , decay = 0.001 ) model . compile ( optimizer = optimizer , loss = 'mse' , metrics = [ rmsle_K ] ) return model
392	grad_1 = GradientBoostingRegressor ( loss = 'ls' , learning_rate = 0.0035 , max_depth = 7 , n_estimators = 1120 , max_features = 7 , min_samples_leaf = 43 , min_samples_split = 14 , min_weight_fraction_leaf = 0.01556 ) grad_2 = GradientBoostingRegressor ( loss = 'ls' , learning_rate = 0.0035 , max_depth = 6 , n_estimators = 3275 , max_features = 2 , min_samples_leaf = 2 , min_samples_split = 2 , min_weight_fraction_leaf = 0.08012 ) def assess_grad ( X , y_list , model_list ) :
393	lgb_1 = lgb . LGBMRegressor ( objective = 'regression' , boosting_type = 'gbdt' , learning_rate = 0.002 , n_estimators = 2000 , num_threads = 3 , bagging_fraction = 0.56369 , bagging_freq = 14 , feature_fraction = 0.88868 , lambda_l2 = 0.0091689 , max_depth = 20 , ) lgb_2 = lgb . LGBMRegressor ( objective = 'regression' , boosting_type = 'gbdt' , learning_rate = 0.002 , n_estimators = 2838 , num_threads = 3 , bagging_fraction = 0.50173 , bagging_freq = 14 , feature_fraction = 0.62509 , lambda_l2 = 0.0086298 , max_depth = 20 , )
394	cat_1 = CatBoostRegressor ( iterations = 2300 , learning_rate = 0.020 , depth = 5 , loss_function = 'RMSE' , eval_metric = 'RMSE' , od_type = 'Iter' , od_wait = 50 , ) def assess_cat ( X , y_list , model_num ) :
395	final = [ ] best_iter = [ [ ] , [ ] ] for idx , y in enumerate ( y_list ) : kfold = KFold ( n_splits = 10 , shuffle = True ) out = [ ] for train_index , test_index in kfold . split ( X ) :
396	def fit ( self , X , y ) : self . base_models_ = [ list ( ) for x in self . base_models ] self . meta_model_ = clone ( self . meta_model ) kfold = KFold ( n_splits = self . n_folds , shuffle = True )
397	from sklearn . linear_model import LinearRegression from sklearn . base import clone import sklearn import xgboost import catboost import lightgbm class StackingStoppingAveragedModels ( ) : def __init__ ( self , base_models , meta_model , n_folds = 10 ) : self . base_models = base_models self . meta_model = meta_model self . n_folds = n_folds
398	def fit ( self , X , y ) : self . base_models_ = [ list ( ) for x in self . base_models ] self . meta_model_ = clone ( self . meta_model ) kfold = KFold ( n_splits = self . n_folds , shuffle = True )
399	stacked_1 = StackingAveragedModels ( base_models = [ xgb_1 , lgb_1 , grad_1 , cat_1 ] , meta_model = linear ) stack_1 = stacked_1 . fit ( X_scale , y1 ) stacked_2 = StackingAveragedModels ( base_models = [ xgb_2 , lgb_2 , grad_2 , cat_1 ] , meta_model = linear ) stack_2 = stacked_2 . fit ( X_scale , y2 )
400	fillna_column = { 'release_year' : 'mode' , 'release_month' : 'mode' , 'release_day' : 'mode' } for k , v in fillna_column . items ( ) : if v == 'mode' : fill = train [ k ] . mode ( ) [ 0 ] else : fill = v print ( k , ': ' , fill ) train [ k ] . fillna ( value = fill , inplace = True ) test [ k ] . fillna ( value = fill , inplace = True )
401	fig = plt . figure ( figsize = ( 10 , 10 ) ) train . groupby ( 'release_month' ) . agg ( 'mean' ) [ 'revenue' ] . plot ( kind = 'bar' , color = 'navy' , rot = 0 ) plt . ylabel ( 'Revenue (100 million dollars)' )
402	mask = np . zeros_like ( corr , dtype = np . bool ) mask [ np . triu_indices_from ( mask ) ] = True f , ax = plt . subplots ( figsize = ( 11 , 9 ) ) cmap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) sns . heatmap ( corr , annot = True , mask = mask , cmap = cmap , vmax = .3 , center = 0 , square = True , linewidths = .5 , cbar_kws = { "shrink" : .5 } )
403	dict_columns = [ 'belongs_to_collection' , 'genres' , 'production_companies' , 'production_countries' , 'spoken_languages' , 'Keywords' , 'cast' , 'crew' ] def text_to_dict ( df ) : for column in dict_columns : df [ column ] = df [ column ] . apply ( lambda x : { } if pd . isna ( x ) else ast . literal_eval ( x ) ) return df train = text_to_dict ( train ) test = text_to_dict ( test )
404	text = " " . join ( review for review in train . title ) print ( "There are {} words in the combination of all review." . format ( len ( text ) ) ) stopwords = set ( stopwords . words ( 'english' ) ) wordcloud = WordCloud ( stopwords = stopwords , background_color = "white" ) . generate ( text ) fig = plt . figure ( figsize = ( 11 , 7 ) ) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . title ( 'movie title in train data' ) plt . axis ( "off" ) plt . show ( )
405	fig = plt . figure ( figsize = ( 30 , 25 ) ) plt . subplot ( 221 ) train [ 'revenue' ] . plot ( kind = 'hist' , bins = 100 ) plt . title ( 'Distribution of Revenue' ) plt . xlabel ( 'Revenue' ) plt . subplot ( 222 ) np . log1p ( train [ 'revenue' ] ) . plot ( kind = 'hist' , bins = 100 ) plt . title ( 'Train Log Revenue Distribution' ) plt . xlabel ( 'Log Revenue' ) print ( 'Skew of revenue attribute: %0.1f' % skew ( train [ 'revenue' ] ) )
406	print ( 'Skew of train budget attribute: %0.1f' % skew ( train [ 'budget' ] ) ) print ( 'Skew of test budget attribute: %0.1f' % skew ( test [ 'budget' ] ) ) print ( 'Skew of train popularity attribute: %0.1f' % skew ( train [ 'popularity' ] ) ) print ( 'Skew of test popularity attribute: %0.1f' % skew ( test [ 'popularity' ] ) )
407	power_six = train . id [ train . budget > 1000 ] [ train . revenue < 100 ] for k in power_six : train . loc [ train [ 'id' ] == k , 'revenue' ] = train . loc [ train [ 'id' ] == k , 'revenue' ] * 1000000
408	train [ 'log_budget' ] = np . log1p ( train [ 'budget' ] ) test [ 'log_budget' ] = np . log1p ( test [ 'budget' ] ) train [ 'log_popularity' ] = np . log1p ( train [ 'popularity' ] ) test [ 'log_popularity' ] = np . log1p ( test [ 'popularity' ] )
409	X = train_new . drop ( [ 'id' , 'revenue' ] , axis = 1 ) y = np . log1p ( train_new [ 'revenue' ] ) X_test = test_new . drop ( [ 'id' ] , axis = 1 ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.2 )
410	opt_parameters = { 'random_state' : 501 , 'objective' : 'regression' , 'num_leaves' : 40 , 'min_data_in_leaf' : 15 , 'max_depth' : 4 , 'learning_rate' : 0.01 , 'boosting_type' : 'gbdt' } params [ 'learning_rate' ] = opt_parameters [ 'learning_rate' ] params [ 'max_depth' ] = opt_parameters [ 'max_depth' ] params [ 'num_leaves' ] = opt_parameters [ 'num_leaves' ] params [ 'min_data_in_leaf' ] = opt_parameters [ 'min_data_in_leaf' ]
411	icn_number = pd . read_csv ( '../input/trends-assessment-prediction/ICN_numbers.csv' ) icn_idx = { } for jj in ntwk_idx . keys ( ) : icn_idx [ jj ] = np . array ( icn_number . index [ icn_number . ICN_number . isin ( ntwk_idx [ jj ] ) ] )
412	name_matrix = { } for fnco in fnc10_cols : name_matrix [ fnco ] = ( [ np . int ( icn_mat_idx [ np . int ( i . split ( ')' ) [ 0 ] ) ] ) for i in fnco . split ( '(' ) [ 1 : ] ] )
413	con_matrix1 = np . zeros ( ( 53 , 53 ) ) con_matrix2 = np . zeros ( ( 53 , 53 ) ) for n in fnc10_cols : r_ , c_ = name_matrix [ n ] con_matrix1 [ c_ , r_ ] = fnc_10 . iloc [ 0 , : ] [ n ] con_matrix2 [ c_ , r_ ] = fnc_10 . iloc [ 1 , : ] [ n ]
414	for de in deciles : ax . axvline ( de , c = 'black' ) ax . set_title ( data ) axes . flatten ( ) [ - 1 ] . set_axis_off ( ) plt . tight_layout ( )
415	for sc in scores . columns [ : 5 ] : deciles = np . percentile ( scores . loc [ : , sc ] , [ 20 , 40 , 60 , 80 ] ) discr = np . digitize ( scores . loc [ : , sc ] , deciles ) scores . loc [ : , sc + '_discrete' ] = discr . astype ( str )
416	scores . loc [ : , 'stratify' ] = ( scores [ 'age_discrete' ] + '_' + scores [ 'domain1_var1_discrete' ] + '_' + scores [ 'domain2_var2_discrete' ] )
417	from sklearn . model_selection import train_test_split train_idx , _ = train_test_split ( scores . index , train_size = 0.2 , random_state = 223 , stratify = scores . stratify )
418	for de in deciles : ax . axvline ( de , c = 'black' ) ax . set_title ( data ) axes . flatten ( ) [ - 1 ] . set_axis_off ( ) plt . tight_layout ( )
419	from nilearn import input_data basc197_masker = input_data . NiftiLabelsMasker ( basc_197 , mask_img = brain_mask ) def load_matlab ( participant_id , masker , path = '../input/trends-assessment-prediction/fMRI_train/' ) : mat = np . array ( h5py . File ( f'{path}{participant_id}.mat' , mode = 'r' ) . get ( 'SM_feature' ) ) mat = masker . fit_transform ( nb . Nifti1Image ( mat . transpose ( [ 3 , 2 , 1 , 0 ] ) , affine = masker . mask_img . affine ) ) return mat . flatten ( )
420	components2 = pd . DataFrame ( components2 , index = scores_stat . index ) pca2_corr = [ ] for kk in range ( 20 ) : pca2_corr . append ( scores_stat [ [ 'age' , 'domain1_var1' , 'domain1_var2' , 'domain2_var1' , 'domain2_var2' ] ] . corrwith ( components2 . loc [ : , kk ] ) ) pca2_scorr = pd . concat ( pca2_corr , axis = 1 ) pca2_scorr
421	assert np . all ( fnc_train . index == loading_train . index ) assert np . all ( fnc_train . index == scores . index ) assert np . all ( loading_train . index == scores . index )
422	def absolute_normalized_error ( y_true , y_pred , multioutput ) : output_errors = np . sum ( np . abs ( y_pred - y_true ) , axis = 0 ) / np . sum ( y_pred , axis = 0 ) return np . average ( output_errors , weights = multioutput )
423	test_index = sample_submission . Id . str . split ( '_' , expand = True ) [ 0 ] . unique ( ) . astype ( 'int' ) fnc_test = fnc . loc [ test_index , : ] loading_test = loading . loc [ test_index , : ]
424	model = sm . PHReg ( ys , xs , cs ) result = model . fit ( ) baseline_cum_hazard_func = result . baseline_cumulative_hazard_function [ 0 ] pred_index = np . arange ( - 99 , 100 )
425	from skimage import measure , morphology from skimage . morphology import ball , binary_closing from skimage . measure import label , regionprops
426	train_csv [ 'Image ID' ] = train_csv [ 'ID' ] . apply ( lambda x : x . split ( '_' ) [ 1 ] ) train_csv [ 'Sub-type' ] = train_csv [ 'ID' ] . apply ( lambda x : x . split ( '_' ) [ 2 ] ) train_csv = pd . pivot_table ( train_csv , index = 'Image ID' , columns = 'Sub-type' )
427	def image_to_hu ( image_path , image_id ) : dicom = pydicom . read_file ( image_path + 'ID_' + image_id + '.dcm' ) image = dicom . pixel_array . astype ( np . float64 ) intercept = dicom . RescaleIntercept slope = dicom . RescaleSlope if slope != 1 : image = slope * image . astype ( np . float64 ) image = image . astype ( np . float64 ) image += np . float64 ( intercept ) image [ image < - 1024 ] = - 1024 return image , dicom
428	spacing = map ( float , dicom_header . PixelSpacing ) spacing = np . array ( list ( spacing ) ) resize_factor = spacing / new_spacing new_real_shape = image . shape * resize_factor new_shape = np . round ( new_real_shape ) real_resize_factor = new_shape / image . shape new_spacing = spacing / real_resize_factor image = scipy . ndimage . interpolation . zoom ( image , real_resize_factor ) return image
429	hu_img = image_windowed ( hu_img ) resamp_img = image_windowed ( resamp_img ) plt . figure ( figsize = ( 7.5 , 5 ) ) plt . subplot ( 121 ) plt . imshow ( hu_img , cmap = 'bone' ) plt . axis ( 'off' ) plt . title ( f'Orig shape\n{hu_img.shape}' ) plt . subplot ( 122 ) plt . imshow ( resamp_img , cmap = 'bone' ) plt . title ( f'New shape\n{resamp_img.shape}' ) ; plt . axis ( 'off' ) ;
430	coords = np . array ( np . nonzero ( ~ mask ) ) top_left = np . min ( coords , axis = 1 ) bottom_right = np . max ( coords , axis = 1 ) out = image [ top_left [ 0 ] : bottom_right [ 0 ] , top_left [ 1 ] : bottom_right [ 1 ] ] return out
431	def image_pad ( image , new_height , new_width ) : height , width = image . shape im_bg = np . zeros ( ( new_height , new_width ) ) pad_left = int ( ( new_width - width ) / 2 ) pad_top = int ( ( new_height - height ) / 2 ) im_bg [ pad_top : pad_top + height , pad_left : pad_left + width ] = image return im_bg
432	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from itertools import cycle color_cycle = cycle ( plt . rcParams [ 'axes.prop_cycle' ] . by_key ( ) [ 'color' ] ) import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
433	k = 10 cols = corrmat . nlargest ( k , 'cases' ) [ 'cases' ] . index cm = np . corrcoef ( train [ cols ] . values . T ) sns . set ( font_scale = 1.25 ) hm = sns . heatmap ( cm , cbar = True , annot = True , square = True , fmt = '.2f' , annot_kws = { 'size' : 10 } , yticklabels = cols . values , xticklabels = cols . values ) plt . show ( )
434	sns . distplot ( train [ 'ed' ] , fit = norm ) ; fig = plt . figure ( ) res = stats . probplot ( train [ 'ed' ] , plot = plt ) train [ 'ed' ] = np . log ( train [ 'ed' ] ) test [ 'ed' ] = np . log ( test [ 'ed' ] ) res = stats . probplot ( train [ 'ed' ] , plot = plt )
435	for model in self . models_ : model . fit ( X , y ) return self
436	def predict ( self , X ) : predictions = np . column_stack ( [ model . predict ( X ) for model in self . models_ ] ) return np . mean ( predictions , axis = 1 )
437	def fit ( self , X , y ) : self . base_models_ = [ list ( ) for x in self . base_models ] self . meta_model_ = clone ( self . meta_model ) kfold = KFold ( n_splits = self . n_folds , shuffle = True , random_state = 156 )
438	TRAIN_ERROR_IDS = [ '7b38c9173ebe69b4c6ba7e703c0c27f39305d9b2910f46405993d2ea7a963b80' ]
439	def image_ids_in ( root_dir , is_train_data = False ) : ids = [ ] for id in os . listdir ( root_dir ) : if id in TRAIN_ERROR_IDS : print ( 'Skipping ID due to bad training data:' , id ) else : ids . append ( id ) return ids TRAIN_IMAGE_IDS = image_ids_in ( TRAIN_DIR , is_train_data = True ) TEST_IMAGE_IDS = image_ids_in ( TEST_DIR ) print ( 'Examples:' , TRAIN_IMAGE_IDS [ 22 ] , TEST_IMAGE_IDS [ 22 ] )
440	import pandas as pd from sklearn . feature_extraction . text import CountVectorizer from nltk . corpus import stopwords from sklearn import preprocessing from sklearn . naive_bayes import MultinomialNB
441	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) sample = pd . read_csv ( '../input/sample_submission.csv' ) train . head ( 3 )
442	xtrain_ctv_all = ctv . fit_transform ( train . text . values ) xtest_ctv_all = ctv . transform ( test . text . values ) clf = MultinomialNB ( alpha = 1.0 ) clf . fit ( xtrain_ctv_all , y )
443	train_text = train_data [ 'question_text' ] test_text = test_data [ 'question_text' ] train_target = train_data [ 'target' ] all_text = train_text . append ( test_text )
444	import os , gc import datetime import numpy as np import pandas as pd import category_encoders from sklearn . impute import SimpleImputer from sklearn . metrics import mean_squared_error from sklearn . model_selection import KFold from sklearn . preprocessing import LabelEncoder from sklearn . linear_model import Lasso from sklearn . linear_model import Ridge from lightgbm import LGBMRegressor from mlxtend . regressor import StackingRegressor from pandas . api . types import is_categorical_dtype from pandas . api . types import is_datetime64_any_dtype as is_datetime
445	train = train . merge ( metadata , on = 'building_id' , how = 'left' ) train = train . merge ( weather_train , on = [ 'site_id' , 'timestamp' ] , how = 'left' ) del weather_train ; gc . collect ( )
446	target = np . log1p ( train [ 'meter_reading' ] ) features = train . drop ( [ 'meter_reading' ] , axis = 1 ) del train ; gc . collect ( )
447	categorical_features = [ 'building_id' , 'site_id' , 'meter' , 'primary_use' ] encoder = category_encoders . CountEncoder ( cols = categorical_features ) encoder . fit ( features ) features = encoder . transform ( features ) features_size = features . shape [ 0 ] for feature in categorical_features : features [ feature ] = features [ feature ] / features_size
448	imputer = SimpleImputer ( missing_values = np . nan , strategy = 'median' ) imputer . fit ( features ) features = imputer . transform ( features )
449	test = test . merge ( metadata , on = 'building_id' , how = 'left' ) test = test . merge ( weather_test , on = [ 'site_id' , 'timestamp' ] , how = 'left' ) del metadata ; gc . collect ( )
450	test = data_parser ( test ) test = encoder . transform ( test ) for feature in categorical_features : test [ feature ] = test [ feature ] / features_size test = imputer . transform ( test )
451	predictions = 0 for model in models : predictions += np . expm1 ( model . predict ( np . array ( test ) ) ) / len ( models ) del model ; gc . collect ( ) del test , models ; gc . collect ( )
452	submission = pd . DataFrame ( { 'row_id' : row_ids , 'meter_reading' : np . clip ( predictions , 0 , a_max = None ) } ) submission . to_csv ( 'submission.csv' , index = False , float_format = '%.4f' )
453	train . signal . plot ( ) plt . title ( 'Train' ) plt . show ( ) test . signal . plot ( ) plt . title ( 'Test' ) plt . show ( ) train . to_csv ( "train_synthetic.csv" , index = False , float_format = '%.4f' ) test . to_csv ( "test_synthetic.csv" , index = False , float_format = '%.4f' )
454	batch_size = 64 gen = ImageDataGenerator ( horizontal_flip = True , vertical_flip = True , width_shift_range = 0.1 , height_shift_range = 0.1 , zoom_range = 0.1 , rotation_range = 10 )
455	tagged_df = pd . DataFrame ( tagged_df . tags . str . split ( ' ' ) . tolist ( ) , index = tagged_df . image_name ) . stack ( ) tagged_df = tagged_df . reset_index ( ) [ [ 0 , 'image_name' ] ] tagged_df . columns = [ 'tags' , 'image_name' ] tagged_df . set_index ( 'image_name' , inplace = True )
456	tagged_df = pd . get_dummies ( tagged_df [ 'tags' ] ) tagged_df = tagged_df . groupby ( tagged_df . index ) . sum ( ) train_imgs = [ ] labels = [ ] im_names = [ ] print ( 'Loading {} image dataset' . format ( ftype ) ) path = os . path . join ( '..' , 'input' , 'train-{}' . format ( ftype ) , '*.' + ftype ) files = glob . glob ( path ) for fs in files [ 1 : n ] : img = imread ( fs )
457	def plot_sample_size ( tagged_df ) : plt . rcParams [ 'figure.figsize' ] = ( 12 , 5 ) print ( 'There are {} unique tags in this data' . format ( len ( tagged_df . columns ) ) ) colors = cm . rainbow ( np . linspace ( 0 , 1 , len ( tagged_df . columns ) ) ) tagged_df . sum ( ) . sort_values ( ascending = False ) . plot ( title = "Counts of Tags" , color = colors , kind = 'bar' ) plt . show ( ) plot_sample_size ( tagged_df )
458	m = img [ : , : , 0 ] . flatten ( ) . mean ( ) left = img [ : , : , 0 ] . flatten ( ) [ img [ : , : , 0 ] . flatten ( ) < m ] right = img [ : , : , 0 ] . flatten ( ) [ img [ : , : , 0 ] . flatten ( ) >= m ]
459	m = img [ : , : , 0 ] . flatten ( ) . mean ( ) left = img [ : , : , 0 ] . flatten ( ) [ img [ : , : , 0 ] . flatten ( ) < m ] right = img [ : , : , 0 ] . flatten ( ) [ img [ : , : , 0 ] . flatten ( ) >= m ]
460	mo1 = np . histogram ( right , bins = steps , density = False ) [ 1 ] [ max_ind_right ] mo2 = np . histogram ( left , bins = steps , density = False ) [ 1 ] [ max_ind_left ] mods_diff_r = abs ( mo1 - mo2 )
461	from sklearn . model_selection import train_test_split X_train , X_validation , y_train , y_validation = train_test_split ( X , y [ 0 : 99 ] , test_size = 0.40 , random_state = 14113 ) print ( 'X_train is a {} object' . format ( type ( X_train ) ) ) print ( 'it has shape {}' . format ( X_train . shape ) ) print ( 'y_train is a {} object' . format ( type ( y_train ) ) ) print ( 'it has {} elements' . format ( len ( y_train ) ) )
462	import warnings warnings . filterwarnings ( "ignore" ) import numpy as np import pandas as pd from datetime import datetime import matplotlib . pyplot as plt import seaborn as sns from scipy import stats import itertools from sklearn import model_selection from sklearn . ensemble import RandomForestRegressor from sklearn import metrics
463	import numpy as np import pandas as pd from datetime import datetime
464	import matplotlib . pyplot as plt import seaborn as sns from scipy import stats import itertools
465	df_train = pd . read_csv ( "../input/train.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False ) df_store = pd . read_csv ( "../input/store.csv" , low_memory = False )
466	df_train = pd . read_csv ( "../input/train.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False )
467	df_test = pd . read_csv ( "../input/test.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False ) print ( "The Test dataset has {} Rows and {} Variables" . format ( str ( df_test . shape [ 0 ] ) , str ( df_test . shape [ 1 ] ) ) )
468	rfr_val = RandomForestRegressor ( n_estimators = 128 , criterion = 'mse' , max_depth = 20 , min_samples_split = 10 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = 4 , random_state = 35 , verbose = 0 , warm_start = False ) model_RF_test = rfr_val . fit ( X_train , y_train )
469	features_ranked = [ ] for f in range ( X_train . shape [ 1 ] ) : features_ranked . append ( X_train . columns [ indices [ f ] ] )
470	plt . figure ( figsize = ( 10 , 15 ) ) plt . title ( "Feature importances" ) plt . barh ( range ( X_train . shape [ 1 ] ) , importances [ indices ] , color = [ next ( palette1 ) ] , align = "center" ) plt . yticks ( range ( X_train . shape [ 1 ] ) , features_ranked ) plt . ylabel ( 'Features' ) plt . ylim ( [ - 1 , X_train . shape [ 1 ] ] ) plt . show ( )
471	main = BatchNormalization ( ) ( max_emb ) main = Dense ( 64 ) ( main ) main = Dropout ( 0.5 ) ( main )
472	test = test env . predict ( test_prediction ) print ( n )
473	back_X = running_back [ 'X' ] . values [ 0 ] back_Y = running_back [ 'Y' ] . values [ 0 ] df [ 'dist_to_back' ] = ( ( df [ 'X' ] - back_X ) ** 2 + ( df [ 'Y' ] - back_Y ) ** 2 ) ** ( 1 / 2 ) return df train = pd . read_csv ( '/kaggle/input/nfl-big-data-bowl-2020/train.csv' , low_memory = False )
474	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib . pyplot as plt import json import os from tqdm . autonotebook import tqdm from pathlib import Path
475	from gluonts . model . deepar import DeepAREstimator from gluonts . distribution . neg_binomial import NegativeBinomialOutput from gluonts . trainer import Trainer estimator = DeepAREstimator ( prediction_length = prediction_length , freq = "D" , distr_output = NegativeBinomialOutput ( ) , use_feat_dynamic_real = True , use_feat_static_cat = True , cardinality = stat_cat_cardinalities , trainer = Trainer ( learning_rate = 1e-3 , epochs = 100 , num_batches_per_epoch = 50 , batch_size = 32 ) ) predictor = estimator . train ( train_ds )
476	if submission == True : forecasts_acc_sub = np . zeros ( ( len ( forecasts ) * 2 , single_prediction_length ) ) forecasts_acc_sub [ : len ( forecasts ) ] = forecasts_acc [ : , : single_prediction_length ] forecasts_acc_sub [ len ( forecasts ) : ] = forecasts_acc [ : , single_prediction_length : ]
477	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib . pyplot as plt import json import os from tqdm . autonotebook import tqdm from pathlib import Path
478	from gluonts . model . deepar import DeepAREstimator from gluonts . distribution . neg_binomial import NegativeBinomialOutput from gluonts . trainer import Trainer estimator = DeepAREstimator ( prediction_length = prediction_length , freq = "D" , distr_output = NegativeBinomialOutput ( ) , use_feat_dynamic_real = True , use_feat_static_cat = True , cardinality = stat_cat_cardinalities , trainer = Trainer ( learning_rate = 1e-3 , epochs = 100 , num_batches_per_epoch = 200 , batch_size = 100 ) ) predictor = estimator . train ( train_ds )
479	if use_keras_pad : sentences_padded = pad_sequences ( sentences , dtype = object , maxlen = _maxlen , value = [ '[PAD]' ] , padding = 'post' ) else : sentences_padded = [ tokens + [ "[PAD]" ] * ( _maxlen - len ( tokens ) ) if len ( tokens ) != _maxlen else tokens for tokens in sentences ] sentences_converted = [ tokenizer . convert_tokens_to_ids ( s ) for s in sentences_padded ]
480	pred_masks = np . zeros ( targ_masks . shape ) for ind , orig_mask in enumerate ( targ_masks ) : aug_mask = ndimage . rotate ( orig_mask , ind * 1.5 , mode = 'constant' , reshape = False , order = 0 ) pred_masks [ ind ] = ndimage . binary_dilation ( aug_mask , iterations = 1 )
481	ax . set_xticks ( np . arange ( - .5 , 7 , 1 ) , minor = True ) ; ax . set_yticks ( np . arange ( - .5 , 7 , 1 ) , minor = True ) ; ax . grid ( which = 'minor' , color = 'w' , linestyle = '-' , linewidth = 1 ) plt . tight_layout ( ) plt . show ( )
482	model . add ( Conv2D ( 64 , kernel_size = 3 , activation = 'relu' , input_shape = ( 20 , 10 , 1 ) ) ) model . add ( Conv2D ( 32 , kernel_size = 3 , activation = 'relu' ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) )
483	model . fit ( X_train , y_train , batch_size = 100 , validation_data = ( X_valid , y_valid ) , epochs = 10 ) y_pred = model . predict_classes ( X_valid ) oof = model . predict_classes ( X_valid )
484	print ( 'Original image shape: {}' . format ( im . shape ) ) from skimage . color import rgb2gray im_gray = rgb2gray ( im ) print ( 'New image shape: {}' . format ( im_gray . shape ) )
485	from scipy import ndimage labels , nlabels = ndimage . label ( mask ) label_arrays = [ ] for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) label_arrays . append ( label_mask ) print ( 'There are {} separate components / objects detected.' . format ( nlabels ) )
486	from matplotlib . colors import ListedColormap rand_cmap = ListedColormap ( np . random . rand ( 256 , 3 ) ) labels_for_display = np . where ( labels > 0 , labels , np . nan ) plt . imshow ( im_gray , cmap = 'gray' ) plt . imshow ( labels_for_display , cmap = rand_cmap ) plt . axis ( 'off' ) plt . title ( 'Labeled Cells ({} Nuclei)' . format ( nlabels ) ) plt . show ( )
487	if np . product ( cell . shape ) < 10 : print ( 'Label {} is too small! Setting to 0.' . format ( label_ind ) ) mask = np . where ( labels == label_ind + 1 , 0 , mask )
488	two_cell_indices = ndimage . find_objects ( labels ) [ 1 ] cell_mask = mask [ two_cell_indices ] cell_mask_opened = ndimage . binary_opening ( cell_mask , iterations = 8 )
489	def rle_encoding ( x ) : dots = np . where ( x . T . flatten ( ) == 1 ) [ 0 ] run_lengths = [ ] prev = - 2 for b in dots : if ( b > prev + 1 ) : run_lengths . extend ( ( b + 1 , 0 ) ) run_lengths [ - 1 ] += 1 prev = b return " " . join ( [ str ( i ) for i in run_lengths ] ) print ( 'RLE Encoding for the current mask is: {}' . format ( rle_encoding ( label_mask ) ) )
490	im_id = im_path . parts [ - 3 ] im = imageio . imread ( str ( im_path ) ) im_gray = rgb2gray ( im )
491	thresh_val = threshold_otsu ( im_gray ) mask = np . where ( im_gray > thresh_val , 1 , 0 ) if np . sum ( mask == 0 ) < np . sum ( mask == 1 ) : mask = np . where ( mask , 0 , 1 ) labels , nlabels = ndimage . label ( mask ) labels , nlabels = ndimage . label ( mask )
492	im_df = pd . DataFrame ( ) for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) if label_mask . flatten ( ) . sum ( ) > 10 : rle = rle_encoding ( label_mask ) s = pd . Series ( { 'ImageId' : im_id , 'EncodedPixels' : rle } ) im_df = im_df . append ( s , ignore_index = True ) return im_df def analyze_list_of_images ( im_path_list ) :
493	import numpy as np import pandas as pd import matplotlib . pyplot as plt df_train = pd . read_csv ( '../input/train.csv' ) subset = df_train . loc [ ( df_train [ 'crew' ] == 1 ) & ( df_train [ 'experiment' ] == 'CA' ) ] subset . sort_values ( by = 'time' ) plt . plot ( subset [ 'r' ] [ 3000 : 4024 ] )
494	DATASET_COLUMNS = [ "sentiment" , "ids" , "date" , "flag" , "user" , "text" ] DATASET_ENCODING = "ISO-8859-1" dataset = pd . read_csv ( '../input/sentiment140/training.1600000.processed.noemoticon.csv' , encoding = DATASET_ENCODING , names = DATASET_COLUMNS )
495	ax = dataset . groupby ( 'sentiment' ) . count ( ) . plot ( kind = 'bar' , title = 'Distribution of data' , legend = False ) ax . set_xticklabels ( [ 'Negative' , 'Positive' ] , rotation = 0 )
496	urlPattern = r"((http://)[^ ]*|(https://)[^ ]*|( www\.)[^ ]*)" userPattern = '@[^\s]+' alphaPattern = "[^a-zA-Z0-9]" sequencePattern = r"(.)\1\1+" seqReplacePattern = r"\1\1" for tweet in textdata : tweet = tweet . lower ( )
497	word = wordLemm . lemmatize ( word ) tweetwords += ( word + ' ' ) processedText . append ( tweetwords ) return processedText
498	X_train , X_test , y_train , y_test = train_test_split ( processedtext , sentiment , test_size = 0.05 , random_state = 0 ) print ( f'Data Split done.' )
499	X_train = vectoriser . transform ( X_train ) X_test = vectoriser . transform ( X_test ) print ( f'Data Transformed.' )
500	LRmodel = LogisticRegression ( C = 2 , max_iter = 1000 , n_jobs = - 1 ) LRmodel . fit ( X_train , y_train ) model_Evaluate ( LRmodel )
501	file = open ( 'vectoriser-ngram-(1,2).pickle' , 'wb' ) pickle . dump ( vectoriser , file ) file . close ( ) file = open ( 'Sentiment-LR.pickle' , 'wb' ) pickle . dump ( LRmodel , file ) file . close ( ) file = open ( 'Sentiment-BNB.pickle' , 'wb' ) pickle . dump ( BNBmodel , file ) file . close ( )
502	file = open ( '..path/vectoriser-ngram-(1,2).pickle' , 'rb' ) vectoriser = pickle . load ( file ) file . close ( )
503	file = open ( '..path/Sentiment-LRv1.pickle' , 'rb' ) LRmodel = pickle . load ( file ) file . close ( ) return vectoriser , LRmodel def predict ( vectoriser , model , text ) :
504	data = [ ] for text , pred in zip ( text , sentiment ) : data . append ( ( text , pred ) )
505	df = pd . DataFrame ( data , columns = [ 'text' , 'sentiment' ] ) df = df . replace ( [ 0 , 1 ] , [ "Negative" , "Positive" ] ) return df if __name__ == "__main__" :
506	text = [ "I hate twitter" , "May the Force be with you." , "Mr. Stark, I don't feel so good" ] df = predict ( vectoriser , LRmodel , text ) print ( df . head ( ) )
507	import os import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
508	concat_sub [ 'ieee_max' ] = concat_sub . iloc [ : , 1 : ncol ] . max ( axis = 1 ) concat_sub [ 'ieee_min' ] = concat_sub . iloc [ : , 1 : ncol ] . min ( axis = 1 ) concat_sub [ 'ieee_mean' ] = concat_sub . iloc [ : , 1 : ncol ] . mean ( axis = 1 ) concat_sub [ 'ieee_median' ] = concat_sub . iloc [ : , 1 : ncol ] . median ( axis = 1 )
509	X = librosa . stft ( data ) Xdb = librosa . amplitude_to_db ( abs ( X ) ) plt . figure ( figsize = ( 14 , 5 ) ) librosa . display . specshow ( Xdb , sr = sr , x_axis = 'time' , y_axis = 'hz' ) plt . colorbar ( )
510	plt . figure ( figsize = ( 14 , 5 ) ) plt . title = "Log Spectrom" , librosa . display . specshow ( Xdb , sr = sr , x_axis = 'time' , y_axis = 'log' ) plt . colorbar ( )
511	n0 = 10000 n1 = 10100 plt . figure ( figsize = ( 14 , 5 ) ) plt . plot ( data [ n0 : n1 ] ) plt . grid ( )
512	n0 = 10000 n1 = 10100 plt . figure ( figsize = ( 14 , 5 ) ) plt . plot ( data [ n0 : n1 ] ) plt . grid ( )
513	plt . figure ( figsize = ( 14 , 5 ) ) librosa . display . waveplot ( data , sr = sr , alpha = 0.4 ) plt . plot ( t , normalize ( spectral_centroids ) , color = 'r' )
514	for i , axial_slice in enumerate ( binary_image ) : axial_slice = axial_slice - 1 labeling = measure . label ( axial_slice ) l_max = largest_label_volume ( labeling , bg = 0 ) if l_max is not None : binary_image [ i ] [ labeling != l_max ] = 1 binary_image -= 1 binary_image = 1 - binary_image
515	labels = measure . label ( binary_image , background = 0 ) l_max = largest_label_volume ( labels , bg = 0 ) if l_max is not None : binary_image [ labels != l_max ] = 0 return binary_image
516	copied_pixels = copy . deepcopy ( imgs ) for i , mask in enumerate ( segmented_lungs_fill ) : get_high_vals = mask == 0 copied_pixels [ i ] [ get_high_vals ] = 0 seg_lung_pixels = copied_pixels
517	def make_lungmask ( img , display = False ) : row_size = img . shape [ 0 ] col_size = img . shape [ 1 ] mean = np . mean ( img ) std = np . std ( img ) img = img - mean img = img / std
518	middle = img [ int ( col_size / 5 ) : int ( col_size / 5 * 4 ) , int ( row_size / 5 ) : int ( row_size / 5 * 4 ) ] mean = np . mean ( middle ) max = np . max ( img ) min = np . min ( img )
519	title_grp = train_labels . groupby ( [ 'title' ] ) [ 'game_session' ] . count ( ) . reset_index ( ) display ( title_grp ) fig = go . Figure ( data = [ go . Pie ( labels = title_grp . title , values = title_grp . game_session ) ] ) fig . show ( )
520	print ( "Qualitative/Categorical Columns:" ) cate_cols = train_data . select_dtypes ( include = [ 'object' ] ) . columns print ( cate_cols ) print ( "\nQuntitative/Numerical Columns:" ) num_cols = train_data . select_dtypes ( exclude = [ 'object' ] ) . columns print ( num_cols )
521	train_data . groupby ( 'game_session' ) [ 'event_count' ] . count ( ) game = train_data [ train_data [ 'game_session' ] == 'fffe9cd0cc5b076b' ] game . shape
522	event_count = test_data [ 'title' ] . value_counts ( ) . reset_index ( ) event_count [ 'index' ] = event_count [ 'index' ] . astype ( 'category' ) fig = px . bar ( event_count [ 0 : 10 ] , x = 'index' , y = 'title' , hover_data = [ 'title' ] , color = 'index' , labels = { 'title' : 'Event Count' } , height = 400 ) fig . show ( )
523	type_count = test_data [ 'type' ] . value_counts ( ) . reset_index ( ) total = len ( test_data ) type_count [ 'percent' ] = round ( ( type_count [ 'type' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'type' , hover_data = [ 'index' , 'percent' ] , color = 'type' , labels = { 'type' : 'Type Count' } , height = 400 ) fig . show ( )
524	type_count = train_data [ 'world' ] . value_counts ( ) . reset_index ( ) total = len ( train_data ) type_count [ 'percent' ] = round ( ( type_count [ 'world' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'world' , hover_data = [ 'index' , 'percent' ] , color = 'world' , labels = { 'world' : 'World Count' } , height = 400 ) fig . show ( )
525	type_count = test_data [ 'world' ] . value_counts ( ) . reset_index ( ) total = len ( test_data ) type_count [ 'percent' ] = round ( ( type_count [ 'world' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'world' , hover_data = [ 'index' , 'percent' ] , color = 'world' , labels = { 'world' : 'World Count' } , height = 400 ) fig . show ( )
526	date_count = train_data . groupby ( [ 'date' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = go . Figure ( data = go . Scatter ( x = date_count [ 'date' ] , y = date_count [ 'installation_id' ] ) ) fig . show ( )
527	week_type_count = train_data . groupby ( [ 'weekofyear' , 'type' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = px . line ( week_type_count , x = "weekofyear" , y = "installation_id" , color = 'type' ) fig . show ( )
528	date_title_count = test_data . groupby ( [ 'title' ] ) [ 'game_time' ] . count ( ) . reset_index ( ) date_title_count . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = True ) print ( date_title_count ) fig = px . line ( date_title_count , x = "title" , y = "game_time" ) fig . show ( )
529	data = train_data [ train_data [ 'event_code' ] == 4030 ] data . shape print ( data [ 'title' ] . value_counts ( ) ) print ( data [ 'type' ] . value_counts ( ) ) print ( data [ 'world' ] . value_counts ( ) )
530	game = train_data . groupby ( [ 'title' ] ) [ 'game_time' ] . max ( ) . reset_index ( ) game . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = False ) fig = px . bar ( game [ 0 : 10 ] , x = 'game_time' , y = 'title' , orientation = 'h' , hover_data = [ 'title' ] , color = 'game_time' , labels = { 'game_time' : 'Game Time' } , height = 400 ) fig . show ( )
531	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'game_time' ] . mean ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "game_time" , hue = "type" , data = world_type )
532	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'event_count' ] . count ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "event_count" , hue = "type" , data = world_type )
533	def get_unique ( data , feat ) : return data [ feat ] . nunique ( ) for col in train_labels . columns . values : print ( "unique number of values in " , col ) print ( get_unique ( train_labels , col ) )
534	def get_unique ( data , feat ) : return data [ feat ] . nunique ( ) for col in train_labels . columns . values : print ( "unique number of values in " , col ) print ( get_unique ( train_labels , col ) )
535	dd = test_data . groupby ( 'date' ) [ 'world' ] . value_counts ( ) dd = dd . reset_index ( name = 'count' ) dd fig = px . line ( dd , x = "date" , y = "count" , color = 'world' ) fig . show ( )
536	n = family_size_dict [ f ] choice_0 = choice_dict [ 'choice_0' ] [ f ] choice_1 = choice_dict [ 'choice_1' ] [ f ] choice_2 = choice_dict [ 'choice_2' ] [ f ] choice_3 = choice_dict [ 'choice_3' ] [ f ] choice_4 = choice_dict [ 'choice_4' ] [ f ] choice_5 = choice_dict [ 'choice_5' ] [ f ] choice_6 = choice_dict [ 'choice_6' ] [ f ] choice_7 = choice_dict [ 'choice_7' ] [ f ] choice_8 = choice_dict [ 'choice_8' ] [ f ] choice_9 = choice_dict [ 'choice_9' ] [ f ]
537	for _ , v in daily_occupancy . items ( ) : if ( v > MAX_OCCUPANCY ) or ( v < MIN_OCCUPANCY ) : penalty += 100000000
538	yesterday_count = daily_occupancy [ days [ 0 ] ] for day in days [ 1 : ] : today_count = daily_occupancy [ day ] diff = abs ( today_count - yesterday_count ) accounting_cost += max ( 0 , ( daily_occupancy [ day ] - 125.0 ) / 400.0 * daily_occupancy [ day ] ** ( 0.5 + diff / 50.0 ) ) yesterday_count = today_count return penalty + accounting_cost , penalty , accounting_cost
539	best = submission [ 'assigned_day' ] . tolist ( ) start_score = cost_function ( best ) new = best . copy ( )
540	for pick in range ( 10 ) : day = choice_dict [ f'choice_{pick}' ] [ fam_id ] temp = new . copy ( ) temp [ fam_id ] = day if cost_function ( temp ) [ 0 ] < start_score [ 0 ] : new = temp . copy ( ) start_score = cost_function ( new ) penalty_list . append ( start_score [ 1 ] ) cost_list . append ( start_score [ 2 ] )
541	train = pd . read_csv ( '/kaggle/input/cat-in-the-dat/train.csv' ) test = pd . read_csv ( '/kaggle/input/cat-in-the-dat/test.csv' ) target = train [ 'target' ] train_id = train [ 'id' ] test_id = test [ 'id' ] train . drop ( [ 'target' , 'id' ] , axis = 1 , inplace = True ) test . drop ( 'id' , axis = 1 , inplace = True )
542	WOE_encoder = WOEEncoder ( ) train_woe = WOE_encoder . fit_transform ( train [ feature_list ] , target ) test_woe = WOE_encoder . transform ( test [ feature_list ] )
543	noaxis = dict ( showbackground = False , showline = False , zeroline = False , showgrid = False , showticklabels = False , title = '' )
544	colors = list ( reversed ( Purples256 ) ) mapper = LinearColorMapper ( palette = colors , low = min ( value ) , high = max ( value ) )
545	fig , ax = plt . subplots ( 1 , 4 , figsize = ( 20 , 5 ) ) for i in range ( 4 ) : sns . countplot ( f'bin_{i}' , hue = 'target' , data = train_df , ax = ax [ i ] ) ax [ i ] . set_title ( f'bin_{i} feature countplot' ) print ( percentage_of_feature_target ( train_df , f'bin_{i}' , 'target' , 1 ) ) plt . show ( )
546	fig , ax = plt . subplots ( 4 , 1 , figsize = ( 40 , 40 ) ) for i in range ( 5 , 9 ) : sns . countplot ( sorted ( train_df [ f'nom_{i}' ] ) , ax = ax [ i - 5 ] ) plt . setp ( ax [ i - 5 ] . get_xticklabels ( ) , rotation = 90 ) plt . show ( ) ;
547	out1 = pd . read_csv ( "../input/statoil-iceberg-submissions/sub_200_ens_densenet.csv" , index_col = 0 ) out2 = pd . read_csv ( "../input/statoil-iceberg-submissions/sub_TF_keras.csv" , index_col = 0 ) out3 = pd . read_csv ( "../input/submission38-lb01448/submission38.csv" , index_col = 0 ) concat_sub = pd . concat ( [ out1 , out2 , out3 ] , axis = 1 ) cols = list ( map ( lambda x : "is_iceberg_" + str ( x ) , range ( len ( concat_sub . columns ) ) ) ) concat_sub . columns = cols concat_sub . reset_index ( inplace = True ) concat_sub . head ( )
548	concat_sub [ 'is_iceberg_max' ] = concat_sub . iloc [ : , 1 : 6 ] . max ( axis = 1 ) concat_sub [ 'is_iceberg_min' ] = concat_sub . iloc [ : , 1 : 6 ] . min ( axis = 1 ) concat_sub [ 'is_iceberg_mean' ] = concat_sub . iloc [ : , 1 : 6 ] . mean ( axis = 1 ) concat_sub [ 'is_iceberg_median' ] = concat_sub . iloc [ : , 1 : 6 ] . median ( axis = 1 )
549	concat_sub [ 'is_iceberg_max' ] = concat_sub . iloc [ : , 1 : 6 ] . max ( axis = 1 ) concat_sub [ 'is_iceberg_min' ] = concat_sub . iloc [ : , 1 : 6 ] . min ( axis = 1 ) concat_sub [ 'is_iceberg_mean' ] = concat_sub . iloc [ : , 1 : 6 ] . mean ( axis = 1 ) concat_sub [ 'is_iceberg_median' ] = concat_sub . iloc [ : , 1 : 6 ] . median ( axis = 1 )
550	out1 = pd . read_csv ( "../input/statoil-iceberg-submissions/sub_200_ens_densenet.csv" , index_col = 0 ) out2 = pd . read_csv ( "../input/statoil-iceberg-submissions/sub_TF_keras.csv" , index_col = 0 ) out3 = pd . read_csv ( "../input/submission38-lb01448/submission38.csv" , index_col = 0 ) out4 = pd . read_csv ( "../input/submission38-lb01448/submission43.csv" , index_col = 0 ) out5 = pd . read_csv ( '../input/submarineering-even-better-public-score-until-now/submission54.csv' , index_col = 0 )
551	out5err = log_loss ( labels , out5 ) Lerr = 0.1427 print ( 'out5 Error:' , Lerr + out5err )
552	embed_size = 300 max_features = 50000 maxlen = 100
553	train_X = train_df [ "question_text" ] . fillna ( "_na_" ) . values val_X = val_df [ "question_text" ] . fillna ( "_na_" ) . values test_X = test_df [ "question_text" ] . fillna ( "_na_" ) . values
554	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( train_X ) ) train_X = tokenizer . texts_to_sequences ( train_X ) val_X = tokenizer . texts_to_sequences ( val_X ) test_X = tokenizer . texts_to_sequences ( test_X )
555	train_X = pad_sequences ( train_X , maxlen = maxlen ) val_X = pad_sequences ( val_X , maxlen = maxlen ) test_X = pad_sequences ( test_X , maxlen = maxlen )
556	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
557	def get_reward ( y_true , y_fit ) : R2 = 1 - np . sum ( ( y_true - y_fit ) ** 2 ) / np . sum ( ( y_true - np . mean ( y_true ) ) ** 2 ) R = np . sign ( R2 ) * math . sqrt ( abs ( R2 ) ) return ( R )
558	train = observation . train mean_values = train . median ( axis = 0 ) train . fillna ( mean_values , inplace = True )
559	low_y_cut = - 0.086093 high_y_cut = 0.093497 y_is_above_cut = ( train . y > high_y_cut ) y_is_below_cut = ( train . y < low_y_cut ) y_is_within_cut = ( ~ y_is_above_cut & ~ y_is_below_cut )
560	target_cols = [ 'ind_ahor_fin_ult1' , 'ind_aval_fin_ult1' , 'ind_cco_fin_ult1' , 'ind_cder_fin_ult1' , 'ind_cno_fin_ult1' , 'ind_ctju_fin_ult1' , 'ind_ctma_fin_ult1' , 'ind_ctop_fin_ult1' , 'ind_ctpp_fin_ult1' , 'ind_deco_fin_ult1' , 'ind_deme_fin_ult1' , 'ind_dela_fin_ult1' , 'ind_ecue_fin_ult1' , 'ind_fond_fin_ult1' , 'ind_hip_fin_ult1' , 'ind_plan_fin_ult1' , 'ind_pres_fin_ult1' , 'ind_reca_fin_ult1' , 'ind_tjcr_fin_ult1' , 'ind_valo_fin_ult1' , 'ind_viv_fin_ult1' , 'ind_nomina_ult1' , 'ind_nom_pens_ult1' , 'ind_recibo_ult1' ] def getTarget ( row ) :
561	train_df = pd . read_csv ( '../input/nyc-taxi-trip-duration/train.csv' , parse_dates = [ 'pickup_datetime' ] ) test_df = pd . read_csv ( '../input/nyc-taxi-trip-duration/test.csv' , parse_dates = [ 'pickup_datetime' ] ) print ( "Train dataframe shape : " , train_df . shape ) print ( "Test dataframe shape : " , test_df . shape )
562	map_dict = { 'N' : 0 , 'Y' : 1 } train_df [ 'store_and_fwd_flag' ] = train_df [ 'store_and_fwd_flag' ] . map ( map_dict ) test_df [ 'store_and_fwd_flag' ] = test_df [ 'store_and_fwd_flag' ] . map ( map_dict )
563	cols_to_drop = [ 'id' , 'pickup_datetime' , 'pickup_date' ] train_id = train_df [ 'id' ] . values test_id = test_df [ 'id' ] . values train_y = train_df . log_trip_duration . values train_X = train_df . drop ( cols_to_drop + [ 'dropoff_datetime' , 'trip_duration' , 'log_trip_duration' ] , axis = 1 ) test_X = test_df . drop ( cols_to_drop , axis = 1 )
564	train_pred_df = pd . DataFrame ( { 'id' : train_id } ) train_pred_df [ 'trip_duration' ] = pred_val_full train_pred_df . to_csv ( "train_preds_lgb_baseline.csv" , index = False )
565	test_pred_df = pd . DataFrame ( { 'id' : test_id } ) test_pred_df [ 'trip_duration' ] = pred_test_full test_pred_df . to_csv ( "test_preds_lgb_baseline.csv" , index = False )
566	cols_to_drop = [ 'id' , 'pickup_datetime' , 'pickup_date' ] train_id = train_df [ 'id' ] . values test_id = test_df [ 'id' ] . values train_y = train_df . log_trip_duration . values train_X = train_df . drop ( cols_to_drop + [ 'dropoff_datetime' , 'trip_duration' , 'log_trip_duration' ] , axis = 1 ) test_X = test_df . drop ( cols_to_drop , axis = 1 )
567	train_pred_df = pd . DataFrame ( { 'id' : train_id } ) train_pred_df [ 'trip_duration' ] = pred_val_full train_pred_df . to_csv ( "train_preds_lgb.csv" , index = False )
568	test_pred_df = pd . DataFrame ( { 'id' : test_id } ) test_pred_df [ 'trip_duration' ] = pred_test_full test_pred_df . to_csv ( "test_preds_lgb.csv" , index = False )
569	cols_to_drop = [ 'id' , 'pickup_datetime' , 'pickup_date' ] train_id = train_df [ 'id' ] . values test_id = test_df [ 'id' ] . values train_y = train_df . log_trip_duration . values train_X = train_df . drop ( cols_to_drop + [ 'dropoff_datetime' , 'trip_duration' , 'log_trip_duration' ] , axis = 1 ) test_X = test_df . drop ( cols_to_drop , axis = 1 )
570	train_pred_df = pd . DataFrame ( { 'id' : train_id } ) train_pred_df [ 'trip_duration' ] = pred_val_full train_pred_df . to_csv ( "train_preds_lgb.csv" , index = False )
571	test_pred_df = pd . DataFrame ( { 'id' : test_id } ) test_pred_df [ 'trip_duration' ] = pred_test_full test_pred_df . to_csv ( "test_preds_lgb.csv" , index = False )
572	train_df = pd . read_csv ( "../input/train.csv" ) test_df = pd . read_csv ( "../input/test.csv" ) train_df . head ( )
573	train_df = pd . merge ( train_df , resource_df , on = "id" , how = 'left' ) test_df = pd . merge ( test_df , resource_df , on = "id" , how = 'left' ) resource_df . head ( )
574	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 7 ) ) sns . distplot ( train_df [ 'X' ] , ax = ax [ 0 ] ) sns . distplot ( train_df [ 'Y' ] , ax = ax [ 1 ] ) plt . show ( )
575	temp_data = StringIO ( ) temp_df = pd . read_csv ( temp_data ) train_df = pd . merge ( train_df , temp_df , on = "category_name" , how = "left" )
576	cnt_srs = test_df [ 'activation_date' ] . value_counts ( ) trace = go . Bar ( x = cnt_srs . index , y = cnt_srs . values , marker = dict ( color = cnt_srs . values , colorscale = 'Picnic' , reversescale = True ) , ) layout = go . Layout ( title = 'Activation Dates in Test' ) data = [ trace ] fig = go . Figure ( data = data , layout = layout ) py . iplot ( fig , filename = "ActivationDate" )
577	plt . figure ( figsize = ( 8 , 8 ) ) sns . jointplot ( x = train_df [ "svd_title_1" ] . values , y = train_df [ "deal_probability" ] . values , size = 10 ) plt . ylabel ( 'Deal Probability' , fontsize = 12 ) plt . xlabel ( 'First SVD component on Title' , fontsize = 12 ) plt . title ( "Deal Probability distribution for First SVD component on title" , fontsize = 15 ) plt . show ( )
578	plt . figure ( figsize = ( 8 , 8 ) ) sns . jointplot ( x = train_df [ "svd_title_2" ] . values , y = train_df [ "deal_probability" ] . values , size = 10 ) plt . ylabel ( 'Deal Probability' , fontsize = 12 ) plt . xlabel ( 'Second SVD component on Title' , fontsize = 12 ) plt . title ( "Deal Probability distribution for Second SVD component on title" , fontsize = 15 ) plt . show ( )
579	plt . figure ( figsize = ( 8 , 8 ) ) sns . jointplot ( x = train_df [ "svd_title_3" ] . values , y = train_df [ "deal_probability" ] . values , size = 10 ) plt . ylabel ( 'Deal Probability' , fontsize = 12 ) plt . xlabel ( 'Third SVD component on Title' , fontsize = 12 ) plt . title ( "Deal Probability distribution for Third SVD component on title" , fontsize = 15 ) plt . show ( )
580	plt . figure ( figsize = ( 8 , 8 ) ) sns . jointplot ( x = train_df [ "svd_desc_1" ] . values , y = train_df [ "deal_probability" ] . values , size = 10 ) plt . ylabel ( 'Deal Probability' , fontsize = 12 ) plt . xlabel ( 'First SVD component on Description' , fontsize = 12 ) plt . title ( "Deal Probability distribution for First SVD component on Description" , fontsize = 15 ) plt . show ( )
581	plt . figure ( figsize = ( 8 , 8 ) ) sns . jointplot ( x = train_df [ "svd_desc_2" ] . values , y = train_df [ "deal_probability" ] . values , size = 10 ) plt . ylabel ( 'Deal Probability' , fontsize = 12 ) plt . xlabel ( 'Second SVD component on Description' , fontsize = 12 ) plt . title ( "Deal Probability distribution for Second SVD component on title" , fontsize = 15 ) plt . show ( )
582	plt . figure ( figsize = ( 8 , 8 ) ) sns . jointplot ( x = train_df [ "svd_desc_3" ] . values , y = train_df [ "deal_probability" ] . values , size = 10 ) plt . ylabel ( 'Deal Probability' , fontsize = 12 ) plt . xlabel ( 'Second SVD component on Description' , fontsize = 12 ) plt . title ( "Deal Probability distribution for Third SVD component on Description" , fontsize = 15 ) plt . show ( )
583	dev_X = train_X . iloc [ : - 200000 , : ] val_X = train_X . iloc [ - 200000 : , : ] dev_y = train_y [ : - 200000 ] val_y = train_y [ - 200000 : ] print ( dev_X . shape , val_X . shape , test_X . shape )
584	pred_test [ pred_test > 1 ] = 1 pred_test [ pred_test < 0 ] = 0 sub_df = pd . DataFrame ( { "item_id" : test_id } ) sub_df [ "deal_probability" ] = pred_test sub_df . to_csv ( "baseline_lgb.csv" , index = False )
585	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) lgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) ax . grid ( False ) plt . title ( "LightGBM - Feature Importance" , fontsize = 15 ) plt . show ( )
586	dev_df = train_df [ train_df [ 'date' ] <= datetime . date ( 2017 , 5 , 31 ) ] val_df = train_df [ train_df [ 'date' ] > datetime . date ( 2017 , 5 , 31 ) ] dev_y = np . log1p ( dev_df [ "totals.transactionRevenue" ] . values ) val_y = np . log1p ( val_df [ "totals.transactionRevenue" ] . values ) dev_X = dev_df [ cat_cols + num_cols ] val_X = val_df [ cat_cols + num_cols ] test_X = test_df [ cat_cols + num_cols ]
587	sns . heatmap ( corrmat , vmax = 1. , square = True , cmap = "YlGnBu" , annot = True ) plt . title ( "Important variables correlation map" , fontsize = 15 ) plt . show ( )
588	train_X = train_df . drop ( constant_df . col_name . tolist ( ) + [ "ID" , "target" ] , axis = 1 ) test_X = test_df . drop ( constant_df . col_name . tolist ( ) + [ "ID" ] , axis = 1 ) train_y = np . log1p ( train_df [ "target" ] . values )
589	sub_df = pd . DataFrame ( { "ID" : test_df [ "ID" ] . values } ) sub_df [ "target" ] = pred_test_full sub_df . to_csv ( "baseline_lgb.csv" , index = False )
590	target_col = "target" plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df [ target_col ] . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'Loyalty Score' , fontsize = 12 ) plt . show ( )
591	gdf = hist_df . groupby ( "card_id" ) gdf = gdf [ "purchase_amount" ] . agg ( [ 'sum' , 'mean' , 'std' , 'min' , 'max' ] ) . reset_index ( ) gdf . columns = [ "card_id" , "sum_hist_trans" , "mean_hist_trans" , "std_hist_trans" , "min_hist_trans" , "max_hist_trans" ] train_df = pd . merge ( train_df , gdf , on = "card_id" , how = "left" ) test_df = pd . merge ( test_df , gdf , on = "card_id" , how = "left" )
592	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 17 , 5 ) ) sns . distplot ( weather_train [ 'wind_direction' ] , ax = ax [ 0 ] ) sns . distplot ( weather_train [ 'wind_speed' ] , ax = ax [ 1 ] ) plt . show ( )
593	def cloud_graph ( df ) : df = df . sort_values ( 'timestamp' ) fig = go . Figure ( ) fig . add_trace ( go . Scatter ( x = df [ 'timestamp' ] , y = df [ 'cloud_coverage' ] , name = "Cloud Coverage" , line_color = 'lightskyblue' , opacity = 0.7 ) ) fig . update_layout ( template = 'plotly_dark' , title_text = 'Cloud' , xaxis_rangeslider_visible = True ) fig . show ( )
594	cnt_srs = train_df [ 'bedrooms' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'bedrooms' , fontsize = 12 ) plt . show ( )
595	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . price . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'price' , fontsize = 12 ) plt . show ( )
596	ulimit = np . percentile ( train_df . price . values , 99 ) train_df [ 'price' ] . ix [ train_df [ 'price' ] > ulimit ] = ulimit plt . figure ( figsize = ( 8 , 6 ) ) sns . distplot ( train_df . price . values , bins = 50 , kde = True ) plt . xlabel ( 'price' , fontsize = 12 ) plt . show ( )
597	plt . figure ( figsize = ( 12 , 6 ) ) wordcloud = WordCloud ( background_color = 'white' , width = 600 , height = 300 , max_font_size = 50 , max_words = 40 ) . generate ( text_da ) wordcloud . recolor ( random_state = 0 ) plt . imshow ( wordcloud ) plt . title ( "Wordcloud for Display Address" , fontsize = 30 ) plt . axis ( "off" ) plt . show ( )
598	ulimit = 180 train_df [ 'y' ] . ix [ train_df [ 'y' ] > ulimit ] = ulimit plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . y . values , bins = 50 , kde = False ) plt . xlabel ( 'y value' , fontsize = 12 ) plt . show ( )
599	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df = missing_df . ix [ missing_df [ 'missing_count' ] > 0 ] missing_df = missing_df . sort_values ( by = 'missing_count' ) missing_df
600	def xgb_r2_score ( preds , dtrain ) : labels = dtrain . get_label ( ) return 'r2' , r2_score ( labels , preds ) xgb_params = { 'eta' : 0.05 , 'max_depth' : 6 , 'subsample' : 0.7 , 'colsample_bytree' : 0.7 , 'objective' : 'reg:linear' , 'silent' : 1 } dtrain = xgb . DMatrix ( train_X , train_y , feature_names = train_X . columns . values ) model = xgb . train ( dict ( xgb_params , silent = 0 ) , dtrain , num_boost_round = 100 , feval = xgb_r2_score , maximize = True )
601	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
602	def generate_ngrams ( text , n_gram = 1 ) : token = [ token for token in text . lower ( ) . split ( " " ) if token != "" if token not in STOPWORDS ] ngrams = zip ( * [ token [ i : ] for i in range ( n_gram ) ] ) return [ " " . join ( ngram ) for ngram in ngrams ]
603	def horizontal_bar_chart ( df , color ) : trace = go . Bar ( y = df [ "word" ] . values [ : : - 1 ] , x = df [ "wordcount" ] . values [ : : - 1 ] , showlegend = False , orientation = 'h' , marker = dict ( color = color , ) , ) return trace
604	freq_dict = defaultdict ( int ) for sent in train0_df [ "question_text" ] : for word in generate_ngrams ( sent ) : freq_dict [ word ] += 1 fd_sorted = pd . DataFrame ( sorted ( freq_dict . items ( ) , key = lambda x : x [ 1 ] ) [ : : - 1 ] ) fd_sorted . columns = [ "word" , "wordcount" ] trace0 = horizontal_bar_chart ( fd_sorted . head ( 50 ) , 'blue' )
605	freq_dict = defaultdict ( int ) for sent in train1_df [ "question_text" ] : for word in generate_ngrams ( sent ) : freq_dict [ word ] += 1 fd_sorted = pd . DataFrame ( sorted ( freq_dict . items ( ) , key = lambda x : x [ 1 ] ) [ : : - 1 ] ) fd_sorted . columns = [ "word" , "wordcount" ] trace1 = horizontal_bar_chart ( fd_sorted . head ( 50 ) , 'blue' )
606	fig = tools . make_subplots ( rows = 1 , cols = 2 , vertical_spacing = 0.04 , subplot_titles = [ "Frequent words of sincere questions" , "Frequent words of insincere questions" ] ) fig . append_trace ( trace0 , 1 , 1 ) fig . append_trace ( trace1 , 1 , 2 ) fig [ 'layout' ] . update ( height = 1200 , width = 900 , paper_bgcolor = 'rgb(233,233,233)' , title = "Word Count Plots" ) py . iplot ( fig , filename = 'word-plots' )
607	fig = tools . make_subplots ( rows = 1 , cols = 2 , vertical_spacing = 0.04 , horizontal_spacing = 0.15 , subplot_titles = [ "Frequent bigrams of sincere questions" , "Frequent bigrams of insincere questions" ] ) fig . append_trace ( trace0 , 1 , 1 ) fig . append_trace ( trace1 , 1 , 2 ) fig [ 'layout' ] . update ( height = 1200 , width = 900 , paper_bgcolor = 'rgb(233,233,233)' , title = "Bigram Count Plots" ) py . iplot ( fig , filename = 'word-plots' )
608	fig = tools . make_subplots ( rows = 1 , cols = 2 , vertical_spacing = 0.04 , horizontal_spacing = 0.2 , subplot_titles = [ "Frequent trigrams of sincere questions" , "Frequent trigrams of insincere questions" ] ) fig . append_trace ( trace0 , 1 , 1 ) fig . append_trace ( trace1 , 1 , 2 ) fig [ 'layout' ] . update ( height = 1200 , width = 1200 , paper_bgcolor = 'rgb(233,233,233)' , title = "Trigram Count Plots" ) py . iplot ( fig , filename = 'word-plots' )
609	tfidf_vec = TfidfVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) tfidf_vec . fit_transform ( train_df [ 'question_text' ] . values . tolist ( ) + test_df [ 'question_text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'question_text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'question_text' ] . values . tolist ( ) )
610	for thresh in np . arange ( 0.1 , 0.201 , 0.01 ) : thresh = np . round ( thresh , 2 ) print ( "F1 score at threshold {0} is {1}" . format ( thresh , metrics . f1_score ( val_y , ( pred_val_y > thresh ) . astype ( int ) ) ) )
611	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "days_since_prior_order" , data = orders_df , color = color [ 3 ] ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Days since prior order' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency distribution by days since prior order" , fontsize = 15 ) plt . show ( )
612	cnt_srs = order_products_prior_df [ 'aisle' ] . value_counts ( ) . head ( 20 ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 5 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Aisle' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
613	plt . figure ( figsize = ( 10 , 10 ) ) temp_series = order_products_prior_df [ 'department' ] . value_counts ( ) labels = ( np . array ( temp_series . index ) ) sizes = ( np . array ( ( temp_series / temp_series . sum ( ) ) * 100 ) ) plt . pie ( sizes , labels = labels , autopct = '%1.1f%%' , startangle = 200 ) plt . title ( "Departments distribution" , fontsize = 15 ) plt . show ( )
614	from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
615	print ( f"The total number of games in the training data is {train_df['GameId'].nunique()}" ) print ( f"The total number of plays in the training data is {train_df['PlayId'].nunique()}" ) print ( f"The NFL seasons in the training data are {train_df['Season'].unique().tolist()}" )
616	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . catplot ( data = temp_df , x = "Quarter" , y = "Yards" , kind = "boxen" ) plt . xlabel ( 'Quarter' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Quarter Vs Yards (target)" , fontsize = 20 ) plt . show ( )
617	import random
618	train_df = pd . read_csv ( "../input/train.csv" , parse_dates = [ 'timestamp' ] ) dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df . groupby ( "Column Type" ) . aggregate ( 'count' ) . reset_index ( )
619	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
620	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "floor" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
621	grouped_df = train_df . groupby ( 'floor' ) [ 'price_doc' ] . aggregate ( np . median ) . reset_index ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . pointplot ( grouped_df . floor . values , grouped_df . price_doc . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Median Price' , fontsize = 12 ) plt . xlabel ( 'Floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
622	train_df [ 'transaction_month' ] = train_df [ 'transactiondate' ] . dt . month cnt_srs = train_df [ 'transaction_month' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 3 ] ) plt . xticks ( rotation = 'vertical' ) plt . xlabel ( 'Month of transaction' , fontsize = 12 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . show ( )
623	plt . figure ( figsize = ( 12 , 12 ) ) sns . jointplot ( x = prop_df . latitude . values , y = prop_df . longitude . values , size = 10 ) plt . ylabel ( 'Longitude' , fontsize = 12 ) plt . xlabel ( 'Latitude' , fontsize = 12 ) plt . show ( )
624	pd . options . display . max_rows = 65 dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df
625	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df [ 'missing_ratio' ] = missing_df [ 'missing_count' ] / train_df . shape [ 0 ] missing_df . ix [ missing_df [ 'missing_ratio' ] > 0.999 ]
626	sns . heatmap ( corrmat , vmax = 1. , square = True ) plt . title ( "Important variables correlation map" , fontsize = 15 ) plt . show ( )
627	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "bathroomcnt" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Bathroom' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of Bathroom count" , fontsize = 15 ) plt . show ( )
628	train_df [ 'bedroomcnt' ] . ix [ train_df [ 'bedroomcnt' ] > 7 ] = 7 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'bedroomcnt' , y = 'logerror' , data = train_df ) plt . xlabel ( 'Bedroom count' , fontsize = 12 ) plt . ylabel ( 'Log Error' , fontsize = 12 ) plt . show ( )
629	from ggplot import * ggplot ( aes ( x = 'yearbuilt' , y = 'logerror' ) , data = train_df ) + \ geom_point ( color = 'steelblue' , size = 1 ) + \ stat_smooth ( )
630	ggplot ( aes ( x = 'latitude' , y = 'longitude' , color = 'logerror' ) , data = train_df ) + \ geom_point ( ) + \ scale_color_gradient ( low = 'red' , high = 'blue' )
631	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
632	text = '' for ind , tag in enumerate ( tags ) : text = " " . join ( [ text , tag ] ) text = text . strip ( ) wordcloud = WordCloud ( background_color = 'white' , width = 600 , height = 300 , max_font_size = 50 , max_words = 80 ) . generate ( text ) wordcloud . recolor ( random_state = 218 ) plt . imshow ( wordcloud ) plt . axis ( "off" ) plt . title ( "Wordcloud on 'tags' for biology " ) plt . show ( )
633	train_df = pd . read_csv ( "../input/train.csv" ) test_df = pd . read_csv ( "../input/test.csv" ) print ( "Number of rows in train dataset : " , train_df . shape [ 0 ] ) print ( "Number of rows in test dataset : " , test_df . shape [ 0 ] )
634	train_df [ 'num_punctuations' ] . loc [ train_df [ 'num_punctuations' ] > 10 ] = 10 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'author' , y = 'num_punctuations' , data = train_df ) plt . xlabel ( 'Author Name' , fontsize = 12 ) plt . ylabel ( 'Number of puntuations in text' , fontsize = 12 ) plt . title ( "Number of punctuations by author" , fontsize = 15 ) plt . show ( )
635	author_mapping_dict = { 'EAP' : 0 , 'HPL' : 1 , 'MWS' : 2 } train_y = train_df [ 'author' ] . map ( author_mapping_dict ) train_id = train_df [ 'id' ] . values test_id = test_df [ 'id' ] . values
636	fig , ax = plt . subplots ( figsize = ( 12 , 12 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
637	tfidf_vec = TfidfVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) full_tfidf = tfidf_vec . fit_transform ( train_df [ 'text' ] . values . tolist ( ) + test_df [ 'text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'text' ] . values . tolist ( ) )
638	tfidf_vec = CountVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) tfidf_vec . fit ( train_df [ 'text' ] . values . tolist ( ) + test_df [ 'text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'text' ] . values . tolist ( ) )
639	tfidf_vec = CountVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) tfidf_vec . fit ( train_df [ 'text' ] . values . tolist ( ) + test_df [ 'text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'text' ] . values . tolist ( ) )
640	train_df [ "nb_cvec_eap" ] = pred_train [ : , 0 ] train_df [ "nb_cvec_hpl" ] = pred_train [ : , 1 ] train_df [ "nb_cvec_mws" ] = pred_train [ : , 2 ] test_df [ "nb_cvec_eap" ] = pred_full_test [ : , 0 ] test_df [ "nb_cvec_hpl" ] = pred_full_test [ : , 1 ] test_df [ "nb_cvec_mws" ] = pred_full_test [ : , 2 ]
641	train_df [ "nb_cvec_char_eap" ] = pred_train [ : , 0 ] train_df [ "nb_cvec_char_hpl" ] = pred_train [ : , 1 ] train_df [ "nb_cvec_char_mws" ] = pred_train [ : , 2 ] test_df [ "nb_cvec_char_eap" ] = pred_full_test [ : , 0 ] test_df [ "nb_cvec_char_hpl" ] = pred_full_test [ : , 1 ] test_df [ "nb_cvec_char_mws" ] = pred_full_test [ : , 2 ]
642	train_df [ "nb_tfidf_char_eap" ] = pred_train [ : , 0 ] train_df [ "nb_tfidf_char_hpl" ] = pred_train [ : , 1 ] train_df [ "nb_tfidf_char_mws" ] = pred_train [ : , 2 ] test_df [ "nb_tfidf_char_eap" ] = pred_full_test [ : , 0 ] test_df [ "nb_tfidf_char_hpl" ] = pred_full_test [ : , 1 ] test_df [ "nb_tfidf_char_mws" ] = pred_full_test [ : , 2 ]
643	fig , ax = plt . subplots ( figsize = ( 12 , 12 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
644	cnf_matrix = confusion_matrix ( val_y , np . argmax ( pred_val_y , axis = 1 ) ) np . set_printoptions ( precision = 2 ) plt . figure ( figsize = ( 8 , 8 ) ) plot_confusion_matrix ( cnf_matrix , classes = [ 'EAP' , 'HPL' , 'MWS' ] , title = 'Confusion matrix of XGB, without normalization' ) plt . show ( )
645	env = kagglegym . make ( ) observation = env . reset ( ) train = observation . train
646	env = kagglegym . make ( ) observation = env . reset ( ) col = 'technical_20' model = models_dict [ col ] while True : observation . features . fillna ( mean_values , inplace = True ) test_x = np . array ( observation . features [ col ] . values ) . reshape ( - 1 , 1 ) observation . target . y = model . predict ( test_x )
647	env = kagglegym . make ( ) observation = env . reset ( ) col = 'fundamental_11' model = models_dict [ col ] while True : observation . features . fillna ( mean_values , inplace = True ) test_x = np . array ( observation . features [ col ] . values ) . reshape ( - 1 , 1 ) observation . target . y = model . predict ( test_x )
648	env = kagglegym . make ( ) observation = env . reset ( ) col = 'technical_19' model = models_dict [ col ] while True : observation . features . fillna ( mean_values , inplace = True ) test_x = np . array ( observation . features [ col ] . values ) . reshape ( - 1 , 1 ) observation . target . y = model . predict ( test_x )
649	is_dup = train_df [ 'is_duplicate' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( is_dup . index , is_dup . values , alpha = 0.8 , color = color [ 1 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Is Duplicate' , fontsize = 12 ) plt . show ( )
650	plt . figure ( figsize = ( 12 , 8 ) ) grouped_df = train_df . groupby ( 'q1_freq' ) [ 'is_duplicate' ] . aggregate ( np . mean ) . reset_index ( ) sns . barplot ( grouped_df [ "q1_freq" ] . values , grouped_df [ "is_duplicate" ] . values , alpha = 0.8 , color = color [ 4 ] ) plt . ylabel ( 'Mean is_duplicate' , fontsize = 12 ) plt . xlabel ( 'Q1 frequency' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
651	cols_to_use = [ 'q1_q2_intersect' , 'q1_freq' , 'q2_freq' ] temp_df = train_df [ cols_to_use ] corrmat = temp_df . corr ( method = 'spearman' ) f , ax = plt . subplots ( figsize = ( 8 , 8 ) ) sns . heatmap ( corrmat , vmax = 1. , square = True ) plt . title ( "Leaky variables correlation map" , fontsize = 15 ) plt . show ( )
652	sns . heatmap ( corrmat , vmax = 1. , square = True ) plt . title ( "Leaky variables correlation map" , fontsize = 15 ) plt . show ( )
653	data_path = "../input/" train_file = data_path + "train.json" test_file = data_path + "test.json" train_df = pd . read_json ( train_file ) test_df = pd . read_json ( test_file ) print ( train_df . shape ) print ( test_df . shape )
654	image = tf . io . read_file ( image_path ) image = tf . image . decode_jpeg ( image , channels = 3 ) image = tf . cast ( image , tf . int64 )
655	landmark_id = labels [ image_id ] landmark_name = urllib . parse . unquote ( CLASSES [ labels [ image_id ] ] . replace ( 'http://commons.wikimedia.org/wiki/Category:' , '' ) ) title = f'{landmark_id}: {landmark_name}' print ( title )
656	plt . figure ( figsize = ( 20 , 10 ) ) plt . axis ( 'off' ) plt . imshow ( image ) plt . title ( title , fontsize = 16 ) plt . show ( )
657	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in the train data set:' , train . shape ) print ( 'Number of rows and columns in the test data set:' , test . shape )
658	vect_word = TfidfVectorizer ( max_features = 20000 , lowercase = True , analyzer = 'word' , stop_words = 'english' , ngram_range = ( 1 , 3 ) , dtype = np . float32 ) vect_char = TfidfVectorizer ( max_features = 40000 , lowercase = True , analyzer = 'char' , stop_words = 'english' , ngram_range = ( 3 , 6 ) , dtype = np . float32 )
659	tr_vect_char = vect_char . fit_transform ( train [ 'comment_text' ] ) ts_vect_char = vect_char . transform ( test [ 'comment_text' ] ) gc . collect ( )
660	col = 'identity_hate' print ( "Column:" , col ) pred = lr . predict ( X ) print ( '\nConfusion matrix\n' , confusion_matrix ( y [ col ] , pred ) ) print ( classification_report ( y [ col ] , pred ) )
661	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in train data set:' , train . shape ) print ( 'Number of rows and columns in test data set:' , test . shape )
662	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 4 ) ) ax1 , ax2 = ax . flatten ( ) sns . distplot ( train [ 'formation_energy_ev_natom' ] , bins = 50 , ax = ax1 , color = 'b' ) sns . distplot ( train [ 'bandgap_energy_ev' ] , bins = 50 , ax = ax2 , color = 'r' )
663	cor = train . corr ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . heatmap ( cor , cmap = 'Set1' , annot = True )
664	train [ 'alpha_rad' ] = np . radians ( train [ 'lattice_angle_alpha_degree' ] ) train [ 'beta_rad' ] = np . radians ( train [ 'lattice_angle_beta_degree' ] ) train [ 'gamma_rad' ] = np . radians ( train [ 'lattice_angle_gamma_degree' ] ) test [ 'alpha_rad' ] = np . radians ( test [ 'lattice_angle_alpha_degree' ] ) test [ 'beta_rad' ] = np . radians ( test [ 'lattice_angle_beta_degree' ] ) test [ 'gamma_rad' ] = np . radians ( test [ 'lattice_angle_gamma_degree' ] )
665	print ( 'Original text:\n' , train [ 'text' ] [ 0 ] ) review = re . sub ( '[^A-Za-z0-9]' , " " , train [ 'text' ] [ 0 ] ) print ( '\nAfter removal of punctuation:\n' , review )
666	cv = CountVectorizer ( max_features = 2000 , ngram_range = ( 1 , 3 ) , dtype = np . int8 , stop_words = 'english' ) X_cv = cv . fit_transform ( train [ 'clean_text' ] ) . toarray ( ) X_test_cv = cv . fit_transform ( test [ 'clean_text' ] ) . toarray ( )
667	y_pred = pred_test_full / 10 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred1.csv' , index = False )
668	y_pred = pred_test_full / 10 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred2.csv' , index = False )
669	unwanted = [ 'text' , 'id' , 'clean_text' ] X_tf = np . concatenate ( ( X_tf , train . drop ( unwanted + [ 'author' ] , axis = 1 ) . values ) , axis = 1 ) X_test_tf = np . concatenate ( ( X_test_tf , test . drop ( unwanted , axis = 1 ) . values ) , axis = 1 )
670	y_pred = pred_test_full / 2 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred3.csv' , index = False )
671	train = pd . read_csv ( "../input/train_1.csv" ) keys = pd . read_csv ( "../input/key_1.csv" ) ss = pd . read_csv ( "../input/sample_submission_1.csv" )
672	y = X_train . dropna ( 0 ) . as_matrix ( ) [ 0 ] y = [ None if i >= np . percentile ( y , 95 ) or i <= np . percentile ( y , 5 ) else i for i in y ] df_na = pd . DataFrame ( { 'ds' : X_train . T . index . values , 'y' : y } )
673	path = '../input/' train = pd . read_csv ( path + 'train.csv' , na_values = - 1 ) test = pd . read_csv ( path + 'test.csv' , na_values = - 1 ) print ( 'Number rows and columns:' , train . shape ) print ( 'Number rows and columns:' , test . shape )
674	k = pd . DataFrame ( ) k [ 'train' ] = train . isnull ( ) . sum ( ) k [ 'test' ] = test . isnull ( ) . sum ( ) fig , ax = plt . subplots ( figsize = ( 16 , 5 ) ) k . plot ( kind = 'bar' , ax = ax )
675	def missing_value ( df ) : col = df . columns for i in col : if df [ i ] . isnull ( ) . sum ( ) > 0 : df [ i ] . fillna ( df [ i ] . mode ( ) [ 0 ] , inplace = True )
676	def basic_details ( df ) : b = pd . DataFrame ( ) b [ 'Missing value' ] = df . isnull ( ) . sum ( ) b [ 'N unique value' ] = df . nunique ( ) b [ 'dtype' ] = df . dtypes return b basic_details ( train )
677	X = train1 . drop ( [ 'target' , 'id' ] , axis = 1 ) y = train1 [ 'target' ] . astype ( 'category' ) x_test = test1 . drop ( [ 'target' , 'id' ] , axis = 1 ) del train1 , test1
678	y_pred = pred_test_full / 5 submit = pd . DataFrame ( { 'id' : test [ 'id' ] , 'target' : y_pred } ) submit . to_csv ( 'lr_porto.csv' , index = False )
679	path = '../input/' train = pd . read_csv ( path + 'train.csv' , na_values = - 1 ) test = pd . read_csv ( path + 'test.csv' , na_values = - 1 ) print ( 'Number rows and columns:' , train . shape ) print ( 'Number rows and columns:' , test . shape )
680	k = pd . DataFrame ( ) k [ 'train' ] = train . isnull ( ) . sum ( ) k [ 'test' ] = test . isnull ( ) . sum ( ) k
681	X = train . drop ( [ 'target' , 'id' ] , axis = 1 ) y = train [ 'target' ] . astype ( 'category' ) x_test = test . drop ( 'id' , axis = 1 ) test_id = test [ 'id' ]
682	y_pred = pred_xgb submit = pd . DataFrame ( { 'id' : test_id , 'target' : y_pred } ) submit . to_csv ( 'xgb_porto.csv' , index = False )
683	sns . heatmap ( corr , cmap = cmap , vmax = .3 , center = 0 , square = True , linewidths = .5 , cbar_kws = { "shrink" : .5 } ) plt . show ( )
684	params = { 'min_child_weight' : 10.0 , 'objective' : 'binary:logistic' , 'max_depth' : 7 , 'max_delta_step' : 1.8 , 'colsample_bytree' : 0.4 , 'subsample' : 0.8 , 'eta' : 0.025 , 'gamma' : 0.65 , 'num_boost_round' : 700 }
685	X = train . drop ( [ 'id' , 'target' ] , axis = 1 ) . values y = train . target . values test_id = test . id . values test = test . drop ( 'id' , axis = 1 )
686	sub = pd . DataFrame ( ) sub [ 'id' ] = test_id sub [ 'target' ] = np . zeros_like ( test_id )
687	d_train = xgb . DMatrix ( X_train , y_train ) d_valid = xgb . DMatrix ( X_valid , y_valid ) d_test = xgb . DMatrix ( test . values ) watchlist = [ ( d_train , 'train' ) , ( d_valid , 'valid' ) ]
688	preds_val = model . predict ( [ x_val_f ] , batch_size = 512 ) preds . append ( model . predict ( x_test ) ) fold += 1 fpr , tpr , thresholds = roc_curve ( y_val_f , preds_val , pos_label = 1 ) aucs += auc ( fpr , tpr ) print ( 'Fold {}, AUC = {}' . format ( fold , auc ( fpr , tpr ) ) ) print ( "Cross Validation AUC = {}" . format ( aucs / 10 ) )
689	def text_to_array ( text ) : empyt_emb = np . zeros ( 300 ) text = basic_tokenizer ( text [ : - 1 ] ) [ : SEQ_LEN ] embeds = [ embeddings_index . get ( x , empyt_emb ) for x in text ] embeds += [ empyt_emb ] * ( SEQ_LEN - len ( embeds ) ) return np . array ( embeds )
690	cost_mat [ np . arange ( cost_mat . shape [ 0 ] ) , np . arange ( cost_mat . shape [ 0 ] ) ] = np . inf draw_cost_mat ( cost_mat , 121 ) solved = cost_mat . copy ( )
691	solved [ x , y ] = np . inf solved [ y , x ] = np . inf draw_cost_mat ( solved , 122 ) plt . show ( )
692	embed_size = 300 max_features = 95000 maxlen = 70
693	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( train_X ) ) train_X = tokenizer . texts_to_sequences ( train_X ) val_X = tokenizer . texts_to_sequences ( val_X ) test_X = tokenizer . texts_to_sequences ( test_X )
694	train_X = pad_sequences ( train_X , maxlen = maxlen ) val_X = pad_sequences ( val_X , maxlen = maxlen ) test_X = pad_sequences ( test_X , maxlen = maxlen )
695	np . random . seed ( 2018 ) trn_idx = np . random . permutation ( len ( train_X ) ) val_idx = np . random . permutation ( len ( val_X ) ) train_X = train_X [ trn_idx ] val_X = val_X [ val_idx ] train_y = train_y [ trn_idx ] val_y = val_y [ val_idx ] return train_X , val_X , test_X , train_y , val_y , tokenizer . word_index
696	if isfile ( P2SIZE ) : print ( "P2SIZE exists." ) with open ( P2SIZE , 'rb' ) as f : p2size = pickle . load ( f ) else : p2size = { } for p in tqdm ( join ) : size = pil_image . open ( expand_path ( p ) ) . size p2size [ p ] = size
697	p2h = { } for p in tqdm ( join ) : img = pil_image . open ( expand_path ( p ) ) h = phash ( img ) p2h [ p ] = h
698	h2ps = { } for p , h in p2h . items ( ) : if h not in h2ps : h2ps [ h ] = [ ] if p not in h2ps [ h ] : h2ps [ h ] . append ( p )
699	h2ps = { } for p , h in p2h . items ( ) : if h not in h2ps : h2ps [ h ] = [ ] if p not in h2ps [ h ] : h2ps [ h ] . append ( p )
700	if p in h2p : p = h2p [ p ] size_x , size_y = p2size [ p ]
701	matrix = trans [ : 2 , : 2 ] offset = trans [ : 2 , 2 ] img = img . reshape ( img . shape [ : - 1 ] ) img = affine_transform ( img , matrix , offset , output_shape = img_shape [ : - 1 ] , order = 1 , mode = 'constant' , cval = np . average ( img ) ) img = img . reshape ( img_shape )
702	img -= np . mean ( img , keepdims = True ) img /= np . std ( img , keepdims = True ) + K . epsilon ( ) return img def read_for_training ( p ) :
703	w2hs = { } for h , ws in h2ws . items ( ) : if len ( ws ) == 1 : w = ws [ 0 ] if w not in w2hs : w2hs [ w ] = [ ] if h not in w2hs [ w ] : w2hs [ w ] . append ( h ) for w , hs in w2hs . items ( ) : if len ( hs ) > 1 : w2hs [ w ] = sorted ( hs )
704	for ts in w2ts . values ( ) : d = ts . copy ( ) while True : random . shuffle ( d ) if not np . any ( ts == d ) : break for ab in zip ( ts , d ) : self . match . append ( ab )
705	for i , j in zip ( x , y ) : if i == j : print ( self . score ) print ( x ) print ( y ) print ( i , j ) assert i != j self . unmatch . append ( ( train [ i ] , train [ j ] ) )
706	self . score [ x , y ] = 10000.0 self . score [ y , x ] = 10000.0 random . shuffle ( self . match ) random . shuffle ( self . unmatch )
707	w2ts = { } for w , hs in w2hs . items ( ) : for h in hs : if h in train_set : if w not in w2ts : w2ts [ w ] = [ ] if h not in w2ts [ w ] : w2ts [ w ] . append ( h ) for w , ts in w2ts . items ( ) : w2ts [ w ] = np . array ( ts )
708	history [ 'epochs' ] = steps history [ 'ms' ] = np . mean ( score ) history [ 'lr' ] = get_lr ( model ) print ( history [ 'epochs' ] , history [ 'lr' ] , history [ 'ms' ] ) histories . append ( history )
709	fknown = branch_model . predict_generator ( FeatureGen ( known ) , max_queue_size = 20 , workers = 10 , verbose = 0 ) fsubmit = branch_model . predict_generator ( FeatureGen ( submit ) , max_queue_size = 20 , workers = 10 , verbose = 0 ) score = head_model . predict_generator ( ScoreGen ( fknown , fsubmit ) , max_queue_size = 20 , workers = 10 , verbose = 0 ) score = score_reshape ( score , fknown , fsubmit )
710	img = cv2 . resize ( img , ( self . sz , self . sz ) ) return img def get_y ( self , i ) : if ( self . path == TEST ) : return 0 return self . train_df . loc [ self . fnames [ i ] ] [ 'Id' ] def get_c ( self ) : return len ( unique_labels )
711	preds_v , y_v = learn . TTA ( is_test = False , n_aug = 2 ) preds_v = np . stack ( preds_v , axis = - 1 ) preds_v = np . exp ( preds_v ) preds_v = preds_v . mean ( axis = - 1 ) y_v += 1
712	img = img [ ( ORG_SIZE - self . sz ) // 2 : ( ORG_SIZE + self . sz ) // 2 , ( ORG_SIZE - self . sz ) // 2 : ( ORG_SIZE + self . sz ) // 2 , : ] return img def get_y ( self , i ) : if ( self . path == TEST ) : return 0 return self . train_df . loc [ self . fnames [ i ] ] [ 'label' ] def get_c ( self ) : return 2
713	new_df_train = df_train . copy ( ) new_df_test = df_test . copy ( ) del new_df_test [ "image" ] del new_df_train [ "image" ] del new_df_test [ "activation_date" ] del new_df_train [ "activation_date" ]
714	from sklearn import preprocessing df = df . drop ( [ "Name" , "Ticket" ] , axis = 1 ) categorical = [ "Sex" , "Embarked" , "Ticket_code" , "Ticket_code_HEAD" , "Ticket_code_TAIL" , "Initial" ] lbl = preprocessing . LabelEncoder ( ) for col in categorical : df [ col ] . fillna ( 'Unknown' ) df [ col ] = lbl . fit_transform ( df [ col ] . astype ( str ) ) df . head ( )
715	df [ "Has_Cabin" ] = df [ "Cabin" ] . apply ( lambda x : 0 if type ( x ) == float else 1 ) df [ "Cabin_Initial" ] = df [ "Cabin" ] . apply ( lambda x : x [ 0 ] if pd . notnull ( x ) else x ) df [ "Cabin_number" ] = df [ "Cabin" ] . apply ( lambda x : x [ - 1 ] if pd . notnull ( x ) else x ) df = df . drop ( "Cabin" , axis = 1 )
716	train_identity = pd . read_csv ( '../input/ieee-fraud-detection/train_identity.csv' ) train_transaction = pd . read_csv ( '../input/ieee-fraud-detection/train_transaction.csv' ) test_identity = pd . read_csv ( '../input/ieee-fraud-detection/test_identity.csv' ) test_transaction = pd . read_csv ( '../input/ieee-fraud-detection/test_transaction.csv' ) sub = pd . read_csv ( '../input/ieee-fraud-detection/sample_submission.csv' ) train = pd . merge ( train_transaction , train_identity , on = 'TransactionID' , how = 'left' ) test = pd . merge ( test_transaction , test_identity , on = 'TransactionID' , how = 'left' )
717	from sklearn . utils import resample not_fraud = train [ train . isFraud == 0 ] fraud = train [ train . isFraud == 1 ] not_fraud_downsampled = resample ( not_fraud , replace = False , n_samples = 400000 , random_state = 27 ) downsampled = pd . concat ( [ not_fraud_downsampled , fraud ] ) downsampled . isFraud . value_counts ( )
718	fig = plt . figure ( figsize = ( 12 , 8 ) ) orig = plt . plot ( timeseries , color = 'blue' , label = 'Original' ) mean = plt . plot ( rolmean , color = 'red' , label = 'Rolling Mean' ) std = plt . plot ( rolstd , color = 'black' , label = 'Rolling Std' ) plt . legend ( loc = 'best' ) plt . title ( 'Rolling Mean & Standard Deviation' ) plt . show ( )
719	fig = plt . figure ( figsize = ( 12 , 8 ) ) ax0 = fig . add_subplot ( 111 ) sns . distplot ( resid , fit = stats . norm , ax = ax0 )
720	plt . legend ( [ 'Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )' . format ( mu , sigma ) ] , loc = 'best' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Residual distribution' )
721	fig = plt . figure ( figsize = ( 12 , 8 ) ) ax1 = fig . add_subplot ( 211 ) fig = sm . graphics . tsa . plot_acf ( arima_mod6 . resid , lags = 40 , ax = ax1 ) ax2 = fig . add_subplot ( 212 ) fig = sm . graphics . tsa . plot_pacf ( arima_mod6 . resid , lags = 40 , ax = ax2 )
722	plt . legend ( [ 'Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )' . format ( mu , sigma ) ] , loc = 'best' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Residual distribution' )
723	fig = plt . figure ( figsize = ( 12 , 8 ) ) ax1 = fig . add_subplot ( 211 ) fig = sm . graphics . tsa . plot_acf ( arima_mod6 . resid , lags = 40 , ax = ax1 ) ax2 = fig . add_subplot ( 212 ) fig = sm . graphics . tsa . plot_pacf ( arima_mod6 . resid , lags = 40 , ax = ax2 )
724	start_index = 1730 end_index = 1826 train_df [ 'forecast' ] = sarima_mod6 . predict ( start = start_index , end = end_index , dynamic = True ) train_df [ start_index : end_index ] [ [ 'sales' , 'forecast' ] ] . plot ( figsize = ( 12 , 8 ) )
725	top_countries = countries . index [ : 10 ] df_top_countries = df_long [ df_long [ 'country' ] . isin ( top_countries ) ] style . use ( 'ggplot' ) rcParams [ 'figure.figsize' ] = 15 , 10 ax = sns . barplot ( x = 'country' , hue = "variable" , y = "value" , data = df_top_countries )
726	p = figure ( title = 'Confirmed Cases of Coronavirus COVID-19' , plot_height = 600 , plot_width = 950 , toolbar_location = None , tools = [ hover ] ) p . xgrid . grid_line_color = None p . ygrid . grid_line_color = None
727	p = figure ( title = 'Fatalities of Coronavirus COVID-19' , plot_height = 600 , plot_width = 950 , toolbar_location = None , tools = [ hover ] ) p . xgrid . grid_line_color = None p . ygrid . grid_line_color = None
728	times_series_cntr = times_series_cntr . groupby ( [ 'Date' , 'Province/State' , 'Country/Region' ] ) [ 'ConfirmedCases' ] . max ( ) \ . groupby ( [ 'Date' , 'Country/Region' ] ) . sum ( ) \ . reset_index ( )
729	times_series_df . plot ( figsize = ( 20 , 10 ) , title = "The Cumulative total of Confirmed cases" ) plt . legend ( loc = 2 , prop = { 'size' : 20 } ) plt . show ( )
730	times_series_df . plot ( figsize = ( 20 , 10 ) , title = "The Cumulative total of Confirmed cases" ) plt . legend ( loc = 2 , prop = { 'size' : 20 } ) plt . show ( )
731	times_series_df . diff ( ) . fillna ( 0 ) . plot ( figsize = ( 20 , 10 ) , title = "New Confirmed cases throughout the time" ) plt . legend ( loc = 2 , prop = { 'size' : 20 } ) plt . show ( )
732	total_days = len ( [ x for x in train_df . Date . unique ( ) if x not in test_df . Date . unique ( ) ] ) + test_df . Date . nunique ( ) indeces = [ ] for j in range ( 0 , 284 ) : for i in range ( 1 , 51 ) : indeces . append ( ( i + j * total_days ) ) pred_cc = full_cc . drop ( indeces ) . reset_index ( ) . ConfirmedCases
733	print ( "Train data" ) train . isna ( ) . sum ( ) print ( "Test data" ) test . isna ( ) . sum ( )
734	cols_drop_at_train = list ( set ( more_than_90_NA_or_same_value_train + many_na_train ) ) cols_drop_at_test = list ( set ( more_than_90_NA_or_same_value_test + many_na_test ) ) print ( "Columns to be dropped in train" , len ( cols_drop_at_train ) ) print ( "Columns to be dropped in test" , len ( cols_drop_at_test ) ) print ( "columns are @ train:" , cols_drop_at_train ) print ( "columns are @ test:" , cols_drop_at_train )
735	for col in m_list : print ( "For the " + str ( col ) ) print ( train [ col ] . value_counts ( ) )
736	someFeature_list = [ 'id_36' , 'id_35' , 'id_34' , 'id_28' , 'id_29' , 'id_12' , 'id_15' , 'id_16' ] a4_dims = ( 20 , 20 ) co = 0 ax = sns . countplot ( x = someFeature_list [ co ] , hue = 'isFraud' , data = train ) ax . set_xlabel ( someFeature_list [ co ] ) ax . set_ylabel ( 'Number of Occurrences' ) ax . set_xticklabels ( ax . get_xticklabels ( ) , rotation = 45 ) co += 1 plt . tight_layout ( )
737	from sklearn import preprocessing for col in train . columns : if train [ col ] . dtype == 'object' :
738	train = clean_inf_nan ( train ) test = clean_inf_nan ( test ) for i in train . columns : train [ i ] . fillna ( train [ i ] . median ( ) , inplace = True )
739	from sklearn . ensemble import RandomForestClassifier from sklearn . linear_model import LogisticRegression from sklearn import svm from sklearn import tree from sklearn . neighbors import KNeighborsClassifier from catboost import CatBoostClassifier
740	if 'mols' in locals ( ) : del mols import gc gc . collect ( )
741	prop . append ( [ res [ _ ] for _ in keys ] ) pbar . update ( ) del _df prop = pd . DataFrame . from_records ( prop , columns = keys ) df = pd . merge ( df , prop , how = 'left' , on = [ 'molecule_name' , 'atom_index_0' , 'atom_index_1' ] ) return df train = features ( train ) test = features ( test )
742	from tensorflow . python . keras import optimizers sgd = optimizers . SGD ( lr = 0.01 , decay = 1e-6 , momentum = 0.9 , nesterov = True ) model . compile ( optimizer = sgd , loss = OBJECTIVE_FUNCTION , metrics = LOSS_METRICS )
743	from keras . applications . resnet50 import preprocess_input from keras . preprocessing . image import ImageDataGenerator image_size = IMAGE_RESIZE data_generator = ImageDataGenerator ( preprocessing_function = preprocess_input ) train_generator = data_generator . flow_from_directory ( '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/train' , target_size = ( image_size , image_size ) , batch_size = BATCH_SIZE_TRAINING , class_mode = 'categorical' ) validation_generator = data_generator . flow_from_directory ( '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/valid' , target_size = ( image_size , image_size ) , batch_size = BATCH_SIZE_VALIDATION , class_mode = 'categorical' )
744	test_generator . reset ( ) pred = model . predict_generator ( test_generator , steps = len ( test_generator ) , verbose = 1 ) predicted_class_indices = np . argmax ( pred , axis = 1 )
745	test_generator . reset ( ) pred = model . predict_generator ( test_generator , steps = len ( test_generator ) , verbose = 1 ) predicted_class_indices = np . argmax ( pred , axis = 1 )
746	predicted_class = "Dog" if predicted_class_indices [ i ] else "Cat" ax [ i // 5 , i % 5 ] . imshow ( imgRGB ) ax [ i // 5 , i % 5 ] . axis ( 'off' ) ax [ i // 5 , i % 5 ] . set_title ( "Predicted:{}" . format ( predicted_class ) ) plt . show ( )
747	print ( "******************************" ) print ( "Column: " , col ) print ( "dtype before: " , props [ col ] . dtype )
748	IsInt = False mx = props [ col ] . max ( ) mn = props [ col ] . min ( )
749	if not np . isfinite ( props [ col ] ) . all ( ) : NAlist . append ( col ) props [ col ] . fillna ( mn - 1 , inplace = True )
750	asint = props [ col ] . fillna ( 0 ) . astype ( np . int64 ) result = ( props [ col ] - asint ) result = result . sum ( ) if result > - 0.01 and result < 0.01 : IsInt = True
751	print ( "___MEMORY USAGE AFTER COMPLETION:___" ) mem_usg = props . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( "Memory usage is: " , mem_usg , " MB" ) print ( "This is " , 100 * mem_usg / start_mem_usg , "% of the initial size" ) return props
752	def clean_latex_tag ( text ) : corr_t = [ ] for t in text . split ( " " ) : t = t . strip ( ) if t != '' : corr_t . append ( t ) text = ' ' . join ( corr_t ) text = re . sub ( '(\[ math \]).+(\[ / math \])' , 'mathematical formula' , text ) return text train [ 'question_text' ] = train [ 'question_text' ] . map ( clean_latex_tag )
753	print ( "Train data" ) train . isna ( ) . sum ( ) print ( "Test data" ) test . isna ( ) . sum ( )
754	cols_drop_at_train = list ( set ( more_than_90_NA_or_same_value_train + many_na_train ) ) cols_drop_at_test = list ( set ( more_than_90_NA_or_same_value_test + many_na_test ) ) print ( "Columns to be dropped in train" , len ( cols_drop_at_train ) ) print ( "Columns to be dropped in test" , len ( cols_drop_at_test ) ) print ( "columns are @ train:" , cols_drop_at_train ) print ( "columns are @ test:" , cols_drop_at_train )
755	for col in m_list : print ( "For the " + str ( col ) ) print ( train [ col ] . value_counts ( ) )
756	someFeature_list = [ 'id_36' , 'id_35' , 'id_34' , 'id_28' , 'id_29' , 'id_12' , 'id_15' , 'id_16' ] a4_dims = ( 20 , 20 ) co = 0 ax = sns . countplot ( x = someFeature_list [ co ] , hue = 'isFraud' , data = train ) ax . set_xlabel ( someFeature_list [ co ] ) ax . set_ylabel ( 'Number of Occurrences' ) ax . set_xticklabels ( ax . get_xticklabels ( ) , rotation = 45 ) co += 1 plt . tight_layout ( )
757	from sklearn import preprocessing for col in train . columns : if train [ col ] . dtype == 'object' :
758	train = clean_inf_nan ( train ) test = clean_inf_nan ( test ) for i in train . columns : train [ i ] . fillna ( train [ i ] . median ( ) , inplace = True )
759	metrics_dict = { 'auc' : { 'lgb_metric_name' : eval_auc , 'catboost_metric_name' : 'AUC' , 'sklearn_scoring_function' : metrics . roc_auc_score } , } result_dict = { } if averaging == 'usual' :
760	def latex_tag_in_text ( text ) : x = text . lower ( ) return ' [ math ] ' in x train [ 'latex_tag_in_text' ] = train [ 'question_text' ] . apply ( lambda x : latex_tag_in_text ( x ) )
761	EMBED_SIZE = 300 MAX_WORDS_LEN = 70 MAX_VOCAB_FEATURES = 200000
762	def clean_latex_tag ( text ) : corr_t = [ ] for t in text . split ( " " ) : t = t . strip ( ) if t != '' : corr_t . append ( t ) text = ' ' . join ( corr_t ) text = re . sub ( '(\[ math \]).+(\[ / math \])' , 'mathematical formula' , text ) return text
763	train_df = pd . read_csv ( "../input/train.csv" , encoding = 'utf8' ) test_df = pd . read_csv ( "../input/test.csv" , encoding = 'utf8' ) all_test_texts = '' . join ( test_df . question_text . values . tolist ( ) ) print ( 'Train:' , train_df . shape ) print ( 'Test:' , test_df . shape )
764	for space in spaces : text = text . replace ( space , ' ' ) text = text . strip ( ) text = re . sub ( '\s+' , ' ' , text ) return text
765	for punc in all_punct : if punc in text : text = text . replace ( punc , f' {punc} ' ) return text
766	text = remove_space ( text ) text = clean_special_punctuations ( text ) text = clean_number ( text ) text = pre_clean_rare_words ( text ) text = decontracted ( text ) text = clean_latex ( text ) text = clean_misspell ( text ) text = spacing_punctuation ( text ) text = spacing_some_connect_words ( text ) text = clean_bad_case_words ( text ) text = clean_repeat_words ( text ) text = remove_space ( text ) return text def text_clean_wrapper ( df ) : df [ "question_text" ] = df [ "question_text" ] . apply ( preprocess ) return df
767	cur_vocabulary = set ( ) for text in tqdm ( train_df [ 'question_text' ] . values . tolist ( ) + test_df [ 'question_text' ] . values . tolist ( ) ) : words = text . split ( ' ' ) cur_vocabulary . update ( set ( words ) ) bug_punc_spacing_words_mapping = { } for vocab in cur_vocabulary : if '-' in vocab :
768	with open ( path , 'rb' ) as f : img = Image . open ( f ) return img . convert ( 'RGB' ) def accimage_loader ( path ) : import accimage try : return accimage . Image ( path ) except IOError :
769	return pil_loader ( path ) def default_loader ( path ) : from torchvision import get_image_backend if get_image_backend ( ) == 'accimage' : return accimage_loader ( path ) else : return pil_loader ( path ) class DogCatDataset ( Dataset ) :
770	model = models . resnet18 ( pretrained = True ) fc_in_features = model . fc . in_features model . fc = nn . Linear ( fc_in_features , 2 ) model = model . to ( device ) loss_fn = nn . CrossEntropyLoss ( ) optimizer = optim . SGD ( model . parameters ( ) , lr = 0.001 , momentum = 0.9 )
771	if avg_val_loss < best_valid_loss : best_valid_loss = avg_val_loss best_valid_acc = valid_acc didnt_improve_count = 0 best_model_wts = copy . deepcopy ( model . state_dict ( ) ) else : didnt_improve_count += 1 if didnt_improve_count > 2 : break print ( 'Best valid-loss={:.4f} \t valid-acc={:.4f}' . format ( best_valid_loss , best_valid_acc ) ) print ( 'save and load best model weights' ) model . load_state_dict ( best_model_wts ) torch . save ( model . state_dict ( ) , 'best_resnet18_weights.model' )
772	inputs , classes = next ( iter ( dataloaders [ 'train' ] ) ) inputs , classes = inputs [ : 4 ] , classes [ : 4 ] ground_truth = [ class_names [ i ] for i in classes ]
773	out = torchvision . utils . make_grid ( inputs ) inputs = inputs . cuda ( ) preds = model ( inputs ) _ , preds = torch . max ( preds , 1 ) predict_class = [ class_names [ i ] for i in preds ] imshow ( out , title = f"Truth : {ground_truth}\nPredict: {predict_class}" )
774	test_pids = test_df . PetID . values input_tensor = torch . zeros ( 1 , 3 , 224 , 224 ) test_image_features = { } for petid in tqdm ( test_pids ) : test_img = f"../input/test_images/{petid}-1.jpg" if not os . path . exists ( test_img ) : continue test_img = Image . open ( test_img ) test_img = extract_transform ( test_img ) input_tensor [ 0 , : , : , : ] = test_img input_tensor = input_tensor . cuda ( ) model ( input_tensor ) test_image_features [ petid ] = image_features [ 0 ] image_features . clear ( )
775	times_series_cntr = times_series_cntr . groupby ( [ 'Date' , 'province' , 'country' ] ) [ 'ConfirmedCases' ] . max ( ) \ . groupby ( [ 'Date' , 'country' ] ) . sum ( ) \ . reset_index ( )
776	top_countries = countries . index [ : 10 ] top_countries_tm = times_series_cntr [ times_series_cntr [ 'country' ] . isin ( top_countries ) ] plt . xticks ( rotation = 45 ) ax = sns . lineplot ( x = top_countries_tm . index , y = "ConfirmedCases" , hue = "country" , data = top_countries_tm ) . set_title ( 'Cumulative line' ) plt . legend ( loc = 2 , prop = { 'size' : 12 } ) ;
777	nb_transformer = NBTransformer ( alpha = 1 ) . fit ( train_word_tfidf_features , y_train ) train_word_tfidf_features = nb_transformer . transform ( train_word_tfidf_features ) test_word_tfidf_features = nb_transformer . transform ( test_word_tfidf_features ) nb_transformer = NBTransformer ( alpha = 1 ) . fit ( train_char_tfidf_features , y_train ) train_char_tfidf_features = nb_transformer . transform ( train_char_tfidf_features ) test_char_tfidf_features = nb_transformer . transform ( test_char_tfidf_features )
778	ds_size = img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ) , nn . Sigmoid ( ) ) def forward ( self , img ) :
779	valid = Variable ( torch . cuda . FloatTensor ( imgs . shape [ 0 ] , 1 ) . fill_ ( 1.0 ) , requires_grad = False ) . cuda ( ) fake = Variable ( torch . cuda . FloatTensor ( imgs . shape [ 0 ] , 1 ) . fill_ ( 0.0 ) , requires_grad = False ) . cuda ( ) real_imgs = Variable ( imgs . type ( torch . cuda . FloatTensor ) )
780	plt . hist ( train_transaction [ 'TransactionDT' ] , label = 'train' ) plt . hist ( test_transaction [ 'TransactionDT' ] , label = 'test' ) plt . legend ( ) plt . title ( 'Distribution of TransactionDT' )
781	plt . figure ( figsize = ( 12 , 12 ) ) a = train_full . groupby ( 'ProductCD' ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) . plot . bar ( stacked = True ) a . set_title ( 'Rate of Fraud by Product Category' , fontsize = 15 ) plt . xticks ( rotation = 'horizontal' )
782	plt . figure ( figsize = ( 12 , 12 ) ) b = train_full . groupby ( 'card4' ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) . plot . bar ( stacked = True ) b . set_title ( 'Rate of Fraud by Card Network' , fontsize = 15 ) plt . xticks ( rotation = 'horizontal' )
783	plt . figure ( figsize = ( 12 , 12 ) ) b = train_full . groupby ( 'card6' ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) . plot . bar ( stacked = True ) b . set_title ( 'Rate of Fraud by Card Type' , fontsize = 15 ) plt . xticks ( rotation = 'horizontal' )
784	protonmail_fraud = len ( train_full [ ( train_full [ 'P_parent_emaildomain' ] == "protonmail" ) & ( train_full [ 'isFraud' ] == 1 ) ] ) protonmail_non_fraud = len ( train_full [ ( train_full [ 'P_parent_emaildomain' ] == "protonmail" ) & ( train_full [ 'isFraud' ] == 0 ) ] ) protonmail_fraud_rate = protonmail_fraud / ( protonmail_fraud + protonmail_non_fraud ) print ( "Number of protonmail fraud transactions:" , protonmail_fraud ) print ( "Number of protonmail non-fraud transactions:" , protonmail_non_fraud ) print ( "Protonmail fraud rate:" , protonmail_fraud_rate )
785	train_full [ 'major_os' ] = train_full [ "id_30" ] . str . split ( ' ' , expand = True ) [ [ 0 ] ] visualize_cat_cariable ( 'major_os' )
786	if len ( df [ variable ] . unique ( ) ) <= 1 : print ( '{} is a homogeneous set' . format ( variable ) ) return
787	train_preds = np . zeros ( ( len ( train_features ) ) ) test_preds = np . zeros ( ( len ( test_features ) ) ) x_test = np . array ( test_features ) x_test_cuda = torch . tensor ( x_test , dtype = torch . float ) . cuda ( ) test = torch . utils . data . TensorDataset ( x_test_cuda ) test_loader = torch . utils . data . DataLoader ( test , batch_size = batch_size , shuffle = False ) avg_losses_f = [ ] avg_val_losses_f = [ ]
788	model = Simple_NN ( 200 , 16 ) model . cuda ( ) optimizer = torch . optim . Adam ( model . parameters ( ) , lr = 0.001 , weight_decay = 1e-5 )
789	step_size = 2000 base_lr , max_lr = 0.001 , 0.005 optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ( ) ) , lr = max_lr ) scheduler = CyclicLR ( optimizer , base_lr = base_lr , max_lr = max_lr , step_size = step_size , mode = 'exp_range' , gamma = 0.99994 )
790	train_features = train_df . drop ( [ 'target' , 'ID_code' ] , axis = 1 ) test_features = test_df . drop ( [ 'ID_code' ] , axis = 1 ) train_target = train_df [ 'target' ]
791	train_features = train_df . drop ( [ 'target' , 'ID_code' ] , axis = 1 ) test_features = test_df . drop ( [ 'ID_code' ] , axis = 1 ) train_target = train_df [ 'target' ]
792	param = { 'num_leaves' : 7 , 'learning_rate' : 0.01 , 'feature_fraction' : 0.04 , 'max_depth' : 17 , 'objective' : 'binary' , 'boosting_type' : 'gbdt' , 'metric' : 'auc' , }
793	my_submission_nn = pd . DataFrame ( { "ID_code" : id_code_test , "target" : test_preds } ) my_submission_lbgm = pd . DataFrame ( { "ID_code" : id_code_test , "target" : predictions } ) my_submission_esemble = pd . DataFrame ( { "ID_code" : id_code_test , "target" : esemble_pred } )
794	data_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_images' mask_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks' train_labels = pd . read_csv ( '/kaggle/input/prostate-cancer-grade-assessment/train.csv' )
795	import os import gc print ( os . listdir ( "../input" ) ) import numpy as np import pandas as pd import time
796	train [ 'exist_ship' ] = train [ 'EncodedPixels' ] . fillna ( 0 ) train . loc [ train [ 'exist_ship' ] != 0 , 'exist_ship' ] = 1 del train [ 'EncodedPixels' ]
797	print ( len ( train [ 'ImageId' ] ) ) print ( train [ 'ImageId' ] . value_counts ( ) . shape [ 0 ] ) train_gp = train . groupby ( 'ImageId' ) . sum ( ) . reset_index ( ) train_gp . loc [ train_gp [ 'exist_ship' ] > 0 , 'exist_ship' ] = 1
798	print ( train_gp [ 'exist_ship' ] . value_counts ( ) ) train_gp = train_gp . sort_values ( by = 'exist_ship' ) train_gp = train_gp . drop ( train_gp . index [ 0 : 100000 ] )
799	print ( train_gp [ 'exist_ship' ] . value_counts ( ) ) train_sample = train_gp . sample ( 5000 ) print ( train_sample [ 'exist_ship' ] . value_counts ( ) ) print ( train_sample . shape )
800	from sklearn . preprocessing import OneHotEncoder targets = data_target . reshape ( len ( data_target ) , - 1 ) enc = OneHotEncoder ( ) enc . fit ( targets ) targets = enc . transform ( targets ) . toarray ( ) print ( targets . shape )
801	from sklearn . model_selection import train_test_split x_train , x_val , y_train , y_val = train_test_split ( data , targets , test_size = 0.2 ) x_train . shape , x_val . shape , y_train . shape , y_val . shape
802	from keras import optimizers epochs = 10 lrate = 0.001 decay = lrate / epochs sgd = optimizers . SGD ( lr = lrate , momentum = 0.9 , decay = decay , nesterov = False ) model_final . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] ) model_final . summary ( )
803	from sklearn . preprocessing import OneHotEncoder targets_predict = data_target_predict . reshape ( len ( data_target_predict ) , - 1 ) enc = OneHotEncoder ( ) enc . fit ( targets_predict ) targets_predict = enc . transform ( targets_predict ) . toarray ( ) print ( targets_predict . shape )
804	drop_cols = [ "bin_0" ] ddall [ "ord_5a" ] = ddall [ "ord_5" ] . str [ 0 ] ddall [ "ord_5b" ] = ddall [ "ord_5" ] . str [ 1 ] drop_cols . append ( "ord_5" )
805	for col in [ "nom_5" , "nom_6" , "nom_7" , "nom_8" , "nom_9" ] : train_vals = set ( dd0 [ col ] . unique ( ) ) test_vals = set ( ddtest0 [ col ] . unique ( ) ) xor_cat_vals = train_vals ^ test_vals if xor_cat_vals : ddall . loc [ ddall [ col ] . isin ( xor_cat_vals ) , col ] = "xor"
806	ohc = scipy . sparse . hstack ( [ ohc1 ] + thermos ) . tocsr ( ) display ( ohc ) X_train = ohc [ : num_train ] X_test = ohc [ num_train : ] y_train = dd0 [ "target" ] . values
807	from oauth2client . client import GoogleCredentials creds = GoogleCredentials . get_application_default ( ) import getpass vcode = getpass . getpass ( )
808	import json import time val_data = json . load ( open ( 'drive/kaggle/validation.json' ) ) for i in range ( len ( val_data [ 'images' ] ) ) : img_url = val_data [ 'images' ] [ i ] [ 'url' ]
809	import json import time test_data = json . load ( open ( 'drive/kaggle/test.json' ) )
810	sub1 = pd . read_csv ( '../input/giba-r-data-table-simple-features-1-17-lb/submission-giba-1.csv' ) sub2 = pd . read_csv ( '../input/keras-neural-net-for-champs/workingsubmission-test.csv' ) sub3 = pd . read_csv ( '../input/no-memory-reduction-workflow-for-each-type-lb-1-28/LGB_2019-07-11_-1.4378.csv' ) sub4 = pd . read_csv ( '../input/giba-r-data-table-simplefeat-cyv-interaction/submission-2.csv' ) print ( sub1 [ 'scalar_coupling_constant' ] . describe ( ) ) print ( sub2 [ 'scalar_coupling_constant' ] . describe ( ) ) print ( sub3 [ 'scalar_coupling_constant' ] . describe ( ) ) print ( sub4 [ 'scalar_coupling_constant' ] . describe ( ) )
811	drop_cols = [ "bin_0" ] ddall [ "ord_5a" ] = ddall [ "ord_5" ] . str [ 0 ] ddall [ "ord_5b" ] = ddall [ "ord_5" ] . str [ 1 ] drop_cols . append ( "ord_5" )
812	for col in [ "nom_5" , "nom_6" , "nom_7" , "nom_8" , "nom_9" ] : train_vals = set ( dd0 [ col ] . unique ( ) ) test_vals = set ( ddtest0 [ col ] . unique ( ) ) xor_cat_vals = train_vals ^ test_vals if xor_cat_vals : ddall . loc [ ddall [ col ] . isin ( xor_cat_vals ) , col ] = "xor"
813	ohc = scipy . sparse . hstack ( [ ohc1 ] + thermos ) . tocsr ( ) display ( ohc ) X_train = ohc [ : num_train ] X_test = ohc [ num_train : ] y_train = dd0 [ "target" ] . values
814	PROJECT_ID = 'YOUR_PROJECT_ID' from google . cloud import storage storage_client = storage . Client ( project = PROJECT_ID )
815	from kaggle_secrets import UserSecretsClient user_secrets = UserSecretsClient ( ) user_credential = user_secrets . get_gcloud_credential ( )
816	tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu )
817	image = parsed_example [ 'image/encoded' ] image = tf . io . decode_jpeg ( image ) image = tf . cast ( image , tf . float32 ) image = tf . math . divide ( tf . subtract ( image , 128.0 ) , 128.0 )
818	label = parsed_example [ 'image/class/label' ] label = tf . reduce_min ( tf . where ( tf . equal ( unique_landmark_ids , label ) ) ) return image , label def create_dataset ( file_pattern , unique_landmark_ids , augmentation : bool = False ) : AUTO = tf . data . experimental . AUTOTUNE ignore_order = tf . data . Options ( ) ignore_order . experimental_deterministic = False filenames = tf . io . gfile . glob ( file_pattern ) dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) . shuffle ( 1000 )
819	import numpy as np from sklearn . metrics import log_loss from sklearn . base import BaseEstimator from scipy . optimize import minimize
820	res = minimize ( objf_ens_optA , x0 , args = ( Xs , y , self . n_class ) , method = 'SLSQP' , bounds = bounds , constraints = cons ) self . w = res . x return self def predict_proba ( self , X ) :
821	Xs = np . hsplit ( X , X . shape [ 1 ] / self . n_class ) y_pred = np . zeros ( Xs [ 0 ] . shape ) for i in range ( len ( self . w ) ) : y_pred += Xs [ i ] * self . w [ i ] return y_pred
822	Xs = np . hsplit ( X , X . shape [ 1 ] / self . n_class ) y_pred = np . zeros ( Xs [ 0 ] . shape ) for i in range ( len ( self . w ) ) : y_pred [ : , i % self . n_class ] += \ Xs [ int ( i / self . n_class ) ] [ : , i % self . n_class ] * self . w [ i ] return y_pred
823	n_classes = 12 data , labels = make_classification ( n_samples = 2000 , n_features = 100 , n_informative = 50 , n_classes = n_classes , random_state = random_state ) X , X_test , y , y_test = train_test_split ( data , labels , test_size = 0.2 , random_state = random_state ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = random_state ) print ( 'Data shape:' ) print ( 'X_train: %s, X_valid: %s, X_test: %s \n' % ( X_train . shape , X_valid . shape , X_test . shape ) )
824	X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = random_state ) print ( 'Data shape:' ) print ( 'X_train: %s, X_valid: %s, X_test: %s \n' % ( X_train . shape , X_valid . shape , X_test . shape ) )
825	clfs = { 'LR' : LogisticRegression ( random_state = random_state ) , 'SVM' : SVC ( probability = True , random_state = random_state ) , 'RF' : RandomForestClassifier ( n_estimators = 100 , n_jobs = - 1 , random_state = random_state ) , 'GBM' : GradientBoostingClassifier ( n_estimators = 50 , random_state = random_state ) , 'ETC' : ExtraTreesClassifier ( n_estimators = 100 , n_jobs = - 1 , random_state = random_state ) , 'KNN' : KNeighborsClassifier ( n_neighbors = 30 ) }
826	p_valid = [ ] p_test = [ ] print ( 'Performance of individual classifiers (1st layer) on X_test' ) print ( '------------------------------------------------------------' ) for nm , clf in clfs . items ( ) :
827	lr = LogisticRegressionCV ( Cs = 10 , dual = False , fit_intercept = True , intercept_scaling = 1.0 , max_iter = 100 , multi_class = 'ovr' , n_jobs = 1 , penalty = 'l2' , random_state = random_state , solver = 'lbfgs' , tol = 0.0001 ) lr . fit ( XV , y_valid ) y_lr = lr . predict_proba ( XT ) print ( '{:20s} {:2s} {:1.7f}' . format ( 'Log_Reg:' , 'logloss =>' , log_loss ( y_test , y_lr ) ) )
828	lr = LogisticRegressionCV ( Cs = 10 , dual = False , fit_intercept = True , intercept_scaling = 1.0 , max_iter = 100 , multi_class = 'ovr' , n_jobs = 1 , penalty = 'l2' , random_state = random_state , solver = 'lbfgs' , tol = 0.0001 ) lr . fit ( XV , y_valid ) y_lr = lr . predict_proba ( XT ) print ( '{:20s} {:2s} {:1.7f}' . format ( 'Log_Reg:' , 'logloss =>' , log_loss ( y_test , y_lr ) ) )
829	data , labels = make_classification ( n_samples = 2000 , n_features = 100 , n_informative = 50 , n_classes = n_classes , random_state = random_state ) X , X_test , y , y_test = train_test_split ( data , labels , test_size = 0.2 , random_state = random_state ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = random_state )
830	DISCOUNT = 0.90 HNORM = 1000 IDLE_COST = 2 N_RTG = 6 SAVE_DIR = "/checkpoints/" ship_action_encode = { 0 : ShipAction . NORTH , 1 : ShipAction . SOUTH , 2 : ShipAction . WEST , 3 : ShipAction . EAST , 4 : None , 5 : ShipAction . CONVERT } shipyard_action_encode = { 0 : None , 1 : ShipyardAction . SPAWN }
831	self . prev_und_gains = defaultdict ( lambda : 0 ) self . paths = defaultdict ( list ) def process_0 ( self , observation , configuration ) :
832	x , y = self . state0_ships_loc [ ID ] last_action = ( int ( self . transitions [ ID ] [ 2 ] ) == np . arange ( 5 ) ) * np . array ( [ - 1 , 1 , - 1 , 1 , 0 ] ) crimeX , crimeY = x + np . sum ( last_action [ : 2 ] ) , y + np . sum ( last_action [ 2 : 5 ] ) team_killerFound = False for IDsus in state1_ships_loc :
833	print ( a_sample [ 2 ] [ 0 ] [ 5 : 16 , 5 : 16 , 0 ] ) print ( "Action: %s" % ship_action_encode [ a_sample [ 2 ] [ 2 ] ] ) print ( a_sample [ 2 ] [ 3 ] [ 5 : 16 , 5 : 16 , 0 ] ) print ( "Reward: %f" % a_sample [ 1 ] [ 5 ] )
834	print ( len ( memory ) ) augmentData ( memory ) print ( len ( memory ) )
835	if itr % add_to_memory_every == 0 : new_samples = generateSamples ( batch_size = batch_size , expl_param = epsilon ) augmentData ( new_samples ) memory . extend ( new_samples ) while len ( memory ) > max_size_memory : memory . pop ( 0 )
836	if dqn . target_update_method == "periodic" : if itr % update_target_every == 0 : dqn . update_target ( ) if dqn . target_update_method == "polyak" : dqn . update_target ( ) losses . append ( loss ) itr_last = itr if itr % 1000 == 0 : print ( "---- Iteration %d ----" % itr ) print ( "Epsilon: %f" % epsilon ) print ( "Loss: %f" % loss ) print ( time . time ( ) - start )
837	from plotly . offline import download_plotlyjs , init_notebook_mode , iplot from plotly import subplots import plotly . express as px import plotly . figure_factory as ff from plotly . graph_objs import * from plotly . graph_objs . layout import Margin , YAxis , XAxis init_notebook_mode ( )
838	train_df = pd . read_csv ( '../input/understanding_cloud_organization/train.csv' ) train_fns = sorted ( glob ( TRAIN_PATH + '*.jpg' ) ) train_image_path = os . path . join ( '/kaggle/input/understanding_cloud_organization' , 'train_images' ) print ( 'There are {} images in the train set.' . format ( len ( train_fns ) ) )
839	labels = 'Train' , 'Test' sizes = [ len ( train_fns ) , len ( test_fns ) ] explode = ( 0 , 0.1 ) fig , ax = plt . subplots ( figsize = ( 6 , 6 ) ) ax . pie ( sizes , explode = explode , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 ) ax . axis ( 'equal' ) ax . set_title ( 'Train and Test Sets' ) plt . show ( )
840	labels = 'Train' , 'Test' sizes = [ len ( train_fns ) , len ( test_fns ) ] explode = ( 0 , 0.1 ) fig , ax = plt . subplots ( figsize = ( 6 , 6 ) ) ax . pie ( sizes , explode = explode , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 ) ax . axis ( 'equal' ) ax . set_title ( 'Train and Test Sets' ) plt . show ( )
841	print ( 'Total number of images: %s' % len ( train_df [ 'Image' ] . unique ( ) ) ) print ( 'Images with at least one label: %s' % len ( train_df [ train_df [ 'EncodedPixels' ] != 'NaN' ] [ 'Image' ] . unique ( ) ) )
842	labels = 'Fish' , 'Flower' , 'Gravel' , 'Sugar' sizes = [ fish , flower , gravel , sugar ] fig , ax = plt . subplots ( figsize = ( 6 , 6 ) ) ax . pie ( sizes , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 ) ax . axis ( 'equal' ) ax . set_title ( 'Cloud Types' ) plt . show ( )
843	def get_dummy_val ( row , cloud_type ) : if cloud_type == 'fish' : return row [ 'Label_Fish' ] * ( row [ 'EncodedPixels' ] != - 1 ) if cloud_type == 'flower' : return row [ 'Label_Flower' ] * ( row [ 'EncodedPixels' ] != - 1 ) if cloud_type == 'gravel' : return row [ 'Label_Gravel' ] * ( row [ 'EncodedPixels' ] != - 1 ) if cloud_type == 'sugar' : return row [ 'Label_Sugar' ] * ( row [ 'EncodedPixels' ] != - 1 )
844	corr_df [ 'Label_Fish' ] = corr_df . apply ( lambda row : get_dummy_val ( row , 'fish' ) , axis = 1 ) corr_df [ 'Label_Flower' ] = corr_df . apply ( lambda row : get_dummy_val ( row , 'flower' ) , axis = 1 ) corr_df [ 'Label_Gravel' ] = corr_df . apply ( lambda row : get_dummy_val ( row , 'gravel' ) , axis = 1 ) corr_df [ 'Label_Sugar' ] = corr_df . apply ( lambda row : get_dummy_val ( row , 'sugar' ) , axis = 1 )
845	labels = 'Non-empty' , 'Empty' sizes = [ train_df . EncodedPixels . count ( ) , len ( train_df ) - train_df . EncodedPixels . count ( ) ] explode = ( 0 , 0.1 ) fig , ax = plt . subplots ( figsize = ( 6 , 6 ) ) ax . pie ( sizes , explode = explode , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 ) ax . axis ( 'equal' ) ax . set_title ( 'Non-empty and Empty Masks' ) plt . show ( )
846	corrs = np . corrcoef ( corr_df . values . T ) sns . set ( font_scale = 1 ) sns . set ( rc = { 'figure.figsize' : ( 7 , 7 ) } ) hm = sns . heatmap ( corrs , cbar = True , annot = True , square = True , fmt = '.2f' , yticklabels = [ 'Fish' , 'Flower' , 'Gravel' , 'Sugar' ] , xticklabels = [ 'Fish' , 'Flower' , 'Gravel' , 'Sugar' ] ) . set_title ( 'Cloud type correlation heatmap' ) fig = hm . get_figure ( )
847	train_widths , train_heights , max_train , min_train = get_img_size ( train = True ) test_widths , test_heights , max_test , min_test = get_img_size ( train = False ) print ( 'Maximum width for training set is {}' . format ( max ( train_widths ) ) ) print ( 'Minimum width for training set is {}' . format ( min ( train_widths ) ) ) print ( 'Maximum height for training set is {}' . format ( max ( train_heights ) ) ) print ( 'Minimum height for training set is {}' . format ( min ( train_heights ) ) )
848	im_df = train_df [ train_df [ 'Image' ] == image_id ] . fillna ( '-1' ) im_df = im_df [ im_df [ 'EncodedPixels' ] != '-1' ] . groupby ( 'Label' ) . count ( ) index = im_df . index all_labels = [ 'Fish' , 'Flower' , 'Gravel' , 'Sugar' ] labels = '' for label in all_labels : if label in index : labels = labels + ' ' + label return labels
849	image = Image . open ( images [ rnd_indices [ im ] ] ) i = im // width j = im % width
850	axs [ i , j ] . imshow ( image ) axs [ i , j ] . axis ( 'off' ) axs [ i , j ] . set_title ( get_labels ( images [ rnd_indices [ im ] ] . split ( '/' ) [ - 1 ] ) )
851	rle = im_df . loc [ line_id ] [ 'EncodedPixels' ] if rle != '-1' : np_mask = rle_to_mask ( rle , shape [ 0 ] , shape [ 1 ] ) np_mask = np . clip ( np_mask , 0 , 1 ) else :
852	side_by_side = np . hstack ( [ segmap . draw_on_image ( np . asarray ( image ) ) ] ) . reshape ( np . asarray ( image ) . shape ) fig , ax = plt . subplots ( figsize = ( 6 , 4 ) ) ax . axis ( 'off' ) plt . title ( im_df . loc [ line_id ] [ 'Label' ] ) ax . imshow ( side_by_side )
853	fish = get_mask_by_image_id ( images [ rnd_indices [ im ] ] , 'Fish' ) flower = get_mask_by_image_id ( images [ rnd_indices [ im ] ] , 'Flower' ) gravel = get_mask_by_image_id ( images [ rnd_indices [ im ] ] , 'Gravel' ) sugar = get_mask_by_image_id ( images [ rnd_indices [ im ] ] , 'Sugar' )
854	ax [ im , 0 ] . imshow ( im_fish ) ax [ im , 0 ] . axis ( 'off' ) ax [ im , 0 ] . set_title ( 'Fish' )
855	ax [ im , 1 ] . imshow ( im_flower ) ax [ im , 1 ] . axis ( 'off' ) ax [ im , 1 ] . set_title ( 'Flower' )
856	ax [ im , 2 ] . imshow ( im_gravel ) ax [ im , 2 ] . axis ( 'off' ) ax [ im , 2 ] . set_title ( 'Gravel' )
857	ax [ im , 3 ] . imshow ( im_sugar ) ax [ im , 3 ] . axis ( 'off' ) ax [ im , 3 ] . set_title ( 'Sugar' ) plt . suptitle ( 'Sample images from the train set' ) plt . show ( )
858	fish_mask = get_mask_by_image_id ( image_id , 'Fish' ) flower_mask = get_mask_by_image_id ( image_id , 'Flower' ) gravel_mask = get_mask_by_image_id ( image_id , 'Gravel' ) sugar_mask = get_mask_by_image_id ( image_id , 'Sugar' )
859	segmap = SegmentationMapOnImage ( segmap , shape = image . shape , nb_classes = 5 ) return segmap def draw_labels ( image , np_mask , label ) :
860	if np . sum ( np_mask ) > 0 : x , y = 0 , 0 x , y = np . argwhere ( np_mask == 1 ) [ 0 ] image = imgaug . imgaug . draw_text ( image , x , y , label , color = ( 255 , 255 , 255 ) , size = 50 ) return image def draw_segmentation_maps ( image_id ) :
861	fish_mask = get_mask_by_image_id ( image_id , 'Fish' ) flower_mask = get_mask_by_image_id ( image_id , 'Flower' ) gravel_mask = get_mask_by_image_id ( image_id , 'Gravel' ) sugar_mask = get_mask_by_image_id ( image_id , 'Sugar' )
862	image = np . asarray ( segmap . draw_on_image ( np . asarray ( image ) ) ) . reshape ( np . asarray ( image ) . shape ) image = draw_labels ( image , fish_mask , 'Fish' ) image = draw_labels ( image , flower_mask , 'Flower' ) image = draw_labels ( image , gravel_mask , 'Gravel' ) image = draw_labels ( image , sugar_mask , 'Sugar' ) return image
863	side_by_side = np . hstack ( [ image ] ) labels = get_labels ( image_id . split ( '/' ) [ - 1 ] ) fig , ax = plt . subplots ( figsize = ( 15 , 7 ) ) ax . axis ( 'off' ) plt . title ( 'Segmentation maps:' + labels ) plt . legend ( ) ax . imshow ( side_by_side )
864	image = draw_segmentation_maps ( images [ rnd_indices [ im ] ] ) i = im // width j = im % width
865	axs [ i , j ] . imshow ( image ) axs [ i , j ] . axis ( 'off' ) axs [ i , j ] . set_title ( get_labels ( images [ rnd_indices [ im ] ] . split ( '/' ) [ - 1 ] ) )
866	self . reduce_lr_on_plateau ( ) def get_pr_auc_history ( self ) : return self . history
867	from tensorflow . keras . applications . inception_resnet_v2 import InceptionResNetV2 def get_model ( ) : base_model = model = ResNeXt101 ( ... , backend = tf . keras . backend , layers = tf . keras . layers , weights = 'imagenet' , models = tf . keras . models , utils = tf . keras . utils ) x = base_model . output y_pred = Dense ( 4 , activation = 'sigmoid' ) ( x ) return Model ( inputs = base_model . input , outputs = y_pred ) model = get_model ( )
868	def plot_with_dots ( ax , np_array ) : ax . scatter ( list ( range ( 1 , len ( np_array ) + 1 ) ) , np_array , s = 50 ) ax . plot ( list ( range ( 1 , len ( np_array ) + 1 ) ) , np_array )
869	train [ 'Date' ] = pd . to_datetime ( train [ 'Date' ] , format = '%Y-%m-%d' ) store_id = train . Store . unique ( ) [ 0 ] print ( store_id ) store_rows = train [ train [ 'Store' ] == store_id ] print ( store_rows . shape ) store_rows . resample ( '1D' , on = 'Date' ) [ 'Sales' ] . sum ( ) . plot . line ( figsize = ( 18 , 8 ) )
870	test [ 'Date' ] = pd . to_datetime ( test [ 'Date' ] , format = '%Y-%m-%d' ) store_test_rows = test [ test [ 'Store' ] == store_id ] print ( store_test_rows . shape ) store_test_rows [ 'Date' ] . min ( ) , store_test_rows [ 'Date' ] . max ( )
871	store [ 'Promo2SinceWeek' ] . fillna ( 0 , inplace = True ) store [ 'Promo2SinceYear' ] . fillna ( store [ 'Promo2SinceYear' ] . mode ( ) [ 0 ] , inplace = True ) store [ 'PromoInterval' ] . fillna ( store [ 'PromoInterval' ] . mode ( ) [ 0 ] , inplace = True )
872	data_merged = train . merge ( store , on = 'Store' , how = 'left' ) print ( train . shape ) print ( data_merged . shape ) print ( data_merged . isnull ( ) . sum ( ) . sum ( ) )
873	flatten = lambda l : [ item for sublist in l for item in sublist ] labels = list ( set ( flatten ( [ l . split ( ' ' ) for l in train [ 'tags' ] . values ] ) ) ) label_map = { l : i for i , l in enumerate ( labels ) } inv_label_map = { i : l for l , i in label_map . items ( ) }
874	for f , tags in tqdm ( train . values , miniters = 1000 ) : targets = np . zeros ( 17 ) for t in tags . split ( ' ' ) : targets [ label_map [ t ] ] = 1 y_train . append ( targets ) y_train = np . array ( y_train , np . uint8 )
875	split = .2 index = np . arange ( len ( train ) ) for i in tqdm ( range ( 0 , 17 ) ) : sss = StratifiedShuffleSplit ( n_splits = 2 , test_size = split , random_state = i ) for train_index , test_index in sss . split ( index , y_train [ : , i ] ) : X_train , X_test = index [ train_index ] , index [ test_index ]
876	size = 1024 for img in hair_images : image = cv2 . imread ( BASE_PATH + '/jpeg/train/' + img + '.jpg' ) image_resize = cv2 . resize ( image , ( size , size ) ) image_resize = cv2 . cvtColor ( image_resize , cv2 . COLOR_BGR2RGB ) plt . imshow ( image_resize ) plt . show ( )
877	image = cv2 . imread ( BASE_PATH + '/jpeg/train/' + image_name + '.jpg' ) image_resize = cv2 . resize ( image , ( size , size ) ) grayScale = cv2 . cvtColor ( image_resize , cv2 . COLOR_RGB2GRAY )
878	final_image = cv2 . inpaint ( image_resize , threshold , 1 , cv2 . INPAINT_TELEA ) threshold = cv2 . bitwise_not ( threshold ) image_resize = cv2 . cvtColor ( image_resize , cv2 . COLOR_BGR2RGB ) final_image = cv2 . cvtColor ( final_image , cv2 . COLOR_BGR2RGB ) return image_resize , threshold , final_image
879	hair_trans = albu . Compose ( [ albu . ShiftScaleRotate ( rotate_limit = [ - 45 , 45 ] , scale_limit = [ - 0.1 , 0.1 ] , shift_limit = [ - 0.1 , 0.15 ] , border_mode = 3 , value = 0 , p = 1. ) ] )
880	rate = train [ "species" ] . value_counts ( ) . sort_values ( ) / 264 print ( f'{"Target" :-<40} {"rate":-<20}' ) for n in range ( len ( rate ) ) : print ( f'{rate.index[n] :-<40} {rate[n]}' )
881	longitude = pd . to_numeric ( train [ 'longitude' ] , errors = 'coerce' ) latitude = pd . to_numeric ( train [ 'latitude' ] , errors = 'coerce' ) df = pd . concat ( [ longitude , latitude ] , axis = 1 )
882	N = 5 ebird_code_simple = sample ( list ( train [ "ebird_code" ] . unique ( ) ) , N ) AudioProcessing ( ) . PlotSampleWave ( nrows = N , captions = ebird_code_simple , df = train )
883	categorical_list = [ ] numerical_list = [ ] for i in application . columns . tolist ( ) : if application [ i ] . dtype == 'object' : categorical_list . append ( i ) else : numerical_list . append ( i ) print ( 'Number of categorical features:' , str ( len ( categorical_list ) ) ) print ( 'Number of numerical features:' , str ( len ( numerical_list ) ) )
884	X = application . drop ( [ 'SK_ID_CURR' , 'TARGET' ] , axis = 1 ) y = application . TARGET feature_name = X . columns . tolist ( )
885	for i in X . columns . tolist ( ) : cor = np . corrcoef ( X [ i ] , y ) [ 0 , 1 ] cor_list . append ( cor )
886	feature_selection_df = feature_selection_df . sort_values ( [ 'Total' , 'Feature' ] , ascending = False ) feature_selection_df . index = range ( 1 , len ( feature_selection_df ) + 1 ) feature_selection_df . head ( 100 )
887	PATH = '/kaggle/input/covid19-global-forecasting-week-4/' train_df = pd . read_csv ( PATH + 'train.csv' , parse_dates = [ 'Date' ] ) test_df = pd . read_csv ( PATH + 'test.csv' , parse_dates = [ 'Date' ] ) add_datepart ( train_df , 'Date' , drop = False ) add_datepart ( test_df , 'Date' , drop = False )
888	res = pd . merge ( res , continent_meta , how = 'left' ) res = pd . merge ( res , recoveries_meta , how = 'left' ) res = pd . merge ( res , meta_df , how = 'left' ) res = pd . merge ( res , countryinfo , how = 'left' ) res = pd . merge ( res , testinfo , how = 'left' , left_on = idx_group , right_on = idx_group )
889	res = pd . merge ( res , fatality , how = 'left' ) res = pd . merge ( res , first_nonzero , how = 'left' ) res = pd . merge ( res , first_fifty , how = 'left' ) return res train_df = joined_data ( train_df ) test_df = joined_data ( test_df )
890	df1 = df . copy ( ) df1 [ 'ConfirmedCases' ] = np . log ( df1 [ 'ConfirmedCases' ] ) df1 [ 'Fatalities' ] = np . log ( df1 [ 'Fatalities' ] + 1 )
891	to_tst = to . new ( test_df ) to_tst . process ( ) to_tst . all_cols . head ( )
892	import pandas as pd pd . set_option ( 'display.max_columns' , None ) import numpy as np import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) import gc import warnings import time warnings . filterwarnings ( "ignore" )
893	plotdata = data [ col ] . value_counts ( ) plt . figure ( figsize = size ) sns . barplot ( x = plotdata . index , y = plotdata . values ) plt . title ( title ) if xlabel_angle != 0 : plt . xticks ( rotation = xlabel_angle ) plt . show ( ) plot_categorical ( data = application_train , col = 'TARGET' , size = [ 8 , 4 ] , xlabel_angle = 0 , title = 'train set: label' )
894	plt . figure ( figsize = size ) plt . title ( "Distribution of %s" % col ) sns . distplot ( data [ col ] . dropna ( ) , kde = True , bins = bins ) plt . show ( ) plot_numerical ( application_train , 'AMT_CREDIT' )
895	avg_repaid = data . ix [ data [ 'TARGET' ] == 0 , col ] . median ( ) avg_not_repaid = data . ix [ data [ 'TARGET' ] == 1 , col ] . median ( ) plt . figure ( figsize = ( 12 , 6 ) )
896	from sklearn . impute import SimpleImputer , MICEImputer application_train = pd . read_csv ( '../input/application_train.csv' ) application_test = pd . read_csv ( '../input/application_test.csv' )
897	categorical_feats = input_df . columns [ input_df . dtypes == 'object' ] for feat in categorical_feats : encoder = LabelEncoder ( ) input_df [ feat ] = encoder . fit_transform ( input_df [ feat ] . fillna ( 'NULL' ) ) return input_df , categorical_feats . tolist ( ) , encoder_dict application_train , categorical_feats , encoder_dict = label_encoder ( application_train ) application_test , categorical_feats , encoder_dict = label_encoder ( application_test )
898	import os import time import gc import warnings warnings . filterwarnings ( "ignore" ) import json from pandas . io . json import json_normalize import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error import lightgbm as lgb
899	def find_missing ( data ) : count_missing = data . isnull ( ) . sum ( ) . values total = data . shape [ 0 ] ratio_missing = count_missing / total return pd . DataFrame ( data = { 'missing_count' : count_missing , 'missing_ratio' : ratio_missing } , index = data . columns . values ) train_missing = find_missing ( train ) test_missing = find_missing ( test )
900	if test . fullVisitorId . nunique ( ) == len ( sub ) : print ( 'Till now, the number of fullVisitorId is equal to the rows in submission. Everything goes well!' ) else : print ( 'Check it again' )
901	categorical_feats = input_df . columns [ input_df . dtypes == 'object' ] for feat in categorical_feats : encoder = LabelEncoder ( ) input_df [ feat ] = encoder . fit_transform ( input_df [ feat ] . fillna ( 'NULL' ) ) return input_df , categorical_feats . tolist ( ) , encoder_dict application_train , categorical_feats , encoder_dict = label_encoder ( application_train ) X = application_train . drop ( 'TARGET' , axis = 1 ) y = application_train . TARGET
902	data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' evaluation_path = data_path / 'evaluation' test_path = data_path / 'test' training_tasks = sorted ( os . listdir ( training_path ) ) eval_tasks = sorted ( os . listdir ( evaluation_path ) )
903	evaluation_examples = [ ] for i in range ( 400 ) : task = Evals [ i ] basic_task = Create ( task , 0 ) a = Function ( basic_task ) if a != - 1 and task [ 'test' ] [ 0 ] [ 'output' ] == a : plot_picture ( a ) plot_task ( task ) print ( i ) evaluation_examples . append ( i )
904	traintypes = { 'fare_amount' : 'float32' , 'pickup_datetime' : 'str' , 'pickup_longitude' : 'float32' , 'pickup_latitude' : 'float32' , 'dropoff_longitude' : 'float32' , 'dropoff_latitude' : 'float32' , 'passenger_count' : 'uint8' } cols = list ( traintypes . keys ( ) )
905	raise NotImplementedError ( ) def notify_action ( self , action : Action ) -> None : pass class Ship ( Strategy ) : @ property def pos ( self ) : return obs . my_ships [ self . id ] . pos @ property def halite ( self ) : return obs . my_ships [ self . id ] . halite class ShipYard ( Strategy ) : @ property def pos ( self ) : return obs . my_shipyards [ self . id ] . pos
906	score : float halite : float steps : int def update_new_state ( new_obs ) : global obs obs = new_obs update_mine_scores ( ) update_strategies ( ) def update_strategies ( ) :
907	player_info = obs_ [ "players" ] [ 0 ] if player_info [ 1 ] : player_info [ 1 ] = { key + "sy" : val for key , val in player_info [ 1 ] . items ( ) } action = agent ( obs_ ) obs_ , reward , done , info = trainer . step ( action ) env . render ( mode = "ipython" , width = 800 , height = 600 )
908	import numpy as np mask = torch . stack ( [ mask , mask , mask ] , dim = 2 ) mask = mask . cpu ( ) . numpy ( ) . astype ( "uint8" ) instances = cv2 . multiply ( image , mask ) plt . imshow ( instances ) plt . show ( )
909	import os import cv2 import pdb import glob import argparse import numpy as np
910	x = ( 255 - x ) . astype ( np . float32 ) / 255. if self . train : y = self . labels [ i ] return x , y else : return x
911	self . to ( input . device ) return F . linear ( input , self . weight , self . bias ) def extra_repr ( self ) : return 'in_features={}, out_features={}, bias={}' . format ( self . in_features , self . out_features , self . bias is not None )
912	train_args_dict = load_json ( os . path . join ( traindir , f'args_{j}.json' ) ) train_args_dict . update ( { 'load_model_path' : os . path . join ( traindir , f'predictor_{j}.pt' ) , 'device' : device , 'batch_size' : batch_size , 'debug' : debug , } ) print ( f'j {j} updated train_args_dict {train_args_dict}' ) test_preds = predict_core ( test_images = test_images , n_total = n_total , ** train_args_dict ) model_preds_list . append ( test_preds )
913	import nltk import re from nltk . corpus import stopwords from nltk . stem . porter import PorterStemmer
914	voc_size = 5000 onehot_repr = [ one_hot ( words , voc_size ) for words in corpus ] onehot_repr
915	from sklearn . model_selection import TimeSeriesSplit cv = TimeSeriesSplit ( n_splits = 5 )
916	oof_qda1 = oof_qda1 . reshape ( - 1 , 1 ) oof_qda2 = oof_qda2 . reshape ( - 1 , 1 ) oof_gmm = oof_gmm . reshape ( - 1 , 1 ) oof_lr = oof_lr . reshape ( - 1 , 1 ) oof_ls = oof_ls . reshape ( - 1 , 1 ) oof_knn = oof_knn . reshape ( - 1 , 1 ) oof_nn = oof_nn . reshape ( - 1 , 1 ) oof_qda3 = oof_qda3 . reshape ( - 1 , 1 )
917	train [ [ 'ID' , 'Image' , 'Diagnosis' ] ] = train [ 'ID' ] . str . split ( '_' , expand = True ) train = train [ [ 'Image' , 'Diagnosis' , 'Label' ] ] train . drop_duplicates ( inplace = True ) train = train . pivot ( index = 'Image' , columns = 'Diagnosis' , values = 'Label' ) . reset_index ( ) train [ 'Image' ] = 'ID_' + train [ 'Image' ] train . head ( )
918	test [ [ 'ID' , 'Image' , 'Diagnosis' ] ] = test [ 'ID' ] . str . split ( '_' , expand = True ) test [ 'Image' ] = 'ID_' + test [ 'Image' ] test = test [ [ 'Image' , 'Label' ] ] test . drop_duplicates ( inplace = True ) test . to_csv ( 'test.csv' , index = False )
919	batch = next ( iter ( data_loader_train ) ) fig , axs = plt . subplots ( 1 , 5 , figsize = ( 15 , 5 ) ) for i in np . arange ( 5 ) : axs [ i ] . imshow ( np . transpose ( batch [ 'image' ] [ i ] . numpy ( ) , ( 1 , 2 , 0 ) ) [ : , : , 0 ] , cmap = plt . cm . bone )
920	batch = next ( iter ( data_loader_test ) ) fig , axs = plt . subplots ( 1 , 5 , figsize = ( 15 , 5 ) ) for i in np . arange ( 5 ) : axs [ i ] . imshow ( np . transpose ( batch [ 'image' ] [ i ] . numpy ( ) , ( 1 , 2 , 0 ) ) [ : , : , 0 ] , cmap = plt . cm . bone )
921	train [ [ 'ID' , 'Image' , 'Diagnosis' ] ] = train [ 'ID' ] . str . split ( '_' , expand = True ) train = train [ [ 'Image' , 'Diagnosis' , 'Label' ] ] train . drop_duplicates ( inplace = True ) train = train . pivot ( index = 'Image' , columns = 'Diagnosis' , values = 'Label' ) . reset_index ( ) train [ 'Image' ] = 'ID_' + train [ 'Image' ] train . head ( )
922	test [ [ 'ID' , 'Image' , 'Diagnosis' ] ] = test [ 'ID' ] . str . split ( '_' , expand = True ) test [ 'Image' ] = 'ID_' + test [ 'Image' ] test = test [ [ 'Image' , 'Label' ] ] test . drop_duplicates ( inplace = True ) test . to_csv ( 'test.csv' , index = False )
923	batch = next ( iter ( data_loader_train ) ) fig , axs = plt . subplots ( 1 , 5 , figsize = ( 15 , 5 ) ) for i in np . arange ( 5 ) : axs [ i ] . imshow ( np . transpose ( batch [ 'image' ] [ i ] . numpy ( ) , ( 1 , 2 , 0 ) ) [ : , : , 0 ] , cmap = plt . cm . bone )
924	batch = next ( iter ( data_loader_test ) ) fig , axs = plt . subplots ( 1 , 5 , figsize = ( 15 , 5 ) ) for i in np . arange ( 5 ) : axs [ i ] . imshow ( np . transpose ( batch [ 'image' ] [ i ] . numpy ( ) , ( 1 , 2 , 0 ) ) [ : , : , 0 ] , cmap = plt . cm . bone )
925	df = pd . concat ( [ train [ [ 'id' , 'comment_text' ] ] , test ] , axis = 0 ) del ( train , test ) gc . collect ( )
926	MAX_NUM_WORDS = 100000 TOXICITY_COLUMN = 'target' TEXT_COLUMN = 'comment_text' tokenizer = Tokenizer ( num_words = MAX_NUM_WORDS ) tokenizer . fit_on_texts ( train_df [ TEXT_COLUMN ] ) MAX_SEQUENCE_LENGTH = 256 def pad_text ( texts , tokenizer ) : return pad_sequences ( tokenizer . texts_to_sequences ( texts ) , maxlen = MAX_SEQUENCE_LENGTH )
927	MAX_SEQUENCE_LENGTH = 256 def pad_text ( texts , tokenizer ) : return pad_sequences ( tokenizer . texts_to_sequences ( texts ) , maxlen = MAX_SEQUENCE_LENGTH )
928	for col in [ 'headlineTag' , 'provider' , 'sourceId' ] : news_train [ col ] , uniques = pd . factorize ( news_train [ col ] ) del uniques
929	news_unstack = index_df . merge ( news_train , how = 'left' , on = 'news_index' ) news_unstack . drop ( [ 'news_index' , 'assetCodes' ] , axis = 1 , inplace = True ) return news_unstack
930	df [ std_column ] = df . groupby ( 'assetCode' ) [ col ] . apply ( lambda x : x . rolling ( window ) . std ( ) ) df [ ma_lag_column ] = df . groupby ( 'assetCode' ) [ ma_column ] . shift ( ) df [ ma_lag_rate_column ] = ( df [ ma_column ] - df [ ma_lag_column ] ) / df [ ma_lag_column ] drop_list . append ( ma_column ) drop_list . append ( ma_lag_column ) df . drop ( drop_list , axis = 1 , inplace = True ) return df
931	df . drop ( [ 'returnsOpenNextMktres10' , 'date' , 'universe' , 'assetCode' , 'assetName' , 'time' ] , axis = 1 , inplace = True ) df = df . astype ( 'float32' ) gc . collect ( )
932	obs_df = market_obs_df . merge ( news_obs_agg , how = 'left' , on = [ 'assetCode' , 'date' ] ) del market_obs_df , news_obs_agg , news_obs_df , news_unstack , index_df gc . collect ( ) obs_df = obs_df [ obs_df . assetCode . isin ( predictions_template_df . assetCode ) ]
933	word_counts_df [ 'scaled_total' ] = word_counts_df [ 'count_x' ] + word_counts_df [ 'count_y' ] * 18 word_counts_df = word_counts_df [ word_counts_df [ 'count_x' ] > 100 ] word_counts_df = word_counts_df [ ( word_counts_df [ 'count_x' ] / word_counts_df [ 'scaled_total' ] > 0.2 ) & ( word_counts_df [ 'count_x' ] / word_counts_df [ 'scaled_total' ] < 0.8 ) ]
934	cv = CV ( vocabulary = word_counts_df [ 'word' ] . tolist ( ) ) print ( datetime . datetime . now ( ) ) train_X_flattened = cv . fit_transform ( list ( train_X [ 'comment_text_arranged' ] . values ) ) test_X_flattened = cv . fit_transform ( list ( test_X [ 'comment_text_arranged' ] . values ) ) print ( datetime . datetime . now ( ) )
935	for col in [ 'headlineTag' , 'provider' , 'sourceId' ] : news_train [ col ] , uniques = pd . factorize ( news_train [ col ] ) del uniques
936	news_unstack = index_df . merge ( news_train , how = 'left' , on = 'news_index' ) news_unstack . drop ( [ 'news_index' , 'assetCodes' ] , axis = 1 , inplace = True ) return news_unstack
937	df . drop ( [ 'returnsOpenNextMktres10' , 'date' , 'universe' , 'assetCode' , 'assetName' , 'time' ] , axis = 1 , inplace = True ) df = df . astype ( 'float32' ) gc . collect ( )
938	obs_df = market_obs_df . merge ( news_obs_agg , how = 'left' , on = [ 'assetCode' , 'date' ] ) del market_obs_df , news_obs_agg , news_obs_df , news_unstack , index_df gc . collect ( ) obs_df = obs_df [ obs_df . assetCode . isin ( predictions_template_df . assetCode ) ]
939	def merge ( df_result , df_result_aggs , df_result_filter_aggs ) : df_result = df_result . join ( df_result_aggs , how = "left" , on = "SeedDiff" ) . join ( df_result_filter_aggs , how = "left" , on = "Seed_combi" ) df_result [ "upset_prob" ] = [ m if c > 20 else a for a , m , c in zip ( df_result [ "upset_mean_all" ] , df_result [ "upset_mean" ] , df_result [ "upset_count" ] , ) ] valid = df_result [ ( df_result [ "Season" ] == ( this_season - 1 ) ) ] return valid
940	data = pd . concat ( ( train , test ) ) np . random . seed ( 42 ) data = data . iloc [ np . random . permutation ( len ( data ) ) ] data . reset_index ( drop = True , inplace = True ) x = data . drop ( [ 'target' , 'ID_code' , 'train_test' ] , axis = 1 ) y = data . train_test
941	INPUT_DIR = '../input/m5-forecasting-accuracy' cal = pd . read_csv ( f'{INPUT_DIR}/calendar.csv' ) stv = pd . read_csv ( f'{INPUT_DIR}/sales_train_validation.csv' ) ss = pd . read_csv ( f'{INPUT_DIR}/sample_submission.csv' ) sellp = pd . read_csv ( f'{INPUT_DIR}/sell_prices.csv' )
942	valid = datetime ( year , 1 , 1 ) . weekday ( ) data [ : valid , 0 ] = np . nan valid = datetime ( year , 12 , 31 ) . weekday ( )
943	transfer_cal = pd . DataFrame ( calendar [ [ 'event_name_1' , 'event_type_1' , 'event_name_2' , 'event_type_2' , 'snap_CA' , 'snap_TX' , 'snap_WI' ] ] . values . T , index = [ 'event_name_1' , 'event_type_1' , 'event_name_2' , 'event_type_2' , 'snap_CA' , 'snap_TX' , 'snap_WI' ] ) transfer_cal
944	npt . word_distribution ( title = 'number of words distribution' )
945	sns . set ( ) x = train [ 'revenue' ] y = train [ 'popularity' ] plt . figure ( figsize = ( 15 , 8 ) ) sns . regplot ( x , y ) plt . xlabel ( 'popularity' ) plt . ylabel ( 'revenue' ) plt . title ( 'Relationship between popularity and revenue of a movie' )
946	def get_json_dict ( df ) : global json_cols result = dict ( ) for e_col in json_cols : d = dict ( ) rows = df [ e_col ] . values for row in rows : if row is None : continue for i in row : if i [ 'name' ] not in d : d [ i [ 'name' ] ] = 0 d [ i [ 'name' ] ] += 1 result [ e_col ] = d return result train_dict = get_json_dict ( train ) test_dict = get_json_dict ( test )
947	lgbmodel = lgb . LGBMRegressor ( n_estimators = 10000 , objective = 'regression' , metric = 'rmse' , max_depth = 5 , num_leaves = 30 , min_child_samples = 100 , learning_rate = 0.01 , boosting = 'gbdt' , min_data_in_leaf = 10 , feature_fraction = 0.9 , bagging_freq = 1 , bagging_fraction = 0.9 , importance_type = 'gain' , lambda_l1 = 0.2 , bagging_seed = random_seed , subsample = .8 , colsample_bytree = .9 , use_best_model = True )
948	from sklearn . metrics import mean_squared_error import lightgbm as lgb model = lgb . LGBMRegressor ( ) model . fit ( xtrain , ytrain ) print ( "RMSE of Validation Data using Light GBM: %.2f" % math . sqrt ( mean_squared_error ( yval , model . predict ( xval ) ) ) )
949	summ = pd . DataFrame ( { 'data' : [ 'train.csv' , 'test.csv' , 'sample_submission.csv' ] , 'rows' : [ len ( trainset ) , len ( testset ) , len ( sample_sub ) ] , 'patient' : [ trainset [ 'Patient' ] . nunique ( ) , testset [ 'Patient' ] . nunique ( ) , sample_sub [ 'Patient_Week' ] . nunique ( ) ] } ) summ . set_index ( 'data' , inplace = True ) display ( summ )
950	fig = px . histogram ( trainset , x = 'Age' , color = 'Sex' , marginal = 'box' , histnorm = 'probability density' , opacity = 0.7 ) fig . update_layout ( title = 'Distribution of Age between Male and Female' , width = 800 , height = 500 ) fig . show ( )
951	parti_patient = trainset . drop_duplicates ( subset = 'Patient' ) fig = px . histogram ( parti_patient , x = 'Age' , facet_row = 'SmokingStatus' , facet_col = 'Sex' , ) fig . for_each_annotation ( lambda a : a . update ( text = a . text . replace ( "SmokingStatus=" , "" ) ) ) fig . update_layout ( title = 'Distribution of Age sperated by Sex (col) and Smoking Status (row)' , autosize = True , width = 800 , height = 600 , font_size = 14 ) fig . show ( )
952	pie_labels = [ 'Male & Ex-smoker' , 'Male & Currently smokes' , 'Male & Never smoked' , 'Female & Ex-smoker' , 'Female & Currently smokes' , 'Female & Never smoked' ] ss_values = [ m_exsmk_age , m_cursmk_age , m_nevsmk_age , f_exsmk_age , f_cursmk_age , f_nevsmk_age ] pie_values = [ * map ( lambda x : len ( x ) , ss_values ) ]
953	fig = px . density_contour ( trainset , x = 'Percent' , y = 'FVC' , marginal_x = "histogram" , marginal_y = "histogram" , color = 'SmokingStatus' , ) fig . update_layout ( title = 'Relationship between Percent and FVC' , width = 800 , height = 400 ) fig . show ( )
954	spacing = np . array ( [ scan [ 0 ] . SliceThickness , scan [ 0 ] . PixelSpacing [ 0 ] , scan [ 0 ] . PixelSpacing [ 1 ] ] , dtype = np . float32 ) resize_factor = spacing / new_spacing new_real_shape = image . shape * resize_factor new_shape = np . round ( new_real_shape ) real_resize_factor = new_shape / image . shape new_spacing = spacing / real_resize_factor image = scipy . ndimage . interpolation . zoom ( image , real_resize_factor , mode = 'nearest' ) return image , new_spacing
955	for i , axial_slice in enumerate ( binary_image ) : axial_slice = axial_slice - 1 labeling = measure . label ( axial_slice ) l_max = largest_label_volume ( labeling , bg = 0 ) if l_max is not None : binary_image [ i ] [ labeling != l_max ] = 1 return binary_image
956	train_df = pd . read_csv ( path / 'train.csv' ) train_df = pd . concat ( [ train_df [ 'id' ] , train_df [ 'category_id' ] ] , axis = 1 , keys = [ 'id' , 'category_id' ] ) train_df . head ( )
957	test_df = pd . read_csv ( path / 'test.csv' ) test_df = pd . DataFrame ( test_df [ 'id' ] ) test_df [ 'predicted' ] = 0 test_df . head ( )
958	import os import numpy as np import pandas as pd import lightgbm as lgb from sklearn . model_selection import train_test_split from sklearn . linear_model import LogisticRegression from sklearn . metrics import classification_report
959	keep_cols = [ 'event_id' , 'game_session' , 'installation_id' , 'event_count' , 'event_code' , 'title' , 'game_time' , 'type' , 'world' ] train = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/train.csv' , usecols = keep_cols ) test = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/test.csv' , usecols = keep_cols ) train_labels = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/train_labels.csv' ) submission = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/sample_submission.csv' )
960	group3 = pd . get_dummies ( group1 . drop ( columns = [ 'game_session' , 'event_count' , 'game_time' ] ) , columns = [ 'title' , 'type' , 'world' ] ) . groupby ( [ 'installation_id' ] ) . sum ( ) group4 = group1 [ [ 'installation_id' , 'event_count' , 'game_time' ] ] . groupby ( [ 'installation_id' ] ) . agg ( [ np . sum , np . mean , np . std ] ) return group2 . join ( group3 ) . join ( group4 )
961	plt . rcParams [ "font.size" ] = "12" ax = df . ffill ( ) \ . count ( axis = 1 ) \ . plot ( figsize = ( 20 , 8 ) , title = 'Number of Teams in the Competition by Date' , color = color_pal [ 5 ] , lw = 5 ) ax . set_ylabel ( 'Number of Teams' ) plt . show ( )
962	plt . rcParams [ "font.size" ] = "12" TOP_TEAMS = df . max ( ) . loc [ df . max ( ) > FIFTYTH_SCORE ] . index . values df [ TOP_TEAMS ] . max ( ) . sort_values ( ascending = True ) . plot ( kind = 'barh' , xlim = ( TOP_SCORE - 0.1 , FIFTYTH_SCORE + 0.1 ) , title = 'Top 50 Public LB Teams' , figsize = ( 12 , 15 ) , color = color_pal [ 3 ] ) plt . show ( )
963	TOP_TEAMS = df . max ( ) . loc [ df . max ( ) > FIFTYTH_SCORE ] . index . values df [ TOP_TEAMS ] . max ( ) . sort_values ( ascending = True ) . plot ( kind = 'barh' , xlim = ( TOP_SCORE - 0.1 , FIFTYTH_SCORE + 0.1 ) , title = 'Top 50 Public LB Teams' , figsize = ( 12 , 15 ) , color = color_pal [ 3 ] ) plt . show ( )
964	plt . rcParams [ "font.size" ] = "12" df [ TOP_TEAMS ] . nunique ( ) . sort_values ( ) . plot ( kind = 'barh' , figsize = ( 12 , 15 ) , color = color_pal [ 1 ] , title = 'Count of Submissions improving LB score by Team' ) plt . show ( )
965	X = df_e . values [ : , 2 : ] . astype ( np . float32 ) Y = df_e . values [ : , 1 ] . astype ( np . float32 ) print ( X . shape )
966	for clzid in range ( len ( clz_attr_num ) ) : if clz_attr_num [ clzid ] > 0 : if not os . path . isfile ( MODEL_FILE_DIR + "attrmodel_%d-%d.model" % ( attr_image_size [ 0 ] , clzid ) ) : model = train_attr_net ( clzid , 32 ) torch . save ( model . state_dict ( ) , MODEL_FILE_DIR + "attrmodel_%d-%d.model" % ( attr_image_size [ 0 ] , clzid ) )
967	class MaskDataset ( object ) : def __init__ ( self , keys ) : self . keys = keys def __getitem__ ( self , idx ) : k = self . keys [ idx ] return ztop ( data_mask [ k ] [ 0 ] ) , ztop ( data_mask [ k ] [ 1 ] ) def __len__ ( self ) : return len ( self . keys )
968	predict_imgeid = [ predict_imgeid [ i ] for i in set ( uses_index ) ] predict_mask = [ predict_mask [ i ] for i in set ( uses_index ) ] predict_rle = [ predict_rle [ i ] for i in set ( uses_index ) ] predict_classid = [ predict_classid [ i ] for i in set ( uses_index ) ] predict_attr = [ predict_attr [ i ] for i in set ( uses_index ) ] predict_attri_str = [ predict_attri_str [ i ] for i in set ( uses_index ) ]
969	def seed_everything ( seed ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True
970	idx = 1 im , cl = learn . data . dl ( DatasetType . Valid ) . dataset [ idx ] cl = int ( cl ) im . show ( title = f"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}" )
971	def show_heatmap ( hm ) : _ , ax = plt . subplots ( ) sz = list ( xb_im . shape [ - 2 : ] ) xb_im . show ( ax , title = f"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}" ) ax . imshow ( hm , alpha = 0.6 , extent = ( 0 , * sz [ : : - 1 ] , 0 ) , interpolation = 'bilinear' , cmap = 'magma' ) return _ , ax
972	import os import random import pandas as pd import numpy as np import glob import matplotlib . pyplot as plt import cv2 import IPython . display as ipd import librosa from albumentations . core . transforms_interface import DualTransform , BasicTransform
973	def __init__ ( self , always_apply = False , p = 0.5 ) : super ( TimeShifting , self ) . __init__ ( always_apply , p ) def apply ( self , data , ** params ) :
974	def __init__ ( self , always_apply = False , p = 0.5 , rate = None ) : super ( StretchAudio , self ) . __init__ ( always_apply , p ) if rate : self . rate = rate else : self . rate = np . random . uniform ( 0.5 , 1.5 ) def apply ( self , data , ** params ) :
975	class PitchShift ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 , n_steps = None ) : super ( PitchShift , self ) . __init__ ( always_apply , p ) self . n_steps = n_steps def apply ( self , data , ** params ) : return librosa . effects . pitch_shift ( data , sr = 22050 , n_steps = self . n_steps )
976	class AddGaussianNoise ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 ) : super ( AddGaussianNoise , self ) . __init__ ( always_apply , p ) def apply ( self , data , ** params ) : noise = np . random . randn ( len ( data ) ) data_wn = data + 0.005 * noise return data_wn
977	def __init__ ( self , always_apply = False , p = 0.5 ) : super ( AddGaussianNoise , self ) . __init__ ( always_apply , p ) def apply ( self , data , ** params ) :
978	import albumentations def get_train_transforms ( ) : return albumentations . Compose ( [ TimeShifting ( p = 0.9 ) , albumentations . OneOf ( [ AddCustomNoise ( file_dir = '../input/freesound-audio-tagging/audio_train' , p = 0.8 ) , SpeedTuning ( p = 0.8 ) , ] ) , AddGaussianNoise ( p = 0.8 ) , PitchShift ( p = 0.5 , n_steps = 4 ) , Gain ( p = 0.9 ) , PolarityInversion ( p = 0.9 ) , StretchAudio ( p = 0.1 ) , ] )
979	def roc_auc ( predictions , target ) : fpr , tpr , thresholds = metrics . roc_curve ( target , predictions ) roc_auc = metrics . auc ( fpr , tpr ) return roc_auc
980	fpr , tpr , thresholds = metrics . roc_curve ( target , predictions ) roc_auc = metrics . auc ( fpr , tpr ) return roc_auc
981	token = text . Tokenizer ( num_words = None ) max_len = 1500 token . fit_on_texts ( list ( xtrain ) + list ( xvalid ) ) xtrain_seq = token . texts_to_sequences ( xtrain ) xvalid_seq = token . texts_to_sequences ( xvalid )
982	xtrain_pad = sequence . pad_sequences ( xtrain_seq , maxlen = max_len ) xvalid_pad = sequence . pad_sequences ( xvalid_seq , maxlen = max_len ) word_index = token . word_index
983	model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , input_length = max_len ) ) model . add ( SimpleRNN ( 100 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
984	embeddings_index = { } f = open ( '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt' , 'r' , encoding = 'utf-8' ) for line in tqdm ( f ) : values = line . split ( ' ' ) word = values [ 0 ] coefs = np . asarray ( [ float ( val ) for val in values [ 1 : ] ] ) embeddings_index [ word ] = coefs f . close ( ) print ( 'Found %s word vectors.' % len ( embeddings_index ) )
985	embedding_matrix = np . zeros ( ( len ( word_index ) + 1 , 300 ) ) for word , i in tqdm ( word_index . items ( ) ) : embedding_vector = embeddings_index . get ( word ) if embedding_vector is not None : embedding_matrix [ i ] = embedding_vector
986	model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( LSTM ( 100 , dropout = 0.3 , recurrent_dropout = 0.3 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
987	model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( SpatialDropout1D ( 0.3 ) ) model . add ( GRU ( 300 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
988	with strategy . scope ( ) : model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( Bidirectional ( LSTM ( 300 , dropout = 0.3 , recurrent_dropout = 0.3 ) ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
989	model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( Bidirectional ( LSTM ( 300 , dropout = 0.3 , recurrent_dropout = 0.3 ) ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
990	import os import tensorflow as tf from tensorflow . keras . layers import Dense , Input from tensorflow . keras . optimizers import Adam from tensorflow . keras . models import Model from tensorflow . keras . callbacks import ModelCheckpoint from kaggle_datasets import KaggleDatasets import transformers from tokenizers import BertWordPieceTokenizer
991	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
992	tokenizer . enable_truncation ( max_length = maxlen ) tokenizer . enable_padding ( max_length = maxlen ) all_ids = [ ] for i in tqdm ( range ( 0 , len ( texts ) , chunk_size ) ) : text_chunk = texts [ i : i + chunk_size ] . tolist ( ) encs = tokenizer . encode_batch ( text_chunk ) all_ids . extend ( [ enc . ids for enc in encs ] ) return np . array ( all_ids )
993	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) tokenizer . save_pretrained ( '.' ) fast_tokenizer = BertWordPieceTokenizer ( 'vocab.txt' , lowercase = False ) fast_tokenizer
994	input_word_ids = Input ( shape = ( max_len , ) , dtype = tf . int32 , name = "input_word_ids" ) sequence_output = transformer ( input_word_ids ) [ 0 ] cls_token = sequence_output [ : , 0 , : ] out = Dense ( 1 , activation = 'sigmoid' ) ( cls_token ) model = Model ( inputs = input_word_ids , outputs = out ) model . compile ( Adam ( lr = 1e-5 ) , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model
995	height = p . get_height ( ) ax . text ( p . get_x ( ) + p . get_width ( ) / 2 , height + 3 , '{:1.2f}%' . format ( 100 * height / 10616 ) , ha = "center" )
996	l_in = torch . randn ( 10 , device = xm . xla_device ( ) ) linear = torch . nn . Linear ( 10 , 20 ) . to ( xm . xla_device ( ) ) l_out = linear ( l_in ) print ( l_out )
997	class config : MAX_LEN = 224 TRAIN_BATCH_SIZE = 32 VALID_BATCH_SIZE = 8 EPOCHS = 1 MODEL_PATH = "model.bin" TRAINING_FILE = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv' TOKENIZER = transformers . BertTokenizer . from_pretrained ( 'bert-base-uncased' , do_lower_case = True )
998	device = xm . xla_device ( ) model = BERTBaseUncased ( ) model . to ( device )
999	for epoch in range ( config . EPOCHS ) : para_loader = pl . ParallelLoader ( train_data_loader , [ device ] ) train_fn ( para_loader . per_device_loader ( device ) , model , optimizer , device , scheduler = None , epoch = epoch , num_steps = num_train_steps ) para_loader = pl . ParallelLoader ( valid_data_loader , [ device ] ) outputs , targets = eval_fn ( para_loader . per_device_loader ( device ) , model , device )
1000	BATCH_SIZE = 1024 EPOCHS = 150 LR = 0.02 seed = 2020 patience = 50 device = torch . device ( 'cuda' ) FOLDS = 5
1001	model = CustomTabnet ( input_dim = len ( features ) , output_dim = 2 , n_d = 32 , n_a = 32 , n_steps = 4 , gamma = 1.6 , n_independent = 2 , n_shared = 2 , momentum = 0.02 , mask_type = "sparsemax" ) model . to ( device )
1002	param_optimizer = list ( model . named_parameters ( ) ) no_decay = [ "bias" , "LayerNorm.bias" , "LayerNorm.weight" ] optimizer_parameters = [ { 'params' : [ p for n , p in param_optimizer if not any ( nd in n for nd in no_decay ) ] , 'weight_decay' : 0.001 } , { 'params' : [ p for n , p in param_optimizer if any ( nd in n for nd in no_decay ) ] , 'weight_decay' : 0.0 } , ] optimizer = torch . optim . Adam ( optimizer_parameters , lr = LR )
1003	scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.1 , patience = 10 , verbose = True , threshold = 0.0001 , threshold_mode = 'rel' , cooldown = 0 , min_lr = 0 , eps = 1e-08 )
1004	history = { 'train_history_loss' : [ ] , 'train_history_auc' : [ ] , 'val_history_loss' : [ ] , 'val_history_auc' : [ ] , }
1005	fig , ax = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 30 , 15 ) ) k = 0 for i , row in enumerate ( ax ) : for j , col in enumerate ( row ) : img = sk . imread ( cover_images_path [ k ] ) col . imshow ( img ) col . set_title ( cover_images_path [ k ] ) k = k + 1 plt . suptitle ( 'Samples from Cover Images' , fontsize = 14 ) plt . show ( )
1006	for ingredient , expected in [ ( 'Eggs' , 'egg' ) , ( 'all-purpose flour' , 'all purpose flour' ) , ( 'purée' , 'puree' ) , ( '1% low-fat milk' , 'low fat milk' ) , ( 'half & half' , 'half half' ) , ( 'safetida (powder)' , 'safetida (powder)' ) ] : actual = preprocess ( [ ingredient ] ) assert actual == expected , f'"{expected}" is excpected but got "{actual}"'
1007	for ingredient , expected in [ ( 'Eggs' , 'egg' ) , ( 'all-purpose flour' , 'all purpose flour' ) , ( 'purée' , 'puree' ) , ( '1% low-fat milk' , 'low fat milk' ) , ( 'half & half' , 'half half' ) , ( 'safetida (powder)' , 'safetida (powder)' ) ] : actual = preprocess ( [ ingredient ] ) assert actual == expected , f'"{expected}" is excpected but got "{actual}"'
1008	merge = gp . merge ( gk , on = [ feature ] , how = 'left' ) sns . lmplot ( x = "mean_download_delay_time" , y = "download_rate" , data = merge ) plt . title ( 'Download-rate vs. Download_delay_time' ) plt . ylim ( 0 , 1 ) plt . xlim ( 0 , 24 )
1009	train = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/train.csv' ) test = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/test.csv' ) ss = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' )
1010	def clean_text ( text ) : text = str ( text ) . lower ( ) text = re . sub ( '\[.*?\]' , '' , text ) text = re . sub ( 'https?://\S+|www\.\S+' , '' , text ) text = re . sub ( '<.*?>+' , '' , text ) text = re . sub ( '[%s]' % re . escape ( string . punctuation ) , '' , text ) text = re . sub ( '\n' , '' , text ) text = re . sub ( '\w*\d\w*' , '' , text ) return text
1011	Positive_sent = train [ train [ 'sentiment' ] == 'positive' ] Negative_sent = train [ train [ 'sentiment' ] == 'negative' ] Neutral_sent = train [ train [ 'sentiment' ] == 'neutral' ]
1012	top = Counter ( [ item for sublist in Positive_sent [ 'temp_list' ] for item in sublist ] ) temp_positive = pd . DataFrame ( top . most_common ( 20 ) ) temp_positive . columns = [ 'Common_words' , 'count' ] temp_positive . style . background_gradient ( cmap = 'Greens' )
1013	top = Counter ( [ item for sublist in Negative_sent [ 'temp_list' ] for item in sublist ] ) temp_negative = pd . DataFrame ( top . most_common ( 20 ) ) temp_negative = temp_negative . iloc [ 1 : , : ] temp_negative . columns = [ 'Common_words' , 'count' ] temp_negative . style . background_gradient ( cmap = 'Reds' )
1014	top = Counter ( [ item for sublist in Neutral_sent [ 'temp_list' ] for item in sublist ] ) temp_neutral = pd . DataFrame ( top . most_common ( 20 ) ) temp_neutral = temp_neutral . loc [ 1 : , : ] temp_neutral . columns = [ 'Common_words' , 'count' ] temp_neutral . style . background_gradient ( cmap = 'Reds' )
1015	df_train = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/train.csv' ) df_test = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/test.csv' ) df_submission = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' )
1016	output_dir = f'../working/{output_dir}' if output_dir is not None : if not os . path . exists ( output_dir ) : os . makedirs ( output_dir ) nlp . meta [ "name" ] = new_model_name nlp . to_disk ( output_dir ) print ( "Saved model to" , output_dir )
1017	"" if model is not None : nlp = spacy . load ( output_dir ) print ( "Loaded model '%s'" % model ) else : nlp = spacy . blank ( "en" ) print ( "Created blank 'en' model" )
1018	model_out_path = None if sentiment == 'positive' : model_out_path = 'models/model_pos' elif sentiment == 'negative' : model_out_path = 'models/model_neg' return model_out_path
1019	train_data = [ ] for index , row in df_train . iterrows ( ) : if row . sentiment == sentiment : selected_text = row . selected_text text = row . text start = text . find ( selected_text ) end = start + len ( selected_text ) train_data . append ( ( text , { "entities" : [ [ start , end , 'selected_text' ] ] } ) ) return train_data
1020	sentiment = 'positive' train_data = get_training_data ( sentiment ) model_path = get_model_out_path ( sentiment ) train ( train_data , model_path , n_iter = 3 , model = None )
1021	train = pd . read_csv ( '../input/train.csv' , index_col = 'plaintext_id' ) test = pd . read_csv ( '../input/test.csv' , index_col = 'ciphertext_id' ) sub = pd . read_csv ( '../input/sample_submission.csv' , index_col = 'ciphertext_id' )
1022	plain_dict = { } for p_id , row in train . iterrows ( ) : text = row [ 'text' ] plain_dict [ text ] = p_id print ( len ( plain_dict ) )
1023	return '' . join ( sorted ( plaintext , key = lambda i : next ( p ) ) ) def decrypt_level_2 ( ciphertext , rails = 21 ) : p = rail_pattern ( rails ) indexes = sorted ( range ( len ( ciphertext ) ) , key = lambda i : next ( p ) ) result = [ '' ] * len ( ciphertext ) for i , c in zip ( indexes , ciphertext ) : result [ i ] = c return '' . join ( result )
1024	c_id = 'ID_0414884b0' index = 42677 sub . loc [ c_id ] = index
1025	from collections import Counter import matplotlib . pyplot as plt plt . rcParams [ "figure.figsize" ] = ( 20 , 10 )
1026	fullcipher3 = " " . join ( ( test3 [ "ciphertext" ] . values ) ) dict_fullcipher3 = Counter ( fullcipher3 . split ( " " ) ) df_fullcipher3 = pd . DataFrame . from_dict ( dict_fullcipher3 , orient = 'index' ) df_fullcipher3 = df_fullcipher3 . reset_index ( ) df_fullcipher3 . columns = [ "num" , "nb" ] df_fullcipher3 . sort_values ( "nb" , ascending = False , inplace = True ) print ( df_fullcipher3 . shape ) df_fullcipher3 . head ( )
1027	print ( "Handling missing values..." ) def handle_missing ( dataset ) : dataset . category_name . fillna ( value = "missing" , inplace = True ) dataset . brand_name . fillna ( value = "missing" , inplace = True ) dataset . item_description . fillna ( value = "missing" , inplace = True ) return ( dataset ) train = handle_missing ( train ) test = handle_missing ( test ) print ( train . shape ) print ( test . shape )
1028	train [ "target" ] = np . log ( train . price + 1 ) target_scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) ) train [ "target" ] = target_scaler . fit_transform ( train . target . reshape ( - 1 , 1 ) ) pd . DataFrame ( train . target ) . hist ( )
1029	dtrain , dvalid = train_test_split ( train , random_state = 123 , train_size = 0.99 ) print ( dtrain . shape ) print ( dvalid . shape )
1030	BATCH_SIZE = 20000 epochs = 5 model = get_model ( ) model . fit ( X_train , dtrain . target , epochs = epochs , batch_size = BATCH_SIZE , validation_data = ( X_valid , dvalid . target ) , verbose = 1 )
1031	import nltk stopwords = nltk . corpus . stopwords . words ( 'english' ) stemmer = nltk . stem . PorterStemmer ( ) def clean_sentence ( doc ) : words = doc . split ( ' ' ) words_clean = [ stemmer . stem ( word ) for word in words if word not in stopwords ] return ' ' . join ( words_clean ) docs = docs . apply ( clean_sentence ) docs . head ( )
1032	from xgboost import XGBRegressor report_cv ( XGBRegressor ( random_state = random_seed ) )
1033	import os import gc import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( 'ignore' )
1034	sns . distplot ( dipole_moments . X , color = 'mediumseagreen' ) plt . title ( 'Dipole moment along X-axis' ) plt . show ( ) sns . distplot ( dipole_moments . Y , color = 'seagreen' ) plt . title ( 'Dipole moment along Y-axis' ) plt . show ( ) sns . distplot ( dipole_moments . Z , color = 'green' ) plt . title ( 'Dipole moment along Z-axis' ) plt . show ( )
1035	plt . figure ( figsize = ( 26 , 24 ) ) for i , col in enumerate ( typelist ) : plt . subplot ( 4 , 2 , i + 1 ) sns . distplot ( potential_energy [ train [ 'type' ] == col ] [ 'potential_energy' ] , color = 'orangered' ) plt . title ( col )
1036	def is_outlier ( points , thresh = 3.5 ) : if len ( points . shape ) == 1 : points = points [ : , None ] median = np . median ( points , axis = 0 ) diff = np . sum ( ( points - median ) ** 2 , axis = - 1 ) diff = np . sqrt ( diff ) med_abs_deviation = np . median ( diff ) modified_z_score = 0.6745 * diff / med_abs_deviation return modified_z_score > thresh
1037	import os import gc import cv2 import json import time import numpy as np import pandas as pd from pathlib import Path from keras . utils import to_categorical import seaborn as sns import plotly . express as px from matplotlib import colors import matplotlib . pyplot as plt import plotly . figure_factory as ff import torch T = torch . Tensor import torch . nn as nn from torch . optim import Adam from torch . utils . data import Dataset , DataLoader
1038	test_task_files = sorted ( os . listdir ( TEST_PATH ) ) test_tasks = [ ] for task_file in test_task_files : with open ( str ( TEST_PATH / task_file ) , 'r' ) as f : task = json . load ( f ) test_tasks . append ( task )
1039	Xs_test , Xs_train , ys_train = [ ] , [ ] , [ ] for task in test_tasks : X_test , X_train , y_train = [ ] , [ ] , [ ] for pair in task [ "test" ] : X_test . append ( pair [ "input" ] ) for pair in task [ "train" ] : X_train . append ( pair [ "input" ] ) y_train . append ( pair [ "output" ] ) Xs_test . append ( X_test ) Xs_train . append ( X_train ) ys_train . append ( y_train )
1040	means = [ np . mean ( X ) for X in matrices ] fig = ff . create_distplot ( [ means ] , group_labels = [ "Means" ] , colors = [ "green" ] ) fig . update_layout ( title_text = "Distribution of matrix mean values" )
1041	def flattener ( pred ) : str_pred = str ( [ row for row in pred ] ) str_pred = str_pred . replace ( ', ' , '' ) str_pred = str_pred . replace ( '[[' , '|' ) str_pred = str_pred . replace ( '][' , '|' ) str_pred = str_pred . replace ( ']]' , '|' ) return str_pred
1042	test_predictions = [ [ list ( pred ) for pred in test_pred ] for test_pred in test_predictions ] for idx , pred in enumerate ( test_predictions ) : test_predictions [ idx ] = flattener ( pred ) submission = pd . read_csv ( SUBMISSION_PATH ) submission [ "output" ] = test_predictions
1043	import os import gc import numpy as np import pandas as pd from tqdm import tqdm_notebook as tqdm import seaborn as sns from collections import Counter import matplotlib . pyplot as plt from IPython . display import SVG import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm import xgboost import catboost import keras from keras . models import Model from keras . utils . vis_utils import model_to_dot from keras . layers import Input , Dense , Dropout , BatchNormalization from sklearn . preprocessing import MinMaxScaler
1044	DATA_PATH = '../input/ieee-fraud-detection/' TRAIN_PATH = DATA_PATH + 'train_transaction.csv' TEST_PATH = DATA_PATH + 'test_transaction.csv'
1045	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "ProductCD" , data = train_df , palette = reversed ( [ 'aquamarine' , 'mediumaquamarine' , 'mediumseagreen' , 'seagreen' , 'darkgreen' ] ) ) . set_title ( 'ProductCD' , fontsize = 16 ) plt . show ( plot )
1046	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) \ . groupby ( "P_emaildomain" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'lightblue' , 'darkblue' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1047	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) \ . groupby ( "R_emaildomain" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'pink' , 'crimson' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1048	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "card4" , data = train_df . query ( "TransactionAmt < 500" ) , palette = reversed ( [ 'orangered' , 'darkorange' , 'orange' , 'peachpuff' , 'navajowhite' ] ) ) . set_title ( 'card4' , fontsize = 16 ) plt . show ( plot )
1049	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) \ . groupby ( "card4" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'peachpuff' , 'darkorange' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1050	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "card6" , data = train_df . query ( "TransactionAmt < 500" ) . query ( "card6 == 'credit' or card6 == 'debit'" ) , palette = reversed ( [ 'red' , 'crimson' , 'mediumvioletred' , 'darkmagenta' , 'indigo' ] ) ) . set_title ( 'card6' , fontsize = 16 ) plt . show ( plot )
1051	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) . query ( "card6 == 'credit' or card6 == 'debit'" ) \ . groupby ( "card6" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'plum' , 'purple' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1052	def prepare_data ( df , cat_cols = cat_cols ) : cat_cols = [ col for col in cat_cols if col in df . columns ] for col in tqdm ( cat_cols ) : \ df [ col ] = pd . factorize ( df [ col ] ) [ 0 ] return df
1053	X = train_data . sort_values ( 'TransactionDT' ) . drop ( [ 'isFraud' , 'TransactionDT' , 'TransactionID' ] , axis = 1 ) y = train_data . sort_values ( 'TransactionDT' ) [ 'isFraud' ] del train_data
1054	parameters = { 'application' : 'binary' , 'objective' : 'binary' , 'metric' : 'auc' , 'is_unbalance' : 'true' , 'boosting' : 'gbdt' , 'num_leaves' : 31 , 'feature_fraction' : 0.5 , 'bagging_fraction' : 0.5 , 'bagging_freq' : 20 , 'learning_rate' : 0.05 , 'verbose' : 0 } train_data = lightgbm . Dataset ( X_train , label = y_train , categorical_feature = cat_cols ) val_data = lightgbm . Dataset ( X_val , label = y_val ) model = lightgbm . train ( parameters , train_data , valid_sets = val_data , num_boost_round = 5000 , early_stopping_rounds = 100 )
1055	plt . rcParams [ "axes.titlesize" ] = 16 plt . rcParams [ "axes.labelsize" ] = 15 plt . rcParams [ "xtick.labelsize" ] = 13 plt . rcParams [ "ytick.labelsize" ] = 13 plot = lightgbm . plot_importance ( model , max_num_features = 10 , figsize = ( 20 , 20 ) , grid = False , color = sns . color_palette ( "husl" , 20 ) ) plt . show ( plot )
1056	fig , ax = plt . subplots ( figsize = ( 7 , 7 ) ) plt . plot ( history . history [ 'acc' ] , color = 'blue' ) plt . plot ( history . history [ 'val_acc' ] , color = 'orangered' ) plt . title ( 'Model accuracy' ) plt . ylabel ( 'Accuracy' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
1057	fig , ax = plt . subplots ( figsize = ( 7 , 7 ) ) plt . plot ( history . history [ 'loss' ] , color = 'blue' ) plt . plot ( history . history [ 'val_loss' ] , color = 'orangered' ) plt . title ( 'Model loss' ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
1058	TEXT_COL = 'comment_text' EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec' MAXLEN = 128 ENDLEN = 32 MAX_FEATURES = 100000 EMBED_SIZE = 300 BATCH_SIZE = 2048 NUM_EPOCHS = 100
1059	lengths = train_df [ TEXT_COL ] . apply ( len ) train_df [ 'lengths' ] = lengths lengths = train_df . loc [ train_df [ 'lengths' ] < 1125 ] [ 'lengths' ] sns . distplot ( lengths , color = 'r' ) plt . show ( )
1060	words = train_df [ TEXT_COL ] . apply ( lambda x : len ( x ) - len ( '' . join ( x . split ( ) ) ) + 1 ) train_df [ 'words' ] = words words = train_df . loc [ train_df [ 'words' ] < 200 ] [ 'words' ] sns . distplot ( words , color = 'g' ) plt . show ( )
1061	avg_word_len = train_df [ TEXT_COL ] . apply ( lambda x : 1.0 * len ( '' . join ( x . split ( ) ) ) / ( len ( x ) - len ( '' . join ( x . split ( ) ) ) + 1 ) ) train_df [ 'avg_word_len' ] = avg_word_len avg_word_len = train_df . loc [ train_df [ 'avg_word_len' ] < 10 ] [ 'avg_word_len' ] sns . distplot ( avg_word_len , color = 'b' ) plt . show ( )
1062	tokenizer = Tokenizer ( num_words = MAX_FEATURES , lower = True ) tokenizer . fit_on_texts ( list ( train_df [ TEXT_COL ] ) + list ( test_df [ TEXT_COL ] ) ) word_index = tokenizer . word_index
1063	def squash ( x , axis = - 1 ) : s_squared_norm = K . sum ( K . square ( x ) , axis , keepdims = True ) + K . epsilon ( ) scale = K . sqrt ( s_squared_norm ) / ( 0.5 + s_squared_norm ) return scale * x
1064	with open ( 'word_index.json' , 'w' ) as f : json . dump ( word_index , f )
1065	import os import gc import pandas as pd import numpy as np from sklearn . metrics import accuracy_score , mean_absolute_error , mean_squared_error import matplotlib . pyplot as plt import seaborn as sns from langdetect import detect import markdown import json import requests import warnings import time from colorama import Fore , Back , Style , init
1066	allowed = [ "TOXICITY" , "SEVERE_TOXICITY" , "TOXICITY_FAST" , "ATTACK_ON_AUTHOR" , "ATTACK_ON_COMMENTER" , "INCOHERENT" , "INFLAMMATORY" , "OBSCENE" , "OFF_TOPIC" , "UNSUBSTANTIAL" , "LIKELY_TO_REJECT" ] class Perspective ( object ) : base_url = "https://commentanalyzer.googleapis.com/v1alpha1" def __init__ ( self , key ) : self . key = key def score ( self , text , tests = [ "TOXICITY" ] , context = None , languages = None , do_not_store = False , token = None , text_type = None ) :
1067	train_df = pd . read_csv ( '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv' ) comments = train_df [ 'comment_text' ] targets = train_df [ 'target' ] severe_toxicities = train_df [ 'severe_toxicity' ] obscenities = train_df [ 'obscene' ] del train_df gc . collect ( )
1068	with open ( '../input/google-api-information/Google API Key.txt' ) as f : google_api_key = f . readline ( ) [ : - 1 ] client = Perspective ( google_api_key )
1069	print ( "Toxicity Mean Absolute Error : " + \ str ( mean_absolute_error ( targets [ : len ( toxicity_scores ) ] , toxicity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Obscneity Mean Absolute Error : " + \ str ( mean_absolute_error ( obscenities [ : len ( toxicity_scores ) ] , obscenity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Severe Toxicity Mean Absolute Error : " + \ str ( mean_absolute_error ( severe_toxicities [ : len ( toxicity_scores ) ] , severe_toxicity_scores [ : len ( toxicity_scores ) ] ) ) )
1070	print ( "Toxicity Squared Absolute Error : " + \ str ( mean_squared_error ( targets [ : len ( toxicity_scores ) ] , toxicity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Obscneity Squared Absolute Error : " + \ str ( mean_squared_error ( obscenities [ : len ( toxicity_scores ) ] , obscenity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Severe Toxicity Squared Absolute Error : " + \ str ( mean_squared_error ( severe_toxicities [ : len ( toxicity_scores ) ] , severe_toxicity_scores [ : len ( toxicity_scores ) ] ) ) )
1071	keys = set ( train_df . ebird_code ) values = np . arange ( 0 , len ( keys ) ) code_dict = dict ( zip ( sorted ( keys ) , values ) )
1072	O = len ( code_dict ) network = BirdNet ( f = F , o = O ) optimizer = Adam ( [ { 'params' : network . resnet . parameters ( ) , 'lr' : LR [ 0 ] } , { 'params' : network . dense_output . parameters ( ) , 'lr' : LR [ 1 ] } ] )
1073	def cel ( y_true , y_pred ) : y_true = torch . argmax ( y_true , axis = - 1 ) return nn . CrossEntropyLoss ( ) ( y_pred , y_true . squeeze ( ) ) def accuracy ( y_true , y_pred ) : y_true = torch . argmax ( y_true , axis = - 1 ) . squeeze ( ) y_pred = torch . argmax ( y_pred , axis = - 1 ) . squeeze ( ) return ( y_true == y_pred ) . float ( ) . sum ( ) / len ( y_true )
1074	network . eval ( ) test_preds = [ ] test_set = BirdTestDataset ( test_df , TEST_AUDIO_PATH ) test_loader = DataLoader ( test_set , batch_size = VAL_BATCH_SIZE ) if os . path . exists ( TEST_AUDIO_PATH ) : for test_X in tqdm ( test_loader ) : test_pred = network . forward ( test_X . view ( - 1 , * D ) . to ( device ) ) test_preds . extend ( softmax ( test_pred . detach ( ) . cpu ( ) . numpy ( ) ) . flatten ( ) )
1075	import os import gc import re import numpy as np import pandas as pd import nltk from nltk . corpus import wordnet , stopwords from nltk . stem import WordNetLemmatizer from nltk . stem . porter import PorterStemmer from colorama import Fore , Back , Style
1076	def remove_numbers ( text ) : text = '' . join ( [ i for i in text if not i . isdigit ( ) ] ) return text
1077	def replace_multi_exclamation_mark ( text ) : text = re . sub ( r"(\!)\1+" , ' multiExclamation ' , text ) return text def replace_multi_question_mark ( text ) : text = re . sub ( r"(\?)\1+" , ' multiQuestion ' , text ) return text def replace_multi_stop_mark ( text ) : text = re . sub ( r"(\.)\1+" , ' multiStop ' , text ) return text
1078	text = re . sub ( r"(\!)\1+" , ' multiExclamation ' , text ) return text def replace_multi_question_mark ( text ) :
1079	text = re . sub ( r"(\?)\1+" , ' multiQuestion ' , text ) return text def replace_multi_stop_mark ( text ) :
1080	def replace_elongated ( word ) : repeat_regexp = re . compile ( r'(\w*)(\w)\2(\w*)' ) repl = r'\1\2\3' if wordnet . synsets ( word ) : return word repl_word = repeat_regexp . sub ( repl , word ) if repl_word != word : return replace_elongated ( repl_word ) else : return repl_word def replace_elongated_words ( text ) : finalTokens = [ ] tokens = nltk . word_tokenize ( text ) for w in tokens : finalTokens . append ( replace_elongated ( w ) ) text = " " . join ( finalTokens ) return text
1081	def get_neural_network ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 10 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 10 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model = get_neural_network ( )
1082	split = np . int32 ( 0.8 * len ( X ) ) X_train = X [ : split ] y_train = np . int32 ( y ) [ : split ] X_val = X [ split : ] y_val = np . int32 ( y ) [ split : ]
1083	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1084	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1085	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1086	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1087	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1088	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1089	preds_one_val = model_one . predict ( X_val ) preds_two_val = model_two . predict ( X_val ) preds_one_train = model_one . predict ( X_train ) preds_two_train = model_two . predict ( X_train )
1090	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1091	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1092	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1093	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1094	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1095	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1096	fig . update_layout ( title = "Accuracy over the epochs" , yaxis = dict ( title = "Accuracy" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1097	fig . update_layout ( title = "Loss over the epochs" , yaxis = dict ( title = "Loss" ) , yaxis_type = "log" ) fig . update_layout ( barmode = 'group' ) fig . show ( )
1098	preds_one_val = model_one . predict ( X_val ) preds_two_val = model_two . predict ( X_val ) preds_one_train = model_one . predict ( X_train ) preds_two_train = model_two . predict ( X_train )
1099	import os import numpy as np import pandas as pd from tqdm import tqdm tqdm . pandas ( ) from nltk import word_tokenize , pos_tag from collections import Counter import matplotlib . pyplot as plt import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' )
1100	SIGNAL_LEN = 150000 MIN_NUM = - 27 MAX_NUM = 28
1101	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
1102	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
1103	def min_max_transfer ( ts , min_value , max_value , range_needed = ( - 1 , 1 ) ) : ts_std = ( ts - min_value ) / ( max_value - min_value ) if range_needed [ 0 ] < 0 : return ts_std * ( range_needed [ 1 ] + abs ( range_needed [ 0 ] ) ) + range_needed [ 0 ] else : return ts_std * ( range_needed [ 1 ] - range_needed [ 0 ] ) + range_needed [ 0 ]
1104	def prepare_data ( start , end ) : train = pd . DataFrame ( np . transpose ( signals [ int ( start ) : int ( end ) ] ) ) X = [ ] for id_measurement in tqdm ( train . index [ int ( start ) : int ( end ) ] ) : X_signal = transform_ts ( train [ id_measurement ] ) X . append ( X_signal ) X = np . asarray ( X ) return X
1105	X = [ ] def load_all ( ) : total_size = len ( signals ) for start , end in [ ( 0 , int ( total_size ) ) ] : X_temp = prepare_data ( start , end ) X . append ( X_temp ) load_all ( ) X = np . concatenate ( X )
1106	plot = sns . jointplot ( x = perm_entropies , y = targets , kind = 'kde' , color = 'orangered' ) plot . set_axis_labels ( 'perm_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1107	plot = sns . jointplot ( x = perm_entropies , y = targets , kind = 'reg' , color = 'orangered' ) plot . set_axis_labels ( 'perm_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1108	n = x . size n1 = n - 1 mm += 1 mm_dbld = 2 * mm
1109	plot = sns . jointplot ( x = app_entropies , y = targets , kind = 'kde' , color = 'magenta' ) plot . set_axis_labels ( 'app_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1110	plot = sns . jointplot ( x = app_entropies , y = targets , kind = 'reg' , color = 'magenta' ) plot . set_axis_labels ( 'app_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1111	m_lm = 0 for m in range ( k ) : m_lm += lm [ m ] m_lm /= k lk [ k - 1 ] = m_lm x_reg [ k - 1 ] = log ( 1. / k ) y_reg [ k - 1 ] = log ( m_lm ) higuchi , _ = _linear_regression ( x_reg , y_reg ) return higuchi def higuchi_fd ( x , kmax = 10 ) :
1112	x = np . asarray ( x , dtype = np . float64 ) kmax = int ( kmax ) return _higuchi_fd ( x , kmax ) @ jit ( 'UniTuple(float64, 2)(float64[:], float64[:])' , nopython = True ) def _linear_regression ( x , y ) :
1113	plot = sns . jointplot ( x = higuchi_fds , y = targets , kind = 'kde' , color = 'crimson' ) plot . set_axis_labels ( 'higuchi_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1114	plot = sns . jointplot ( x = katz_fds , y = targets , kind = 'reg' , color = 'forestgreen' ) plot . set_axis_labels ( 'katz_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1115	import os import gc import numpy as np from numpy . fft import * import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import pywt from statsmodels . robust import mad import scipy from scipy import signal from scipy . signal import butter , deconvolve import warnings warnings . filterwarnings ( 'ignore' )
1116	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
1117	def maddest ( d , axis = None ) : return np . mean ( np . absolute ( d - np . mean ( d , axis ) ) , axis )
1118	sos = butter ( 10 , Wn = [ norm_low_cutoff ] , btype = 'highpass' , output = 'sos' ) filtered_sig = signal . sosfilt ( sos , x ) return filtered_sig
1119	SIGNAL_LEN = 150000 MIN_NUM = - 27 MAX_NUM = 28
1120	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
1121	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
1122	def min_max_transfer ( ts , min_value , max_value , range_needed = ( - 1 , 1 ) ) : ts_std = ( ts - min_value ) / ( max_value - min_value ) if range_needed [ 0 ] < 0 : return ts_std * ( range_needed [ 1 ] + abs ( range_needed [ 0 ] ) ) + range_needed [ 0 ] else : return ts_std * ( range_needed [ 1 ] - range_needed [ 0 ] ) + range_needed [ 0 ]
1123	def prepare_data ( start , end ) : train = pd . DataFrame ( np . transpose ( signals [ int ( start ) : int ( end ) ] ) ) X = [ ] for id_measurement in tqdm ( train . index [ int ( start ) : int ( end ) ] ) : X_signal = transform_ts ( train [ id_measurement ] ) X . append ( X_signal ) X = np . asarray ( X ) return X
1124	X = [ ] def load_all ( ) : total_size = len ( signals ) for start , end in [ ( 0 , int ( total_size ) ) ] : X_temp = prepare_data ( start , end ) X . append ( X_temp ) load_all ( ) X = np . concatenate ( X )
1125	if method == 'fft' : _ , psd = periodogram ( x , sf ) elif method == 'welch' : _ , psd = welch ( x , sf , nperseg = nperseg ) psd_norm = np . divide ( psd , psd . sum ( ) ) se = - np . multiply ( psd_norm , np . log2 ( psd_norm ) ) . sum ( ) if normalize : se /= np . log2 ( psd_norm . size ) return se
1126	plot = sns . jointplot ( x = spectral_entropies , y = targets , kind = 'kde' , color = 'blueviolet' ) plot . set_axis_labels ( 'spectral_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1127	plot = sns . jointplot ( x = spectral_entropies , y = targets , kind = 'reg' , color = 'blueviolet' ) plot . set_axis_labels ( 'spectral_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1128	n = x . size n1 = n - 1 mm += 1 mm_dbld = 2 * mm
1129	x = np . asarray ( x , dtype = np . float64 ) if metric == 'chebyshev' and x . size < 5000 : return _numba_sampen ( x , mm = order , r = 0.2 ) else : phi = _app_samp_entropy ( x , order = order , metric = metric , approximate = False ) return - np . log ( np . divide ( phi [ 1 ] , phi [ 0 ] ) )
1130	plot = sns . jointplot ( x = sample_entropies , y = targets , kind = 'kde' , color = 'mediumvioletred' ) plot . set_axis_labels ( 'sample_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1131	plot = sns . jointplot ( x = sample_entropies , y = targets , kind = 'reg' , color = 'mediumvioletred' ) plot . set_axis_labels ( 'sample_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1132	for p in [ slope [ i ] , intercept [ i ] ] : y = y * ran_n + p trend [ i , : ] = y
1133	plot = sns . jointplot ( x = detrended_fluctuations , y = targets , kind = 'kde' , color = 'mediumblue' ) plot . set_axis_labels ( 'detrended_fluctuation' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1134	plot = sns . jointplot ( x = detrended_fluctuations , y = targets , kind = 'reg' , color = 'mediumblue' ) plot . set_axis_labels ( 'detrended_fluctuation' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1135	INPUT_DIR = '../input/m5-forecasting-accuracy' calendar = pd . read_csv ( f'{INPUT_DIR}/calendar.csv' ) selling_prices = pd . read_csv ( f'{INPUT_DIR}/sell_prices.csv' ) sample_submission = pd . read_csv ( f'{INPUT_DIR}/sample_submission.csv' ) sales_train_val = pd . read_csv ( f'{INPUT_DIR}/sales_train_validation.csv' )
1136	error = [ error_naive , error_avg , error_holt , error_exponential , error_arima , error_prophet ] names = [ "Naive approach" , "Moving average" , "Holt linear" , "Exponential smoothing" , "ARIMA" , "Prophet" ] df = pd . DataFrame ( np . transpose ( [ error , names ] ) ) df . columns = [ "RMSE Loss" , "Model" ] px . bar ( df , y = "RMSE Loss" , x = "Model" , color = "Model" , title = "RMSE Loss vs. Model" )
1137	import os import gc import numpy as np import pandas as pd from tqdm import tqdm tqdm . pandas ( ) from collections import Counter from operator import itemgetter import scipy import cv2 from cv2 import imread import matplotlib import matplotlib . pyplot as plt import seaborn as sns
1138	train_images = [ ] image_dirs = np . take ( os . listdir ( '../input/train' ) , select_rows ) for image_dir in tqdm ( sorted ( image_dirs ) ) : image = imread ( '../input/train/' + image_dir ) train_images . append ( image ) del image gc . collect ( ) train_images = np . array ( train_images )
1139	labels_df = pd . read_csv ( '../input/labels.csv' ) label_dict = dict ( zip ( labels_df . attribute_id , labels_df . attribute_name ) ) for key in label_dict : if 'culture' in label_dict [ key ] : label_dict [ key ] = label_dict [ key ] [ 9 : ] if 'tag' in label_dict [ key ] : label_dict [ key ] = label_dict [ key ] [ 5 : ]
1140	train_targets = [ ] for targets in targets_df . attribute_ids : target = targets . split ( ) target = list ( map ( lambda x : label_dict [ int ( x ) ] , target ) ) train_targets . append ( target ) train_targets = np . array ( train_targets )
1141	fig , ax = plt . subplots ( nrows = 4 , ncols = 4 , figsize = ( 50 , 50 ) ) count = 0 for i in range ( 4 ) : for j in range ( 4 ) : ax [ i , j ] . imshow ( cv2 . cvtColor ( train_images [ count ] , cv2 . COLOR_BGR2RGB ) ) ax [ i , j ] . set_title ( str ( train_targets [ count ] ) , fontsize = 24 ) count = count + 1
1142	FOLDS = 8 EPOCHS = 4 RRC = 1.0 FLIP = 1.0 NORM = 1.0 ROTATE = 1.0 LR = ( 1e-4 , 1e-3 ) MODEL_SAVE_PATH = "resnet_model" WIDTH = 512 HEIGHT = 512 BATCH_SIZE = 128 VAL_BATCH_SIZE = 128 DATA_PATH = '../input/prostate-cancer-grade-assessment/' RESIZED_PATH = '../input/panda-resized-train-data-512x512/train_images/'
1143	test_df = pd . read_csv ( TEST_DATA_PATH ) train_df = pd . read_csv ( TRAIN_DATA_PATH ) sample_submission = pd . read_csv ( SAMPLE_SUB_PATH )
1144	gleason_replace_dict = { 0 : 0 , 1 : 1 , 3 : 2 , 4 : 3 , 5 : 4 } def process_gleason ( gleason ) : if gleason == 'negative' : gs = ( 1 , 1 ) else : gs = tuple ( gleason . split ( '+' ) ) return [ gleason_replace_dict [ int ( g ) ] for g in gs ] train_df . gleason_score = train_df . gleason_score . apply ( process_gleason )
1145	model = ResNetDetector ( ) x = torch . randn ( 2 , 3 , 32 , 32 ) . requires_grad_ ( True ) y = model ( x ) make_dot ( y , params = dict ( list ( model . named_parameters ( ) ) + [ ( 'x' , x ) ] ) )
1146	def cel ( inp , targ ) : _ , labels = targ . max ( dim = 1 ) return nn . CrossEntropyLoss ( ) ( inp , labels ) def acc ( inp , targ ) : inp_idx = inp . max ( axis = 1 ) . indices targ_idx = targ . max ( axis = 1 ) . indices return ( inp_idx == targ_idx ) . float ( ) . sum ( axis = 0 ) / len ( inp_idx )
1147	EPOCHS = 8 BATCH_SIZE = 128 DATA_PATH = '../input/nfl-big-data-bowl-2020/'
1148	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "X" ] , y = data [ "Yards" ] , kind = 'kde' , color = 'forestgreen' , height = 7 ) plot . set_axis_labels ( 'X coordinate' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1149	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "Y" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 179 / 255 , 0 , 30 / 255 ) , height = 7 ) plot . set_axis_labels ( 'Y coordinate' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1150	data = train_df . sample ( frac = 0.025 ) plot = sns . jointplot ( x = data [ "X" ] , y = data [ "Y" ] , kind = 'kde' , color = 'mediumvioletred' , height = 7 ) plot . set_axis_labels ( 'X coordinate' , 'Y coordinate' , fontsize = 16 ) plt . show ( plot )
1151	fig = ff . create_distplot ( hist_data = [ train_df . sample ( frac = 0.025 ) [ "S" ] ] , group_labels = "S" , colors = [ 'rgb(230, 0, 191)' ] ) fig . update_layout ( title = "S" , yaxis = dict ( title = "Probability Density" ) , xaxis = dict ( title = "S" ) ) fig . show ( )
1152	cat_cols = [ 'Team' , 'FieldPosition' , 'OffenseFormation' ] value_dicts = [ ] for feature in cat_cols : values = set ( train_df [ feature ] ) value_dicts . append ( dict ( zip ( values , np . arange ( len ( values ) ) ) ) )
1153	def indices ( data , feat_index ) : value_dict = value_dicts [ feat_index ] return data [ cat_cols [ feat_index ] ] . apply ( lambda x : value_dict [ x ] ) def one_hot ( indices , feat_index ) : return to_categorical ( indices , num_classes = len ( value_dicts [ feat_index ] ) )
1154	num_cols = [ 'X' , 'S' , 'A' , 'Dis' , 'Orientation' , 'Dir' , 'YardLine' , 'Quarter' , 'Down' , 'Distance' , 'HomeScoreBeforePlay' , 'VisitorScoreBeforePlay' , 'DefendersInTheBox' , 'PlayerWeight' , 'Week' , 'Temperature' , 'Humidity' ] def get_numerical_features ( sample ) : return sample [ num_cols ] . values
1155	hl_graph = hl . build_graph ( CNN1DNetwork ( ) , torch . zeros ( [ 1 , 25 , 17 ] ) ) hl_graph . theme = hl . graph . THEMES [ "blue" ] . copy ( ) hl_graph
1156	mean = 0. std = 0. nb_samples = 0. for data , _ in tqdm ( train_loader ) : batch_samples = data . size ( 0 ) data = data . view ( batch_samples , data . size ( 1 ) , - 1 ) mean += data . mean ( ( 0 , 1 ) ) std += data . std ( ( 0 , 1 ) ) nb_samples += batch_samples mean /= nb_samples std /= nb_samples
1157	def nonan ( x ) : if type ( x ) == str : return x . replace ( "\n" , "" ) else : return "" text = ' ' . join ( [ nonan ( abstract ) for abstract in train_data [ "comment_text" ] ] ) wordcloud = WordCloud ( max_font_size = None , background_color = 'black' , collocations = False , width = 1200 , height = 1000 ) . generate ( text ) fig = px . imshow ( wordcloud ) fig . update_layout ( title_text = 'Common words in comments' )
1158	df [ "country" ] = df [ "Language" ] . apply ( get_country ) df = df . query ( "country != 'None'" ) fig = px . choropleth ( df , locations = "country" , hover_name = "country" , projection = "natural earth" , locationmode = "country names" , title = "Average comment length vs. Country" , color = "Average_comment_words" , template = "plotly" , color_continuous_scale = "aggrnyl" ) fig
1159	fig = go . Figure ( go . Histogram ( x = [ pols [ "compound" ] for pols in train_data [ "polarity" ] if pols [ "compound" ] != 0 ] , marker = dict ( color = 'orchid' ) ) ) fig . update_layout ( xaxis_title = "Compound sentiment" , title_text = "Compound sentiment" , template = "simple_white" ) fig . show ( )
1160	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "compound" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "compound" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Compound vs. Toxicity" , xaxis_title = "Compound" , template = "simple_white" ) fig . show ( )
1161	fig = go . Figure ( go . Histogram ( x = train_data . query ( "flesch_reading_ease > 0" ) [ "flesch_reading_ease" ] , marker = dict ( color = 'darkorange' ) ) ) fig . update_layout ( xaxis_title = "Flesch reading ease" , title_text = "Flesch reading ease" , template = "simple_white" ) fig . show ( )
1162	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "flesch_reading_ease" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "flesch_reading_ease" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Flesch reading ease vs. Toxicity" , xaxis_title = "Flesch reading ease" , template = "simple_white" ) fig . show ( )
1163	fig = go . Figure ( go . Histogram ( x = train_data . query ( "automated_readability < 100" ) [ "automated_readability" ] , marker = dict ( color = 'mediumaquamarine' ) ) ) fig . update_layout ( xaxis_title = "Automated readability" , title_text = "Automated readability" , template = "simple_white" ) fig . show ( )
1164	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "automated_readability" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "automated_readability" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Automated readability vs. Toxicity" , xaxis_title = "Automated readability" , template = "simple_white" ) fig . show ( )
1165	fig = go . Figure ( data = [ go . Pie ( labels = train_data . columns [ 2 : 7 ] , values = train_data . iloc [ : , 2 : 7 ] . sum ( ) . values , marker = dict ( colors = px . colors . qualitative . Plotly ) ) ] ) fig . update_traces ( textposition = 'outside' , textfont = dict ( color = "black" ) ) fig . update_layout ( title_text = "Pie chart of labels" ) fig . show ( )
1166	AUTO = tf . data . experimental . AUTOTUNE tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( 'jigsaw-multilingual-toxic-comment-classification' ) EPOCHS = 2 BATCH_SIZE = 32 * strategy . num_replicas_in_sync
1167	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) save_path = '/kaggle/working/distilbert_base_uncased/' if not os . path . exists ( save_path ) : os . makedirs ( save_path ) tokenizer . save_pretrained ( save_path ) fast_tokenizer = BertWordPieceTokenizer ( 'distilbert_base_uncased/vocab.txt' , lowercase = True )
1168	x_train = fast_encode ( train . comment_text . astype ( str ) , fast_tokenizer , maxlen = 512 ) x_valid = fast_encode ( val_data . comment_text . astype ( str ) . values , fast_tokenizer , maxlen = 512 ) x_test = fast_encode ( test_data . content . astype ( str ) . values , fast_tokenizer , maxlen = 512 ) y_valid = val . toxic . values y_train = train . toxic . values
1169	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
1170	with strategy . scope ( ) : transformer_layer = transformers . TFDistilBertModel . \ from_pretrained ( 'distilbert-base-multilingual-cased' ) model_vnn = build_vnn_model ( transformer_layer , max_len = 512 ) model_vnn . summary ( )
1171	def callback ( ) : cb = [ ] reduceLROnPlat = ReduceLROnPlateau ( monitor = 'val_loss' , factor = 0.3 , patience = 3 , verbose = 1 , mode = 'auto' , epsilon = 0.0001 , cooldown = 1 , min_lr = 0.000001 ) cb . append ( reduceLROnPlat ) log = CSVLogger ( 'log.csv' ) cb . append ( log ) RocAuc = RocAucEvaluation ( validation_data = ( x_valid , y_valid ) , interval = 1 ) cb . append ( RocAuc ) return cb
1172	N_STEPS = x_train . shape [ 0 ] // BATCH_SIZE calls = callback ( ) train_history = model_vnn . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1173	with strategy . scope ( ) : model_cnn = build_cnn_model ( transformer_layer , max_len = 512 ) model_cnn . summary ( )
1174	train_history = model_cnn . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1175	with strategy . scope ( ) : model_lstm = build_lstm_model ( transformer_layer , max_len = 512 ) model_lstm . summary ( )
1176	train_history = model_lstm . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1177	with strategy . scope ( ) : model_capsule = build_capsule_model ( transformer_layer , max_len = 512 ) model_capsule . summary ( )
1178	train_history = model_capsule . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1179	with strategy . scope ( ) : model_distilbert = build_distilbert_model ( transformer_layer , max_len = 512 ) model_distilbert . summary ( )
1180	train_history = model_distilbert . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1181	EPOCHS = 5 MAXLEN = 64 SPLIT = 0.8 DROP_RATE = 0.3 LR = ( 4e-5 , 1e-2 ) BATCH_SIZE = 256 VAL_BATCH_SIZE = 8192 MODEL_SAVE_PATH = 'insincerity_model.pt'
1182	EPOCHS = 20 SAMPLE_LEN = 100 IMAGE_PATH = "../input/plant-pathology-2020-fgvc7/images/" TEST_PATH = "../input/plant-pathology-2020-fgvc7/test.csv" TRAIN_PATH = "../input/plant-pathology-2020-fgvc7/train.csv" SUB_PATH = "../input/plant-pathology-2020-fgvc7/sample_submission.csv" sub = pd . read_csv ( SUB_PATH ) test_data = pd . read_csv ( TEST_PATH ) train_data = pd . read_csv ( TRAIN_PATH )
1183	def load_image ( image_id ) : file_path = image_id + ".jpg" image = cv2 . imread ( IMAGE_PATH + file_path ) return cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) train_images = train_data [ "image_id" ] [ : SAMPLE_LEN ] . progress_apply ( load_image )
1184	fig = ff . create_distplot ( [ values ] , group_labels = [ "Channels" ] , colors = [ "purple" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1185	fig = ff . create_distplot ( [ red_values ] , group_labels = [ "R" ] , colors = [ "red" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of red channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1186	fig = ff . create_distplot ( [ green_values ] , group_labels = [ "G" ] , colors = [ "green" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of green channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1187	fig = ff . create_distplot ( [ blue_values ] , group_labels = [ "B" ] , colors = [ "blue" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of blue channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1188	AUTO = tf . data . experimental . AUTOTUNE tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) BATCH_SIZE = 16 * strategy . num_replicas_in_sync GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( )
1189	def format_path ( st ) : return GCS_DS_PATH + '/images/' + st + '.jpg' test_paths = test_data . image_id . apply ( format_path ) . values train_paths = train_data . image_id . apply ( format_path ) . values train_labels = np . float32 ( train_data . loc [ : , 'healthy' : 'scab' ] . values ) train_paths , valid_paths , train_labels , valid_labels = \ train_test_split ( train_paths , train_labels , test_size = 0.15 , random_state = 2020 )
1190	lrfn = build_lrfn ( ) STEPS_PER_EPOCH = train_labels . shape [ 0 ] // BATCH_SIZE lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lrfn , verbose = 1 )
1191	EPOCHS = 20 SPLIT = 0.8 MAXLEN = 48 DROP_RATE = 0.3 np . random . seed ( 42 ) OUTPUT_UNITS = 3 BATCH_SIZE = 384 LR = ( 4e-5 , 1e-2 ) ROBERTA_UNITS = 768 VAL_BATCH_SIZE = 384 MODEL_SAVE_PATH = 'sentiment_model.pt'
1192	def cel ( inp , target ) : _ , labels = target . max ( dim = 1 ) return nn . CrossEntropyLoss ( ) ( inp , labels ) * len ( inp ) def accuracy ( inp , target ) : inp_ind = inp . max ( axis = 1 ) . indices target_ind = target . max ( axis = 1 ) . indices return ( inp_ind == target_ind ) . float ( ) . sum ( axis = 0 )
1193	val_losses = [ torch . load ( 'val_loss_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] train_losses = [ torch . load ( 'train_loss_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] val_accuracies = [ torch . load ( 'val_acc_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] train_accuracies = [ torch . load ( 'train_acc_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ]
1194	slide = openslide . OpenSlide ( os . path . join ( train_images_path , f'{image_id}.tiff' ) ) mask = openslide . OpenSlide ( os . path . join ( mask_images_path , f'{image_id}_mask.tiff' ) ) slide_data = slide . read_region ( ( 0 , 0 ) , slide . level_count - 1 , slide . level_dimensions [ - 1 ] ) mask_data = mask . read_region ( ( 0 , 0 ) , mask . level_count - 1 , mask . level_dimensions [ - 1 ] ) mask_data = mask_data . split ( ) [ 0 ]
1195	W = 512 H = 512 B = 0.5 SPLIT = 0.8 SAMPLE = True MU = [ 0.485 , 0.456 , 0.406 ] SIGMA = [ 0.229 , 0.224 , 0.225 ] EPOCHS = 5 LR = 1e-3 , 1e-3 BATCH_SIZE = 32 VAL_BATCH_SIZE = 32 MODEL = 'efficientnet-b3' IMG_PATHS = [ '../working/test' , '../working/train_1' , '../working/train_2' ]
1196	PATH_DICT = { } for folder_path in tqdm ( IMG_PATHS ) : for img_path in os . listdir ( folder_path ) : PATH_DICT [ img_path ] = folder_path + '/'
1197	def bce ( y_true , y_pred ) : return nn . BCEWithLogitsLoss ( ) ( y_pred , y_true ) def acc ( y_true , y_pred ) : y_true = y_true . squeeze ( ) y_pred = nn . Sigmoid ( ) ( y_pred ) . squeeze ( ) return ( y_true == torch . round ( y_pred ) ) . float ( ) . sum ( ) / len ( y_true )
1198	C = np . array ( [ B , ( 1 - B ) ] ) * 2 ones = len ( train_df . query ( 'target == 1' ) ) zeros = len ( train_df . query ( 'target == 0' ) ) weightage_fn = { 0 : C [ 1 ] / zeros , 1 : C [ 0 ] / ones } weights = [ weightage_fn [ target ] for target in train_df . target ]
1199	length = len ( train_df ) val_ids = val_df . image_name . apply ( lambda x : x + '.jpg' ) train_ids = train_df . image_name . apply ( lambda x : x + '.jpg' ) val_set = SIIMDataset ( val_df , False , True , ids = val_ids ) train_set = SIIMDataset ( train_df , True , True , ids = train_ids )
1200	train_sampler = WeightedRandomSampler ( weights , length ) if_sample , if_shuffle = ( train_sampler , False ) , ( None , True ) sample_fn = lambda is_sample , sampler : if_sample if is_sample else if_shuffle sampler , shuffler = sample_fn ( SAMPLE , train_sampler ) val_loader = DataLoader ( val_set , VAL_BATCH_SIZE , shuffle = False ) train_loader = DataLoader ( train_set , BATCH_SIZE , sampler = sampler , shuffle = shuffler )
1201	device = xm . xla_device ( ) network = CancerNet ( features = 1536 ) . to ( device ) optimizer = Adam ( [ { 'params' : network . efn . parameters ( ) , 'lr' : LR [ 0 ] } , { 'params' : network . dense_output . parameters ( ) , 'lr' : LR [ 1 ] } ] )
1202	bpps_files = os . listdir ( '../input/stanford-covid-vaccine/bpps/' ) example_bpps = np . load ( f'../input/stanford-covid-vaccine/bpps/{bpps_files[0]}' ) print ( 'bpps file shape:' , example_bpps . shape )
1203	plt . rcParams [ 'figure.figsize' ] = [ 20.0 , 7.0 ] plt . rcParams . update ( { 'font.size' : 22 , } ) sns . set_palette ( 'viridis' ) sns . set_style ( 'white' ) sns . set_context ( 'talk' , font_scale = 0.8 )
1204	X_train = train . drop ( [ 'id' , 'target' ] , axis = 1 ) y_train = train [ 'target' ] X_test = test . drop ( [ 'id' ] , axis = 1 )
1205	def get_cv_scores ( model ) : scores = cross_val_score ( model , X_train , y_train , cv = 5 , scoring = 'roc_auc' ) print ( 'CV Mean: ' , np . mean ( scores ) ) print ( 'STD: ' , np . std ( scores ) ) print ( '\n' )
1206	for model in models : print ( model ) get_cv_scores ( model )
1207	plt . rcParams [ 'figure.figsize' ] = [ 20.0 , 7.0 ] plt . rcParams . update ( { 'font.size' : 22 , } ) sns . set_palette ( 'viridis' ) sns . set_style ( 'white' ) sns . set_context ( 'talk' , font_scale = 0.8 )
1208	fig , ax = plt . subplots ( ) g = sns . countplot ( raw_data . target , palette = 'viridis' ) g . set_xticklabels ( [ 'Sincere' , 'Insincere' ] ) g . set_yticklabels ( [ ] )
1209	import random index = random . sample ( raw_data . index [ raw_data . target == 1 ] . tolist ( ) , 5 ) for i in index : print ( raw_data . iloc [ i , 1 ] )
1210	nlp = spacy . load ( 'en' ) df [ 'tokens' ] = [ nlp ( text , disable = [ 'ner' , 'tagger' , 'textcat' , ] ) for text in df . question_text ] df . sample ( 5 )
1211	print ( list ( df . iloc [ 0 , 3 ] . sents ) ) sents = [ list ( x . sents ) for x in df . tokens ] df [ 'num_sents' ] = [ len ( sent ) for sent in sents ] df . sample ( 5 )
1212	insincere_text = [ text for text in df [ df [ 'target' ] == 1 ] [ 'question_text' ] ] insincere_clean = cleanup_text ( insincere_text ) insincere_clean = ' ' . join ( insincere_clean ) . split ( )
1213	sincere_text = [ text for text in df [ df [ 'target' ] == 0 ] [ 'question_text' ] ] sincere_clean = cleanup_text ( sincere_text ) sincere_clean = ' ' . join ( sincere_clean ) . split ( )
1214	from sklearn . metrics import roc_auc_score def auc_score ( preds , targets ) : return torch . tensor ( roc_auc_score ( targets , preds [ : , 1 ] ) )
1215	x = hits_df . x . values y = hits_df . y . values z = hits_df . z . values r = np . sqrt ( x ** 2 + y ** 2 + z ** 2 ) x2 = x / r y2 = y / r r2 = np . sqrt ( x ** 2 + y ** 2 ) z2 = z / r2
1216	if x > 0 : return atan ( y / x ) else : return pi - atan ( y / x )
1217	labels_df = pd . read_json ( os . path . join ( path , 'train_sample_videos/metadata.json' ) ) labels_df = labels_df . T print ( labels_df . shape ) labels_df . head ( )
1218	labels_df = pd . read_json ( os . path . join ( path , 'train_sample_videos/metadata.json' ) ) labels_df = labels_df . T print ( labels_df . shape ) labels_df . head ( )
1219	def get_frame_size ( file ) : cap = cv2 . VideoCapture ( file ) ret , frame = cap . read ( )
1220	def get_video_length ( file ) : cap = cv2 . VideoCapture ( file ) fps = cap . get ( cv2 . CAP_PROP_FPS ) frame_count = int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT ) ) duration = frame_count / fps cap . release ( ) return round ( fps ) , round ( duration )
1221	import numpy as np import pandas as pd import matplotlib . pyplot as plt import plotly . offline as py py . init_notebook_mode ( connected = True ) import plotly . tools as tls import warnings import seaborn as sns plt . style . use ( 'fivethirtyeight' ) from collections import Counter warnings . filterwarnings ( 'ignore' ) import plotly . graph_objs as go import plotly . tools as tls import plotly . plotly as plpl
1222	import plotly . tools as tls import warnings import seaborn as sns plt . style . use ( 'fivethirtyeight' ) from collections import Counter warnings . filterwarnings ( 'ignore' ) import plotly . graph_objs as go import plotly . tools as tls import plotly . plotly as plpl
1223	bin_col = [ col for col in train . columns if '_bin' in col ] zeros = [ ] ones = [ ] for col in bin_col : zeros . append ( ( train [ col ] == 0 ) . sum ( ) ) ones . append ( ( train [ col ] == 1 ) . sum ( ) )
1224	num_folds = 10 seed = 8 scoring = 'accuracy' X = train . drop ( [ 'id' , 'target' ] , axis = 1 ) Y = train . target validation_size = 0.3 X_train , X_validation , Y_train , Y_validation = train_test_split ( X , Y , test_size = validation_size , random_state = seed )
1225	x = torch . from_numpy ( inp2img ( sample [ "input" ] ) ) . unsqueeze ( 0 ) . float ( ) . to ( device ) y = torch . tensor ( sample [ "output" ] ) . long ( ) . unsqueeze ( 0 ) . to ( device ) y_pred = model ( x , num_steps ) loss += criterion ( y_pred , y )
1226	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'yaw' ] , color = 'darkgreen' , ax = ax ) . set_title ( 'yaw' , fontsize = 16 ) plt . xlabel ( 'yaw' , fontsize = 15 ) plt . show ( )
1227	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "class_name" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' ) , palette = [ 'navy' , 'darkblue' , 'blue' , 'dodgerblue' , 'skyblue' , 'lightblue' ] ) . set_title ( 'Object Frequencies' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xlabel ( "Count" , fontsize = 15 ) plt . ylabel ( "Class Name" , fontsize = 15 ) plt . show ( plot )
1228	assert points . shape [ 0 ] == self . nbr_dims ( ) , ( "Error: Pointcloud points must have format: %d x n" % self . nbr_dims ( ) ) self . points = points @ staticmethod @ abstractmethod def nbr_dims ( ) -> int :
1229	pass @ classmethod @ abstractmethod def from_file ( cls , file_name : str ) -> "PointCloud" :
1230	ref_sd_token = sample_rec [ "data" ] [ ref_chan ] ref_sd_rec = lyftd . get ( "sample_data" , ref_sd_token ) ref_pose_rec = lyftd . get ( "ego_pose" , ref_sd_rec [ "ego_pose_token" ] ) ref_cs_rec = lyftd . get ( "calibrated_sensor" , ref_sd_rec [ "calibrated_sensor_token" ] ) ref_time = 1e-6 * ref_sd_rec [ "timestamp" ]
1231	sample_data_token = sample_rec [ "data" ] [ chan ] current_sd_rec = lyftd . get ( "sample_data" , sample_data_token ) for _ in range ( num_sweeps ) :
1232	current_pose_rec = lyftd . get ( "ego_pose" , current_sd_rec [ "ego_pose_token" ] ) global_from_car = transform_matrix ( current_pose_rec [ "translation" ] , Quaternion ( current_pose_rec [ "rotation" ] ) , inverse = False )
1233	current_cs_rec = lyftd . get ( "calibrated_sensor" , current_sd_rec [ "calibrated_sensor_token" ] ) car_from_current = transform_matrix ( current_cs_rec [ "translation" ] , Quaternion ( current_cs_rec [ "rotation" ] ) , inverse = False )
1234	current_pc . remove_close ( min_distance ) time_lag = ref_time - 1e-6 * current_sd_rec [ "timestamp" ] times = time_lag * np . ones ( ( 1 , current_pc . nbr_points ( ) ) ) all_times = np . hstack ( ( all_times , times ) )
1235	if current_sd_rec [ "prev" ] == "" : break else : current_sd_rec = lyftd . get ( "sample_data" , current_sd_rec [ "prev" ] ) return all_pc , all_times def nbr_points ( self ) -> int :
1236	x_filt = np . abs ( self . points [ 0 , : ] ) < radius y_filt = np . abs ( self . points [ 1 , : ] ) < radius not_close = np . logical_not ( np . logical_and ( x_filt , y_filt ) ) self . points = self . points [ : , not_close ] def translate ( self , x : np . ndarray ) -> None :
1237	return 4 @ classmethod def from_file ( cls , file_name : Path ) -> "LidarPointCloud" :
1238	return 18 @ classmethod def from_file ( cls , file_name : Path , invalid_states : List [ int ] = None , dynprop_states : List [ int ] = None , ambig_states : List [ int ] = None , ) -> "RadarPointCloud" :
1239	unpacking_lut = { "F" : { 2 : "e" , 4 : "f" , 8 : "d" } , "I" : { 1 : "b" , 2 : "h" , 4 : "i" , 8 : "q" } , "U" : { 1 : "B" , 2 : "H" , 4 : "I" , 8 : "Q" } , } types_str = "" . join ( [ unpacking_lut [ t ] [ int ( s ) ] for t , s in zip ( types , sizes ) ] )
1240	offset = 0 point_count = width points = [ ] for i in range ( point_count ) : point = [ ] for p in range ( feature_count ) : start_p = offset end_p = start_p + int ( sizes [ p ] ) assert end_p < len ( data_binary ) point_p = struct . unpack ( types_str [ p ] , data_binary [ start_p : end_p ] ) [ 0 ] point . append ( point_p ) offset = end_p points . append ( point )
1241	point = np . array ( points [ 0 ] ) if np . any ( np . isnan ( point ) ) : return cls ( np . zeros ( ( feature_count , 0 ) ) )
1242	invalid_states = cls . invalid_states if invalid_states is None else invalid_states dynprop_states = cls . dynprop_states if dynprop_states is None else dynprop_states ambig_states = cls . ambig_states if ambig_states is None else ambig_states
1243	return self . corners ( ) [ : , [ 2 , 3 , 7 , 6 ] ] def render ( self , axis : Axes , view : np . ndarray = np . eye ( 3 ) , normalize : bool = False , colors : Tuple = ( "b" , "r" , "k" ) , linewidth : float = 2 , ) :
1244	for i in range ( 4 ) : axis . plot ( [ corners . T [ i ] [ 0 ] , corners . T [ i + 4 ] [ 0 ] ] , [ corners . T [ i ] [ 1 ] , corners . T [ i + 4 ] [ 1 ] ] , color = colors [ 2 ] , linewidth = linewidth , )
1245	for i in range ( 4 ) : cv2 . line ( image , ( int ( corners . T [ i ] [ 0 ] ) , int ( corners . T [ i ] [ 1 ] ) ) , ( int ( corners . T [ i + 4 ] [ 0 ] ) , int ( corners . T [ i + 4 ] [ 1 ] ) ) , colors [ 2 ] [ : : - 1 ] , linewidth , )
1246	self . data_path = Path ( data_path ) . expanduser ( ) . absolute ( ) self . json_path = Path ( json_path ) self . table_names = [ "category" , "attribute" , "visibility" , "instance" , "sensor" , "calibrated_sensor" , "ego_pose" , "log" , "scene" , "sample" , "sample_data" , "sample_annotation" , "map" , ] start_time = time . time ( )
1247	for map_record in self . map : map_record [ "mask" ] = MapMask ( self . data_path / 'train_maps/map_raster_palo_alto.png' , resolution = map_resolution ) if verbose : for table in self . table_names : print ( "{} {}," . format ( len ( getattr ( self , table ) ) , table ) ) print ( "Done loading in {:.1f} seconds.\n======" . format ( time . time ( ) - start_time ) )
1248	with open ( str ( self . json_path . joinpath ( "{}.json" . format ( table_name ) ) ) ) as f : table = json . load ( f ) return table def __make_reverse_index__ ( self , verbose : bool ) -> None :
1249	self . _token2ind = dict ( ) for table in self . table_names : self . _token2ind [ table ] = dict ( ) for ind , member in enumerate ( getattr ( self , table ) ) : self . _token2ind [ table ] [ member [ "token" ] ] = ind
1250	assert table_name in self . table_names , "Table {} not found" . format ( table_name ) return getattr ( self , table_name ) [ self . getind ( table_name , token ) ] def getind ( self , table_name : str , token : str ) -> int :
1251	if selected_anntokens is not None : boxes = list ( map ( self . get_box , selected_anntokens ) ) else : boxes = self . get_boxes ( sample_data_token )
1252	box_list = [ ] for box in boxes : if flat_vehicle_coordinates :
1253	ypr = Quaternion ( pose_record [ "rotation" ] ) . yaw_pitch_roll yaw = ypr [ 0 ] box . translate ( - np . array ( pose_record [ "translation" ] ) ) box . rotate ( Quaternion ( scalar = np . cos ( yaw / 2 ) , vector = [ 0 , 0 , np . sin ( yaw / 2 ) ] ) . inverse ) else :
1254	box . translate ( - np . array ( cs_record [ "translation" ] ) ) box . rotate ( Quaternion ( cs_record [ "rotation" ] ) . inverse ) if sensor_record [ "modality" ] == "camera" and not box_in_image ( box , cam_intrinsic , imsize , vis_level = box_vis_level ) : continue box_list . append ( box ) return data_path , box_list , cam_intrinsic def get_box ( self , sample_annotation_token : str ) -> Box :
1255	record = self . get ( "sample_annotation" , sample_annotation_token ) return Box ( record [ "translation" ] , record [ "size" ] , Quaternion ( record [ "rotation" ] ) , name = record [ "category_name" ] , token = record [ "token" ] , ) def get_boxes ( self , sample_data_token : str ) -> List [ Box ] :
1256	box = self . get_box ( curr_ann_rec [ "token" ] ) boxes . append ( box ) return boxes def box_velocity ( self , sample_annotation_token : str , max_time_diff : float = 1.5 ) -> np . ndarray :
1257	def __init__ ( self , lyftd : LyftDataset ) : self . lyftd = lyftd @ staticmethod def get_color ( category_name : str ) -> Tuple [ int , int , int ] :
1258	categories = dict ( ) for record in self . lyftd . sample_annotation : if record [ "category_name" ] not in categories : categories [ record [ "category_name" ] ] = [ ] categories [ record [ "category_name" ] ] . append ( record [ "size" ] + [ record [ "size" ] [ 1 ] / record [ "size" ] [ 0 ] ] )
1259	attribute_counts = dict ( ) for record in self . lyftd . sample_annotation : for attribute_token in record [ "attribute_tokens" ] : att_name = self . lyftd . get ( "attribute" , attribute_token ) [ "name" ] if att_name not in attribute_counts : attribute_counts [ att_name ] = 0 attribute_counts [ att_name ] += 1 for name , count in sorted ( attribute_counts . items ( ) ) : print ( "{}: {}" . format ( name , count ) ) def list_scenes ( self ) -> None :
1260	radar_data = { } nonradar_data = { } for channel , token in record [ "data" ] . items ( ) : sd_record = self . lyftd . get ( "sample_data" , token ) sensor_modality = sd_record [ "sensor_modality" ] if sensor_modality in [ "lidar" , "camera" ] : nonradar_data [ channel ] = token else : radar_data [ channel ] = token num_radar_plots = 1 if len ( radar_data ) > 0 else 0
1261	ax = axes [ 0 , 0 ] for i , ( _ , sd_token ) in enumerate ( radar_data . items ( ) ) : self . render_sample_data ( sd_token , with_anns = i == 0 , box_vis_level = box_vis_level , ax = ax , num_sweeps = nsweeps ) ax . set_title ( "Fused RADARs" )
1262	def crop_image ( image : np . array , x_px : int , y_px : int , axes_limit_px : int ) -> np . array : x_min = int ( x_px - axes_limit_px ) x_max = int ( x_px + axes_limit_px ) y_min = int ( y_px - axes_limit_px ) y_max = int ( y_px + axes_limit_px ) cropped_image = image [ y_min : y_max , x_min : x_max ] return cropped_image sd_record = self . lyftd . get ( "sample_data" , sample_data_token )
1263	sd_record = self . lyftd . get ( "sample_data" , sample_data_token ) sensor_modality = sd_record [ "sensor_modality" ] if sensor_modality == "lidar" :
1264	_ , boxes , _ = self . lyftd . get_sample_data ( sample_data_token , box_vis_level = box_vis_level , flat_vehicle_coordinates = True )
1265	sample_rec = self . lyftd . get ( "sample" , sd_record [ "sample_token" ] ) chan = sd_record [ "channel" ] ref_chan = "LIDAR_TOP" pc , times = LidarPointCloud . from_file_multisweep ( self . lyftd , sample_rec , chan , ref_chan , num_sweeps = num_sweeps )
1266	ax . set_xlim ( - axes_limit , axes_limit ) ax . set_ylim ( - axes_limit , axes_limit ) elif sensor_modality == "radar" :
1267	sample_rec = self . lyftd . get ( "sample" , sd_record [ "sample_token" ] ) lidar_token = sample_rec [ "data" ] [ "LIDAR_TOP" ] _ , boxes , _ = self . lyftd . get_sample_data ( lidar_token , box_vis_level = box_vis_level )
1268	chan = sd_record [ "channel" ] ref_chan = "LIDAR_TOP" pc , times = RadarPointCloud . from_file_multisweep ( self . lyftd , sample_rec , chan , ref_chan , num_sweeps = num_sweeps )
1269	ax . set_xlim ( - axes_limit , axes_limit ) ax . set_ylim ( - axes_limit , axes_limit ) elif sensor_modality == "camera" :
1270	data_path , boxes , camera_intrinsic = self . lyftd . get_sample_data ( sample_data_token , box_vis_level = box_vis_level ) data = Image . open ( str ( data_path ) [ : len ( str ( data_path ) ) - 46 ] + 'train_images/' + \ str ( data_path ) [ len ( str ( data_path ) ) - 39 : len ( str ( data_path ) ) ] )
1271	image_path , boxes , camera_intrinsic = self . lyftd . get_sample_data ( sd_rec [ "token" ] , box_vis_level = BoxVisibility . ANY )
1272	scene_rec = self . lyftd . get ( "scene" , scene_token ) sample_rec = self . lyftd . get ( "sample" , scene_rec [ "first_sample_token" ] ) sd_rec = self . lyftd . get ( "sample_data" , sample_rec [ "data" ] [ channel ] )
1273	name = "{}: {} (Space to pause, ESC to exit)" . format ( scene_rec [ "name" ] , channel ) cv2 . namedWindow ( name ) cv2 . moveWindow ( name , 0 , 0 ) if out_path is not None : fourcc = cv2 . VideoWriter_fourcc ( * "MJPG" ) out = cv2 . VideoWriter ( out_path , fourcc , freq , image_size ) else : out = None has_more_frames = True while has_more_frames :
1274	image_path , boxes , camera_intrinsic = self . lyftd . get_sample_data ( sd_rec [ "token" ] , box_vis_level = BoxVisibility . ANY )
1275	if not image_path . exists ( ) : raise Exception ( "Error: Missing image %s" % image_path ) image = cv2 . imread ( str ( image_path ) ) for box in boxes : c = self . get_color ( box . name ) box . render_cv2 ( image , view = camera_intrinsic , normalize = True , colors = ( c , c , c ) )
1276	scene_record = self . lyftd . get ( "scene" , scene_token ) log_record = self . lyftd . get ( "log" , scene_record [ "log_token" ] ) map_record = self . lyftd . get ( "map" , log_record [ "map_token" ] ) map_mask = map_record [ "mask" ]
1277	sample_tokens = self . lyftd . field2token ( "sample" , "scene_token" , scene_token ) for sample_token in sample_tokens : sample_record = self . lyftd . get ( "sample" , sample_token )
1278	map_poses . append ( np . concatenate ( map_mask . to_pixel_coords ( pose_record [ "translation" ] [ 0 ] , pose_record [ "translation" ] [ 1 ] ) ) )
1279	print ( "Creating plot..." ) map_poses = np . vstack ( map_poses ) dists = sklearn . metrics . pairwise . euclidean_distances ( map_poses * map_mask . resolution ) close_poses = np . sum ( dists < close_dist , axis = 0 ) if len ( np . array ( map_mask . mask ( ) ) . shape ) == 3 and np . array ( map_mask . mask ( ) ) . shape [ 2 ] == 3 :
1280	def render_scene ( index ) : my_scene = lyft_dataset . scene [ index ] my_sample_token = my_scene [ "first_sample_token" ] lyft_dataset . render_sample ( my_sample_token )
1281	sensor_channel = 'CAM_BACK' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
1282	my_scene = lyft_dataset . scene [ 0 ] my_sample_token = my_scene [ "first_sample_token" ] my_sample = lyft_dataset . get ( 'sample' , my_sample_token ) lyft_dataset . render_sample_data ( my_sample [ 'data' ] [ 'LIDAR_TOP' ] , nsweeps = 5 )
1283	test [ 'batch' ] = 0 for i in range ( 0 , test . shape [ 0 ] // ROW_PER_BATCH ) : test . iloc [ i * ROW_PER_BATCH : ( i + 1 ) * ROW_PER_BATCH , 2 ] = i
1284	plt . figure ( figsize = ( 20 , 5 ) ) plt . plot ( train . signal [ 500000 : 1000000 ] [ : : 100 ] ) plt . show ( )
1285	metrics_dict = { 'f1score' : { 'lgb_metric_name' : lgb_Metric , } } result_dict = { }
1286	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from kmodes . kmodes import KModes from sklearn import preprocessing from sklearn . decomposition import PCA pd . set_option ( 'mode.chained_assignment' , None )
1287	train_nom = train [ nom_features ] for col in nom_features : le = preprocessing . LabelEncoder ( ) train_nom [ col ] = le . fit_transform ( train_nom [ col ] ) train_nom . head ( )
1288	b , a = butter ( order , normal_cutoff , btype = 'low' , analog = False ) y = filtfilt ( b , a , data ) return y def butter_highpass_filter ( data , cutoff , fs , order ) : normal_cutoff = cutoff / nyq
1289	b , a = butter ( order , normal_cutoff , btype = 'high' , analog = False ) y = filtfilt ( b , a , data ) return y def butter_bandpass_filter ( data , cutoff_low , cuttoff_high , fs , order ) : normal_cutoff_low = cutoff_low / nyq normal_cutoff_high = cutoff_high / nyq
1290	b , a = butter ( order , [ normal_cutoff_low , normal_cutoff_high ] , btype = 'band' , analog = False ) y = filtfilt ( b , a , data ) return y def signaltonoise ( a , axis = 0 , ddof = 0 ) : a = np . asanyarray ( a ) m = a . mean ( axis ) sd = a . std ( axis = axis , ddof = ddof ) return np . where ( sd == 0 , 0 , m / sd )
1291	b , a = butter ( order , lpf_cutoff / nyq , btype = 'low' , analog = False ) w , h = freqz ( b , a , fs = fs ) plt . figure ( figsize = ( 16 , 8 ) ) ; plt . plot ( w , 20 * np . log10 ( abs ( h ) ) , 'b' ) plt . ylabel ( 'Amplitude [dB]' , color = 'b' ) plt . xlabel ( 'Frequency [Hz]' ) plt . title ( 'Low-pass Butterworth Filter, cutoff @ 600Hz' )
1292	metrics_dict = { 'f1score' : { 'lgb_metric_name' : lgb_Metric , } } result_dict = { }
1293	observation_covariance = damping initial_value_guess = observations [ 0 ] transition_matrix = 1 transition_covariance = 0.1 initial_value_guess kf = KalmanFilter ( initial_state_mean = initial_value_guess , initial_state_covariance = observation_covariance , observation_covariance = observation_covariance , transition_covariance = transition_covariance , transition_matrices = transition_matrix ) pred_state , state_cov = kf . smooth ( observations ) return pred_state
1294	metrics_dict = { 'f1score' : { 'lgb_metric_name' : lgb_Metric , } } result_dict = { }
1295	observation_covariance = damping initial_value_guess = observations [ 0 ] transition_matrix = 1 transition_covariance = 0.1 initial_value_guess kf = KalmanFilter ( initial_state_mean = initial_value_guess , initial_state_covariance = observation_covariance , observation_covariance = observation_covariance , transition_covariance = transition_covariance , transition_matrices = transition_matrix ) pred_state , state_cov = kf . smooth ( observations ) return pred_state
1296	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) from time import time from tqdm import tqdm_notebook as tqdm from collections import Counter from scipy import stats import lightgbm as lgb from sklearn . metrics import cohen_kappa_score from sklearn . model_selection import KFold , StratifiedKFold import gc import json pd . set_option ( 'display.max_columns' , 1000 )
1297	from time import time from tqdm import tqdm_notebook as tqdm from collections import Counter from scipy import stats import lightgbm as lgb from sklearn . metrics import cohen_kappa_score from sklearn . model_selection import KFold , StratifiedKFold import gc import json pd . set_option ( 'display.max_columns' , 1000 )
1298	train [ 'timestamp' ] = pd . to_datetime ( train [ 'timestamp' ] ) test [ 'timestamp' ] = pd . to_datetime ( test [ 'timestamp' ] ) return train , test , train_labels , win_code , list_of_user_activities , list_of_event_code , activities_labels , assess_titles , list_of_event_id , all_title_event_code
1299	session_type = session [ 'type' ] . iloc [ 0 ] session_title = session [ 'title' ] . iloc [ 0 ] session_title_text = activities_labels [ session_title ]
1300	if durations == [ ] : features [ 'duration_mean' ] = 0 else : features [ 'duration_mean' ] = np . mean ( durations ) durations . append ( ( session . iloc [ - 1 , 2 ] - session . iloc [ 0 , 2 ] ) . seconds )
1301	features [ 'accumulated_accuracy' ] = accumulated_accuracy / counter if counter > 0 else 0 accuracy = true_attempts / ( true_attempts + false_attempts ) if ( true_attempts + false_attempts ) != 0 else 0 accumulated_accuracy += accuracy last_accuracy_title [ 'acc_' + session_title_text ] = accuracy
1302	def preprocess ( reduce_train , reduce_test ) : for df in [ reduce_train , reduce_test ] : df [ 'installation_session_count' ] = df . groupby ( [ 'installation_id' ] ) [ 'Clip' ] . transform ( 'count' ) df [ 'installation_duration_mean' ] = df . groupby ( [ 'installation_id' ] ) [ 'duration_mean' ] . transform ( 'mean' )
1303	cities = pd . read_csv ( '../input/WCities.csv' ) gamecities = pd . read_csv ( '../input/WGameCities.csv' ) tourneycompactresults = pd . read_csv ( '../input/WNCAATourneyCompactResults.csv' ) tourneyseeds = pd . read_csv ( '../input/WNCAATourneySeeds.csv' ) tourneyslots = pd . read_csv ( '../input/WNCAATourneySlots.csv' ) regseasoncompactresults = pd . read_csv ( '../input/WRegularSeasonCompactResults.csv' ) seasons = pd . read_csv ( '../input/WSeasons.csv' ) teamspellings = pd . read_csv ( '../input/WTeamSpellings.csv' , engine = 'python' ) teams = pd . read_csv ( '../input/WTeams.csv' )
1304	averageseed = tourneyseeds . groupby ( [ 'TeamID' ] ) . agg ( np . mean ) . sort_values ( 'SeedNumber' ) averageseed = averageseed . merge ( teams , left_index = True , right_on = 'TeamID' ) averageseed . head ( 20 ) . plot ( x = 'TeamName' , y = 'SeedNumber' , kind = 'bar' , figsize = ( 15 , 5 ) , title = 'Top 20 Average Tournament Seed' ) ;
1305	train2 = train_GMM [ train_GMM [ 'wheezy-copper-turtle-magic' ] == i ] test2 = test_GMM [ test_GMM [ 'wheezy-copper-turtle-magic' ] == i ] idx1 = train2 . index ; idx2 = test2 . index train2 . reset_index ( drop = True , inplace = True )
1306	auc = roc_auc_score ( train_GMM [ 'target' ] , oof_GMM ) print ( 'QDA scores CV =' , round ( auc , 5 ) ) return oof_GMM , preds_GMM
1307	def gini ( y , pred ) : fpr , tpr , thr = metrics . roc_curve ( y , pred , pos_label = 1 ) g = 2 * metrics . auc ( fpr , tpr ) - 1 return g
1308	plt . figure ( figsize = ( 20 , 10 ) ) train , test = plt . hist ( np . ceil ( train_trans [ 'TransactionDT' ] / 86400 ) , bins = 182 ) , plt . hist ( np . ceil ( test_trans [ 'TransactionDT' ] / 86400 ) , bins = 182 )
1309	gp = train [ all_features ] . \ groupby ( spec [ 'groupby' ] ) [ spec [ 'select' ] ] . \ agg ( spec [ 'agg' ] ) . \ reset_index ( ) . \ rename ( index = str , columns = { spec [ 'select' ] : new_feature } ) . astype ( spec [ 'type' ] )
1310	import numpy as np import pandas as pd train = pd . read_csv ( '/kaggle/input/covid19-global-forecasting-week-1/train.csv' ) test = pd . read_csv ( '/kaggle/input/covid19-global-forecasting-week-1/test.csv' )
1311	valid = train [ train [ 'Date' ] >= test [ 'Date' ] . min ( ) ] train = train [ train [ 'Date' ] < test [ 'Date' ] . min ( ) ] train . shape , valid . shape
1312	train_agg = train [ [ 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' ] ] . groupby ( [ 'Country_Region' , 'Date' ] , as_index = False ) . agg ( { 'ConfirmedCases' : 'sum' , 'Fatalities' : 'sum' } ) train_agg [ 'Date' ] = pd . to_datetime ( train_agg [ 'Date' ] )
1313	def do_fuzzy_search ( country ) : try : result = pycountry . countries . search_fuzzy ( country ) except Exception : return np . nan else : return result [ 0 ] . alpha_2 train_continent = train_agg
1314	fig = px . line ( train_agg , x = 'Date' , y = 'ConfirmedCases' , color = "Country_Region" , hover_name = "Country_Region" ) fig . update_layout ( autosize = False , width = 1000 , height = 500 , title = 'Confirmed Cases Over Time for Each Country' ) fig . show ( )
1315	fig = px . line ( train_agg , x = 'Date' , y = 'ConfirmedCases' , color = "Country_Region" , hover_name = "Country_Region" ) fig . update_layout ( autosize = False , width = 1000 , height = 500 , title = 'Confirmed Cases Over Time for Each Country' ) fig . show ( )
1316	fig = px . line ( train_agg , x = 'Date' , y = 'Fatalities' , color = "Country_Region" , hover_name = "Country_Region" ) fig . update_layout ( autosize = False , width = 1000 , height = 500 , title = 'Fatalities Over Time for Each Country' ) fig . show ( )
1317	import geopandas as gpd shapefile = '/kaggle/input/natural-earth-maps/ne_110m_admin_0_countries.shp' gdf = gpd . read_file ( shapefile ) gdf = gdf . drop ( gdf . index [ 159 ] )
1318	def do_fuzzy_search ( country ) : try : result = pycountry . countries . search_fuzzy ( country ) except Exception : return np . nan else : return result [ 0 ] . alpha_3
1319	import seaborn as sns sns . regplot ( x = 'hits' , y = 'ConfirmedCases_log10' , data = cc_google , scatter_kws = { 's' : 25 } , fit_reg = True , line_kws = { "color" : "black" } )
1320	popCountries = pc_cc . merge ( pc_h , left_on = [ 'Country_Region' , 'Date' ] , right_on = [ 'Country_Region' , 'Date' ] ) popCountries = popCountries . merge ( pc_f , left_on = [ 'Country_Region' , 'Date' ] , right_on = [ 'Country_Region' , 'Date' ] ) popCountries = popCountries [ [ 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' , 'hits' ] ] popCountries = popCountries . rename ( columns = { 'ConfirmedCases' : 'val1' , 'Fatalities' : 'val2' , 'hits' : 'val3' } )
1321	def replaceVal ( x ) : if x == 1 : val = 'Confirmed Cases' elif x == 2 : val = 'Fatalities' else : val = 'Hits' return val long [ 'CC_F_Hits' ] = long [ 'CC_F_Hits' ] . apply ( replaceVal )
1322	import scipy . stats sns . regplot ( x = 'hits' , y = 'ConfirmedCases' , data = ir , scatter_kws = { 's' : 25 } , fit_reg = True , line_kws = { "color" : "black" } ) scipy . stats . pearsonr ( ir . ConfirmedCases , ir . hits )
1323	ir_val = valid [ valid . Country_Region == 'Iran' ] ir_val . Date = pd . to_datetime ( ir_val . Date ) ir_val . index = ir_val . Date
1324	fitted_series = pd . Series ( fitted , index = index_of_fc ) lower_series = pd . Series ( confint [ : , 0 ] , index = index_of_fc ) upper_series = pd . Series ( confint [ : , 1 ] , index = index_of_fc )
1325	def __init__ ( self , num_channels , num_latents , dead_rate = 100 ) : super ( ) . __init__ ( ) self . num_channels = num_channels self . num_latents = num_latents self . dead_rate = dead_rate self . dictionary = nn . Parameter ( torch . randn ( num_latents , num_channels ) ) self . usage_count = nn . Parameter ( dead_rate * torch . ones ( num_latents ) . long ( ) , requires_grad = False ) self . _last_batch = None def embed ( self , idxs ) :
1326	embedded = embedded . permute ( 0 , 3 , 1 , 2 ) . contiguous ( ) return embedded def forward ( self , inputs ) :
1327	x = F . pad ( x , ( 1 , 2 , 1 , 2 ) ) x = self . conv1 ( x ) x = F . relu ( x ) x = F . pad ( x , ( 1 , 2 , 1 , 2 ) ) x = self . conv2 ( x ) x = x + self . residual1 ( x ) x = x + self . residual2 ( x ) return x class HalfEncoder ( Encoder ) :
1328	x = torch . cat ( [ x , inputs [ 1 ] ] , dim = 1 ) x = self . conv2 ( x ) x = x + self . residual3 ( x ) x = x + self . residual4 ( x ) x = F . relu ( x ) x = self . conv3 ( x ) x = F . relu ( x ) x = self . conv4 ( x ) return x class VQVAE ( nn . Module ) :
1329	def __init__ ( self , encoders , decoders ) : super ( ) . __init__ ( ) assert len ( encoders ) == len ( decoders ) self . encoders = encoders self . decoders = decoders for i , enc in enumerate ( encoders ) : self . add_module ( 'encoder_%d' % i , enc ) for i , dec in enumerate ( decoders ) : self . add_module ( 'decoder_%d' % i , dec ) def forward ( self , inputs , commitment = 0.25 ) :
1330	for enc in self . encoders : enc . vq . revive_dead_entries ( ) def full_reconstructions ( self , inputs ) :
1331	def __init__ ( self , num_channels , num_heads = 8 ) : super ( ) . __init__ ( ) self . attention = MaskedAttention ( num_channels , num_heads = num_heads ) def forward ( self , * images , conds = None ) :
1332	def __init__ ( self , num_channels , num_heads = 8 ) : super ( ) . __init__ ( ) assert not num_channels % num_heads , 'heads must evenly divide channels' self . num_channels = num_channels self . num_heads = num_heads self . kqv_projection = nn . Linear ( num_channels , num_channels * 3 ) self . mix_heads = nn . Linear ( num_channels , num_channels ) def forward ( self , sequence ) :
1333	import torch import torch . nn as nn import torch . nn . functional as F class PixelCNN ( nn . Module ) :
1334	def __init__ ( self , * layers ) : super ( ) . __init__ ( ) for i , layer in enumerate ( layers ) : self . add_module ( 'layer_%d' % i , layer ) self . layers = layers def forward ( self , images , conds = None ) :
1335	outputs = self . layers [ 0 ] ( images , conds = conds ) for layer in self . layers [ 1 : ] : outputs = layer ( * outputs , conds = conds ) return outputs class PixelConv ( nn . Module ) :
1336	import torch import torch . nn as nn import torch . nn . functional as F def make_vae ( ) :
1337	DATA_PATH = "../input/santander-customer-transaction-prediction/" train = pd . read_csv ( str ( Path ( DATA_PATH ) / "train.csv" ) ) test = pd . read_csv ( str ( Path ( DATA_PATH ) / "test.csv" ) ) print ( "Train and test shapes" , train . shape , test . shape )
1338	from sklearn import metrics def gini_xgb ( preds , dtrain ) : labels = dtrain . get_label ( ) gini_score = gini_normalizedc ( labels , preds ) return [ ( 'gini' , gini_score ) ] def gini_lgb ( actuals , preds ) : return 'gini' , gini_normalizedc ( actuals , preds ) , True gini_sklearn = metrics . make_scorer ( gini_normalizedc , True , True )
1339	new = df . Image_Label . str . split ( '_' , expand = True ) . rename ( columns = { 0 : 'id' , 1 : 'labels' } ) df [ 'id' ] = new [ 'id' ] df [ 'labels' ] = new [ 'labels' ] df . head ( )
1340	def __init__ ( self ) : self . coef_ = 0 def _f1_loss ( self , coef , X , y ) :
1341	valid = - np . ones ( ( batch_size , 1 ) ) fake = np . ones ( ( batch_size , 1 ) ) for epoch in range ( epochs ) : for _ in range ( n_critic ) :
1342	d_loss_real = critic . train_on_batch ( imgs , valid ) d_loss_fake = critic . train_on_batch ( gen_imgs , fake ) d_loss = 0.5 * np . add ( d_loss_fake , d_loss_real )
1343	for l in critic . layers : weights = l . get_weights ( ) weights = [ np . clip ( w , - clip_value , clip_value ) for w in weights ] l . set_weights ( weights )
1344	if epoch % 1000 == 0 : print ( "%d [D loss: %f] [G loss: %f]" % ( epoch , 1 - d_loss [ 0 ] , 1 - g_loss [ 0 ] ) ) generator . save ( './generator_%d.h5' % ( epoch ) ) generator . save_weights ( './generator_weights_%d.h5' % ( epoch ) ) critic . save ( './critic_%d.h5' % ( epoch ) ) critic . save_weights ( './critic_weights_%d.h5' % ( epoch ) )
1345	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os print ( os . listdir ( "../input" ) ) from PIL import Image import random from tqdm import tqdm_notebook
1346	punctuations = string . punctuation stopwords = list ( STOP_WORDS ) parser = English ( ) def spacy_tokenizer ( sentence ) : mytokens = parser ( sentence ) mytokens = [ word . lemma_ . lower ( ) . strip ( ) if word . lemma_ != "-PRON-" else word . lower_ for word in mytokens ] mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] mytokens = " " . join ( [ i for i in mytokens ] ) return mytokens
1347	fre_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( textstat . flesch_reading_ease ) ) fre_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( textstat . flesch_reading_ease ) ) plot_readability ( fre_sincere , fre_insincere , "Flesch Reading Ease" , 20 )
1348	def consensus_all ( text ) : return textstat . text_standard ( text , float_output = True ) con_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( consensus_all ) ) con_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( consensus_all ) ) plot_readability ( con_sincere , con_insincere , "Readability Consensus based upon all the above tests" , 2 )
1349	vectorizer_sincere = CountVectorizer ( min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}' ) sincere_questions_vectorized = vectorizer_sincere . fit_transform ( sincere_questions ) vectorizer_insincere = CountVectorizer ( min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}' ) insincere_questions_vectorized = vectorizer_insincere . fit_transform ( insincere_questions )
1350	lda_sincere = LatentDirichletAllocation ( n_components = 10 , max_iter = 5 , learning_method = 'online' , verbose = True ) sincere_lda = lda_sincere . fit_transform ( sincere_questions_vectorized ) lda_insincere = LatentDirichletAllocation ( n_components = 10 , max_iter = 5 , learning_method = 'online' , verbose = True ) insincere_lda = lda_insincere . fit_transform ( insincere_questions_vectorized )
1351	def selected_topics ( model , vectorizer , top_n = 10 ) : for idx , topic in enumerate ( model . components_ ) : print ( "Topic %d:" % ( idx ) ) print ( [ ( vectorizer . get_feature_names ( ) [ i ] , topic [ i ] ) for i in topic . argsort ( ) [ : - top_n - 1 : - 1 ] ] )
1352	pyLDAvis . enable_notebook ( ) dash = pyLDAvis . sklearn . prepare ( lda_sincere , sincere_questions_vectorized , vectorizer_sincere , mds = 'tsne' ) dash
1353	pyLDAvis . enable_notebook ( ) dash = pyLDAvis . sklearn . prepare ( lda_insincere , insincere_questions_vectorized , vectorizer_insincere , mds = 'tsne' ) dash
1354	aldfly = '../input/birdsong-recognition/train_audio/aldfly/XC134874.mp3' y , sr = librosa . load ( aldfly , sr = None ) ipd . Audio ( aldfly )
1355	sample_limit = 5000 with tqdm ( total = sample_limit ) as pbar : for idx , row in train_df [ : sample_limit ] . iterrows ( ) : pbar . update ( 1 ) audio_file_path = "/kaggle/input/birdsong-recognition/train_audio/" audio_file_path += row . ebird_code samples_df = get_sample ( '{}/{}' . format ( audio_file_path , row . filename ) , row . ebird_code , samples_df )
1356	song_sample = np . array ( sample [ int ( row . seconds - 5 ) * data_point_per_second : int ( row . seconds ) * data_point_per_second ] ) input_data = np . reshape ( np . asarray ( [ song_sample ] ) , ( 1 , sequence_length ) ) . astype ( np . float32 ) prediction = model . predict ( np . array ( [ input_data ] ) ) predicted_bird = id_to_ebird [ np . argmax ( prediction ) ] df . at [ idx , "birds" ] = predicted_bird return df
1357	def clean_filename ( fname , string ) : file_name = fname . split ( '/' ) [ 1 ] if file_name [ : 2 ] == '__' : file_name = string + file_name return file_name
1358	def proba2labels ( preds , i2c , k = 3 ) : ans = [ ] ids = [ ] for p in preds : idx = np . argsort ( p ) [ : : - 1 ] ids . append ( [ i for i in idx [ : k ] ] ) ans . append ( ' ' . join ( [ i2c [ i ] for i in idx [ : k ] ] ) ) return ans , ids
1359	from plotly import tools import plotly . plotly as py from plotly . offline import init_notebook_mode , iplot init_notebook_mode ( connected = True ) import plotly . graph_objs as go import plotly . figure_factory as ff
1360	import spacy from spacy . lang . en . stop_words import STOP_WORDS from spacy . lang . en import English
1361	from keras . preprocessing . text import Tokenizer from keras . preprocessing . sequence import pad_sequences from keras . utils . vis_utils import plot_model from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers import Flatten from keras . layers import Dropout from keras . layers import Embedding from keras . layers import CuDNNLSTM , CuDNNGRU , Bidirectional from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D from keras . layers . merge import concatenate
1362	def create_tokenizer ( lines ) : tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( lines ) return tokenizer
1363	vocab_size = len ( tokenizer . word_index ) + 1 print ( 'Max document length: %d' % length ) print ( 'Vocabulary size: %d' % vocab_size )
1364	kappa = KappaScore ( ) kappa . weights = "quadratic" Path ( '/tmp/.cache/torch/checkpoints/' ) . mkdir ( exist_ok = True , parents = True ) learn = cnn_learner ( data , models . resnet34 , metrics = [ accuracy , kappa ] , pretrained = True , callback_fns = [ partial ( CSVLogger , append = True ) ] , path = '../tmp/model/' ) learn . fit_one_cycle ( 5 , slice ( 0.01 ) )
1365	sample_df = pd . read_csv ( PATH / 'sample_submission.csv' ) learn . data . add_test ( ImageList . from_df ( sample_df , PATH , folder = 'test_images' , suffix = '.png' ) ) preds , y = learn . get_preds ( DatasetType . Test ) sample_df . diagnosis = preds . argmax ( 1 ) sample_df . head ( ) sample_df . to_csv ( 'submission.csv' , index = False )
1366	mis_val_table_ren_columns = mis_val_table_ren_columns [ mis_val_table_ren_columns . iloc [ : , 1 ] != 0 ] . sort_values ( '% of Total Values' , ascending = False ) . round ( 1 )
1367	print ( "Your selected dataframe has " + str ( df . shape [ 1 ] ) + " columns.\n" "There are " + str ( mis_val_table_ren_columns . shape [ 0 ] ) + " columns that have missing values." )
1368	html_string = js_string = from IPython . core . display import display , HTML , Javascript h2 = display ( HTML ( ) ) h = display ( HTML ( html_string ) ) j = IPython . display . Javascript ( js_string ) IPython . display . display_javascript ( j )
1369	fig = plt . figure ( figsize = ( 15 , 10 ) ) columns = 4 ; rows = 5 for i in range ( 1 , columns * rows + 1 ) : ds = pydicom . dcmread ( train_images_dir + train [ train [ 'benign_malignant' ] == 'benign' ] [ 'image_name' ] [ i ] + '.dcm' ) fig . add_subplot ( rows , columns , i ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone ) fig . add_subplot
1370	ds = pydicom . dcmread ( train_images_dir + train [ train [ 'benign_malignant' ] == 'benign' ] [ 'image_name' ] [ i ] + '.dcm' ) fig . add_subplot ( rows , columns , i ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone ) fig . add_subplot
1371	vals = train [ train [ 'benign_malignant' ] == 'malignant' ] [ 'image_name' ] . index . values fig = plt . figure ( figsize = ( 15 , 10 ) ) columns = 4 ; rows = 5 for i in range ( 1 , columns * rows + 1 ) :
1372	ds = pydicom . dcmread ( train_images_dir + train [ train [ 'benign_malignant' ] == 'malignant' ] [ 'image_name' ] [ vals [ i ] ] + '.dcm' ) fig . add_subplot ( rows , columns , i ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone ) fig . add_subplot
1373	sure_fg = np . uint8 ( sure_fg ) unknown = cv2 . subtract ( sure_bg , sure_fg ) i = im // width j = im % width axs [ i , j ] . imshow ( sure_bg , cmap = plt . cm . bone ) axs [ i , j ] . axis ( 'off' ) plt . suptitle ( title ) view_images_aug ( train [ train [ 'diagnosis' ] == 'lentigo NOS' ] [ 'image_name' ] , title = "Lentigo NOS's growth" ) ;
1374	sure_fg = np . uint8 ( sure_fg ) unknown = cv2 . subtract ( sure_bg , sure_fg ) ret , markers = cv2 . connectedComponents ( sure_fg )
1375	class Config : BATCH_SIZE = 8 EPOCHS = 40 WARMUP_EPOCHS = 2 LEARNING_RATE = 1e-4 WARMUP_LEARNING_RATE = 1e-3 HEIGHT = 224 WIDTH = 224 CANAL = 3 N_CLASSES = train [ 'target' ] . nunique ( ) ES_PATIENCE = 5 RLROP_PATIENCE = 3 DECAY_DROP = 0.5
1376	for order in product_list [ 'product_id' ] : for product in order : index = column_position . setdefault ( product , len ( column_position ) ) indices . append ( index ) data . append ( 1 ) indptr . append ( len ( indices ) ) prod_matrix = csr_matrix ( ( data , indices , indptr ) , dtype = int )
1377	def id_values ( row , overlap ) : for key , value in row . items ( ) : if key in overlap : print ( key , value )
1378	def filter_signal ( signal , threshold = 1e8 ) : fourier = rfft ( signal ) frequencies = rfftfreq ( signal . size , d = 1e-5 ) fourier [ frequencies > threshold ] = 0 return irfft ( fourier )
1379	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'dark_background' ) from IPython . display import display from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = "all" import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1380	from IPython . display import display from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = "all"
1381	print ( "Shape of data is " ) train . shape print ( 'The total number of movies are' , train . shape [ 0 ] )
1382	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_year' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Year" , fontsize = 20 ) plt . xlabel ( 'Release Year' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 , rotation = 90 ) plt . show ( )
1383	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . distplot ( train [ 'popularity' ] , kde = False ) plt . title ( "Movie Popularity Count" , fontsize = 20 ) plt . xlabel ( 'Popularity' ) plt . ylabel ( 'Count' ) plt . xticks ( fontsize = 12 , rotation = 90 ) plt . show ( )
1384	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_day' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Day of Month" , fontsize = 20 ) plt . xlabel ( 'Release Day' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 ) plt . show ( )
1385	plt . figure ( figsize = ( 20 , 12 ) ) sns . countplot ( train [ 'release_weekday' ] . sort_values ( ) , palette = 'Dark2' ) loc = np . array ( range ( len ( train [ 'release_weekday' ] . unique ( ) ) ) ) day_labels = [ 'Mon' , 'Tue' , 'Wed' , 'Thu' , 'Fri' , 'Sat' , 'Sun' ] plt . xlabel ( 'Release Day of Week' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( loc , day_labels , fontsize = 12 ) plt . show ( )
1386	def sieve_eratosthenes ( n ) : primes = [ False , False ] + [ True for i in range ( n - 1 ) ] p = 2 while ( p * p <= n ) : if ( primes [ p ] == True ) : for i in range ( p * 2 , n + 1 , p ) : primes [ i ] = False p += 1 return primes
1387	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
1388	def add_lower ( embedding , vocab ) : count = 0 for word in vocab : if word in embedding and word . lower ( ) not in embedding : embedding [ word . lower ( ) ] = embedding [ word ] count += 1 print ( f"Added {count} words to embedding" )
1389	def clean_contractions ( text , mapping ) : specials = [ "’" , "‘" , "´" , "`" ] for s in specials : text = text . replace ( s , "'" ) text = ' ' . join ( [ mapping [ t ] if t in mapping else t for t in text . split ( " " ) ] ) return text
1390	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
1391	def clean_contractions ( text , mapping ) : specials = [ "’" , "‘" , "´" , "`" ] for s in specials : text = text . replace ( s , "'" ) text = ' ' . join ( [ mapping [ t ] if t in mapping else t for t in text . split ( " " ) ] ) return text
1392	def make_treated_data ( X ) : t = Tokenizer ( num_words = len_voc , filters = '' ) t . fit_on_texts ( X ) X = t . texts_to_sequences ( X ) X = pad_sequences ( X , maxlen = max_len ) return X , t . word_index
1393	for wav in pywt . wavelist ( ) : try : filtered = wavelet_denoising ( signal , wavelet = wav , level = 1 ) except : pass plt . figure ( figsize = ( 10 , 6 ) ) plt . plot ( signal , label = 'Raw' ) plt . plot ( filtered , label = 'Filtered' ) plt . legend ( ) plt . title ( f"DWT Denoising with {wav} Wavelet" , size = 15 ) plt . show ( )
1394	plt . figure ( figsize = ( 15 , 10 ) ) sns . countplot ( 'event' , hue = 'seat' , data = train_df ) plt . xlabel ( "Seat and state of the pilot" , fontsize = 12 ) plt . ylabel ( "Count (log)" , fontsize = 12 ) plt . yscale ( 'log' ) plt . title ( "Left seat or right seat ?" , fontsize = 15 ) plt . show ( )
1395	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'time' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Time (s)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Which time do events occur at ?" , fontsize = 15 ) plt . show ( )
1396	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'gsr' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Electrodermal activity measure (µV)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Electrodermal activity influence" , fontsize = 15 ) plt . show ( )
1397	read_args = { 'nrows' : rows , 'parse_dates' : [ 'click_time' ] , 'infer_datetime_format' : True , 'index_col' : 'click_time' , 'usecols' : list ( types . keys ( ) ) , 'dtype' : types , 'engine' : 'c' , 'sep' : ',' }
1398	file_path = '{}{}' . format ( kaggle_path , name ) with open ( '{}.csv' . format ( file_path ) , 'rb' ) as File : data = ( pd . read_csv ( File , ** read_args ) . tz_localize ( 'UTC' ) . tz_convert ( 'Asia/Shanghai' ) . reset_index ( ) )
1399	plt . figure ( figsize = ( 12 , 12 ) ) plt . plot ( ts . rolling ( window = 30 , center = False ) . mean ( ) , label = 'Rolling Mean' ) ; plt . plot ( ts . rolling ( window = 30 , center = False ) . std ( ) , label = 'Rolling sd' ) ; plt . legend ( ) ;
1400	residuals = pd . DataFrame ( model_fit . resid ) fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 12 , 6 ) ) residuals . plot ( ax = axes [ 0 ] ) residuals . plot ( kind = 'kde' , ax = axes [ 1 ] ) ; residuals . describe ( ) . T
1401	def concorde_tsp ( seed = 42 ) : cities = pd . read_csv ( '../input/cities.csv' ) solver = TSPSolver . from_data ( cities . X , cities . Y , norm = "EUC_2D" ) tour_data = solver . solve ( time_bound = 60.0 , verbose = True , random_seed = seed ) if tour_data . found_tour : path = np . append ( tour_data . tour , [ 0 ] ) make_submission ( 'concorde' , path ) return path else : return None path_cc = concorde_tsp ( )
1402	cities = pd . read_csv ( '../input/cities.csv' ) cities [ 'isPrime' ] = cities . CityId . apply ( isprime ) prime_cities = cities . loc [ ( cities . CityId == 0 ) | ( cities . isPrime ) ] solver = TSPSolver . from_data ( prime_cities . X , prime_cities . Y , norm = "EUC_2D" ) tour_data = solver . solve ( time_bound = 5.0 , verbose = True , random_seed = 42 ) prime_path = np . append ( tour_data . tour , [ 0 ] )
1403	from xgboost import XGBRegressor regressor = XGBRegressor ( n_estimators = 300 ) regressor . fit ( X , y )
1404	import glob import matplotlib . pyplot as plt import seaborn as sns import pandas as pd import pydicom import numpy as np import warnings import multiprocessing import os from skimage import morphology from skimage import feature from skimage import measure from skimage import util from skimage import transform warnings . filterwarnings ( 'ignore' )
1405	boxes_per_patient = tr . groupby ( 'patientId' ) [ 'Target' ] . sum ( ) ax = ( boxes_per_patient > 0 ) . value_counts ( ) . plot . bar ( ) _ = ax . set_title ( 'Are the classes imbalanced?' ) _ = ax . set_xlabel ( 'Has Pneumonia' ) _ = ax . set_ylabel ( 'Count' ) _ = ax . xaxis . set_tick_params ( rotation = 0 )
1406	ax = boxes_per_patient . value_counts ( ) . plot . bar ( ) _ = ax . set_title ( 'How many cases are there per image?' ) _ = ax . set_xlabel ( 'Number of cases' ) _ = ax . xaxis . set_tick_params ( rotation = 0 )
1407	centers = ( tr . dropna ( subset = [ 'x' ] ) . assign ( center_x = tr . x + tr . width / 2 , center_y = tr . y + tr . height / 2 ) ) ax = sns . jointplot ( "center_x" , "center_y" , data = centers , height = 9 , alpha = 0.1 ) _ = ax . fig . suptitle ( "Where is Pneumonia located?" , y = 1.01 )
1408	g = sns . FacetGrid ( col = 'Target' , hue = 'gender' , data = tr . drop_duplicates ( subset = [ 'patientId' ] ) , height = 9 , palette = dict ( F = "red" , M = "blue" ) ) _ = g . map ( sns . distplot , 'age' , hist_kws = { 'alpha' : 0.3 } ) . add_legend ( ) _ = g . fig . suptitle ( "What is the age distribution by gender and target?" , y = 1.02 , fontsize = 20 )
1409	areas = tr . dropna ( subset = [ 'area' ] ) g = sns . FacetGrid ( hue = 'gender' , data = areas , height = 9 , palette = dict ( F = "red" , M = "blue" ) , aspect = 1.4 ) _ = g . map ( sns . distplot , 'area' , hist_kws = { 'alpha' : 0.3 } ) . add_legend ( ) _ = g . fig . suptitle ( 'What are the areas of the bounding boxes by gender?' , y = 1.01 )
1410	pixel_vc = tr . drop_duplicates ( 'patientId' ) [ 'pixel_spacing' ] . value_counts ( ) ax = pixel_vc . iloc [ : 6 ] . plot . bar ( ) _ = ax . set_xticklabels ( [ f'{ps:.4f}' for ps in pixel_vc . index [ : 6 ] ] ) _ = ax . set_xlabel ( 'Pixel Spacing' ) _ = ax . set_ylabel ( 'Count' ) _ = ax . set_title ( 'How is the pixel spacing distributed?' , fontsize = 20 )
1411	areas_with_count = areas . merge ( pd . DataFrame ( boxes_per_patient ) . rename ( columns = { 'Target' : 'bbox_count' } ) , on = 'patientId' ) g = sns . FacetGrid ( hue = 'bbox_count' , data = areas_with_count , height = 8 , aspect = 1.4 ) _ = g . map ( sns . distplot , 'area' ) . add_legend ( ) _ = g . fig . suptitle ( "How are the bounding box areas distributed by the number of boxes?" , y = 1.01 )
1412	ax = sns . boxplot ( tr . mean_black_pixels ) _ = ax . set_xlabel ( 'Percentage of black pixels' ) _ = ax . set_title ( 'Are there images with mostly black pixels?' )
1413	ax = sns . distplot ( tr [ 'aspect_ratio' ] . dropna ( ) , norm_hist = True ) _ = ax . set_title ( "What does the distribution of bounding aspect ratios look like?" ) _ = ax . set_xlabel ( "Aspect Ratio" )
1414	from sklearn . discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA ( ) X = lda . fit_transform ( X , y . astype ( int ) ) X_Test = lda . transform ( X_Test )
1415	from sklearn . discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA ( ) X = lda . fit_transform ( X , y . astype ( int ) ) X_Test = lda . transform ( X_Test )
1416	params = { 'boosting_type' : 'gbdt' , 'metric' : 'rmse' , 'zero_as_missing' : True } regressor = lgb . train ( params , train , 3000 , valid_sets = [ test ] , early_stopping_rounds = 100 , verbose_eval = 100 )
1417	return id * 2 + 1 @ classmethod def right_child_id ( cls , id : int ) -> int :
1418	return id * 2 + 2 @ staticmethod cdef DOUBLE_t loss ( DOUBLE_t sum_grad , DOUBLE_t sum_hess ) :
1419	sum_grad , sum_hess = 0.0 , 0.0 for i in range ( n ) : if node_ids_data [ i ] != node_id : continue sum_grad += grad [ i ] sum_hess += hess [ i ]
1420	best_gain , best_feature_id , best_feature_value = 0.0 , 0 , - np . inf best_left_weight , best_right_weight = node . weight , 0.0 if sum_hess > 0 : sum_loss = TreeUtil . loss ( sum_grad , sum_hess ) else : sum_loss = 0.0
1421	for feature_id in range ( data . values . shape [ 1 ] ) : prev_value = - np . inf left_grad , left_hess = 0.0 , 0.0 sorted_index = sorted_indexes [ : , feature_id ]
1422	if value != prev_value and left_hess > 0 and ( sum_hess - left_hess ) > 0 : right_grad = sum_grad - left_grad right_hess = sum_hess - left_hess left_loss = TreeUtil . loss ( left_grad , left_hess ) right_loss = TreeUtil . loss ( right_grad , right_hess ) gain = sum_loss - ( left_loss + right_loss )
1423	for i in range ( len ( node_ids_depth ) ) : node_id = node_ids_depth [ i ] feature_id = feature_ids [ i ] feature_value = feature_values [ i ] left_weight = left_weights [ i ] right_weight = right_weights [ i ]
1424	node = self . nodes [ node_id ] node . feature_id = feature_id node . feature_value = feature_value
1425	left_node = Node ( TreeUtil . left_child_id ( node_id ) , left_weight ) right_node = Node ( TreeUtil . right_child_id ( node_id ) , right_weight ) self . nodes += [ left_node , right_node ]
1426	node_id = node_ids_data [ i ] node = self . nodes [ node_id ] feature_id , feature_value = node . feature_id , node . feature_value
1427	node_ids_data = np . zeros ( len ( values ) , dtype = int ) for depth in range ( self . max_depth ) : for i in range ( len ( node_ids_data ) ) :
1428	node_id = node_ids_data [ i ] node = self . nodes [ node_id ] feature_id , feature_value = node . feature_id , node . feature_value
1429	ret = [ ] for depth in range ( self . max_depth + 1 ) : node_ids_depth = TreeUtil . node_ids_depth ( depth ) for node_id in node_ids_depth : node = self . nodes [ node_id ] if node . is_leaf ( ) : ret . append ( f"{node_id}:leaf={node.weight}" ) else : ret . append ( f"{node_id}:[f{node.feature_id}<{node.feature_value}] " + f"yes={TreeUtil.left_child_id(node_id)},no={TreeUtil.right_child_id(node_id)}" ) return "\n" . join ( ret )
1430	return id * 2 + 1 @ classmethod def right_child_id ( cls , id : int ) -> int :
1431	node = Node ( 0 , 0.0 ) self . nodes . append ( node ) def construct ( self , data : Data , grad : np . ndarray , hess : np . ndarray ) :
1432	sum_grad , sum_hess = 0.0 , 0.0 for i in range ( n ) : if node_ids_data [ i ] != node_id : continue sum_grad += grad [ i ] sum_hess += hess [ i ]
1433	right_grad = sum_grad - left_grad right_hess = sum_hess - left_hess left_loss = TreeUtil . loss ( left_grad , left_hess ) right_loss = TreeUtil . loss ( right_grad , right_hess ) if left_loss is not None and right_loss is not None : gain = sum_loss - ( left_loss + right_loss )
1434	if gain > best_gain : best_gain = gain best_feature_id = feature_id best_feature_value = value best_left_weight = TreeUtil . weight ( left_grad , left_hess ) best_right_weight = TreeUtil . weight ( right_grad , right_hess ) prev_value = value left_grad += grad [ i ] left_hess += hess [ i ]
1435	for i in range ( len ( node_ids_depth ) ) : node_id = node_ids_depth [ i ] feature_id = feature_ids [ i ] feature_value = feature_values [ i ] left_weight = left_weights [ i ] right_weight = right_weights [ i ]
1436	node = self . nodes [ node_id ] node . feature_id = feature_id node . feature_value = feature_value
1437	left_node = Node ( TreeUtil . left_child_id ( node_id ) , left_weight ) right_node = Node ( TreeUtil . right_child_id ( node_id ) , right_weight ) self . nodes += [ left_node , right_node ]
1438	node_id = node_ids_data [ i ] node = self . nodes [ node_id ] feature_id , feature_value = node . feature_id , node . feature_value
1439	node_ids_data = np . zeros ( len ( values ) , dtype = int ) for depth in range ( self . max_depth ) : for i in range ( len ( node_ids_data ) ) :
1440	node_id = node_ids_data [ i ] node = self . nodes [ node_id ] feature_id , feature_value = node . feature_id , node . feature_value
1441	ret = [ ] for depth in range ( self . max_depth + 1 ) : node_ids_depth = TreeUtil . node_ids_depth ( depth ) for node_id in node_ids_depth : node = self . nodes [ node_id ] if node . is_leaf ( ) : ret . append ( f"{node_id}:leaf={node.weight}" ) else : ret . append ( f"{node_id}:[f{node.feature_id}<{node.feature_value}] " + f"yes={TreeUtil.left_child_id(node_id)},no={TreeUtil.right_child_id(node_id)}" ) return "\n" . join ( ret )
1442	offset_index = index - len ( self . train_df ) image_id = self . _get_row_id ( self . test_df , offset_index ) img = np . concatenate ( [ self . _load_image ( TEST_PATH / ( image_id + "_" + i + ".png" ) , size ) for i in FILTERS ] , axis = 2 ) label = 0 else :
1443	image_id = self . _get_row_id ( self . train_df , index ) img = np . concatenate ( [ self . _load_image ( TRAIN_PATH / ( image_id + "_" + i + ".png" ) , size ) for i in FILTERS ] , axis = 2 ) label = 1 img = np . transpose ( img , axes = [ 2 , 0 , 1 ] ) if self . augment is not None : img = self . augment ( img ) return img , label def __len__ ( self ) : return self . total_length
1444	import seaborn as sns fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( train_correlations , annot = True , vmin = - 0.23 , vmax = 0.23 , center = 0.0 , ax = ax )
1445	import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import numpy as np from scipy . stats import norm from sklearn . preprocessing import StandardScaler from scipy import stats import warnings warnings . filterwarnings ( 'ignore' ) import gc from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error import lightgbm as lgb import xgboost as xgb from scipy . optimize import minimize
1446	final_prediction = 0 for weight , prediction in zip ( weights , preds ) : final_prediction += weight * prediction return np . sqrt ( mean_squared_error ( final_prediction , target ) )
1447	def __init__ ( self ) : self . state_shape = None self . action_shape = None self . memory = { } def convert_obs ( self , uid , obs ) : return np . array ( [ 0 ] ) def convert_action ( self , uid , obs , action ) : return 'NONE' def reset ( self ) : pass def step ( self ) : pass class BasicShipyardHelper ( Helper ) :
1448	import os from copy import deepcopy from kaggle_environments import make class HaliteEnv : def __init__ ( self , opponents , shipyard_helper , ship_helper , replay_save_dir = '' , ** kwargs ) : self . shipyard_helper = shipyard_helper self . ship_helper = ship_helper self . env = make ( 'halite' , ** kwargs ) self . trainer = self . env . train ( [ None , * opponents ] ) self . replay_save_dir = replay_save_dir def update_obs ( self , uid , action ) :
1449	self . obs = deepcopy ( self . trainer . reset ( ) ) return self . obs def step ( self , actions ) :
1450	self . obs , reward , terminal , info = self . trainer . step ( actions ) self . obs = deepcopy ( self . obs ) return self . obs , reward , terminal def play_episdoe ( self , shipyard_agent , ship_agent , max_steps = 400 ) :
1451	self . shipyard_helper . reset ( ) self . ship_helper . reset ( ) obs = self . reset ( ) states = { } total_reward = 0 for step in range ( 1 , max_steps + 1 ) : actions = { } phalite , shipyards , ships = obs . players [ obs . player ] init_shipyards = deepcopy ( shipyards ) init_ships = deepcopy ( ships ) for uid in init_shipyards . keys ( ) :
1452	log_file = open ( 'Porto-AUC-5fold-XGB-run-01-v1-full.log' , 'a' ) AUCbest = - 1. ITERbest = 0
1453	print ( '\n Number of selected features:' ) print ( boruta_selector . n_features_ ) feature_df = pd . DataFrame ( train . drop ( [ 'id' , 'target' ] , axis = 1 ) . columns . tolist ( ) , columns = [ 'features' ] ) feature_df [ 'rank' ] = boruta_selector . ranking_ feature_df = feature_df . sort_values ( 'rank' , ascending = True ) . reset_index ( drop = True ) print ( '\n Top %d features:' % boruta_selector . n_features_ ) print ( feature_df . head ( boruta_selector . n_features_ ) ) feature_df . to_csv ( 'boruta-feature-ranking.csv' , index = False )
1454	for c in classes : partition = data_classes [ data_classes == c ] proportion = len ( partition ) / N
1455	if features : self . _features = [ f for f in features if f in self . _data_raw . columns ] missing = set ( features ) - set ( self . _features ) if missing : print ( 'WARNING: user-specified features %s not in input dataframe' % str ( missing ) ) else : numeric_cols = self . _data_raw . _data . get_numeric_data ( ) . items self . _features = [ f for f in numeric_cols if f != class_label ]
1456	gain_threshold = ( log ( N - 1 , 2 ) + delta ) / N if cut_point_gain > gain_threshold : return True else : return False def feature_boundary_points ( self , data , feature ) :
1457	data_partition = data . copy ( deep = True ) data_partition . sort_values ( feature , ascending = True , inplace = True ) boundary_points = [ ]
1458	cut_candidate = self . best_cut_point ( data = data_partition , feature = feature ) if cut_candidate == None : return decision = self . MDLPC_criterion ( data = data_partition , feature = feature , cut_point = cut_candidate )
1459	left_partition = data_partition [ data_partition [ feature ] <= cut_candidate ] right_partition = data_partition [ data_partition [ feature ] > cut_candidate ] if left_partition . empty or right_partition . empty : return self . _cuts [ feature ] += [ cut_candidate ] self . single_feature_accepted_cutpoints ( feature = feature , partition_index = left_partition . index ) self . single_feature_accepted_cutpoints ( feature = feature , partition_index = right_partition . index )
1460	self . _cuts [ feature ] = sorted ( self . _cuts [ feature ] ) return def all_features_accepted_cutpoints ( self ) :
1461	if out_bins_path : with open ( out_bins_path , 'w' ) as bins_file : print >> bins_file , 'Description of bins in file: %s' % out_data_path for attr in self . _features : print >> bins_file , 'attr: %s\n\t%s' % ( attr , ', ' . join ( [ pbin_label for pbin_label in pbin_label_collection [ attr ] ] ) )
1462	starttime = timer ( None ) start_time = timer ( None ) rfecv . fit ( X , y ) timer ( start_time )
1463	plt . figure ( figsize = ( 12 , 9 ) ) plt . xlabel ( 'Number of features tested x 2' ) plt . ylabel ( 'Cross-validation score (AUC)' ) plt . plot ( range ( 1 , len ( rfecv . grid_scores_ ) + 1 ) , rfecv . grid_scores_ ) plt . savefig ( 'Porto-RFECV-01.png' , dpi = 150 ) plt . show ( )
1464	ranking = pd . DataFrame ( { 'Features' : all_features } ) ranking [ 'Rank' ] = np . asarray ( rfecv . ranking_ ) ranking . sort_values ( 'Rank' , inplace = True ) ranking . to_csv ( 'Porto-RFECV-ranking-01.csv' , index = False )
1465	params = { 'min_child_weight' : [ 1 , 5 , 10 ] , 'gamma' : [ 0.5 , 1 , 1.5 , 2 , 5 ] , 'subsample' : [ 0.6 , 0.8 , 1.0 ] , 'colsample_bytree' : [ 0.6 , 0.8 , 1.0 ] , 'max_depth' : [ 3 , 4 , 5 ] }
1466	params = { 'min_child_weight' : [ 1 , 5 , 10 ] , 'gamma' : [ 0.5 , 1 , 1.5 , 2 , 5 ] , 'subsample' : [ 0.6 , 0.8 , 1.0 ] , 'colsample_bytree' : [ 0.6 , 0.8 , 1.0 ] , 'max_depth' : [ 3 , 4 , 5 ] }
1467	start_time = timer ( None ) random_search . fit ( X , Y ) timer ( start_time )
1468	if i > 0 : fpred = pred + y_pred avreal = np . concatenate ( ( avreal , y_val ) , axis = 0 ) avpred = np . concatenate ( ( avpred , scores_val ) , axis = 0 ) avids = np . concatenate ( ( avids , val_ids ) , axis = 0 ) else : fpred = y_pred avreal = y_val avpred = scores_val avids = val_ids pred = fpred cv_LL = cv_LL + LL cv_AUC = cv_AUC + AUC cv_gini = cv_gini + ( AUC * 2 - 1 )
1469	LL_oof = log_loss ( avreal , avpred ) print ( '\n Average Log-loss: %.5f' % ( cv_LL / folds ) ) print ( ' Out-of-fold Log-loss: %.5f' % LL_oof ) AUC_oof = roc_auc_score ( avreal , avpred ) print ( '\n Average AUC: %.5f' % ( cv_AUC / folds ) ) print ( ' Out-of-fold AUC: %.5f' % AUC_oof ) print ( '\n Average normalized gini: %.5f' % ( cv_gini / folds ) ) print ( ' Out-of-fold normalized gini: %.5f' % ( AUC_oof * 2 - 1 ) ) score = str ( round ( ( AUC_oof * 2 - 1 ) , 5 ) ) timer ( starttime ) mpred = pred / folds
1470	result = pd . DataFrame ( mpred , columns = [ 'target' ] ) result [ 'id' ] = te_ids result = result . set_index ( 'id' ) print ( '\n First 10 lines of your 5-fold average prediction:\n' ) print ( result . head ( 10 ) ) sub_file = 'submission_5fold-average-keras-run-01-v1_' + str ( score ) + '_' + str ( now . strftime ( '%Y-%m-%d-%H-%M' ) ) + '.csv' print ( '\n Writing submission: %s' % sub_file ) result . to_csv ( sub_file , index = True , index_label = 'id' )
1471	mean_eval = np . mean ( xgb_evals , axis = 1 ) std_eval = np . std ( xgb_evals , axis = 1 ) best_round = np . argsort ( mean_eval ) [ : : - 1 ] [ 0 ] print ( ' Stopped after %d iterations with val-gini = %.6f +- %.6f' % ( best_round , mean_eval [ best_round ] , std_eval [ best_round ] ) ) if ( mean_eval [ best_round ] > GINIbest ) : GINIbest = mean_eval [ best_round ] return mean_eval [ best_round ]
1472	print ( '-' * 126 ) print ( '\n Final Results' ) print ( ' Maximum XGBOOST value: %f' % XGB_BO . res [ 'max' ] [ 'max_val' ] ) print ( ' Best XGBOOST parameters: ' , XGB_BO . res [ 'max' ] [ 'max_params' ] ) grid_file = 'Bayes-gini-5fold-XGB-target-enc-run-04-v1-grid.csv' print ( ' Saving grid search parameters to %s' % grid_file ) XGB_BO . points_to_csv ( grid_file )
1473	'X232' , 'X29' , 'X263' ,
1474	mtcnn = MTCNN ( margin = 14 , keep_all = True , factor = 0.5 , device = device ) . eval ( ) resnet = InceptionResnetV1 ( pretrained = 'vggface2' , device = device ) . eval ( )
1475	faces = [ ] frames = [ ] for j in range ( v_len ) : success = v_cap . grab ( ) if j in sample :
1476	if self . resize is not None : frame = frame . resize ( [ int ( d * self . resize ) for d in frame . size ] ) frames . append ( frame )
1477	if len ( frames ) % self . batch_size == 0 or j == sample [ - 1 ] : faces . extend ( self . detector ( frames ) ) frames = [ ] v_cap . release ( ) return faces def process_faces ( faces , resnet ) :
1478	filenames = glob . glob ( '/kaggle/input/deepfake-detection-challenge/test_videos/*.mp4' ) X = [ ] start = time . time ( ) n_processed = 0 with torch . no_grad ( ) : for i , filename in tqdm ( enumerate ( filenames ) , total = len ( filenames ) ) : try :
1479	fast_mtcnn = FastMTCNN ( stride = 4 , resize = 1 , margin = 14 , factor = 0.6 , keep_all = True , device = device )
1480	fast_mtcnn = FastMTCNN ( stride = 4 , resize = 0.5 , margin = 14 , factor = 0.5 , keep_all = True , device = device )
1481	from dlib import get_frontal_face_detector detector = get_frontal_face_detector ( ) def detect_dlib ( detector , images ) : faces = [ ] for image in images : image_gray = cv2 . cvtColor ( image , cv2 . COLOR_BGR2GRAY ) boxes = detector ( image_gray ) box = boxes [ 0 ] face = image [ box . top ( ) : box . bottom ( ) , box . left ( ) : box . right ( ) ] faces . append ( face ) return faces times_dlib = [ ]
1482	from mtcnn import MTCNN detector = MTCNN ( ) def detect_mtcnn ( detector , images ) : faces = [ ] for image in images : boxes = detector . detect_faces ( image ) box = boxes [ 0 ] [ 'box' ] face = image [ box [ 1 ] : box [ 3 ] + box [ 1 ] , box [ 0 ] : box [ 2 ] + box [ 0 ] ] faces . append ( face ) return faces times_mtcnn = [ ]
1483	def __init__ ( self , cfg ) : super ( ) . __init__ ( ) self . fc1 = nn . Linear ( cfg . dim , cfg . dim_ff ) self . fc2 = nn . Linear ( cfg . dim_ff , cfg . dim )
1484	lr = 0.005 epochs = 360 netD = Discriminator ( ) . to ( device ) optimizerD = optim . Adam ( netD . parameters ( ) , lr = lr ) criteria = nn . BCELoss ( ) netD . conv1 . weight = nn . Parameter ( torch . Tensor ( [ [ [ [ - 1.0 ] , [ 1.0 ] ] ] ] ) . to ( device ) ) for param in netD . conv1 . parameters ( ) : param . requires_grad = False
1485	for k in tqdm ( range ( epochs ) ) : for i , ( y , X , Zeros ) in enumerate ( data_loader ) : netD . zero_grad ( ) y_pred = netD ( Zeros , X ) loss = criteria ( y_pred , y ) loss . backward ( ) optimizerD . step ( ) if ( k + 1 ) % 2 == 0 : print ( f"Epoch: {k+1}/{epochs} | Loss: {loss}" )
1486	z = zipfile . PyZipFile ( 'images.zip' , mode = 'w' ) d = DogGenerator ( ) for k in range ( 10000 ) : img = d . getDog ( np . random . normal ( 0 , 1 , 100 ) ) f = str ( k ) + '.png' img . save ( f , 'PNG' ) ; z . write ( f ) ; os . remove ( f ) z . close ( )
1487	return sales . loc [ sales . id == _id ] \ . iloc [ : , 6 : ] \ . T
1488	daily_sales_dept_lookup = sales [ [ "dept_id" ] + list ( sales . columns [ 6 : ] ) ] \ . melt ( id_vars = "dept_id" ) \ . groupby ( "dept_id variable" . split ( ) ) \ . agg ( { "value" : "sum" } ) def series_from_dept ( dept : str ) -> pd . DataFrame : return daily_sales_dept_lookup . loc [ dept ]
1489	daily_sales_item_lookup = sales [ [ "item_id" ] + list ( sales . columns [ 6 : ] ) ] \ . melt ( id_vars = "item_id" ) \ . groupby ( "item_id variable" . split ( ) ) \ . agg ( { "value" : "sum" } ) def series_from_item ( item : str ) -> pd . DataFrame : return daily_sales_item_lookup . loc [ item ]
1490	fig , axes = plt . subplots ( nrows = 5 , figsize = ( 12 , 20 ) ) _ids = sales [ "id" ] . sample ( n = 5 , random_state = 1 ) for i in range ( len ( _ids ) ) : series_from_id_binned ( _ids . iloc [ i ] , bin_every = 7 ) . plot ( ax = axes [ i ] )
1491	daily_sales_item_lookup_scaled = daily_sales_item_lookup \ . pivot_table ( index = "variable" , columns = "item_id" , values = "value" ) . copy ( ) daily_sales_item_lookup_scaled = daily_sales_item_lookup_scaled . div ( daily_sales_item_lookup_scaled . mean ( axis = 0 ) , axis = 1 )
1492	daily_sales_item_lookup_scaled_weekly = daily_sales_item_lookup_scaled . copy ( ) . reset_index ( ) daily_sales_item_lookup_scaled_weekly [ "variable" ] = daily_sales_item_lookup_scaled_weekly . variable . map ( lambda x : x - ( x % 7 ) ) daily_sales_item_lookup_scaled_weekly = daily_sales_item_lookup_scaled_weekly . groupby ( "variable" ) . mean ( )
1493	counts = np . zeros ( model . children_ . shape [ 0 ] ) n_samples = len ( model . labels_ ) for i , merge in enumerate ( model . children_ ) : current_count = 0 for child_idx in merge : if child_idx < n_samples : current_count += 1 else : current_count += counts [ child_idx - n_samples ] counts [ i ] = current_count linkage_matrix = np . column_stack ( [ model . children_ , model . distances_ , counts ] ) . astype ( float )
1494	dendrogram ( linkage_matrix , ** kwargs ) return linkage_matrix plt . figure ( figsize = ( 14 , 6 ) ) plt . title ( 'Hierarchical Clustering Dendrogram' )
1495	daily_sales_item_lookup_scaled_clustered = daily_sales_item_lookup_scaled_weekly . T . reset_index ( ) daily_sales_item_lookup_scaled_clustered [ "cluster" ] = clusters daily_sales_item_lookup_scaled_clustered = daily_sales_item_lookup_scaled_clustered . set_index ( "cluster item_id" . split ( ) ) \ . sort_index ( )
1496	random . seed ( 1 ) daily_sales_item_lookup_scaled_clustered . loc [ 1 ] \ . T \ . iloc [ : , random . sample ( range ( daily_sales_item_lookup_scaled_clustered . loc [ 1 ] . shape [ 0 ] ) , 10 ) ] \ . plot ( figsize = ( 12 , 6 ) )
1497	fig , [ ax1 , ax2 ] = plt . subplots ( nrows = 2 , figsize = ( 12 , 6 ) ) daily_sales_item_lookup_scaled_weekly [ "HOBBIES_1_062" ] . plot ( ax = ax1 , color = "C0" ) daily_sales_item_lookup_scaled_weekly [ "HOUSEHOLD_2_040" ] . plot ( ax = ax2 , color = "C1" ) ax1 . set_title ( "HOBBIES_1_062" , fontsize = 14 ) ax2 . set_title ( "HOUSEHOLD_2_040" , fontsize = 14 ) ax1 . set_xlabel ( "" ) ax2 . set_xlabel ( "Days since start" )
1498	diff_matrix = { } cross = itertools . product ( cols , cols ) for ( col1 , col2 ) in cross : series1 = daily_sales_item_lookup_scaled_weekly [ col1 ] series2 = daily_sales_item_lookup_scaled_weekly [ col2 ] diff = dtw ( series1 , series2 , keep_internals = True , step_pattern = rabinerJuangStepPattern ( 2 , "c" ) ) \ . normalizedDistance diff_matrix [ ( col1 , col2 ) ] = [ diff ] return diff_matrix
1499	dtw_diff_df = pd . DataFrame ( dtw_diff_dict ) . T . reset_index ( ) \ . rename ( columns = { "level_0" : "item1" , "level_1" : "item2" , 0 : "diff" } ) \ . pivot_table ( index = "item1" , columns = "item2" , values = "diff" )
1500	daily_sales_item_lookup_scaled_weekly . T . merge ( dtw_clusters . loc [ dtw_clusters . cluster == 5 ] , left_index = True , right_index = True ) \ . T \ . plot ( figsize = ( 12 , 4 ) )
1501	plot_dtw ( "FOODS_3_247" , "FOODS_3_284" ) plot_dtw ( "FOODS_3_247" , "HOBBIES_1_122" ) plot_dtw ( "FOODS_3_247" , "HOUSEHOLD_1_164" ) plot_dtw ( "FOODS_3_247" , "HOUSEHOLD_1_429" ) plot_dtw ( "FOODS_3_247" , "HOUSEHOLD_2_318" )
1502	cards_cols = [ 'card1' , 'card2' , 'card3' , 'card5' ] for card in cards_cols : if '1' in card : df [ 'card_id' ] = df [ card ] . map ( str ) else : df [ 'card_id' ] += ' ' + df [ card ] . map ( str )
1503	import os import numpy as np from PIL import Image from scipy . fftpack import fft
1504	audio_path = '../input/train/audio/' pict_Path = '../input/picts/train/' test_pict_Path = '../input/picts/test/' test_audio_path = '../input/test/audio/' samples = [ ]
1505	sample_audio = [ ] total = 0 for x in subFolderList : all_files = [ y for y in os . listdir ( audio_path + x ) if '.wav' in y ] total += len ( all_files ) sample_audio . append ( audio_path + x + '/' + all_files [ 0 ] ) print ( 'count: %d : %s' % ( len ( all_files ) , x ) ) print ( total )
1506	def log_specgram ( audio , sample_rate , window_size = 20 , step_size = 10 , eps = 1e-10 ) : nperseg = int ( round ( window_size * sample_rate / 1e3 ) ) noverlap = int ( round ( step_size * sample_rate / 1e3 ) ) freqs , _ , spec = signal . spectrogram ( audio , fs = sample_rate , window = 'hann' , nperseg = nperseg , noverlap = noverlap , detrend = False ) return freqs , np . log ( spec . T . astype ( np . float32 ) + eps )
1507	fig = plt . figure ( figsize = ( 8 , 20 ) ) for i , filepath in enumerate ( sample_audio [ : 6 ] ) : plt . subplot ( 9 , 1 , i + 1 ) samplerate , test_sound = wavfile . read ( filepath ) plt . title ( filepath . split ( '/' ) [ - 2 ] ) plt . axis ( 'off' ) plt . plot ( test_sound )
1508	fig = plt . figure ( figsize = ( 8 , 20 ) ) for i , filepath in enumerate ( five_samples ) : plt . subplot ( 9 , 1 , i + 1 ) samplerate , test_sound = wavfile . read ( filepath ) plt . title ( filepath . split ( '/' ) [ - 2 ] ) plt . axis ( 'off' ) plt . plot ( test_sound )
1509	def wav2img ( wav_path , targetdir = '' , figsize = ( 4 , 4 ) ) : fig = plt . figure ( figsize = figsize ) samplerate , test_sound = wavfile . read ( filepath ) _ , spectrogram = log_specgram ( test_sound , samplerate ) output_file = wav_path . split ( '/' ) [ - 1 ] . split ( '.wav' ) [ 0 ] output_file = targetdir + '/' + output_file plt . imsave ( '%s.png' % output_file , spectrogram ) plt . close ( )
1510	import shutil shutil . make_archive ( 'train_zipped' , 'zip' , '/kaggle/working/train' ) shutil . make_archive ( 'test_zipped' , 'zip' , '/kaggle/working/test' ) shutil . make_archive ( 'exa_test_zipped' , 'zip' , '/kaggle/working/exa_test' )
1511	from sklearn import preprocessing import matplotlib . pyplot as plt plt . rc ( "font" , size = 14 ) from sklearn . linear_model import LogisticRegression from sklearn . cross_validation import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X_smt , y_smt , test_size = 0.2 , random_state = 0 ) from sklearn import metrics logreg = LogisticRegression ( ) logreg . fit ( X_train , y_train )
1512	ax . scatter ( X [ : , 0 ] , X [ : , 1 ] , c = y , s = 30 , cmap = cmap , clim = ( y . min ( ) , y . max ( ) ) , zorder = 3 ) ax . axis ( 'tight' ) ax . axis ( 'off' ) xlim = ax . get_xlim ( ) ylim = ax . get_ylim ( )
1513	model . fit ( X , y ) xx , yy = np . meshgrid ( np . linspace ( * xlim , num = 200 ) , np . linspace ( * ylim , num = 200 ) ) Z = model . predict ( np . c_ [ xx . ravel ( ) , yy . ravel ( ) ] ) . reshape ( xx . shape )
1514	n_classes = len ( np . unique ( y ) ) contours = ax . contourf ( xx , yy , Z , alpha = 0.3 , levels = np . arange ( n_classes + 1 ) - 0.5 , cmap = cmap , clim = ( y . min ( ) , y . max ( ) ) , zorder = 1 ) ax . set ( xlim = xlim , ylim = ylim )
1515	max_features = 95000 maxlen = 70 embed_size = 300 run_name = 'LSTM_GTU_Attention_class_balance'
1516	train_df [ "question_text" ] = train_df [ "question_text" ] . str . replace ( r"([^\w\s\'\"])" , r" \1 " ) train_df [ "question_text" ] = train_df [ "question_text" ] . str . replace ( r"\s{2,}" , r" " ) return train_df train_df = clean_text ( train_df )
1517	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( X ) ) X = tokenizer . texts_to_sequences ( X )
1518	def objective ( params ) : max_features = 95000 maxlen = 70 embed_size = 300 print ( 'Currently searching over : {}' . format ( params ) )
1519	model . compile ( loss = "binary_crossentropy" , optimizer = Adam ( ) , \ metrics = [ "accuracy" ] ) filepath = "weights_best.h5"
1520	train_audio_path = '../input/freesound-audio-tagging-2019/train_curated/' train_files = os . listdir ( train_audio_path ) train_annot = pd . read_csv ( '../input/freesound-audio-tagging-2019/train_curated.csv' ) test_audio_path = '../input/freesound-audio-tagging-2019/test/' test_files = np . sort ( os . listdir ( test_audio_path ) ) print ( test_files [ : 5 ] )
1521	from sklearn . preprocessing import MultiLabelBinarizer , LabelEncoder binarize = MultiLabelBinarizer ( classes = classes ) encode = LabelEncoder ( ) Y_split = encode . fit_transform ( first_labels_set ) Y = binarize . fit_transform ( all_labels_set )
1522	from tqdm import tqdm_notebook X_raw = [ ] for f in tqdm_notebook ( files ) : sample_rate , sample_ = wavfile . read ( str ( train_audio_path ) + f ) X_raw . append ( sample_ )
1523	def pad_audio ( samples ) : if len ( samples ) >= L : return samples else : return np . pad ( samples , pad_width = ( L - len ( samples ) , 0 ) , mode = 'constant' , constant_values = ( 0 , 0 ) )
1524	precisions_for_samples_by_classes = np . zeros ( ( num_samples , num_classes ) ) for sample_num in range ( num_samples ) : pos_class_indices , precision_at_hits = ( _one_sample_positive_class_precisions ( scores [ sample_num , : ] , truth [ sample_num , : ] ) ) precisions_for_samples_by_classes [ sample_num , pos_class_indices ] = ( precision_at_hits ) labels_per_class = np . sum ( truth > 0 , axis = 0 ) weight_per_class = labels_per_class / float ( np . sum ( labels_per_class ) )
1525	return ( self . _per_class_cumulative_count / float ( np . sum ( self . _per_class_cumulative_count ) ) ) def overall_lwlrap ( self ) :
1526	var_stats = { } hist_df = pd . DataFrame ( ) for var in features : var_stats = train_df [ var ] . append ( test_df [ var ] ) . value_counts ( ) hist_df [ var ] = pd . Series ( test_df [ var ] ) . map ( var_stats ) hist_df [ var ] = hist_df [ var ] > 1
1527	var_stats = { } for var in features : var_stats [ var ] = train_df [ var ] . append ( test_df [ ind ] [ var ] ) . value_counts ( )
1528	meanmask = np . zeros ( ( 256 , 1600 ) ) for i in range ( train . shape [ 0 ] ) : meanmask += rle2mask ( train [ 'EncodedPixels' ] . iloc [ i ] , ( 256 , 1600 , 3 ) ) . astype ( np . float64 ) meanmask /= train . shape [ 0 ] plt . imshow ( meanmask )
1529	fig , ax = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 22 , 8.27 ) sns . lineplot ( x = 'Weeks' , y = 'Percent' , data = df , ax = ax [ 0 ] ) sns . lineplot ( x = 'Weeks' , y = 'FVC' , data = df , ax = ax [ 1 ] ) fig . savefig ( "weeksvsfvc.jpeg" )
1530	files = [ ] for dirname , _ , filenames in os . walk ( '../input/osic-pulmonary-fibrosis-progression/train' ) : for filename in filenames : files . append ( os . path . join ( dirname , filename ) )
1531	def __init__ ( self , df , tokenizer , labeled = True ) : self . labeled = labeled if labeled : tmp = df [ [ "A-coref" , "B-coref" ] ] . copy ( ) tmp [ "Neither" ] = ~ ( df [ "A-coref" ] | df [ "B-coref" ] ) self . y = tmp . values . astype ( "bool" )
1532	imp = pd . DataFrame ( index = feature_names ) imp [ 'train' ] = pd . Series ( bst . get_score ( importance_type = 'gain' ) , index = feature_names ) imp [ 'OOB' ] = pd . Series ( bst_after . get_score ( importance_type = 'gain' ) , index = feature_names ) imp = imp . fillna ( 0 )
1533	import plotly . offline as pyo import plotly . plotly as py from plotly . graph_objs import * import pandas as pd import plotly plotly . offline . init_notebook_mode ( ) from scipy import signal pyo . offline . init_notebook_mode ( ) import plotly . plotly as py from plotly . graph_objs import * import plotly . plotly as py from plotly . graph_objs import *
1534	edge = get_edge ( nb , data , 500 ) num_of_adjacencies = get_numbers_of_adjcs ( edge , nb ) text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
1535	text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
1536	edge = get_edge ( nb , data , 2000 ) num_of_adjacencies = get_numbers_of_adjcs ( edge , nb ) text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
1537	bbox = ( bbox / zoom_factor ) . astype ( np . int ) y1 , x1 , y2 , x2 = bbox cropped_img = img [ y1 : y2 , x1 : x2 ]
1538	oof_mean_cv = oof_mean_cv . join ( pd . DataFrame ( inner_oof_mean_cv ) , rsuffix = split , how = 'outer' ) oof_mean_cv . fillna ( value = oof_default_mean , inplace = True ) split += 1 impact_coded = impact_coded . append ( data . iloc [ oof ] . apply ( lambda x : inner_oof_mean_cv . loc [ x [ feature ] ] . mean ( ) if x [ feature ] in inner_oof_mean_cv . index else oof_default_mean , axis = 1 ) ) return impact_coded , oof_mean_cv . mean ( axis = 1 ) , oof_default_mean
1539	impact_coding_map = { } for f in categorical_features : print ( "Impact coding for {}" . format ( f ) ) train_data [ "impact_encoded_{}" . format ( f ) ] , impact_coding_mapping , default_coding = impact_coding ( train_data , f ) impact_coding_map [ f ] = ( impact_coding_mapping , default_coding ) mapping , default_mean = impact_coding_map [ f ] test_data [ "impact_encoded_{}" . format ( f ) ] = test_data . apply ( lambda x : mapping [ x [ f ] ] if x [ f ] in mapping else default_mean , axis = 1 )
1540	import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt
1541	from sklearn . preprocessing import StandardScaler from sklearn . cross_validation import train_test_split from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import StratifiedShuffleSplit
1542	from keras . models import Sequential from keras . layers import merge from keras . layers import Dense , Dropout , Activation , Flatten from keras . layers . advanced_activations import PReLU from keras . layers import Convolution2D , MaxPooling2D from keras . utils . np_utils import to_categorical from keras . callbacks import EarlyStopping
1543	data = pd . read_csv ( '../input/train.csv' ) parent_data = data . copy ( ) ID = data . pop ( 'id' )
1544	y = data . pop ( 'species' ) y = LabelEncoder ( ) . fit ( y ) . transform ( y ) print ( y . shape )
1545	from sklearn import preprocessing X = preprocessing . MinMaxScaler ( ) . fit ( data ) . transform ( data ) X = StandardScaler ( ) . fit ( data ) . transform ( data )
1546	sss = StratifiedShuffleSplit ( n_splits = 10 , test_size = 0.2 , random_state = 12345 ) train_index , val_index = next ( iter ( sss . split ( X , y ) ) ) x_train , x_val = X [ train_index ] , X [ val_index ] y_train , y_val = y_cat [ train_index ] , y_cat [ val_index ] print ( "x_train dim: " , x_train . shape ) print ( "x_val dim: " , x_val . shape )
1547	early_stopping = EarlyStopping ( monitor = 'val_loss' , patience = 300 ) history = model . fit ( x_train , y_train , batch_size = 192 , epochs = 2500 , verbose = 0 , validation_data = ( x_val , y_val ) , callbacks = [ early_stopping ] )
1548	plt . semilogy ( history . history [ 'loss' ] ) plt . semilogy ( history . history [ 'val_loss' ] ) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ( [ 'train' , 'test' ] , loc = 'upper left' ) plt . show ( )
1549	plt . plot ( history . history [ 'acc' ] ) plt . plot ( history . history [ 'val_acc' ] ) plt . title ( 'model accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ( [ 'train' , 'test' ] , loc = 'upper left' ) plt . show ( )
1550	df_train = pd . read_csv ( 'train.csv' ) df_test = pd . read_csv ( 'test.csv' ) df_struct = pd . read_csv ( 'structures.csv' ) df_train_sub_charge = pd . read_csv ( 'mulliken_charges.csv' ) df_train_sub_tensor = pd . read_csv ( 'magnetic_shielding_tensors.csv' )
1551	df_struct [ 'c_x' ] = df_struct . groupby ( 'molecule_name' ) [ 'x' ] . transform ( 'mean' ) df_struct [ 'c_y' ] = df_struct . groupby ( 'molecule_name' ) [ 'y' ] . transform ( 'mean' ) df_struct [ 'c_z' ] = df_struct . groupby ( 'molecule_name' ) [ 'z' ] . transform ( 'mean' ) df_struct [ 'atom_n' ] = df_struct . groupby ( 'molecule_name' ) [ 'atom_index' ] . transform ( 'max' ) show_ram_usage ( ) print ( df_train . shape , df_test . shape )
1552	config = tf . ConfigProto ( device_count = { 'GPU' : 1 , 'CPU' : 2 } ) config . gpu_options . allow_growth = True config . gpu_options . per_process_gpu_memory_fraction = 0.6 sess = tf . Session ( config = config ) K . set_session ( sess ) start_time = datetime . now ( )
1553	for mol_type in mol_types : model_name_rd = ( '../keras-neural-net-for-champs/molecule_model_%s.hdf5' % mol_type ) model_name_wrt = ( '/kaggle/working/molecule_model_%s.hdf5' % mol_type ) print ( 'Training %s' % mol_type , 'out of' , mol_types , '\n' ) df_train_ = df_train [ df_train [ "type" ] == mol_type ] df_test_ = df_test [ df_test [ "type" ] == mol_type ]
1554	m1 = 1 m2 = 4 m3 = 1 target_data_1 = m1 * ( StandardScaler ( ) . fit_transform ( target_data_1 ) ) target_data_2 = m2 * ( StandardScaler ( ) . fit_transform ( target_data_2 ) ) target_data_3 = m3 * ( StandardScaler ( ) . fit_transform ( target_data_3 ) )
1555	train_input = input_data [ train_index ] cv_input = input_data [ cv_index ] train_target = target_data [ train_index ] cv_target = target_data [ cv_index ] train_target_1 = target_data_1 [ train_index ] cv_target_1 = target_data_1 [ cv_index ] train_target_2 = target_data_2 [ train_index ] cv_target_2 = target_data_2 [ cv_index ] train_target_3 = target_data_3 [ train_index ] cv_target_3 = target_data_3 [ cv_index ] test_input = input_data [ len ( df_train_ ) : , : ]
1556	def submit ( predictions ) : submit = pd . read_csv ( 'sample_submission.csv' ) print ( len ( submit ) , len ( predictions ) ) submit [ "scalar_coupling_constant" ] = predictions submit . to_csv ( "/kaggle/working/workingsubmission-test.csv" , index = False ) submit ( test_prediction ) print ( 'Total training time: ' , datetime . now ( ) - start_time ) i = 0 for mol_type in mol_types : print ( mol_type , ": cv score is " , cv_score [ i ] ) i += 1 print ( "total cv score is" , cv_score_total )
1557	precisions_for_samples_by_classes = np . zeros ( ( num_samples , num_classes ) ) for sample_num in range ( num_samples ) : pos_class_indices , precision_at_hits = ( _one_sample_positive_class_precisions ( scores [ sample_num , : ] , truth [ sample_num , : ] ) ) precisions_for_samples_by_classes [ sample_num , pos_class_indices ] = ( precision_at_hits ) labels_per_class = np . sum ( truth > 0 , axis = 0 ) weight_per_class = labels_per_class / float ( np . sum ( labels_per_class ) )
1558	def lwlrap ( scores , truth , ** kwargs ) : score , weight = calculate_per_class_lwlrap ( to_np ( truth ) , to_np ( scores ) ) return torch . Tensor ( [ ( score * weight ) . sum ( ) ] )
1559	class EasyDict ( dict ) : def __init__ ( self , d = None , ** kwargs ) : if d is None : d = { } if kwargs : d . update ( ** kwargs ) for k , v in d . items ( ) : setattr ( self , k , v )
1560	import numpy as np import pandas as pd import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors import numpy as np for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : print ( dirname ) from pathlib import Path
1561	import numpy as np import pandas as pd import matplotlib . pyplot as plt import itertools import string import re import os plt . style . use ( 'Solarize_Light2' ) pd . set_option ( "display.max_columns" , 2 ** 10 )
1562	import numpy as np import pandas as pd import matplotlib . pyplot as plt import itertools import string import re import os plt . style . use ( 'Solarize_Light2' ) pd . set_option ( "display.max_columns" , 2 ** 10 )
1563	date_cols = [ 'project_submitted_datetime' ] train = pd . read_csv ( os . path . join ( r'../input' , 'train.csv' ) , low_memory = False , parse_dates = date_cols ) test = pd . read_csv ( os . path . join ( r'../input' , 'test.csv' ) , low_memory = False , parse_dates = date_cols )
1564	print ( "=" * 60 ) print ( "Detecting NaN values in data:" ) print ( "=" * 60 ) print ( test_train . isnull ( ) . sum ( axis = 0 ) [ test_train . isnull ( ) . sum ( axis = 0 ) > 0 ] )
1565	print ( "=" * 30 ) print ( "Detecting NaN values in data:" ) print ( "=" * 30 ) print ( resources . isnull ( ) . sum ( axis = 0 ) [ resources . isnull ( ) . sum ( axis = 0 ) > 0 ] )
1566	print ( "=" * 30 ) print ( "Detecting NaN values in data:" ) print ( "=" * 30 ) print ( resource_stats . isnull ( ) . sum ( axis = 0 ) [ resource_stats . isnull ( ) . sum ( axis = 0 ) > 0 ] )
1567	print ( "=" * 30 ) print ( "Detecting NaN values in data:" ) print ( "=" * 30 ) print ( resource_stats . isnull ( ) . sum ( axis = 0 ) [ resource_stats . isnull ( ) . sum ( axis = 0 ) > 0 ] )
1568	test_train [ 'daytime' ] = pd . Series ( np . where ( ( ( 7 <= test_train . project_submitted_datetime . dt . hour ) & ( test_train . project_submitted_datetime . dt . hour <= 10 ) ) , 1 , 0 ) ) . apply ( str )
1569	dummy_colnames = [ 'teacher_prefix' , 'month' , 'school_state' , 'daytime' ] dummies = pd . get_dummies ( test_train . loc [ : , dummy_colnames ] ) dummy_categorical_cols += dummies . columns . tolist ( )
1570	list_train = test_train [ col_name ] . tolist ( ) list_test = test_train [ col_name ] . tolist ( ) return set ( ', ' . join ( list_train + list_test ) . split ( ', ' ) ) unique_categories = set_of_categories ( 'project_subject_categories' ) unique_subcategories = set_of_categories ( 'project_subject_subcategories' ) unique_cats_total = list ( unique_categories . union ( unique_subcategories ) ) dummy_categorical_cols += unique_cats_total print ( 'Categories:' , reprlib . repr ( unique_cats_total ) )
1571	plt . figure ( figsize = ( 14 , 10 ) ) plt_num = 1 for column in text_cols :
1572	plt . subplot ( 4 , 2 , plt_num ) plt . title ( f'{column} - {feature.capitalize()}' , fontsize = 12 ) plt_num += 1
1573	approved_mask = ( data . project_is_approved == 1 ) approved = data [ approved_mask ] . sample ( 1000 ) . assign ( feat = lambda df : df [ column ] . apply ( feature_from_txt ) ) not_approved = data [ ~ approved_mask ] . sample ( 1000 ) . assign ( feat = lambda df : df [ column ] . apply ( feature_from_txt ) )
1574	bandwidth = 0.225 ax = approved . feat . plot . kde ( bw_method = bandwidth , label = 'Approved' ) ax = not_approved . feat . plot . kde ( ax = ax , bw_method = bandwidth , label = 'Not approved' ) plt . xlim ( [ - .5 , 1 ] ) plt . legend ( loc = 'best' )
1575	unique_words = ( X > 0 ) . sum ( axis = 1 ) num_words = ( X ) . sum ( axis = 1 ) test_train [ col_new_name + '_unique_words' ] = unique_words test_train [ col_new_name + '_num_words' ] = num_words test_train [ col_new_name + '_vocab' ] = np . exp ( unique_words / ( num_words + 10e-10 ) )
1576	from csv import QUOTE_ALL for text_col in text_cols : test_train [ text_col ] = test_train [ text_col ] . str . replace ( '"' , ' ' )
1577	from tempfile import mkdtemp from sklearn . pipeline import Pipeline , FeatureUnion from sklearn . feature_extraction . text import TfidfVectorizer from sklearn . linear_model import LogisticRegression from sklearn . base import BaseEstimator , TransformerMixin from sklearn . decomposition import TruncatedSVD from sklearn . metrics import roc_auc_score , roc_curve , auc from scipy . sparse import coo_matrix , hstack from sklearn . preprocessing import RobustScaler
1578	def __init__ ( self , cols , ravel = False ) : self . cols = cols self . ravel = ravel def fit ( self , x , y = None ) : return self def transform ( self , x ) : return ( x . loc [ : , self . cols ] . values . ravel ( ) if self . ravel else x . loc [ : , self . cols ] . values ) class LogTransform ( BaseEstimator , TransformerMixin ) :
1579	grid_search = GridSearchCV ( pipeline_logreg , param_grid = param_grid , scoring = 'roc_auc' , n_jobs = 1 , verbose = 1 , cv = 3 )
1580	n = 25000 subset_A = test_train . loc [ lambda df : ( df . project_is_approved == 1 ) ] . sample ( n ) subset_B = test_train . loc [ lambda df : ( df . project_is_approved == 0 ) ] . sample ( n ) test_train_subset = pd . concat ( ( subset_A , subset_B ) ) test_X = test_train_subset test_y = test_X . project_is_approved
1581	import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1582	import numpy as np import pandas as pd import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( "ignore" )
1583	import pandas as pd import torch import torch . nn . functional as F from torch . optim import Adam import schnetpack as spk import schnetpack . atomistic as atm import schnetpack . representation as rep from schnetpack . datasets import * device = torch . device ( "cuda" )
1584	n_dataset = len ( dataset [ property ] ) n_val = n_dataset // 10 train_data , val_data , test_data = dataset [ property ] . create_splits ( n_dataset - n_val * 2 , n_val ) train_loader = spk . data . AtomsLoader ( train_data , batch_size = 128 , num_workers = 2 ) val_loader = spk . data . AtomsLoader ( val_data , batch_size = 256 , num_workers = 2 )
1585	model . load_state_dict ( torch . load ( property + '/output/best_model' ) ) loader = spk . data . AtomsLoader ( dataset , batch_size = 256 , num_workers = 2 ) model . eval ( )
1586	targets = [ ] predictions = [ ] with torch . no_grad ( ) : for batch in loader : batch = { k : v . to ( device ) for k , v in batch . items ( ) } result = model ( batch ) targets += batch [ property ] . squeeze ( ) . tolist ( ) predictions += result [ 'y' ] . squeeze ( ) . tolist ( ) return targets , predictions
1587	import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1588	train [ 'bin_3' ] = train [ 'bin_3' ] . map ( bin_dict ) train [ 'bin_4' ] = train [ 'bin_4' ] . map ( bin_dict ) test [ 'bin_3' ] = test [ 'bin_3' ] . map ( bin_dict ) test [ 'bin_4' ] = test [ 'bin_4' ] . map ( bin_dict )
1589	test [ 'target' ] = 'test' df = pd . concat ( [ train , test ] , axis = 0 , sort = False ) print ( "Data shape:" , df . shape )
1590	train , test = df [ df [ 'target' ] != 'test' ] , df [ df [ 'target' ] == 'test' ] . drop ( 'target' , axis = 1 ) train [ 'target' ] = train [ 'target' ] . astype ( int ) del df
1591	train . ord_1 = train . ord_1 . astype ( ord_1 ) train . ord_2 = train . ord_2 . astype ( ord_2 ) train . ord_3 = train . ord_3 . astype ( ord_3 ) train . ord_4 = train . ord_4 . astype ( ord_4 )
1592	from glob import glob from PIL import Image import cv2
1593	import matplotlib . pyplot as plt import matplotlib . patches as patches import seaborn as sns from bokeh . plotting import figure from bokeh . io import output_notebook , show , output_file from bokeh . models import ColumnDataSource , HoverTool , Panel from bokeh . models . widgets import Tabs
1594	DATASET = '../input/osic-pulmonary-fibrosis-progression' TEST_DIR = '../input/osic-pulmonary-fibrosis-progression/test' TRAIN_CSV_PATH = '../input/osic-pulmonary-fibrosis-progression/train.csv'
1595	train = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) train_dcm = pd . read_csv ( '../input/osic-image-eda/n_dicom_df.csv' ) train_dcm_shp = pd . read_csv ( '../input/osic-image-eda/shape_df.csv' ) train_meta_dcm = pd . read_csv ( '../input/pulmonary-fibrosis-prep-data/meta_train_data.csv' ) test = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' )
1596	test_patient_weeklist = test [ 'Patient_Week' ] = test [ 'Patient' ] . astype ( str ) + "_" + test [ 'Weeks' ] . astype ( str ) test2 = test . drop ( 'Patient' , axis = 1 ) test3 = test . drop ( 'Weeks' , axis = 1 ) test4 = test . reindex ( columns = [ 'Patient_Week' , 'FVC' , 'Percent' , 'Age' , 'Sex' , 'SmokingStatus' ] ) test4 . head ( 7 )
1597	corr_mat = df . corr ( method = 'pearson' ) sns . heatmap ( corr_mat , vmin = - 1.0 , vmax = 1.0 , center = 0 , annot = True , fmt = '.1f' , xticklabels = corr_mat . columns . values , yticklabels = corr_mat . columns . values ) plt . show ( )
1598	plt . pie ( train [ "Sex" ] . value_counts ( ) , labels = [ "Male" , "Female" ] , autopct = "%.1f%%" ) plt . title ( "Ratio of Sex" ) plt . show ( )
1599	plt . pie ( train [ "SmokingStatus" ] . value_counts ( ) , labels = [ "Ex-smoker" , "Never smoked" , "Currently smokes" ] , autopct = "%.1f%%" ) plt . title ( "SmokingStatus" ) plt . show ( )
1600	plt . pie ( train [ "SmokingStatus" ] . value_counts ( ) , labels = [ "Ex-smoker" , "Never smoked" , "Currently smokes" ] , autopct = "%.1f%%" ) plt . title ( "SmokingStatus" ) plt . show ( )
1601	train_x2 [ 'Patient_Week' ] = train_x2 [ 'Patient' ] . astype ( str ) + "_" + train_x2 [ 'Weeks' ] . astype ( str ) train_x3 = train_x2 . drop ( 'Patient' , axis = 1 ) train_x4 = train_x3 . drop ( 'Weeks' , axis = 1 ) train_x5 = train_x4 . reindex ( columns = [ 'Patient_Week' , 'Percent' , 'Age' , 'Sex' , 'SmokingStatus' , 'n_dicom' , 'n_list' ] ) train_x5 . head ( 7 )
1602	import pandas as pd import torch import torch . nn . functional as F from torch . optim import Adam import schnetpack as spk import schnetpack . atomistic as atm import schnetpack . representation as rep from schnetpack . datasets import * device = torch . device ( "cuda" )
1603	n_dataset = len ( dataset ) n_val = n_dataset // 10 train_data , val_data , test_data = dataset . create_splits ( n_dataset - n_val * 2 , n_val ) train_loader = spk . data . AtomsLoader ( train_data , batch_size = 128 , num_workers = 2 ) val_loader = spk . data . AtomsLoader ( val_data , batch_size = 256 , num_workers = 2 )
1604	model . load_state_dict ( torch . load ( property + '/output/best_model' ) ) loader = spk . data . AtomsLoader ( dataset , batch_size = 256 , num_workers = 2 ) model . eval ( )
1605	targets = [ ] predictions = [ ] with torch . no_grad ( ) : for batch in loader : batch = { k : v . to ( device ) for k , v in batch . items ( ) } result = model ( batch ) targets += batch [ property + '_true' ] . tolist ( ) predictions += result [ property + '_pred' ] . tolist ( ) return targets , predictions
1606	n_val = 10000 train , val , test = data . create_splits ( len ( data ) - n_val * 2 , n_val ) loader = spk . data . AtomsLoader ( train , batch_size = 128 , num_workers = 2 ) val_loader = spk . data . AtomsLoader ( val , batch_size = 256 , num_workers = 2 )
1607	def get_size_list ( targets , dir_target ) : result = list ( ) for target in tqdm ( targets ) : img = np . array ( Image . open ( os . path . join ( dir_target , target ) ) ) result . append ( str ( img . shape ) ) return result
1608	data = pd . read_csv ( '../input/train.csv' ) data [ 'size_info' ] = get_size_list ( data . Image . tolist ( ) , dir_target = '../input/train' ) data . to_csv ( './size_train.csv' , index = False )
1609	counts = data . size_info . value_counts ( ) agg = data . groupby ( 'size_info' ) . Id . agg ( { 'number_sample' : len , 'rate_new_whale' : lambda g : np . mean ( g == 'new_whale' ) } ) agg = agg . sort_values ( 'number_sample' , ascending = False ) agg . to_csv ( 'result.csv' ) print ( agg . head ( 20 ) )
1610	imageio . imwrite ( 'quality-70.jpg' , original_img , quality = 70 ) new_img = skimage . io . imread ( 'quality-70.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-70" ) plt . show ( )
1611	imageio . imwrite ( 'quality-90.jpg' , original_img , quality = 90 ) new_img = skimage . io . imread ( 'quality-90.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-90" ) plt . show ( )
1612	import random import numpy as np import pandas as pd import chainer import chainer_chemistry from IPython . display import display
1613	train_iter = chainer . iterators . SerialIterator ( train_dataset , batch_size , order_sampler = train_sampler ) valid_iter = chainer . iterators . SerialIterator ( valid_dataset , batch_size , repeat = False , order_sampler = valid_sampler ) test_iter = chainer . iterators . SerialIterator ( test_dataset , batch_size , repeat = False , order_sampler = test_sampler )
1614	from chainer import optimizers optimizer = optimizers . Adam ( alpha = 1e-3 ) optimizer . setup ( model )
1615	def process_image ( path ) : path = '../input/images_sample/' + path [ 0 : 7 ] + '/' + path im = np . array ( Image . open ( path ) )
1616	polygonsList = { } image = df [ df . ImageId == '6120_2_2' ] for cType in image . ClassType . unique ( ) : polygonsList [ cType ] = loads ( image [ image . ClassType == cType ] . MultipolygonWKT . values [ 0 ] )
1617	for p in polygonsList : for polygon in polygonsList [ p ] : mpl_poly = Polygon ( np . array ( polygon . exterior ) , color = plt . cm . Set1 ( p * 10 ) , lw = 0 , alpha = 0.3 ) ax . add_patch ( mpl_poly ) ax . relim ( ) ax . autoscale_view ( )
1618	df [ 'polygons' ] = df . apply ( lambda row : loads ( row . MultipolygonWKT ) , axis = 1 ) df [ 'nPolygons' ] = df . apply ( lambda row : len ( row [ 'polygons' ] . geoms ) , axis = 1 ) pvt = df . pivot ( index = 'ImageId' , columns = 'ClassType' , values = 'nPolygons' ) pvt
1619	for p in polygonsList : for polygon in polygonsList [ p ] : mpl_poly = Polygon ( np . array ( polygon . exterior ) , color = plt . cm . Set1 ( p * 10 ) , lw = 0 , alpha = 0.3 ) ax . add_patch ( mpl_poly ) ax . relim ( ) ax . autoscale_view ( ) plt . show ( )
1620	from PIL import Image import os os . listdir ( '../input/three_band' ) with open ( '../input/three_band/6120_2_2.tif' , encoding = 'utf-8' , errors = 'ignore' ) as f : print ( f . readlines ( ) )
1621	model = Sampler ( train , test ) model . compute_bounds ( ) y = model . compute_samples ( ) sample_sub = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/sample_submission.csv' ) sample_sub [ 'open_channels' ] = np . array ( y ) . astype ( 'int64' ) sample_sub . to_csv ( 'submission_0.csv' , index = False , float_format = '%.4f' )
1622	clf = RandomForestClassifier ( n_estimators = 150 , max_depth = 19 , random_state = 42 , n_jobs = 10 , verbose = 2 ) clf . fit ( X , open_channels )
1623	train_sales = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv' ) sell_prices = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sell_prices.csv' ) calendar = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/calendar.csv' ) submission_file = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' )
1624	with_prices_df = pd . merge ( left = tidy_df , right = calendar , on = 'd' ) with_prices_df . head ( 10 )
1625	def categorically_encode_col ( df , col ) : encoded_df = pd . get_dummies ( df [ col ] , prefix = str ( col ) , drop_first = False ) return encoded_df total_tidy_df . columns
1626	cols_to_encode = [ 'cat_id' , 'store_id' , 'weekday' , 'event_type_1' , 'event_type_2' ] for col in cols_to_encode : new_cols = pd . DataFrame ( categorically_encode_col ( total_tidy_df , col ) ) total_tidy_df = pd . concat ( [ total_tidy_df , new_cols ] , axis = 1 )
1627	n_items_dept = train_sales [ 'dept_id' ] . value_counts ( ) mean_of_total_sales_per_dept = dept_sum . mean ( axis = 0 ) ax = sns . regplot ( n_items_dept , mean_of_total_sales_per_dept ) ax . set ( title = 'Do departments with more items sell more? - No' , xlabel = 'Number of items Per Department' , ylabel = 'Mean total sales per department.' ) plt . show ( )
1628	cat_sum = train_sales . groupby ( [ 'cat_id' ] ) . sum ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = cat_sum , title = 'Total Sales by Category' , xlabel = "Category" , ylabel = "Total Sales" )
1629	state_sum = train_sales . groupby ( [ 'state_id' ] ) . sum ( ) . T . reset_index ( drop = True ) state_mean = train_sales . groupby ( [ 'state_id' ] ) . mean ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = state_sum , title = 'Total Sales by State ID' , xlabel = "State ID" , ylabel = "Total Sales" ) disp_boxplot ( data = state_mean , title = 'Mean Sales by State ID' , xlabel = "State ID" , ylabel = "Mean Sales" )
1630	store_sum = train_sales . groupby ( [ 'store_id' ] ) . sum ( ) . T . reset_index ( drop = True ) store_mean = train_sales . groupby ( [ 'store_id' ] ) . mean ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = store_sum , title = 'Total Sales by Store ID' , xlabel = "Store ID" , ylabel = "Total Sales" ) disp_boxplot ( data = store_mean , title = 'Mean Sales Per Day by Store ID' , xlabel = "Store ID" , ylabel = "Total Sales" )
1631	ax = sns . regplot ( x = np . arange ( dept_sales . shape [ 0 ] ) , y = dept_sales , scatter_kws = { 'color' : 'blue' , 'alpha' : 0.1 } , order = 3 , line_kws = { 'color' : 'green' } , ) ax . set ( title = "Mean Total Sales Per Item Per Day Over Time" , xlabel = 'Day ID' , ylabel = 'Total sale per item per day' ) plt . show ( )
1632	sarima_preds = pd . read_csv ( '/kaggle/input/m5-untuned-sarima-preds/Sarima_preds_submission.csv' ) sarima_preds [ sarima_preds < 0 ] = 0 sarima_preds [ 'id' ] = submission_file [ 'id' ]
1633	final_submission_df = train_sales . copy ( ) final_cols = [ 'id' ] for i in tqdm ( range ( 28 ) ) : important_days_df = prepare_submission_file ( train_sales , i , val_or_eval = 'val' )
1634	final_submission_df_eval = train_sales . copy ( ) final_cols = [ 'id' ] for i in tqdm ( range ( 28 ) ) : important_days_df_eval = prepare_submission_file ( train_sales , i , val_or_eval = 'eval' ) mean_of_days = important_days_df_eval . mean ( axis = 1 ) this_col = "F" + ( str ( i + 1 ) ) final_cols . append ( this_col ) final_submission_df_eval [ this_col ] = mean_of_days final_submission_df_eval [ 'id' ] = final_submission_df_eval [ 'id' ] . str . replace ( 'validation' , 'evaluation' )
1635	submission_df = pd . concat ( [ final_submission_df , final_submission_df_eval ] ) def clean_submission_file ( df ) : df = df [ final_cols ] return df mean_of_days_df = submission_df . copy ( ) mean_of_days_df = clean_submission_file ( mean_of_days_df )
1636	submission_df . to_csv ( 'submission.csv' , index = False ) print ( submission_df . shape ) print ( "Submission file created" )
1637	columns = train . columns for cc in tqdm_notebook ( columns ) : train [ cc ] = train [ cc ] . fillna ( train [ cc ] . mode ( ) [ 0 ] ) test [ cc ] = test [ cc ] . fillna ( test [ cc ] . mode ( ) [ 0 ] )
1638	X_train = train . copy ( ) X_test = test . copy ( ) ohe = OneHotEncoder ( dtype = 'uint16' , handle_unknown = "ignore" ) ohe . fit ( train ) X_train = ohe . transform ( train ) X_test = ohe . transform ( test )
1639	y = X . target . values id_train = X . id id_test = Xt . id X . drop ( [ 'id' , 'target' ] , axis = 1 , inplace = True ) Xt . drop ( [ 'id' ] , axis = 1 , inplace = True )
1640	ent = np . zeros ( X . shape [ 0 ] ) n = 2000 ent_temp = np . zeros ( n ) cv = KFold ( n , shuffle = False ) for idx in tqdm_notebook ( range ( X . shape [ 0 ] ) ) : for idx2 , ( train_idx , test_idx ) in enumerate ( cv . split ( X [ idx ] ) ) : ent_temp [ idx2 ] = entropy_fast ( X [ idx , test_idx ] , 300 ) ent [ idx ] = np . mean ( ent_temp )
1641	plt . figure ( figsize = ( 15 , 5 ) ) plt . plot ( ent ) plt . xlabel ( 'time' ) plt . ylabel ( 'entropy' ) ;
1642	plt . figure ( figsize = ( 15 , 5 ) ) plt . plot ( feature , 'r' ) plt . plot ( y , 'k' ) plt . ylabel ( 'TTF' ) plt . xlabel ( 'time' ) plt . grid ( )
1643	import matplotlib . pyplot as plt import numpy as np import pandas as pd from math import floor , log from scipy . stats import skew , kurtosis from scipy . io import loadmat
1644	import matplotlib . pyplot as plt import numpy as np import pandas as pd
1645	from math import floor , log from scipy . stats import skew , kurtosis from scipy . io import loadmat
1646	def openfile_dialog ( ) : return '../input/train_1/1_25_1.mat'
1647	def convertMatToDictionary ( path ) : try : mat = loadmat ( path ) names = mat [ 'dataStruct' ] . dtype . names ndata = { n : mat [ 'dataStruct' ] [ n ] [ 0 , 0 ] for n in names } except ValueError : print ( 'File ' + path + ' is corrupted. Will skip this file in the analysis.' ) ndata = None return ndata
1648	def calcNormalizedFFT ( epoch , lvl , nt , fs ) : lseg = np . round ( nt / fs * lvl ) . astype ( 'int' ) D = np . absolute ( np . fft . fft ( epoch , n = lseg [ - 1 ] , axis = 0 ) ) D [ 0 , : ] = 0 D /= D . sum ( ) return D
1649	def defineEEGFreqs ( ) : return ( np . array ( [ 0.1 , 4 , 8 , 14 , 30 , 45 , 70 , 180 ] ) )
1650	def petrosianFD ( X , D = None ) : if D is None : D = np . diff ( X ) N_delta = 0 ; for i in range ( 1 , len ( D ) ) : if D [ i ] * D [ i - 1 ] < 0 : N_delta += 1 n = len ( X ) return np . log10 ( n ) / ( np . log10 ( n ) + np . log10 ( n / n + 0.4 * N_delta ) )
1651	N_delta = 0 ; for i in range ( 1 , len ( D ) ) : if D [ i ] * D [ i - 1 ] < 0 : N_delta += 1 n = len ( X )
1652	def katzFD ( epoch ) : L = np . abs ( epoch - epoch [ 0 ] ) . max ( ) d = len ( epoch ) return ( np . log ( L ) / np . log ( d ) )
1653	L = np . abs ( epoch - epoch [ 0 ] ) . max ( ) d = len ( epoch ) return ( np . log ( L ) / np . log ( d ) )
1654	x = np . arange ( n ) tpoly = np . array ( [ np . polyfit ( x , d [ i ] , order ) for i in range ( len ( d ) ) ] ) trend = np . array ( [ np . polyval ( tpoly [ i ] , x ) for i in range ( len ( d ) ) ] )
1655	f_n = np . nansum ( flucs ) / len ( flucs ) fluctuations . append ( f_n ) fluctuations = np . array ( fluctuations )
1656	nonzero = np . where ( fluctuations != 0 ) nvals = np . array ( nvals ) [ nonzero ] fluctuations = fluctuations [ nonzero ] if len ( fluctuations ) == 0 :
1657	ldat = int ( floor ( nt / 2.0 ) ) no_levels = int ( floor ( log ( ldat , 2.0 ) ) ) seg = floor ( ldat / pow ( 2.0 , no_levels - 1 ) ) D = calcNormalizedFFT ( epoch , lvl , nt , fs )
1658	dspect = np . zeros ( ( no_levels , nc ) ) for j in range ( no_levels - 1 , - 1 , - 1 ) : dspect [ j , : ] = 2 * np . sum ( D [ int ( floor ( ldat / 2.0 ) ) + 1 : ldat , : ] , axis = 0 ) ldat = int ( floor ( ldat / 2.0 ) ) return dspect
1659	data = pd . DataFrame ( data = dspect ) type_corr = 'pearson' lxchannelsDyd = corr ( data , type_corr ) return lxchannelsDyd
1660	def replaceZeroRuns ( df ) : return ( df . replace ( 0 , np . nan ) . fillna ( ) )
1661	from sklearn import preprocessing def normalizeFeatures ( df ) : min_max_scaler = preprocessing . MinMaxScaler ( ) x_scaled = min_max_scaler . fit_transform ( df ) df_normalized = pd . DataFrame ( x_scaled , columns = df . columns ) return df_normalized def normalizePanel ( pf ) : pf2 = { } for i in range ( pf . shape [ 2 ] ) : pf2 [ i ] = normalizeFeatures ( pf . ix [ : , : , i ] ) return pd . Panel ( pf2 )
1662	from os import listdir def ieegGetFilePaths ( directory , extension = '.mat' ) : filenames = sorted ( listdir ( directory ) ) files_with_extension = [ directory + '/' + f for f in filenames if f . endswith ( extension ) and not f . startswith ( '.' ) ] return files_with_extension
1663	train = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) test = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' ) submission = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' )
1664	mse = mean_squared_error ( train [ 'FVC' ] , predictions , squared = False ) mae = mean_absolute_error ( train [ 'FVC' ] , predictions ) print ( 'MSE Loss: {0:.2f}' . format ( mse ) ) print ( 'MAE Loss: {0:.2f}' . format ( mae ) )
1665	fig = px . histogram ( new_df , x = 'Age' , nbins = 42 ) fig . update_traces ( marker_color = 'rgb(158,202,225)' , marker_line_color = 'rgb(8,48,107)' , marker_line_width = 1.5 , opacity = 0.6 ) fig . update_layout ( title = 'Distribution of Age' ) fig . show ( )
1666	fig = px . histogram ( train_x , x = 'Age' , color = 'SmokingStatus' , color_discrete_map = { 'Never smoked' : 'yellow' , 'Currently smokes' : 'cyan' , 'Ex-smoker' : 'green' , } , hover_data = train_x . columns ) fig . update_layout ( title = 'Distribution of Age w.r.t. SmokingStatus for unique patients' ) fig . update_traces ( marker_line_color = 'black' , marker_line_width = 1.5 , opacity = 0.85 ) fig . show ( )
1667	fig = px . line ( train_x , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'Sex' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines + markers' )
1668	fig = px . line ( train_x , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'SmokingStatus' , title = 'Pulmonary Condition Progression by Smoking Status' ) fig . update_traces ( mode = 'lines+markers' )
1669	def eval_metric ( FVC , FVC_Pred , sigma ) : n = len ( sigma ) a = np . empty ( n ) a . fill ( 70 ) sigma_clipped = np . maximum ( sigma , a ) delta = np . minimum ( np . abs ( FVC , FVC_Pred ) , 1000 ) eval_metric = - np . sqrt ( 2 ) * delta / sigma_clipped - np . log ( np . sqrt ( 2 ) * sigma_clipped ) return eval_metric
1670	sub_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' ) print ( f"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns." )
1671	_df . loc [ _df . Source == 'test' , 'min_week' ] = np . nan _df [ "min_week" ] = _df . groupby ( 'Patient' ) [ 'Weeks' ] . transform ( 'min' ) _df [ 'baselined_week' ] = _df [ 'Weeks' ] - _df [ 'min_week' ] return _df
1672	for idx in _df . index : patient_id = _df . at [ idx , 'Patient' ] _df . at [ idx , 'base_FVC' ] = baseline . loc [ baseline . Patient == patient_id , 'base_FVC' ] . iloc [ 0 ] _df . drop ( [ 'min_week' ] , axis = 1 ) return _df
1673	_df = df . copy ( ) base = _df . loc [ _df . Weeks == _df . min_week ] base = base [ [ 'Patient' , 'FVC' ] ] . copy ( ) base . columns = [ 'Patient' , 'base_FVC' ]
1674	from sklearn . preprocessing import OneHotEncoder , LabelEncoder from sklearn . preprocessing import StandardScaler , MinMaxScaler , RobustScaler from sklearn . compose import ColumnTransformer no_transform_attribs = [ 'Patient' , 'Weeks' , 'min_week' ] num_attribs = [ 'FVC' , 'Percent' , 'Age' , 'baselined_week' , 'base_FVC' ] cat_attribs = [ 'Sex' , 'SmokingStatus' ]
1675	no_transform_attribs = [ 'Patient' , 'Weeks' , 'min_week' ] num_attribs = [ 'FVC' , 'Percent' , 'Age' , 'baselined_week' , 'base_FVC' ] cat_attribs = [ 'Sex' , 'SmokingStatus' ]
1676	for col in cat_attribs : for value in df [ col ] . unique ( ) : df [ value ] = ( df [ col ] == value ) . astype ( int )
1677	own_MinMaxColumnScaler ( data_df , num_attribs ) own_OneHotColumnCreator ( data_df , cat_attribs ) data_df [ data_df . Source != "train" ] . head ( )
1678	def loss ( y_true , y_pred ) : return _lambda * qloss ( y_true , y_pred ) + ( 1 - _lambda ) * score ( y_true , y_pred ) return loss
1679	groups = train_df [ 'Patient' ] . values count = 0 for train_idx , val_idx in gkf . split ( X_train , y , groups = groups ) : count += 1 print ( f"FOLD {count}:" )
1680	net = get_model ( ) net . fit ( X_train [ train_idx ] , y [ train_idx ] , batch_size = BATCH_SIZE , epochs = EPOCHS , validation_data = ( X_train [ val_idx ] , y [ val_idx ] ) , verbose = 0 )
1681	train_preds [ val_idx ] = net . predict ( X_train [ val_idx ] , batch_size = BATCH_SIZE , verbose = 0 ) print ( "Predicting Test..." ) test_preds += net . predict ( X_test , batch_size = BATCH_SIZE , verbose = 0 ) / NFOLDS
1682	def expon ( x , y , log = False ) : if log : return np . log1p ( ( x ** ( 0.5 + abs ( x - y ) / 50 ) ) ) else : return ( x ** ( 0.5 + abs ( x - y ) / 50 ) )
1683	x = range ( 125 , 301 ) y = range ( 125 , 301 ) X , Y = meshgrid ( x , y )
1684	import random from tqdm . notebook import tqdm from sklearn . model_selection import train_test_split , KFold from sklearn . metrics import mean_absolute_error from tensorflow_addons . optimizers import RectifiedAdam from tensorflow . keras import Model import tensorflow . keras . backend as K import tensorflow . keras . layers as L import tensorflow . keras . models as M from tensorflow . keras . optimizers import Nadam import seaborn as sns import plotly . express as px import plotly . graph_objects as go from PIL import Image import tensorflow as tf
1685	if n : order = rate . iloc [ : n ] . index else : order = rate . index g1 = sns . countplot ( train_df [ col ] . fillna ( fillna ) , hue = train_df [ 'isFraud' ] , order = order ) g1 . set_ylabel ( '' )
1686	if show_rate : g2 = g1 . twinx ( ) g2 = sns . pointplot ( x = rate . index . values , y = rate . fillna ( 0 ) . values , order = order , color = 'black' ) plt . xticks ( rotation = rot )
1687	short_ordinal = [ ] long_ordinal = [ ] for col in ordinal : if train_df [ col ] . value_counts ( ) . shape [ 0 ] > 20 : long_ordinal . append ( col ) else : short_ordinal . append ( col ) print ( f'Short: {len(short_ordinal)}' , short_ordinal , '\n' ) print ( f'Long: {len(long_ordinal)}' , long_ordinal )
1688	import shutil import glob import numpy as np import pandas as pd import gc import os import matplotlib . pyplot as plt import tensorflow as tf print ( tf . __version__ )
1689	kmeans_model = KMeans ( n_clusters = 200 , random_state = 111 ) . fit ( ( X_node_org ) [ : , : , 0 ] ) org_train [ 'cluster_id' ] = kmeans_model . labels_ gkf = GroupKFold ( n_splits = n_folds ) if 'cluster_id' not in train . columns : train = train . merge ( org_train [ [ 'id' , 'cluster_id' ] ] , how = 'left' , on = 'id' )
1690	def is_failure ( df ) : failures = df . index [ ( df . time_to_failure . shift ( 1 ) > df . time_to_failure ) & ( df . time_to_failure . shift ( - 1 ) > df . time_to_failure ) ] . tolist ( ) return failures
1691	if beforePos < chunk . index . min ( ) : beforePos = chunk . index . min ( ) if afterPos > chunk . index . max ( ) : afterPos = chunk . index . max ( )
1692	for chunk in pd . read_csv ( '../input/train.csv' , chunksize = 5e7 , dtype = { 'acoustic_data' : np . int16 , 'time_to_failure' : np . float64 } ) : failure_pos = is_failure ( chunk ) for pos in failure_pos : data = region_readChunk ( pos , chunk ) failure_regions [ pos ] = data
1693	def region_plot ( df ) : data = df . copy ( ) data [ 'time_to_failure' ] = data [ 'time_to_failure' ] * 100 data [ 'time' ] = data . index data [ 'time' ] = data [ 'time' ] * ( 1 / 4e6 ) data [ 'Time [sec]' ] = data [ 'time' ] - data [ 'time' ] . min ( ) data [ [ 'acoustic_data' , 'time_to_failure' , 'Time [sec]' ] ] . plot ( x = 'Time [sec]' , figsize = ( 8 , 5 ) ) return
1694	sample = scipyImg . imread ( directory + filename , mode = 'RGB' ) if sample . shape [ 2 ] != 3 : return 'The input must be an RGB image.' return sample def basic_showImg ( img , size = 4 ) :
1695	plt . figure ( figsize = ( size , size ) ) plt . imshow ( img ) plt . show ( ) def basic_writeImg ( directory , filename , img ) : misc . imsave ( directory + filename , img )
1696	df1 = depths . set_index ( 'id' ) df2 = train_masks . set_index ( 'id' ) dataset = pd . concat ( [ df1 , df2 ] , axis = 1 , join = 'inner' ) dataset = dataset . reset_index ( )
1697	def __init__ ( self , in_dim : int , hidden_dims : List [ int ] , drop_rates : List [ float ] = None , use_bn = False , use_tail_as_out = True ) -> None :
1698	def __init__ ( self , left_mlp : MLP , right_mlp : MLP , tail_mlp : MLP ) -> None :
1699	def __init__ ( self , predictor , lossfun , evalfun_dict ) :
1700	for name in self . evalfun_dict . keys ( ) : setattr ( self , name , None ) loss = self ( * in_arrs ) for name , evalfun in self . evalfun_dict . items ( ) : setattr ( self , name , evalfun ( self . y , in_arrs [ - 1 ] ) ) reporter . report ( { name : getattr ( self , name ) } , self ) del loss
1701	train_model = Regressor ( predictor = model , lossfun = WeightedNormalizedAbsoluteError ( weights = [ .3 , .175 , .175 , .175 , .175 ] ) , evalfun_dict = { "NAE_Age" : SelectNormalizedAbsoluteError ( 0 ) , "NAE_Domain1Var1" : SelectNormalizedAbsoluteError ( 1 ) , "NAE_Domain1Var2" : SelectNormalizedAbsoluteError ( 2 ) , "NAE_Domain2Var1" : SelectNormalizedAbsoluteError ( 3 ) , "NAE_Domain2Var2" : SelectNormalizedAbsoluteError ( 4 ) } ) return train_model
1702	for i in range ( 5 ) : train_all . iloc [ : , i + 1 ] = train_all . iloc [ : , i + 1 ] . fillna ( train_all . iloc [ : , i + 1 ] . mean ( ) ) kf = KFold ( n_splits = 5 , shuffle = True , random_state = 1086 ) train_val_splits = list ( kf . split ( X = train_scores . Id ) )
1703	if sampler != None : dataloader_train = DataLoader ( dataset_train , batch_size = batch_size , sampler = sampler , num_workers = num_workers ) else : dataloader_train = DataLoader ( dataset_train , batch_size = batch_size , shuffle = True , num_workers = num_workers )
1704	model . load_state_dict ( torch . load ( f'./state_dict_{model_id}.pt' , map_location = device ) ) for param in model . parameters ( ) : param . requires_grad = True for g in optimizer . param_groups : g [ 'lr' ] = 1e-4
1705	if epoch == 1 or auc_valid > max_auc : saved = True max_auc = auc_valid torch . save ( model . state_dict ( ) , f'./state_dict_{model_id}.pt' ) counter = 0 else : saved = False counter += 1
1706	if epoch < freezed_epochs + 1 : counter = 0 else :
1707	class_sample_count = np . array ( [ len ( np . where ( tr [ 'target' ] == t ) [ 0 ] ) for t in np . unique ( tr [ 'target' ] ) ] ) weight = 1. / class_sample_count samples_weight = np . array ( [ weight [ t ] for t in tr [ 'target' ] ] ) sampler = WeightedRandomSampler ( samples_weight , len ( samples_weight ) )
1708	print ( f"---- fold: {i + 1} ------------" ) train_model ( f"{config['MODEL']}_{i + 1}" , dataset_train , dataset_valid , config [ 'BATCH_SIZE' ] , net , criterion , optimizer , scheduler , config [ 'NUM_EPOCHS' ] , config [ 'FREEZED_EPOCHS' ] , config [ 'INPUT_DIR' ] , config [ 'NUM_WORKERS' ] , sampler , config [ 'DEVICE' ] , config [ 'EARLY_STOPPING' ] )
1709	from kaggle_datasets import KaggleDatasets GCS_PATH = KaggleDatasets ( ) . get_gcs_path ( f'melanoma-{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}' ) GCS_PATH2 = KaggleDatasets ( ) . get_gcs_path ( f'malignant-v2-{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}' )
1710	img = tf . reshape ( img , [ dim , dim , 3 ] ) return img def count_data_items ( filenames ) : n = [ int ( re . compile ( r"-([0-9]*)\." ) . search ( filename ) . group ( 1 ) ) for filename in filenames ] return np . sum ( n )
1711	NUM_TRAINING_IMAGES = int ( count_data_items ( train_files ) * ( FOLDS - 1. ) / FOLDS ) NUM_VALIDATION_IMAGES = int ( count_data_items ( train_files ) * ( 1. / FOLDS ) ) NUM_TEST_IMAGES = count_data_items ( test_files ) STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE print ( 'Dataset: {} training images, {} validation images, {} unlabeled test images' . format ( NUM_TRAINING_IMAGES , NUM_VALIDATION_IMAGES , NUM_TEST_IMAGES ) )
1712	np . set_printoptions ( threshold = 15 , linewidth = 80 ) def batch_to_numpy_images_and_labels ( data ) : images , labels = data numpy_images = images . numpy ( ) numpy_labels = labels . numpy ( )
1713	FIGSIZE = 13.0 SPACING = 0.1 subplot = ( rows , cols , 1 ) if rows < cols : plt . figure ( figsize = ( FIGSIZE , FIGSIZE / cols * rows ) ) else : plt . figure ( figsize = ( FIGSIZE / rows * cols , FIGSIZE ) )
1714	plt . tight_layout ( ) if label is None and predictions is None : plt . subplots_adjust ( wspace = 0 , hspace = 0 ) else : plt . subplots_adjust ( wspace = SPACING , hspace = SPACING ) plt . show ( )
1715	training_dataset = get_dataset ( train_files , labeled = True , course_drop = False , all_aug = False , grid_mask = False , mat_aug = False , shuffle = True , repeat = True ) training_dataset = training_dataset . unbatch ( ) . batch ( 20 ) train_batch = iter ( training_dataset )
1716	test_dataset = get_dataset ( test_files , labeled = False , course_drop = False , grid_mask = False , mat_aug = False , all_aug = False , shuffle = True , repeat = False ) test_dataset = test_dataset . unbatch ( ) . batch ( 20 ) test_batch = iter ( test_dataset )
1717	if tf . random . uniform ( [ ] , 0 , 1 ) > .5 : img = apply_grid_mask ( img ) else :
1718	if tf . random . uniform ( [ ] , 0 , 1 ) > .5 : img = transform ( img ) else :
1719	LR_START = 5e-6 LR_MAX = 5e-6 * 8 LR_MIN = 1e-5 LR_RAMPUP_EPOCHS = 5 LR_SUSTAIN_EPOCHS = 0 LR_DECAY = .8
1720	def lr_schedule ( epoch ) : if epoch < LR_RAMPUP_EPOCHS : lr = ( LR_MAX - LR_START ) / LR_RAMPUP_EPOCHS * epoch + LR_START elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS : lr = LR_MAX else : lr = ( LR_MAX - LR_MIN ) * LR_DECAY ** ( epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS ) + LR_MIN return lr lr_callback = tf . keras . callbacks . LearningRateScheduler ( lr_schedule , verbose = True )
1721	rng = [ i for i in range ( EPOCHS ) ] y = [ lr_schedule ( x ) for x in rng ] plt . plot ( rng , y ) print ( "Learning rate schedule: {:.3g} to {:.3g} to {:.3g}" . format ( y [ 0 ] , max ( y ) , y [ - 1 ] ) )
1722	def get_DenseNet201 ( ) : with strategy . scope ( ) : dnet = DenseNet201 ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , weights = 'imagenet' , include_top = False )
1723	def get_Xception ( ) : with strategy . scope ( ) : xception = Xception ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , weights = 'imagenet' , include_top = False )
1724	def get_InceptionV3 ( ) : with strategy . scope ( ) : inception = InceptionV3 ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , weights = 'imagenet' , include_top = False )
1725	def get_InceptionResNetV2 ( ) : with strategy . scope ( ) : inception_res = InceptionResNetV2 ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , weights = 'imagenet' , include_top = False )
1726	train_ds = get_dataset ( train_files , labeled = True , return_image_names = False , all_aug = True , mat_aug = False , course_drop = False , repeat = True , shuffle = True ) val_ds = get_dataset ( val_files , mat_aug = False , course_drop = False , all_aug = False , grid_mask = False , repeat = False , shuffle = False , labeled = True , return_image_names = False )
1727	sv = tf . keras . callbacks . ModelCheckpoint ( f'fold-{f}.h5' , monitor = 'val_loss' , verbose = 0 , save_best_only = True , save_weights_only = True , mode = 'min' , save_freq = 'epoch' )
1728	submission = pd . DataFrame ( dict ( image_name = image_names , target = preds [ : , 0 ] ) ) submission = submission . sort_values ( 'image_name' ) submission . to_csv ( 'submission.csv' , index = False )
1729	fig , ax = plt . subplots ( figsize = ( 15 , 7 ) ) sns . kdeplot ( submission . target , shade = True ) plt . show ( )
1730	from fastai2 . basics import * from fastai2 . vision . all import * from fastai2 . medical . imaging import *
1731	files = glob . glob ( '../input/osic-pulmonary-fibrosis-progression/train/*/*.dcm' ) file = Path ( np . random . choice ( files ) ) dcm = file . dcmread ( ) for s , a , t in zip ( scales , subplots ( 1 , 2 , imsize = 7.3 ) [ 1 ] . flat , titles ) : dcm . show ( scale = s , ax = a , title = t )
1732	bins = pixel_dist . freqhist_bins ( 20 ) print ( bins ) fig , ax = plt . subplots ( figsize = ( 20 , 7 ) ) plt . hist ( pixel_dist , bins = bins ) plt . show ( )
1733	fig , ax = plt . subplots ( figsize = ( 20 , 7 ) ) plt . plot ( bins , torch . linspace ( 0 , 1 , len ( bins ) ) ) plt . show ( )
1734	scales = False , dicom_windows . lungs , True titles = 'Raw' , 'Windowed - Lung' , 'Scaled' for s , a , t in zip ( scales , subplots ( 1 , 3 , imsize = 7 ) [ 1 ] . flat , titles ) : dcm . show ( scale = s , ax = a , title = t )
1735	file_dir = Path ( '../input/osic-pulmonary-fibrosis-progression/train/ID00007637202177411956430/' ) dicom_meta = pd . DataFrame . from_dicoms ( file_dir . ls ( ) ) print ( f"Extracted DICOM data is of dimension: {dicom_meta.shape}" ) dicom_meta . head ( )
1736	index = [ 'BitsStored' , 'PixelRepresentation' ] dicom_meta . pivot_table ( values = [ 'img_mean' , 'img_max' , 'img_min' , 'PatientID' ] , index = index , aggfunc = { 'img_mean' : 'mean' , 'img_max' : 'max' , 'img_min' : 'min' , 'PatientID' : 'count' } )
1737	if ( reload_mmap ) : del l [ 0 ] mmap [ index , : ] = l
1738	with tf . variable_scope ( "layer_inputs" ) : inputs = tf . placeholder ( dtype = tf . float32 , shape = [ None , max_sentence_len , glove_dim ] , name = "input" ) batch_sequence_lengths = tf . placeholder ( dtype = tf . int32 , name = "sequence_length" ) with tf . variable_scope ( 'layer_conv_1D' ) :
1739	tf_padded_final_2 = tf_padded_final_2 [ 1 : , : ] tf_y_final_2 = tf_y_final_2 [ 1 : , : ] with tf . variable_scope ( 'layer_sentence_conv' ) :
1740	pass def build_session ( train_file , glove_file , mmap_loc , chkpoint_dir , train_tensorboard_dir , valid_tensorboard_dir ) : num_epochs = 7000 mini_batch_size = 64 learning_rate = 0.001 if ( os . path . exists ( train_tensorboard_dir ) ) : shutil . rmtree ( train_tensorboard_dir ) os . mkdir ( train_tensorboard_dir ) if ( os . path . exists ( valid_tensorboard_dir ) ) : shutil . rmtree ( valid_tensorboard_dir ) os . mkdir ( valid_tensorboard_dir )
1741	with tf . Graph ( ) . as_default ( ) as gr : final_probs , logits , inputs , batch_sequence_lengths , sentence_batch_len , \ sentence_index_offsets , sentence_batch_length_2 , tf_y_final_2 , ylen_2 , \ learning_rate_input , train_step , confusion_matrix , cross_entropy_mean , \ loss , global_step , predicted_indices , keep_fc , keep_conv = \ build_graph ( cutoff_seq )
1742	mmap = np . memmap ( mmap_loc , dtype = 'float32' , mode = 'r' , shape = ( cutoff_shape + 2 , glove_dim ) ) with tf . Session ( graph = gr , config = tf . ConfigProto ( log_device_placement = False ) ) as sess : sess . run ( tf . global_variables_initializer ( ) ) saver = tf . train . Saver ( )
1743	ckpt = tf . train . get_checkpoint_state ( chkpoint_dir ) if ckpt and ckpt . model_checkpoint_path : saver . restore ( sess , ckpt . model_checkpoint_path )
1744	with tf . Graph ( ) . as_default ( ) as gr : final_probs , logits , inputs , batch_sequence_lengths , sentence_batch_len , \ sentence_index_offsets , sentence_batch_length_2 , tf_y_final_2 , ylen_2 , \ learning_rate_input , train_step , confusion_matrix , cross_entropy_mean , \ loss , global_step , predicted_indices , keep_fc , keep_conv = \ build_graph ( cutoff_seq )
1745	ckpt = tf . train . get_checkpoint_state ( chkpoint_dir ) if ckpt and ckpt . model_checkpoint_path : saver . restore ( sess , ckpt . model_checkpoint_path ) while ( i <= test_len ) : if ( i + test_batch > test_len ) : test_batch = test_len - i else : test_batch = 10000 test_sample = meta_data [ i : i + test_batch ]
1746	print ( "Handling missing values..." ) def handle_missing ( dataset ) : dataset . category_name . fillna ( value = "missing" , inplace = True ) dataset . brand_name . fillna ( value = "missing" , inplace = True ) dataset . item_description . fillna ( value = "missing" , inplace = True ) return ( dataset ) train = handle_missing ( train ) test = handle_missing ( test ) print ( train . shape ) print ( test . shape )
1747	train [ "target" ] = np . log ( train . price + 1 ) target_scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) ) train [ "target" ] = target_scaler . fit_transform ( train . target . reshape ( - 1 , 1 ) ) pd . DataFrame ( train . target ) . hist ( )
1748	dtrain , dvalid = train_test_split ( train , random_state = 123 , train_size = 0.99 ) print ( dtrain . shape ) print ( dvalid . shape )
1749	BATCH_SIZE = 10000 epochs = 3 model = get_model ( ) model . fit ( X_train , dtrain . target , epochs = epochs , batch_size = BATCH_SIZE , validation_data = ( X_valid , dvalid . target ) , verbose = 1 )
1750	train = pd . read_json ( '../input/stanford-covid-vaccine/train.json' , lines = True ) test = pd . read_json ( '../input/stanford-covid-vaccine/test.json' , lines = True ) sample_sub = pd . read_csv ( '../input/stanford-covid-vaccine/sample_submission.csv' )
1751	public_df = test . query ( "seq_length == 107" ) . copy ( ) private_df = test . query ( "seq_length == 130" ) . copy ( ) private_preds = np . zeros ( ( private_df . shape [ 0 ] , 130 , 5 ) ) public_preds = np . zeros ( ( public_df . shape [ 0 ] , 107 , 5 ) ) public_inputs = preprocess_inputs ( public_df ) private_inputs = preprocess_inputs ( private_df )
1752	trn = train . iloc [ train_index ] trn_ = preprocess_inputs ( trn ) trn_labs = np . array ( trn [ target_cols ] . values . tolist ( ) ) . transpose ( ( 0 , 2 , 1 ) )
1753	val = train . iloc [ val_index ] val_all = preprocess_inputs ( val ) val = val [ val . SN_filter == 1 ] val_ = preprocess_inputs ( val ) val_labs = np . array ( val [ target_cols ] . values . tolist ( ) ) . transpose ( ( 0 , 2 , 1 ) )
1754	model . load_weights ( f'model-{f}.h5' ) model_short . load_weights ( f'model-{f}.h5' ) model_long . load_weights ( f'model-{f}.h5' ) holdouts . append ( train . iloc [ val_index ] ) holdout_preds . append ( model . predict ( val_all ) ) public_preds += model_short . predict ( public_inputs ) / ( FOLDS * REPEATS ) private_preds += model_long . predict ( private_inputs ) / ( FOLDS * REPEATS ) del model , model_short , model_long return holdouts , holdout_preds , public_df , public_preds , private_df , private_preds , histories
