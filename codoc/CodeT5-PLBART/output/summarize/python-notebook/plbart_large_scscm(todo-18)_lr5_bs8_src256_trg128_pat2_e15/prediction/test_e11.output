69	add trailing channel dimension
1437	Number of Patients and Images
93	Add the before values to the dataframe
384	to reduce memory , dtype is specified
1817	Extract lidar data from the dataset
229	Ensemble final scores
539	Naive Bayes on Test Data
1763	Extract null values from feature matrix
1447	Create submission file
607	Return a normalized weight vector for the contributions of each class
1508	Iterate over the training dataset
1188	Calculate the average accuracy of each assessment
1399	Compute series mean and standard deviation
473	Loading the data
309	Create an embedding matrix of words in the data
258	Make Submission File
750	Combinations of TTA
435	Visualise DICOM Info
505	Joint plot for all columns
984	Days with Days
423	Monthly readings
41	check for missing values
686	Create data generator and get ready for training
1232	Load Train , Validation and Test data
575	Correlation with Mattention
1140	Plot the dependence of the target
716	We now have the data ready for the model
1751	es es đồ tube
850	Get the data type
524	Create our Pipelines
1485	Use the TFRecords generated by the generator
32	Identity Hate Feature
1113	Subset text features
1372	Creating the submission file
1720	SAVE DATASET TO DISK
149	Quantiles of the IPs
197	blackhat image processing
1073	Distribution of the target variable
1670	Set seed for reproducability
987	Previous Loan Feature
340	Making predictions dataframe
73	create network and compiler
742	Clean the data
780	Range of the target variable
221	highlight the value
869	Create a file and open a connection
710	PRINT CV AUC
248	Getting the target data for training
1001	get the most recent time series
822	Read the image on how to visualise it
900	check for encoding
88	Creating and checking the models
789	ignore all warnings
1444	Square for both Train and Test Arrays
1150	Cropping with image augmentation
896	There might be a more efficient method to accomplish this
774	that have not been eliminated yet
1433	Visualizing the Link count
507	split into training and test based on dates
1054	Clean up memory
595	Create the model and train
1260	Combine the filename column with the variable column
953	import seaborn as sns
649	Building the model
191	Description Length Vs Price
1788	Peak frequency plot
1708	Exploring the data
1019	Merge with bureau data
1012	Add the column names
1737	The competition metric relies only on the order of recods ignoring IDs
830	We can see the distribution of surface
1809	Columns to be consolidated
1432	Extract the link count for each title
1107	Load sentiment file
1023	one hot encoding
1499	Check if the latest checkpoint exists
110	Plotting sales volumes over the year
1102	Prepare for Submission
954	set the target column
480	Tokenize the text
1005	We can see that we have done some mistake
388	Helper function for histogram processing
0	DICOM image ID
687	Get a sample from the dataset
951	Reading the datasets
1818	CHECK FOR EACH CATEGORY VARIABLE
1573	Prepare data for our model
833	Distribution of Fare
1577	Prepare the continuous features
964	Feature matrix with feature selection
1206	Define the model
1286	Brightness Manipulation with imgaug
51	Add columns for each row
587	There is a skewed distribution of bathroom count and lgerror
1801	k is camera instrinsic matrix
1728	This enables operations which are only applied during training like dropout
399	Print the confusion matrix
1169	Check the shape of dataframe
141	Number of Fraud vs
1654	get list of columns to use
1282	get train and test dataframes
1100	Get the best fold AUC
329	read in the images
788	Plot the importance line
882	Create a LightGBM dataset with hyperparameters
358	skin like mask
189	Shortest and longest coms
216	MinMax scale all variables
1014	Clean up memory
1177	change color of the object
1125	Cast the output of the decoder to a sigmoid function
1403	Compare the input data with the test data
532	Create our Pipelines
859	run randomized search
674	define training and validation sets
924	Draw the threshold lines
405	Calculate the log loss
132	Create Testing Generator
1661	Read in the OEM files
1006	Remove low features from training and testing features
543	Naive Bayes on Test Data
1090	Encodes predicted to binary
1673	Add box if opacity is present
1448	Scaling is necessary for linear models
1678	convert text into datetime
1120	Change column names
851	Calculate elapsed time
931	count number of combinations
1331	Loads pretrained weights , and downloads if loading for the first time
1230	Load the model into the TPU
497	Create DF for binary target
302	Read in image and resize
1681	the accurace is the all time wins divided by the all time attempts
1015	Import the datasets
84	Frequency distribution of class
519	Get the training and test sets
1405	rolling mean for each store
1352	Leak Data loading and concat
162	Get the indices of the two cells
628	Group by day
728	Read in the labels
1484	Create examples from the training dataset
1650	Cluster the results
848	Create random Forest Object using the mentioned parameters
511	Group by season
895	Find the features with zero importance
46	Distribution of the Target Variable
211	label encode categorical features
561	We can see there are some rows with zero values
124	pct change of columns
1776	Importing important libraries
614	Weight of the class
1320	convert column names to int
670	Create a function to transpose the data
1662	Example from sample image
1197	Predict data and create submission file
525	Run Grid Search
1617	remove layter activation layer and use losvasz loss
885	import seaborn as sns
225	Scatter plot of the three outputs
589	Number of Storeys Log Error
250	Linear SVR model
952	Get data ready for modeling
192	Show original image and grayscale
1738	Import and Read Data
1781	Tokenize the corpus
1546	extract the date columns
588	Room Count Vs Log Error
1764	Extract null values from feature matrix
1208	define parameters for model training
1657	get data for each sentiment
1333	Squeeze and Excitation layer , if desired
71	create numpy batch
1492	Predict on validation set
175	Load the image data
1358	iterate through all the columns of a dataframe and modify the data type
1149	Get the preprocessed data
413	OSIC Python Libraries
1589	to truncate it
1056	create train and validation sets
1495	set the necessary directories
764	msizes of sqm values
59	unfreeze and search appropriate learning rate for full training
795	delete hyperparameters from the hyp dictionary
1341	measure measurable variables over time
523	Remove Confidence Stadium
459	fitting random search
150	lets see if the attributed variables are correct
1505	ROTATION PIXELS ONTO ORIGIN PIXELS
1689	Sort by weight
15	Common data processors
301	Move image to sub folder
1209	Save model and weights
1389	StratifiedKFold On Labels
1565	skimage image processing packages
1245	Convert floats to integers
474	Create submission file
281	Number of files in the input directory
1398	this is the start of data in the process of data
621	Area of contour area
70	add trailing channel dimension
1786	load a table
592	Read the data
1805	Now we can look at how efficient generator approach
183	Brand name and number of item
1196	Get fold results
645	try random samples
96	Save before and after converters
174	The following was copied from
1624	Exploring the data
637	Peek of the input data folder
1246	Training History Plots
279	so we have pca with different clusters
1049	one hot encoding
24	Remove the Outliers if any
1450	create the look back datasets
354	Check if everything is ok
1700	Evaluate the Genetic Operators
1625	Make sure they are in the same state
315	SVR model on train and test
131	Prepare Testing Data
1289	Initialize input shape
1111	Extract processed data and format them as DFs
914	Joining with the aggregated data
1179	calculate the confusion matrix
1011	count categorical features
1467	temp test df fillna with frequency
1779	Generate the Mask for EAP
35	create a list of embeddings from train text
596	bird is a type of regularisation
1236	Draw the text boxes with the text size
1452	Extract target variable
1476	MAKE CUTMIX LABEL
1302	Load the model into the TPU
360	Show the training dataset
626	Sort by day
697	Precision helper function
243	Filter Albania , run the Linear Regression workflow
1202	Load Model into TPU
1378	Get random labels
1317	Define GPU configuration
411	What logistic regression wants to tell us ..
1812	get feature vector and response vector
886	Loading raw data
868	Sample out data
1070	Convert image to RGB
1198	Create submission file
609	if save to dir
814	Train and Validation
950	Iterate through hyperparameters
266	There are too many values , so we will subset the first
80	Passes through the convolutional filter set
618	Area of contour area
1346	plot prediction results
659	Getting the model accuracy
1735	set color palette
1017	Sort the table by percentage of missing descending
533	Run Grid Search
1606	I think the way we perform split is important
1367	Plot the distribution of districts of the day
1171	save images in order of time
1758	Feature Engineering with Feature Engineering
1647	Create a training and validation sets
453	Draw the image on the current image
1419	Imports and data loading
28	MODEL WITH SUPPORT VECTOR MACHINE
606	Only the classes that are true for each sample will be filled in
7	declare target , categorical and numeric columns
1215	Only load those columns in order to save space
1085	resize the image to the desired size
538	Fit and predict
472	Precision and Recall
290	cleanup working directory
1542	Fit the model
418	Preview of Data
9	fill test weather data
327	Add box if opacity is present
1460	checking missing data
743	pivot to have one row per type
847	Train model on all data
134	Initializing a CatBoostClassifier
662	date aggregates
1307	Calculate the quantized value
1545	Create new column name
1454	inverse transform yhat
1608	Checking Best Feature for Final Model
143	Compile and fit model
1614	Split the train dataset into development and valid based on time
1621	What about for a single type
902	Train the model with early stopping
631	calculate I , E , D
679	Splitting the labels into a few parts
355	Target values that differ a lot
8	merge with building info
845	Set missing values to
988	Data is balanced on the time axis
870	Write column names
66	load and shuffle filenames
491	Pearson Correlation heatmap
1233	Build datasets objects
799	Train the model with early stopping
432	Apply each model on all folds
805	Add subsample parameters
691	Computes gradient of the Lovasz extension w.r.t sorted errors
20	Check for missing values in training set
1087	Transform the coverage into class probabilities
1431	Save the files
336	batch size for training and validation
1615	show mask class example
1371	create the LightGBM dataset
790	run model on full training set and predict the results
1665	fill in mean for floats
604	Extracting Features from Raw Data
838	Plot the binary features
442	Latitude and Longitude
1475	MAKE MIXUP IMAGE
647	importing all the necessory Libraries
911	Convert categorical features into categorical features
350	Linear SVR model
308	Padding thedocs
858	Show plot of actual validation and predicted
1021	Sea lion metadata
321	Extra Tree Model
1755	Bureau Bureau Feature
1691	lifting function for univariate functions
580	Logmel feature extractor
1145	Curve Fit
1640	Load the Data
414	Check for Class Imbalance
1373	We fit the model
1701	Add new candidates to the candidates list
1203	Listing the fake files
1583	Multiply the new features with the original features
375	Run the build
1724	text version of squash , slight different from original one
945	iteration score 两列
994	What is the most common client type where Contract was approved
218	MinMax scale all feature scores
1798	shift train predictions for plotting
992	create Relationship objects
1267	Here we take the test data and generate sequences from the test data
310	Preparing the data
424	LISTING FOR METER READING
14	visualization of target histograms
195	inpaint with original image and threshold image
264	Prepare Training Data
147	Number of click by IP
754	Distribution for Poverty Level
718	Preprocess classifiers
569	Hours of Order Day
1502	Order does not matter since we will be shuffling the data anyway
919	Brand new features
878	extract scores from random parameters
986	Adding some new features from previous days
751	Settings for pretty nice plots
256	Voting Regression
1745	Here is the univariate approach
1726	for numerical features
22	Impute any values will significantly affect the RMSE score for test set
1238	Create a session and run
891	Drop the columns from the testing set
571	Hour of Day when the Reorders are high
582	Get the batch probabilities
164	Reading the image file and converting to grayscale
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1354	Fast data loading
72	define iou or jaccard loss function
367	SGD regressor
163	RLE Encoding for the Mask
1465	Define dataset and model
546	Create our Pipelines
921	Sum up the importance column
1497	Use pretrained model
421	set color of meter reading
275	Dropout RandomForest
242	Filter Albania , run the Linear Regression workflow
1064	Generate data for the TPU
578	Calculate spectrogram using pytorch
372	Voting Regression
422	Distribution of meter reading over hour
829	Read the image on how to visualise it
351	Find Feature Importance
512	Extract the season ID from the data frame
617	Split into train and test sets
1560	Correlation of macro features
1523	oversampled training dataset
801	Prepairing the target and confidence columns
1548	Add new features
1136	PUBLIC TEST DATASET
1141	Plot the dependence of the target
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
643	Can I get your attention
1018	Print some summary information
332	read in the images
37	Lets take the natural log on the training set
267	Find the missing values
1664	import stacknet
42	Distribution of number of items in official dataset
910	unique values of each column
81	Now through the second fully connected layer
1810	Importing all metrics
1040	Merge with previous counts
1540	Scoring the data
415	Visualise DICOM Info
832	Observation Looks like orientation features are cyclic
1723	missing entries in the embedding are set using np.random.normal
260	get the numerical values
167	Only the classes that are true for each sample will be filled in
205	Importing relevant Libraries
1563	import xgboost as xgb
934	Fitting and predicting
295	Binary target feature
296	Sample the data
1803	Correlation between images
137	clear output after batching
857	Distribution of Validation Fares
1330	Encodes a list of BlockArgs to a list of strings
747	Unfreeze backbone layers
1572	Distribution of data for each diagnosis
1632	Seting the date column
1610	Logcut the target variable
1053	get score on best score
1266	Extracting the test data
623	replace the country variable with China
397	Random Forest Classifier
1604	Nulls in Train and Test Data
753	Import Train and Test dataset
1212	Applying Quadratic Spline
1635	Splitting the data
1190	Event code distribution
966	Plotting Feature by Target Value
600	Get the mask directory
501	show the graphs
770	Is there a difference in the heads
682	Creating a DataFrame out of the training labels
1778	Import Train and Test dataset
715	Remove outliers
791	Train the model with pretrained data
494	Distribution of the new features
1507	Iterate over the training dataset
1009	remove variables with too many values
534	Fit and predict
630	Numba model to be multiplied with I
417	Preview of Data
182	Performance of Top Features
1434	if unk is true and there is no previous word
1796	timeseries time series
1144	Growth Rate Percentage
1482	set the necessary directories
409	Feature extraction from train and test
1799	shift test predictions for plotting
1550	Calculate the average week of each year
10	merge weather data
50	Distribution of the Target Variable
390	Set some parameters
1067	split training and validation data
492	find top correlated features
608	finding the absolute path in the directory
382	Now we can take a look at the image
1536	Check for Null values
161	make sure labels are small
972	Show random search
1424	function for cleaning lower case words
30	Load the train and test data sets
1600	Diverging Color palette
1002	Custom Feature Pipeline
1031	Suppress warnings due to deprecation of methods used
389	Check for empty images
1386	add image dimensions
1439	Reading in the data
1791	Flatten the date columns
1672	Initialize patient entry into parsed
658	Implementing the Random Forest
1568	Pinball loss for multiple quantiles
234	Filter selected features
209	FIND ORIGIN PIXEL VALUES
1244	Load the data
1706	Print best candidates
109	Plot rolling mean
1055	Calculate metrics for each fold
1397	Exploring the date variable
362	Get image file path
1459	checking missing data
1174	Named Color Analysis
16	To plot pretty figures
1703	Find the best candidates
1178	take a look of .dcm extension
567	A simple Keras implementation that mimics that of
194	display threshold image
231	Merge train and test , exclude overlap
724	Treating values with Simple Imputer
917	Amount loaned relative to salary
1003	We can see that we have done some basic aggregations
890	Correlation with PCA
1201	Detect hardware , return appropriate distribution strategy
970	align the data
971	Get score on random search
1537	Importing all the required packages
1417	BUILD MODEL NEW TOP
704	MODEL AND PREDICT WITH QDA
749	This is the primary method this needs to be defined
1182	inversely proportional to out product
481	Tokenize the text
434	Exploring the data
559	Create a dictionary with the values in order
1591	Load and check the models
843	separate train and validation sets
1156	watch out for overfitting
1655	Draw the heatmap using seaborn
54	Examine the duration of the taxi trip
208	ROTATION PIXELS ONTO ORIGIN PIXELS
734	Classify image and return top matches
244	Filter Andorra , run the Linear Regression workflow
1629	creating a time series for each country
616	Add some random data
136	Save the model
823	Custom Cutout augmentation with handling of bounding boxes
101	load the image file using cv
300	make a folder for each file
407	Run the map
52	Number of colors
159	Extracting connected objects
1350	iterate through all the columns of a dataframe and modify the data type
1487	Check if the latest checkpoint exists
1151	Create train set shapes
1359	Fast data loading
601	And see above link regarding mask size
324	Save them as pickle files
121	Checking the current coverage
1167	Import required libraries
864	Fitting the model on the test set
1262	code , image , mask
314	Getting the target data for training
665	date agg ditribution
583	Calculates the batch probabilities
740	add white noise
1551	Now we need to add the month information into the dataframe
345	Fully connected generator
1630	Go though the Province column
1184	v is set to all of the variables
1493	print validation results
53	Quadplot of the dataset
386	Verify that the length of the file is the same
646	For each type get the mean value
1425	total tokens and unique tokens
1275	Remove the background from the image
692	Using previous model
1519	Number of repetitions for each class
440	Top most commmon path
549	Naive Bayes on Test Data
1513	Order does not matter since we will be shuffling the data anyway
1543	find the median and arima model
138	Compiling GPU and the previous compiler
337	Train the model
1025	Hyperparameters search for LGBMClassifier
1511	Create the input layer and the augmentation layer
441	Latitude and Longitude
122	Function for cleaning special chars
95	Define utility functions
1582	coluns with new features
1375	Initialize empty arrays for feature extraction
1365	Download the kaggle data
451	Only the classes that are true for each sample will be filled in
808	Split up with indices
1571	load best model
912	function to create a new dataframe by aggregating all the values
125	Preparing the data
479	From Strings to Vectors
1611	Distribution of attributes into a single dataframe
1718	Tokenize the sentences
1277	Find the objects identified by color
1446	Order does not matter since we will be shuffling the data anyway
1574	Read the data
1430	make train and test features
246	Set the dataframe where we will update the predictions
1773	for numerical features
975	iteration score 两列
1407	Train the model
603	Output class encoding
615	An optimizer for rounding thresholds
1616	Computes gradient of the Lovasz extension w.r.t sorted errors
553	Naive Bayes on Test Data
688	draw box over image
1122	create a baseline model with all features
1734	Fill in missing values
1082	A single set of features of data
1503	of the TPU while respecting TPU
1528	Join examples with features and raw results
91	Augmentations and Feature Engineering
581	ceil and divide by the number of steps
496	get the data
304	Set class weights
1325	Calculate and round number of filters based on depth multiplier
1224	select proper model parameters
339	create test generator
249	SVR model on train and test
477	Vectorize the data
1392	Save the dataframe to the parquet file
128	Prepare Traning Data
1029	Calculate metrics for each fold
1506	FIND ORIGIN PIXEL VALUES
660	Computes and stores the average and current value
1401	Explore the series structure
245	Filter Andorra , run the Linear Regression workflow
140	Set seed for reproducability
1562	Get feature importances
178	Get the mean price for each category
293	Extract the id from file names
166	Analyze npy data and make a submission
930	Create random results
198	display threshold image
1531	Previous applications categorical features
517	Get the seeds as integers
548	Fit and predict
840	Manhattan Distance by Fare Amount
590	Gaussian target noise
67	split into train and validation filenames
1581	colunms new features
1222	create placeholders for the grid
181	Prices of the First Level categories
1478	LIST DESTINATION PIXELS
235	Clean Id columns and keep ForecastId as index
1242	Run the session and append the output to the list
344	create my generator
45	Quadplot of the dataset
1226	Plot some random images to check how cleaning works
433	Ignore the warnings
574	bedrooms per Interest Level
240	Filter Germany , run the Linear Regression workflow
1078	Set values for various parameters
1126	Load image and mask
678	using outliers column as labels instead of target column
1576	checking missing data
661	get different test sets and process each
929	find parameters between 0.005 and 0.0
877	Evaluate Bayesian Results
841	uclidean Distance by Fare Amount
666	Add products to the dataframe
1408	Predicting with kfold
145	Frame creation and gc.collect
154	Remove not attributed time
313	so we label encode categorical features
1385	suppose all instances are not crowd
252	Decision Tree Regression
1554	Train the model
232	Double check there are no informed ConfirmedCases and Fatalities after
853	What is the Average Fare amount by Day of Week
1618	average the predictions from different folds
398	Print the confusion matrix
316	Linear SVR model
21	Check for missing values in training set
85	Fake data preparation
106	load the data
1295	Prepare the data
773	Extract most correlated variables
876	iteration score 两列
598	Extracting images without ship information
1328	Gets a block through a string notation of arguments
712	ADD PSEUDO LABELED DATA
1696	This is the main prediction steps , along with some cleaning
898	Get the features with zero importance
1088	Remove padding from images
807	Convert features and labels to numpy array
1638	Check the distribution of min
57	just to be sane
1534	This code is copied from here
1494	Predict on test set
410	OneVsRestClassifier with SGDClassifier
1522	FIND ORIGIN PIXEL VALUES
1680	the time spent in the app so far
3	Reset Index for Fast Update
1216	creating a function that aggregates game time stats by installation id
714	Count the missing values
193	blackhat image processing
1132	make a prediction
1065	Model Hyper Parameters
956	Create Elastic Index
923	Cumulative importance plot
1300	It is here a simple function to preprocess the text
1757	adding application level features
210	for train and test create target dict
5	Encode Categorical Data
983	Adding new features from bureau
1356	meter split based
1115	Check if columns between the two DFs match
27	histogram of the data
1406	Get just the digits from the seeding
450	Create a DataLoader
464	Merge datasets into full training and test dataframe
504	create a lineplot
1089	Resize test predictions
487	load all data
1750	Creating an entity from dataframe
1036	Calculate the medians for all the variables
224	get the best score
1052	Train the model with early stopping
1521	ROTATION PIXELS ONTO ORIGIN PIXELS
1524	Eval data available for a single example
257	Predict and Submit
43	Add new features
467	Precision and Recall
1030	There might be a more efficient method to accomplish this
1736	Visualizing the Target Feature
1488	Eval data available for a single example
518	Make a Train and Loss DataFrames
228	Ensemble final scores
320	Gradient Boosting
624	Groping the dataframe by country
702	ADD PSEUDO LABELED DATA
1370	Label encode categorical variables
1205	Read in the eval data
735	Classify an image with different models
1804	Plotting ROC Curve
1345	The first prediction is easy
1075	Get the training data
1274	Each color is one of the original images
401	obtain one batch of training images
1770	missing entries in the embedding are set using np.random.normal
291	predictions image by image
1305	Submission file
18	impute missing values
1366	Read in the Data
694	remove activation layer and use losvasz loss
653	Read the training data
1780	The wordcloud of the raven for Edgar Allen Poe
937	This is the out file we will use for training
944	loop over all hyperparameters
1748	EDA and Feature Engineering
759	households without head
1413	Which attributes are not in train set
381	Convert item data to bson representation
915	unique values in each column
1443	Square for both Train and Test Arrays
76	load and shuffle filenames
1200	Plot RMSE
1471	Order does not matter since we will be shuffling the data anyway
1199	plot error vs boosting iterations
133	Spliting the data and training and evaluation
888	Merge Previous Features
879	Iterate through random parameters
1709	Importing sklearn libraries
1588	predict labels and compute F1 score
1096	Generate a submission
776	Create the PairGrid
213	creating dummies columns
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
825	Set to instance variables to use this later
144	get the data fields ready for stacking
439	Top most commmon IntersectionID
787	Cumulative importance plot
270	lightgbm weight distribution
1376	Deep Learning Libraries
1038	Previous aggregation layer
449	Creating train and test sets
897	Plot the cumulative importance
1410	Assigning variables to test set
404	create some empty arrays
748	Load the trained weights
1311	Display current run and time used
1416	Detect hardware , return appropriate distribution strategy
1653	Visualization of target variable
821	Random Forest Model
1558	Creation of the Watershed Marker
1117	Compute QWK based on OOF train predictions
794	Create a DataFrame with the selected features
707	print CV AUC
1158	size and spacing
207	LIST DESTINATION PIXELS
1749	Create an entity from dataframe
723	Sort ordinal features
513	Conference Tourney Games
1387	Apply transforms to sample
1645	Extracting coefficients from raw data
262	Visualizing parameters and LB score
1774	Shuffling happens here
1797	Plot rolling statistics
1627	Plot country predictions
1747	Percentage of the Age field
1155	Create strategy from tpu
1623	Logistic Regression on Hyperopt
114	merging the day and month columns with the previous dataframe
667	Ploting the distribuitions of the numerical features
1173	convert to HU
1721	LOAD DATASET FROM DISK
1355	iterate through all the columns of a dataframe and modify the data type
526	Fit and predict
755	Copy some features from another column
284	Define target and labels
1602	Different Time Series Modelling
1280	Analyzing the ARC
1142	Feature importance with shapap
380	Verify that the length of the file is the same
482	Now , define the model
385	Lets validate the test files
1165	Ready to set parameters
1619	checking missing data
1584	Checking only one value columns
1007	Calculate the medians for all the variables
172	Save the model as a JSON file
129	See sample image
1628	Some place I should know ..
1504	LIST DESTINATION PIXELS
880	Reading the datasets
1633	Age vs Gender In Patient Dataframe
105	load the data
1290	Squeeze and Excitation block
732	Function that detects and computes the image
1658	Tokenize the selected text
1707	Run the program
1712	Since the labels are textual , we encode them categorically
1306	Importing relevant Libraries
108	Sales volume per year
644	Perfect submission and target vector
810	store performance stats
374	Avoid division by zero by setting zero values to tiny float
436	Preview of Train and Test Data
737	draw the image we found
1514	size and spacing
1525	Span logits minus the cls logits seems to have some impact
948	Visualize the Random Search
784	Random Forest Validation
118	Pulmonary Condition by Sex
139	print the version of the compiler
1593	fill up the missing values
1651	Cluster the results
1451	inverse transform yhat
1118	Manually adjusted coefficients
1183	We need to pass the weightage as a dict
1249	parse trials and create submission file
949	Visualizing Learning Rate
557	Number of times the feature is interacted
635	Deterministic model
364	Scaling the features
836	Zooming nyc map
1351	Fast data loading
1321	Define GPU configuration
1688	Check if all the colors match the image
1679	get some sessions information
913	agg categorical features
1309	Ready to start training
1227	Read the data
1815	Load the data
33	proteins the folllwing label names
82	make submission for model
1435	take a look of .dcm extension
227	Scatter plot of RNN features
31	Vectorize the data
1313	Extract feature importances
188	wordcloud for item description
201	Neatmap of current working directory
990	Amount installments due date
757	Create a bar chart
708	ADD PSEUDO LABELED DATA
577	Get the sample
1659	for Neutral tweets keep things same
1294	warm up model
92	A fast pickle library for Python
1342	Calculates ratios for all columns
1189	Start generate data sets
305	Load the model and evaluate the model
927	the score , parameters and iteration
852	Fare Amount versus Time since Start of Records
247	Apply exponential transf
489	Now , we will take a look at how efficient generator approach
1445	we assume we are less accurate here , boost confidence
664	date agg with top
866	choice of boosting type
1698	test if the program has any empty mask
905	Calculate metrics for each fold
1110	Extract processed data and format them as DFs
871	Run the model
642	neutral word count in each sentiment
1500	Print directory contents
343	create my generator
1099	predict oof on validation set
1569	Initial Bayesian Optimization
1713	Building the model
119	Pulmonary Condition by Sex
299	Read in image and resize
443	Plot lat and long
1785	Avoid division by zero by setting zero values to tiny float
1697	Necessary function to return a list of images
359	Extract skin slices
727	Most frequent items in a category
1265	Create train and validation datagens
1172	Read in the Image Data
153	summarize by click
985	Using Bureau balance date
1103	Import the datasets
941	Evaluate the objective function
1695	Plot the sample
806	Convert parameters into int
445	Extracting informations from street features
223	get the best score
1414	This is how our data looks like
328	read the DICOM files
1163	Order does not matter since we will be shuffling the data anyway
1592	some config values
939	Create dataframe with scores and parameters
999	Returns the longest repetitions of the element
425	Distribution of the Target Variable
572	Number of orders by week
203	For every slice we determine the largest solid structure
1732	Samples which have unique values are real the others are fake
1123	Create the training and validation sets
1453	drop rows with NaN values
1464	Load test dataset
573	Number of bathrooms per Interest Level
1529	Read candidates with real multiple processes
1636	Brand new features
1058	Initialize and start training of the model
1028	Clean up memory
1438	Create data augmentation
1362	Converting to Total Days , Weekdays and Hour
365	SVR model on train and test
903	get score on best score
837	Handle legends
370	Gradient Boosting
322	Voting Regression
142	get the data fields ready for stacking
277	reorder the input data
1620	checking missing data
540	Create our Pipelines
828	Read the image on how to visualise it
1160	ROTATION PIXELS ONTO ORIGIN PIXELS
1470	scale the data
1292	Instantiating the model
1473	MAKE CUTMIX LABEL
274	Strings and Directories
68	if augment then horizontal flip half the time
1469	splitted features into a bin dataframe
1820	Expand if missing
400	Convert images to numpy array
254	Gradient Boosting
1344	Show some examples
680	Stacking the data
709	create stratified labels
564	extract item description as integers
982	Converting columns into timedelta object
960	What are the feature names
1278	Count the objects in the image
1181	conf mtx product
920	Cleaning credit card balance
1152	create validation set
1326	Round number of filters based on depth multiplier
1498	Decide the variables of the model
1428	add PAD to each sequence
731	Sample out hits
1510	ROTATION PIXELS ONTO ORIGIN PIXELS
514	Summary of NCAA Teams
40	load the dataset
676	Store data for training and validation
1377	Visualizing the augmented images
1161	FIND ORIGIN PIXEL VALUES
1092	Applying CRF seems to have smoothed the model output
276	sort the validation data
1043	Print some summary information
157	Print final result
738	draw the image we found
369	Create Validation Sets
636	Deterministic model
706	MODEL AND PREDICT WITH QDA
1229	Build datasets objects
681	Creating a DataFrame for the labels
1046	Group by LOAN
943	ROC AUC on train data
996	Get all the features
1116	Returns the counts of each type of rating that a rater made
893	Get data ready for modeling
819	Add to dataframe
116	Defining the parameter grid for the Random Search
593	factorize categorical features
783	Import all that we need
1044	drop missing columns
1766	cross validation and metrics
555	Defining the data types
1685	Load the training data
1041	Clean up memory
1771	text version of squash , slight different from original one
865	Create hyperparameters
1743	EXTRACT DEVELOPTMENT TEST
148	Clicks by IP
610	finding the absolute path in the directory
1643	How many clicks and downloads by device
1418	Extracting the word vectors from the corpus
849	Extract feature importances
49	Quadrate the data
720	Importing the Libraries
1768	shuffling the data
495	Read the data
1409	Assigning variables to test set
1207	load and shuffle data
1784	Compute the STA and the LTA
1135	Load the timestamps
1296	To use this feature we follow the following steps
1263	Using original generator
834	Next we define a function to calculate the evaluation metric
695	remove layter activation layer and use losvasz loss
1426	Number of characters in the sentence
1404	Let us look at the same format for our data
179	now we can get the mean price by category
469	Data processing , metrics and modeling
1520	LIST DESTINATION PIXELS
1552	Calculate the average day of the year
1357	Find Best Weight
816	Transforming the prediction column with the highest probability
503	scale pixel values to grayscale
356	Read the image from image id
528	Create our Pipelines
1682	An optimizer for rounding thresholds
104	Compile and fit model
1759	Feature extraction and preparation
812	Write column names
1400	Transform series into binary
1383	Split the data into train and validation sets
802	Prepairing the target and confidence columns
1077	Strips stopwords from text
1364	Address change function
1241	Load an image from image id
498	read in header and get dimensions
552	Fit and predict
261	Ploting parameters and LB score visualization
1693	lifting function name
303	Load the data
19	Check for missing values in training set
1221	Find the best score
1526	Default empty prediction
1237	Augmentations with TensorFlow
361	Image data processing
366	Linear SVR model
766	Add a legend and annotate it
818	Add the prediction values to the dataframe
1258	so we have to pad the labels
1303	Delete unnecessary model
215	converting our data into XGBoost format
11	Compute the STA and the LTA
1752	Creating an entity from dataframe
1384	convert to numpy array
376	Mean absolute error
1	Resize image to desired size
34	Loading Train and Test Data
1129	Define train and test paths
107	Checking for duplicate values
331	read the DICOM files
899	one hot encoding
671	Joining all the dataframes in one dataframe
1324	Change namedtuple defaults
744	Convert DNN images to DNE
1578	replace NaNs with
272	configurations and main hyperparammeters
1739	distribution of winPlacePerc
1042	Sort the table by percentage of missing descending
1027	get score on best score
565	Predict the feature
486	Importing the Keras libraries and packages
558	If you want to change the order of the parameters
430	Encode Categorical Data
1634	Find the columns where the dates are very different
935	sort by score
579	Calculate logmel spectrogram using pytorch
1794	Train the model
1332	Depthwise convolution phase
815	Training and Evaluating the Model
1789	Forceasting with decompasable model
1104	Credit card balance
1466	Run the model on the test set
378	Mean absolute error
56	Modeling with Fastai Library
1360	Leak Data loading and concat
1509	LIST DESTINATION PIXELS
1340	Remove signal to noise
928	Get a random sample if possible
1097	Check if train and test indices overlap
395	Print the confusion matrix
1369	Draw the centroids of the districts dataframe
347	Import Libraries and Data Input
306	Combining all the datasets into one DataFrame
334	Looking some informations of our data
1744	FITTING THE MODEL
1008	Correlation of the target variables
298	Read in image and resize
263	We will use xgb as our model parameters
909	extract the parent ids
1287	Brand new features
1062	Here are the main phases of the encoder
530	Fit and predict
268	Find the columns where the variance is high enough
733	Function that detects and computes the image
206	CONVERT DEGREES TO RADIANS
1390	Data augmentation and converting to Wheat dataset
1539	creating testing series
283	How many data are there in the dataset
545	Select Percentile
1051	Hyperparameters search for LGBMClassifier
962	Feature in Pipeline
86	Raw data analysis
698	Applying CRF seems to have smoothed the model output
160	Make a few adjustments for visualization
775	Draw the heatmap using seaborn
130	Look at how data generator augment the data
961	Put all features into a single dataframe
576	Target and parameters
26	visualization of target histograms
185	Categories of items with a price of
638	Function to generate a word cloud image
1319	Read the target and the input data
936	Train the model with random search params
1162	Order does not matter since we will be shuffling the data anyway
1076	Average length of the comment
113	Merging the month and day lag columns
1753	Automatic Feature Engineering
792	Merge with predictions
1746	Replace some missing values
1068	Print CV scores , as well as score on the test data
171	Fully connected model
87	fake data sampling
684	Creating a DataFrame for the labels
1725	The method for training is borrowed from
102	grid mask augmentation
978	There might be a more efficient method to accomplish this
1098	Create Validation Sets
1575	Reading the data into dataframe using pandas
613	Importing the Libraries
563	Length of items and item description
827	Read the image on how to visualise it
204	Remove other air pockets insided body
1248	Load and preprocess data
1130	Create dataloader
1792	Web Traffic Months cross days
448	Updated Train and Test Data for Modelling
38	observation data as pandas DataFrame
199	inpaint with original image and threshold image
651	create a submission
551	Run Grid Search
883	Fit the model
64	Distribution of continuous variables
1730	Add leak to test
318	Decision Tree Regression
406	Exploring the data
650	Convolutional Neural Network
1800	plot the predictions vs
126	Prepare continuous features
1423	function for transform sentence to wordlist
1047	load data from csv file
379	Print RMSE for validation set
1667	Stacking and Submission
1048	Credit card balance
1180	add the confmtx to the TPU
458	Checking Best Feature for Final Model
1456	Checking the distribution of price for each room
452	import required libraries
368	Decision Tree Regression
1057	Transform images and masks
1656	written by SeuTao
289	Classification Report
319	Create Validation Sets
1105	load mapping dictionaries
461	fitting random search
965	reset index and style
1666	StackNetClassifier with GPU
342	save predictions for submission
202	Determine current pixel spacing
809	Train the model with early stopping
1000	get custom feature names
4	Remove Unused Columns
835	new observations after NORMALIZE
566	Keras is only imported for splitting the data
470	Merge datasets into full training and test dataframe
403	Train the model
1806	Loading the train and test data
722	Sort ordinal features
348	highlight the upper triangle
1461	Make a Baseline model
1193	Go to actual revenues
1381	split the dataset in train and test set
253	Create Validation Sets
217	MinMax scale feature score
1412	Number of classes allowed in each class
1146	load mapping dictionaries
719	save preprocessed weights
520	Train the model
611	if save to dir
947	random hyperes
429	normalize year column
1754	create application installment records
1479	ROTATION PIXELS ONTO ORIGIN PIXELS
396	Calculate the confusion matrix
1071	You can access to the actual face itself like this
94	Creating a new row for each column
1164	DISPLAY VALIDATION IMAGES
1022	remove columns that do not exist in the data
1195	Make a prediction
1580	Multiply the new columns by the training and test
1281	from tqdm import tqdm
1590	Load the data
1327	Convolutions like TensorFlow , for a fixed image size
974	loop over all hyperparameters
1811	Plot the actual vs predicted
693	Using previous model
456	Load packages and data
1527	Computes official answer key from raw logits
1264	Load and predict
222	Linear SVR model
169	Fully Connected Models
856	Get the list of features
44	Normalize colors based on the image
1729	Add train leak
1013	Clean up memory
1121	Extract target variable
1742	SCALE target variable
488	extract categorical and numerical features
294	patch predictions for each image
1379	If the object is zero , warp the image
685	Creating a DataFrame out of the training labels
831	replace NaNs in train and test data with
1517	Get raw training dataset
1711	Read data from the CSV file
591	Combining all the augmentations in one
976	Setting up the hyperparameters
745	build a dict to convert surface names into numbers
438	We will look at the dimensions of our data
648	Load Train and Test Data
989	Adding new features from installments
483	Define the model
946	altair is a very nice plotting library by the way
1322	taking the input and target data
168	exp avg sqm
1595	Pad the sentences
963	Feature in Pipeline
2	Add new Features
1549	Merge Weather Data
717	Random Forest Regressor
1808	How many transactions are there
212	update feature scores
1204	Create fake folder
657	get the columns names
99	intialize the environment
1705	Iterate over the candidates
862	Standard deviation of best score
1740	Distribution of DBNOs
1772	always call this before training for deterministic results
1318	build train and test dataframes
1285	Members of the model
1039	Previous counts features
1683	contrast , etc
1641	ip count in each session
1486	Use pretrained model
1154	Representing the differences between datasets
1586	Remove unwanted columns
1219	Function for extracting title mode
627	Groping the same spain categories by day
785	Sum up the importance column
460	fitting random search
1253	Train the model
391	RGB test image
654	Read the test data
499	handle .ahi files
1061	Depth first conv layer
1512	size and spacing
1034	unique values of each column
1080	Divide feature by the number of words to get the average
1684	Load the training data
655	There are no missing values in the dataframe
1273	Count the number of objects in the object
1669	gather the parts of the pattern
103	Deleting unnecessary columns
1649	Extra Time Series
1555	Convert training set to lightgbm dataset format
1035	Convert categorical features into categorical features
1026	Train the model with early stopping
1020	Remove all the columns that are toxic
1442	prepare template with subsample images
338	plot training and validation losses
820	Random Forest Model
1710	Keras Libraries for Neural Networks
867	Extract parameters from various type
1775	This enables operations which are only applied during training like dropout
1441	read test images
233	Create date columns
1033	extract the parent ids
798	Split up with indices
855	separate train and validation sets
1501	Detect hardware , return appropriate distribution strategy
1270	Removing duplicate images
542	Fit and predict
907	Get the size of the data
353	folds for feature importance
1257	Build new files
509	Here is the custom threshold used by Kaggle
1159	LIST DESTINATION PIXELS
65	save pneumonia location in dictionary
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1339	Final linear layer
1599	checking missing data
1316	convert column names to int
1072	You can access to the actual pixel values like this
506	a tiny bit better but lets check with more features
1668	Clearly stores of type A
271	Partial imports
280	Visualizing the clusters
47	Add month information
884	Loading the required packages
1605	I think the way we perform split is important
25	Load train and test data
803	Visualizing Confidence by Target
1715	Ensure determinism in the results
826	Read the image on how to visualise it
541	Run Grid Search
186	Are there outliers
419	Examine the shape of our data
1541	checking the score of the model
916	Merge application data with bureau data
89	We can see that the model has not overfit
408	tag to count map
1338	The first block needs to take care of filter size increase
471	Plot ROC Curve
1561	Macro columns to a list
556	Extract feature names
926	The objective function
765	Visualize the markers
6	eliminate bad rows
1168	Transforming the features into log format
1137	Distribution of Date and Months
165	find the threshold for that image
872	Create a file and open a connection
906	Visualizing Cumulative Variance
312	Stratified Feature Importance
1380	Creating the xy columns
1225	Make a picture format from flat vector
1597	checking missing data
1587	Building Majority Features
237	Filter Spain , run the Linear Model
239	Filter Italy , run the Linear Regression workflow
1816	Reading the datasets
317	SGD regressor
1699	convert the sample of input and output to array
1279	Identify by both
1060	split train set to train and validation set
1153	Visualize the Data
752	so we can see the data
1769	SAVE DATASET TO DISK
60	Predicting and final submission
1795	fit the model
1050	check for encoding
605	Pad the audio data
796	parameter value is copied from
995	plot the most common client type
979	get the categorical variables
1063	We will use the most basic of all of them
97	Remove the Outliers
1613	Split the train dataset into development and valid based on time
1567	Pinball loss for multiple quantiles
1596	Train vs test
958	We can see some distributions of features
594	text features to full text
333	read in the test images
346	Fully connected generator
677	filtering out out out outliers
1299	Embedding function for feature extraction
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
968	Remove low features
307	Tokenize the text
1363	Some place I should know ..
1269	If the image is empty return the color of the image
1228	Load Train , Validation and Test data
779	escolari Age Vs
187	No description item
454	Run the object detection on the image
457	Importing important libraries
500	Separate zone and subject id into a df
1631	Create full table of data
656	COMBINE TRAIN AND TEST DATA
1343	Number of steps per SN
1093	salt parameters are from the above mentioned tutorial
515	Calculate the distribution of team preferences
767	drop high correlation columns
777	Plot the feature plots
349	highlight the value
1489	Read candidates from a .gz file
1069	Write the prediction to file for submission
1676	Lung Opacity
402	Credits and comments on changes
640	MosT common positive words
768	Example of walls vs epared
311	Stacking the target variables
13	Load train and test data
214	make sure to apply mean encoding only to infold set
1474	Cutmix iteration
58	you can play around with tfms and image sizes
673	add order features
1819	Join market and news
1648	Time series validation
1622	Print the feature ranking
1415	Number of labels for each instance
265	Scaling the features
991	EDA and Feature Engineering
255	Extra Tree Model
1530	Previous app data
1687	Mask for each image
462	Data Preprocessing Libraries
1395	Graphs and Targets
74	cosine learning rate annealing
842	Correlation with Fare Amount
1131	define the dataloaders here
416	load train data
1480	run this on the grid
892	Find the columns with more than 0.75
1170	Split the data back into its training and testing columns
811	Create a file and open a connection
1538	actual is lagged here
1388	flatten the bboxes to have one batch
1756	Feature Engineering with previous applications
1429	make train features
177	Most common level
619	Area of contour area
824	Applies the cutout augmentation on the given image
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
1692	lifting function to convert a sequence of arguments to a list of results
1601	checking missing data
1490	Read candidates with real multiple processes
1334	Expansion and Depthwise Convolution
1109	Unique IDs from train and test
1644	Clicks by Click Speed
1272	Check for the current object pairs
1626	Optimal Predictions
123	Process text for RNNs
341	change column names
1481	Histogram of continuous variables
1585	fill all na as
854	Investigation of Fare Amount vs
1686	Get the pixel values for a pixel
1037	Get the size of the data frame
652	whitegrid and seaborn for plotting
1139	Plot the dependence plot
781	change column names
629	Groping Data Set
846	BanglaLekha Benchmark
1293	Load dataset info
1314	Checking the distribution of values for each variable
1674	Add boxes with random color if present
446	Encoding the Regions
1297	Predicting the Test Set
904	Clean up memory
634	Define the run methods
957	create Relationship objects
1186	count of each session type
1382	Stemming and Lemmatization
1175	Add the cylindrical actor to the display
259	get the numerical values
633	Running all the training processes
393	What are the clusters we want to use
570	Days of the Week
1814	Copy predictions to submission file
288	sklearn is only imported for splitting the data
1637	count features by device and app
1271	Compute the distances of the object detections
1762	checking missing data
1462	Create dataset for training and Validation
1787	index to vectorize the error
1210	Pad the image to be PCA of the Image
1609	XGBOOST Sparse Feature Storage
1084	Resize images to desired size
1256	create train , test and test directories
516	add team conf
200	for later plotting
286	Specify train and validation paths
1468	Find max features
1704	delete the best candidate
39	Get the next batch
668	Read our data
932	Evaluate the model
560	Plot Gain importances
323	Wrap the prediction in iou or jaccard
887	Print the original features vs
1353	iterate through all the columns of a dataframe and modify the data type
326	Initialize patient entry into parsed
1391	Draw bounding boxes on the image file
1348	Fast data loading
1094	salt parameters to use later
938	Write column names
1411	Create the layout
352	load the train and test images
736	Remove zero features
287	Load the model and evaluate the model
1337	Update block input and output filters based on depth multiplier
1220	Drop nuisance columns and fill in missing values
1675	Visualize patient entry
83	Read the Text Data
863	Train the model and predict the test set
357	get different image data types
485	Now through the second convolutional layer
62	Count plot for categorical variables
236	Filter Spain , run the Linear Model
508	Here is the custom threshold used by Kaggle
23	Remove the Outliers if any
771	calculate the households for each customer
1516	Detect hardware , return appropriate distribution strategy
1213	Create strategy from tpu
1556	some config values
813	distribution of label for each target
1240	Read sample submission file
1472	size and spacing
1347	iterate through all the columns of a dataframe and modify the data type
980	changing the data type
1074	Distribution of our application and our data
1671	get train and test data
602	Reading the files
475	vectorize the input text
173	Find the unique calsses values
1091	Create submission dataframe
98	This is just a simple function that compares two sets
1533	Mean ROC Curve
226	Is there time leak in numerical features
431	Extract target variable
79	Resize image to desired size
146	Number of different values
1223	simple xgboost
562	create a baseline model with all features
1016	Bureau Bureau Feature Engineering
476	the times in the text
531	Naive Bayes on Test Data
1566	same as above
1128	Compute coverage per class
437	Preview of Train and Test Data
371	Extra Tree Model
894	Extract feature importances
1603	Checking for missing values
997	seed feature set
782	change column names
1310	Get feature importances
1147	Unique IDs from train and test
1553	Average day weights
1731	Function to create a video file
36	Log target variable
993	get interesting features and interesting features
184	Brand name price
1283	get train and test IDs
1086	Get the training set ready for the model
998	Calculate the normalized mode values
1349	Leak Data loading and concat
881	Get data ready for modeling
800	Light GBM Results
156	Frame creation and gc.collect
115	Defining the parameter grid for the Random Search
1010	Add the columns to the aggregation
127	get categorical features
1211	so we have to pad the images
1777	plot the correlation matrix
1717	LOAD PROCESSED DATA FROM DISK
335	Looking some informations of our data
1112	extract different column types
1598	checking missing data
760	Histogram of heads only
1477	batch mean
1457	checking missing data
392	Resize Train Images
1250	save best model
428	all other columns
238	Filter Italy , run the Linear Regression workflow
696	Exclude background from the analysis
466	Plot ROC Curve
1790	import libraries for time series forecasting
1252	Load Model Weights
1134	Stacked Validation Index
78	save dictionary as csv file
1594	Tokenize the sentences
844	Train model on all data
940	sort by score
969	Getting the training and testing sets
1124	Get the predictions
1547	inplace input data
689	functions to show an image
797	Convert data to numpy array
725	Predicting on Test Set
241	Filter Germany , run the Linear Regression workflow
180	zoom to the second level
219	Import Libraries and Data Input
1579	colunms new features
1716	FUNCTIONS TAKEN FROM
786	Plot the important features
1765	PLOT FOR RELEASED TIME
1760	This function preprocess the data
75	create train and validation generators
861	Split into training and testing data
112	Merge input and output dataframes
1157	numpy and matplotlib defaults
586	There is a distribution of the log error vs bedroom count
1045	Count installments products by LOAN
521	Generate the model winners
1559	Listing the features in order
426	Distribution after log transformation
620	Area of contour area
1646	Top Convolutional Layer
273	get lead and lags features
874	ROC AUC on train data
804	Get subsample parameters
839	Handle legends
120	Function for extracting words from text
1234	Model initialization and fitting on train and valid sets
554	Add RUC metric to monitor NN
1702	Evaluate the candidates
63	Distribution of continuous variables
1491	Returns a dictionary of counts
1059	Visualize Generated Images
641	For negative words
29	Load train and test data
1449	Helper function for create_dataset
1793	Plot the traffic month cross days
1243	to truncate it
1148	some tests for our dataset
1422	deep copy the sentences
1247	to truncate it
170	Second component of main path
55	find the number of clusters
1694	Plot images as a figure
1639	Clicks with Clicks
176	Visualizing Random Images
959	Default Feature Names
672	List all models
1436	How many Patients and Images are there
1394	Number of masks per image
61	Function to extract the sex , smoking , sex
756	Distribution for Poverty Level
1483	This is a very performative way to compute the metrics
1239	Run the model
817	apply method to train and test
1440	read data and prepare features
1106	Load metadata file
1004	We can see that we have done some basic aggregations
1261	Create test generator
793	Create a selector
669	Load all the data into respective dataframes
942	sort by score
1032	remove variables with too many missing values
1767	Tokenize the sentences
1095	Predict on validation set
1564	Now we can label encode the categorical features
1402	Extract data from first n samples
455	Draw boxes on the image file
683	Creating a DataFrame out of the training labels
196	Show original image and grayscale
1108	Load image file
152	Categories of Clickers
1081	Loop over rows
1133	Stacking the validation predictions and masks
1570	Convert DICOM images to PNG via openCV
282	Data Prepparation Check
1192	fill in zero for categorical features
1420	from googletrans import Translator
387	Convert item data to bson representation
251	SGD regressor
568	Read the Data
1255	Load Model into TPU
1374	Preparing the data
529	Run Grid Search
547	Run Grid Search
599	Read the image on how to visualize it
1291	Squeeze and Excitation
597	Data loading and data explanation
585	Building .csv files
1268	Linear Weighted Kappa
444	Latitude and Longitude
1714	cross validation and metrics
1544	inplace input data
625	Sort by day
1660	Read the cities and convert to integers
1458	checking missing data
394	Decision Tree Classifier
1515	Uncomment line below to run
427	first column only
1642	ip count in each session
721	nominal variables countplots
703	STRATIFIED K FOLD
12	This block is SPPED UP
522	Add the Train Predictions
675	Run a grid search
447	Encoding the Regions
420	Distribution of meter type
1301	load test data
1802	This code is for classifcation model
90	fast less accurate
746	Get the counts of each label
908	remove variables with too many missing values
1298	make a submission
1166	Parallel the test samples
1315	ATOMIC Numbering
1612	Split the train dataset into development and valid based on time
741	Scatter plot of single line
1761	Label encode categoricals
981	replace day outliers
1807	import some data
468	Loading the data
918	extract installments payments
527	Naive Bayes on Test Data
383	Read the data
1066	First dense layer
778	drop high correlation columns
537	Run Grid Search
155	Plot the download rate over the day
1783	Word cloud of First Topic
17	Now extract the data from the new transactions
330	Visualizing the images
769	we need to predict the heads
875	dict存储的参数转化的数据 df
612	Playing some audio
1557	Creation of the External Marker
412	Importing necessary libraries
1276	get the inputs and targets of the task
1427	Set values for various parameters
363	Getting the target data for training
117	Submit data preparation
1335	Squeeze and Excitation
1677	Visualizing Patient Effusion
1176	save images in order of time
1396	Convert Time Series to Frequency Series
639	Segregating the sentiment data
925	Fitting the model on the test set
1235	Draw the image on the current image
632	Load the population data
490	plot distribution of the validation features
1329	Encodes a block to a string
761	There is missing data in test and train data
544	Creating Training Set
151	Distribution of the Target Variable
463	Data processing , metrics and modeling
465	get bayesian train and validation sets
1663	kick off the animation
1217	Group by event time
762	Plot the counts
739	Compute the ratio of x and y
967	There might be a more efficient method to accomplish this
663	Create a date aggregation for
77	retrieve x , y , height and width
100	code takesn from
1119	Distribution inspection of original target and predicted train and test
1312	reduce validation set
1741	HANDLE MISSING VALUES
285	Define target and labels
48	Normalize colors based on the image
873	Write column names
1259	Using original generator
726	Photo image and mask
493	Applicatoin train data
1652	Load the data
325	Save them as pickle files
220	highlight the upper triangle
1813	summarize history for loss
1532	Choose and initialize a model
1304	Delete unnecessary model
1496	This is a very performative way to compute the metrics
889	align the columns
922	Plot the top features
478	Vectorizing the data
550	Create our Pipelines
1024	check for encoding
1214	Order does not matter since we will be shuffling the data anyway
955	EDA and Feature Engineering
1421	strips whitespace from comment texts
1251	Splits the category variable into train and validation sets
1284	Get the data ready for the Neural Network
1127	Initialize the data
772	CALCULATE NEW FEATURES
1690	Sort by length
584	Transforms a pandas DataFrame into a Pandas dataframe
1361	import modules and define models
622	Examples for usage and understanding
713	Predict on label data
758	Which of the households have the same target
1079	vocaublary , add its feature vector
292	Load and predict
1143	This is how our data looks like
1463	CNN Model for multiclass classification
190	Distribution of the description length
1185	update user samples
230	Implementing the SIR model
1518	Get raw training dataset
269	Merge Dense Players
1114	Remove missing target column from test
700	MODEL AND PREDICT WITH QDA
373	Compute the STA and the LTA
1191	PREDICTIONS FOR SUBMISSION
977	Converting data into numeric format
1368	Display the map
510	process remaining batch
1187	Randomly sample the training set to the test set
1722	The mean of the two is used as the final matrix
536	Create our Pipelines
1727	Shuffling happens here
1218	We can extract the world time stats for each game group
973	Get score for random set
111	month over day lag
901	credits to for the parameters values
860	We now have the data ready for the model
1336	Skip connection and drop connect
729	Distribution of coefficient values for each image
1782	Calling our overwritten Count vectorizer
1733	Importing the required libraries
690	Iterate over the rows
535	Naive Bayes on Test Data
1607	one hot encode the features
1393	Set Categorical Features
135	Checking for Class Imbalance
730	Extract time features
1101	Training the model
1194	Predict on train set
763	current y value
484	example of a cnn for image classification
1138	SHAP interaction values
1535	load the data
297	Split training and validation sets
933	sort by score
1288	Load dataset info
1083	Load csv files
1323	Parameters for an individual model block
1254	make a prediction
377	RMSEs for each fold
1455	inverse transform for regression
1308	Ready to start training
278	Word Cloud visualization
158	We need to convert images to numpy array
1719	shuffling the data
