143	model = get_model ( ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . fit ( X_train , y_train , epochs = 150 , batch_size = 400 , verbose = 1 )
1735	sns . set ( style = 'darkgrid' ) sns_plot = sns . palplot ( sns . color_palette ( 'Accent' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Accent_d' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'CMRmap' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Set1' ) ) sns_plot = sns . palplot ( sns . color_palette ( 'Set3' ) )
756	sns . kdeplot ( train . loc [ train [ 'Target' ] == poverty_level , col ] . dropna ( ) , ax = ax , color = color , label = poverty_mapping [ poverty_level ] ) plt . title ( f'{col.capitalize()} Distribution' ) ; plt . xlabel ( f'{col}' ) ; plt . ylabel ( 'Density' ) plt . subplots_adjust ( top = 2 )
217	feature_score = pd . DataFrame ( preprocessing . MinMaxScaler ( ) . fit_transform ( feature_score ) , columns = feature_score . columns , index = feature_score . index ) feature_score [ 'mean' ] = feature_score . mean ( axis = 1 ) feature_score . sort_values ( 'mean' , ascending = False ) . plot ( kind = 'bar' , figsize = ( 20 , 10 ) )
1581	for col1 in [ 'etecho1' , 'etecho2' , 'etecho3' ] : for col2 in [ 'eviv1' , 'eviv2' , 'eviv3' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
1485	creator = TFExampleCreator ( is_training = False ) nq_lines = jsonl_iterator ( [ FLAGS . predict_file ] ) creator . process_nq_lines ( nq_lines = nq_lines , output_tfrecord = FLAGS . test_tf_record , max_examples = 0 , collect_nq_features = False )
970	train , test = train . align ( test , join = 'inner' , axis = 1 ) test = test . drop ( columns = [ 'TARGET' ] ) print ( 'Final Training Shape: ' , train . shape ) print ( 'Final Testing Shape: ' , test . shape )
649	model = Sequential ( ) model . add ( Embedding ( max_features , embedding_dim , embeddings_initializer = tf . keras . initializers . Constant ( embedding_matrix ) , trainable = False ) ) model . add ( Dropout ( 0.2 ) )
175	def LoadImageData ( path ) : xs = [ ] ys = [ ]
852	sns . lmplot ( 'pickup_Elapsed' , 'fare_amount' , hue = 'pickup_Year' , palette = palette , size = 8 , scatter_kws = { 'alpha' : 0.05 } , markers = '.' , fit_reg = False , data = data . sample ( 1000000 , random_state = RSEED ) ) ; plt . title ( 'Fare Amount versus Time Since Start of Records' ) ;
343	def my_generator ( ) : for i in range ( 0 , 3 ) : yield print ( i ) my_gen = my_generator ( )
1015	train = pd . read_csv ( '../input/application_train.csv' ) bureau = pd . read_csv ( '../input/bureau.csv' ) bureau_balance = pd . read_csv ( '../input/bureau_balance.csv' )
174	def LoadImage ( img_path ) : image = color . rgb2gray ( io . imread ( img_path ) ) image_resized = resize ( image , ( IMG_SIZE , IMG_SIZE ) ) return image_resized [ : , : ] / 255.
1575	train_data = pd . read_csv ( '../input/augmented-dataset/aug_train_df_2.csv' ) test_data = pd . read_csv ( '../input/augmented-dataset/aug_test_df_2.csv' ) submission = pd . read_csv ( '../input/stanford-covid-vaccine/sample_submission.csv' )
1570	convert_path = './convert_dir' if not os . path . isdir ( convert_path ) : os . mkdir ( convert_path ) else : pass for f in os . listdir ( sample_path ) : if f [ - 3 : ] == 'dcm' : ds = pydicom . read_file ( sample_path + '/' + f ) img = ds . pixel_array cv2 . imwrite ( convert_path + '/' + f . replace ( '.dcm' , '.png' ) , img ) os . listdir ( convert_path )
481	from keras . preprocessing . text import Tokenizer docs = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( docs )
1367	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) pd_districts . plot ( column = 'inc_per_day' , cmap = 'Reds' , alpha = 0.6 , edgecolor = 'r' , linestyle = '-' , linewidth = 1 , legend = True , ax = ax ) def add_basemap ( ax , zoom , url = 'http://tile.stamen.com/terrain/tileZ/tileX/tileY.png' ) :
1110	train_dfs_sentiment = [ x [ 0 ] for x in dfs_train if isinstance ( x [ 0 ] , pd . DataFrame ) ] train_dfs_metadata = [ x [ 1 ] for x in dfs_train if isinstance ( x [ 1 ] , pd . DataFrame ) ] train_dfs_sentiment = pd . concat ( train_dfs_sentiment , ignore_index = True , sort = False ) train_dfs_metadata = pd . concat ( train_dfs_metadata , ignore_index = True , sort = False ) print ( train_dfs_sentiment . shape , train_dfs_metadata . shape )
1119	print ( "True Distribution:" ) print ( pd . value_counts ( X_train [ 'AdoptionSpeed' ] , normalize = True ) . sort_index ( ) ) print ( "\nTrain Predicted Distribution:" ) print ( pd . value_counts ( train_predictions , normalize = True ) . sort_index ( ) ) print ( "\nTest Predicted Distribution:" ) print ( pd . value_counts ( test_predictions , normalize = True ) . sort_index ( ) )
200	import matplotlib . pyplot as plt from itertools import combinations_with_replacement from matplotlib . colors import ListedColormap import warnings warnings . filterwarnings ( 'ignore' )
745	surface_names = df_train_y [ 'surface' ] . unique ( ) num_surfaces = len ( surface_names ) surface_to_numeric = dict ( zip ( surface_names , range ( num_surfaces ) ) ) print ( 'Convert to numbers: ' , surface_to_numeric )
1658	selected_text = tokenizer . convert_tokens_to_string ( tokenized_text [ start_position : end_position + 1 ] ) for original_token in df_test [ 'text' ] . iloc [ i ] . split ( ) : tokenized_form = tokenizer . convert_tokens_to_string ( tokenizer . tokenize ( original_token ) ) selected_text = selected_text . replace ( tokenized_form , original_token , 1 ) df_test [ 'selected_text' ] . iloc [ i ] = selected_text
1245	token2int = { x : i for i , x in enumerate ( '().ACGUBEHIMSX' ) } train_inputs = preprocess_inputs ( train , token2int ) train_labels = pandas_list_to_array ( train [ pred_cols ] )
1375	df [ "diff_V319_V320" ] = np . zeros ( df . shape [ 0 ] ) df [ "diff_V320_V321" ] = np . zeros ( df . shape [ 0 ] ) df [ "diff_V319_V321" ] = np . zeros ( df . shape [ 0 ] )
245	country_name = "Andorra" shift = 7 day_start = 39 + shift dates_list2 = dates_list [ shift : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , shift )
1699	for sample in task : i = np . array ( sample [ 'input' ] ) o = np . array ( sample [ 'output' ] )
1704	del best_candidates [ best_score ] best_candidates [ score ] = candidate is_uncomparable = False if product_less ( best_score , score ) or best_score == score : is_uncomparable = False if is_uncomparable : best_candidates [ score ] = candidate
1502	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_tfrecord if labeled else read_unlabeled_tfrecord , num_parallel_calls = AUTO ) return dataset def data_augment ( image , label ) :
246	data_pred = data [ data . ForecastId != - 1 ] [ [ 'Country_Region' , 'Province_State' , 'Day_num' , 'ForecastId' ] ] data_pred = data_pred . loc [ data_pred [ 'Day_num' ] >= day_start ] data_pred [ 'Predicted_ConfirmedCases' ] = [ 0 ] * len ( data_pred ) data_pred [ 'Predicted_Fatalities' ] = [ 0 ] * len ( data_pred ) print ( "Currently running Logistic Regression for all countries" )
1212	smoothing = 0 plt . figure ( figsize = ( 15 , 10 ) ) for phase in range ( 3 ) : sig = signals [ 1 , phase , : ] filtered = signal . qspline1d ( sig , smoothing ) plt . plot ( sig , label = f'Phase {phase} Raw' ) plt . plot ( filtered , label = f'Phase {phase} Filtered' ) plt . legend ( ) plt . title ( f"Applying Quadratic Spline, Smoothing: {smoothing}" , size = 15 ) plt . show ( )
1342	ratios = np . zeros ( 5 ) for i , ( deg , err ) in enumerate ( zip ( mes_cols , err_cols ) ) : ratio = ( np . array ( sample [ deg ] ) / np . array ( sample [ err ] ) ) . mean ( ) ratios [ i ] = ratio return ratios . mean ( )
1214	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_tfrecord if labeled else read_unlabeled_tfrecord , num_parallel_calls = AUTO )
330	while True : batch = [ ] k = 0
1207	x_train = np . load ( '../input/reducing-image-sizes-to-32x32/X_train.npy' ) x_test = np . load ( '../input/reducing-image-sizes-to-32x32/X_test.npy' ) y_train = np . load ( '../input/reducing-image-sizes-to-32x32/y_train.npy' ) print ( 'x_train shape:' , x_train . shape ) print ( x_train . shape [ 0 ] , 'train samples' ) print ( x_test . shape [ 0 ] , 'test samples' )
1807	members = pd . read_csv ( '../input/members_v3.csv' ) change_datatype ( members ) print ( "Memo of members: " ) memo ( members ) trans = pd . read_csv ( '../input/transactions.csv' ) trans = trans . append ( pd . read_csv ( '../input/transactions_v2.csv' ) ) trans . index = range ( len ( trans ) ) change_datatype ( trans ) print ( "Memo of trans: " ) memo ( trans )
353	predictions = np . zeros ( len ( scaled_test_X ) ) n_fold = 5 folds = KFold ( n_splits = n_fold , shuffle = True , random_state = 42 ) fold_importance_df = pd . DataFrame ( ) fold_importance_df [ "Feature" ] = scaled_train_X . columns print ( 'ok' )
513	df_ConferenceTourneyGames . head ( ) ConferenceTourney_SummaryW = pd . DataFrame ( ) ConferenceTourney_SummaryL = pd . DataFrame ( ) ConferenceTourney_SummaryW [ 'WINS' ] = df_ConferenceTourneyGames [ 'WTeamID' ] . groupby ( [ df_ConferenceTourneyGames [ 'Season' ] , df_ConferenceTourneyGames [ 'WTeamID' ] ] ) . count ( ) ConferenceTourney_SummaryL [ 'LOSSES' ] = df_ConferenceTourneyGames [ 'LTeamID' ] . groupby ( [ df_ConferenceTourneyGames [ 'Season' ] , df_ConferenceTourneyGames [ 'LTeamID' ] ] ) . count ( )
786	df . loc [ : n , : ] . plot . barh ( y = 'importance_normalized' , x = 'feature' , color = 'darkgreen' , edgecolor = 'k' , figsize = ( 12 , 8 ) , legend = False , linewidth = 2 ) plt . xlabel ( 'Normalized Importance' , size = 18 ) ; plt . ylabel ( '' ) ; plt . title ( f'{n} Most Important Features' , size = 18 ) plt . gca ( ) . invert_yaxis ( ) if threshold :
201	def render_neato ( s , format = 'png' , dpi = 100 ) : p = subprocess . Popen ( [ 'neato' , '-T' , format , '-o' , '/dev/stdout' , '-Gdpi={}' . format ( dpi ) ] , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) image , _ = p . communicate ( bytes ( s , encoding = 'utf-8' ) ) return image
1707	program = build_model ( task [ 'train' ] , verbose = True ) print ( ) if program is None : print ( "No program was found" ) else : print ( "Found program:" , program_desc ( program ) )
432	i = 0 result = [ ] step_size = 50000 for j in tqdm ( range ( int ( np . ceil ( test . shape [ 0 ] / 50000 ) ) ) ) : result . append ( np . expm1 ( sum ( [ model . predict ( test . iloc [ i : i + step_size ] ) for model in models ] ) / folds ) ) i += step_size
1819	market_df = market_df . loc [ market_valid_indecies ] return news_valid_indecies def join_market_news ( market_df , news_df , nulls = False ) :
282	df_data = pd . read_csv ( '../input/train_labels.csv' ) df_data [ df_data [ 'id' ] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' ] df_data [ df_data [ 'id' ] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' ] print ( df_data . shape )
1690	xs = [ x for x in xs if len ( x . reshape ( - 1 ) ) > 0 ] return list ( sorted ( xs , key = lambda x : ( x > 0 ) . sum ( ) ) ) def reverse ( x ) :
1564	for c in test_df . columns : if c == 'mo_ye' : continue if test_df [ c ] . dtype == 'object' : lbl = preprocessing . LabelEncoder ( ) lbl . fit ( list ( test_df [ c ] . values ) ) test_df [ c ] = lbl . transform ( list ( test_df [ c ] . values ) ) test_df [ 'mo_ye' ] = test_df [ 'mo_ye' ] . apply ( lambda x : 100 * pd . to_datetime ( x ) . year + pd . to_datetime ( x ) . month )
1334	x = inputs if self . _block_args . expand_ratio != 1 : x = relu_fn ( self . _bn0 ( self . _expand_conv ( inputs ) ) ) x = relu_fn ( self . _bn1 ( self . _depthwise_conv ( x ) ) )
57	def seed_everything ( seed ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( 43 )
960	feature_names = ft . dfs ( entityset = es , target_entity = 'app' , trans_primitives = default_trans_primitives , agg_primitives = default_agg_primitives , max_depth = 2 , features_only = True ) print ( '%d Total Features' % len ( feature_names ) )
431	target = train_processed [ 'meter_reading' ] train = train_processed . drop ( [ 'meter_reading' ] , axis = 1 ) test = test_processed . drop ( [ 'row_id' ] , axis = 1 )
663	date_agg_2 = train_agg . groupby ( level = 1 ) . sum ( ) date_agg_2 . columns = ( 'bookings' , 'total' ) date_agg_2 . index . name = 'Year' date_agg_2 . plot ( kind = 'bar' , stacked = 'True' )
762	plt . scatter ( counts [ x ] , counts [ y ] , edgecolor = 'k' , color = 'lightgreen' , s = 100 * np . sqrt ( counts [ 'raw_count' ] ) , marker = 'o' , alpha = 0.6 , linewidth = 1.5 ) if annotate :
891	train = train . drop ( columns = to_drop ) test = test . drop ( columns = to_drop ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
820	model = RandomForestClassifier ( max_depth = None , n_estimators = 10 ) model . fit ( train_selected , train_labels ) estimator_nonlimited = model . estimators_ [ 5 ] export_graphviz ( estimator_nonlimited , out_file = 'tree_nonlimited.dot' , feature_names = train_selected . columns , class_names = [ 'extreme' , 'moderate' , 'vulnerable' , 'non-vulnerable' ] , rounded = True , proportion = False , precision = 2 )
719	model . save ( 'UNET.h5' , include_optimizer = False ) f = open ( "preprocess.dill" , "wb" ) dill . dump ( preprocess , f ) f . close classify_model . save ( 'classify_model.h5' , include_optimizer = False ) f = open ( "classify_preprocess.dill" , "wb" ) dill . dump ( classify_preprocess , f ) f . close if mode != 'train' :
1157	np . set_printoptions ( threshold = 15 , linewidth = 80 ) def batch_to_numpy_images_and_labels ( data ) : images , labels = data numpy_images = images . numpy ( ) numpy_labels = labels . numpy ( ) if numpy_labels . dtype == object : numpy_labels = [ None for _ in enumerate ( numpy_images ) ]
784	model = RandomForestClassifier ( n_estimators = 100 , random_state = 10 , n_jobs = - 1 ) cv_score = cross_val_score ( model , train_set , train_labels , cv = 10 , scoring = scorer ) print ( f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}' )
1071	face_image = image [ y : y + h , x : x + w ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 ) ) plt . grid ( False ) ax . xaxis . set_visible ( False ) ax . yaxis . set_visible ( False ) ax . imshow ( face_image )
215	data_tr = xgb . DMatrix ( Xtrain , label = Ztrain ) data_cv = xgb . DMatrix ( Xval , label = Zval ) evallist = [ ( data_tr , 'train' ) , ( data_cv , 'valid' ) ]
1269	else : unique_colors , counts = np . unique ( image , return_counts = True ) background = unique_colors [ np . argmax ( counts ) ] return background def check_pairs ( self , inx_pairs , this_pair , return_inx = False ) :
1720	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
410	clf = OneVsRestClassifier ( SGDClassifier ( loss = 'log' , alpha = 0.00001 , penalty = 'l2' ) ) clf . fit ( X_train_multilabel , y_train ) y_pred = clf . predict ( X_test_multilabel )
176	xs = [ random . randint ( 0 , X_train . shape [ 0 ] - 1 ) for _ in range ( 9 ) ] print ( "XS " , xs ) plot_images ( X_train [ xs ] )
291	test_path = 'test_dir' test_gen = datagen . flow_from_directory ( test_path , target_size = ( IMAGE_SIZE , IMAGE_SIZE ) , batch_size = 1 , class_mode = 'categorical' , shuffle = False )
873	headers = [ 'loss' , 'hyperparameters' , 'iteration' , 'runtime' , 'score' ] writer . writerow ( headers ) of_connection . close ( )
618	areas = [ ] for c in contours : areas . append ( cv2 . contourArea ( c ) )
1639	train [ [ 'click_rnd' , 'is_attributed' ] ] . groupby ( [ 'click_rnd' ] , as_index = True ) . count ( ) . plot ( ) plt . title ( 'HOURLY CLICK FREQUENCY' ) ; plt . ylabel ( 'Number of Clicks' ) ; train [ [ 'click_rnd' , 'is_attributed' ] ] . groupby ( [ 'click_rnd' ] , as_index = True ) . mean ( ) . plot ( ) plt . title ( 'HOURLY CONVERSION RATIO' ) ; plt . ylabel ( 'Converted Ratio' ) ;
1451	yhat = model . predict ( testX ) yhat_inverse = scaler . inverse_transform ( yhat . reshape ( - 1 , 1 ) ) testY_inverse = scaler . inverse_transform ( testY . reshape ( - 1 , 1 ) )
355	dup_diff_target = dup [ ( dup [ 'mean' ] != 0.0 ) & ( dup [ 'mean' ] != 1.0 ) ] len_dup_diff_target = len ( dup_diff_target ) print ( 'NUmber of duplicate clicks with different target values in train data: ' , len_dup_diff_target )
749	def process_det ( index , outputs , score_threshold = 0.5 ) : boxes = outputs [ index ] [ 'boxes' ] . data . cpu ( ) . numpy ( ) scores = outputs [ index ] [ 'scores' ] . data . cpu ( ) . numpy ( ) boxes = ( boxes ) . clip ( min = 0 , max = 1023 ) . astype ( int ) indexes = np . where ( scores > score_threshold ) boxes = boxes [ indexes ] scores = scores [ indexes ] return boxes , scores
1426	maxlen = max ( [ len ( s ) for s in clean_lower ] ) minlen = min ( [ len ( s ) for s in clean_lower ] ) print ( maxlen ) print ( minlen )
309	embedding_matrix = np . zeros ( ( vocab_size , 300 ) ) for word , i in t . word_index . items ( ) : embedding_vector = embeddings_index . get ( word ) if embedding_vector is not None : embedding_matrix [ i ] = embedding_vector
1780	plt . subplot ( 211 ) wc = WordCloud ( background_color = "black" , max_words = 10000 , mask = hcmask , stopwords = STOPWORDS , max_font_size = 40 ) wc . generate ( " " . join ( eap ) ) plt . title ( "Edgar Allen Poe (The Raven)" ) plt . imshow ( wc . recolor ( colormap = 'PuBu' , random_state = 17 ) , alpha = 0.9 ) plt . axis ( 'off' )
1427	num_workers = multiprocessing . cpu_count ( ) context_size = 7 downsampling = 1e-3 seed = 1
86	print ( 'There are ' + str ( y . count ( 1 ) ) + ' fake train samples' ) print ( 'There are ' + str ( y . count ( 0 ) ) + ' real train samples' ) print ( 'There are ' + str ( val_y . count ( 1 ) ) + ' fake val samples' ) print ( 'There are ' + str ( val_y . count ( 0 ) ) + ' real val samples' )
462	import numpy as np import pandas as pd from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import train_test_split , StratifiedKFold , KFold from bayes_opt import BayesianOptimization from datetime import datetime from sklearn . metrics import precision_score , recall_score , confusion_matrix , accuracy_score , roc_auc_score , f1_score , roc_curve , auc , precision_recall_curve from sklearn import metrics from sklearn import preprocessing import lightgbm as lgb import warnings warnings . filterwarnings ( "ignore" ) import itertools from scipy import interp import seaborn as sns import matplotlib . pyplot as plt from matplotlib import rcParams
835	data = data . loc [ data [ 'pickup_latitude' ] . between ( 40 , 42 ) ] data = data . loc [ data [ 'pickup_longitude' ] . between ( - 75 , - 72 ) ] data = data . loc [ data [ 'dropoff_latitude' ] . between ( 40 , 42 ) ] data = data . loc [ data [ 'dropoff_longitude' ] . between ( - 75 , - 72 ) ] print ( f'New number of observations: {data.shape[0]}' )
1309	[ x1 , x2 , x3 , x4 ] = sess . run ( [ num_frames , video_matrix , batch_labels , batch_frames ] ) [ z1 , z2 ] = sess . run ( [ labels , num_frames ] ) vid_byte = sess . run ( batch_video_ids ) vid = vid_byte [ 0 ] . decode ( ) print ( 'vid = %s' % vid )
1327	def __init__ ( self , in_channels , out_channels , kernel_size , image_size = None , ** kwargs ) : super ( ) . __init__ ( in_channels , out_channels , kernel_size , ** kwargs ) self . stride = self . stride if len ( self . stride ) == 2 else [ self . stride [ 0 ] ] * 2
871	results = objective ( sample ( space ) ) print ( 'The cross validation loss = {:.5f}.' . format ( results [ 'loss' ] ) ) print ( 'The optimal number of estimators was {}.' . format ( results [ 'hyperparameters' ] [ 'n_estimators' ] ) )
1385	iscrowd = torch . zeros ( ( records . shape [ 0 ] , ) , dtype = torch . int64 ) target = { } target [ 'boxes' ] = boxes target [ 'labels' ] = labels
1598	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
398	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yT , yR_pred ) print ( confusion )
1583	for col1 in [ 'lugar1' , 'lugar2' , 'lugar3' , 'lugar4' , 'lugar5' , 'lugar6' ] : for col2 in [ 'instlevel1' , 'instlevel2' , 'instlevel3' , 'instlevel4' , 'instlevel5' , 'instlevel6' , 'instlevel7' , 'instlevel8' , 'instlevel9' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
1659	for i in tqdm ( range ( len ( df_test ) ) ) : if df_test [ 'sentiment' ] . iloc [ i ] == 'neutral' : df_test [ 'selected_text' ] . iloc [ i ] = df_test [ 'text' ] . iloc [ i ] else : pass
1154	rep = [ ] for dataset in datasets : lines = str ( dataset ) . split ( "\n" ) lines = [ line + "\n" for line in lines ] rep . append ( lines ) diff = difflib . Differ ( ) for line in diff . compare ( rep [ 0 ] , rep [ 1 ] ) : if line [ 0 ] != "?" : print ( line )
1178	plt . figure ( figsize = ( 5 , 5 ) ) dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/train/ID00009637202177434476278/' import pydicom as dcm plt . imshow ( dcm . dcmread ( dir + os . listdir ( dir ) [ 0 ] ) . pixel_array )
1656	for col in df . columns : dtype = str ( df [ col ] . dtype ) data_type . append ( dtype ) output [ 'Types' ] = data_type return ( np . transpose ( output ) ) else : return ( False )
963	feature_matrix_spec , feature_names_spec = ft . dfs ( entityset = es , target_entity = 'app' , agg_primitives = [ 'sum' , 'count' , 'min' , 'max' , 'mean' , 'mode' ] , max_depth = 2 , features_only = False , verbose = True )
943	model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] print ( 'ROC AUC from {} on test data = {:.5f}.' . format ( name , roc_auc_score ( test_labels , preds ) ) )
352	DATA_DIR = '../input/gplearn-data' submission = pd . read_csv ( os . path . join ( '../input/LANL-Earthquake-Prediction' , 'sample_submission.csv' ) , index_col = 'seg_id' ) scaled_train_X = pd . read_csv ( os . path . join ( DATA_DIR , 'scaled_train_X_AF0.csv' ) ) scaled_test_X = pd . read_csv ( os . path . join ( DATA_DIR , 'scaled_test_X_AF0.csv' ) ) train_y = pd . read_csv ( os . path . join ( DATA_DIR , 'train_y_AF0.csv' ) ) predictions = np . zeros ( len ( scaled_test_X ) ) print ( 'ok' )
62	for col in catvar : sns . countplot ( train [ col ] ) sns . plt . title ( col ) plt . show ( )
30	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in the train data set:' , train . shape ) print ( 'Number of rows and columns in the test data set:' , test . shape )
194	ret , threshold = cv2 . threshold ( blackhat , 10 , 255 , cv2 . THRESH_BINARY ) plt . subplot ( l , 5 , ( i * 5 ) + 4 ) plt . imshow ( threshold ) plt . axis ( 'off' ) plt . title ( 'threshold : ' + image_name )
868	x = sample ( space ) subsample = x [ 'boosting_type' ] . get ( 'subsample' , 1.0 ) x [ 'boosting_type' ] = x [ 'boosting_type' ] [ 'boosting_type' ] x [ 'subsample' ] = subsample x
1292	model = SEResNet154 ( ) for layer in model . layers : layer . trainable = True model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( 1e-3 ) )
155	plt . figure ( figsize = ( 10 , 5 ) ) df . resample ( 'H' ) . is_attributed . mean ( ) . plot ( ) plt . title ( 'Download rate evolution over the day' , fontsize = 15 ) plt . xlabel ( 'Time' ) plt . ylabel ( 'Download rate' )
967	ax . barh ( list ( reversed ( list ( df . index [ : 15 ] ) ) ) , df [ 'importance_normalized' ] . head ( 15 ) , align = 'center' , edgecolor = 'k' )
1446	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_tfrecord if labeled else read_unlabeled_tfrecord , num_parallel_calls = AUTO )
983	bureau [ 'bureau_credit_application_date' ] = start_date + bureau [ 'DAYS_CREDIT' ] bureau [ 'bureau_credit_end_date' ] = start_date + bureau [ 'DAYS_CREDIT_ENDDATE' ] bureau [ 'bureau_credit_close_date' ] = start_date + bureau [ 'DAYS_ENDDATE_FACT' ] bureau [ 'bureau_credit_update_date' ] = start_date + bureau [ 'DAYS_CREDIT_UPDATE' ]
886	train_bureau = pd . read_csv ( '../input/home-credit-manual-engineered-features/train_bureau_raw.csv' , nrows = 1000 ) test_bureau = pd . read_csv ( '../input/home-credit-manual-engineered-features/test_bureau_raw.csv' , nrows = 1000 ) train_previous = pd . read_csv ( '../input/home-credit-manual-engineered-features/train_previous_raw.csv' , nrows = 1000 ) test_previous = pd . read_csv ( '../input/home-credit-manual-engineered-features/test_previous_raw.csv' , nrows = 1000 )
461	start_time = timer ( None ) random_search . fit ( X , Y ) timer ( start_time )
83	df_text = pd . read_csv ( '../input/training_text' , sep = '\|\|' , engine = 'python' , skiprows = 1 , names = [ 'ID' , 'Text' ] ) . set_index ( 'ID' ) df_text . head ( )
400	def im_convert ( tensor ) : image = tensor . cpu ( ) . clone ( ) . detach ( ) . numpy ( ) image = image . transpose ( 1 , 2 , 0 ) image = image * np . array ( ( 0.5 , 0.5 , 0.5 ) ) + np . array ( ( 0.5 , 0.5 , 0.5 ) ) image = image . clip ( 0 , 1 ) return image
1130	train_loader = data . DataLoader ( dataset_train , batch_size = 32 , shuffle = True , num_workers = 4 , pin_memory = True )
422	bold ( '**READINGS HIGHEST DURING THE MIDDLE OF THE DAY**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( 'hour' ) . meter_reading . sum ( ) temp_df . plot ( linewidth = 5 , color = 'teal' ) plt . xlabel ( 'Reading Hour' , fontsize = 15 ) plt . ylabel ( 'Meter Reading' ) plt . show ( )
667	df_hmp [ 'log1p_Demanda_uni_equil_sum' ] = np . log1p ( df_hmp . Demanda_uni_equil_sum ) g = sns . FacetGrid ( df_hmp , row = 'Semana' ) g = g . map ( sns . distplot , 'log1p_Demanda_uni_equil_sum' )
772	x = np . array ( range ( - 19 , 20 ) ) y = 2 * np . sin ( x ) plot_corrs ( x , y )
1587	binary_cat_features = [ col for col in train . columns if train [ col ] . value_counts ( ) . shape [ 0 ] == 2 ] object_features = [ 'edjefe' , 'edjefa' ] categorical_feats = binary_cat_features + object_features
588	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "roomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Room Count ",title=" Room Count Vs Log Error " )
1045	df_by_loan = df_counts . merge ( df_agg , on = group_vars [ 0 ] , how = 'outer' ) gc . enable ( ) del df_agg , df_counts gc . collect ( )
13	train = pd . read_csv ( "../input/train.csv" , parse_dates = [ "first_active_month" ] ) test = pd . read_csv ( "../input/test.csv" , parse_dates = [ "first_active_month" ] ) print ( "{} observations and {} features in train set." . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( "{} observations and {} features in test set." . format ( test . shape [ 0 ] , test . shape [ 1 ] ) )
1030	ax . barh ( list ( reversed ( list ( df . index [ : 15 ] ) ) ) , df [ 'importance_normalized' ] . head ( 15 ) , align = 'center' , edgecolor = 'k' )
793	from sklearn . feature_selection import RFECV estimator = RandomForestClassifier ( random_state = 10 , n_estimators = 100 , n_jobs = - 1 ) selector = RFECV ( estimator , step = 1 , cv = 3 , scoring = scorer , n_jobs = - 1 )
1010	columns . append ( '%s_%s_%s' % ( df_name , var , stat ) ) agg . columns = columns return agg
710	auc = roc_auc_score ( train [ 'target' ] , oof_QDA_PL ) print ( 'Pseudo Labeled QDA scores CV =' , round ( auc , 8 ) ) sub_QDA_PL = pd . read_csv ( '../input/sample_submission.csv' ) sub_QDA_PL [ 'target' ] = preds_QDA_PL sub_QDA_PL . to_csv ( 'submission_QDA_PL.csv' , index = False ) oof_preds_QDA_PL = train [ [ 'id' , 'target' ] ] . copy ( ) oof_preds_QDA_PL [ 'target' ] = oof_QDA_PL oof_preds_QDA_PL . to_csv ( 'oof_preds_QDA_PL.csv' , index = False )
448	bold ( '**Updated train data for modelling:**' ) display ( df_train . head ( 3 ) ) bold ( '**Updated test data for modelling:**' ) display ( df_test . head ( 3 ) )
28	clf = SVC ( probability = True , kernel = 'poly' , degree = 4 , gamma = 'auto' ) clf . fit ( train3 [ train_index , : ] , train2 . loc [ train_index ] [ 'target' ] ) oof_svm [ idx1 [ test_index ] ] = clf . predict_proba ( train3 [ test_index , : ] ) [ : , 1 ] preds_svm [ idx2 ] += clf . predict_proba ( test3 ) [ : , 1 ] / skf . n_splits
1039	previous_counts = agg_categorical ( previous , 'SK_ID_CURR' , 'previous' ) print ( 'Previous counts shape: ' , previous_counts . shape ) previous_counts . head ( )
1742	train [ "target" ] = np . log ( train . price + 1 ) target_scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) ) train [ "target" ] = target_scaler . fit_transform ( train . target . values . reshape ( - 1 , 1 ) ) pd . DataFrame ( train . target ) . hist ( )
954	app_train [ 'set' ] = 'train' app_test [ 'set' ] = 'test' app_test [ "TARGET" ] = np . nan
1728	start_time = time . time ( ) model . train ( ) avg_loss = 0. for i , ( x_batch , y_batch , index ) in enumerate ( train_loader ) :
1759	feature_matrix , feature_defs = ft . dfs ( entityset = es , target_entity = 'applications' , drop_contains = [ 'SK_ID_PREV' ] , max_depth = 2 , verbose = True )
654	def read_test_data ( filepath ) : test_df = pd . read_csv ( filepath ) return test_df
186	plt . figure ( figsize = ( 10 , 10 ) ) sns . boxplot ( x = train . shipping , y = train . price , showfliers = False , orient = 'v' ) plt . title ( 'Does shipping depend of prices ?' , fontsize = 25 ) plt . xlabel ( 'Shipping fee paid by seller (1) or by buyer (0)' , fontsize = 20 ) plt . ylabel ( 'Price without outliers' , fontsize = 20 )
123	def clean_up_text_with_all_process ( text ) : text = text . lower ( ) text = clean_contractions ( text ) text = clean_special_chars ( text ) text = clean_small_caps ( text ) return text
1601	total = application_train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( application_train . isnull ( ) . sum ( ) / application_train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_application_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_application_train_data . head ( 20 )
1481	fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 8 , 4 ) ) plt . subplots_adjust ( wspace = 0.5 , hspace = 0.5 ) sns . distplot ( train . cod_prov [ train . cod_prov >= 0 ] , kde = False , ax = axes [ 0 ] , axlabel = 'train cod_prov' ) sns . distplot ( test . cod_prov [ test . cod_prov >= 0 ] , kde = False , ax = axes [ 1 ] , axlabel = 'test cod_prov' )
1677	plt . figure ( figsize = ( 20 , 40 ) ) plt . subplot ( 121 ) plt . title ( "Sample Patient 6 - Normal" ) draw ( parsed [ df [ 'patientId' ] [ 59 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 59 ] ] ) plt . subplot ( 122 ) plt . title ( "Sample Patient 7 - Pleural Effusion" ) draw ( parsed [ df [ 'patientId' ] [ 125 ] ] ) print ( patient_class . loc [ df [ 'patientId' ] [ 125 ] ] )
31	vect_word = TfidfVectorizer ( max_features = 10000 , lowercase = True , analyzer = 'word' , stop_words = 'english' , ngram_range = ( 1 , 2 ) , dtype = np . float32 ) vect_char = TfidfVectorizer ( max_features = 30000 , lowercase = True , analyzer = 'char' , stop_words = 'english' , ngram_range = ( 1 , 6 ) , dtype = np . float32 )
1425	total_tokens_l = [ ] for s in clean_lower : total_tokens_l . extend ( s ) unk_tokens_l = list ( set ( total_tokens_l ) ) print ( "[!]Total number of tokens:" , len ( total_tokens_l ) ) print ( "[!]Total number of unique tokens:" , len ( unk_tokens_l ) )
670	def transpose_df ( df ) : df = df . drop ( [ 'Lat' , 'Long' ] , axis = 1 ) . groupby ( 'Country/Region' ) . sum ( ) . T df . index = pd . to_datetime ( df . index ) return df
1589	truncated = hidden [ : , : pred_len ] out = L . Dense ( 5 , activation = 'linear' ) ( truncated ) model = tf . keras . Model ( inputs = inputs , outputs = out ) model . compile ( tf . keras . optimizers . Adam ( ) , loss = 'mse' ) return model
372	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
1751	es = es . entity_from_dataframe ( entity_id = 'cc_balance' , make_index = True , dataframe = cc_balance_df , index = 'cc_balance_id' )
1091	submission = pd . DataFrame . from_dict ( y_test_pred_rle , orient = 'index' ) submission . index . names = [ 'id' ] submission . columns = [ 'rle_mask' ] return submission def return_padding_borders ( self ) :
1667	stack_submission = sample_submission . copy ( ) preds = model . predict_proba ( X_test ) [ : , 1 ] stack_submission [ 'isFraud' ] = preds
1755	r_bureau_bureaubalance = ft . Relationship ( es [ 'bureau' ] [ 'SK_ID_BUREAU' ] , es [ 'bureau_balance' ] [ 'SK_ID_BUREAU' ] ) es = es . add_relationship ( r_bureau_bureaubalance )
231	dates_overlap = [ '2020-03-19' , '2020-03-20' , '2020-03-21' , '2020-03-22' , '2020-03-23' , '2020-03-24' , '2020-03-25' , '2020-03-26' , '2020-03-27' , '2020-03-28' , '2020-03-29' , '2020-03-30' , '2020-03-31' ] train2 = train . loc [ ~ train [ 'Date' ] . isin ( dates_overlap ) ] all_data = pd . concat ( [ train2 , test ] , axis = 0 , sort = False )
1216	def compute_game_time_stats ( group , col ) : return group [ [ 'installation_id' , col , 'event_count' , 'game_time' ] ] . groupby ( [ 'installation_id' , col ] ) . agg ( [ np . mean , np . sum , np . std ] ) . reset_index ( ) . pivot ( columns = col , index = 'installation_id' )
484	from keras . utils import plot_model from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers . convolutional import Conv2D from keras . layers . pooling import MaxPooling2D
1383	features = data_merged . columns . drop ( [ 'Sales' , 'Date' ] ) from sklearn . model_selection import train_test_split train_x , validate_x , train_y , validate_y = train_test_split ( data_merged [ features ] , np . log ( data_merged [ 'Sales' ] + 1 ) , test_size = 0.2 , random_state = 1 ) train_x . shape , validate_x . shape , train_y . shape , validate_y . shape
1313	feature_importance = pd . DataFrame ( columns = [ 'feature' , 'importance' ] ) feature_importance . feature = X . columns . values feature_importance . importance = model . feature_importance ( ) feature_importance . sort_values ( by = 'importance' , ascending = False , inplace = True ) plt . figure ( figsize = ( 10 , 50 ) ) sns . barplot ( 'importance' , 'feature' , data = feature_importance )
1359	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
1517	def get_training_dataset_raw ( ) : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True , ordered = False ) return dataset raw_training_dataset = get_training_dataset_raw ( ) label_counter = Counter ( ) for images , labels in raw_training_dataset : label_counter . update ( [ labels . numpy ( ) ] ) del raw_training_dataset label_counting_sorted = label_counter . most_common ( ) NUM_TRAINING_IMAGES = sum ( [ x [ 1 ] for x in label_counting_sorted ] ) print ( "number of examples in the original training dataset: {}" . format ( NUM_TRAINING_IMAGES ) ) print ( "labels in the original training dataset, sorted by occurrence" ) label_counting_sorted
682	d = { "unicode" : label_count [ - 11 : - 1 , ] . index . values , "str" : [ unicode_map [ l ] for l in label_count [ - 11 : - 1 , ] . index . values ] } pd . DataFrame ( d )
375	def run_mp_build ( ) : t0 = time . time ( ) num_proc = NUM_THREADS pool = mp . Pool ( processes = num_proc ) results = [ pool . apply_async ( build_fields , args = ( pid , ) ) for pid in range ( NUM_THREADS ) ] output = [ p . get ( ) for p in results ] num_built = sum ( output ) pool . close ( ) pool . join ( ) print ( num_built ) print ( 'Run time: %.2f' % ( time . time ( ) - t0 ) )
1619	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
1800	plt . plot ( np . array ( df_dl . Visits ) ) plt . plot ( trainPredictPlot ) plt . plot ( testPredictPlot ) plt . title ( 'Predicition with Keras' ) plt . show ( )
1199	plt . subplot ( 2 , 2 , 1 ) for K in range ( results . shape [ 0 ] ) : plt . plot ( np . arange ( len ( results . evals_result_ [ K ] ) ) , results . evals_result_ [ K ] , label = 'fold {}' . format ( K ) ) plt . xlabel ( 'Boosting iterations' ) plt . ylabel ( 'RMSE' ) plt . title ( 'Validation loss vs boosting iterations' ) plt . legend ( )
558	elif isinstance ( order , list ) : order = list ( X_train . columns . values ) i = 0 orderdict = { } for k in order : orderdict [ k ] = i i += 1 order = orderdict
1447	sub = pd . DataFrame ( ) sub [ 'id' ] = id_test sub [ 'target' ] = y_pred sub . to_csv ( '2-level_stacked_1.csv' , index = False )
751	plt . style . use ( 'fivethirtyeight' ) plt . rcParams [ 'font.size' ] = 18 plt . rcParams [ 'patch.edgecolor' ] = 'k'
645	tries = 300 scores , types = [ ] , [ ] np . random . seed ( 888 ) for fix in fix_samples : goal_counter = 0
393	best_n_clusters = 5 norm_feat = MinMaxScaler ( ) . fit_transform ( test_feat ) clt = KMeans ( n_clusters = best_n_clusters , random_state = RANDOM_SEED ) . fit ( norm_feat ) test_clusters = pd . DataFrame ( clt . labels_ , columns = [ 'cluster' ] , index = test_meta [ 'signal_id' ] ) stats_df = test_clusters . reset_index ( ) . groupby ( 'cluster' ) . count ( ) stats_df . columns = [ 'count' ] display ( stats_df )
1815	lyft_data = LyftDataset ( data_path = '.' , json_path = '/kaggle/input/3d-object-detection-for-autonomous-vehicles/train_data' , verbose = True )
1480	n_iter = 100 start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_grid_mask ( images ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_grid_mask: {timing}" )
717	df_ = df [ df [ 'timestamp' ] < 100 ] X2 = df_ . drop ( [ 'y' , 'id' , 'timestamp' ] , axis = 1 ) y2 = df_ [ 'y' ] rf4 = RandomForestRegressor ( ) rf4 . fit ( X2 , y2 ) rf4 . score ( X2 , y2 )
778	corr_matrix = ind . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
1364	def change ( addr ) : if addr == 16.0 : return 1 elif addr == 65.0 : return 1 elif addr == np . nan : return np . nan else : return 0 df [ "Asia" ] = df [ "addr2" ] . map ( change )
1546	data_frame [ f'{new_col_name}year' ] = data_frame [ date_col ] . dt . year data_frame [ f'{new_col_name}month' ] = data_frame [ date_col ] . dt . month data_frame [ f'{new_col_name}day' ] = data_frame [ date_col ] . dt . day data_frame [ f'{new_col_name}weeknum' ] = data_frame [ date_col ] . dt . weekofyear data_frame [ f'{new_col_name}dayofweek' ] = data_frame [ date_col ] . dt . dayofweek data_frame [ f'{new_col_name}quarter' ] = data_frame [ date_col ] . dt . quarter return data_frame
617	X_train = X_all_train [ : : 2 ] y_train = y_all_train [ : : 2 ] gc . collect ( ) print ( f'Original sizes: X_all_train: {X_all_train.shape}, y_all_train: {y_all_train.shape}, X_all_test: {X_all_test.shape}' ) print ( f'Reduced train sizes: X_train: {X_train.shape}, y_train: {y_train.shape}' )
1600	cmap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( correlations , cmap = cmap , vmax = 1.0 , center = 0 , fmt = '.2f' , square = True , linewidths = .5 , annot = True , cbar_kws = { "shrink" : .75 } ) plt . show ( )
120	def count_words_from ( series ) : sentences = series . str . split ( ) vocab = { } for sentence in tqdm ( sentences ) : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
1799	testPredictPlot = np . empty_like ( df_dl ) testPredictPlot [ : , : ] = np . nan testPredictPlot [ len ( trainPredict ) + ( look_back * 2 ) + 1 : len ( df_dl ) - 1 , : ] = testPredict
254	gradient_boosting = GradientBoostingRegressor ( ** params ) gradient_boosting . fit ( train , target ) acc_model ( 9 , gradient_boosting , train , test )
192	plt . imshow ( cv2 . cvtColor ( image_resize , cv2 . COLOR_BGR2RGB ) ) plt . axis ( 'off' ) plt . title ( 'Original : ' + image_name ) grayScale = cv2 . cvtColor ( image_resize , cv2 . COLOR_RGB2GRAY ) plt . subplot ( l , 5 , ( i * 5 ) + 2 ) plt . imshow ( grayScale ) plt . axis ( 'off' ) plt . title ( 'GrayScale : ' + image_name )
334	print ( df_train . shape ) print ( df_val . shape ) print ( df_test . shape )
1812	features = np . array ( train [ RF_important_cols ] ) features_test = np . array ( test [ RF_important_cols ] ) response = np . array ( train [ 'is_churn' ] )
903	valid_score = model . best_score_ [ 'valid' ] [ 'auc' ] train_score = model . best_score_ [ 'train' ] [ 'auc' ] valid_scores . append ( valid_score ) train_scores . append ( train_score )
716	rf1 = RandomForestRegressor ( n_jobs = - 1 , verbose = 1 ) rf1 . fit ( test [ 'x' ] [ 0 ] . ix [ : , range ( 34 ) ] , test [ 'y' ] [ 0 ] ) rf1 . score ( test [ 'x' ] [ 0 ] . ix [ : , range ( 34 ) ] , test [ 'y' ] [ 0 ] )
364	scaler = StandardScaler ( ) train0 = pd . DataFrame ( scaler . fit_transform ( train0 ) , columns = train0 . columns ) test0 = pd . DataFrame ( scaler . fit_transform ( test0 ) , columns = test0 . columns )
1097	i = 0 for train_index , valid_index in kf . split ( X = X_train , y = y_train ) : assert len ( np . intersect1d ( train_index , valid_index ) ) == 0 , '\ Train and test indices must not overlap.' print ( 'Running on fold: {}' . format ( i + 1 ) )
589	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "numberofstories" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" No Of Storeys ",title=" No Of Storeys Vs Log Error " )
1765	plt . figure ( figsize = ( 20 , 5 ) ) plt . plot ( - plot1 . compute ( ) , plot2 . compute ( ) ) ; plt . xlabel ( "- Quaketime" ) plt . ylabel ( "Signal" ) plt . title ( "PLOT 0" ) ;
1109	debug = False train_pet_ids = train . PetID . unique ( ) test_pet_ids = test . PetID . unique ( ) if debug : train_pet_ids = train_pet_ids [ : 1000 ] test_pet_ids = test_pet_ids [ : 500 ]
541	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
472	g = plt . figure ( 2 ) precision , recall , _ = precision_recall_curve ( train_df . iloc [ valid_idx ] [ target ] . values , oof [ valid_idx ] ) y_real . append ( train_df . iloc [ valid_idx ] [ target ] . values ) y_proba . append ( oof [ valid_idx ] ) plt . plot ( recall , precision , lw = 2 , alpha = 0.3 , label = 'P|R fold %d' % ( i ) ) i = i + 1
1473	a = tf . cast ( WIDTH * WIDTH / DIM / DIM , tf . float32 ) if len ( label . shape ) == 1 : lab1 = tf . one_hot ( label [ j ] , CLASSES ) lab2 = tf . one_hot ( label [ k ] , CLASSES ) else : lab1 = label [ j , ] lab2 = label [ k , ] labs . append ( ( 1 - a ) * lab1 + a * lab2 )
644	import numpy as np test = pd . read_csv ( "../input/train.csv" ) length = len ( test ) np . random . seed ( 287 ) perfect_sub = np . random . rand ( length ) target = ( perfect_sub > 0.963552 ) . astype ( dtype = int ) print ( "Perfect submission looks like: " , perfect_sub ) print ( "Target vector looks like: " , target ) print ( "Target vector class distibution: " ) counts = pd . Series ( target ) . value_counts ( ) counts / counts . sum ( )
66	folder = '../input/stage_1_train_images' filenames = os . listdir ( folder ) random . shuffle ( filenames )
247	data_pred [ [ 'Predicted_ConfirmedCases' , 'Predicted_Fatalities' ] ] = data_pred [ [ 'Predicted_ConfirmedCases' , 'Predicted_Fatalities' ] ] . apply ( lambda x : np . expm1 ( x ) ) data_pred . replace ( [ np . inf , - np . inf ] , 0 , inplace = True ) return data_pred day_start = 52 data_pred = ridge_reg_basic_all_countries ( data , day_start ) get_submission ( 'submission' , data_pred , 'Predicted_ConfirmedCases' , 'Predicted_Fatalities' ) print ( "Process finished in " , round ( time . time ( ) - ts , 2 ) , " seconds" )
286	train_path = 'base_dir/train_dir' valid_path = 'base_dir/val_dir' test_path = '../input/test' num_train_samples = len ( df_train ) num_val_samples = len ( df_val ) train_batch_size = 10 val_batch_size = 10 train_steps = np . ceil ( num_train_samples / train_batch_size ) val_steps = np . ceil ( num_val_samples / val_batch_size )
1340	train_hsnr = train . query ( 'signal_to_noise > 5' ) sample = train_hsnr . iloc [ 49 , : ] seq = sample [ 'sequence' ] struc = sample [ 'structure' ]
230	def fa ( N , a , b , beta ) : fa = - beta * a * b return fa def fb ( N , a , b , beta , gamma ) : fb = beta * a * b - gamma * b return fb def fc ( N , b , gamma ) : fc = gamma * b return fc
429	train [ 'year_built' ] = np . uint8 ( train [ 'year_built' ] - 1900 , inplace = True ) test [ 'year_built' ] = np . uint8 ( test [ 'year_built' ] - 1900 , inplace = True )
1727	train_loader = torch . utils . data . DataLoader ( train , batch_size = batch_size , shuffle = True ) valid_loader = torch . utils . data . DataLoader ( valid , batch_size = batch_size , shuffle = False ) print ( f'Fold {i + 1}' ) for epoch in range ( n_epochs ) :
1724	def squash ( self , x , axis = - 1 ) : s_squared_norm = ( x ** 2 ) . sum ( axis , keepdim = True ) scale = t . sqrt ( s_squared_norm + T_epsilon ) return x / scale class Capsule_Main ( nn . Module ) : def __init__ ( self , embedding_matrix = None , vocab_size = None ) : super ( Capsule_Main , self ) . __init__ ( ) self . embed_layer = Embed_Layer ( embedding_matrix , vocab_size ) self . gru_layer = GRU_Layer ( )
1556	DESIRED_SIZE = ( 30 , 256 , 256 ) BATCH_SIZE = 256 MASK_ITERATION = 4 clip_bounds = ( - 1000 , 200 ) pre_calculated_mean = 0.02865046213070556
1706	if verbose : print ( "Best candidates lenght:" , len ( best_candidates ) ) random_candidate_score = random . choice ( list ( best_candidates . keys ( ) ) ) print ( "Random candidate score:" , random_candidate_score ) print ( "Random candidate implementation:" , program_desc ( best_candidates [ random_candidate_score ] ) ) return None
1683	glcm = greycomatrix ( patch , [ 1 ] , [ 0 ] , 256 , symmetric = True , normed = True ) for f in props : lf . append ( greycoprops ( glcm , f ) [ 0 , 0 ] )
1525	score = summary . short_span_score - summary . cls_token_score predictions . append ( ( score , i , summary , start_span , end_span ) ) i += 1
277	inpT = [ arr [ shuffleT ] for arr in inpT ] targetT = [ arr [ shuffleT ] for arr in targetT ] model . fit ( inpT , targetT , epochs = epoch , initial_epoch = epoch - 1 , batch_size = BATCH_SIZE , verbose = DISPLAY , callbacks = [ ] , validation_data = ( inpV , targetV ) , shuffle = False )
1197	pred_df = pd . DataFrame ( data = { 'fullVisitorId' : df [ 'fullVisitorId' ] . values , 'transactionRevenue' : df [ 'totals.transactionRevenue' ] . values , 'predictedRevenue' : np . expm1 ( pred ) } )
1082	def __init__ ( self , data_src = '../input/' , image_size = ( 128 , 128 ) , pad_images = False , grayscale = True , load_test_data = True ) : self . data_src = data_src self . image_size = image_size self . pad_images = pad_images self . grayscale = grayscale self . load_test_data = load_test_data self . train_df = None self . test_df = None self . padding_pixels = None self . X_train = [ ] self . y_train = [ ] self . X_test = [ ] self . orig_image_size = ( 101 , 101 )
701	train2 = train [ train [ 'wheezy-copper-turtle-magic' ] == k ] train2p = train2 . copy ( ) ; idx1 = train2 . index test2 = test [ test [ 'wheezy-copper-turtle-magic' ] == k ]
652	import matplotlib as mpl import matplotlib . pyplot as plt import seaborn as sns sns . set ( style = "whitegrid" , color_codes = True )
1693	import re lifted_function . __name__ = re . sub ( '_unlifted$' , '_lifted' , fct . __name__ ) return lifted_function cropToContent = lift ( cropToContent_unlifted ) groupByColor = lift ( groupByColor_unlifted ) splitH = lift ( splitH_unlifted ) negative = lift ( negative_unlifted )
1547	if inplace : data_frame = input_data else : data_frame = input_data . copy ( )
840	plt . figure ( figsize = ( 12 , 6 ) ) for f , grouped in data . groupby ( 'fare-bin' ) : sns . kdeplot ( grouped [ 'manhattan' ] , label = f'{f}' , color = list ( grouped [ 'color' ] ) [ 0 ] ) ; plt . xlabel ( 'degrees' ) ; plt . ylabel ( 'density' ) plt . title ( 'Manhattan Distance by Fare Amount' ) ;
1787	idx = 1 sig = xs . iloc [ : , idx ] idx_error = 3 sig_error = xs . iloc [ : , idx_error ] print ( sig . shape )
540	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
623	df_covid [ 'country' ] = df_covid [ 'country' ] . replace ( 'Mainland China' , 'China' ) df_covid
817	if name == 'tsne' : start = timer ( ) reduction = method . fit_transform ( train_selected ) end = timer ( ) else : start = timer ( ) reduction = method . fit_transform ( train_selected ) end = timer ( ) test_reduction = method . transform ( test_selected )
1252	model . load_weights ( 'model.h5' ) model . compile ( Adam ( 0.0001 ) , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )
579	super ( LogmelFilterBank , self ) . __init__ ( ) self . is_log = is_log self . ref = ref self . amin = amin self . top_db = top_db self . melW = librosa . filters . mel ( sr = sr , n_fft = n_fft , n_mels = n_mels , fmin = fmin , fmax = fmax ) . T
1487	if ckpt_manager . latest_checkpoint : ckpt . restore ( ckpt_manager . latest_checkpoint ) last_epoch = int ( ckpt_manager . latest_checkpoint . split ( "-" ) [ - 1 ] ) print ( f'Latest BertNQ checkpoint restored -- Model trained for {last_epoch} epochs' ) else : print ( 'Checkpoint not found. Train BertNQ from scratch' ) last_epoch = 0
22	numeric_dtypes = [ 'float64' ] numerics = [ ] for i in train . columns : if train [ i ] . dtype in numeric_dtypes : numerics . append ( i )
1424	clean_lower = [ ] for i in tqdm ( range ( len ( total_ ) ) ) : clean_lower . append ( sentence_to_wordlist ( total_ [ i ] ) )
516	df_team_conferences_stage = pd . merge ( left = df_team_conferences_stage , right = df_team_conference_strength , how = 'left' , on = [ 'Season' , 'ConfAbbrev' ] ) df_team_conferences_stage . drop ( labels = [ 'OrdinalRank' , 'ConfAbbrev' ] , inplace = True , axis = 1 ) df_team_conferences_stage . tail ( )
451	precisions_for_samples_by_classes = np . zeros ( ( num_samples , num_classes ) ) for sample_num in range ( num_samples ) : pos_class_indices , precision_at_hits = ( _one_sample_positive_class_precisions ( scores [ sample_num , : ] , truth [ sample_num , : ] ) ) precisions_for_samples_by_classes [ sample_num , pos_class_indices ] = ( precision_at_hits ) labels_per_class = np . sum ( truth > 0 , axis = 0 ) weight_per_class = labels_per_class / float ( np . sum ( labels_per_class ) )
1183	config = { "num_classes" : self . num_classes , "weightage" : self . weightage , } base_config = super ( CohenKappa , self ) . get_config ( ) return dict ( list ( base_config . items ( ) ) + list ( config . items ( ) ) ) def reset_states ( self ) :
439	plt . figure ( figsize = ( 15 , 6 ) ) df_train . IntersectionId . value_counts ( ) [ : 50 ] . plot ( kind = 'bar' , color = 'teal' ) plt . xlabel ( "Intersection Number" , fontsize = 18 ) plt . ylabel ( "Count" , fontsize = 18 ) plt . title ( "TOP 50 most commmon IntersectionID's " , fontsize = 22 ) plt . show ( )
877	bayes_results = pd . read_csv ( '../input/home-credit-model-tuning/bayesian_trials_1000.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) random_results = pd . read_csv ( '../input/home-credit-model-tuning/random_search_trials_1000.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) random_results [ 'loss' ] = 1 - random_results [ 'score' ] bayes_params = evaluate ( bayes_results , name = 'Bayesian' ) random_params = evaluate ( random_results , name = 'random' )
878	scores = pd . DataFrame ( { 'ROC AUC' : random_params [ 'score' ] , 'iteration' : random_params [ 'iteration' ] , 'search' : 'Random' } ) scores = scores . append ( pd . DataFrame ( { 'ROC AUC' : bayes_params [ 'score' ] , 'iteration' : bayes_params [ 'iteration' ] , 'search' : 'Bayesian' } ) ) scores [ 'ROC AUC' ] = scores [ 'ROC AUC' ] . astype ( np . float32 ) scores [ 'iteration' ] = scores [ 'iteration' ] . astype ( np . int32 ) scores . head ( )
1314	fig , ax = plt . subplots ( nrows = 10 , ncols = 10 , figsize = ( 50 , 50 ) ) for i in range ( 10 ) : for j in range ( 10 ) : ids = i * 10 + j sns . scatterplot ( X [ 'var_' + str ( ids ) ] , shap_values [ : , ids ] , ax = ax [ i , j ] )
728	train_image_labels = pd . read_csv ( '../input/avito-images-recognized/train_image_labels.csv' , index_col = 'image_id' ) test_image_labels = pd . read_csv ( '../input/avito-images-recognized/test_image_labels.csv' , index_col = 'image_id' ) all_image_labels = pd . concat ( [ train_image_labels , test_image_labels ] , axis = 0 )
1746	prev_app_df [ 'DAYS_LAST_DUE' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_TERMINATION' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_FIRST_DRAWING' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_FIRST_DUE' ] . replace ( 365243 , np . nan , inplace = True ) prev_app_df [ 'DAYS_LAST_DUE_1ST_VERSION' ] . replace ( 365243 , np . nan , inplace = True )
1115	train_cols = X_train . columns . tolist ( ) train_cols . remove ( 'AdoptionSpeed' ) test_cols = X_test . columns . tolist ( ) assert np . all ( train_cols == test_cols )
68	if self . augment and random . random ( ) > 0.5 : img = np . fliplr ( img ) msk = np . fliplr ( msk )
573	fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 13 , 8 ) sn . countplot ( x = "bathrooms" , data = data , ax = ax1 ) data1 = data . groupby ( [ 'bathrooms' , 'interest_level' ] ) [ 'bathrooms' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) data1 [ [ 'low' , 'medium' , "high" ] ] . plot ( kind = 'bar' , stacked = True , ax = ax2 )
870	headers = [ 'loss' , 'hyperparameters' , 'iteration' , 'runtime' , 'score' ] writer . writerow ( headers ) of_connection . close ( )
499	elif extension == '.ahi' : data = np . fromfile ( fid , dtype = np . float32 , count = 2 * nx * ny * nt ) data = data . reshape ( 2 , ny , nx , nt , order = 'F' ) . copy ( ) real = data [ 0 , : , : , : ] . copy ( ) imag = data [ 1 , : , : , : ] . copy ( ) if extension != '.ahi' : return data else : return real , imag
1432	link_count = np . zeros ( [ len ( title_dic ) , len ( title_dic ) ] , dtype = np . int ) node_count = np . zeros ( [ len ( title_dic ) ] ) for i in tqdm ( train . index ) : link_count [ train . loc [ i , 'previous_title' ] ] [ train . loc [ i , 'title' ] ] += 1 node_count [ train . loc [ i , 'title' ] ] += 1
1370	le1 = LabelEncoder ( ) train [ 'PdDistrict' ] = le1 . fit_transform ( train [ 'PdDistrict' ] ) test [ 'PdDistrict' ] = le1 . transform ( test [ 'PdDistrict' ] ) le2 = LabelEncoder ( ) X = train . drop ( columns = [ 'Category' ] ) y = le2 . fit_transform ( train [ 'Category' ] )
54	print ( "The duration of %i %% of the taxi trips was less than 35 minutes." % ( 100 * np . where ( trip_dur_mins < 35 ) [ 0 ] . size / train [ 'trip_duration' ] . size ) ) print ( "The median taxi trip duration was %i minutes." % trip_dur_mins . median ( ) ) print ( "The shortest taxi trip duration was %i second." % train [ 'trip_duration' ] . min ( ) ) print ( "The longest taxi trip duration was %i days and %i hours." % ( np . floor ( trip_dur_mins . max ( ) / ( 60 * 24 ) ) , np . floor ( trip_dur_mins . max ( ) / ( 60 * 24 ) % 1 * 24 ) ) )
1739	plt . hist ( df_train [ 'winPlacePerc' ] ) plt . xlabel ( "winPlacePerc" ) plt . ylabel ( "count" ) plt . title ( 'Distribution of winPlacePerc' )
1469	splitted = feat_bin . apply ( lambda x : pd . Series ( list ( x ) ) . astype ( np . uint8 ) ) splitted . columns = [ feat + '_bin_' + str ( x ) for x in splitted . columns ] bin_df = bin_df . join ( splitted )
1049	if encoding == 'ohe' : features = pd . get_dummies ( features ) test_features = pd . get_dummies ( test_features )
1676	patientId = df [ 'patientId' ] [ 8 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 2 - Lung Opacity" ) draw ( parsed [ patientId ] )
1679	session_type = session [ 'type' ] . iloc [ 0 ] session_title = session [ 'title' ] . iloc [ 0 ] session_title_text = activities_labels [ session_title ]
1341	_measured = mlines . Line2D ( [ ] , [ ] , color = 'blue' , linestyle = 'None' , marker = 'o' , markersize = 15 , label = 'Measured' ) _unmeasured = mlines . Line2D ( [ ] , [ ] , color = 'red' , linestyle = 'None' , marker = 'o' , markersize = 15 , label = 'Unmeasured' ) for i , mes_col in enumerate ( mes_cols ) : measure = np . array ( sample [ mes_col ] )
550	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
647	import numpy as np import pandas as pd import seaborn as sns import matplotlib as plt import tensorflow as tf from tensorflow . keras . preprocessing import text , sequence from tensorflow . keras . models import Sequential from tensorflow . keras . layers import Dense , Dropout , Activation from tensorflow . keras . layers import Embedding from tensorflow . keras . layers import Conv1D , GlobalMaxPooling1D , MaxPooling1D from sklearn . model_selection import train_test_split print ( tf . __version__ )
1305	of [ 'toxic' ] = mybest . toxic . values * 0.8 + of . toxic . values * 0.2 score1 = roc_auc_score ( mybest . toxic . round ( ) . astype ( int ) , of . toxic . values ) score2 = roc_auc_score ( of . toxic . round ( ) . astype ( int ) , mybest . toxic . values ) print ( '%2.4f\t%2.4f' % ( 100 * score1 , 100 * score2 ) ) print ( of . head ( ) ) of . to_csv ( 'submission.csv' , index = False )
831	train . fillna ( 0 , inplace = True ) train . replace ( - np . inf , 0 , inplace = True ) train . replace ( np . inf , 0 , inplace = True ) test . fillna ( 0 , inplace = True ) test . replace ( - np . inf , 0 , inplace = True ) test . replace ( np . inf , 0 , inplace = True )
1709	from sklearn . preprocessing import StandardScaler from sklearn . model_selection import train_test_split from sklearn . preprocessing import LabelEncoder
337	filepath = "model.h5" checkpoint = ModelCheckpoint ( filepath , monitor = 'val_loss' , verbose = 1 , save_best_only = True , mode = 'min' ) callbacks_list = [ checkpoint ] history = model . fit_generator ( generator = train_gen , steps_per_epoch = num_train_batches , epochs = 3 , verbose = 1 , callbacks = callbacks_list , validation_data = val_gen , validation_steps = num_val_batches , class_weight = None , max_queue_size = 2 , workers = 4 , use_multiprocessing = True , shuffle = False , initial_epoch = 0 )
1234	with strategy . scope ( ) : transformer_layer = ( transformers . TFDistilBertModel . from_pretrained ( 'distilbert-base-multilingual-cased' ) ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
494	val_p = [ 'AMT_ANNUITY' , 'AMT_CREDIT' , 'AMT_GOODS_PRICE' , 'HOUR_APPR_PROCESS_START' ] for i in val_p : plt . figure ( figsize = ( 5 , 5 ) ) sns . distplot ( application_train [ i ] . dropna ( ) , kde = True , color = 'g' ) plt . title ( i ) plt . xticks ( rotation = - 90 ) plt . show ( )
678	target = df_train [ 'outliers' ] del df_train [ 'outliers' ] del df_train [ 'target' ]
1354	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
900	else : raise ValueError ( "Encoding must be either 'ohe' or 'le'" ) print ( 'Training Data Shape: ' , features . shape ) print ( 'Testing Data Shape: ' , test_features . shape )
209	idx3 = tf . stack ( [ DIM // 2 - idx2 [ 0 , ] , DIM // 2 - 1 + idx2 [ 1 , ] ] ) d = tf . gather_nd ( image , tf . transpose ( idx3 ) ) return tf . reshape ( d , [ DIM , DIM , 3 ] )
547	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
874	model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] print ( 'ROC AUC from {} on test data = {:.5f}.' . format ( name , roc_auc_score ( test_labels , preds ) ) )
250	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
466	f = plt . figure ( 1 ) fpr , tpr , t = roc_curve ( train_df . iloc [ valid_idx ] [ target ] . values , oof [ valid_idx ] ) tprs . append ( interp ( mean_fpr , fpr , tpr ) ) roc_auc = auc ( fpr , tpr ) aucs . append ( roc_auc ) plt . plot ( fpr , tpr , lw = 2 , alpha = 0.3 , label = 'ROC fold %d (AUC = %0.4f)' % ( i , roc_auc ) )
1266	directory = '/kaggle/input/tensorflow2-question-answering/' test_path = directory + 'simplified-nq-test.jsonl' test = build_test ( test_path ) submission = pd . read_csv ( "../input/tensorflow2-question-answering/sample_submission.csv" ) test . head ( )
373	sta [ length_sta : ] = sta [ length_sta : ] - sta [ : - length_sta ] sta /= length_sta lta [ length_lta : ] = lta [ length_lta : ] - lta [ : - length_lta ] lta /= length_lta
1121	y = train_ . target X = train_ . drop ( [ 'target' ] , axis = 1 ) X_test = test_ . copy ( ) features_to_remove = [ 'first_active_month' ] X = X . drop ( features_to_remove , axis = 1 ) X_test = X_test . drop ( features_to_remove , axis = 1 ) assert np . all ( X . columns == X_test . columns ) del train_ , test_ gc . collect ( )
324	df_train = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftrain.pickle' , 'rb' ) ) df_test = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftest.pickle' , 'rb' ) ) print ( df_train . shape ) print ( df_test . shape )
1251	category_df = train_df [ train_df [ 'category' ] == category ] cat_test_df = test_df [ test_df [ 'category' ] == category ] . copy ( ) print ( '\n' + '=' * 40 ) print ( "CURRENT CATEGORY:" , category ) print ( '-' * 40 ) train_idx , val_idx = train_test_split ( category_df . index , random_state = 2019 , test_size = 0.15 )
138	import torch , torchvision print ( torch . __version__ , torch . cuda . is_available ( ) ) import mmdet print ( mmdet . __version__ ) from mmcv . ops import get_compiling_cuda_version , get_compiler_version print ( get_compiling_cuda_version ( ) ) print ( get_compiler_version ( ) )
1048	credit = pd . read_csv ( '../input/credit_card_balance.csv' ) credit = convert_types ( credit , print_info = True ) credit . head ( )
1284	train_x = train_df . values train_y = train_label . values test_x = test_df . values print ( "shape of train_x :" , train_x . shape ) print ( "shape of train_y :" , train_y . shape ) print ( "shape of test_x :" , test_x . shape )
287	model . load_weights ( 'model.h5' ) val_loss , val_acc = \ model . evaluate_generator ( test_gen , steps = len ( df_val ) ) print ( 'val_loss:' , val_loss ) print ( 'val_acc:' , val_acc )
1417	def get_model ( ) : with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB3 ( input_shape = ( * IMAGE_SIZE , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ tf . keras . metrics . AUC ( ) ] , ) return model
1567	qs = PINBALL_QUANTILE q = tf . constant ( PINBALL_QUANTILE , dtype = tf . float32 ) e = y_true - y_pred v = tf . maximum ( q * e , ( q - 1 ) * e ) return tf . keras . backend . mean ( v )
575	corrMatt = data [ [ "bedrooms" , "bathrooms" , "price" ] ] . corr ( ) mask = np . array ( corrMatt ) mask [ np . tril_indices_from ( mask ) ] = False fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) sn . heatmap ( corrMatt , mask = mask , vmax = .8 , square = True , annot = True )
1320	for col in atoms . columns : if col . startswith ( 'atom_' ) : atoms [ col ] = atoms [ col ] . fillna ( 0 ) . astype ( 'int8' ) atoms [ 'molecule_index' ] = atoms [ 'molecule_index' ] . astype ( 'int32' ) full = add_atoms ( base , atoms ) add_distances ( full ) full . sort_values ( 'id' , inplace = True ) return full , base2
1625	full_table [ 'Active' ] = full_table [ 'Confirmed' ] - full_table [ 'Deaths' ] - full_table [ 'Recovered' ] full_table [ 'Country/Region' ] = full_table [ 'Country/Region' ] . replace ( 'Mainland China' , 'China' ) full_table [ [ 'Province/State' ] ] = full_table [ [ 'Province/State' ] ] . fillna ( '' ) full_table
626	df_china_cases_by_day = df_grouped_china [ df_grouped_china . confirmed > 0 ] df_china_cases_by_day = df_china_cases_by_day . reset_index ( drop = True ) df_china_cases_by_day [ 'day' ] = df_china_cases_by_day . date . apply ( lambda x : ( x - df_china_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_china_cases_by_day = df_china_cases_by_day [ reordered_columns ] df_china_cases_by_day
972	plt . figure ( figsize = ( 10 , 6 ) ) sns . kdeplot ( opt [ 'score' ] , label = 'Bayesian Opt' ) sns . kdeplot ( random [ 'score' ] , label = 'Random Search' ) plt . xlabel ( 'Score (5 Fold Validation ROC AUC)' ) ; plt . ylabel ( 'Density' ) ; plt . title ( 'Random Search and Bayesian Optimization Results' ) ;
261	fig = px . scatter_3d ( commits_df , x = 'FVC_weight' , y = 'Dropout_model' , z = 'LB_score' , color = 'max' , symbol = 'GaussianNoise_stddev' , title = 'Parameters and LB score visualization of OSIC PFP solutions' ) fig . update ( layout = dict ( title = dict ( x = 0.1 ) ) )
1504	x = tf . repeat ( tf . range ( DIM // 2 , - DIM // 2 , - 1 ) , DIM ) y = tf . tile ( tf . range ( - DIM // 2 , DIM // 2 ) , [ DIM ] ) z = tf . ones ( [ DIM * DIM ] , dtype = 'int32' ) idx = tf . stack ( [ x , y , z ] )
610	absolute_path = os . path . join ( root , fname ) filenames . append ( os . path . relpath ( absolute_path , basedir ) ) return classes , filenames class SpeechDirectoryIterator ( Iterator ) :
1218	world_time_stats = compute_game_time_stats ( group_game_time , 'world' ) type_time_stats = compute_game_time_stats ( group_game_time , 'type' ) return ( title_group . join ( event_game_time_group ) . join ( world_time_stats ) . join ( type_time_stats ) . fillna ( 0 ) )
1582	for col1 in [ 'epared1' , 'epared2' , 'epared3' ] : for col2 in [ 'etecho1' , 'etecho2' , 'etecho3' ] : for col3 in [ 'eviv1' , 'eviv2' , 'eviv3' ] : new_col_name = 'new_{}_x_{}_x_{}' . format ( col1 , col2 , col3 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] * df_train [ col3 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ] * df_train [ col3 ]
1777	sns_plot2 = sns . heatmap ( corr , xticklabels = corr . columns , yticklabels = corr . columns )
1654	cols_to_use = corr_df [ ( corr_df [ 'corr_values' ] > 0.05 ) | ( corr_df [ 'corr_values' ] < - 0.05 ) ] . col_labels . tolist ( ) temp_df = train [ cols_to_use ] corrmat = temp_df . corr ( method = 'spearman' ) f , ax = plt . subplots ( figsize = ( 18 , 18 ) )
810	valid_scores . append ( model . best_score_ [ 'valid' ] [ 'macro_f1' ] ) best_estimators . append ( model . best_iteration_ ) run_times . append ( end - start ) score = np . mean ( valid_scores ) score_std = np . std ( valid_scores ) loss = 1 - score run_time = np . mean ( run_times ) run_time_std = np . std ( run_times ) estimators = int ( np . mean ( best_estimators ) ) hyperparameters [ 'n_estimators' ] = estimators
196	plt . imshow ( cv2 . cvtColor ( image_resize , cv2 . COLOR_BGR2RGB ) ) plt . axis ( 'off' ) plt . title ( 'Original : ' + image_name ) grayScale = cv2 . cvtColor ( image_resize , cv2 . COLOR_RGB2GRAY ) plt . subplot ( l , 5 , ( i * 5 ) + 2 ) plt . imshow ( grayScale ) plt . axis ( 'off' ) plt . title ( 'GrayScale : ' + image_name )
1430	test_features = [ ] print ( '[!]Making training features...' ) for i in tqdm ( range ( len ( data_testing ) ) ) : sentence = clean_ [ i + len ( data_training ) ] temp = [ ] for token in sentence : temp . append ( word2vec_ [ token ] . tolist ( ) ) test_features . append ( temp )
574	fig , ( ax1 , ax2 ) = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 13 , 8 ) sn . countplot ( x = "bedrooms" , data = data , ax = ax1 ) data1 = data . groupby ( [ 'bedrooms' , 'interest_level' ] ) [ 'bedrooms' ] . count ( ) . unstack ( 'interest_level' ) . fillna ( 0 ) data1 [ [ 'low' , 'medium' , "high" ] ] . plot ( kind = 'bar' , stacked = True , ax = ax2 )
1795	model1 . fit ( xt , yt ) return model1 , prediction , model0 model0 = AdaBoostRegressor ( n_estimators = 5000 , random_state = 42 , learning_rate = 0.01 ) model1 = AdaBoostRegressor ( n_estimators = 5000 , random_state = 42 , learning_rate = 0.01 ) clr , prediction , clr0 = modelisation ( x_train , y_train , x_test , y_test , xt , yt , model0 , model1 )
1253	checkpoint = ModelCheckpoint ( f'model_{category}.h5' , monitor = 'val_loss' , verbose = 0 , save_best_only = True , save_weights_only = False , mode = 'auto' ) history_category = model . fit_generator ( train_generator , validation_data = val_generator , callbacks = [ checkpoint ] , use_multiprocessing = False , workers = 1 , verbose = 2 , epochs = 10 )
198	ret , threshold = cv2 . threshold ( blackhat , 10 , 255 , cv2 . THRESH_BINARY ) plt . subplot ( l , 5 , ( i * 5 ) + 4 ) plt . imshow ( threshold ) plt . axis ( 'off' ) plt . title ( 'threshold : ' + image_name )
182	plt . figure ( figsize = ( 20 , 20 ) ) sns . boxplot ( y = 'cat2' , x = 'price' , data = top_cat2_full , orient = 'h' ) plt . title ( 'Top 15 second levels categories with highest prices ' , fontsize = 30 ) plt . ylabel ( 'Second level categories' , fontsize = 20 ) plt . xlabel ( 'Price' , fontsize = 20 )
423	bold ( '**MONTHLY READINGS ARE HIGHEST CHANGES BASED ON BUILDING TYPE**' ) temp_df = train . groupby ( [ 'month' , 'primary_use' ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . FacetGrid ( temp_df , col = "primary_use" , col_wrap = 2 , height = 4 , aspect = 2 , sharey = False ) ax . map ( plt . plot , 'month' , 'meter_reading' , color = "teal" , linewidth = 3 ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
742	train_df [ [ 'ID' , 'Subtype' ] ] = train_df [ 'ID' ] . str . rsplit ( pat = '_' , n = 1 , expand = True ) print ( train_df . shape ) train_df . head ( )
1330	block_strings = [ ] for block in blocks_args : block_strings . append ( BlockDecoder . _encode_block_string ( block ) ) return block_strings def efficientnet ( width_coefficient = None , depth_coefficient = None , dropout_rate = 0.2 , drop_connect_rate = 0.2 , image_size = None , num_classes = 1000 ) :
1646	def model_retop ( model , ntop , last_conv_layer_name = 'Conv5' , n_first = 64 ) : convout = model . get_layer ( name = last_conv_layer_name ) . output inputs = [ ] for inp in model . inputs : if 'timeseries' in inp . name : inputs . append ( inp ) dinputs , out = build_top ( ntop , convout , n_first = n_first ) inputs = inputs + dinputs topmodel = Model ( inputs , out ) return topmodel model_retop ( build_model16 ( ) , 8 ) . summary ( )
1280	task_num = np . random . randint ( 1 , 400 ) arc = ARC_solver ( task_num ) arc . plot_task ( ) image = np . array ( train_tasks [ task_num ] [ 'train' ] [ 0 ] [ 'input' ] ) arc . identify_object ( image , method = 2 ) arc . plot_identified_objects ( arc . identified_objects )
1407	logregModel = LogisticRegression ( ) params = { 'C' : np . logspace ( start = - 5 , stop = 3 , num = 9 ) } clf = GridSearchCV ( logregModel , params , scoring = 'neg_log_loss' , refit = True ) clf . fit ( X_train , y_train )
1542	print ( train . iloc [ [ 2 ] ] ) df_train = extract_series ( X_train , 2 , 5 ) df_actual = extract_series ( y_train , 2 , 5 ) lang = X_train . iloc [ 2 , 1 ] score = holiday_model_log ( df_train . copy ( ) , df_actual . copy ( ) , lang , review = True ) print ( "The SMAPE score is : %.5f" % score )
939	results = pd . DataFrame ( columns = [ 'score' , 'params' , 'iteration' ] , index = list ( range ( MAX_EVALS ) ) ) for i in range ( MAX_EVALS ) :
283	df_0 = df_data [ df_data [ 'label' ] == 0 ] . sample ( SAMPLE_SIZE , random_state = 101 ) df_1 = df_data [ df_data [ 'label' ] == 1 ] . sample ( SAMPLE_SIZE , random_state = 101 ) df_data = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_data = shuffle ( df_data ) df_data [ 'label' ] . value_counts ( )
1307	assert max_quantized_value > min_quantized_value quantized_range = max_quantized_value - min_quantized_value scalar = quantized_range / 255.0 bias = ( quantized_range / 512.0 ) + min_quantized_value return feat_vector * scalar + bias
325	df_train = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftrain.pickle' , 'rb' ) ) df_test = pickle . load ( open ( '../input/python-generators-to-reduce-ram-usage-part-1/dftest.pickle' , 'rb' ) ) print ( df_train . shape ) print ( df_test . shape )
1463	class Net ( nn . Module ) : def __init__ ( self , num_classes ) : super ( ) . __init__ ( ) self . model = EfficientNet . from_name ( 'efficientnet-b0' ) self . dense_output = nn . Linear ( 1280 , num_classes ) def forward ( self , x ) : feat = self . model . extract_features ( x ) feat = F . avg_pool2d ( feat , feat . size ( ) [ 2 : ] ) . reshape ( - 1 , 1280 ) return self . dense_output ( feat )
41	n_train_rows , n_train_cols = train . shape n_test_rows , n_test_cols = test . shape print ( '- Training data has {:9,} rows and {:2,} columns.' . format ( * train . shape ) ) print ( '- Testing data has {:9,} rows and {:2,} columns.' . format ( * test . shape ) ) print ( '- There are {:.1f} times more ( ' testing data examples . ' . format ( n_train_rows / n_test_rows , n_train_rows - n_test_rows ) ) print ( "- There are %i missing values in the training data." % train . isnull ( ) . sum ( ) . sum ( ) ) print ( "- There are %i missing values in the testing data." % test . isnull ( ) . sum ( ) . sum ( ) )
826	image_id = 'c14c1e300' image = cv2 . imread ( os . path . join ( BASE_DIR , 'train' , f'{image_id}.jpg' ) , cv2 . IMREAD_COLOR ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )
279	x_3d = pca . fit_transform ( data ) for k in range ( 1 , 8 ) : kmeans = KMeans ( n_clusters = k , random_state = 1 ) . fit ( x_3d ) inertia . append ( np . sqrt ( kmeans . inertia_ ) ) plt . plot ( range ( 1 , 8 ) , inertia , marker = 's' ) ; plt . xlabel ( '$k$' ) plt . ylabel ( '$J(C_k)$' ) ;
1545	if use_col_name : new_col_name = f'{date_col}_' else : new_col_name = ''
781	new_col = [ ] for c in ind_agg . columns . levels [ 0 ] : for stat in ind_agg . columns . levels [ 1 ] : new_col . append ( f'{c}-{stat}' ) ind_agg . columns = new_col ind_agg . head ( )
738	i = test [ test [ 'key_id' ] == 9000052667981386 ] . iloc [ 0 ] [ 'drawing' ] img = draw_cv2 ( ast . literal_eval ( i ) , img_size = 256 ) plt . imshow ( img )
205	import os , random , re , math , time random . seed ( a = 42 ) import numpy as np import pandas as pd import tensorflow as tf import tensorflow . keras . backend as K import efficientnet . tfkeras as efn import PIL from kaggle_datasets import KaggleDatasets from tqdm import tqdm
744	def id_to_filepath ( img_id , img_dir = TRAIN_DIR ) : filepath = f'{img_dir}/{img_id}.dcm' if os . path . exists ( filepath ) : return filepath else : return 'DNE'
834	def ecdf ( x ) : x = np . sort ( x ) n = len ( x ) y = np . arange ( 1 , n + 1 , 1 ) / n return x , y
1592	embed_size = 300 max_features = 50000 maxlen = 100
1306	import sys , os , multiprocessing , csv from urllib import request , error from PIL import Image from io import BytesIO
1139	shap . dependence_plot ( "returnsClosePrevRaw10_lag_3_mean" , shap_values , X_importance )
1026	model . fit ( train_features , train_labels , eval_metric = 'auc' , eval_set = [ ( valid_features , valid_labels ) , ( train_features , train_labels ) ] , eval_names = [ 'valid' , 'train' ] , categorical_feature = cat_indices , early_stopping_rounds = 100 , verbose = 200 )
1227	train = pd . read_csv ( "../input/liverpool-ion-switching/train.csv" ) test = pd . read_csv ( "../input/liverpool-ion-switching/test.csv" ) sub = pd . read_csv ( "../input/liverpool-ion-switching/sample_submission.csv" , dtype = dict ( time = str ) )
1410	X_COL = "var_81" Y_COL = "var_68" Z_COL = "var_108" HUE_COL = "target" N_SAMPLES = 10000 df = train_df . sample ( N_SAMPLES )
354	print ( 'ok' )
1387	sample = self . transforms ( ** sample ) image = sample [ 'image' ] boxes = sample [ 'bboxes' ]
914	df_agg_cat_client = agg_numeric ( df_agg_cat , grandparent_var , '%s_CLIENT' % df_name ) df_info = df_agg_client . merge ( df_agg_cat_client , on = grandparent_var , how = 'outer' ) gc . enable ( ) del df_agg , df_agg_client , df_agg_cat , df_agg_cat_client gc . collect ( )
1486	bert_tokenizer , bert_nq = get_pretrained_model ( FLAGS . model_name ) if not IS_KAGGLE : bert_nq . trainable_variables
1633	import plotly . express as px fig = px . histogram ( df [ [ 'age' , 'gender' , 'hospital_death' , 'bmi' ] ] . dropna ( ) , x = "age" , y = "hospital_death" , color = "gender" , marginal = "box" , hover_data = df [ [ 'age' , 'gender' , 'hospital_death' , 'bmi' ] ] . columns ) fig . show ( )
1346	for task , prediction in tqdm ( zip ( train_tasks , train_predictions ) ) : if input_output_shape_is_same ( task ) : for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
1438	def create_datagen ( ) : return ImageDataGenerator ( zoom_range = 0.15 , fill_mode = 'constant' , cval = 0. , horizontal_flip = True , vertical_flip = True , ) data_generator = create_datagen ( ) . flow ( x_train , y_train , batch_size = BATCH_SIZE , seed = 2019 )
296	df_0 = df_train [ df_train [ 'binary_target' ] == 0 ] df_1 = df_train [ df_train [ 'binary_target' ] == 1 ] . sample ( len ( df_0 ) , random_state = 101 ) df_data = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_data = shuffle ( df_data ) print ( df_data . shape ) df_data . head ( )
447	directions = { 'N' : 0 , 'NE' : 1 / 4 , 'E' : 1 / 2 , 'SE' : 3 / 4 , 'S' : 1 , 'SW' : 5 / 4 , 'W' : 3 / 2 , 'NW' : 7 / 4 }
363	train0b = train0 train_target0b = train0b [ target_name ] train0b = train0b . drop ( [ target_name ] , axis = 1 )
430	from sklearn . preprocessing import LabelEncoder le = LabelEncoder ( ) train [ 'primary_use' ] = le . fit_transform ( train [ 'primary_use' ] ) test [ 'primary_use' ] = le . fit_transform ( test [ 'primary_use' ] )
79	h , w , _ = img . shape interp = np . random . randint ( 0 , 5 ) img = gcv . data . transforms . image . imresize ( img , self . _width , self . _height , interp = interp ) bbox = gcv . data . transforms . bbox . resize ( bbox , ( w , h ) , ( self . _width , self . _height ) )
532	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
1395	for molecule_name in MOLECULE_NAMES [ : 10 ] : graph , labels = get_molecule_graph ( df , molecule_name ) ax = draw_graph ( graph , labels ) ax . set_title ( f"Graph for {molecule_name}" )
937	out_file = 'random_search_trials.csv' of_connection = open ( out_file , 'w' ) writer = csv . writer ( of_connection )
1692	def lifted_function ( xs ) : list_of_results = [ fct ( x ) for x in xs ] return list ( itertools . chain ( * list_of_results ) )
116	learning_rate = [ 0.15 , 0.2 , 0.25 ] max_depth = [ 15 , 20 , 25 ] param_grid = { 'learning_rate' : learning_rate , 'max_depth' : max_depth }
686	n = 1000 train_data_gen = image_generator . flow_from_dataframe ( directory = data_train_dir , dataframe = train_df . sample ( n = n ) , class_mode = 'raw' , x_col = 'id_path' , y_col = 'landmark_id' , batch_size = BATCH_SIZE , shuffle = True , target_size = ( IMG_HEIGHT , IMG_WIDTH ) , )
759	households_leader = train . groupby ( 'idhogar' ) [ 'parentesco1' ] . sum ( ) households_no_head = train . loc [ train [ 'idhogar' ] . isin ( households_leader [ households_leader == 0 ] . index ) , : ] print ( 'There are {} households without a head.' . format ( households_no_head [ 'idhogar' ] . nunique ( ) ) )
992	r_previous_cash = ft . Relationship ( es [ 'previous' ] [ 'SK_ID_PREV' ] , es [ 'cash' ] [ 'SK_ID_PREV' ] ) r_previous_installments = ft . Relationship ( es [ 'previous' ] [ 'SK_ID_PREV' ] , es [ 'installments' ] [ 'SK_ID_PREV' ] ) r_previous_credit = ft . Relationship ( es [ 'previous' ] [ 'SK_ID_PREV' ] , es [ 'credit' ] [ 'SK_ID_PREV' ] )
1060	train_imgs , val_imgs = train_test_split ( selected_imgs , test_size = 0.15 , stratify = selected_imgs [ 'has_ship' ] , random_state = 69278 ) train_fnames = train_imgs [ 'ImageId' ] . values val_fnames = val_imgs [ 'ImageId' ] . values
1168	train_clean [ 'TransactionAmt' ] = np . log ( train_clean [ 'TransactionAmt' ] + 1 ) train_clean [ 'dist1' ] = np . log ( train_clean [ 'dist1' ] + 1 ) train_clean [ 'dist2' ] = np . log ( train_clean [ 'dist2' ] + 1 ) test_clean [ 'TransactionAmt' ] = np . log ( test_clean [ 'TransactionAmt' ] + 1 ) test_clean [ 'dist1' ] = np . log ( test_clean [ 'dist1' ] + 1 ) test_clean [ 'dist2' ] = np . log ( test_clean [ 'dist2' ] + 1 )
821	model = RandomForestClassifier ( max_depth = None , n_estimators = 10 ) model . fit ( train_selected , train_labels ) estimator_nonlimited = model . estimators_ [ 5 ] export_graphviz ( estimator_nonlimited , out_file = 'tree_nonlimited.dot' , feature_names = train_selected . columns , class_names = [ 'extreme' , 'moderate' , 'vulnerable' , 'non-vulnerable' ] , rounded = True , proportion = False , precision = 2 )
1584	cols_with_only_one_value = [ ] for col in df_train . columns : if col == 'Target' : continue if df_train [ col ] . value_counts ( ) . shape [ 0 ] == 1 or df_test [ col ] . value_counts ( ) . shape [ 0 ] == 1 : print ( col ) cols_with_only_one_value . append ( col )
761	data . loc [ data [ 'v2a1' ] . isnull ( ) , own_variables ] . sum ( ) . plot . bar ( figsize = ( 10 , 8 ) , color = 'green' , edgecolor = 'k' , linewidth = 2 ) ; plt . xticks ( [ 0 , 1 , 2 , 3 , 4 ] , [ 'Owns and Paid Off' , 'Owns and Paying' , 'Rented' , 'Precarious' , 'Other' ] , rotation = 60 ) plt . title ( 'Home Ownership Status for Households Missing Rent Payments' , size = 18 ) ;
1736	numerical1 = [ 'v2a11' , 'meaneduc' , 'overcrowding' ] for numy in numerical1 : plot_distribution ( train , numy , 'Target' )
433	import warnings as wrn wrn . filterwarnings ( 'ignore' , category = DeprecationWarning ) wrn . filterwarnings ( 'ignore' , category = FutureWarning ) wrn . filterwarnings ( 'ignore' , category = UserWarning )
1397	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 8 ) ) sorted_train_df . groupby ( 'date' ) [ 'var_91' ] . count ( ) . plot ( ax = ax , label = "train" ) sorted_test_df . groupby ( 'date' ) [ 'var_91' ] . count ( ) . plot ( ax = ax , label = "test" ) ax . legend ( )
1602	df [ 'MA_7MA' ] = df [ 'close' ] . rolling ( window = 7 ) . mean ( ) df [ 'MA_15MA' ] = df [ 'close' ] . rolling ( window = 15 ) . mean ( ) df [ 'MA_30MA' ] = df [ 'close' ] . rolling ( window = 30 ) . mean ( ) df [ 'MA_60MA' ] = df [ 'close' ] . rolling ( window = 60 ) . mean ( )
1401	series_array = np . expm1 ( series_array ) series_array = np . clip ( series_array , 0.0 , None ) return series_array def predict_sequences ( input_sequences , batch_size ) : history_sequences = input_sequences . copy ( )
880	train = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_train.csv' ) test = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_test.csv' ) test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
1275	image [ image != color ] = background self . identify_object_by_isolation ( image , background = background ) def identify_object ( self , image , method ) :
800	feature_importances = pd . DataFrame ( { 'feature' : feature_names , 'importance' : importances } ) valid_scores = np . array ( valid_scores ) display ( f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.' )
1675	patientId = df [ 'patientId' ] [ 3 ] print ( patient_class . loc [ patientId ] ) plt . figure ( figsize = ( 10 , 8 ) ) plt . title ( "Sample Patient 1 - Normal Image" ) draw ( parsed [ patientId ] )
1528	examples = [ ] print ( 'merging examples...' ) merged = sorted ( examples_by_id + raw_results_by_id + features_by_id ) print ( 'done.' ) for idx , type_ , datum in merged : if type_ == 0 : examples . append ( EvalExample ( idx , datum ) ) elif type_ == 2 : examples [ - 1 ] . features [ idx ] = datum else : examples [ - 1 ] . results [ idx ] = datum
915	_ , idx = np . unique ( df_info , axis = 1 , return_index = True ) df_info = df_info . iloc [ : , idx ] return df_info
689	def imshow ( img ) : npimg = img . numpy ( ) plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ) ) )
1000	custom_features , custom_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ NormalizedModeCount , LongestSeq ] , max_depth = 2 , trans_primitives = [ ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] ) custom_features . iloc [ : , - 40 : ] . head ( )
522	df_model_losers = pd . merge ( left = df_tourney_list , right = df_tourney_final , how = 'left' , left_on = [ 'Season' , 'LTeamID' ] , right_on = [ 'Season' , 'TeamID' ] ) df_model_losers . drop ( labels = [ 'TeamID' ] , inplace = True , axis = 1 ) df_model_losers . head ( )
504	def plot_lineplot ( train_flat , cols , col_y ) : for col in cols : fig = plt . figure ( figsize = ( 15 , 8 ) ) sns . set_style ( "whitegrid" ) g = sns . lineplot ( col , col_y , hue = 'device.isMobile' , data = train_flat ) plt . xlabel ( col ) plt . ylabel ( 'log of transaction revenue' ) fig . show ( ) col_y = train_flat [ 'totals.transactionRevenueLogNAN' ] cat_cols = [ 'datestr' , 'day' , 'month' , 'year' , 'week' ] plot_lineplot ( train_flat , cat_cols , col_y )
1749	es = es . entity_from_dataframe ( entity_id = 'installments' , make_index = True , dataframe = installments_df , index = 'installment_id' )
361	from PIL import Image import seaborn as sns def _get_image_data_pil ( image_id , image_type , return_exif_md = False ) : fname = get_filename ( image_id , image_type ) try : img_pil = Image . open ( fname ) except Exception as e : assert False , "Failed to read image : %s, %s. Error message: %s" % ( image_id , image_type , e ) img = np . asarray ( img_pil ) assert isinstance ( img , np . ndarray ) , "Open image is not an ndarray. Image id/type : %s, %s" % ( image_id , image_type ) if not return_exif_md : return img else : return img , img_pil . _getexif ( )
156	df = pd . read_csv ( '../input/train.csv' , skiprows = 9308568 , nrows = 59633310 ) header = pd . read_csv ( '../input/train.csv' , nrows = 0 ) df . columns = header . columns df del header gc . collect ( ) print ( "The created dataframe contains" , df . shape [ 0 ] , "rows." )
884	import pandas as pd import numpy as np import featuretools as ft import matplotlib . pyplot as plt plt . rcParams [ 'font.size' ] = 22 import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm as lgb from sklearn . model_selection import train_test_split from sklearn . model_selection import KFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import LabelEncoder import gc
395	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yT , y_pred ) print ( confusion )
875	for i , hyp in enumerate ( new_results [ 'hyperparameters' ] ) : hyp_df = hyp_df . append ( pd . DataFrame ( hyp , index = [ 0 ] ) , ignore_index = True )
774	scorr . append ( spearmanr ( train_heads [ c ] , train_heads [ 'Target' ] ) . correlation ) pvalues . append ( spearmanr ( train_heads [ c ] , train_heads [ 'Target' ] ) . pvalue ) scorrs = pd . DataFrame ( { 'feature' : feats , 'scorr' : scorr , 'pvalue' : pvalues } ) . sort_values ( 'scorr' )
1595	train_X = pad_sequences ( train_X , maxlen = maxlen ) val_X = pad_sequences ( val_X , maxlen = maxlen ) test_X = pad_sequences ( test_X , maxlen = maxlen )
239	country_name = "Italy" march_day = 15 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
782	new_col = [ ] for c in ind_agg . columns . levels [ 0 ] : for stat in ind_agg . columns . levels [ 1 ] : new_col . append ( f'{c}-{stat}' ) ind_agg . columns = new_col ind_agg . head ( )
163	def rle_encoding ( x ) : dots = np . where ( x . T . flatten ( ) == 1 ) [ 0 ] run_lengths = [ ] prev = - 2 for b in dots : if ( b > prev + 1 ) : run_lengths . extend ( ( b + 1 , 0 ) ) run_lengths [ - 1 ] += 1 prev = b return " " . join ( [ str ( i ) for i in run_lengths ] ) print ( 'RLE Encoding for the current mask is: {}' . format ( rle_encoding ( label_mask ) ) )
507	train_s1x = train_flat [ train_flat [ 'datestr' ] <= '2017-06-30' ] train_s2x = train_flat [ train_flat [ 'datestr' ] > '2017-06-30' ] train_s1ylog = train_s1x [ "totals.transactionRevenueLog" ] . values train_s2ylog = train_s2x [ "totals.transactionRevenueLog" ] . values train_s1x = train_s1x [ cat_cols + num_cols ] train_s2x = train_s2x [ cat_cols + num_cols ] test_X = test_flat [ cat_cols + num_cols ] train_flat_x = train_flat [ cat_cols + num_cols ] train_flat_ylog = train_flat [ "totals.transactionRevenueLog" ] . values
1689	xs = [ x for x in xs if len ( x . reshape ( - 1 ) ) > 0 ] return list ( sorted ( xs , key = lambda x : x . max ( ) ) ) def sortByWeight ( xs ) :
743	train_new = train_df . pivot_table ( index = 'ID' , columns = 'Subtype' ) . reset_index ( ) print ( train_new . shape ) train_new . head ( )
976	random_hyp [ 'set' ] = 'Random Search' opt_hyp [ 'set' ] = 'Bayesian' hyp = random_hyp . append ( opt_hyp , ignore_index = True , sort = True ) hyp . head ( )
907	return round ( sys . getsizeof ( df ) / 1e9 , 2 ) def convert_types ( df ) : print ( f'Original size of data: {return_size(df)} gb.' ) for c in df : if df [ c ] . dtype == 'object' : df [ c ] = df [ c ] . astype ( 'category' ) print ( f'New size of data: {return_size(df)} gb.' ) return df
114	out_df = pd . merge ( out_df , state_day_lag , left_on = "state_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , state_day_year_lag , left_on = "state_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , month_state_lag , left_on = "state_id" , right_index = True , how = "left" )
563	train [ 'item_description' ] = train [ 'item_description' ] . astype ( str ) test [ 'item_description' ] = test [ 'item_description' ] . astype ( str ) train [ 'des_len' ] = train [ 'item_description' ] . apply ( lambda x : len ( x ) ) test [ 'des_len' ] = test [ 'item_description' ] . apply ( lambda x : len ( x ) )
752	pd . options . display . max_columns = 150 train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
303	train_path = 'base_dir/train_dir' val_path = 'base_dir/val_dir' num_train_samples = len ( df_train ) num_val_samples = len ( df_val ) train_batch_size = 5 val_batch_size = 5 train_steps = np . ceil ( num_train_samples / train_batch_size ) val_steps = np . ceil ( num_val_samples / val_batch_size )
1353	start_mem = df . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( 'Memory usage of dataframe is {:.2f} MB' . format ( start_mem ) ) for col in df . columns : if is_datetime ( df [ col ] ) or is_categorical_dtype ( df [ col ] ) :
449	target_var = df_train . iloc [ : , 7 : 22 ] X_train = df_train . drop ( target_var , axis = 1 ) y1_train = df_train [ "TotalTimeStopped_p20" ] y2_train = df_train [ "TotalTimeStopped_p50" ] y3_train = df_train [ "TotalTimeStopped_p80" ] y4_train = df_train [ "DistanceToFirstStop_p20" ] y5_train = df_train [ "DistanceToFirstStop_p50" ] y6_train = df_train [ "DistanceToFirstStop_p80" ] X_test = df_test
1278	arc . reset ( ) arc . identify_object ( image , method = 2 ) arc . plot_identified_objects ( arc . identified_objects , title = 'by isolation' )
238	country_name = "Italy" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1204	save_dir = '/kaggle/tmp/fake/' if not os . path . exists ( save_dir ) : os . makedirs ( save_dir )
691	gts = tf . reduce_sum ( gt_sorted ) intersection = gts - tf . cumsum ( gt_sorted ) union = gts + tf . cumsum ( 1. - gt_sorted ) jaccard = 1. - intersection / union jaccard = tf . concat ( ( jaccard [ 0 : 1 ] , jaccard [ 1 : ] - jaccard [ : - 1 ] ) , 0 ) return jaccard
1100	best_fold_auc = np . max ( gbm_history [ 'valid_1' ] [ 'auc' ] ) folds_auc . append ( best_fold_auc ) print ( 'Best fold GBM AUC: {:.4f}\n' . format ( best_fold_auc ) ) i += 1 print ( 'Mean KFold AUC: {:.4f}' . format ( np . asarray ( folds_auc ) . mean ( ) ) ) return oof_train , oof_test
609	if self . save_to_dir : for i , j in enumerate ( index_array ) : img = array_to_img ( batch_x [ i ] , self . data_format , scale = True ) fname = '{prefix}_{index}_{hash}.{format}' . format ( prefix = self . save_prefix , index = j , hash = np . random . randint ( 1e7 ) , format = self . save_format ) img . save ( os . path . join ( self . save_to_dir , fname ) )
128	filenames = os . listdir ( "../input/train/train" ) categories = [ ] for filename in filenames : category = filename . split ( '.' ) [ 0 ] if category == 'dog' : categories . append ( 1 ) else : categories . append ( 0 ) df = pd . DataFrame ( { 'filename' : filenames , 'category' : categories } )
1336	input_filters , output_filters = self . _block_args . input_filters , self . _block_args . output_filters if self . id_skip and self . _block_args . stride == 1 and input_filters == output_filters : if drop_connect_rate : x = drop_connect ( x , p = drop_connect_rate , training = self . training ) x = x + inputs return x class EfficientNet ( nn . Module ) :
1445	train_df [ 'Confidence' ] . iloc [ i ] = originalValue + md_learning_rate ( j ) train_df [ 'sigma_clipped' ] = train_df [ 'Confidence' ] . apply ( lambda x : max ( x , 70 ) ) train_df [ 'score' ] = - math . sqrt ( 2 ) * train_df [ 'delta' ] / train_df [ 'sigma_clipped' ] - np . log ( math . sqrt ( 2 ) * train_df [ 'sigma_clipped' ] ) scoreup = train_df [ 'score' ] . mean ( )
1521	idx2 = K . dot ( m , tf . cast ( idx , dtype = 'float32' ) ) idx2 = K . cast ( idx2 , dtype = 'int32' ) idx2 = K . clip ( idx2 , - DIM // 2 + XDIM + 1 , DIM // 2 )
1399	series_array = np . log1p ( np . nan_to_num ( series_array ) ) series_mean = series_array . mean ( axis = 1 ) . reshape ( - 1 , 1 ) series_std = series_array . std ( axis = 1 ) . reshape ( - 1 , 1 )
780	range_ = lambda x : x . max ( ) - x . min ( ) range_ . __name__ = 'range_' ind_agg = ind . drop ( columns = 'Target' ) . groupby ( 'idhogar' ) . agg ( [ 'min' , 'max' , 'sum' , 'count' , 'std' , range_ ] ) ind_agg . head ( )
1701	new_candidates = [ ] length_limit = 4 def random_node ( ) : return random . choice ( allowed_nodes )
289	from sklearn . metrics import classification_report y_pred_binary = predictions . argmax ( axis = 1 ) report = classification_report ( y_true , y_pred_binary , target_names = cm_plot_labels ) print ( report )
1345	for task , prediction , solved in tqdm ( zip ( evaluation_tasks , evaluation_predictions , evaluation_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
602	train_audio_path = '../input/freesound-audio-tagging-2019/train_curated/' train_files = os . listdir ( train_audio_path ) train_annot = pd . read_csv ( '../input/freesound-audio-tagging-2019/train_curated.csv' ) test_audio_path = '../input/freesound-audio-tagging-2019/test/' test_files = np . sort ( os . listdir ( test_audio_path ) ) print ( test_files [ : 5 ] )
559	newdict = { } for k , _ in order . items ( ) : v = unordered_dict [ k ] newdict [ k ] = v del order , orderdict return newdict
55	n_clusters = ( len ( set ( db . labels_ ) ) - 1 if - 1 in db . labels_ else len ( set ( db . labels_ ) ) ) print ( 'Found {} {} clusters.\n-----------------------------------' . format ( n_clusters , var_descr . lower ( ) ) ) clusters_coords = [ coords [ db . labels_ == n ] for n in range ( n_clusters ) ]
298	dst = os . path . join ( train_dir , sub_folder , fname ) image = cv2 . imread ( src ) image = cv2 . resize ( image , ( IMAGE_HEIGHT , IMAGE_WIDTH ) ) cv2 . imwrite ( dst , image )
202	spacing = map ( float , ( [ scan [ 0 ] . SliceThickness ] + scan [ 0 ] . PixelSpacing ) ) spacing = np . array ( list ( spacing ) ) resize_factor = spacing / new_spacing new_real_shape = image . shape * resize_factor new_shape = np . round ( new_real_shape ) real_resize_factor = new_shape / image . shape new_spacing = spacing / real_resize_factor image = scipy . ndimage . interpolation . zoom ( image , real_resize_factor ) return image , new_spacing
240	country_name = "Germany" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1453	if dropnan : agg . dropna ( inplace = True ) return agg
326	pid = row [ 'patientId' ] if pid not in parsed : parsed [ pid ] = { 'dicom' : '../input/stage_1_train_images/%s.dcm' % pid , 'label' : row [ 'Target' ] , 'boxes' : [ ] }
1123	dtrain = lgb . Dataset ( X_tr . values , y_tr . values , feature_name = train_cols ) dvalid = lgb . Dataset ( X_val . values , y_val . values , feature_name = train_cols , reference = dtrain )
73	model = create_network ( input_size = 256 , channels = 32 , n_blocks = 2 , depth = 4 ) model . compile ( optimizer = 'adam' , loss = iou_bce_loss , metrics = [ 'accuracy' , mean_iou ] )
1411	layout = go . Layout ( width = 600 , height = 600 , margin = dict ( l = 0 , r = 0 , b = 0 , t = 0 ) , scene = dict ( xaxis = dict ( title = X_COL ) , yaxis = dict ( title = Y_COL ) , zaxis = dict ( title = Z_COL ) , ) , )
392	print ( 'resize train images... ' ) sys . stdout . flush ( ) for n , id_ in tqdm ( enumerate ( train_ids ) , total = len ( train_ids ) ) : path = TRAIN_PATH + "/" + id_ img = imread ( path + '/images/' + id_ + '.png' ) [ : , : , : IMG_CHANNELS ] img = resize ( img , ( IMG_HEIGHT , IMG_WIDTH ) , mode = 'constant' , preserve_range = True ) train_images [ n ] = img
1406	s_int = int ( seed [ 1 : 3 ] ) return ( s_int ) df_seeds [ 'seed_int' ] = df_seeds . Seed . apply ( seed_to_int ) df_seeds . drop ( labels = [ 'Seed' ] , inplace = True , axis = 1 )
272	EPOCHS = EPOCHS_wn NNBATCHSIZE = 16 GROUP_BATCH_SIZE = 4000 SEED = 321 LR = lr_wn
1147	train_pet_ids = train . PetID . unique ( ) test_pet_ids = test . PetID . unique ( ) if DEBUG : train_pet_ids = train_pet_ids [ : 32 ] test_pet_ids = test_pet_ids [ : 16 ] print ( len ( train_pet_ids ) , len ( test_pet_ids ) )
1259	data_generator = create_datagen ( ) train_gen = create_flow ( data_generator , 'training' ) val_gen = create_flow ( data_generator , 'validation' ) test_gen = create_test_gen ( )
627	df_grouped_spain = get_df_country_cases ( df_covid , "Spain" ) df_spain_cases_by_day = df_grouped_spain [ df_grouped_spain . confirmed > 0 ] df_spain_cases_by_day = df_spain_cases_by_day . reset_index ( drop = True ) df_spain_cases_by_day [ 'day' ] = df_spain_cases_by_day . date . apply ( lambda x : ( x - df_spain_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_spain_cases_by_day = df_spain_cases_by_day [ reordered_columns ] df_spain_cases_by_day
1329	args = [ 'r%d' % block . num_repeat , 'k%d' % block . kernel_size , 's%d%d' % ( block . strides [ 0 ] , block . strides [ 1 ] ) , 'e%s' % block . expand_ratio , 'i%d' % block . input_filters , 'o%d' % block . output_filters ] if 0 < block . se_ratio <= 1 : args . append ( 'se%s' % block . se_ratio ) if block . id_skip is False : args . append ( 'noskip' ) return '_' . join ( args ) @ staticmethod def decode ( string_list ) :
918	installments = pd . read_csv ( '../input/installments_payments.csv' ) . replace ( { 365243 : np . nan } ) installments = convert_types ( installments ) installments [ 'LATE' ] = installments [ 'DAYS_ENTRY_PAYMENT' ] > installments [ 'DAYS_INSTALMENT' ] installments [ 'LOW_PAYMENT' ] = installments [ 'AMT_PAYMENT' ] < installments [ 'AMT_INSTALMENT' ]
1318	for mol_type in mol_types : model_name_wrt = ( '/kaggle/working/molecule_model_%s.hdf5' % mol_type ) print ( 'Training %s' % mol_type , 'out of' , mol_types , '\n' ) full = build_couple_dataframe ( train_csv , structures_csv , mol_type , n_atoms = 11 ) full2 = build_couple_dataframe ( test_csv , structures_csv , mol_type , n_atoms = 11 ) df_train_ = take_n_atoms ( full , 11 ) df_test_ = take_n_atoms ( full2 , 11 ) df_train_ = df_train_ . fillna ( 0 ) df_test_ = df_test_ . fillna ( 0 )
397	from sklearn . ensemble import RandomForestClassifier rfc = RandomForestClassifier ( n_estimators = 10 , n_jobs = 2 , random_state = 0 ) rfc . fit ( x , y )
235	x_train . drop ( 'Id' , inplace = True , errors = 'ignore' , axis = 1 ) x_train . drop ( 'ForecastId' , inplace = True , errors = 'ignore' , axis = 1 ) x_test . drop ( 'Id' , inplace = True , errors = 'ignore' , axis = 1 ) x_test . drop ( 'ForecastId' , inplace = True , errors = 'ignore' , axis = 1 ) return x_train , y_train_1 , y_train_2 , x_test
906	plt . figure ( figsize = ( 10 , 8 ) ) plt . plot ( list ( range ( train . shape [ 1 ] ) ) , np . cumsum ( pca . explained_variance_ratio_ ) , 'r-' ) plt . xlabel ( 'Number of PC' ) ; plt . ylabel ( 'Cumulative Variance Explained' ) ; plt . title ( 'Cumulative Variance Explained with PCA' ) ;
919	cash = pd . read_csv ( '../input/POS_CASH_balance.csv' ) . replace ( { 365243 : np . nan } ) cash = convert_types ( cash ) cash [ 'LATE_PAYMENT' ] = cash [ 'SK_DPD' ] > 0.0 cash [ 'INSTALLMENTS_PAID' ] = cash [ 'CNT_INSTALMENT' ] - cash [ 'CNT_INSTALMENT_FUTURE' ]
631	S , E , I , R , D = X S_prime = - beta * S * I - gamma * E * S - omega * S E_prime = beta * S * I - alpha * E + gamma * E * S - omega * E I_prime = alpha * E - zeta * I - delta * I - omega * I R_prime = zeta * I + omega * ( S + E + I ) D_prime = delta * I return S_prime , E_prime , I_prime , R_prime , D_prime
1265	BATCH_SIZE = 8 train_idx , val_idx = train_test_split ( non_missing_train_idx . index , random_state = 2019 , test_size = 0.2 ) train_generator = DataGenerator ( train_idx , reshape = ( 256 , 512 ) , df = mask_count_df , target_df = train_df , augment = True , batch_size = BATCH_SIZE , n_classes = 4 ) val_generator = DataGenerator ( val_idx , reshape = ( 256 , 512 ) , df = mask_count_df , target_df = train_df , augment = False , batch_size = BATCH_SIZE , n_classes = 4 )
1680	if durations == [ ] : features [ 'duration_mean' ] = 0 else : features [ 'duration_mean' ] = np . mean ( durations ) durations . append ( ( session . iloc [ - 1 , 2 ] - session . iloc [ 0 , 2 ] ) . seconds )
266	v = train [ "DisplayName" ] . value_counts ( ) missing_values = list ( v [ v < 5 ] . index ) train [ "DisplayName" ] = train [ "DisplayName" ] . where ( ~ train [ "DisplayName" ] . isin ( missing_values ) , "nan" )
916	app = app . set_index ( 'SK_ID_CURR' ) app = app . merge ( bureau_info , on = 'SK_ID_CURR' , how = 'left' ) del bureau_info app . shape
594	df_text = df [ text_feature ] . fillna ( ' ' ) df_text [ 'full_text' ] = '' for f in text_feature : df_text [ 'full_text' ] = df_text [ 'full_text' ] + df_text [ f ]
1781	text = list ( train . text . values ) tf_vectorizer = LemmaCountVectorizer ( max_df = 0.95 , min_df = 2 , stop_words = 'english' , decode_error = 'ignore' ) tf = tf_vectorizer . fit_transform ( text ) print ( tf_vectorizer . get_feature_names ( ) )
536	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
614	hist , _ = np . histogram ( classes , bins = np . arange ( 12 ) - 0.5 ) class_weight = hist . sum ( ) / np . power ( hist , exp ) return class_weight
799	model . fit ( X_train , y_train , early_stopping_rounds = 100 , eval_metric = macro_f1_score , eval_set = [ ( X_train , y_train ) , ( X_valid , y_valid ) ] , eval_names = [ 'train' , 'valid' ] , verbose = 200 )
867	hyperparams [ 'boosting_type' ] = hyperparams [ 'boosting_type' ] [ 'boosting_type' ] hyperparams [ 'subsample' ] = subsample hyperparams
1052	model . fit ( train_features , train_labels , eval_metric = 'auc' , eval_set = [ ( valid_features , valid_labels ) , ( train_features , train_labels ) ] , eval_names = [ 'valid' , 'train' ] , categorical_feature = cat_indices , early_stopping_rounds = 100 , verbose = 200 )
958	primitives = ft . list_primitives ( ) pd . options . display . max_colwidth = 100 primitives [ primitives [ 'type' ] == 'aggregation' ] . head ( 10 )
64	for col in contvar : sns . distplot ( np . log1p ( train [ col ] ) ) plt . show ( )
371	etr = ExtraTreesRegressor ( ) etr . fit ( train , target ) acc_model ( 12 , etr , train , test )
32	col = 'identity_hate' print ( "Column:" , col ) pred = lr . predict ( X ) print ( '\nConfusion matrix\n' , confusion_matrix ( y [ col ] , pred ) ) print ( classification_report ( y [ col ] , pred ) )
1210	img = cv2 . imread ( f'../input/train/{label_df.loc[0,"Image"]}' ) pad_width = get_pad_width ( img , max ( img . shape ) ) padded = np . pad ( img , pad_width = pad_width , mode = 'constant' , constant_values = 0 ) resized = cv2 . resize ( padded , ( 224 , 224 ) ) plt . imshow ( resized )
746	new_label [ last_series_id ] = df_train_y [ 'surface' ] [ ( df_train_y [ 'series_id' ] == idx1 ) | ( df_train_y [ 'series_id' ] == idx2 ) ] . value_counts ( ascending = False ) . index [ 0 ] last_series_id += 1 new_train = pd . concat ( [ new_train , df ] , ignore_index = True ) print ( "Final Train Size :: " , new_train . shape ) print ( "Time Taken :: " , time . time ( ) - start_time )
1083	train_df = pd . read_csv ( '{}train.csv' . format ( self . data_src ) , usecols = [ 0 ] , index_col = 'id' ) depths_df = pd . read_csv ( '{}depths.csv' . format ( self . data_src ) , index_col = 'id' ) self . train_df = train_df . join ( depths_df ) self . test_df = depths_df [ ~ depths_df . index . isin ( train_df . index ) ] return def load_data ( self ) :
1328	assert isinstance ( block_string , str ) ops = block_string . split ( '_' ) options = { } for op in ops : splits = re . split ( r'(\d.*)' , op ) if len ( splits ) >= 2 : key , value = splits [ : 2 ] options [ key ] = value
986	previous [ 'previous_decision_date' ] = start_date + previous [ 'DAYS_DECISION' ] previous [ 'previous_drawing_date' ] = start_date + previous [ 'DAYS_FIRST_DRAWING' ] previous [ 'previous_first_due_date' ] = start_date + previous [ 'DAYS_FIRST_DUE' ] previous [ 'previous_last_duefirst_date' ] = start_date + previous [ 'DAYS_LAST_DUE_1ST_VERSION' ] previous [ 'previous_last_due_date' ] = start_date + previous [ 'DAYS_LAST_DUE' ] previous [ 'previous_termination_date' ] = start_date + previous [ 'DAYS_TERMINATION' ]
1196	fold_results = { 'best_iteration_' : model . best_iteration_ , 'best_score_' : model . best_score_ [ 'valid_0' ] [ 'rmse' ] , 'evals_result_' : model . evals_result_ [ 'valid_0' ] [ 'rmse' ] , 'feature_importances_' : model . feature_importances_ } all_K_fold_results . append ( fold_results . copy ( ) )
953	import matplotlib . pyplot as plt plt . rcParams [ 'font.size' ] = 22 import seaborn as sns
162	two_cell_indices = ndimage . find_objects ( labels ) [ 1 ] cell_mask = mask [ two_cell_indices ] cell_mask_opened = ndimage . binary_opening ( cell_mask , iterations = 8 )
360	plt_st ( 20 , 20 ) plt . imshow ( complete_images [ 0 ] ) plt . title ( "Training dataset of type %i" % ( 0 ) )
150	data_to_plot2 [ "cat_DL" ] = '' data_to_plot2 . loc [ data_to_plot2 [ 'is_attributed' ] == 0 , "cat_DL" ] = "No" data_to_plot2 . loc [ data_to_plot2 [ 'is_attributed' ] == 1 , "cat_DL" ] = "Yes, once" data_to_plot2 . loc [ data_to_plot2 [ 'is_attributed' ] > 1 , "cat_DL" ] = "Yes, multiple times"
1771	def squash ( self , x , axis = - 1 ) : s_squared_norm = ( x ** 2 ) . sum ( axis , keepdim = True ) scale = t . sqrt ( s_squared_norm + T_epsilon ) return x / scale class Capsule_Main ( nn . Module ) : def __init__ ( self , embedding_matrix = None , vocab_size = None ) : super ( Capsule_Main , self ) . __init__ ( ) self . embed_layer = Embed_Layer ( embedding_matrix , vocab_size ) self . gru_layer = GRU_Layer ( )
1522	idx3 = tf . stack ( [ DIM // 2 - idx2 [ 0 , ] , DIM // 2 - 1 + idx2 [ 1 , ] ] ) d = tf . gather_nd ( image , tf . transpose ( idx3 ) ) return tf . reshape ( d , [ DIM , DIM , 3 ] ) , label
1571	loss = ckpt_list [ loss_list . index ( min ( loss_list ) ) ] best_model = LOG_DIR + '/' + loss model . load_weights ( best_model )
1572	plt . figure ( figsize = ( 12 , 6 ) ) sns . countplot ( submission [ "diagnosis" ] ) plt . title ( "Number of data per each diagnosis" ) plt . show ( )
91	if blaze_bboxes == [ ] : print ( 'BlazeFace is unable to detect face in this frame.' ) if mtcnn_bboxes == [ ] : print ( 'MTCNN is unable to detect face in this frame.' ) if mobilenet_bboxes == [ ] : print ( 'mobilenet is unable to detect face in this frame.' ) if yolo_bboxes == [ ] : print ( 'mobilenet is unable to detect face in this frame.' )
1202	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB3 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . summary ( )
1141	shap . dependence_plot ( ( "returnsClosePrevRaw10_lag_3_mean" , "returnsOpenPrevMktres10" ) , shap_interaction_values , X_interaction )
946	import altair as alt alt . renderers . enable ( 'notebook' )
1373	model = LGBMClassifier ( ** params ) . fit ( X , y , categorical_feature = [ 'PdDistrict' ] ) pdp_Pd = pdp . pdp_isolate ( model = model , dataset = X , model_features = X . columns . tolist ( ) , feature = 'Hour' , n_jobs = - 1 ) pdp . pdp_plot ( pdp_Pd , 'Hour' , ncols = 3 ) plt . show ( )
188	wc = WordCloud ( background_color = "white" , max_words = 5000 , stopwords = STOPWORDS , max_font_size = 50 ) wc . generate ( " " . join ( str ( s ) for s in train . item_description . values ) ) plt . figure ( figsize = ( 20 , 12 ) ) plt . axis ( 'off' ) plt . imshow ( wc , interpolation = 'bilinear' )
284	if target == 0 : label = 'a_no_tumor_tissue' if target == 1 : label = 'b_has_tumor_tissue'
1660	cities = pd . read_csv ( '../input/cities.csv' ) xy_int = ( cities [ [ 'X' , 'Y' ] ] * 1000 ) . astype ( np . int64 ) with open ( 'xy_int.csv' , 'w' ) as fp : print ( len ( xy_int ) , file = fp ) print ( xy_int . to_csv ( index = False , header = False , sep = ' ' ) , file = fp )
1496	PRETRAINED_MODELS = { "BERT" : [ 'bert-base-uncased' , 'bert-large-uncased' , 'bert-base-cased' , 'bert-large-cased' , 'bert-base-multilingual-uncased' , 'bert-base-multilingual-cased' , 'bert-base-chinese' , 'bert-base-german-cased' , 'bert-large-uncased-whole-word-masking' , 'bert-large-cased-whole-word-masking' , 'bert-large-uncased-whole-word-masking-finetuned-squad' , 'bert-large-cased-whole-word-masking-finetuned-squad' , 'bert-base-cased-finetuned-mrpc' ] , "DISTILBERT" : [ 'distilbert-base-uncased' , 'distilbert-base-uncased-distilled-squad' ] }
794	selected_features = train_set . columns [ np . where ( selector . ranking_ == 1 ) ] train_selected = pd . DataFrame ( train_selected , columns = selected_features ) test_selected = pd . DataFrame ( test_selected , columns = selected_features )
435	from IPython . display import Markdown def bold ( string ) : display ( Markdown ( string ) )
346	def my_generator ( ) : while True : for i in range ( 0 , 4 ) : yield i infinity_gen = my_generator ( )
856	features = list ( data . columns ) for f in [ 'pickup_datetime' , 'fare_amount' , 'fare-bin' , 'color' ] : features . remove ( f ) len ( features )
643	import transformers import tokenizers import string import torch import torch . nn as nn from torch . nn import functional as F from tqdm import tqdm import re import json import requests
1450	look_back = 1 trainX , trainY = create_dataset ( V_train , look_back ) testX , testY = create_dataset ( V_test , look_back )
1594	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( train_X ) ) train_X = tokenizer . texts_to_sequences ( train_X ) val_X = tokenizer . texts_to_sequences ( val_X ) test_X = tokenizer . texts_to_sequences ( test_X )
1098	X_tr = X_train . iloc [ train_index ] X_val = X_train . iloc [ valid_index ] y_tr = y_train . iloc [ train_index ] y_val = y_train . iloc [ valid_index ] dtrain = lgb . Dataset ( X_tr , y_tr ) dvalid = lgb . Dataset ( X_val , y_val , reference = dtrain )
1326	multiplier = global_params . depth_coefficient if not multiplier : return repeats return int ( math . ceil ( multiplier * repeats ) ) def drop_connect ( inputs , p , training ) :
107	print ( "*" * 30 , "store_id" , "*" * 30 ) print ( "store_id unique value counts:{}" . format ( len ( price [ "store_id" ] . unique ( ) ) ) ) print ( price [ "store_id" ] . unique ( ) ) print ( "*" * 30 , "item_id" , "*" * 30 ) print ( "item_id unique value counts:{}" . format ( len ( price [ "item_id" ] . unique ( ) ) ) ) print ( price [ "item_id" ] . unique ( ) )
797	features = np . array ( features ) test_features = np . array ( test_features ) labels = np . array ( labels ) . reshape ( ( - 1 ) ) valid_scores = [ ]
1400	series_array = np . log1p ( np . nan_to_num ( series_array ) ) series_array = ( series_array - encode_series_mean ) series_array = series_array . reshape ( ( series_array . shape [ 0 ] , series_array . shape [ 1 ] , 1 ) ) return series_array def untransform_series_decode ( series_array , encode_series_mean , encode_series_std ) : series_array = series_array . reshape ( series_array . shape [ 0 ] , series_array . shape [ 1 ] ) series_array = series_array + encode_series_mean
1191	preds = test_preds . argmax ( axis = 1 ) . astype ( np . int8 ) print ( f'predicted accuracy_group distribution:\n\n{pd.Series(preds).value_counts(normalize=True)} \n\n' ) submission [ 'accuracy_group' ] = preds submission . to_csv ( 'submission.csv' , index = False )
243	country_name = "Albania" march_day = 15 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
525	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
1089	else : y_pred_test = np . asarray ( [ cv2 . resize ( x , self . orig_image_size ) for x in y_pred_test ] ) assert y_pred_test . shape == ( 18000 , 101 , 101 ) , '\ Test predictions shape must be equal to (18000, 101, 101).' print ( 'Test predictions shape: {}' . format ( y_pred_test . shape ) )
1497	bert_tokenizer , bert_nq = get_pretrained_model ( FLAGS . model_name ) if not IS_KAGGLE : bert_nq . trainable_variables
207	x = tf . repeat ( tf . range ( DIM // 2 , - DIM // 2 , - 1 ) , DIM ) y = tf . tile ( tf . range ( - DIM // 2 , DIM // 2 ) , [ DIM ] ) z = tf . ones ( [ DIM * DIM ] , dtype = 'int32' ) idx = tf . stack ( [ x , y , z ] )
1145	plot_curve_fit ( gaussian , roc_by_date , 'Active' , 'China w/o Hubei' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , china_by_date , 'Active' , 'China' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , skorea_by_date , 'Active' , 'South Korea' + ' - Curve for Cases ' , False , 'Gauss' ) plot_curve_fit ( gaussian , italy_by_date , 'Active' , 'Italy' + ' - Curve for Cases ' , False , 'Gauss' )
1457	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_data . head ( 20 )
17	new_transactions = reduce_mem_usage ( pd . read_csv ( '../input/new_merchant_transactions.csv' ) ) new_transactions [ 'authorized_flag' ] = new_transactions [ 'authorized_flag' ] . map ( { 'Y' : 1 , 'N' : 0 } ) new_transactions [ 'category_1' ] = new_transactions [ 'category_1' ] . map ( { 'Y' : 1 , 'N' : 0 } )
416	building = pd . read_csv ( '../input/ashrae-energy-prediction/building_metadata.csv' ) weather_train = pd . read_csv ( '../input/ashrae-energy-prediction/weather_train.csv' ) weather_test = pd . read_csv ( '../input/ashrae-energy-prediction/weather_test.csv' ) train = pd . read_csv ( '../input/ashrae-energy-prediction/train.csv' ) test = pd . read_csv ( '../input/ashrae-energy-prediction/test.csv' )
1530	df_previous_app = pd . merge ( left = df_previous_app , right = df_installments [ [ key_prev ] + payment_cols ] . groupby ( key_prev ) . min ( ) , left_on = key_prev , right_index = True , how = 'left' )
378	mae = mean_absolute_error ( y_val , preds ) print ( 'MAE: %.6f' % mae ) maes . append ( mae )
1442	N = sub_df . shape [ 0 ] x_sub = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( sub_df [ 'Patient' ] ) ) : x_sub [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
1289	input_shape = _obtain_input_shape ( input_shape , default_size = 224 , min_size = 32 , data_format = K . image_data_format ( ) , require_flatten = False ) if input_tensor is None : img_input = Input ( shape = input_shape ) else : if not K . is_keras_tensor ( input_tensor ) : img_input = Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor x = _create_se_resnet ( classes , img_input , include_top , initial_conv_filters , filters , depth , width , bottleneck , weight_decay , pooling )
812	headers = [ 'loss' , 'hyperparameters' , 'iteration' , 'runtime' , 'score' , 'std' ] writer . writerow ( headers ) of_connection . close ( )
885	import matplotlib . pyplot as plt plt . rcParams [ 'font.size' ] = 22 import seaborn as sns
1804	plt . title ( 'Receiver Operating Characteristic' ) plt . plot ( false_positive_rate , true_positive_rate , label = 'ROC curve (area = %0.2f)' % roc_auc ) plt . plot ( [ 0 , 1 ] , ls = "--" )
1638	sns . boxplot ( train_smp . is_attributed , train_smp [ 'min' ] ) plt . title ( 'Boxplot of Minute distribution' ) plt . show ( ) sns . violinplot ( train_smp . is_attributed , train_smp [ 'min' ] ) plt . title ( 'Violinplot of Minute distribution' ) plt . show ( )
926	def objective ( hyperparameters , iteration ) : if 'n_estimators' in hyperparameters . keys ( ) : del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , nfold = N_FOLDS , early_stopping_rounds = 100 , metrics = 'auc' , seed = 42 ) score = cv_results [ 'auc-mean' ] [ - 1 ] estimators = len ( cv_results [ 'auc-mean' ] ) hyperparameters [ 'n_estimators' ] = estimators return [ score , hyperparameters , iteration ]
997	seed_features , seed_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'percent_true' , 'mean' ] , trans_primitives = [ ] , seed_features = [ past_due ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
305	model . load_weights ( 'model.h5' ) val_loss , val_categorical_accuracy = \ model . evaluate_generator ( test_gen , steps = len ( df_val ) ) print ( 'val_loss:' , val_loss ) print ( 'val_categorical_accuracy:' , val_categorical_accuracy )
832	sub [ 'surface' ] = le . inverse_transform ( predicted . argmax ( axis = 1 ) ) sub . to_csv ( 'rand_sub_10.csv' , index = False ) sub . head ( )
1714	from sklearn . model_selection import StratifiedKFold from sklearn . metrics import f1_score from torch . optim . optimizer import Optimizer from unidecode import unidecode
1767	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( train_X ) ) train_X = tokenizer . texts_to_sequences ( train_X ) test_X = tokenizer . texts_to_sequences ( test_X )
1792	f , ax = plt . subplots ( figsize = ( 50 , 30 ) ) sns . heatmap ( train_group , annot = False , ax = ax , fmt = "d" , linewidths = 2 ) plt . title ( 'Web Traffic Months cross Weekdays' ) plt . show ( )
1414	for item in sorted ( cls_counts . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) [ : 20 ] : _id , count = item [ 0 ] , item [ 1 ] label = label_map [ int ( _id ) ] print ( f'attribute_name: {label} count: {count}' )
213	columns = [ i for i in data . columns ] dummies = pd . get_dummies ( data , columns = columns , drop_first = True , sparse = True ) del data
381	item = bson . BSON . decode ( item_data ) IDS_MAPPING [ item [ '_id' ] ] = ( offset , length ) offset += length def get_item ( item_id ) : assert item_id in IDS_MAPPING with open ( os . path . join ( INPUT_PATH , 'train.bson' ) , 'rb' ) as f : offset , length = IDS_MAPPING [ item_id ] f . seek ( offset ) item_data = f . read ( length ) return bson . BSON . decode ( item_data )
1563	from sklearn . metrics import mean_squared_error watchlist = [ ( dtrain , 'train' ) ] num_round = 600 bst = xgb . train ( dict ( xgb_params , silent = 0 ) , dtrain , num_boost_round = num_round ) preds = bst . predict ( dtest ) err = ( mean_squared_error ( test [ target ] . values , preds ) ) print ( 'MSE ={}' . format ( err ) )
1371	train_data = lgb . Dataset ( X , label = y , categorical_feature = [ 'PdDistrict' ] ) params = { 'boosting' : 'gbdt' , 'objective' : 'multiclass' , 'num_class' : 39 , 'max_delta_step' : 0.9 , 'min_data_in_leaf' : 21 , 'learning_rate' : 0.4 , 'max_bin' : 465 , 'num_leaves' : 41 } bst = lgb . train ( params , train_data , 100 ) predictions = bst . predict ( test )
362	fname = get_filename ( image_id , image_type ) try : img_pil = Image . open ( fname ) except Exception as e : assert False , "Failed to read image : %s, %s. Error message: %s" % ( image_id , image_type , e ) img = np . asarray ( img_pil ) assert isinstance ( img , np . ndarray ) , "Open image is not an ndarray. Image id/type : %s, %s" % ( image_id , image_type ) if not return_exif_md : return img else : return img , img_pil . _getexif ( )
1573	cat_cols = [ 'ProductCD' , 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'addr1' , 'addr2' , 'P_emaildomain' , 'R_emaildomain' , 'DeviceType' , 'DeviceInfo' , ] + [ f'M{n}' for n in range ( 1 , 10 ) ] + [ f'id_{n}' for n in range ( 12 , 39 ) ] num_cols = list ( set ( df_train . columns ) - set ( cat_cols ) )
1392	df . to_parquet ( output_path , engine = PARQUET_ENGINE ) return df def load_historical_transactions ( path = None ) : if path is None : return smaller_historical_transactions ( INPUT_PATH , OUTPUT_PATH ) else : df = pd . read_parquet ( path , engine = PARQUET_ENGINE )
1721	x_train = np . load ( "x_train.npy" ) x_test = np . load ( "x_test.npy" ) y_train = np . load ( "y_train.npy" ) features = np . load ( "features.npy" ) test_features = np . load ( "test_features.npy" ) word_index = np . load ( "word_index.npy" ) . item ( )
1051	model = lgb . LGBMClassifier ( n_estimators = 10000 , objective = 'binary' , class_weight = 'balanced' , learning_rate = 0.05 , reg_alpha = 0.1 , reg_lambda = 0.1 , subsample = 0.8 , n_jobs = - 1 , random_state = 50 )
59	learn . unfreeze ( ) learn . lr_find ( ) learn . recorder . plot ( suggestion = True )
1186	accumu_actions += len ( session ) if last_type != session_type : types_count [ session_type ] += 1 last_type = session_type
1616	gts = tf . reduce_sum ( gt_sorted ) intersection = gts - tf . cumsum ( gt_sorted ) union = gts + tf . cumsum ( 1. - gt_sorted ) jaccard = 1. - intersection / union jaccard = tf . concat ( ( jaccard [ 0 : 1 ] , jaccard [ 1 : ] - jaccard [ : - 1 ] ) , 0 ) return jaccard
489	def group_by ( df , t1 = '' , t2 = '' ) : a1 = df . groupby ( [ t1 , t2 ] ) [ t2 ] . count ( ) return a1
1519	TARGET_MIN_COUNTING = 100 def get_num_of_repetition_for_class ( class_id ) : counting = label_counter [ class_id ] if counting >= TARGET_MIN_COUNTING : return 1.0 num_to_repeat = TARGET_MIN_COUNTING / counting return num_to_repeat numbers_of_repetition_for_classes = { class_id : get_num_of_repetition_for_class ( class_id ) for class_id in range ( 104 ) } print ( "number of repetitions for each class (if > 1)" ) { k : v for k , v in sorted ( numbers_of_repetition_for_classes . items ( ) , key = lambda item : item [ 1 ] , reverse = True ) if v > 1 }
386	length = struct . unpack ( "<i" , item_length_bytes ) [ 0 ] f . seek ( offset ) item_data = f . read ( length ) assert len ( item_data ) == length , "%i vs %i" % ( len ( item_data ) , length )
628	df_grouped_iran = get_df_country_cases ( df_covid , "Iran" ) df_iran_cases_by_day = df_grouped_iran [ df_grouped_iran . confirmed > 0 ] df_iran_cases_by_day = df_iran_cases_by_day . reset_index ( drop = True ) df_iran_cases_by_day [ 'day' ] = df_iran_cases_by_day . date . apply ( lambda x : ( x - df_iran_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_iran_cases_by_day = df_iran_cases_by_day [ reordered_columns ] df_iran_cases_by_day
226	fig = px . scatter_3d ( commits_df , x = 'hidden_dim_second' , y = 'dropout_model' , z = 'LB_score' , color = 'best' , symbol = 'hidden_dim_first' , title = 'hidden_dim_2nd & dropout and LB score visualization of COVID-19 mRNA VDP solutions' ) fig . update ( layout = dict ( title = dict ( x = 0.07 ) ) )
480	words = set ( text_to_word_sequence ( text ) ) vocab_size = len ( words ) print ( vocab_size )
384	train_df = pd . read_csv ( os . path . join ( DATA_DIR , 'train.csv' ) , dtype = { 'acoustic_data' : np . int16 , 'time_to_failure' : np . float32 } ) print ( train_df . shape ) print ( 'ok' )
1120	def rename_columns ( df ) : df . columns = pd . Index ( [ '{}{}' . format ( c [ 0 ] , c [ 1 ] . upper ( ) ) for c in df . columns . tolist ( ) ] ) return df
1124	oof_val [ val ] = lgb_model . predict ( X_val ) oof_test [ i , : ] = lgb_model . predict ( X_test ) i += 1
1190	event_code_list = np . unique ( np . hstack ( [ train_events [ 'event_code' ] . values , test_events [ 'event_code' ] . values ] ) ) . tolist ( )
684	d = { "unicode" : label_count [ : 10 , ] . index . values , "str" : [ unicode_map [ l ] for l in label_count [ : 10 , ] . index . values ] } pd . DataFrame ( d )
994	plt . figure ( figsize = ( 20 , 6 ) ) plt . subplot ( 1 , 2 , 1 ) plt . bar ( list ( range ( 3 ) ) , interesting_features [ 'MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Approved)' ] . value_counts ( ) ) plt . xticks ( list ( range ( 3 ) ) , interesting_features [ 'MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Approved)' ] . value_counts ( ) . index ) ; plt . xlabel ( "Client Type" ) ; plt . ylabel ( "Counts" ) ; plt . title ( "Most Common Client Type where Contract was Approved" ) ;
339	test_gen = \ test_generator ( df_test , test_batch_size , num_rows , num_cols ) model . load_weights ( filepath = 'model.h5' ) predictions = model . predict_generator ( test_gen , steps = num_test_batches , max_queue_size = 1 , workers = 1 , use_multiprocessing = False , verbose = 1 )
702	test2p = test2 [ ( test2 [ 'target' ] <= 0.01 ) | ( test2 [ 'target' ] >= 0.99 ) ] . copy ( ) test2p . loc [ test2p [ 'target' ] >= 0.5 , 'target' ] = 1 test2p . loc [ test2p [ 'target' ] < 0.5 , 'target' ] = 0 train2p = pd . concat ( [ train2p , test2p ] , axis = 0 ) train2p . reset_index ( drop = True , inplace = True )
208	idx2 = K . dot ( m , tf . cast ( idx , dtype = 'float32' ) ) idx2 = K . cast ( idx2 , dtype = 'int32' ) idx2 = K . clip ( idx2 , - DIM // 2 + XDIM + 1 , DIM // 2 )
1436	files = folders = 0 path = "/kaggle/input/osic-pulmonary-fibrosis-progression/train" for _ , dirnames , filenames in os . walk ( path ) : files += len ( filenames ) folders += len ( dirnames ) print ( "{:,} files/images, {:,} folders/patients" . format ( files , folders ) )
1810	from sklearn . metrics import ( confusion_matrix , precision_recall_curve , auc , roc_curve , recall_score , classification_report , f1_score , precision_recall_fscore_support )
685	d = { "unicode" : label_count [ - 11 : - 1 , ] . index . values , "str" : [ unicode_map [ l ] for l in label_count [ - 11 : - 1 , ] . index . values ] } pd . DataFrame ( d )
1554	model_lgb = lgb . train ( lgb_params , d_train , feval = lgb_rmsle_score , num_boost_round = n_rounds )
1055	metrics = pd . DataFrame ( { 'fold' : fold_names , 'train' : train_scores , 'valid' : valid_scores } ) return submission , feature_importances , metrics
159	from scipy import ndimage labels , nlabels = ndimage . label ( mask ) label_arrays = [ ] for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) label_arrays . append ( label_mask ) print ( 'There are {} separate components / objects detected.' . format ( nlabels ) )
1260	test_df [ 'ID' ] = test_df . filename . apply ( lambda x : x . replace ( '.png' , '' ) ) + '_' + test_df . variable test_df [ 'Label' ] = test_df [ 'value' ] test_df [ [ 'ID' , 'Label' ] ] . to_csv ( 'submission.csv' , index = False )
1174	colors = vtk . vtkNamedColors ( ) bkg = map ( lambda x : x / 255.0 , [ 26 , 51 , 102 , 255 ] ) colors . SetColor ( "BkgColor" , * bkg )
1041	gc . enable ( ) del previous , previous_agg , previous_counts gc . collect ( )
1290	x = squeeze_excite_block ( x ) m = add ( [ x , init ] ) return m def _resnet_bottleneck_block ( input , filters , k = 1 , strides = ( 1 , 1 ) ) :
533	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
1632	global fulltable_us df = fulltable_us [ ( fulltable_us [ 'Province/State' ] == province ) ] return df . set_index ( 'Date' ) [ [ 'Confirmed' , 'Deaths' ] ]
1068	print ( 'CV mean score: {0:.4f}, std: {1:.4f}.' . format ( np . mean ( scores ) , np . std ( scores ) ) ) print ( scores ) print ( "Test score:" , log_loss ( Y_development , prediction ) )
1606	etc_ordianal_features = [ 'ps_ind_01' , 'ps_ind_03' , 'ps_ind_14' , 'ps_ind_15' , 'ps_reg_01' , 'ps_reg_02' , 'ps_car_11' , 'ps_calc_01' , 'ps_calc_02' , 'ps_calc_03' , 'ps_calc_04' , 'ps_calc_05' , 'ps_calc_06' , 'ps_calc_07' , 'ps_calc_08' , 'ps_calc_09' , 'ps_calc_10' , 'ps_calc_11' , 'ps_calc_12' , 'ps_calc_13' , 'ps_calc_14' ] etc_continuous_features = [ 'ps_reg_03' , 'ps_car_12' , 'ps_car_13' , 'ps_car_14' , 'ps_car_15' ] train_null_columns = train_null . columns test_null_columns = test_null . columns
766	plt . legend ( handles = markers , title = 'Counts' , labelspacing = 3 , handletextpad = 2 , fontsize = 16 , loc = ( 1.10 , 0.19 ) ) plt . annotate ( f'* Size represents raw count while % is for a given y value.' , xy = ( 0 , 1 ) , xycoords = 'figure points' , size = 10 )
285	if target == 0 : label = 'a_no_tumor_tissue' if target == 1 : label = 'b_has_tumor_tissue'
1726	loss_fn = torch . nn . BCEWithLogitsLoss ( reduction = 'sum' ) step_size = 300 base_lr , max_lr = 0.001 , 0.003 optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ( ) ) , lr = max_lr )
320	gradient_boosting = GradientBoostingRegressor ( ** params ) gradient_boosting . fit ( train , target ) acc_model ( 9 , gradient_boosting , train , test )
1518	def get_training_dataset_raw ( ) : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True , ordered = False ) return dataset raw_training_dataset = get_training_dataset_raw ( ) label_counter = Counter ( ) for images , labels in raw_training_dataset : label_counter . update ( [ labels . numpy ( ) ] ) del raw_training_dataset label_counting_sorted = label_counter . most_common ( ) NUM_TRAINING_IMAGES = sum ( [ x [ 1 ] for x in label_counting_sorted ] ) print ( "number of examples in the original training dataset: {}" . format ( NUM_TRAINING_IMAGES ) ) print ( "labels in the original training dataset, sorted by occurrence" ) label_counting_sorted
127	cat_dim = [ int ( all_df [ col ] . nunique ( ) ) for col in categorical ] cat_dim = [ [ x , min ( 200 , ( x + 1 ) // 2 ) ] for x in cat_dim ] for el in cat_dim : if el [ 0 ] < 10 : el [ 1 ] = el [ 0 ] cat_dim
1243	truncated = hidden [ : , : pred_len ] out = L . Dense ( 5 , activation = 'linear' ) ( truncated ) model = tf . keras . Model ( inputs = inputs , outputs = out ) model . compile ( tf . optimizers . Adam ( ) , loss = MCRMSE ) return model
1106	with open ( filename , 'r' ) as f : metadata_file = json . load ( f ) return metadata_file def open_sentiment_file ( self , filename ) :
806	for parameter_name in [ 'max_depth' , 'num_leaves' , 'subsample_for_bin' , 'min_child_samples' , 'subsample_freq' ] : hyperparameters [ parameter_name ] = int ( hyperparameters [ parameter_name ] ) if 'n_estimators' in hyperparameters : del hyperparameters [ 'n_estimators' ]
708	test2p = pd . concat ( [ test2 [ : n ] , test2 [ - n : ] ] , axis = 0 ) test2p . loc [ test2p [ 'target' ] >= 0.5 , 'target' ] = 1 test2p . loc [ test2p [ 'target' ] < 0.5 , 'target' ] = 0 train2p = pd . concat ( [ train2p , test2p ] , axis = 0 ) train2p . reset_index ( drop = True , inplace = True )
791	model . fit ( train , train_labels ) predictions = model . predict ( test ) predictions = pd . DataFrame ( { 'idhogar' : test_ids , 'Target' : predictions } )
1381	indices = torch . randperm ( len ( train_dataset ) ) . tolist ( ) train_data_loader = DataLoader ( train_dataset , batch_size = 16 , shuffle = True , num_workers = 4 , collate_fn = collate_fn ) valid_data_loader = DataLoader ( valid_dataset , batch_size = 4 , shuffle = False , num_workers = 4 , collate_fn = collate_fn ) test_data_loader = DataLoader ( test_dataset , batch_size = 4 , shuffle = False , num_workers = 4 , drop_last = False , collate_fn = collate_fn )
690	dataiter = iter ( loader ) images , label , masks = dataiter . next ( ) print ( "image.shape: {}" . format ( images . shape ) ) print ( "mask.shape: {}" . format ( masks . shape ) )
576	TARGET_SR = 32_000 PP_NORMALIZE = True MIN_SEC = 1 DEVICE = 'cuda' BATCH_SIZE = 64 SIGMOID_THRESH = 0.3 MAX_BIRDS = None
1134	random_index = np . random . randint ( 0 , val_masks_stacked . shape [ 0 ] ) print ( 'Validation Index: {}' . format ( random_index ) ) fig , ax = plt . subplots ( 2 , 1 ) ax [ 0 ] . imshow ( val_masks_stacked [ random_index ] , cmap = 'seismic' ) ax [ 1 ] . imshow ( val_predictions_stacked [ random_index ] > 0.5 , cmap = 'seismic' )
256	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
122	def clean_special_chars ( text ) : for s in specail_signs : text = text . replace ( s , specail_signs [ s ] ) for p in punct : text = text . replace ( p , f' {p} ' ) return text
1090	y_test_pred_rle = { idx : rle_encode ( y_pred_test [ i ] > confidence_threshold_best ) for i , idx in enumerate ( tqdm ( self . test_df . index . values ) ) } return y_test_pred_rle def generate_submission ( self , y_test_pred_rle ) :
703	skf = StratifiedKFold ( n_splits = 11 , random_state = 42 , shuffle = True ) for train_index , test_index in skf . split ( train3p , train2p [ 'target' ] ) : test_index3 = test_index [ test_index < len ( train3 ) ]
1239	result_out , image_out = sess . run ( [ detector_output , decoded_image ] , feed_dict = { image_string_placeholder : image_string } )
379	rmse = mean_squared_error ( y_val , preds ) print ( 'RMSE: %.6f' % rmse ) rmses . append ( rmse )
348	corr_matrix = features . corr ( ) . abs ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) ; threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style collinear_features = [ column for column in upper . columns if any ( upper [ column ] > threshold ) ] upper . style . applymap ( highlight )
1762	all_data_na = ( feature_matrix_enc . isnull ( ) . sum ( ) / len ( feature_matrix_enc ) ) * 100 all_data_na = all_data_na . drop ( all_data_na [ all_data_na == 0 ] . index ) . sort_values ( ascending = False ) [ : 30 ] missing_data = pd . DataFrame ( { 'Missing Ratio' : all_data_na } ) missing_data . head ( 20 )
1687	first_shape = tuple ( x [ 0 ] . shape ) for pixmap in x [ 1 : ] : if first_shape != tuple ( pixmap . shape ) : return [ ] return [ np . bitwise_or . reduce ( np . array ( x ) . astype ( int ) ) ] def intersect ( x ) :
473	import pandas as pd import numpy as np from sklearn . model_selection import StratifiedKFold import lightgbm as lgb from sklearn import metrics import gc pd . set_option ( 'display.max_columns' , 200 )
1013	import gc gc . enable ( ) del train , bureau , bureau_balance , bureau_agg , bureau_agg_new , bureau_balance_agg , bureau_balance_counts , bureau_by_loan , bureau_balance_by_client , bureau_counts gc . collect ( )
1038	previous_agg = agg_numeric ( previous , 'SK_ID_CURR' , 'previous' ) print ( 'Previous aggregation shape: ' , previous_agg . shape ) previous_agg . head ( )
1793	f , ax = plt . subplots ( figsize = ( 50 , 30 ) ) sns . heatmap ( train_day , annot = False , ax = ax , fmt = "d" , linewidths = 2 ) plt . title ( 'Web Traffic Months cross days' ) plt . show ( )
1223	clf = xgb . XGBClassifier ( ** grid . best_params_ ) clf . fit ( X_train , y_train ) sample_submission [ 'isFraud' ] = clf . predict_proba ( X_test ) [ : , 1 ] sample_submission . to_csv ( 'simple_xgboost.csv' )
698	def rle_encode ( im ) : pixels = im . flatten ( order = 'F' ) pixels = np . concatenate ( [ [ 0 ] , pixels , [ 0 ] ] ) runs = np . where ( pixels [ 1 : ] != pixels [ : - 1 ] ) [ 0 ] + 1 runs [ 1 : : 2 ] -= runs [ : : 2 ] return ' ' . join ( str ( x ) for x in runs )
931	com = 1 for x in param_grid . values ( ) : com *= len ( x ) print ( 'There are {} combinations' . format ( com ) )
1014	import gc gc . enable ( ) del train , bureau , bureau_balance , bureau_agg , bureau_agg_new , bureau_balance_agg , bureau_balance_counts , bureau_by_loan , bureau_balance_by_client , bureau_counts gc . collect ( )
1509	x = tf . repeat ( tf . range ( DIM // 2 , - DIM // 2 , - 1 ) , DIM ) y = tf . tile ( tf . range ( - DIM // 2 , DIM // 2 ) , [ DIM ] ) z = tf . ones ( [ DIM * DIM ] , dtype = 'int32' ) idx = tf . stack ( [ x , y , z ] )
1257	def build_new_df ( df , extension = 'jpeg' ) : new_df = pd . concat ( [ df , df ] ) new_df [ 'filename' ] = pd . concat ( [ df [ 'id_code' ] . apply ( lambda string : string + f'_s1.{extension}' ) , df [ 'id_code' ] . apply ( lambda string : string + f'_s2.{extension}' ) ] ) return new_df new_train = build_new_df ( train_df ) new_test = build_new_df ( test_df ) new_train . to_csv ( 'new_train.csv' , index = False ) new_test . to_csv ( 'new_test.csv' , index = False )
1198	sub_df = pd . DataFrame ( data = { 'fullVisitorId' : test_df [ 'fullVisitorId' ] , 'predictedRevenue' : np . expm1 ( pred_test ) } ) sub_df = sub_df . groupby ( 'fullVisitorId' ) . sum ( ) . reset_index ( ) sub_df . columns = [ 'fullVisitorId' , 'predictedLogRevenue' ] sub_df [ 'predictedLogRevenue' ] = np . log1p ( sub_df [ 'predictedLogRevenue' ] ) sub_df . to_csv ( file_name , index = False ) def visualize_results ( results ) :
1169	print ( df . shape ) print ( train_clean_rob . shape ) return train_clean_rob
1431	print ( '[!]Saving training data file at:' , PATH_SAVE_DATA_TRAIN , ' ...' ) np . save ( PATH_SAVE_DATA_TRAIN , train_data ) print ( '[!]Saving testing data file at:' , PATH_SAVE_DATA_TEST , ' ...' ) np . save ( PATH_SAVE_DATA_TEST , test_data )
629	df_grouped_usa = get_df_country_cases ( df_covid , "US" ) df_usa_cases_by_day = df_grouped_usa [ df_grouped_usa . confirmed > 0 ] df_usa_cases_by_day = df_usa_cases_by_day . reset_index ( drop = True ) df_usa_cases_by_day [ 'day' ] = df_usa_cases_by_day . date . apply ( lambda x : ( x - df_usa_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_usa_cases_by_day = df_usa_cases_by_day [ reordered_columns ] df_usa_cases_by_day
290	shutil . rmtree ( 'base_dir' )
911	_ , idx = np . unique ( categorical , axis = 1 , return_index = True ) categorical = categorical . iloc [ : , idx ] return categorical
514	NCAATourney_SummaryW = pd . DataFrame ( ) NCAATourney_SummaryL = pd . DataFrame ( ) NCAATourney_SummaryW [ 'WINS' ] = df_tourney [ 'WTeamID' ] . groupby ( [ df_tourney [ 'Season' ] , df_tourney [ 'WTeamID' ] ] ) . count ( ) NCAATourney_SummaryL [ 'LOSSES' ] = df_tourney [ 'LTeamID' ] . groupby ( [ df_tourney [ 'Season' ] , df_tourney [ 'LTeamID' ] ] ) . count ( )
1484	def jsonl_iterator ( jsonl_files , to_json = False ) : for file_path in jsonl_files : with open ( file_path , "r" , encoding = "UTF-8" ) as fp : for jsonl in fp : raw_example = jsonl if to_json : raw_example = json . loads ( jsonl ) yield raw_example creator = TFExampleCreator ( is_training = False ) nq_lines = jsonl_iterator ( [ FLAGS . predict_file ] ) creator . process_nq_lines ( nq_lines = nq_lines , output_tfrecord = FLAGS . test_tf_record , max_examples = 0 , collect_nq_features = False )
84	plt . figure ( ) ax = df [ 'Class' ] . value_counts ( ) . plot ( kind = 'bar' ) ax . set_title ( 'Class Distribution Over Entries' ) ax . set_xlabel ( 'Class' ) ax . set_ylabel ( 'Frequency' ) plt . tight_layout ( ) plt . show ( )
1324	GlobalParams . __new__ . __defaults__ = ( None , ) * len ( GlobalParams . _fields ) BlockArgs . __new__ . __defaults__ = ( None , ) * len ( BlockArgs . _fields ) def relu_fn ( x ) :
4	drop = [ "timestamp" ] df = df . drop ( drop , axis = 1 ) gc . collect ( )
750	from itertools import product tta_transforms = [ ] for tta_combination in product ( [ TTAHorizontalFlip ( ) , None ] , [ TTAVerticalFlip ( ) , None ] , [ TTARotate90 ( ) , None ] ) : tta_transforms . append ( TTACompose ( [ tta_transform for tta_transform in tta_combination if tta_transform ] ) )
1642	temp = train [ 'ip' ] . value_counts ( ) . reset_index ( name = 'counts' ) temp . columns = [ 'ip' , 'counts' ] temp [ : 10 ]
1070	image = cv . cvtColor ( image , cv . COLOR_BGR2RGB ) cap . release ( ) ax . imshow ( image ) ax . xaxis . set_visible ( False ) ax . yaxis . set_visible ( False ) ax . title . set_text ( f"FRAME 0: {video_file.split('/')[-1]}" ) plt . grid ( False )
616	STD = 0.01 old_data = train [ 'signal' ] new_data = old_data + np . random . normal ( 0 , STD , size = len ( train ) ) train [ 'signal' ] = new_data old_data = test [ 'signal' ] new_data = old_data + np . random . normal ( 0 , STD , size = len ( test ) ) test [ 'signal' ] = new_data del old_data , new_data
1036	avg_repaid = df . ix [ df [ 'TARGET' ] == 0 , var_name ] . median ( ) avg_not_repaid = df . ix [ df [ 'TARGET' ] == 1 , var_name ] . median ( ) plt . figure ( figsize = ( 12 , 6 ) )
184	group = train . groupby ( train . brand_name ) brands_prices = pd . DataFrame ( group . price . mean ( ) ) brands_prices . reset_index ( level = 0 , inplace = True )
1192	for n in [ num_cols + time_cols ] : train_df [ n ] = train_df [ n ] . fillna ( 0 ) . astype ( 'int' ) test_df [ n ] = test_df [ n ] . fillna ( 0 ) . astype ( 'int' )
971	random = pd . read_csv ( '../input/home-credit-model-tuning/random_search_simple.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) opt = pd . read_csv ( '../input/home-credit-model-tuning/bayesian_trials_simple.csv' ) . sort_values ( 'score' , ascending = False ) . reset_index ( ) print ( 'Best score from random search: {:.5f} found on iteration: {}.' . format ( random . loc [ 0 , 'score' ] , random . loc [ 0 , 'iteration' ] ) ) print ( 'Best score from bayesian optimization: {:.5f} found on iteration: {}.' . format ( opt . loc [ 0 , 'score' ] , opt . loc [ 0 , 'iteration' ] ) )
1019	test = pd . read_csv ( '../input/application_test.csv' ) test = test . merge ( bureau_counts , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( bureau_agg , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( bureau_balance_by_client , on = 'SK_ID_CURR' , how = 'left' )
863	model . n_estimators = len ( cv_results [ 'auc-mean' ] ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] baseline_auc = roc_auc_score ( test_labels , preds ) print ( 'The baseline model scores {:.5f} ROC AUC on the test set.' . format ( baseline_auc ) )
551	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
1507	for image , labe in training_dataset : pass end = datetime . datetime . now ( ) elapsed = ( end - start ) . total_seconds ( ) average = elapsed / n_iter print ( "Average timing for 1 iteration = {}" . format ( average ) )
935	results . sort_values ( 'score' , ascending = False , inplace = True ) results . reset_index ( inplace = True ) return results
1416	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
299	dst = os . path . join ( train_dir , sub_folder , fname ) image = cv2 . imread ( src ) image = cv2 . resize ( image , ( IMAGE_HEIGHT , IMAGE_WIDTH ) ) cv2 . imwrite ( dst , image )
770	heads [ 'bonus' ] = 1 * ( heads [ 'refrig' ] + heads [ 'computer' ] + ( heads [ 'v18q1' ] > 0 ) + heads [ 'television' ] ) sns . violinplot ( 'bonus' , 'Target' , data = heads , figsize = ( 10 , 6 ) ) ; plt . title ( 'Target vs Bonus Variable' ) ;
1312	reduce_valid = pd . DataFrame ( ) for i , row in reduce_train_org . groupby ( 'installation_id' , sort = False ) : reduce_valid = reduce_valid . append ( row . sample ( 1 ) ) reduce_train = reduce_train_org . drop ( reduce_valid . index )
1472	FIGSIZE = 13.0 SPACING = 0.1 subplot = ( rows , cols , 1 ) if rows < cols : plt . figure ( figsize = ( FIGSIZE , FIGSIZE / cols * rows ) ) else : plt . figure ( figsize = ( FIGSIZE / rows * cols , FIGSIZE ) )
1774	train_loader = torch . utils . data . DataLoader ( train , batch_size = batch_size , shuffle = True ) valid_loader = torch . utils . data . DataLoader ( valid , batch_size = batch_size , shuffle = False ) print ( f'Fold {i + 1}' ) for epoch in range ( n_epochs ) :
1607	def oneHotEncode_dataframe ( df , features ) : for feature in features : temp_onehot_encoded = pd . get_dummies ( df [ feature ] ) column_names = [ "{}_{}" . format ( feature , x ) for x in temp_onehot_encoded . columns ] temp_onehot_encoded . columns = column_names df = df . drop ( feature , axis = 1 ) df = pd . concat ( [ df , temp_onehot_encoded ] , axis = 1 ) return df
1061	for i in range ( depth ) : ins = self . in_channels if i == 0 else outs outs = self . start_filts * ( 2 ** i ) pooling = True if i < depth - 1 else False down_conv = DownConv ( ins , outs , pooling = pooling ) self . down_convs . append ( down_conv )
212	feature_score [ 'mean' ] = feature_score . mean ( axis = 1 ) pred_train [ 'mean' ] = pred_train . mean ( axis = 1 ) pred_test [ 'mean' ] = pred_test . mean ( axis = 1 ) acc . append ( round ( r2_score ( labels , pred_train [ 'mean' ] ) * 100 , 2 ) )
1084	if self . grayscale : img_temp = cv2 . imread ( img_src , 0 ) else : img_temp = cv2 . imread ( img_src )
300	val_list = list ( df_val [ 'file_name' ] ) for fname in val_list : label = df_data . loc [ fname , 'binary_target' ] if label == 0 : sub_folder = 'a_0'
658	def perform_rfc ( df_X , df_Y , test_df_X , test_Y ) : rfr_clf = RandomForestRegressor ( n_estimators = 100 , oob_score = True , max_features = "auto" ) rfr_clf . fit ( df_X , df_Y ) pred_Y = rfr_clf . predict ( test_df_X ) r2_score_rfc = round ( r2_score ( test_Y , pred_Y ) , 3 ) accuracy = round ( rfr_clf . score ( df_X , df_Y ) * 100 , 2 ) returnval = { 'model' : 'RandomForestRegressor' , 'r2_score' : r2_score_rfc } return returnval
1768	np . random . seed ( 123 ) trn_idx = np . random . permutation ( len ( train_X ) ) train_X = train_X [ trn_idx ] train_y = train_y [ trn_idx ] features = features [ trn_idx ] return train_X , test_X , train_y , features , test_features , tokenizer . word_index
252	decision_tree = DecisionTreeRegressor ( ) decision_tree . fit ( train , target ) acc_model ( 5 , decision_tree , train , test )
1232	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) train2 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv" ) train2 . toxic = train2 . toxic . round ( ) . astype ( int ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
1339	self . _dropout = self . _global_params . dropout_rate self . _fc = nn . Linear ( out_channels , self . _global_params . num_classes ) def extract_features ( self , inputs ) :
764	msizes = list ( range ( sqr_min , sqr_max , int ( ( sqr_max - sqr_min ) / 5 ) ) ) markers = [ ]
210	numerics = [ 'int8' , 'int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64' ] categorical_columns = [ ] features = data . columns . values . tolist ( ) for col in features : if data [ col ] . dtype in numerics : continue categorical_columns . append ( col )
468	import pandas as pd import numpy as np from sklearn . model_selection import StratifiedKFold from sklearn import metrics import gc import xgboost as xgb pd . set_option ( 'display.max_columns' , 200 )
819	train_df [ '%s_c1' % name ] = reduction [ : , 0 ] train_df [ '%s_c2' % name ] = reduction [ : , 1 ] train_df [ '%s_c3' % name ] = reduction [ : , 2 ] print ( f'Method: {name} {round(end - start, 2)} seconds elapsed.' )
642	neutral_train [ 'temp_list' ] = neutral_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) neutral_train [ 'temp_list' ] = neutral_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) neutral_top = Counter ( [ item for sublist in neutral_train [ 'temp_list' ] for item in sublist ] ) neutral_temp = pd . DataFrame ( neutral_top . most_common ( 20 ) ) neutral_temp . columns = [ 'Common_words' , 'count' ] neutral_temp . style . background_gradient ( cmap = 'Blues' )
81	x = Dropout ( 0.5 ) ( pool ) x = Flatten ( ) ( x ) x = Dense ( 128 , activation = 'relu' ) ( x ) x = Dropout ( 0.5 ) ( x )
712	test2p = pd . concat ( [ test2 [ : n ] , test2 [ - n : ] ] , axis = 0 ) test2p . loc [ test2p [ 'target' ] >= 0.5 , 'target' ] = 1 test2p . loc [ test2p [ 'target' ] < 0.5 , 'target' ] = 0 train2p = pd . concat ( [ train2p , test2p ] , axis = 0 ) train2p . reset_index ( drop = True , inplace = True )
693	from pathlib import Path if Path ( previous_model_name ) . is_file ( ) : print ( "Using previous sucessful run's model" ) model2 = load_model ( previous_model_name , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } ) else : print ( "Using stored trained model" ) model2 = load_model ( stored_trained_model , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } )
1816	train = pd . read_csv ( '/kaggle/input/3d-object-detection-for-autonomous-vehicles/train.csv' ) sub = pd . read_csv ( '/kaggle/input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv' ) print ( train . shape ) train . head ( )
651	submission_df = pd . DataFrame ( columns = [ 'id' ] + label_names ) submission_df [ 'id' ] = test_df [ 'id' ] . values submission_df [ label_names ] = y_test submission_df . to_csv ( "./cnn_fasttext_submission.csv" , index = False )
454	with tf . gfile . Open ( image_path , "rb" ) as binfile : image_string = binfile . read ( ) result_out , image_out = session . run ( [ result , decoded_image ] , feed_dict = { image_string_placeholder : image_string } ) print ( "Found %d objects." % len ( result_out [ "detection_scores" ] ) )
35	embeddings_train = [ ] for i in tqdm ( range ( 25 ) ) : embeddings = embed ( train_text [ i ] ) embeddings_train . append ( embeddings )
633	has_to_run_sir = True has_to_run_sird = False has_to_run_seir = True has_to_run_seird = False has_to_run_seirdq = True
656	train_df_cat = ret_train_df . loc [ : , [ 'X0' , 'X1' , 'X2' , 'X3' , 'X4' , 'X5' , 'X6' , 'X8' ] ] test_df_cat = ret_test_df . loc [ : , [ 'X0' , 'X1' , 'X2' , 'X3' , 'X4' , 'X5' , 'X6' , 'X8' ] ] train_df_cat = train_df_cat . add_prefix ( 'train_' ) test_df_cat = test_df_cat . add_prefix ( 'test_' ) combined = train_df_cat . append ( test_df_cat , ignore_index = True )
467	g = plt . figure ( 2 ) precision , recall , _ = precision_recall_curve ( train_df . iloc [ valid_idx ] [ target ] . values , oof [ valid_idx ] ) y_real . append ( train_df . iloc [ valid_idx ] [ target ] . values ) y_proba . append ( oof [ valid_idx ] ) plt . plot ( recall , precision , lw = 2 , alpha = 0.3 , label = 'P|R fold %d' % ( i ) ) i = i + 1
80	convs = [ ] filter_sizes = [ 3 , 4 , 5 ] for filter_size in filter_sizes : l_conv = Conv1D ( filters = 128 , kernel_size = filter_size , activation = 'relu' ) ( embedded_sequences ) l_pool = MaxPooling1D ( pool_size = 3 ) ( l_conv ) convs . append ( l_pool ) l_merge = Merge ( mode = 'concat' , concat_axis = 1 ) ( convs )
683	d = { "unicode" : label_count [ - 11 : - 1 , ] . index . values , "str" : [ unicode_map [ l ] for l in label_count [ - 11 : - 1 , ] . index . values ] } pd . DataFrame ( d )
1267	def compute_text_and_questions ( test , tokenizer ) : test_text = tokenizer . texts_to_sequences ( test . text . values ) test_questions = tokenizer . texts_to_sequences ( test . question . values ) test_text = sequence . pad_sequences ( test_text , maxlen = 300 ) test_questions = sequence . pad_sequences ( test_questions ) return test_text , test_questions
837	for lh in leg . legendHandles : lh . set_alpha ( 1 ) leg . set_title ( 'Fare Bin' , prop = { 'size' : 28 } )
1023	if encoding == 'ohe' : features = pd . get_dummies ( features ) test_features = pd . get_dummies ( test_features )
1464	test_filenames = sorted ( glob ( f"{data_dir}/Test/*.jpg" ) ) test_df = pd . DataFrame ( { 'ImageFileName' : list ( test_filenames ) } , columns = [ 'ImageFileName' ] ) batch_size = 16 num_workers = 4 test_dataset = Alaska2Dataset ( test_df , augmentations = AUGMENTATIONS_TEST , test = True ) test_loader = torch . utils . data . DataLoader ( test_dataset , batch_size = batch_size , num_workers = num_workers , shuffle = False , drop_last = False )
1059	img , _ = valid_ds [ 0 ] input_img = torch . unsqueeze ( variable ( img , volatile = True ) , dim = 0 ) mask = F . sigmoid ( model ( input_img ) ) out_mask = torch . squeeze ( mask . data . cpu ( ) , dim = 0 ) imshow ( img , out_mask )
301	dst = os . path . join ( val_dir , sub_folder , fname ) image = cv2 . imread ( src ) image = cv2 . resize ( image , ( IMAGE_HEIGHT , IMAGE_WIDTH ) ) cv2 . imwrite ( dst , image ) if label == 1 : sub_folder = 'b_1'
491	def correlation_heatmap ( df ) : _ , ax = plt . subplots ( figsize = ( 20 , 15 ) ) colormap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) _ = sns . heatmap ( df . corr ( ) , cmap = colormap , square = True , ) plt . title ( 'Pearson Correlation of Features' ) correlation_heatmap ( application_train )
1650	history = [ ] resa2 = clustering ( hits , stds , filters , phik = 3.3 , nu = nu , truth = truth , history = history ) resa2 [ "event_id" ] = event_num score = score_event_fast ( truth , resa2 . rename ( index = str , columns = { "label" : "track_id" } ) ) print ( "Your score: " , score )
587	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "bathroomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Bathroom Count ",title=" Bathroom Count Vs Log Error " )
1112	column_types = X . dtypes int_cols = column_types [ column_types == 'int' ] float_cols = column_types [ column_types == 'float' ] cat_cols = column_types [ column_types == 'object' ] print ( '\tinteger columns:\n{}' . format ( int_cols ) ) print ( '\n\tfloat columns:\n{}' . format ( float_cols ) ) print ( '\n\tto encode categorical columns:\n{}' . format ( cat_cols ) )
635	fitting_model = pm . Deterministic ( 'sir_model' , sir_ode_solver_wrapper ( theano . shared ( data_time ) , theano . shared ( infected_individuals ) , theano . shared ( np . array ( y0_sir ) ) , beta , ) )
1419	import numpy as np import pandas as pd from tqdm import tqdm import copy import multiprocessing
981	app_train = replace_day_outliers ( app_train ) app_test = replace_day_outliers ( app_test ) bureau = replace_day_outliers ( bureau ) bureau_balance = replace_day_outliers ( bureau_balance ) credit = replace_day_outliers ( credit ) cash = replace_day_outliers ( cash ) previous = replace_day_outliers ( previous ) installments = replace_day_outliers ( installments )
1182	numerator = tf . reduce_sum ( conf_mtx * weight_mtx ) denominator = tf . reduce_sum ( out_prod * weight_mtx ) kp = 1 - ( numerator / denominator ) return kp def get_config ( self ) :
1356	sns . distplot ( leak_df . pred3_l1p ) sns . distplot ( leak_df . meter_reading_l1p ) leak_score = np . sqrt ( mean_squared_error ( leak_df . pred3_l1p , leak_df . meter_reading_l1p ) ) print ( 'score3=' , leak_score )
23	train [ 'outliers' ] = 0 train . loc [ train [ 'target' ] < - 30 , 'outliers' ] = 1 train [ 'outliers' ] . value_counts ( )
1818	def check_index ( index , indecies ) : if index in indecies : return True else : return False
102	transforms_train = albumentations . Compose ( [ GridMask ( num_grid = 3 , rotate = 15 , p = 1 ) , ] )
1665	for c in X_train . columns : if X_train [ c ] . dtype == 'float16' or X_train [ c ] . dtype == 'float32' or X_train [ c ] . dtype == 'float64' : X_train [ c ] . fillna ( X_train [ c ] . mean ( ) ) X_test [ c ] . fillna ( X_train [ c ] . mean ( ) )
383	DATA_DIR = r'../input' TEST_DIR = r'../input/test' print ( 'ok' )
1009	for col in df : if col != group_var and 'SK_ID' in col : df = df . drop ( columns = col ) group_ids = df [ group_var ] numeric_df = df . select_dtypes ( 'number' ) numeric_df [ group_var ] = group_ids
557	ax = plt . subplot ( 122 ) sns . barplot ( x = 'count' , y = 'Interactions' , data = data . sort_values ( 'sum' , ascending = False ) , ax = ax ) ax . set_title ( 'No. of times Feature interacted' , fontweight = 'bold' , fontsize = 14 ) plt . tight_layout ( ) plot_feat_interaction ( data )
665	date_agg_4 = train_agg . groupby ( level = [ 1 , 2 , 3 ] ) . sum ( ) date_agg_4 . columns = ( 'bookings' , 'total' ) date_agg_4 . reset_index ( inplace = True ) date_agg_4 [ 'dt' ] = pd . to_datetime ( date_agg_4 . year * 10000 + date_agg_4 . month * 100 + date_agg_4 . day , format = '%Y%m%d' ) date_agg_4 . head ( )
662	date_agg_1 = train_agg . groupby ( level = 0 ) . agg ( [ 'sum' ] ) date_agg_1 . columns = ( 'bookings' , 'total' ) date_agg_1 . head ( )
1233	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
923	plt . figure ( figsize = ( 8 , 6 ) ) plt . plot ( list ( range ( len ( df ) ) ) , df [ 'cumulative_importance' ] , 'b-' ) plt . xlabel ( 'Number of Features' , size = 16 ) ; plt . ylabel ( 'Cumulative Importance' , size = 16 ) ; plt . title ( 'Cumulative Feature Importance' , size = 18 ) ;
1053	valid_score = model . best_score_ [ 'valid' ] [ 'auc' ] train_score = model . best_score_ [ 'train' ] [ 'auc' ] valid_scores . append ( valid_score ) train_scores . append ( train_score )
549	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
131	test_filenames = os . listdir ( "../input/test1/test1" ) test_df = pd . DataFrame ( { 'filename' : test_filenames } ) nb_samples = test_df . shape [ 0 ]
755	df [ 'dependency' ] = df [ 'dependency' ] . replace ( mapping ) . astype ( np . float64 ) df [ 'edjefa' ] = df [ 'edjefa' ] . replace ( mapping ) . astype ( np . float64 ) df [ 'edjefe' ] = df [ 'edjefe' ] . replace ( mapping ) . astype ( np . float64 ) train [ [ 'dependency' , 'edjefa' , 'edjefe' ] ] . describe ( )
1201	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
1302	with strategy . scope ( ) : transformer_layer = TFAutoModel . from_pretrained ( MODEL ) model = build_model_PT ( transformer_layer , max_len = MAX_LEN )
1733	import seaborn as sns import numpy as np import pandas as pd from mpl_toolkits . mplot3d import Axes3D import matplotlib . pyplot as plt sns . set ( style = 'darkgrid' ) import os print ( os . listdir ( "../input" ) ) from pylab import rcParams rcParams [ 'figure.figsize' ] = 25 , 12.5 train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
1734	test [ 'meaneduc' ] . fillna ( test [ 'meaneduc' ] . mean ( ) , inplace = True ) test [ 'SQBmeaned' ] . fillna ( test [ 'SQBmeaned' ] . mean ( ) , inplace = True ) train [ 'rez_esc' ] . fillna ( 0 , inplace = True ) train [ 'v18q1' ] . fillna ( 0 , inplace = True ) train [ 'v2a1' ] . fillna ( 0 , inplace = True )
146	print ( "Number of different values :" ) print ( "IP :" , len ( df [ 'ip' ] . unique ( ) ) ) print ( "App :" , len ( df [ 'app' ] . unique ( ) ) ) print ( "Device :" , len ( df [ 'device' ] . unique ( ) ) ) print ( "OS :" , len ( df [ 'os' ] . unique ( ) ) ) print ( "Channel :" , len ( df [ 'channel' ] . unique ( ) ) )
1103	train = pd . read_csv ( "../input/application_train.csv" ) test = pd . read_csv ( "../input/application_test.csv" ) bureau = pd . read_csv ( "../input/bureau.csv" ) bureau_bal = pd . read_csv ( '../input/bureau_balance.csv' )
1532	clf = RandomForestClassifier ( n_estimators = 50 , criterion = 'gini' , max_depth = 5 , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = - 1 , random_state = 0 , verbose = 0 , warm_start = False , class_weight = 'balanced' )
809	model . fit ( X_train , y_train , early_stopping_rounds = 100 , eval_metric = macro_f1_score , eval_set = [ ( X_train , y_train ) , ( X_valid , y_valid ) ] , eval_names = [ 'train' , 'valid' ] , verbose = 400 ) end = timer ( )
1477	start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_mixup ( images , labels , PROBABILITY = 1.0 ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_mixup: {timing}" )
895	zero_features = list ( feature_importances [ feature_importances [ 'importance' ] == 0.0 ] [ 'feature' ] ) print ( 'There are %d features with 0.0 importance' % len ( zero_features ) ) feature_importances . tail ( )
1696	def evaluate ( program : [ ] , input_image : np . array ) : input_image = np . array ( input_image ) assert type ( input_image ) == np . ndarray image_list = [ input_image ] for fct in program : image_list = fct ( image_list ) image_list = [ img for img in image_list if img . shape [ 0 ] > 0 and img . shape [ 1 ] > 0 ] if image_list == [ ] : return [ ] return image_list
1741	print ( "Handling missing values..." ) def handle_missing ( dataset ) : dataset . category_name . fillna ( value = "missing" , inplace = True ) dataset . brand_name . fillna ( value = "missing" , inplace = True ) dataset . item_description . fillna ( value = "missing" , inplace = True ) return ( dataset ) train = handle_missing ( train ) test = handle_missing ( test ) print ( train . shape ) print ( test . shape )
861	train_features , test_features , train_labels , test_labels = train_test_split ( features , labels , test_size = 6000 , random_state = 42 ) print ( 'Train shape: ' , train_features . shape ) print ( 'Test shape: ' , test_features . shape ) train_features . head ( )
1797	orig = plt . plot ( timeseries , color = 'blue' , label = 'Original' ) mean = plt . plot ( rolmean , color = 'red' , label = 'Rolling Mean' ) std = plt . plot ( rolstd , color = 'black' , label = 'Rolling Std' ) plt . legend ( loc = 'best' ) plt . title ( 'Rolling Mean & Standard Deviation' ) plt . show ( block = False )
1006	feature_matrix2 = selection . remove_low_information_features ( feature_matrix ) print ( 'Removed %d features from training features' % ( feature_matrix . shape [ 1 ] - feature_matrix2 . shape [ 1 ] ) ) feature_matrix_test2 = selection . remove_low_information_features ( feature_matrix_test ) print ( 'Removed %d features from testing features' % ( feature_matrix_test . shape [ 1 ] - feature_matrix_test2 . shape [ 1 ] ) )
1761	print ( 'Label encoding categorical features...' ) categorical_feats = input_df . columns [ input_df . dtypes == 'object' ] for feat in categorical_feats : encoder = LabelEncoder ( ) input_df [ feat ] = encoder . fit_transform ( input_df [ feat ] . fillna ( 'NULL' ) ) print ( 'Label encoding complete.' ) return input_df , categorical_feats . tolist ( ) , encoder_dict
1095	y_pred_valid = model . predict ( X_val ) y_pred_test = model . predict ( X_test ) del X_tr , X_val , X_test gc . collect ( )
653	def read_training_data ( filepath ) : train_df = pd . read_csv ( filepath ) return train_df
1081	while duration < 2500 and pos < total_rows : for i in range ( 0 , 6 ) : if pos + batch_size > total_rows : batch_size = total_rows - pos X_p = f_matrix_train [ pos : pos + batch_size ] y_p = y [ i ] [ pos : pos + batch_size ] model [ i ] . partial_fit ( X_p , y_p , classes ) pos = pos + batch_size duration = time . time ( ) - start_train print ( "Pos %d/%d duration %d" % ( pos , total_rows , duration ) )
1513	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_tfrecord if labeled else read_unlabeled_tfrecord , num_parallel_calls = AUTO ) return dataset def count_data_items ( filenames ) :
603	from sklearn . preprocessing import MultiLabelBinarizer , LabelEncoder binarize = MultiLabelBinarizer ( classes = classes ) encode = LabelEncoder ( ) Y_split = encode . fit_transform ( first_labels_set ) Y = binarize . fit_transform ( all_labels_set )
1666	model = StackNetClassifier ( models , metric = "auc" , folds = 4 , restacking = False , use_retraining = True , use_proba = True , random_state = 0 , n_jobs = - 1 , verbose = 1 )
811	OUT_FILE = 'optimization.csv' of_connection = open ( OUT_FILE , 'w' ) writer = csv . writer ( of_connection ) MAX_EVALS = 100 PROGRESS = 10 N_FOLDS = 5 ITERATION = 0
913	df_agg_cat = agg_categorical ( df , parent_var , '%s_LOAN' % df_name ) df_agg_cat = df_agg_cat . merge ( parent_df , on = parent_var , how = 'left' )
2	weather_df [ "datetime" ] = pd . to_datetime ( weather_df [ "timestamp" ] ) weather_df [ "day" ] = weather_df [ "datetime" ] . dt . day weather_df [ "week" ] = weather_df [ "datetime" ] . dt . week weather_df [ "month" ] = weather_df [ "datetime" ] . dt . month
479	from keras . preprocessing . text import hashing_trick text = 'The quick brown fox jumped over the lazy dog.' words = set ( text_to_word_sequence ( text ) ) vocab_size = len ( words ) print ( vocab_size ) result = hashing_trick ( text , round ( vocab_size * 1.3 ) , hash_function = 'md5' ) print ( result )
1325	multiplier = global_params . width_coefficient if not multiplier : return filters divisor = global_params . depth_divisor min_depth = global_params . min_depth filters *= multiplier min_depth = min_depth or divisor new_filters = max ( min_depth , int ( filters + divisor / 2 ) // divisor * divisor ) if new_filters < 0.9 * filters : new_filters += divisor return int ( new_filters ) def round_repeats ( repeats , global_params ) :
117	submit_data = pd . concat ( [ Pred_data_val , Pred_data_eval ] ) submit_data = submit_data . reset_index ( ) . drop ( "index" , axis = 1 ) col = sample . loc [ : , 'F1' : ] . columns sample [ col ] = submit_data
1789	from pylab import rcParams import statsmodels . api as sm from statsmodels . tsa . stattools import adfuller
130	example_df = train_df . sample ( n = 1 ) . reset_index ( drop = True ) example_generator = train_datagen . flow_from_dataframe ( example_df , "../input/train/train/" , x_col = 'filename' , y_col = 'category' , target_size = IMAGE_SIZE , class_mode = 'categorical' )
1369	for index in pd_districts . index : plt . annotate ( pd_districts . loc [ index ] . district , ( pd_districts . loc [ index ] . geometry . centroid . x , pd_districts . loc [ index ] . geometry . centroid . y ) , color = ' fontsize=' large ', fontweight=' heavy ', horizontalalignment=' center ' ) ax . set_axis_off ( ) plt . show ( )
157	print ( "Memory usage before optimization :" , str ( round ( total_before_opti / 1000000000 , 2 ) ) + 'GB' ) print ( "Memory usage after optimization :" , str ( round ( sum ( df . memory_usage ( ) ) / 1000000000 , 2 ) ) + 'GB' ) print ( "We reduced the dataframe size by" , str ( round ( ( ( total_before_opti - sum ( df . memory_usage ( ) ) ) / total_before_opti ) * 100 , 2 ) ) + '%' )
815	model = lgb . LGBMClassifier ( ** best_hyp , class_weight = 'balanced' , random_state = 10 ) model . fit ( X_train , y_train ) ;
581	if len ( batch ) > BATCH_SIZE + 1 : n_steps = math . ceil ( len ( batch ) / BATCH_SIZE ) for step in range ( n_steps ) : small_batch = batch [ step * BATCH_SIZE : ( step + 1 ) * BATCH_SIZE ] small_batch = torch . stack ( small_batch ) . to ( DEVICE ) . float ( )
1102	submission_df = X_test [ [ 'SK_ID_CURR' ] ] . copy ( ) submission_df [ 'TARGET' ] = submission_preds submission_df [ [ 'SK_ID_CURR' , 'TARGET' ] ] . to_csv ( 'submission.csv' , index = False )
1177	colors . SetColor ( "Furniture" , [ 204 , 204 , 153 , 255 ] ) scalarRange = [ 0.0 , 0.0 ] maxTime = 0 aren = vtk . vtkRenderer ( )
1465	path_data = '../input' device = 'cuda' batch_size = 32 torch . manual_seed ( 0 )
1293	path_to_train = '../input/imet-2019-fgvc6/train/' data = pd . read_csv ( '../input/imet-2019-fgvc6/train.csv' ) train_dataset_info = [ ] for name , labels in zip ( data [ 'id' ] , data [ 'attribute_ids' ] . str . split ( ' ' ) ) : train_dataset_info . append ( { 'path' : os . path . join ( path_to_train , name ) , 'labels' : np . array ( [ int ( label ) for label in labels ] ) } ) train_dataset_info = np . array ( train_dataset_info )
1344	for task , prediction , solved in tqdm ( zip ( train_tasks , train_predictions , train_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
1743	dtrain , dvalid = train_test_split ( train , random_state = 123 , train_size = 0.99 ) print ( dtrain . shape ) print ( dvalid . shape )
342	ID = df_preds [ 'patientId' ] preds = df_preds [ 'PredictionString' ] submission = pd . DataFrame ( { 'patientId' : ID , 'PredictionString' : preds , } ) . set_index ( 'patientId' ) submission . to_csv ( 'pneu_keras_model.csv' , columns = [ 'PredictionString' ] )
611	if self . save_to_dir : for i , j in enumerate ( index_array ) : img = array_to_img ( batch_x [ i ] , self . data_format , scale = True ) fname = '{prefix}_{index}_{hash}.{format}' . format ( prefix = self . save_prefix , index = j , hash = np . random . randint ( 1e7 ) , format = self . save_format ) img . save ( os . path . join ( self . save_to_dir , fname ) )
394	from sklearn . tree import DecisionTreeClassifier dt = DecisionTreeClassifier ( ) dt = dt . fit ( xT , yT )
767	corr_matrix = heads . corr ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in upper . columns if any ( abs ( upper [ column ] ) > 0.95 ) ] to_drop
10	test_df = test_df . merge ( weather_df , how = 'left' , on = [ 'timestamp' , 'site_id' ] ) del weather_df gc . collect ( )
1535	train = pd . read_csv ( "../input/web-traffic-time-series-forecasting/train_1.csv" ) keys = pd . read_csv ( "../input/web-traffic-time-series-forecasting/key_1.csv" ) ss = pd . read_csv ( "../input/web-traffic-time-series-forecasting/sample_submission_1.csv" )
1698	images = evaluate ( program , i ) if len ( images ) < 1 : return False
726	import os print ( '' . join ( [ str ( train_photos . photo_id [ 0 ] ) , '.jpg' ] ) ) from PIL import Image im = Image . open ( os . path . join ( '../input/' , 'train_photos' , '' . join ( [ str ( train_photos . photo_id [ 0 ] ) , '.jpg' ] ) ) ) plt . imshow ( im )
1631	fulltable_us = pd . read_csv ( '../input/us-counties-covid-19-dataset/us-counties.csv' ) fulltable_us = fulltable_us . drop ( [ 'fips' ] , axis = 1 ) . groupby ( [ 'date' , 'state' ] ) . sum ( ) . reset_index ( ) fulltable_us . columns = [ 'Date' , 'Province/State' , 'Confirmed' , 'Deaths' ] fulltable_us [ 'Date' ] = pd . to_datetime ( fulltable_us [ 'Date' ] ) fulltable_us
599	imid = 'TCGA-G9-6362-01Z-00-DX1' image_path = df [ df . id == imid ] . path . values [ 0 ] image = cv2 . imread ( image_path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( image ) plt . show ( )
219	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) import matplotlib . pyplot as plt import featuretools as ft from featuretools . primitives import * from featuretools . variable_types import Numeric from sklearn . preprocessing import LabelEncoder , MinMaxScaler from sklearn . svm import LinearSVR from sklearn . feature_selection import SelectFromModel import warnings warnings . filterwarnings ( "ignore" )
1672	pid = row [ 'patientId' ] if pid not in parsed : parsed [ pid ] = { 'dicom' : '../input/stage_1_train_images/%s.dcm' % pid , 'label' : row [ 'Target' ] , 'boxes' : [ ] }
720	import warnings warnings . filterwarnings ( "ignore" ) import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns from sklearn . impute import SimpleImputer import scipy from sklearn . linear_model import LogisticRegression from sklearn . metrics import auc , roc_curve from sklearn . model_selection import StratifiedKFold , GridSearchCV from tqdm import tqdm_notebook
1489	candidates_dict = { } if input_path . endswith ( ".gz" ) : with gzip . GzipFile ( fileobj = tf . io . gfile . GFile ( input_path , "rb" ) ) as input_file : print ( "Reading examples from: {}" . format ( input_path ) ) for index , line in enumerate ( input_file ) : e = json . loads ( line ) candidates_dict [ e [ "example_id" ] ] = e [ "long_answer_candidates" ]
896	ax . barh ( list ( reversed ( list ( df . index [ : 15 ] ) ) ) , df [ 'importance_normalized' ] . head ( 15 ) , align = 'center' , edgecolor = 'k' )
169	x = BatchNormalization ( ) ( x ) x = Activation ( 'relu' ) ( x ) return x def build_model ( input_dims , output_dim , n , k , act = "relu" , dropout = None ) :
1813	plt . plot ( history [ 'loss' ] ) plt . plot ( history [ 'val_loss' ] ) plt . title ( 'model loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ( [ 'train' , 'test' ] , loc = 'upper right' ) ;
463	from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import train_test_split , StratifiedKFold , KFold from bayes_opt import BayesianOptimization from datetime import datetime from sklearn . metrics import precision_score , recall_score , confusion_matrix , accuracy_score , roc_auc_score , f1_score , roc_curve , auc , precision_recall_curve from sklearn import metrics from sklearn import preprocessing
456	import numpy as np import pandas as pd import os from tqdm . auto import tqdm import shutil as sh import matplotlib . pyplot as plt
1428	print ( '[!]Adding \'PAD\' to each sequence...' ) for i in tqdm ( range ( len ( clean_ ) ) ) : sentence = clean_ [ i ] [ : : - 1 ] for _ in range ( maxlen - len ( sentence ) ) : sentence . append ( 'PAD' ) clean_ [ i ] = sentence [ : : - 1 ] print ( ) PAD = np . zeros ( word2vec_ [ 'guy' ] . shape )
889	train , test = train . align ( test , join = 'inner' , axis = 1 ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
33	X = pd . DataFrame ( index = range ( 0 , len ( data ) ) ) X [ 'ds' ] = data . index X [ 'y' ] = data [ 'visits' ] . values X . tail ( )
795	if 'n_estimators' in hyp : del hyp [ 'n_estimators' ] params = hyp else :
941	eval_results = objective ( parameters , i ) results . loc [ i , : ] = eval_results i += 1
630	S , I , R , D = X S_prime = - beta * S * I I_prime = beta * S * I - zeta * I - delta * I R_prime = zeta * I D_prime = delta * I return S_prime , I_prime , R_prime , D_prime @ jit ( nopython = True ) def seir2_model ( t , X , alpha = 1 / 5 , beta = 1 , gamma = 0 , zeta = 1 / 15 , delta = 0.02 ) :
1405	def compute_rolling_mean_per_store_df ( df , period = 30 ) : return ( df . set_index ( "date" ) . groupby ( "store_id" ) . rolling ( period ) . mean ( ) . reset_index ( ) )
356	fname = get_filename ( image_id , image_type ) img = cv2 . imread ( fname ) assert img is not None , "Failed to read image : %s, %s" % ( image_id , image_type ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) return img
1135	avsig_timestamp = np . load ( '../input/timestamps/AvSigVersionTimestamps.npy' ) [ ( ) ] osver_timestamp = np . load ( '../input/timestamps/OSVersionTimestamps.npy' ) [ ( ) ] print ( 'data loaded.' )
1460	total = new_merchant . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( new_merchant . isnull ( ) . sum ( ) / new_merchant . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_data . head ( 20 )
739	ratio = float ( l_dis ) / ( curve_dis + eps ) if ratio > 1 : return [ x , y ]
1035	_ , idx = np . unique ( categorical , axis = 1 , return_index = True ) categorical = categorical . iloc [ : , idx ] return categorical
676	list_train2 . append ( y ) ; list_valid2 . append ( y_valid ) list_train_fc2 . append ( train_fc ) ; list_valid_fc2 . append ( valid_fc ) list_inpfeat2 . append ( inpfeature )
893	train_labels = train [ "TARGET" ] train_ids = train [ 'SK_ID_CURR' ] test_ids = test [ 'SK_ID_CURR' ] train = pd . get_dummies ( train . drop ( columns = all_missing ) ) test = pd . get_dummies ( test . drop ( columns = all_missing ) ) train , test = train . align ( test , join = 'inner' , axis = 1 ) print ( 'Training set full shape: ' , train . shape ) print ( 'Testing set full shape: ' , test . shape )
409	vectorizer = TfidfVectorizer ( min_df = 0.00009 , max_features = 200000 , tokenizer = lambda x : x . split ( ) , ngram_range = ( 1 , 3 ) ) X_train_multilabel = vectorizer . fit_transform ( X_train [ 'question' ] ) X_test_multilabel = vectorizer . transform ( X_test [ 'question' ] )
1066	X = layers . Dense ( dense_layer_sizes [ 0 ] , name = 'dense0' ) ( X_input ) X = layers . BatchNormalization ( name = 'bn0' ) ( X ) X = layers . Activation ( 'relu' ) ( X ) X = layers . Dropout ( dropout_rate , seed = 7 ) ( X )
1151	filenames_tr = tr_df . image_filename . values y_tr = tr_df . AdoptionSpeed . values y_tr = to_categorical ( y_tr ) print ( 'train set shapes:' ) print ( filenames_tr . shape , y_tr . shape ) tr_parser = PetfinderImageParser ( preproc_func , image_size , train_aug ) tr_datagen = PetfinderDataGenerator ( filenames_tr , y_tr , tr_parser , image_size = image_size , batch_size = batch_size )
1024	else : raise ValueError ( "Encoding must be either 'ohe' or 'le'" ) print ( 'Training Data Shape: ' , features . shape ) print ( 'Testing Data Shape: ' , test_features . shape )
385	ld = os . listdir ( TEST_DIR ) sizes = np . zeros ( len ( ld ) ) for i , f in enumerate ( ld ) : df = pd . read_csv ( os . path . join ( TEST_DIR , f ) ) sizes [ i ] = df . shape [ 0 ] print ( np . mean ( sizes ) ) print ( np . min ( sizes ) ) print ( np . max ( sizes ) ) print ( 'ok' )
148	plt . figure ( figsize = [ 10 , 5 ] ) sns . boxplot ( IP ) plt . title ( 'Number of click by IP' , fontsize = 15 )
531	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
1008	new_corrs = [ ] for col in columns : corr = train [ 'TARGET' ] . corr ( train [ col ] ) new_corrs . append ( ( col , corr ) )
1748	es = es . entity_from_dataframe ( entity_id = 'bureau_balance' , make_index = True , dataframe = bureau_balance_df , index = 'bureau_balance_id' )
1520	x = tf . repeat ( tf . range ( DIM // 2 , - DIM // 2 , - 1 ) , DIM ) y = tf . tile ( tf . range ( - DIM // 2 , DIM // 2 ) , [ DIM ] ) z = tf . ones ( [ DIM * DIM ] , dtype = 'int32' ) idx = tf . stack ( [ x , y , z ] )
1300	def text_process ( text ) : ws = text . split ( ' ' ) if ( len ( ws ) > 160 ) : text = ' ' . join ( ws [ : 160 ] ) + ' ' + ' ' . join ( ws [ - 32 : ] ) return text
1610	y_log = np . log1p ( y ) y_categorized = pd . cut ( y_log , bins = range ( 0 , 25 , 3 ) , include_lowest = True , right = False , labels = range ( 0 , 24 , 3 ) )
1294	model = create_model ( input_shape = ( SIZE , SIZE , 3 ) , n_out = NUM_CLASSES ) for layer in model . layers : layer . trainable = False for i in range ( - 5 , 0 ) : model . layers [ i ] . trainable = True model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( 1e-3 ) )
905	metrics = pd . DataFrame ( { 'fold' : fold_names , 'train' : train_scores , 'valid' : valid_scores } ) return submission , feature_importances , metrics
1505	idx2 = K . dot ( m , tf . cast ( idx , dtype = 'float32' ) ) idx2 = K . cast ( idx2 , dtype = 'int32' ) idx2 = K . clip ( idx2 , - DIM // 2 + XDIM + 1 , DIM // 2 )
359	skin_segm_rgb = cv2 . bitwise_and ( image , image , mask = skin_mask ) return skin_segm_rgb for image in [ img_1 , img_2 , img_3 , img_4 , img_5 ] : image = cv2 . resize ( image , dsize = ( 512 , 512 ) ) skin_segm_rgb = detect_skin ( image ) plt_st ( 12 , 4 ) plt . subplot ( 121 ) plt . title ( "Original image" ) plt . imshow ( image ) plt . subplot ( 122 ) plt . title ( "Skin segmentation" ) plt . imshow ( skin_segm_rgb )
1288	path_to_train = '../input/imet-2019-fgvc6/train/' data = pd . read_csv ( '../input/imet-2019-fgvc6/train.csv' ) train_dataset_info = [ ] for name , labels in zip ( data [ 'id' ] , data [ 'attribute_ids' ] . str . split ( ' ' ) ) : train_dataset_info . append ( { 'path' : os . path . join ( path_to_train , name ) , 'labels' : np . array ( [ int ( label ) for label in labels ] ) } ) train_dataset_info = np . array ( train_dataset_info )
1368	xmin , xmax , ymin , ymax = ax . axis ( ) basemap , extent = ctx . bounds2img ( xmin , ymin , xmax , ymax , zoom = zoom , url = url ) ax . imshow ( basemap , extent = extent , interpolation = 'bilinear' )
399	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yt , yr_pred ) print ( confusion )
546	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
1279	arc . reset ( ) arc . identify_object ( image , method = 3 ) arc . plot_identified_objects ( arc . identified_objects , title = 'by both' )
1758	r_applications_pos_balance = ft . Relationship ( es [ 'applications' ] [ 'SK_ID_CURR' ] , es [ 'pos_balance' ] [ 'SK_ID_CURR' ] ) es = es . add_relationship ( r_applications_pos_balance ) print ( es )
862	best_std = cv_results [ 'auc-stdv' ] [ - 1 ] print ( 'The maximium ROC AUC in cross validation was {:.5f} with std of {:.5f}.' . format ( best , best_std ) ) print ( 'The ideal number of iterations was {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
48	norm = colors . Normalize ( vmax = grouped [ 'Count' ] . max ( ) , vmin = grouped [ 'Count' ] . min ( ) - 1 * grouped [ 'Count' ] . min ( ) ) bar_colors = [ colors . rgb2hex ( cmap ( norm ( c ) ) ) for c in grouped [ 'Count' ] ]
483	from keras . models import Model from keras . layers import Input from keras . layers import Dense visible = Input ( shape = ( 2 , ) ) hidden = Dense ( 2 ) ( visible ) model = Model ( inputs = visible , outputs = hidden )
132	test_gen = ImageDataGenerator ( rescale = 1. / 255 ) test_generator = test_gen . flow_from_dataframe ( test_df , "../input/test1/test1/" , x_col = 'filename' , y_col = None , class_mode = None , target_size = IMAGE_SIZE , batch_size = batch_size , shuffle = False )
763	plt . annotate ( xy = ( row [ x ] - ( 1 / counts [ x ] . nunique ( ) ) , row [ y ] - ( 0.15 / counts [ y ] . nunique ( ) ) ) , color = 'navy' , s = f"{round(row['percent'], 1)}%" )
621	areas = [ ] for c in contours : areas . append ( cv2 . contourArea ( c ) )
7	target = 'meter_reading' categorical = [ 'building_id' , 'site_id' , 'primary_use' , 'meter' , 'is_holiday' , 'dayofweek' ] numeric_cols = [ col for col in train_df . columns if col not in categorical + [ target , 'timestamp' , 'group' ] ] features = categorical + numeric_cols
1468	train_feat_max = train_df [ feat ] . max ( ) test_feat_max = test_df [ feat ] . max ( ) if train_feat_max > test_feat_max : feat_max = train_feat_max else : feat_max = test_feat_max
1488	def __init__ ( self , example_id , candidates ) : self . example_id = example_id self . candidates = candidates self . results = { } self . features = { } class ScoreSummary ( object ) : def __init__ ( self ) : self . predicted_label = None self . short_span_score = None self . cls_token_score = None self . answer_type_logits = None self . start_prob = None self . end_prob = None self . answer_type_prob_dist = None def read_candidates_from_one_split ( input_path ) :
921	df [ 'importance_normalized' ] = df [ 'importance' ] / df [ 'importance' ] . sum ( ) df [ 'cumulative_importance' ] = np . cumsum ( df [ 'importance_normalized' ] ) plt . rcParams [ 'font.size' ] = 12
1062	for i , module in enumerate ( self . down_convs ) : x , before_pool = module ( x ) encoder_outs . append ( before_pool ) for i , module in enumerate ( self . up_convs ) : before_pool = encoder_outs [ - ( i + 2 ) ] x = module ( before_pool , x )
1163	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_id_tfrecord , num_parallel_calls = AUTO )
596	y [ bird ] = 1 return images , y def train ( model , optimizer , epochs , train_accuracy , all_loss , best_bird_count , best_score , t_scores , f1_scores , b_scores ) :
965	df = df . reset_index ( ) plt . figure ( figsize = ( 10 , 6 ) ) plt . style . use ( 'fivethirtyeight' )
1398	class DataProcessing : pass def get_time_block_series ( series_array , date_to_index , start_date , end_date ) : inds = date_to_index [ start_date : end_date ] return series_array [ : , inds ] def transform_series_encode ( series_array ) :
1200	plt . subplot ( 2 , 2 , 2 ) scores = results . best_score_ plt . boxplot ( scores ) rmse_mean = np . mean ( scores ) rmse_std = np . std ( scores ) plt . title ( 'RMSE Mean:{:.3f} Std: {:.4f}' . format ( rmse_mean , rmse_std ) )
60	preds = learn . get_preds ( ds_type = DatasetType . Test ) preds = np . argmax ( preds [ 0 ] . numpy ( ) , axis = 1 ) categories = sorted ( train . genres . unique ( ) . astype ( 'str' ) ) final_preds = [ ] for idx in preds : final_preds . append ( categories [ idx ] ) final_submit = pd . read_csv ( '../input/clabscvcomp/data/sample_submission.csv' ) final_submit . genres = final_preds final_submit . head ( ) final_submit . to_csv ( 'submission.csv' , index = False )
1478	x = tf . repeat ( tf . range ( DIM // 2 , - DIM // 2 , - 1 ) , DIM ) y = tf . tile ( tf . range ( - DIM // 2 , DIM // 2 ) , [ DIM ] ) z = tf . ones ( [ DIM * DIM ] , dtype = 'int32' ) idx = tf . stack ( [ x , y , z ] )
537	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
1348	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
1678	train [ 'timestamp' ] = pd . to_datetime ( train [ 'timestamp' ] ) test [ 'timestamp' ] = pd . to_datetime ( test [ 'timestamp' ] ) return train , test , train_labels , win_code , list_of_user_activities , list_of_event_code , activities_labels , assess_titles , list_of_event_id , all_title_event_code
1238	init_ops = [ tf . global_variables_initializer ( ) , tf . tables_initializer ( ) ] sess = tf . Session ( ) sess . run ( init_ops )
450	pds = { 'num_leaves' : ( 100 , 230 ) , 'feature_fraction' : ( 0.1 , 0.5 ) , 'bagging_fraction' : ( 0.8 , 1 ) , 'lambda_l1' : ( 0 , 3 ) , 'lambda_l2' : ( 0 , 5 ) , 'max_depth' : ( 8 , 19 ) , 'min_split_gain' : ( 0.001 , 0.1 ) , 'min_child_weight' : ( 1 , 20 ) }
1146	labels_breed = pd . read_csv ( '../input/breed_labels.csv' ) labels_state = pd . read_csv ( '../input/color_labels.csv' ) labels_color = pd . read_csv ( '../input/state_labels.csv' )
966	plt . title ( 'Distribution of Feature by Target Value' ) plt . xlabel ( '%s' % feature ) ; plt . ylabel ( 'Density' ) ; plt . show ( )
177	cats = pd . DataFrame ( train . category_name . value_counts ( ) ) cats . reset_index ( level = 0 , inplace = True ) cats = cats . sort_values ( by = 'category_name' , ascending = False ) . head ( 20 ) cats . columns = ( 'category_name' , 'size' )
646	mean = try_history . groupby ( "type" ) [ "score" ] . agg ( [ "mean" ] ) [ "mean" ] plt . plot ( range ( len ( fix_samples ) ) , mean , color = "red" ) plt . legend ( ) plt . grid ( )
376	mae = mean_absolute_error ( y_val , preds ) print ( 'MAE: %.6f' % mae ) maes . append ( mae )
178	group = train . groupby ( train . category_name ) mean_price = group . price . mean ( ) mean_price = pd . DataFrame ( mean_price ) mean_price . reset_index ( level = 0 , inplace = True )
1651	history = [ ] resa2 = clustering ( hits , stds , filters , phik = 3.3 , nu = nu , truth = truth , history = history ) resa2 [ "event_id" ] = event_num score = score_event_fast ( truth , resa2 . rename ( index = str , columns = { "label" : "track_id" } ) ) print ( "Your score: " , score )
1379	M = S @ T @ R if ( border != 0 ) or ( M != np . eye ( 3 ) ) . any ( ) : img = cv2 . warpAffine ( img , M [ : 2 ] , dsize = ( width , height ) , flags = cv2 . INTER_LINEAR , borderValue = ( 114 , 114 , 114 ) )
578	super ( Spectrogram , self ) . __init__ ( ) self . power = power self . stft = STFT ( n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode , freeze_parameters = True ) def forward ( self , input ) :
236	country_name = "Spain" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1310	imp_df = pd . DataFrame ( ) imp_df [ "feature" ] = list ( features ) imp_df [ "importance_gain" ] = clf . feature_importance ( importance_type = 'gain' ) imp_df [ "importance_split" ] = clf . feature_importance ( importance_type = 'split' ) imp_df [ 'trn_score' ] = sqrt ( mean_squared_error ( y , clf . predict ( data [ features ] ) ) ) return imp_df
1390	train_transforms = get_train_transforms ( ) train_dataset = WheatDataset ( processed_train_labels_df , mode = "train" , image_dir = TRAIN_IMG_FOLDER , transforms = train_transforms ) image , target = train_dataset [ 0 ]
344	def my_generator ( ) : for i in range ( 0 , 3 ) : yield print ( i ) my_gen = my_generator ( )
329	for j in range ( 0 , len ( batch1 ) ) : patientId = batch1 [ j ] path = \ '../input/rsna-pneumonia-detection-challenge/stage_1_train_images/%s.dcm' % patientId dcm_data = pydicom . read_file ( path )
1047	cash = pd . read_csv ( '../input/POS_CASH_balance.csv' ) cash = convert_types ( cash , print_info = True ) cash . head ( )
1162	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_id_tfrecord if labeled else read_unlabeled_tfrecord , num_parallel_calls = AUTO ) return dataset def load_dataset_with_id ( filenames , ordered = False ) :
249	svr = SVR ( ) svr . fit ( train , target ) acc_model ( 1 , svr , train , test )
1142	shap_sum = np . abs ( shap_values ) . mean ( axis = 0 ) importance_df = pd . DataFrame ( [ X_importance . columns . tolist ( ) , shap_sum . tolist ( ) ] ) . T importance_df . columns = [ 'column_name' , 'shap_importance' ] importance_df = importance_df . sort_values ( 'shap_importance' , ascending = False ) importance_df
1099	oof_train [ valid_index ] = gbm . predict ( X_val , num_iteration = gbm . best_iteration ) oof_test [ : , i ] = gbm . predict ( X_test , num_iteration = gbm . best_iteration )
438	bold ( '**Shape of our train and test data**' ) print ( 'Dimension of train:' , df_train . shape ) print ( 'Dimension of test:' , df_test . shape )
668	dftrain = pd . read_csv ( '../input/covid19-global-forecasting-week-2/train.csv' , parse_dates = [ 'Date' ] ) . sort_values ( by = [ 'Country_Region' , 'Date' ] ) dftest = pd . read_csv ( '../input/covid19-global-forecasting-week-2/test.csv' , parse_dates = [ 'Date' ] ) . sort_values ( by = [ 'Country_Region' , 'Date' ] ) dftrain . head ( )
1702	for candidate in candidates : score = evaluate_fitness ( candidate , task ) is_uncomparable = True
1323	BlockArgs = collections . namedtuple ( 'BlockArgs' , [ 'kernel_size' , 'num_repeat' , 'input_filters' , 'output_filters' , 'expand_ratio' , 'id_skip' , 'stride' , 'se_ratio' ] )
1515	if not os . path . isdir ( INPUT_DIR ) : IS_KAGGLE = False INPUT_DIR = "./" NQ_DIR = "./" MY_OWN_NQ_DIR = "./" for dirname , _ , filenames in os . walk ( INPUT_DIR ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
515	df_team_conference_strength = pd . DataFrame ( ) df_team_conference_strength [ 'Conf_Strength' ] = df_team_conferences_stage [ 'OrdinalRank' ] . groupby ( [ df_team_conferences_stage [ 'Season' ] , df_team_conferences_stage [ 'ConfAbbrev' ] ] ) . mean ( ) df_team_conference_strength . reset_index ( inplace = True ) df_team_conference_strength . tail ( )
1319	train_target = target_data [ train_index ] cv_target = target_data [ cv_index ] train_input = input_data [ train_index ] cv_input = input_data [ cv_index ] test_input = input_data [ len ( df_train_ ) : , : ]
1551	day_month_avg = train_info . groupby ( 'day' ) . trip_duration . mean ( ) day_month_avg = day_month_avg . reset_index ( ) day_month_avg . columns = [ 'day' , 'day_of_month_avg' ] df = pd . merge ( left = df , right = day_month_avg , on = 'day' , how = 'left' )
405	fit_vec = count_vec * ( np . log ( count_vec ) - np . log ( width ) ) fit_vec -= 4 fit_vec [ 1 : ] += best [ : K ]
1085	if self . orig_image_size != self . image_size : if self . pad_images : img_temp = self . __pad_image ( img_temp ) mask_temp = self . __pad_image ( mask_temp ) else : img_temp = cv2 . resize ( img_temp , self . image_size ) mask_temp = cv2 . resize ( mask_temp , self . image_size )
1382	import nltk stopwords = nltk . corpus . stopwords . words ( 'english' ) stemmer = nltk . stem . PorterStemmer ( ) def clean_sentence ( doc ) : words = doc . split ( ' ' ) words_clean = [ stemmer . stem ( word ) for word in words if word not in stopwords ] return ' ' . join ( words_clean ) docs = docs . apply ( clean_sentence ) docs . head ( )
1058	model = UNet ( ) model_path = 'model_1.pt' state = torch . load ( str ( model_path ) ) state = { key . replace ( 'module.' , '' ) : value for key , value in state [ 'model' ] . items ( ) } model . load_state_dict ( state ) if torch . cuda . is_available ( ) : model . cuda ( ) model . eval ( )
97	beforeM = before - np . transpose ( before ) bef_unc = ( np . abs ( beforeM ) / ( before + np . transpose ( before ) + 1e-30 ) < 0.3 ) * ( beforeM != 0 ) showBefore ( bef_unc )
21	nulls = np . sum ( test . isnull ( ) ) nullcols = nulls . loc [ ( nulls != 0 ) ] dtypes = test . dtypes dtypes2 = dtypes . loc [ ( nulls != 0 ) ] info = pd . concat ( [ nullcols , dtypes2 ] , axis = 1 ) . sort_values ( by = 0 , ascending = False )
845	train_ape [ train_ape == np . inf ] = 0 train_ape [ train_ape == - np . inf ] = 0 valid_ape [ valid_ape == np . inf ] = 0 valid_ape [ valid_ape == - np . inf ] = 0 train_mape = 100 * np . mean ( train_ape ) valid_mape = 100 * np . mean ( valid_ape ) return train_rmse , valid_rmse , train_mape , valid_mape def evaluate ( model , features , X_train , X_valid , y_train , y_valid ) :
1189	print ( 'starting generate data sets...' ) title_list = np . unique ( np . hstack ( [ train_events [ 'title' ] . values , test_events [ 'title' ] . values ] ) ) . tolist ( )
369	Xtrain , Xval , Ztrain , Zval = train_test_split ( trainb , targetb , test_size = 0.2 , random_state = 0 ) train_set = lgb . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgb . Dataset ( Xval , Zval , silent = False )
328	for j in range ( 0 , len ( batch ) ) : patientId = batch [ j ] path = \ '../input/rsna-pneumonia-detection-challenge/stage_1_train_images/%s.dcm' % patientId dcm_data = pydicom . read_file ( path )
1033	parent_ids = df [ parent_var ] . copy ( ) numeric_df = df . select_dtypes ( 'number' ) . copy ( ) numeric_df [ parent_var ] = parent_ids
1064	guid = f'{set_type}' examples = [ ] if guid == 'train' : for line , label in zip ( lines , labels ) : text_a = line label = str ( label ) examples . append ( run_classifier . InputExample ( guid = guid , text_a = text_a , text_b = None , label = label ) ) else : for line in lines : text_a = line label = '0' examples . append ( run_classifier . InputExample ( guid = guid , text_a = text_a , text_b = None , label = label ) ) return examples
1111	test_dfs_sentiment = [ x [ 0 ] for x in dfs_test if isinstance ( x [ 0 ] , pd . DataFrame ) ] test_dfs_metadata = [ x [ 1 ] for x in dfs_test if isinstance ( x [ 1 ] , pd . DataFrame ) ] test_dfs_sentiment = pd . concat ( test_dfs_sentiment , ignore_index = True , sort = False ) test_dfs_metadata = pd . concat ( test_dfs_metadata , ignore_index = True , sort = False ) print ( test_dfs_sentiment . shape , test_dfs_metadata . shape )
697	def precision_at ( threshold , iou ) : matches = iou > threshold true_positives = np . sum ( matches , axis = 1 ) == 1 false_positives = np . sum ( matches , axis = 0 ) == 0 false_negatives = np . sum ( matches , axis = 1 ) == 0 tp , fp , fn = np . sum ( true_positives ) , np . sum ( false_positives ) , np . sum ( false_negatives ) return tp , fp , fn
1042	mis_val_table_ren_columns = mis_val_table_ren_columns [ mis_val_table_ren_columns . iloc [ : , 1 ] != 0 ] . sort_values ( '% of Total Values' , ascending = False ) . round ( 1 ) if print_info :
1578	for col in new_feats : df_train [ col ] . replace ( [ np . inf ] , np . nan , inplace = True ) df_train [ col ] . fillna ( 0 , inplace = True ) df_test [ col ] . replace ( [ np . inf ] , np . nan , inplace = True ) df_test [ col ] . fillna ( 0 , inplace = True )
555	dtypestrain = { } dtypestrain [ 'ID_code' ] = 'category' dtypestrain [ 'target' ] = 'int8' for i in range ( 0 , 200 ) : dtypestrain [ 'var_' + str ( i ) ] = 'float32' dtypestest = { } dtypestest [ 'ID_code' ] = 'category' for i in range ( 0 , 200 ) : dtypestest [ 'var_' + str ( i ) ] = 'float32'
968	from featuretools import selection feature_matrix2 = selection . remove_low_information_features ( feature_matrix ) print ( 'Removed %d features' % ( feature_matrix . shape [ 1 ] - feature_matrix2 . shape [ 1 ] ) )
1443	x_train_full2 = np . square ( x_train_full ) x_test_full2 = np . square ( x_test_full ) x_sub_full2 = np . square ( x_sub_full )
945	hyp_df [ 'iteration' ] = results [ 'iteration' ] hyp_df [ 'score' ] = results [ 'score' ] return hyp_df
1455	test_y = test_y . reshape ( ( len ( test_y ) , 1 ) ) inv_y = np . concatenate ( ( test_y , test_X [ : , 1 : ] ) , axis = 1 ) inv_y = scaler . inverse_transform ( inv_y ) inv_y = inv_y [ : , 0 ]
648	train_df = pd . read_csv ( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip' ) . fillna ( ' ' ) test_df = pd . read_csv ( '/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip' ) . fillna ( ' ' ) train_df . sample ( 10 )
776	grid = sns . PairGrid ( data = plot_data , size = 4 , diag_sharey = False , hue = 'Target' , hue_order = [ 4 , 3 , 2 , 1 ] , vars = [ x for x in list ( plot_data . columns ) if x != 'Target' ] )
389	empty_images = [ ] for image_id , image_type in id_type_list : size = os . path . getsize ( get_filename ( image_id , image_type ) ) if size == 0 : empty_images . append ( ( image_id , image_type ) ) print ( "Number of empty images: " , len ( empty_images ) )
1752	es = es . entity_from_dataframe ( entity_id = 'pos_balance' , make_index = True , dataframe = pos_balance_df , index = 'pos_balance_id' )
1072	attribute_image = image [ y : y + h , x : x + w ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 ) ) plt . grid ( False ) ax . xaxis . set_visible ( False ) ax . yaxis . set_visible ( False ) ax . imshow ( attribute_image ) plt . show ( )
991	es = es . entity_from_dataframe ( entity_id = 'app_train' , dataframe = app_train , index = 'SK_ID_CURR' , variable_types = app_types ) es = es . entity_from_dataframe ( entity_id = 'app_test' , dataframe = app_test , index = 'SK_ID_CURR' , variable_types = app_test_types ) es = es . entity_from_dataframe ( entity_id = 'bureau' , dataframe = bureau , index = 'SK_ID_BUREAU' , time_index = 'bureau_credit_application_date' ) es = es . entity_from_dataframe ( entity_id = 'previous' , dataframe = previous , index = 'SK_ID_PREV' , time_index = 'previous_decision_date' , variable_types = previous_types )
1226	fig = plt . figure ( 200 , figsize = ( 15 , 15 ) ) random_indicies = np . random . choice ( range ( len ( X_images ) ) , 9 , False ) subset = X_images [ random_indicies ] for i in range ( 9 ) : ax = fig . add_subplot ( 3 , 3 , i + 1 ) ax . imshow ( subset [ i ] ) plt . show ( )
134	model = CatBoostClassifier ( iterations = 100 , learning_rate = 0.003 , depth = 10 , l2_leaf_reg = 0.01 , loss_function = 'MultiClass' )
288	from sklearn . metrics import roc_auc_score roc_auc_score ( y_true , y_pred )
1817	lidar_data = [ ] image_data = [ ] for record in data_json : if record [ 'fileformat' ] == 'jpeg' : image_data . append ( record ) else : lidar_data . append ( record )
706	clf = QuadraticDiscriminantAnalysis ( reg_param = 0.5 ) clf . fit ( train3 [ train_index , : ] , train2 . loc [ train_index ] [ 'target' ] ) oof_QDA [ idx1 [ test_index ] ] = clf . predict_proba ( train3 [ test_index , : ] ) [ : , 1 ] preds_QDA [ idx2 ] += clf . predict_proba ( test3 ) [ : , 1 ] / skf . n_splits
855	X_train , X_valid , y_train , y_valid = train_test_split ( data , np . array ( data [ 'fare_amount' ] ) , stratify = data [ 'fare-bin' ] , random_state = RSEED , test_size = 1_000_000 )
1498	decay_var_list = [ ] for i in range ( len ( bert_nq . trainable_variables ) ) : name = bert_nq . trainable_variables [ i ] . name if any ( x in name for x in [ "LayerNorm" , "layer_norm" , "bias" ] ) : decay_var_list . append ( name ) decay_var_list
1067	print ( 'Fold' , fold_n , 'started at' , time . ctime ( ) ) X_tr , X_val = X_train [ train_index ] , X_train [ valid_index ] Y_tr , Y_val = Y_train [ train_index ] , Y_train [ valid_index ]
1027	valid_score = model . best_score_ [ 'valid' ] [ 'auc' ] train_score = model . best_score_ [ 'train' ] [ 'auc' ] valid_scores . append ( valid_score ) train_scores . append ( train_score )
1107	with open ( filename , 'r' ) as f : sentiment_file = json . load ( f ) return sentiment_file def open_image_file ( self , filename ) :
1615	for c in range ( 8 ) : j = 0 for i in train_index : if train_df . coverage_class [ i ] == c : axes [ j , c ] . imshow ( np . array ( train_df . masks [ i ] ) ) axes [ j , c ] . set_axis_off ( ) axes [ j , c ] . set_title ( 'class {}' . format ( c ) ) j += 1 if ( j >= 2 ) : break
436	bold ( '**Preview of Train Data:**' ) display ( df_train . head ( ) ) bold ( '**Preview of Test Data:**' ) display ( df_test . head ( ) )
517	df_seeds [ 'seed_int' ] = df_seeds [ 'Seed' ] . apply ( lambda x : int ( x [ 1 : 3 ] ) ) df_seeds . drop ( labels = [ 'Seed' ] , inplace = True , axis = 1 ) df_seeds . rename ( columns = { 'seed_int' : 'Seed' } , inplace = True )
1653	colors = [ 'darkseagreen' , 'lightcoral' ] plt . figure ( figsize = ( 6 , 6 ) ) plt . pie ( train [ "target" ] . value_counts ( ) , explode = ( 0 , 0.25 ) , labels = [ "0" , "1" ] , startangle = 45 , autopct = '%1.1f%%' , colors = colors ) plt . axis ( 'equal' ) plt . show ( )
1716	def load_glove ( word_index ) : EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt' def get_coefs ( word , * arr ) : return word , np . asarray ( arr , dtype = 'float32' ) [ : 300 ] embeddings_index = dict ( get_coefs ( * o . split ( " " ) ) for o in open ( EMBEDDING_FILE ) ) all_embs = np . stack ( embeddings_index . values ( ) ) emb_mean , emb_std = - 0.005838499 , 0.48782197 embed_size = all_embs . shape [ 1 ]
455	image_with_boxes = draw_boxes ( np . array ( image_out ) , result_out [ "detection_boxes" ] , result_out [ "detection_class_entities" ] , result_out [ "detection_scores" ] ) display_image ( image_with_boxes )
823	def __init__ ( self , fill_value = 0 , bbox_removal_threshold = 0.50 , min_cutout_size = 192 , max_cutout_size = 512 , number = 1 , always_apply = False , p = 0.5 ) :
1050	else : raise ValueError ( "Encoding must be either 'ohe' or 'le'" ) print ( 'Training Data Shape: ' , features . shape ) print ( 'Testing Data Shape: ' , test_features . shape )
147	IP = df [ 'ip' ] . value_counts ( ) plt . figure ( figsize = [ 10 , 5 ] ) sns . boxplot ( IP ) plt . title ( 'Number of click by IP' , fontsize = 15 )
962	feature_matrix_spec , feature_names_spec = ft . dfs ( entityset = es , target_entity = 'app' , agg_primitives = [ 'sum' , 'count' , 'min' , 'max' , 'mean' , 'mode' ] , max_depth = 2 , features_only = False , verbose = True )
624	df_grouped_italy = get_df_country_cases ( df_covid , "Italy" ) df_grouped_italy
1492	if FLAGS . do_valid : validation_features = ( tf . train . Example . FromString ( r . numpy ( ) ) for r in tf . data . TFRecordDataset ( valid_tf_record ) ) valid_predictions_json = get_prediction_json ( mode = 'valid' , max_nb_pos_logits = FLAGS . n_best_size )
1221	idx_best = np . argmax ( self . average_scores ) self . best_score_ = self . average_scores [ idx_best ] self . best_params_ = self . param_list [ idx_best ] def _expand_params ( self ) :
1219	def create_title_mode ( train_labels ) : titles = train_labels . title . unique ( ) title2mode = { } for title in titles : mode = ( train_labels [ train_labels . title == title ] . accuracy_group . value_counts ( ) . index [ 0 ] ) title2mode [ title ] = mode return title2mode def add_title_mode ( labels , title2mode ) : labels [ 'title_mode' ] = labels . title . apply ( lambda title : title2mode [ title ] ) return labels
735	img = Image . open ( image_path ) resnet_preds = image_classify ( resnet_model , resnet50 , img ) xception_preds = image_classify ( xception_model , xception , img ) inception_preds = image_classify ( inception_model , inception_v3 , img ) preds_arr = [ ( 'Resnet50' , resnet_preds ) , ( 'xception' , xception_preds ) , ( 'Inception' , inception_preds ) ] plot_preds ( img , preds_arr )
191	plt . figure ( figsize = ( 20 , 20 ) ) sns . regplot ( x = 'coms_length' , y = 'price' , data = train , scatter_kws = { 's' : 2 } ) plt . title ( 'Description length VS price' , fontsize = 20 ) plt . xlabel ( 'Description length' , fontsize = 20 ) plt . ylabel ( 'Price' , fontsize = 20 )
1475	img1 = image [ j , ] img2 = image [ k , ] imgs . append ( ( 1 - a ) * img1 + a * img2 )
995	plt . subplot ( 1 , 2 , 2 ) plt . bar ( list ( range ( 4 ) ) , interesting_features [ 'MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Refused)' ] . value_counts ( ) ) plt . xticks ( list ( range ( 4 ) ) , interesting_features [ 'MODE(previous.NAME_CLIENT_TYPE WHERE NAME_CONTRACT_STATUS = Refused)' ] . value_counts ( ) . index ) ; plt . xlabel ( "Client Type" ) ; plt . ylabel ( "Counts" ) ; plt . title ( "Most Common Client Type where Contract was Refused" ) ;
1337	block_args = block_args . _replace ( input_filters = round_filters ( block_args . input_filters , self . _global_params ) , output_filters = round_filters ( block_args . output_filters , self . _global_params ) , num_repeat = round_repeats ( block_args . num_repeat , self . _global_params ) )
1506	idx3 = tf . stack ( [ DIM // 2 - idx2 [ 0 , ] , DIM // 2 - 1 + idx2 [ 1 , ] ] ) d = tf . gather_nd ( image , tf . transpose ( idx3 ) ) return tf . reshape ( d , [ DIM , DIM , 3 ] ) , label
95	def indices ( S , k ) : vals = valrow ( k ) idx = [ ] for num in S : idx . extend ( list ( np . where ( vals == num ) [ 0 ] ) ) return idx used = np . zeros ( values . shape [ 0 ] , np . bool ) for i in range ( values . shape [ 0 ] ) :
927	score = cv_results [ 'auc-mean' ] [ - 1 ] estimators = len ( cv_results [ 'auc-mean' ] ) hyperparameters [ 'n_estimators' ] = estimators return [ score , hyperparameters , iteration ]
1236	for display_str in display_str_list [ : : - 1 ] : text_width , text_height = font . getsize ( display_str ) margin = np . ceil ( 0.05 * text_height ) draw . rectangle ( [ ( left , text_bottom - text_height - 2 * margin ) , ( left + text_width , text_bottom ) ] , fill = color ) draw . text ( ( left + margin , text_bottom - text_height - margin ) , display_str , fill = "black" , font = font ) text_bottom -= text_height - 2 * margin
1794	model0 . fit ( x_tr , y_tr ) prediction = model0 . predict ( x_ts ) r2 = r2_score ( y_ts . as_matrix ( ) , model0 . predict ( x_ts ) ) mae = mean_absolute_error ( y_ts . as_matrix ( ) , model0 . predict ( x_ts ) ) print ( "-----------------------------------------------" ) print ( "mae with 70% of the data to train:" , mae ) print ( "-----------------------------------------------" )
478	from sklearn . feature_extraction . text import HashingVectorizer text = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] vectorizer = HashingVectorizer ( n_features = 6 ) vector = vectorizer . transform ( text ) print ( vector . shape ) print ( vector . toarray ( ) )
276	shuffleV = np . int32 ( sorted ( range ( len ( inpV [ 0 ] ) ) , key = lambda k : ( inpV [ 0 ] [ k ] == PAD_ID ) . sum ( ) , reverse = True ) ) inpV = [ arr [ shuffleV ] for arr in inpV ] targetV = [ arr [ shuffleV ] for arr in targetV ]
788	plt . vlines ( importance_index + 1 , ymin = 0 , ymax = 1.05 , linestyles = '--' , colors = 'red' ) plt . show ( ) ; print ( '{} features required for {:.0f}% of cumulative importance.' . format ( importance_index + 1 , 100 * threshold ) ) return df
1725	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
982	for col in [ 'DAYS_CREDIT' , 'DAYS_CREDIT_ENDDATE' , 'DAYS_ENDDATE_FACT' , 'DAYS_CREDIT_UPDATE' ] : bureau [ col ] = pd . to_timedelta ( bureau [ col ] , 'D' ) bureau [ [ 'DAYS_CREDIT' , 'DAYS_CREDIT_ENDDATE' , 'DAYS_ENDDATE_FACT' , 'DAYS_CREDIT_UPDATE' ] ] . head ( )
639	positive_train = train [ train [ "sentiment" ] == "positive" ] negative_train = train [ train [ "sentiment" ] == "negative" ] neutral_train = train [ train [ "sentiment" ] == "neutral" ]
1184	for v in self . variables : K . set_value ( v , np . zeros ( ( self . num_classes , self . num_classes ) , np . int32 ) )
802	predictions [ 'Target' ] = predictions [ [ 1 , 2 , 3 , 4 ] ] . idxmax ( axis = 1 ) predictions [ 'confidence' ] = predictions [ [ 1 , 2 , 3 , 4 ] ] . max ( axis = 1 ) predictions = predictions . drop ( columns = [ 'fold' ] )
160	from matplotlib . colors import ListedColormap rand_cmap = ListedColormap ( np . random . rand ( 256 , 3 ) ) labels_for_display = np . where ( labels > 0 , labels , np . nan ) plt . imshow ( im_gray , cmap = 'gray' ) plt . imshow ( labels_for_display , cmap = rand_cmap ) plt . axis ( 'off' ) plt . title ( 'Labeled Cells ({} Nuclei)' . format ( nlabels ) ) plt . show ( )
377	rmse = mean_squared_error ( y_val , preds ) print ( 'RMSE: %.6f' % rmse ) rmses . append ( rmse ) fold_importance_df [ 'importance_%d' % fold_ ] = model . feature_importances_ [ : len ( scaled_train_X . columns ) ] print ( 'MAEs' , maes ) print ( 'MAE mean: %.6f' % np . mean ( maes ) ) print ( 'RMSEs' , rmses ) print ( 'RMSE mean: %.6f' % np . mean ( rmses ) ) submission . time_to_failure = predictions submission . to_csv ( 'submission_lgb_8_80k_108dp.csv' , index = False ) fold_importance_df . to_csv ( 'fold_imp_lgb_8_80k_108dp.csv' )
1391	for bbox in bboxes : x , y , w , h = bbox transformed_bbox = [ x , y , x + w , y + h ] draw . rectangle ( transformed_bbox , outline = "red" , width = 3 ) return img plot_image_with_bboxes ( "00333207f" , processed_train_labels_df )
166	im_df = pd . DataFrame ( ) for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) if label_mask . flatten ( ) . sum ( ) > 10 : rle = rle_encoding ( label_mask ) s = pd . Series ( { 'ImageId' : im_id , 'EncodedPixels' : rle } ) im_df = im_df . append ( s , ignore_index = True ) return im_df def analyze_list_of_images ( im_path_list ) :
105	train_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv" ) calendar_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/calendar.csv" ) price_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv" ) sample = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv" )
1808	print ( "Creating number of transactions... " ) ttemp = trans [ [ 'msno' ] ] temp = pd . DataFrame ( ttemp [ 'msno' ] . value_counts ( ) . reset_index ( ) ) temp . columns = [ 'msno' , 'trans_count' ]
773	train_heads = heads . loc [ heads [ 'Target' ] . notnull ( ) , : ] . copy ( ) pcorrs = pd . DataFrame ( train_heads . corr ( ) [ 'Target' ] . sort_values ( ) ) . rename ( columns = { 'Target' : 'pcorr' } ) . reset_index ( ) pcorrs = pcorrs . rename ( columns = { 'index' : 'feature' } ) print ( 'Most negatively correlated variables:' ) print ( pcorrs . head ( ) ) print ( '\nMost positively correlated variables:' ) print ( pcorrs . dropna ( ) . tail ( ) )
1321	config = tf . ConfigProto ( device_count = { 'GPU' : 1 , 'CPU' : 2 } ) config . gpu_options . allow_growth = True config . gpu_options . per_process_gpu_memory_fraction = 0.6 sess = tf . Session ( config = config ) K . set_session ( sess )
76	folder = '../input/stage_1_test_images' test_filenames = os . listdir ( folder ) print ( 'n test samples:' , len ( test_filenames ) )
841	plt . figure ( figsize = ( 12 , 6 ) ) for f , grouped in data . groupby ( 'fare-bin' ) : sns . kdeplot ( grouped [ 'euclidean' ] , label = f'{f}' , color = list ( grouped [ 'color' ] ) [ 0 ] ) ; plt . xlabel ( 'degrees' ) ; plt . ylabel ( 'density' ) plt . title ( 'Euclidean Distance by Fare Amount' ) ;
52	norm = colors . Normalize ( vmax = grouped [ 'Count' ] . max ( ) , vmin = grouped [ 'Count' ] . min ( ) - 7 * grouped [ 'Count' ] . min ( ) ) bar_colors = [ colors . rgb2hex ( cmap ( norm ( c ) ) ) for c in grouped [ 'Count' ] ]
1017	mis_val_table_ren_columns = mis_val_table_ren_columns [ mis_val_table_ren_columns . iloc [ : , 1 ] != 0 ] . sort_values ( '% of Total Values' , ascending = False ) . round ( 1 )
1056	from sklearn . model_selection import train_test_split unique_img_ids = masks . groupby ( 'ImageId' ) . size ( ) . reset_index ( name = 'counts' ) train_ids , valid_ids = train_test_split ( unique_img_ids , test_size = 0.05 , stratify = unique_img_ids [ 'counts' ] , random_state = 42 ) train_df = pd . merge ( masks , train_ids ) valid_df = pd . merge ( masks , valid_ids ) print ( train_df . shape [ 0 ] , 'training masks' ) print ( valid_df . shape [ 0 ] , 'validation masks' )
597	masks = pd . read_csv ( os . path . join ( '../input/' , 'train_ship_segmentations_v2.csv' ) ) print ( masks . shape [ 0 ] , 'masks found' ) print ( masks [ 'ImageId' ] . value_counts ( ) . shape [ 0 ] ) masks . head ( )
263	w_lgb = 0.4 w_xgb = 0.5 w_logreg = 1 - w_lgb - w_xgb w_logreg
1627	stats = [ ] for country in [ 'US' , 'United Kingdom' , 'Russia' , 'Singapore' , 'New Zealand' ] : df = get_time_series ( country ) print ( '{} COVID-19 Prediction' . format ( country ) ) opt_display_model ( df , stats )
612	test_path_audio = os . path . join ( test_path , 'audio' ) test_filenames = os . listdir ( test_path_audio ) test_filenames = np . sort ( test_filenames ) list ( test_filenames ) [ : 10 ]
306	train_len = len ( df_train ) df_combined = pd . concat ( objs = [ df_train , df_test ] , axis = 0 ) . reset_index ( drop = True ) print ( df_combined . shape )
1389	from sklearn . model_selection import StratifiedKFold skf = StratifiedKFold ( n_splits = 5 ) for fold , ( train_index , valid_index ) in enumerate ( skf . split ( processed_train_labels_df , y = processed_train_labels_df [ "source" ] ) ) : processed_train_labels_df . loc [ valid_index , "fold" ] = fold
1688	first_shape = tuple ( x [ 0 ] . shape ) for pixmap in x [ 1 : ] : if first_shape != tuple ( pixmap . shape ) : return [ ] return [ ( np . prod ( np . array ( x ) , axis = 0 ) > 0 ) . astype ( int ) ] def sortByColor ( xs ) :
529	gcv = GridSearchCV ( estimator = est , param_grid = pgrid , scoring = 'neg_log_loss' , cv = outer_cv , verbose = 0 , refit = True , return_train_score = False ) gcv . fit ( X_train , y_train ) gridcvs [ name ] = gcv print ( name ) print ( ) print ( gcv . best_estimator_ ) print ( ) print ( 'Best score on Grid Search Cross Validation is %.5f%%' % ( gcv . best_score_ ) ) print ( ) results = pd . DataFrame ( gcv . cv_results_ )
1001	return df . iloc [ 0 ] [ 'y' ] MostRecent = make_agg_primitive ( function = most_recent , input_types = [ Discrete , Datetime ] , return_type = Discrete )
1108	image = np . asarray ( Image . open ( filename ) ) return image def parse_sentiment_file ( self , file ) :
1641	temp = train [ 'ip' ] . value_counts ( ) . reset_index ( name = 'counts' ) temp . columns = [ 'ip' , 'counts' ] temp [ : 10 ]
704	clf = QuadraticDiscriminantAnalysis ( reg_param = 0.5 ) clf . fit ( train3p [ train_index , : ] , train2p . loc [ train_index ] [ 'target' ] ) oof [ idx1 [ test_index3 ] ] = clf . predict_proba ( train3 [ test_index3 , : ] ) [ : , 1 ] preds [ test2 . index ] += clf . predict_proba ( test3 ) [ : , 1 ] / skf . n_splits
1125	output = F . sigmoid ( self . final ( dec1 ) ) return output def get_model ( params ) : model = UNet11 ( ** params ) model . train ( ) return model . to ( device )
1712	y = data . pop ( 'species' ) y = LabelEncoder ( ) . fit ( y ) . transform ( y ) print ( y . shape )
280	X_clustered = kmeans . fit_predict ( x_3d ) LABEL_COLOR_MAP = { 0 : 'r' , 1 : 'g' , 2 : 'b' , 3 : 'y' , 4 : 'c' } label_color = [ LABEL_COLOR_MAP [ l ] for l in X_clustered ] plt . figure ( figsize = ( 7 , 7 ) ) plt . scatter ( x_3d [ : , 0 ] , x_3d [ : , 1 ] , c = label_color , alpha = 0.9 ) plt . show ( )
1783	firstcloud = WordCloud ( stopwords = STOPWORDS , background_color = 'black' , width = 2500 , height = 1800 ) . generate ( " " . join ( first_topic_words ) ) plt . imshow ( firstcloud ) plt . axis ( 'off' ) plt . show ( )
39	batch = tr . next_batch ( BATCH_SIZE ) x = batch [ 0 ] y = batch [ 1 ]
1363	def change ( addr ) : if addr == 60.0 : return 1 elif addr == 96.0 : return 1 elif addr == np . nan : return np . nan else : return 0 df [ "Europe" ] = df [ "addr2" ] . map ( change )
74	def cosine_annealing ( x ) : lr = 0.01 epochs = 20 return lr * ( np . cos ( np . pi * x / epochs ) + 1. ) / 2 learning_rate = tf . keras . callbacks . LearningRateScheduler ( cosine_annealing )
1423	def sentence_to_wordlist ( raw ) : clean = re . sub ( "[^a-zA-Z0-9]" , " " , raw ) words = clean . split ( ) return words
203	for i , axial_slice in enumerate ( binary_image ) : axial_slice = axial_slice - 1 labeling = measure . label ( axial_slice ) l_max = largest_label_volume ( labeling , bg = 0 ) if l_max is not None : binary_image [ i ] [ labeling != l_max ] = 1 binary_image -= 1 binary_image = 1 - binary_image
1063	BERT_MODEL = 'uncased_L-12_H-768_A-12' BERT_PRETRAINED_DIR = f'{repo}/uncased_L-12_H-768_A-12' OUTPUT_DIR = f'{repo}/outputs' print ( f'***** Model output directory: {OUTPUT_DIR} *****' ) print ( f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****' )
464	train_df = train_transaction . merge ( train_identity , how = 'left' , left_index = True , right_index = True ) test_df = test_transaction . merge ( test_identity , how = 'left' , left_index = True , right_index = True ) print ( "Train shape : " + str ( train_df . shape ) ) print ( "Test shape : " + str ( test_df . shape ) )
1002	custom_features , custom_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'last' , MostRecent ] , max_depth = 1 , trans_primitives = [ ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
524	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
1561	sig_macro_columns = [ 'oil_urals' , 'gdp_quart_growth' , 'cpi' , 'usdrub' , \ 'salary_growth' , 'unemployment' , 'mortgage_rate' , \ 'deposits_rate' , 'rent_price_3room_bus' ]
199	final_image = cv2 . inpaint ( image_resize , threshold , 1 , cv2 . INPAINT_TELEA ) plt . subplot ( l , 5 , ( i * 5 ) + 5 ) plt . imshow ( cv2 . cvtColor ( final_image , cv2 . COLOR_BGR2RGB ) ) plt . axis ( 'off' ) plt . title ( 'final_image : ' + image_name ) plt . plot ( )
1086	print ( 'Training set ready.' ) print ( 'X_train shape: {}' . format ( self . X_train . shape ) ) print ( 'y_train shape: {}' . format ( self . y_train . shape ) ) print ( 'X_train - min: {}, max: {}' . format ( np . min ( self . X_train ) , np . max ( self . X_train ) ) ) print ( 'y_train - min: {}, max: {}' . format ( np . min ( self . y_train ) , np . max ( self . y_train ) ) )
1776	import numpy as np import pandas as pd import matplotlib . pyplot as plt import os import datetime import warnings warnings . filterwarnings ( "ignore" ) import seaborn as sns import scipy
503	base_range = np . amax ( img ) - np . amin ( img ) rescaled_range = 255 - 0 img_rescaled = ( ( ( img - np . amin ( img ) ) * rescaled_range ) / base_range ) return np . uint8 ( img_rescaled )
1435	filename = "/kaggle/input/osic-pulmonary-fibrosis-progression/train/ID00123637202217151272140/137.dcm" ds = pydicom . dcmread ( filename ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone )
99	SEED = int ( time . time ( ) ) epoch = 3 batch_size = 32 dim = ( 125 , 125 ) SIZE = 125 stats = ( 0.0692 , 0.2051 ) HEIGHT = 137 WIDTH = 236 def seed_all ( SEED ) : random . seed ( SEED ) np . random . seed ( SEED )
669	confirmed = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv' ) . sort_values ( by = 'Country/Region' ) deaths = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv' ) recovered = pd . read_csv ( 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv' )
267	v = train [ "PlayerCollegeName" ] . value_counts ( ) missing_values = list ( v [ v < 10 ] . index ) train [ "PlayerCollegeName" ] = train [ "PlayerCollegeName" ] . where ( ~ train [ "PlayerCollegeName" ] . isin ( missing_values ) , "nan" )
894	feature_importances = feature_importances / 2 feature_importances = pd . DataFrame ( { 'feature' : list ( train . columns ) , 'importance' : feature_importances } ) . sort_values ( 'importance' , ascending = False ) feature_importances . head ( )
237	country_name = "Spain" march_day = 15 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1596	train_df . head ( ) percent = ( 100 * train_df . isnull ( ) . sum ( ) / train_df . shape [ 0 ] ) . sort_values ( ascending = False ) percent [ : 10 ]
1230	with strategy . scope ( ) : transformer_layer = TFAutoModel . from_pretrained ( MODEL ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
141	sns . set ( ) plt . hist ( sub1 [ 'isFraud' ] , bins = 100 ) plt . show ( )
825	self . image = image self . cutout_pos = cutout_pos self . cutout_size = cutout_size image [ cutout_pos . y : cutout_pos . y + cutout_size , cutout_pos . x : cutout_size + cutout_pos . x , : ] = cutout_arr return image def apply_to_bbox ( self , bbox , ** params ) :
846	train_preds = [ train_mean for _ in range ( len ( y_train ) ) ] valid_preds = [ train_mean for _ in range ( len ( y_valid ) ) ] tr , vr , tm , vm = metrics ( train_preds , valid_preds , y_train , y_valid ) print ( f'Baseline Training: rmse = {round(tr, 2)} \t mape = {round(tm, 2)}' ) print ( f'Baseline Validation: rmse = {round(vr, 2)} \t mape = {round(vm, 2)}' )
351	model = RandomForestRegressor ( max_depth = 2 , random_state = 0 , n_estimators = 100 ) embeded_rf_selector = SelectFromModel ( model , threshold = '1.25*median' ) embeded_rf_selector . fit ( dfe , target_fe )
977	train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test_ids = list ( test [ 'SK_ID_CURR' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] )
1274	unique_colors = np . unique ( true_image ) for i , color in enumerate ( unique_colors ) : image = np . copy ( true_image ) if color == background : continue
1069	submission = pd . read_csv ( "../input/sample_submission_stage_1.csv" , index_col = "ID" ) submission [ "A" ] = prediction [ : , 0 ] submission [ "B" ] = prediction [ : , 1 ] submission [ "NEITHER" ] = prediction [ : , 2 ] submission . to_csv ( "submission_bert.csv" )
366	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
1657	pos_train = df_train . query ( 'sentiment=="positive"' ) neg_train = df_train . query ( 'sentiment=="negative"' ) neu_train = df_train . query ( 'sentiment=="neutral"' ) pos_val = df_train . query ( 'sentiment=="positive"' ) neg_val = df_train . query ( 'sentiment=="negative"' ) neu_val = df_train . query ( 'sentiment=="neutral"' ) pos_test = df_test . query ( 'sentiment=="positive"' ) neg_test = df_test . query ( 'sentiment=="negative"' ) neu_test = df_test . query ( 'sentiment=="neutral"' )
1193	predictions_train . loc [ val_index , 'predictedRevenue' ] = gbm . predict ( X_val , num_iteration = model . best_iteration_ ) . copy ( ) predictions_train [ predictions_train [ 'predictedRevenue' ] < 0 ] [ 'predictedRevenue' ] = 0 print ( predictions_train . groupby ( 'fullVisitorId' ) . count ( ) . info ( ) )
50	grouped = pd . DataFrame ( dataset [ var_name_official ] . groupby ( [ dataset [ var_name_official ] . dt . hour ] ) . count ( ) ) . rename ( columns = { var_name_official : 'Count' } )
1170	train_col = train_clean_rob . columns test_col = test_clean_rob . columns cols = [ c for c in train_clean_rob if c not in [ 'isFraud' , 'TransactionID' , 'TransactionDT' ] ] y = np . array ( train_clean_rob [ 'isFraud' ] ) X = np . array ( train_clean_rob [ cols ] )
1412	from collections import Counter cls_counts = Counter ( cls for classes in train [ 'attribute_ids' ] . str . split ( ) for cls in classes ) print ( len ( cls_counts ) )
470	train_df = train_transaction . merge ( train_identity , how = 'left' , left_index = True , right_index = True ) test_df = test_transaction . merge ( test_identity , how = 'left' , left_index = True , right_index = True ) print ( "Train shape : " + str ( train_df . shape ) ) print ( "Test shape : " + str ( test_df . shape ) )
836	BB_zoom = ( - 74.1 , - 73.7 , 40.6 , 40.85 ) nyc_map_zoom = plt . imread ( 'https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/images/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true' )
145	df = pd . read_csv ( '../input/train.csv' , skiprows = 9308568 , nrows = 59633310 ) header = pd . read_csv ( '../input/train.csv' , nrows = 0 ) df . columns = header . columns df del header gc . collect ( ) print ( "The created dataframe contains" , df . shape [ 0 ] , "rows." )
190	plt . figure ( figsize = ( 10 , 10 ) ) sns . kdeplot ( train [ 'coms_length' ] , shade = True ) plt . title ( 'Distribution of the description length' , fontsize = 20 ) plt . xlabel ( 'Description length' , fontsize = 12 )
402	CALENDAR_DTYPES = { 'date' : 'str' , 'wm_yr_wk' : 'int16' , 'weekday' : 'object' , 'wday' : 'int16' , 'month' : 'int16' , 'year' : 'int16' , 'd' : 'object' , 'event_name_1' : 'object' , 'event_type_1' : 'object' , 'event_name_2' : 'object' , 'event_type_2' : 'object' , 'snap_CA' : 'int16' , 'snap_TX' : 'int16' , 'snap_WI' : 'int16' } PARSE_DATES = [ 'date' ] SPRICES_DTYPES = { 'store_id' : 'object' , 'item_id' : 'object' , 'wm_yr_wk' : 'int16' , 'sell_price' : 'float32' }
333	for j in range ( 0 , len ( batch ) ) : patientId = batch [ j ] path = \ '../input/rsna-pneumonia-detection-challenge/stage_1_test_images/%s.dcm' % patientId dcm_data = pydicom . read_file ( path )
519	X_train = df_predictions . SeedDiff . values . reshape ( - 1 , 1 ) y_train = df_predictions . Result . values X_train , y_train = shuffle ( X_train , y_train )
580	self . logmel_extractor = LogmelFilterBank ( sr = sample_rate , n_fft = window_size , n_mels = mel_bins , fmin = fmin , fmax = fmax , ref = ref , amin = amin , top_db = top_db , freeze_parameters = True )
1355	start_mem = df . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( 'Memory usage of dataframe is {:.2f} MB' . format ( start_mem ) ) for col in df . columns : if is_datetime ( df [ col ] ) or is_categorical_dtype ( df [ col ] ) :
1181	conf_mtx = self . conf_mtx / tf . reduce_sum ( self . conf_mtx ) out_prod = out_prod / tf . reduce_sum ( out_prod ) conf_mtx = tf . cast ( conf_mtx , dtype = tf . float32 ) out_prod = tf . cast ( out_prod , dtype = tf . float32 )
1775	start_time = time . time ( ) model . train ( ) avg_loss = 0. for i , ( x_batch , y_batch , index ) in enumerate ( train_loader ) :
736	train_all_zero_features = cr . index [ cr ] train . drop ( columns = train_all_zero_features , inplace = True ) count_of_binary_features = ( train . max ( ) == 1 ) . sum ( ) print ( 'Number of binary features: {}' . format ( count_of_binary_features ) )
1361	import pandas as pd import numpy as np import matplotlib . pylab as plt pd . set_option ( 'display.max_rows' , 500 ) pd . get_option ( "display.max_columns" , 500 )
1456	rooms = train [ [ "num_room" , "price_doc" ] ] . groupby ( "num_room" ) . aggregate ( np . mean ) . reset_index ( ) mplt . scatter ( x = rooms . num_room , y = rooms . price_doc ) mplt . xlabel ( "Num rooms" ) mplt . ylabel ( 'Mean Price' )
1394	with_masks_df = ( train_df . groupby ( 'img_id' ) [ 'EncodedPixels' ] . count ( ) . reset_index ( ) . rename ( columns = { "EncodedPixels" : "n_masks" } ) ) with_masks_df = with_masks_df . loc [ lambda df : df [ "n_masks" ] > 0 , : ]
265	scaler = StandardScaler ( ) train_log = pd . DataFrame ( scaler . fit_transform ( X ) , columns = X . columns , index = X . index ) test_log = pd . DataFrame ( scaler . transform ( test_df ) , columns = test_df . columns , index = test_df . index )
715	q_high = df . y . quantile ( 0.75 ) q_low = df . y . quantile ( 0.25 ) iqr = ( q_high - q_low ) * 1.5 high = q_high + iqr low = q_low - iqr df = df . drop ( df [ df . y > high ] . index ) df = df . drop ( df [ df . y < low ] . index )
20	nulls = np . sum ( train . isnull ( ) ) nullcols = nulls . loc [ ( nulls != 0 ) ] dtypes = train . dtypes dtypes2 = dtypes . loc [ ( nulls != 0 ) ] info = pd . concat ( [ nullcols , dtypes2 ] , axis = 1 ) . sort_values ( by = 0 , ascending = False )
426	train [ 'square_feet' ] = np . log1p ( train [ 'square_feet' ] ) test [ 'square_feet' ] = np . log1p ( test [ 'square_feet' ] ) bold ( '**Distribution after log tranformation**' ) distplot ( train [ 'square_feet' ] , 'darkgreen' )
1500	print ( f"{ckpt_manager._directory}" ) os . system ( f"ls -l {ckpt_manager._directory} > results.txt" ) with open ( "results.txt" , "r" , encoding = "UTF-8" ) as fp : for line in fp : print ( line . strip ( ) )
187	train [ 'no_descrip' ] = 0 train . loc [ train . item_description == 'No description yet' , 'no_descrip' ] = 1 i = str ( round ( train [ 'no_descrip' ] . value_counts ( normalize = True ) . iloc [ 1 ] * 100 , 2 ) ) + '%' print ( i , 'of the items have no a description.' )
380	length = struct . unpack ( "<i" , item_length_bytes ) [ 0 ] f . seek ( offset ) item_data = f . read ( length ) assert len ( item_data ) == length , "%i vs %i" % ( len ( item_data ) , length )
1459	total = merchant . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( merchant . isnull ( ) . sum ( ) / merchant . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_data . head ( 20 )
1662	for i in range ( 27 , 28 ) : pylab . imshow ( sample_image [ i ] , cmap = pylab . cm . bone ) pylab . show ( )
1574	DATA_PATH = '../input/aptos2019-blindness-detection' TRAIN_IMG_PATH = os . path . join ( DATA_PATH , 'train_images' ) TEST_IMG_PATH = os . path . join ( DATA_PATH , 'test_images' ) TRAIN_LABEL_PATH = os . path . join ( DATA_PATH , 'train.csv' ) TEST_LABEL_PATH = os . path . join ( DATA_PATH , 'test.csv' ) train_df = pd . read_csv ( TRAIN_LABEL_PATH ) test_df = pd . read_csv ( TEST_LABEL_PATH ) train_df . head ( )
1458	total = test . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( test . isnull ( ) . sum ( ) / test . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_data . head ( 20 )
876	hyp_df [ 'iteration' ] = new_results [ 'iteration' ] hyp_df [ 'score' ] = new_results [ 'score' ] return hyp_df
211	data2 = data for col in categorical_columns : if col in data . columns : le = LabelEncoder ( ) le . fit ( list ( data [ col ] . astype ( str ) . values ) ) data2 [ col ] = le . transform ( list ( data [ col ] . astype ( str ) . values ) ) train = data2 . iloc [ : train . shape [ 0 ] , : ] test = data2 . iloc [ train . shape [ 0 ] : , : ]
446	directions = { 'N' : 0 , 'NE' : 1 / 4 , 'E' : 1 / 2 , 'SE' : 3 / 4 , 'S' : 1 , 'SW' : 5 / 4 , 'W' : 3 / 2 , 'NW' : 7 / 4 }
865	for parameter_name in [ 'num_leaves' , 'subsample_for_bin' , 'min_child_samples' ] : hyperparameters [ parameter_name ] = int ( hyperparameters [ parameter_name ] ) start = timer ( )
1022	if x not in cols_seen : cols_to_remove . append ( x ) cols_to_remove_pair . append ( key ) cols_to_remove = list ( set ( cols_to_remove ) ) print ( 'Number of columns to remove: ' , len ( cols_to_remove ) )
294	submission = pd . DataFrame ( { 'id' : image_id , 'label' : y_pred , } ) . set_index ( 'id' ) submission . to_csv ( 'patch_preds.csv' , columns = [ 'label' ] )
225	fig = px . scatter_3d ( commits_df , x = 'hidden_dim_first' , y = 'hidden_dim_second' , z = 'LB_score' , color = 'best' , symbol = 'dropout_model' , title = 'hidden_dim_1st & 2nd and LB score visualization of COVID-19 mRNA VDP solutions' ) fig . update ( layout = dict ( title = dict ( x = 0.07 ) ) )
1681	features [ 'accumulated_accuracy' ] = accumulated_accuracy / counter if counter > 0 else 0 accuracy = true_attempts / ( true_attempts + false_attempts ) if ( true_attempts + false_attempts ) != 0 else 0 accumulated_accuracy += accuracy last_accuracy_title [ 'acc_' + session_title_text ] = accuracy
1483	PRETRAINED_MODELS = { "BERT" : [ 'bert-base-uncased' , 'bert-large-uncased' , 'bert-base-cased' , 'bert-large-cased' , 'bert-base-multilingual-uncased' , 'bert-base-multilingual-cased' , 'bert-base-chinese' , 'bert-base-german-cased' , 'bert-large-uncased-whole-word-masking' , 'bert-large-cased-whole-word-masking' , 'bert-large-uncased-whole-word-masking-finetuned-squad' , 'bert-large-cased-whole-word-masking-finetuned-squad' , 'bert-base-cased-finetuned-mrpc' ] , "DISTILBERT" : [ 'distilbert-base-uncased' , 'distilbert-base-uncased-distilled-squad' ] }
1040	train = train . merge ( previous_counts , on = 'SK_ID_CURR' , how = 'left' ) train = train . merge ( previous_agg , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( previous_counts , on = 'SK_ID_CURR' , how = 'left' ) test = test . merge ( previous_agg , on = 'SK_ID_CURR' , how = 'left' )
1129	train_path = data_src + 'train' test_path = data_src train_ids = train_df . index . values test_ids = test_df . index . values
292	num_test_images = 57458 model . load_weights ( 'model.h5' ) predictions = model . predict_generator ( test_gen , steps = num_test_images , verbose = 1 )
1244	data_dir = '/kaggle/input/stanford-covid-vaccine/' train = pd . read_json ( data_dir + 'train.json' , lines = True ) test = pd . read_json ( data_dir + 'test.json' , lines = True ) sample_df = pd . read_csv ( data_dir + 'sample_submission.csv' )
220	corr_matrix = features . corr ( ) . abs ( ) upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ) , k = 1 ) . astype ( np . bool ) ) ; threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style collinear_features = [ column for column in upper . columns if any ( upper [ column ] > threshold ) ] upper . style . applymap ( highlight )
26	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . target . values , bins = 200 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
1421	train_sentences = data_train [ 'comment_text' ] . values . tolist ( ) test_sentences = data_test [ 'comment_text' ] . values . tolist ( ) total_ = copy . deepcopy ( train_sentences ) total_ . extend ( test_sentences ) print ( '[*]Training Sentences:' , len ( train_sentences ) ) print ( '[*]Test Sentences:' , len ( test_sentences ) ) print ( '[*]Total Sentences:' , len ( total_ ) ) for i in tqdm ( range ( len ( total_ ) ) ) : total_ [ i ] = str ( total_ [ i ] ) . lower ( )
90	'2-16/9' : { 'aspect_ratio' : 16 / 9 , 'shots' : [ ( 0 , 0 , 9 / 16 , 1 , 1 ) , ( 7 / 16 , 0 , 9 / 16 , 1 , 1 ) ] } ,
989	installments [ 'installments_due_date' ] = start_date + installments [ 'DAYS_INSTALMENT' ] installments = installments . drop ( columns = [ 'DAYS_INSTALMENT' ] ) installments [ 'installments_paid_date' ] = start_date + installments [ 'DAYS_ENTRY_PAYMENT' ] installments = installments . drop ( columns = [ 'DAYS_ENTRY_PAYMENT' ] )
336	train_batch_size = 10 val_batch_size = 10 test_batch_size = 1
482	model = Sequential ( ) model . add ( Dense ( 5 , input_dim = 2 ) ) model . add ( Activation ( 'relu' ) ) model . add ( Dense ( 1 ) ) model . add ( Activation ( 'sigmoid' ) )
920	credit = pd . read_csv ( '../input/credit_card_balance.csv' ) . replace ( { 365243 : np . nan } ) credit = convert_types ( credit ) credit [ 'OVER_LIMIT' ] = credit [ 'AMT_BALANCE' ] > credit [ 'AMT_CREDIT_LIMIT_ACTUAL' ] credit [ 'BALANCE_CLEARED' ] = credit [ 'AMT_BALANCE' ] == 0.0 credit [ 'LOW_PAYMENT' ] = credit [ 'AMT_PAYMENT_CURRENT' ] < credit [ 'AMT_INST_MIN_REGULARITY' ] credit [ 'LATE' ] = credit [ 'SK_DPD' ] > 0.0
492	corrmat = application_train . corr ( ) top_corr_features = corrmat . index [ abs ( corrmat [ "TARGET" ] ) >= 0.03 ] plt . figure ( figsize = ( 20 , 10 ) ) g = sns . heatmap ( application_train [ top_corr_features ] . corr ( ) , annot = True , cmap = "Oranges" )
1057	return self . img_transform ( img ) , torch . from_numpy ( np . moveaxis ( mask , - 1 , 0 ) ) . float ( ) else : return self . img_transform ( img ) , str ( img_file_name )
161	if np . product ( cell . shape ) < 10 : print ( 'Label {} is too small! Setting to 0.' . format ( label_ind ) ) mask = np . where ( labels == label_ind + 1 , 0 , mask )
1773	loss_fn = torch . nn . BCEWithLogitsLoss ( reduction = 'sum' ) step_size = 400 base_lr , max_lr = 0.001 , 0.004 optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ( ) ) , lr = max_lr )
1150	A . Resize ( image_size [ 0 ] + image_crop_pad , image_size [ 1 ] + image_crop_pad ) , A . RandomScale ( 0.25 ) , A . RandomCrop ( image_size [ 0 ] , image_size [ 1 ] ) ] ) valid_aug = A . Compose ( [
477	from sklearn . feature_extraction . text import TfidfVectorizer text = [ "It was the best of times" , "it was the worst of times" , "it was the age of wisdom" , "it was the age of foolishness" ] vectorizer = TfidfVectorizer ( ) vectorizer . fit ( text ) print ( sorted ( vectorizer . vocabulary_ ) ) vector = vectorizer . transform ( [ text [ 0 ] ] )
1437	files = folders = 0 path = "/kaggle/input/osic-pulmonary-fibrosis-progression/test" for _ , dirnames , filenames in os . walk ( path ) : files += len ( filenames ) folders += len ( dirnames ) print ( "{:,} files/images, {:,} folders/patients" . format ( files , folders ) )
258	ridge . fit ( train0 , train_target0 ) Submission2 [ 'Predicted' ] = ridge . predict ( testn ) Submission2 . to_csv ( 'submission2.csv' , index = False ) Submission2 . head ( 3 )
1770	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) fasttext_embeddings = load_fasttext ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings , fasttext_embeddings ] , axis = 0 )
930	random_results = pd . DataFrame ( columns = [ 'score' , 'params' , 'iteration' ] , index = list ( range ( MAX_EVALS ) ) ) grid_results = pd . DataFrame ( columns = [ 'score' , 'params' , 'iteration' ] , index = list ( range ( MAX_EVALS ) ) )
1203	original_fake_paths = [ ] for dirname , _ , filenames in tqdm ( os . walk ( '/kaggle/input/1-million-fake-faces/' ) ) : for filename in filenames : original_fake_paths . append ( [ os . path . join ( dirname , filename ) , filename ] )
1448	train_size = int ( len ( scaled ) * 0.7 ) test_size = len ( scaled ) - train_size V_train , V_test = scaled [ 0 : train_size , : ] , scaled [ train_size : len ( scaled ) , : ] print ( len ( V_train ) , len ( V_test ) )
173	unique_calsses_value = np . unique ( df_train [ [ 'Id' ] ] . values ) print ( unique_calsses_value ) unique_classes_id_dict = { } unique_id_classes_dict = { } for i in range ( len ( unique_calsses_value ) ) : unique_classes_id_dict [ unique_calsses_value [ i ] ] = i unique_id_classes_dict [ i ] = unique_calsses_value [ i ]
24	train [ 'outliers' ] = 0 train . loc [ train [ 'target' ] < - 30 , 'outliers' ] = 1 train [ 'outliers' ] . value_counts ( )
1738	import numpy as np import pandas as pd from sklearn import model_selection , preprocessing , metrics import matplotlib . pyplot as plt import seaborn as sns pd . set_option ( 'display.width' , 1000 ) pd . set_option ( 'display.max_rows' , 200 ) pd . set_option ( 'display.max_columns' , 200 ) df_train = pd . read_csv ( '../input/train.csv' ) df_test = pd . read_csv ( '../input/test.csv' )
1444	x_train_full2 = np . square ( x_train_full ) x_test_full2 = np . square ( x_test_full ) x_sub_full2 = np . square ( x_sub_full )
206	rotation = math . pi * rotation / 180. shear = math . pi * shear / 180. def get_3x3_mat ( lst ) : return tf . reshape ( tf . concat ( [ lst ] , axis = 0 ) , [ 3 , 3 ] )
1159	x = tf . repeat ( tf . range ( DIM // 2 , - DIM // 2 , - 1 ) , DIM ) y = tf . tile ( tf . range ( - DIM // 2 , DIM // 2 ) , [ DIM ] ) z = tf . ones ( [ DIM * DIM ] , dtype = 'int32' ) idx = tf . stack ( [ x , y , z ] )
543	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
993	interesting_features , interesting_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , max_depth = 1 , where_primitives = [ 'mean' , 'mode' ] , trans_primitives = [ ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
189	train [ 'coms_length' ] = train [ 'item_description' ] . str . len ( ) pd . options . display . float_format = '{:.2f}' . format train [ 'coms_length' ] . describe ( )
223	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'best' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmin ( ) , 'best' ] = 1
754	sns . kdeplot ( train . loc [ train [ 'Target' ] == poverty_level , col ] . dropna ( ) , ax = ax , color = color , label = poverty_mapping [ poverty_level ] ) plt . title ( f'{col.capitalize()} Distribution' ) ; plt . xlabel ( f'{col}' ) ; plt . ylabel ( 'Density' ) plt . subplots_adjust ( top = 2 )
502	def convert_to_grayscale ( img ) : base_range = np . amax ( img ) - np . amin ( img ) rescaled_range = 255 - 0 img_rescaled = ( ( ( img - np . amin ( img ) ) * rescaled_range ) / base_range ) return np . uint8 ( img_rescaled )
1479	idx2 = tf . linalg . matmul ( m , tf . cast ( idx , dtype = 'float32' ) ) idx2 = K . cast ( idx2 , dtype = 'int32' ) idx2 = K . clip ( idx2 , - DIM // 2 + XDIM + 1 , DIM // 2 )
813	preds [ 'Target' ] . sort_index ( ) . plot . hist ( normed = True , edgecolor = 'k' , linewidth = 2 , ax = axes [ 1 ] ) axes [ 1 ] . set_xticks ( [ 1 , 2 , 3 , 4 ] ) ; axes [ 1 ] . set_xticklabels ( poverty_mapping . values ( ) , rotation = 60 ) plt . subplots_adjust ( ) plt . title ( 'Predicted Label Distribution' ) ;
1360	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak_df [ 'meter_reading' ] = leak_df . meter_reading_scraped leak_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak_df . fillna ( 0 , inplace = True ) leak_df . loc [ leak_df . meter_reading < 0 , 'meter_reading' ] = 0 leak_df = leak_df [ leak_df . timestamp . dt . year > 2016 ] print ( len ( leak_df ) )
1187	if is_test_set : return user_samples [ - 1 ] return user_samples def gen_assess_avg_acc ( train_labels , title2num ) :
582	batch_probs = sum ( m ( small_batch ) . detach ( ) . cpu ( ) for m in models ) / len ( models ) batch_probs = batch_probs . numpy ( ) probs . append ( batch_probs ) else : batch = torch . stack ( batch ) . to ( DEVICE ) . float ( )
98	def compareSets ( A , B , i , j ) : I = A & B B = B - I A = A - I befA = 0 befB = 0
126	for feat in tqdm ( features ) : lbl_enc = preprocessing . LabelEncoder ( ) all_df [ feat ] = lbl_enc . fit_transform ( all_df [ feat ] . \ fillna ( '-1' ) . \ astype ( str ) . values ) all_df [ 'target' ] = all_df [ 'target' ] . fillna ( - 1 ) all_df [ continuous ] = all_df [ continuous ] . fillna ( - 2 )
1560	corr_df = pd . DataFrame ( columns = [ 'feature' , 'pearson' , 'kendall' , 'spearman' ] ) corr = macro_df [ macro_columns ] . corr ( method = 'spearman' ) fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( corr , annot = True , linewidths = .5 , ax = ax )
938	headers = [ 'score' , 'hyperparameters' , 'iteration' ] writer . writerow ( headers ) of_connection . close ( )
82	submission_df = pd . DataFrame ( columns = [ 'id' ] + label_names ) submission_df [ 'id' ] = test_comments [ 'id' ] . values submission_df [ label_names ] = y_test submission_df . to_csv ( "./cnn_submission.csv" , index = False )
779	ind [ 'escolari/age' ] = ind [ 'escolari' ] / ind [ 'age' ] plt . figure ( figsize = ( 10 , 8 ) ) sns . violinplot ( 'Target' , 'escolari/age' , data = ind ) ;
112	out_df = pd . merge ( out_df , cat_day_lag , left_on = "cat_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , cat_day_year_lag , left_on = "cat_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , month_cat_lag , left_on = "cat_id" , right_index = True , how = "left" )
998	return counts [ mode ] / np . sum ( list ( counts . values ( ) ) ) NormalizedModeCount = make_agg_primitive ( function = normalized_mode_count , input_types = [ Discrete ] , return_type = Numeric )
488	def type_features ( data ) : categorical_features = data . select_dtypes ( include = [ "object" ] ) . columns numerical_features = data . select_dtypes ( exclude = [ "object" ] ) . columns print ( "categorical_features :" , categorical_features ) print ( '-----' * 40 ) print ( "numerical_features:" , numerical_features )
1074	application [ 'income_bins' ] = pd . cut ( application [ 'AMT_INCOME_TOTAL' ] , range ( 0 , 1000000 , 10000 ) ) ed = application . groupby ( [ 'TARGET' , 'income_bins' ] ) [ 'TARGET' ] . count ( ) . unstack ( 'TARGET' ) . fillna ( 0 ) ed . plot ( kind = 'bar' , stacked = True , figsize = ( 50 , 15 ) ) print ( ed )
985	example_credit = bureau_balance [ bureau_balance [ 'SK_ID_BUREAU' ] == 5001709 ] plt . plot ( example_credit [ 'bureau_balance_date' ] , example_credit [ 'STATUS' ] , 'ro' ) ; plt . title ( 'Loan 5001709 over Time' ) ; plt . xlabel ( 'Date' ) ; plt . ylabel ( 'Status' ) ;
262	fig = px . scatter_3d ( commits_df , x = 'FVC_weight' , y = 'Dropout_model' , z = 'LB_score' , color = 'max' , symbol = 'seed' , title = 'Parameters and LB score visualization of OSIC PFP solutions' ) fig . update ( layout = dict ( title = dict ( x = 0.1 ) ) )
1731	def create_video ( image_list , out_file ) : height , width = image_list [ 0 ] . shape fourcc = cv2 . VideoWriter_fourcc ( * 'X264' ) fps = 30.0 video = cv2 . VideoWriter ( out_file , fourcc , fps , ( width , height ) , False ) for im in image_list : video . write ( im . astype ( np . uint8 ) ) cv2 . destroyAllWindows ( ) video . release ( )
18	new_transactions [ 'category_2' ] = new_transactions [ 'category_2' ] . fillna ( 1.0 , inplace = True ) new_transactions [ 'category_3' ] = new_transactions [ 'category_3' ] . fillna ( 'A' , inplace = True ) new_transactions [ 'merchant_id' ] = new_transactions [ 'merchant_id' ] . fillna ( 'M_ID_00a6ca8a8a' , inplace = True )
909	parent_ids = df [ parent_var ] . copy ( ) numeric_df = df . select_dtypes ( 'number' ) . copy ( ) numeric_df [ parent_var ] = parent_ids
1295	train_filenames = os . listdir ( TRAIN_DIR ) train_labels = [ ] for filename in train_filenames : label = filename . split ( '.' ) [ 0 ] train_labels . append ( label ) train_df = pd . DataFrame ( { 'id' : train_filenames , 'label' : train_labels } )
92	from six . moves import cPickle as pickle import bz2 def loadPickleBZ ( pickle_file ) : with bz2 . BZ2File ( pickle_file , 'r' ) as f : loadedData = pickle . load ( f ) return loadedData def savePickleBZ ( pickle_file , data ) : with bz2 . BZ2File ( pickle_file , 'w' ) as f : pickle . dump ( data , f , pickle . HIGHEST_PROTOCOL ) return
556	def lgbm_create_feat_dict ( model ) : feat_dict = dict ( enumerate ( model [ 'feature_names' ] ) ) feat_dict [ - 1 ] = 'base' return feat_dict def analyze_model ( model ) :
1114	X_test = X_test . drop ( [ 'AdoptionSpeed' ] , axis = 1 ) print ( 'X_train shape: {}' . format ( X_train . shape ) ) print ( 'X_test shape: {}' . format ( X_test . shape ) ) assert X_train . shape [ 0 ] == train . shape [ 0 ] assert X_test . shape [ 0 ] == test . shape [ 0 ]
1332	k = self . _block_args . kernel_size s = self . _block_args . stride self . _depthwise_conv = Conv2d ( in_channels = oup , out_channels = oup , groups = oup , kernel_size = k , stride = s , bias = False ) self . _bn1 = nn . BatchNorm2d ( num_features = oup , momentum = self . _bn_mom , eps = self . _bn_eps )
1343	fig , axs = plt . subplots ( 5 , 1 , figsize = ( 10 , 15 ) ) axs . flatten ( ) sample = train [ train . SN_filter == 1 ] . sample ( 1 ) . iloc [ 0 ] for i , err_col in enumerate ( err_cols ) : axs [ i ] . plot ( sample [ err_col ] , color = 'red' , drawstyle = 'steps-mid' ) axs [ i ] . set_title ( err_col )
844	lr . fit ( X_train [ [ 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train ) print ( 'Intercept' , round ( lr . intercept_ , 4 ) ) print ( 'abs_lat_diff coef: ' , round ( lr . coef_ [ 0 ] , 4 ) , '\tabs_lon_diff coef:' , round ( lr . coef_ [ 1 ] , 4 ) , '\tpassenger_count coef:' , round ( lr . coef_ [ 2 ] , 4 ) )
233	le = preprocessing . LabelEncoder ( ) all_data [ 'Day_num' ] = le . fit_transform ( all_data . Date ) all_data [ 'Day' ] = all_data [ 'Date' ] . dt . day all_data [ 'Month' ] = all_data [ 'Date' ] . dt . month all_data [ 'Year' ] = all_data [ 'Date' ] . dt . year
1007	avg_repaid = df . ix [ df [ 'TARGET' ] == 0 , var_name ] . median ( ) avg_not_repaid = df . ix [ df [ 'TARGET' ] == 1 , var_name ] . median ( ) plt . figure ( figsize = ( 12 , 6 ) )
1171	img_list = [ ] for fi in range ( N ) : img_list . append ( mpimg . imread ( name + '/shot' + str ( fi ) + '.png' ) ) shutil . rmtree ( name ) imageio . mimsave ( name + ".gif" , img_list , duration = 0.5 )
1229	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
584	probs = np . concatenate ( probs ) result_df = pd . DataFrame ( { 'row_id' : names , 'birds' : [ probs [ i ] for i in range ( probs . shape [ 0 ] ) ] } ) result_df = result_df . groupby ( 'row_id' ) [ 'birds' ] . apply ( lambda x : np . stack ( x ) . max ( 0 ) ) . reset_index ( ) result_df
321	etr = ExtraTreesRegressor ( ) etr . fit ( train , target ) acc_model ( 12 , etr , train , test )
1132	y_pred = model ( image ) . cpu ( ) . detach ( ) . numpy ( ) val_predictions . append ( y_pred ) val_masks . append ( mask )
180	i = 0 fig , axs = plt . subplots ( 5 , 2 , figsize = ( 20 , 20 ) ) plt . suptitle ( 'Zoom on the second level of categories' , fontsize = 40 ) for cat in alldf : temp = alldf [ cat ] sns . barplot ( 'cat2' , 'index' , data = temp , ax = axs . flatten ( ) [ i ] ) axs . flatten ( ) [ i ] . set_ylabel ( '' ) axs . flatten ( ) [ i ] . set_xlabel ( 'Frequency' ) i += 1
1096	y_pred_test_rle = salt_parser . predictions_rle_encode ( y_pred_test , confidence_threshold_best = 0.5 ) submission = salt_parser . generate_submission ( y_pred_test_rle )
1138	X_interaction = X_importance . iloc [ : 500 , : ] shap_interaction_values = shap . TreeExplainer ( lgb_model ) . shap_interaction_values ( X_interaction )
1576	total = df_train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = 100 * ( df_train . isnull ( ) . sum ( ) / df_train . isnull ( ) . count ( ) ) . sort_values ( ascending = False ) missing_df = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_df . head ( 20 )
1365	url = 'https://data.sfgov.org/api/geospatial/wkhw-cjsf?method=export&format=Shapefile' with urllib . request . urlopen ( url ) as response , open ( 'pd_data.zip' , 'wb' ) as out_file : shutil . copyfileobj ( response , out_file )
1626	stats = [ ] for Province in [ 'Hong Kong' , 'Hubei' ] : df = get_time_series_province ( Province ) print ( '{} COVID-19 Prediction' . format ( Province ) ) opt_display_model ( df , stats )
660	def __init__ ( self ) : self . reset ( ) def reset ( self ) : self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def update ( self , val , n = 1 ) : self . val = val self . sum += val * n self . count += n self . avg = self . sum / self . count
1231	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) tokenizer . save_pretrained ( '.' ) fast_tokenizer = BertWordPieceTokenizer ( 'vocab.txt' , lowercase = False ) fast_tokenizer
281	print ( len ( os . listdir ( '../input/train' ) ) ) print ( len ( os . listdir ( '../input/test' ) ) )
1429	train_features = [ ] print ( '[!]Making training features...' ) for i in tqdm ( range ( len ( data_training ) ) ) : sentence = clean_ [ i ] temp = [ ] for token in sentence : temp . append ( word2vec_ [ token ] . tolist ( ) ) train_features . append ( temp )
1087	self . train_df [ 'coverage_class' ] = self . train_df . coverage . map ( cov_to_class ) return self . train_df def predictions_rle_encode ( self , y_pred_test , confidence_threshold_best ) :
260	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'max' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmax ( ) , 'max' ] = 1
85	import random real = [ ] fake = [ ] for m , n in zip ( X , y ) : if n == 0 : real . append ( m ) else : fake . append ( m ) fake = random . sample ( fake , len ( real ) ) X , y = [ ] , [ ] for x in real : X . append ( x ) y . append ( 0 ) for x in fake : X . append ( x ) y . append ( 1 )
1222	keys , values = zip ( * self . param_grid . items ( ) ) self . param_list = [ dict ( zip ( keys , v ) ) for v in itertools . product ( * values ) ]
949	sns . kdeplot ( param_grid [ 'learning_rate' ] , label = 'Sampling Distribution' , linewidth = 4 ) sns . kdeplot ( random_hyp [ 'learning_rate' ] , label = 'Random Search' , linewidth = 4 ) plt . vlines ( [ best_random_hyp [ 'learning_rate' ] ] , ymin = 0.0 , ymax = 50.0 , linestyles = '--' , linewidth = 4 , colors = [ 'orange' ] ) plt . legend ( ) plt . xlabel ( 'Learning Rate' ) ; plt . ylabel ( 'Density' ) ; plt . title ( 'Learning Rate Distribution' ) ;
1723	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) fasttext_embeddings = load_fasttext ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings , fasttext_embeddings ] , axis = 0 )
1722	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) fasttext_embeddings = load_fasttext ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings , fasttext_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings , fasttext_embeddings gc . collect ( ) np . shape ( embedding_matrix )
936	model = lgb . LGBMClassifier ( ** random_search_params , random_state = 42 ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] print ( 'The best model from random search scores {:.5f} ROC AUC on the test set.' . format ( roc_auc_score ( test_labels , preds ) ) )
947	random_hyp [ 'search' ] = 'random' grid_hyp [ 'search' ] = 'grid' hyp = random_hyp . append ( grid_hyp ) hyp . head ( )
1415	import matplotlib . pyplot as plt df_label_len = train . attribute_ids . str . split ( " " ) . apply ( len ) plt . figure ( figsize = ( 25 , 4 ) ) df_label_len . value_counts ( ) . plot . bar ( ) plt . title ( f"Number of labels for each instance" )
1745	def sieve_eratosthenes ( n ) : primes = [ False , False ] + [ True for i in range ( n - 1 ) ] p = 2 while ( p * p <= n ) : if ( primes [ p ] == True ) : for i in range ( p * 2 , n + 1 , p ) : primes [ i ] = False p += 1 return primes
1299	def lang_embed ( lang , tran ) : lang_codes = { 'en' : '000' , 'es' : '100' , 'fr' : '010' , 'it' : '001' , 'pt' : '110' , 'ru' : '101' , 'tr' : '011' } tran_codes = { 'orig' : '0' , 'tran' : '1' } vec = lang_codes [ lang ] + tran_codes [ tran ] vec = [ int ( v ) for v in vec ] return vec
790	model_results = cv_model ( train_set , train_labels , RandomForestClassifier ( 100 , random_state = 10 ) , 'RF' , model_results )
124	pct_df = df . groupby ( group_by_cols ) [ value_cols ] . pct_change ( ) . fillna ( 0 ) pct_df . columns = [ col + "_pct_change" for col in value_cols ] value_cols += [ col + "_pct_change" for col in value_cols ] df = pd . concat ( [ df , pct_df ] , axis = 1 )
925	model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] baseline_auc = roc_auc_score ( test_labels , preds ) print ( 'The baseline model scores {:.5f} ROC AUC on the test set.' . format ( baseline_auc ) )
1695	task_file = str ( training_path / training_tasks [ 13 ] ) with open ( task_file , 'r' ) as f : task = json . load ( f ) plot_task ( task )
1235	draw = ImageDraw . Draw ( image ) im_width , im_height = image . size ( left , right , top , bottom ) = ( xmin * im_width , xmax * im_width , ymin * im_height , ymax * im_height ) draw . line ( [ ( left , top ) , ( left , bottom ) , ( right , bottom ) , ( right , top ) , ( left , top ) ] , width = thickness , fill = color )
51	grouped [ 'left' ] = grouped . index - 0.5 grouped [ 'right' ] = grouped . index + 0.5 grouped [ 'bottom' ] = [ 0 ] * grouped . shape [ 0 ]
1029	metrics = pd . DataFrame ( { 'fold' : fold_names , 'train' : train_scores , 'valid' : valid_scores } ) return submission , feature_importances , metrics
1211	train_resized_imgs = [ ] test_resized_imgs = [ ] for image_path in label_df [ 'Image' ] : train_resized_imgs . append ( pad_and_resize ( image_path , 'train' ) ) for image_path in submission_df [ 'Image' ] : test_resized_imgs . append ( pad_and_resize ( image_path , 'test' ) )
1694	p = plt . figure ( ) . subplots ( 1 , len ( images ) ) if len ( images ) > 1 : for i , image in enumerate ( images ) : p [ i ] . imshow ( image , cmap = cmap , norm = norm ) elif len ( images ) == 1 : p . imshow ( images [ 0 ] , cmap = cmap , norm = norm )
1092	def rle_encode ( im ) : pixels = im . flatten ( order = 'F' ) pixels = np . concatenate ( [ [ 0 ] , pixels , [ 0 ] ] ) runs = np . where ( pixels [ 1 : ] != pixels [ : - 1 ] ) [ 0 ] + 1 runs [ 1 : : 2 ] -= runs [ : : 2 ] return ' ' . join ( str ( x ) for x in runs )
1422	total_ = copy . deepcopy ( train_sentences ) total_ . extend ( test_sentences ) print ( '[*]Training Sentences:' , len ( train_sentences ) ) print ( '[*]Test Sentences:' , len ( test_sentences ) ) print ( '[*]Total Sentences:' , len ( total_ ) )
273	def lag_with_pct_change ( df , windows ) : for window in windows : df [ 'signal_shift_pos_' + str ( window ) ] = df . groupby ( 'group' ) [ 'signal' ] . shift ( window ) . fillna ( 0 ) df [ 'signal_shift_neg_' + str ( window ) ] = df . groupby ( 'group' ) [ 'signal' ] . shift ( - 1 * window ) . fillna ( 0 ) return df
149	print ( DL_by_IP . describe ( ) , '\n Quantile 99% :' , DL_by_IP . quantile ( 0.99 ) , \ '\n Quantile 99,9% :' , DL_by_IP . quantile ( 0.999 ) , \ '\n Quantile 99,999% :' , DL_by_IP . quantile ( 0.9999 ) )
694	input_x = model2 . layers [ 0 ] . input output_layer = Activation ( 'sigmoid' , name = 'output_activaton' ) ( model2 . layers [ - 1 ] . output ) model1 = Model ( input_x , output_layer ) c = optimizers . adam ( lr = 0.01 ) model1 . compile ( loss = "binary_crossentropy" , optimizer = c , metrics = [ my_iou_metric ] )
197	blackhat = cv2 . morphologyEx ( grayScale , cv2 . MORPH_BLACKHAT , kernel ) plt . subplot ( l , 5 , ( i * 5 ) + 3 ) plt . imshow ( blackhat ) plt . axis ( 'off' ) plt . title ( 'blackhat : ' + image_name )
695	input_x = model1 . layers [ 0 ] . input output_layer = model1 . layers [ - 1 ] . input model = Model ( input_x , output_layer ) c = optimizers . adam ( lr = 0.01 )
8	test_df = test_df . merge ( building_df , left_on = 'building_id' , right_on = 'building_id' , how = 'left' ) del building_df gc . collect ( )
1541	print ( train . iloc [ [ 2 ] ] ) df_train = extract_series ( X_train , 2 , 5 ) df_actual = extract_series ( y_train , 2 , 5 ) lang = X_train . iloc [ 2 , 1 ] score = arima_model ( df_train . copy ( ) , df_actual . copy ( ) , 2 , 1 , 2 , review = True ) print ( "The SMAPE score is : %.5f" % score )
154	not_missing = df [ df [ 'attributed_time' ] . isna ( ) == False ] not_missing [ 'gap' ] = pd . to_datetime ( not_missing [ 'attributed_time' ] ) . sub ( pd . to_datetime ( not_missing [ 'click_time' ] ) ) for i in range ( 0 , 11 ) : y = i / 10 print ( "{0:.0f}" . format ( y * 100 ) , "quantile :" , not_missing [ 'gap' ] . quantile ( y ) )
367	sgd = SGDRegressor ( ) sgd . fit ( train , target ) acc_model ( 4 , sgd , train , test )
47	grouped [ 'left' ] = grouped [ 'date' ] grouped [ 'right' ] = [ d + datetime . timedelta ( calendar . monthrange ( 2016 , d . month ) [ 1 ] ) for d in grouped [ 'date' ] ] grouped [ 'bottom' ] = [ 0 ] * grouped . shape [ 0 ]
1011	def count_categorical ( df , group_var , df_name ) : categorical = pd . get_dummies ( df . select_dtypes ( 'object' ) ) categorical [ group_var ] = df [ group_var ] categorical = categorical . groupby ( group_var ) . agg ( [ 'sum' , 'mean' ] ) column_names = [ ] for var in categorical . columns . levels [ 0 ] : for stat in [ 'count' , 'count_norm' ] : column_names . append ( '%s_%s_%s' % ( df_name , var , stat ) ) categorical . columns = column_names return categorical
419	bold ( '**Shape of our train and test data**' ) print ( 'Dimension of building:' , building . shape ) print ( 'Dimension of Weather train:' , weather_train . shape ) print ( 'Dimension of Weather test:' , weather_test . shape ) print ( 'Dimension of train:' , train . shape ) print ( 'Dimension of test:' , test . shape )
1156	SKIP_VALIDATION = True if SKIP_VALIDATION : TRAINING_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES
996	seed_features , seed_feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'percent_true' , 'mean' ] , trans_primitives = [ ] , seed_features = [ late_payment ] , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
1205	real_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' eval_partition = pd . read_csv ( '/kaggle/input/celeba-dataset/list_eval_partition.csv' ) eval_partition [ 'filename' ] = eval_partition . image_id . apply ( lambda st : real_dir + st ) eval_partition [ 'class' ] = 'REAL'
848	random_forest = RandomForestRegressor ( n_estimators = 20 , max_depth = 20 , max_features = None , oob_score = True , bootstrap = True , verbose = 1 , n_jobs = - 1 )
1476	if len ( label . shape ) == 1 : lab1 = tf . one_hot ( label [ j ] , CLASSES ) lab2 = tf . one_hot ( label [ k ] , CLASSES ) else : lab1 = label [ j , ] lab2 = label [ k , ] labs . append ( ( 1 - a ) * lab1 + a * lab2 )
940	results . sort_values ( 'score' , ascending = False , inplace = True ) results . reset_index ( inplace = True ) return results
498	h = read_header ( infile ) nx = int ( h [ 'num_x_pts' ] ) ny = int ( h [ 'num_y_pts' ] ) nt = int ( h [ 'num_t_pts' ] ) extension = os . path . splitext ( infile ) [ 1 ] with open ( infile , 'rb' ) as fid :
1255	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB7 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( train_labels . shape [ 1 ] , activation = 'softmax' ) ] ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) model . summary ( )
713	preds_LS_label_slice = preds_LS_label [ list ( idx2 [ : n ] ) + list ( idx2 [ - n : ] ) ] skf = StratifiedKFold ( n_splits = 25 , random_state = 42 , shuffle = True ) for train_index , test_index in skf . split ( train3p , train2p [ 'target' ] ) : test_index3 = test_index [ test_index < len ( train3 ) ] test_index4 = test_index [ test_index >= len ( train3 ) ]
1	resized_img = resize ( img , ( IMG_PX_SIZE , IMG_PX_SIZE ) , anti_aliasing = True , preserve_range = True ) ArrayDicom [ : , : , lstFilesDCM . index ( filenameDCM ) ] = resized_img del ds , resized_img , img except Exception as e : continue
1215	keep_cols = [ 'event_id' , 'game_session' , 'installation_id' , 'event_count' , 'event_code' , 'title' , 'game_time' , 'type' , 'world' ] train = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/train.csv' , usecols = keep_cols ) test = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/test.csv' , usecols = keep_cols ) train_labels = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/train_labels.csv' ) submission = pd . read_csv ( '/kaggle/input/data-science-bowl-2019/sample_submission.csv' )
544	predictions_2014 = df_sample_sub1 [ : 2278 ] predictions_2015 = df_sample_sub1 [ 2278 : 4556 ] predictions_2016 = df_sample_sub1 [ 4556 : 6833 ] predictions_2017 = df_sample_sub1 [ 6833 : ] Length2014 = len ( df_predictions_tourney [ df_predictions_tourney [ 'Season' ] < 2015 ] ) Length2015 = len ( df_predictions_tourney [ df_predictions_tourney [ 'Season' ] < 2016 ] ) Length2016 = len ( df_predictions_tourney [ df_predictions_tourney [ 'Season' ] < 2017 ] ) Length2017 = len ( df_predictions_tourney [ df_predictions_tourney [ 'Season' ] < 2018 ] )
796	model = lgb . LGBMClassifier ( ** params , objective = 'multiclass' , n_jobs = - 1 , n_estimators = 10000 , random_state = 10 )
1604	train_null = train_null . loc [ : , train_null . isnull ( ) . any ( ) ] test_null = test_null . loc [ : , test_null . isnull ( ) . any ( ) ] print ( train_null . columns ) print ( test_null . columns )
1449	def create_dataset ( dataset , look_back = 1 ) : dataX , dataY = [ ] , [ ] for i in range ( len ( dataset ) - look_back ) : a = dataset [ i : ( i + look_back ) , 0 ] dataX . append ( a ) dataY . append ( dataset [ i + look_back , 0 ] ) print ( len ( dataY ) ) return np . array ( dataX ) , np . array ( dataY )
444	plt . figure ( figsize = [ 10 , 10 ] ) map4 = df_train [ df_train [ 'City' ] == 'Chicago' ] . groupby ( [ 'Latitude' , 'Longitude' ] ) [ 'RowId' ] . count ( ) . reset_index ( ) sns . kdeplot ( map4 [ 'Longitude' ] , map4 [ 'Latitude' ] ) mplleaflet . display ( )
607	return ( self . _per_class_cumulative_count / float ( np . sum ( self . _per_class_cumulative_count ) ) ) def overall_lwlrap ( self ) :
1352	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak0_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak0_df [ 'meter_reading' ] = leak0_df . meter_reading_scraped leak0_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak0_df . fillna ( 0 , inplace = True ) leak0_df . loc [ leak0_df . meter_reading < 0 , 'meter_reading' ] = 0 leak0_df = leak0_df [ leak0_df . timestamp . dt . year > 2016 ] print ( len ( leak0_df ) )
561	pd . set_option ( 'display.max_rows' , 500 ) na_columns = train_transaction_data . isna ( ) . sum ( ) print ( na_columns [ na_columns == 0 ] ) print ( na_columns [ na_columns > 0 ] / train_transaction_data . shape [ 0 ] )
1629	stats = [ ] for country in sorted ( full_table [ 'Country/Region' ] . unique ( ) ) : df = get_time_series ( country )
1668	sns . catplot ( x = "store_id" , y = "total_sales" , hue = "cat_id" , data = train_sales , kind = "bar" , height = 8 , aspect = 1 ) ;
424	bold ( '**READINGS REALLY PEAKED FROM MAY TO OCTOBER**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) temp_df = train . groupby ( [ 'timestamp' , 'month' ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . lineplot ( data = temp_df , x = 'timestamp' , y = 'meter_reading' , color = 'teal' ) plt . xlabel ( 'Timestamp' , fontsize = 15 ) plt . ylabel ( 'Meter Reading' ) plt . show ( )
699	train2 = train [ train [ 'wheezy-copper-turtle-magic' ] == i ] test2 = test [ test [ 'wheezy-copper-turtle-magic' ] == i ] idx1 = train2 . index ; idx2 = test2 . index train2 . reset_index ( drop = True , inplace = True )
608	absolute_path = os . path . join ( root , fname ) filenames . append ( os . path . relpath ( absolute_path , basedir ) ) return classes , filenames class SpeechDirectoryIterator ( Iterator ) :
1287	def display_blurry_samples ( df , img_id_list , columns = 4 , rows = 3 ) : fig = plt . figure ( figsize = ( 5 * columns , 4 * rows ) ) for i in range ( columns * rows ) : img = preprocess_image ( f'../input/train_images/{img_id_list[i]}.png' ) fig . add_subplot ( rows , columns , i + 1 ) plt . title ( f'index:{i} isclear:{isClear(img)}' ) plt . imshow ( img ) plt . tight_layout ( ) display_blurry_samples ( train_df , blur_list_id )
1386	target [ 'img_size' ] = torch . tensor ( [ ( IMG_SIZE , IMG_SIZE ) ] ) target [ 'img_scale' ] = torch . tensor ( [ 1. ] ) else :
898	zero_features = list ( feature_importances [ feature_importances [ 'importance' ] == 0.0 ] [ 'feature' ] ) print ( '\nThere are %d features with 0.0 importance' % len ( zero_features ) ) return zero_features , feature_importances
170	x = Conv2D ( 16 , ( 3 , 3 ) , padding = "same" ) ( inputs ) x = BatchNormalization ( ) ( x ) x = Activation ( 'relu' ) ( x )
1078	num_features = 300 min_word_count = 40 num_workers = 4 context = 10 downsampling = 1e-3
1471	ignore_order = tf . data . Options ( ) if not ordered : ignore_order . experimental_deterministic = False dataset = tf . data . TFRecordDataset ( filenames , num_parallel_reads = AUTO ) dataset = dataset . with_options ( ignore_order ) dataset = dataset . map ( read_labeled_tfrecord if labeled else read_unlabeled_tfrecord , num_parallel_calls = AUTO ) return dataset def count_data_items ( filenames ) :
67	n_valid_samples = 2560 train_filenames = filenames [ n_valid_samples : ] valid_filenames = filenames [ : n_valid_samples ] print ( 'n train samples' , len ( train_filenames ) ) print ( 'n valid samples' , len ( valid_filenames ) ) n_train_samples = len ( filenames ) - n_valid_samples
1241	image_path = f'../input/test/{image_id}.jpg' with tf . gfile . Open ( image_path , "rb" ) as binfile : image_string = binfile . read ( )
664	date_agg_3 = train_agg . groupby ( level = [ 1 , 2 ] ) . sum ( ) date_agg_3 . columns = ( 'bookings' , 'total' ) date_agg_3 . plot ( kind = 'bar' , stacked = 'True' , figsize = ( 16 , 10 ) )
1452	train = train . sort_values ( 'visit_date' ) target_train = np . log1p ( train [ 'visitors' ] . values ) col = [ c for c in train if c not in [ 'id' , 'air_store_id' , 'visitors' ] ] train = train [ col ] train . set_index ( 'visit_date' , inplace = True ) train . head ( )
224	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'best' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmin ( ) , 'best' ] = 1
12	train_p_0 = train [ [ 'x_0' , 'y_0' , 'z_0' ] ] . values train_p_1 = train [ [ 'x_1' , 'y_1' , 'z_1' ] ] . values test_p_0 = test [ [ 'x_0' , 'y_0' , 'z_0' ] ] . values test_p_1 = test [ [ 'x_1' , 'y_1' , 'z_1' ] ] . values train [ 'dist' ] = np . linalg . norm ( train_p_0 - train_p_1 , axis = 1 ) test [ 'dist' ] = np . linalg . norm ( test_p_0 - test_p_1 , axis = 1 )
929	a = 0 b = 0 for x in param_grid [ 'learning_rate' ] : if x >= 0.005 and x < 0.05 : a += 1 elif x >= 0.05 and x < 0.5 : b += 1 print ( 'There are {} values between 0.005 and 0.05' . format ( a ) ) print ( 'There are {} values between 0.05 and 0.5' . format ( b ) )
768	heads [ 'walls' ] = np . argmax ( np . array ( heads [ [ 'epared1' , 'epared2' , 'epared3' ] ] ) , axis = 1 ) plot_categoricals ( 'walls' , 'Target' , heads )
1136	X_test_pub = X_test . loc [ X_test . DateAvSigVersion <= '2018-10-25' , : ] X_test_priv = X_test . loc [ X_test . DateAvSigVersion > '2018-10-25' , : ] print ( 'public test shape: {}' . format ( X_test_pub . shape ) ) print ( 'private test shape: {}' . format ( X_test_priv . shape ) ) print ( 'fraction of private test split: {:.3f}' . format ( X_test_priv . shape [ 0 ] / X_test . shape [ 0 ] ) )
185	price_of_zero = train . loc [ train . price == 0 ] plt . figure ( figsize = ( 17 , 10 ) ) sns . countplot ( y = price_of_zero . category_name , \ order = price_of_zero . category_name . value_counts ( ) . iloc [ : 10 ] . index , \ orient = 'v' ) plt . title ( 'Top 10 categories of items with a price of 0' , fontsize = 25 ) plt . ylabel ( 'Category name' , fontsize = 20 ) plt . xlabel ( 'Number of product in the category' , fontsize = 20 )
1609	d_train = xgb . DMatrix ( X_train , y_train ) d_valid = xgb . DMatrix ( X_valid , y_valid ) d_test = xgb . DMatrix ( test . values ) watchlist = [ ( d_train , 'train' ) , ( d_valid , 'valid' ) ]
1268	lwk = metrics . cohen_kappa_score ( y_true , y_pred , weights = 'linear' ) qwk = metrics . cohen_kappa_score ( y_true , y_pred , weights = 'quadratic' ) \ print ( "Linear Weighted Kappa Score:" , lwk ) print ( "Quadratic Weighted Kappa Score:" , qwk )
1540	print ( train . iloc [ [ 2 ] ] ) df_train = extract_series ( X_train , 2 , 5 ) df_actual = extract_series ( y_train , 2 , 5 ) lang = X_train . iloc [ 2 , 1 ] score = median_model ( df_train . copy ( ) , df_actual . copy ( ) , 15 , review = True ) print ( "The SMAPE score is : %.5f" % score )
1756	r_applications_prev_apps = ft . Relationship ( es [ 'applications' ] [ 'SK_ID_CURR' ] , es [ 'previous_application' ] [ 'SK_ID_CURR' ] ) es = es . add_relationship ( r_applications_prev_apps )
16	plt . rcParams [ 'axes.labelsize' ] = 14 plt . rcParams [ 'xtick.labelsize' ] = 12 plt . rcParams [ 'ytick.labelsize' ] = 12
933	results . sort_values ( 'score' , ascending = False , inplace = True ) results . reset_index ( inplace = True ) return results
411	clf2 = OneVsRestClassifier ( LogisticRegression ( penalty = 'l1' ) ) clf2 . fit ( X_train_multilabel , y_train ) y_pred2 = clf2 . predict ( X_test_multilabel )
1133	val_predictions_stacked = val_predictions_stacked [ : , y_min_pad : - y_max_pad , x_min_pad : - x_max_pad ] val_masks_stacked = val_masks_stacked [ : , y_min_pad : - y_max_pad , x_min_pad : - x_max_pad ] print ( val_masks_stacked . shape , val_predictions_stacked . shape )
1043	print ( "Your selected dataframe has " + str ( df . shape [ 1 ] ) + " columns.\n" "There are " + str ( mis_val_table_ren_columns . shape [ 0 ] ) + " columns that have missing values." )
125	base_df = pd . read_csv ( "/kaggle/input/covid19-global-forecasting-week-2/train.csv" ) base_df = base_df . rename ( { "Province_State" : "state" , "Country_Region" : "country" } , axis = 1 ) base_df . loc [ base_df [ "state" ] . isna ( ) , "state" ] = "Unknown" scoring_dates = test [ "Date" ] . unique ( )
1291	x = squeeze_excite_block ( x ) m = add ( [ x , init ] ) return m def _create_se_resnet ( classes , img_input , include_top , initial_conv_filters , filters , depth , width , bottleneck , weight_decay , pooling ) : channel_axis = 1 if K . image_data_format ( ) == 'channels_first' else - 1 N = list ( depth )
144	concat_sub [ 'm_max' ] = concat_sub . iloc [ : , 1 : ncol ] . max ( axis = 1 ) concat_sub [ 'm_min' ] = concat_sub . iloc [ : , 1 : ncol ] . min ( axis = 1 ) concat_sub [ 'm_mean' ] = concat_sub . iloc [ : , 1 : ncol ] . mean ( axis = 1 ) concat_sub [ 'm_median' ] = concat_sub . iloc [ : , 1 : ncol ] . median ( axis = 1 )
897	plt . figure ( figsize = ( 8 , 6 ) ) plt . plot ( list ( range ( len ( df ) ) ) , df [ 'cumulative_importance' ] , 'r-' ) plt . xlabel ( 'Number of Features' ) ; plt . ylabel ( 'Cumulative Importance' ) ; plt . title ( 'Cumulative Feature Importance' ) ; plt . show ( ) ; importance_index = np . min ( np . where ( df [ 'cumulative_importance' ] > threshold ) ) print ( '%d features required for %0.2f of cumulative importance' % ( importance_index + 1 , threshold ) ) return df
183	brands = pd . DataFrame ( train . brand_name . value_counts ( ) ) brands . reset_index ( level = 0 , inplace = True ) brands = brands . sort_values ( by = 'brand_name' , ascending = False ) . head ( 15 ) brands . columns = ( 'brand_name' , 'number_of_item' )
950	for i , hyper in enumerate ( random_hyp . columns ) : if hyper not in [ 'boosting_type' , 'iteration' , 'subsample' , 'score' , 'learning_rate' , 'is_unbalance' , 'metric' , 'verbose' , 'iteration' , 'n_estimators' , 'search' ] : plt . figure ( figsize = ( 14 , 6 ) )
15	from sklearn . preprocessing import OneHotEncoder , LabelEncoder from sklearn import feature_selection from sklearn import model_selection from sklearn import metrics from sklearn . base import BaseEstimator , TransformerMixin from sklearn . utils import check_array from scipy import sparse
140	random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) set_seed ( )
1750	es = es . entity_from_dataframe ( entity_id = 'previous_application' , make_index = True , dataframe = prev_app_df , index = 'prev_app_id' )
572	orderCount = orders [ orders [ "eval_set" ] == "prior" ] . groupby ( by = [ "user_id" ] ) [ "order_id" ] . count ( ) . to_frame ( ) fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( color = " ax.set(xlabel='Order Count',title=" Order Count " )
1175	ren . AddActor ( cylinderActor ) ren . SetBackground ( colors . GetColor3d ( "BkgColor" ) ) ren . ResetCamera ( ) ren . GetActiveCamera ( ) . Zoom ( 1.5 )
69	img = np . expand_dims ( img , - 1 ) msk = np . expand_dims ( msk , - 1 ) return img , msk def __loadpredict__ ( self , filename ) :
29	train = pd . read_csv ( "../input/train.csv" ) test = pd . read_csv ( "../input/test.csv" ) print ( "{} observations and {} features in train set." . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( "{} observations and {} features in test set." . format ( test . shape [ 0 ] , test . shape [ 1 ] ) )
152	plt . figure ( figsize = ( 8 , 5 ) ) plt . title ( 'How many times, each categories of clickers download the app ? \n' , fontsize = 15 ) sns . heatmap ( cross_tab , annot = True , fmt = '.0%' ) plt . xlabel ( 'Categories of clickers ' , fontsize = 10 ) plt . ylabel ( 'Categories of downloaders' , fontsize = 10 )
530	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
1510	idx2 = tf . linalg . matmul ( m , tf . cast ( idx , dtype = 'float32' ) ) idx2 = K . cast ( idx2 , dtype = 'int32' ) idx2 = K . clip ( idx2 , - DIM // 2 + XDIM + 1 , DIM // 2 )
332	for j in range ( 0 , len ( batch1 ) ) : patientId = batch1 [ j ] path = \ '../input/rsna-pneumonia-detection-challenge/stage_1_train_images/%s.dcm' % patientId dcm_data = pydicom . read_file ( path )
490	val_p = [ 'APARTMENTS_AVG' , 'BASEMENTAREA_AVG' , 'YEARS_BEGINEXPLUATATION_AVG' , 'YEARS_BUILD_AVG' , 'COMMONAREA_AVG' , 'ELEVATORS_AVG' , 'ENTRANCES_AVG' , 'FLOORSMAX_AVG' , 'FLOORSMIN_AVG' ] for i in val_p : plt . figure ( figsize = ( 5 , 5 ) ) sns . distplot ( application_train [ i ] . dropna ( ) , kde = True , color = 'g' ) plt . title ( i ) plt . xticks ( rotation = - 45 ) plt . show ( )
1137	train_months = X_train . DateAvSigVersion . apply ( lambda x : '{}-{}' . format ( x . year , x . month ) ) test_months = X_test . DateAvSigVersion . apply ( lambda x : '{}-{}' . format ( x . year , x . month ) ) df_months = pd . DataFrame ( train_months . value_counts ( ) ) . reset_index ( ) df_months = df_months . merge ( pd . DataFrame ( test_months . value_counts ( ) ) . reset_index ( ) , how = 'left' , on = 'index' ) df_months . sort_values ( 'index' )
181	plt . figure ( figsize = ( 20 , 20 ) ) sns . boxplot ( x = 'price' , y = 'cat1' , data = train , orient = 'h' ) plt . title ( 'Prices of the first level of categories' , fontsize = 30 ) plt . ylabel ( 'First level categories' , fontsize = 20 ) plt . xlabel ( 'Price' , fontsize = 20 )
1016	bureau_by_loan = bureau_balance_agg . merge ( bureau_balance_counts , right_index = True , left_on = 'SK_ID_BUREAU' , how = 'outer' ) bureau_by_loan = bureau [ [ 'SK_ID_BUREAU' , 'SK_ID_CURR' ] ] . merge ( bureau_by_loan , on = 'SK_ID_BUREAU' , how = 'left' ) bureau_balance_by_client = agg_numeric ( bureau_by_loan . drop ( columns = [ 'SK_ID_BUREAU' ] ) , group_var = 'SK_ID_CURR' , df_name = 'client' )
1802	def img_cor_2_world_cor ( ) : x_img , y_img , z_img = img_cor_points [ 0 ] xc , yc , zc = x_img * z_img , y_img * z_img , z_img p_cam = np . array ( [ xc , yc , zc ] ) xw , yw , zw = np . dot ( np . linalg . inv ( k ) , p_cam ) print ( xw , yw , zw ) print ( x , y , z )
679	length = 5 labels = [ ] for label in df_train [ "labels" ] : if type ( label ) == str : split_label = label . split ( ) [ : : length ] labels += split_label
382	TRAIN_MASKS_CSV [ 'id' ] = TRAIN_MASKS_CSV [ 'img' ] . apply ( lambda x : x [ : - 7 ] ) len ( TRAIN_MASKS_CSV [ 'id' ] . unique ( ) ) , len ( TRAIN_MASKS_CSV [ 'id' ] . unique ( ) ) * 16
1744	BATCH_SIZE = 20000 epochs = 5 model = get_model ( ) model . fit ( X_train , dtrain . target , epochs = epochs , batch_size = BATCH_SIZE , validation_data = ( X_valid , dvalid . target ) , verbose = 1 )
899	if encoding == 'ohe' : features = pd . get_dummies ( features ) test_features = pd . get_dummies ( test_features )
1711	data = pd . read_csv ( '../input/train.csv' ) parent_data = data . copy ( ) ID = data . pop ( 'id' )
785	df [ 'importance_normalized' ] = df [ 'importance' ] / df [ 'importance' ] . sum ( ) df [ 'cumulative_importance' ] = np . cumsum ( df [ 'importance_normalized' ] ) plt . rcParams [ 'font.size' ] = 12
974	for i , hyp in enumerate ( results [ 'hyperparameters' ] ) : hyp_df = hyp_df . append ( pd . DataFrame ( hyp , index = [ 0 ] ) , ignore_index = True , sort = True )
1732	real_samples_indexes = np . argwhere ( np . sum ( unique_count , axis = 1 ) > 0 ) [ : , 0 ] synthetic_samples_indexes = np . argwhere ( np . sum ( unique_count , axis = 1 ) == 0 ) [ : , 0 ] print ( len ( real_samples_indexes ) ) print ( len ( synthetic_samples_indexes ) ) test1 = read_test ( ) ids = ids [ real_samples_indexes ] return ids
100	import albumentations from albumentations . core . transforms_interface import DualTransform , ImageOnlyTransform from albumentations . augmentations import functional as F class GridMask ( DualTransform ) :
486	from keras . utils import plot_model from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers . recurrent import LSTM
1647	fold = tqdm_notebook ( folds , desc = 'fold:' ) for f in fold : smin1 = 100
604	from tqdm import tqdm_notebook X_raw = [ ] for f in tqdm_notebook ( files ) : sample_rate , sample_ = wavfile . read ( str ( train_audio_path ) + f ) X_raw . append ( sample_ )
908	for col in df : if col != parent_var and 'SK_ID' in col : df = df . drop ( columns = col )
227	fig = px . scatter_3d ( commits_df , x = 'hidden_dim_second' , y = 'hidden_dim_third' , z = 'LB_score' , color = 'best' , symbol = 'hidden_dim_first' , title = 'hidden_dim_2nd & 3nd and LB score visualization of COVID-19 mRNA VDP solutions' ) fig . update ( layout = dict ( title = dict ( x = 0.07 ) ) )
1005	feature_matrix_test , feature_names_test = ft . dfs ( entityset = es , target_entity = 'app_test' , agg_primitives = [ 'mean' , 'max' , 'min' , 'trend' , 'mode' , 'count' , 'sum' , 'percent_true' , NormalizedModeCount , MostRecent , LongestSeq ] , trans_primitives = [ 'diff' , 'cum_sum' , 'cum_mean' , 'percentile' ] , where_primitives = [ 'mean' , 'sum' ] , seed_features = [ late_payment , past_due ] , max_depth = 2 , features_only = False , verbose = True , chunk_size = len ( app_test ) , ignore_entities = [ 'app_train' ] )
838	for b , df in plot_data . groupby ( 'fare-bin' ) : axs . scatter ( df . dropoff_longitude , df . dropoff_latitude , zorder = 1 , alpha = 0.2 , c = df . color , s = 30 , label = f'{b}' ) axs . set_xlim ( ( BB [ 0 ] , BB [ 1 ] ) ) axs . set_ylim ( ( BB [ 2 ] , BB [ 3 ] ) ) axs . set_title ( 'Dropoff locations' , size = 32 ) axs . axis ( 'off' )
1225	band_1 = gray_reshape ( band_1 ) band_2 = gray_reshape ( band_2 ) band_3 = gray_reshape ( band_3 ) print ( 'Denoising and reshaping' ) if train_b and clean_b :
469	from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import train_test_split , StratifiedKFold from bayes_opt import BayesianOptimization from datetime import datetime from sklearn . metrics import precision_score , recall_score , confusion_matrix , accuracy_score , roc_auc_score , f1_score , roc_curve , auc , precision_recall_curve from sklearn import metrics from sklearn import preprocessing import catboost from catboost import Pool
1539	testing_series_1 = X_train . iloc [ 0 , 5 : 494 ] testing_series_2 = X_train . iloc [ 0 , 5 : 494 ] . shift ( - 1 ) testing_series_3 = X_train . iloc [ 1 , 5 : 494 ] testing_series_4 = pd . Series ( [ 0 , 0 , 0 , 0 ] )
6	bad_rows = pd . read_csv ( '../input/bad-rows-ashrae/rows_to_drop.csv' ) train_df . drop ( bad_rows . loc [ : , '0' ] , inplace = True ) train_df . reset_index ( drop = True , inplace = True )
1297	test_filenames = os . listdir ( TEST_DIR ) test_df = pd . DataFrame ( { 'id' : test_filenames } ) test_datagen = tf . keras . preprocessing . image . ImageDataGenerator ( rescale = 1.0 / 255. ) test_generator = test_datagen . flow_from_dataframe ( test_df , TEST_DIR , x_col = 'id' , y_col = None , class_mode = None , target_size = ( IMAGE_WIDTH , IMAGE_HEIGHT ) , batch_size = BATCH_SIZE , shuffle = False ) yhat = model . predict_generator ( test_generator , steps = np . ceil ( TEST_SIZE / BATCH_SIZE ) )
1644	train_smp [ [ 'click_rnd' , 'is_attributed' ] ] . groupby ( [ 'click_rnd' ] , as_index = True ) . count ( ) . plot ( ) plt . title ( 'HOURLY CLICK FREQUENCY' ) ; plt . ylabel ( 'Number of Clicks' ) ; train_smp [ [ 'click_rnd' , 'is_attributed' ] ] . groupby ( [ 'click_rnd' ] , as_index = True ) . mean ( ) . plot ( ) plt . title ( 'HOURLY CONVERSION RATIO' ) ; plt . ylabel ( 'Converted Ratio' ) ;
1686	h = pixmap . shape [ 0 ] if h % 2 == 1 : h = h // 2 return [ pixmap [ : h , : ] , pixmap [ h + 1 : , : ] ] else : h = h // 2 return [ pixmap [ : h , : ] , pixmap [ h : , : ] ]
453	draw = ImageDraw . Draw ( image ) im_width , im_height = image . size ( left , right , top , bottom ) = ( xmin * im_width , xmax * im_width , ymin * im_height , ymax * im_height ) draw . line ( [ ( left , top ) , ( left , bottom ) , ( right , bottom ) , ( right , top ) , ( left , top ) ] , width = thickness , fill = color )
1655	sns . heatmap ( corrmat , vmax = 1. , square = True , cmap = "Blues" , annot = True ) plt . title ( "Important variables correlation map" , fontsize = 15 ) plt . show ( )
1630	stats = [ ] for Province in sorted ( full_table [ 'Province/State' ] . unique ( ) ) : if ( Province == '' ) : continue df = get_time_series_province ( Province ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 500 ) : continue print ( '{} COVID-19 Prediction' . format ( Province ) ) opt_display_model ( df , stats )
1814	predictions = json . load ( open ( 'results_roberta_large/predictions_.json' , 'r' ) ) submission = pd . read_csv ( open ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' , 'r' ) ) for i in range ( len ( submission ) ) : id_ = submission [ 'textID' ] [ i ] if pd_test [ 'sentiment' ] [ i ] == 'neutral' : submission . loc [ i , 'selected_text' ] = f ( pd_test [ 'text' ] [ i ] ) else : submission . loc [ i , 'selected_text' ] = f ( predictions [ id_ ] )
692	from pathlib import Path if Path ( previous_model_name ) . is_file ( ) : print ( "Using previous sucessful run's model" ) model2 = load_model ( previous_model_name , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } ) else : print ( "Using stored trained model" ) model2 = load_model ( stored_trained_model , custom_objects = { 'my_iou_metric_2' : my_iou_metric , 'lovasz_loss' : lovasz_loss , 'my_iou_metric' : my_iou_metric } )
1614	dev_df = df_train [ train_dates <= datetime . date ( 2017 , 5 , 31 ) ] val_df = df_train [ train_dates > datetime . date ( 2017 , 5 , 31 ) ] dev_y = np . log1p ( dev_df [ "totals_transactionRevenue" ] . values ) val_y = np . log1p ( val_df [ "totals_transactionRevenue" ] . values ) use_cols = [ col for col in df_train . columns if col not in not_use_cols ] dev_X = dev_df [ use_cols ] val_X = val_df [ use_cols ] test_X = df_test [ use_cols ]
417	bold ( '**Preview of building data**' ) display ( building . head ( 3 ) ) bold ( '**Preview of Weather Train Data:**' ) display ( weather_train . head ( 3 ) ) bold ( '**Preview of Weather Test Data:**' ) display ( weather_test . head ( 3 ) ) bold ( '**Preview of Train Data:**' ) display ( train . head ( 3 ) ) bold ( '**Preview of Test Data:**' ) display ( test . head ( 3 ) )
1206	densenet = DenseNet121 ( weights = '/kaggle/input/densenet-keras/DenseNet-BC-121-32-no-top.h5' , include_top = False , input_shape = ( 224 , 224 , 3 ) ) for layer in densenet . layers : layer . trainable = False
1493	if FLAGS . do_valid : if FLAGS . smaller_valid_dataset : predict_file = FLAGS . validation_predict_file_small else : predict_file = FLAGS . validation_predict_file f1 , long_f1 , short_f1 = compute_f1_scores ( valid_predictions_json , predict_file ) print ( f" valid f1: {f1}\n valid long_f1: {long_f1}\nvalid short_f1: {short_f1}" )
315	svr = SVR ( ) svr . fit ( train , target ) acc_model ( 1 , svr , train , test )
1514	FIGSIZE = 13.0 SPACING = 0.1 subplot = ( rows , cols , 1 ) if rows < cols : plt . figure ( figsize = ( FIGSIZE , FIGSIZE / cols * rows ) ) else : plt . figure ( figsize = ( FIGSIZE / rows * cols , FIGSIZE ) )
270	w_lgb = 0.5 w_wnet = 1 - w_lgb print ( w_wnet )
1409	X_COL = "var_81" Y_COL = "var_68" Z_COL = "var_108" HUE_COL = "target" N_SAMPLES = 10000 df = train_df . sample ( N_SAMPLES )
340	df_preds = pd . DataFrame ( predictions ) new_names = [ 'conf_1' , 'x_1' , 'y_1' , 'width_1' , 'height_1' , 'conf_2' , 'x_2' , 'y_2' , 'width_2' , 'height_2' ] df_preds . columns = new_names df_preds [ 'patientId' ] = df_test [ 'patientId' ] df_preds [ 'PredictionString' ] = 0
1246	fig = px . line ( history . history , y = [ 'loss' , 'val_loss' ] , labels = { 'index' : 'epoch' , 'value' : 'MCRMSE' } , title = 'Training History' ) fig . show ( )
1613	dev_df = df_train [ train_dates <= datetime . date ( 2017 , 5 , 31 ) ] val_df = df_train [ train_dates > datetime . date ( 2017 , 5 , 31 ) ] dev_y = np . log1p ( dev_df [ "totals_transactionRevenue" ] . values ) val_y = np . log1p ( val_df [ "totals_transactionRevenue" ] . values ) use_cols = [ col for col in df_train . columns if col not in not_use_cols ] dev_X = dev_df [ use_cols ] val_X = val_df [ use_cols ] test_X = df_test [ use_cols ]
1778	train = pd . read_csv ( "../input/train.csv" ) train . head ( )
733	img = cv2 . imread ( os . path . join ( dataset_path , img_name ) ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) kp , des = detector . detectAndCompute ( img , None ) return img , kp , des def draw_image_matches ( detector , img1_name , img2_name , nmatches = 10 ) :
56	from fastai import * from fastai . vision import * from sklearn . metrics import f1_score
509	def evaluate_threshold ( tpr , fpr , clf_threshold , threshold ) : print ( 'Sensitivity:' , tpr [ clf_threshold > threshold ] [ - 1 ] ) print ( 'Specificity:' , 1 - fpr [ clf_threshold > threshold ] [ - 1 ] )
1440	N = train_df . shape [ 0 ] x_train = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( train_df [ 'Patient' ] ) ) : x_train [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
1674	for box in data [ 'boxes' ] : rgb = np . floor ( np . random . rand ( 3 ) * 256 ) . astype ( 'int' ) im = overlay_box ( im = im , box = box , rgb = rgb , stroke = 6 ) plt . imshow ( im , cmap = plt . cm . gist_gray ) plt . axis ( 'off' ) def overlay_box ( im , box , rgb , stroke = 1 ) :
961	feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'app' , trans_primitives = default_trans_primitives , agg_primitives = default_agg_primitives , max_depth = 2 , features_only = False , verbose = True ) pd . options . display . max_columns = 1700 feature_matrix . head ( 10 )
957	r_previous_cash = ft . Relationship ( es [ 'previous' ] [ 'SK_ID_PREV' ] , es [ 'cash' ] [ 'SK_ID_PREV' ] ) r_previous_installments = ft . Relationship ( es [ 'previous' ] [ 'SK_ID_PREV' ] , es [ 'installments' ] [ 'SK_ID_PREV' ] ) r_previous_credit = ft . Relationship ( es [ 'previous' ] [ 'SK_ID_PREV' ] , es [ 'credit' ] [ 'SK_ID_PREV' ] )
523	temp = df_predictions_tourney [ 'Conf_Strength' ] df_predictions_tourney . drop ( labels = [ 'Conf_Strength' ] , axis = 1 , inplace = True ) df_predictions_tourney . insert ( 0 , 'Conf_Strength' , temp ) df_predictions_tourney . head ( )
1122	params = { 'learning_rate' : 0.03 , 'boosting' : 'gbdt' , 'objective' : 'regression' , 'metric' : 'rmse' , 'num_leaves' : 64 , 'min_data_in_leaf' : 6 , 'max_bin' : 255 , 'bagging_fraction' : 0.7 , 'lambda_l2' : 1e-4 , 'max_depth' : 12 , 'seed' : 1337 , 'nthreads' : 6 }
1012	column_names . append ( '%s_%s_%s' % ( df_name , var , stat ) ) categorical . columns = column_names return categorical
979	app_types = { } for col in app_train : if ( app_train [ col ] . dtype != 'object' ) and ( len ( app_train [ col ] . unique ( ) ) <= 2 ) : app_types [ col ] = ft . variable_types . Boolean print ( 'Number of boolean variables: ' , len ( app_types ) )
1663	ani = animation . FuncAnimation ( fig , updatefig , frames = range ( len ( sample_image ) ) , interval = 50 , blit = True ) ani . save ( 'Chest_Cavity.gif' , writer = 'imagemagick' ) plt . show ( )
1534	std_tpr = np . std ( tprs_interp , axis = 0 ) tprs_upper = np . minimum ( mean_tpr + std_tpr , 1 ) tprs_lower = np . maximum ( mean_tpr - std_tpr , 0 ) ax . fill_between ( mean_fpr , tprs_lower , tprs_upper , color = 'grey' , alpha = .2 , label = r'$\pm$ 1 std. dev.' )
1643	proportion = train [ [ 'device' , 'is_attributed' ] ] . groupby ( 'device' , as_index = False ) . mean ( ) . sort_values ( 'is_attributed' , ascending = False ) counts = train [ [ 'device' , 'is_attributed' ] ] . groupby ( 'device' , as_index = False ) . count ( ) . sort_values ( 'is_attributed' , ascending = False ) merge = counts . merge ( proportion , on = 'device' , how = 'left' ) merge . columns = [ 'device' , 'click_count' , 'prop_downloaded' ] print ( 'Count of clicks and proportion of downloads by device:' ) print ( merge )
569	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( data = orders , x = "order_hour_of_day" , ax = ax , color = " ax.set(xlabel='Hour Of The Day',title=" Order Count Across Hour Of The Day " )
1408	preds = clf . predict_proba ( X_test ) [ : , 1 ] clipped_preds = np . clip ( preds , 0.05 , 0.95 ) df_sample_sub . Pred = clipped_preds
1403	cmp_enc_start = TRAIN_END_DATE - timedelta ( days = 28 * 3 ) cmp_enc_end = TRAIN_END_DATE encoder_input_data , encode_series_mean , encode_series_std = get_data_encode_decode ( series_array , exog_array , first_n_samples , date_to_index , cmp_enc_start , cmp_enc_end , pred = True )
1127	print ( 'Initialize.' ) train_df = pd . read_csv ( '{}train.csv' . format ( data_src ) , usecols = [ 0 ] , index_col = 'id' ) depths_df = pd . read_csv ( '{}depths.csv' . format ( data_src ) , index_col = 'id' ) train_df = train_df . join ( depths_df ) test_df = depths_df [ ~ depths_df . index . isin ( train_df . index ) ]
193	blackhat = cv2 . morphologyEx ( grayScale , cv2 . MORPH_BLACKHAT , kernel ) plt . subplot ( l , 5 , ( i * 5 ) + 3 ) plt . imshow ( blackhat ) plt . axis ( 'off' ) plt . title ( 'blackhat : ' + image_name )
440	plt . figure ( figsize = ( 15 , 6 ) ) df_train . Path . value_counts ( ) [ : 50 ] . plot ( kind = 'bar' , color = 'teal' ) plt . xlabel ( "Path" , fontsize = 18 ) plt . ylabel ( "Count" , fontsize = 18 ) plt . title ( "TOP 50 most commmon Paths" , fontsize = 22 ) plt . show ( )
1358	start_mem = df . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( 'Memory usage of dataframe is {:.2f} MB' . format ( start_mem ) ) for col in df . columns : if is_datetime ( df [ col ] ) or is_categorical_dtype ( df [ col ] ) :
1046	df_by_loan = df_agg . merge ( df [ [ group_vars [ 0 ] , group_vars [ 1 ] ] ] , on = group_vars [ 0 ] , how = 'left' ) gc . enable ( ) del df_agg gc . collect ( )
1593	train_X = train_df [ "question_text" ] . fillna ( "_na_" ) . values val_X = val_df [ "question_text" ] . fillna ( "_na_" ) . values test_X = test_df [ "question_text" ] . fillna ( "_na_" ) . values
1718	tokenizer = Tokenizer ( num_words = max_features ) tokenizer . fit_on_texts ( list ( train_X ) ) train_X = tokenizer . texts_to_sequences ( train_X ) test_X = tokenizer . texts_to_sequences ( test_X )
741	def local_deform ( lines ) : res = [ ] for line in lines : res . append ( deform_single_line ( line ) ) return res def scatter_line ( line , base_size = 256 , c = 'b' ) : x , y = line [ 0 ] , line [ 1 ] for idx , ele in enumerate ( y ) : y [ idx ] = base_size - ele plt . scatter ( x , y , c = c ) plt . xlim ( 0 , base_size ) plt . ylim ( 0 , base_size )
512	columns = list ( df_season_composite . columns . values ) columns . pop ( columns . index ( 'WINPCT' ) ) columns . append ( 'WINPCT' ) df_season_composite = df_season_composite [ columns ] df_season_composite . rename ( columns = { 'WTeamID' : 'TeamID' } , inplace = True ) df_season_composite . head ( )
583	batch_probs = sum ( m ( batch ) . detach ( ) . cpu ( ) for m in models ) / len ( models ) batch_probs = batch_probs . numpy ( ) probs . append ( batch_probs ) batch = [ ]
655	def check_missing_values ( df ) : if df . isnull ( ) . any ( ) . any ( ) : print ( "There are missing values in the dataframe" ) else : print ( "There are no missing values in the dataframe" ) check_missing_values ( train_df ) check_missing_values ( test_df )
1684	import numpy as np import pandas as pd import itertools import random import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' training_tasks = sorted ( os . listdir ( training_path ) )
1533	mean_tpr = np . mean ( tprs_interp , axis = 0 ) mean_tpr [ - 1 ] = 1.0 mean_auc = auc ( mean_fpr , mean_tpr ) std_auc = np . std ( aucs ) ax . plot ( mean_fpr , mean_tpr , color = 'b' , label = r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % ( mean_auc , std_auc ) , lw = 2 , alpha = .8 )
9	weather_df = pd . read_csv ( '/kaggle/input/ashrae-energy-prediction/weather_test.csv' ) weather_df = fill_weather_dataset ( weather_df ) weather_df = reduce_mem_usage ( weather_df )
89	import scipy print ( model_pred . clip ( 0.35 , 0.65 ) . mean ( ) ) print ( scipy . stats . median_absolute_deviation ( model_pred . clip ( 0.35 , 0.65 ) ) [ 0 ] )
475	vector = vectorizer . transform ( text ) print ( vector . shape ) print ( vector . toarray ( ) )
167	precisions_for_samples_by_classes = np . zeros ( ( num_samples , num_classes ) ) for sample_num in range ( num_samples ) : pos_class_indices , precision_at_hits = ( _one_sample_positive_class_precisions ( scores [ sample_num , : ] , truth [ sample_num , : ] ) ) precisions_for_samples_by_classes [ sample_num , pos_class_indices ] = ( precision_at_hits ) labels_per_class = np . sum ( truth > 0 , axis = 0 ) weight_per_class = labels_per_class / float ( np . sum ( labels_per_class ) )
1608	params = { 'min_child_weight' : 10.0 , 'max_depth' : 7 , 'max_delta_step' : 1.8 , 'colsample_bytree' : 0.4 , 'subsample' : 0.8 , 'eta' : 0.025 , 'gamma' : 0.65 , 'num_boost_round' : 700 }
888	train_labels = train_bureau [ 'TARGET' ] previous_features . append ( 'SK_ID_CURR' ) train_ids = train_bureau [ 'SK_ID_CURR' ] test_ids = test_bureau [ 'SK_ID_CURR' ] train = train_bureau . merge ( train_previous [ previous_features ] , on = 'SK_ID_CURR' ) test = test_bureau . merge ( test_previous [ previous_features ] , on = 'SK_ID_CURR' ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
1790	from statsmodels . tsa . tsatools import lagmat from sklearn . linear_model import LinearRegression , RidgeCV from sklearn . ensemble import RandomForestRegressor from sklearn . metrics import r2_score
1113	X_text = X_temp [ text_columns ] for i in X_text . columns : X_text . loc [ : , i ] = X_text . loc [ : , i ] . fillna ( '<MISSING>' )
368	decision_tree = DecisionTreeRegressor ( ) decision_tree . fit ( train , target ) acc_model ( 5 , decision_tree , train , test )
1224	train_b = True or train_all train_img = True or train_all train_total = True or train_all predict_submission = True and train_all clean_all = False clean_b = False or clean_all clean_img = False or clean_all load_all = False load_b = False or load_all load_img = False or load_all
956	es = es . entity_from_dataframe ( entity_id = 'bureau_balance' , dataframe = bureau_balance , make_index = True , index = 'bureaubalance_index' ) es = es . entity_from_dataframe ( entity_id = 'cash' , dataframe = cash , make_index = True , index = 'cash_index' ) es = es . entity_from_dataframe ( entity_id = 'installments' , dataframe = installments , make_index = True , index = 'installments_index' ) es = es . entity_from_dataframe ( entity_id = 'credit' , dataframe = credit , make_index = True , index = 'credit_index' )
857	valid_preds = random_forest . predict ( X_valid [ features ] ) plt . figure ( figsize = ( 10 , 6 ) ) sns . kdeplot ( y_valid , label = 'Actual' ) sns . kdeplot ( valid_preds , label = 'Predicted' ) plt . legend ( prop = { 'size' : 30 } ) plt . title ( "Distribution of Validation Fares" ) ;
476	text2 = [ "the the the times" ] vector = vectorizer . transform ( text2 ) print ( vector . toarray ( ) )
135	from sklearn import metrics fbeta_sklearn = metrics . fbeta_score ( valid_labels , preds , 2 , average = 'samples' ) print ( fbeta_sklearn )
1618	t1 = time . time ( ) preds_test = np . zeros ( np . squeeze ( x_test ) . shape ) for cv_index in range ( cv_total ) : basic_name = 'Unet_resnet_v{}_cv{}' . format ( version , cv_index + 1 ) model . load_weights ( basic_name + '.model' ) preds_test += predict_result ( model , x_test , img_size_target ) / cv_total t2 = time . time ( ) print ( "Usedtime = {} s" . format ( t2 - t1 ) )
347	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) import matplotlib . pyplot as plt import featuretools as ft from featuretools . primitives import * from featuretools . variable_types import Numeric from sklearn . preprocessing import LabelEncoder , MinMaxScaler from sklearn . svm import LinearSVR from sklearn . feature_selection import SelectFromModel from sklearn . ensemble import RandomForestRegressor import warnings warnings . filterwarnings ( "ignore" )
775	plt . rcParams [ 'font.size' ] = 18 plt . figure ( figsize = ( 12 , 12 ) ) sns . heatmap ( corr_mat , vmin = - 0.5 , vmax = 0.8 , center = 0 , cmap = plt . cm . RdYlGn_r , annot = True ) ;
1635	data = All_df [ 0 : 91713 ] Test_data = All_df [ 91713 : 131021 ] y = np . array ( All_y [ 0 : 91713 ] . tolist ( ) ) random_state = 23 X_train , X_test , y_train , y_test = train_test_split ( data , y , test_size = 0.2 , random_state = random_state , stratify = y ) X_train = pd . DataFrame ( X_train , columns = data . columns ) X_test = pd . DataFrame ( X_test , columns = data . columns )
311	target_cols = [ y_toxic , y_severe_toxic , y_obscene , y_threat , y_insult , y_identity_hate ] preds = [ ] for col in target_cols : print ( '\n' )
427	ax . bar ( bar_dir , rosedata [ c1 ] . values , width = bar_width , color = palette [ 0 ] , edgecolor = 'none' , label = c1 , linewidth = 0 )
718	if mode == 'train' : classify_model , classify_preprocess , model , preprocess = load_objects ( './' ) else : classify_model , classify_preprocess , model , preprocess = load_objects ( path_self )
401	dataiter = iter ( training_loader ) images , labels = dataiter . next ( ) fig = plt . figure ( figsize = ( 25 , 4 ) )
988	example_credit = cash [ cash [ 'SK_ID_PREV' ] == 1369693 ] plt . plot ( example_credit [ 'cash_balance_date' ] , example_credit [ 'NAME_CONTRACT_STATUS' ] , 'ro' ) ; plt . title ( 'Loan 1369693 over Time' ) ; plt . xlabel ( 'Date' ) ; plt . ylabel ( 'Contract Status' ) ;
568	aisles = pd . read_csv ( '../input/aisles.csv' ) departments = pd . read_csv ( '../input//departments.csv' ) orderProductsTrain = pd . read_csv ( '../input/order_products__train.csv' ) orders = pd . read_csv ( '../input/orders.csv' ) products = pd . read_csv ( '../input/products.csv' ) orderProductsPrior = pd . read_csv ( '../input/order_products__prior.csv' )
622	class DatasetRetriever ( Dataset ) : def __init__ ( self , marking , image_ids , transforms = None , test = False ) : super ( ) . __init__ ( ) self . image_ids = image_ids self . marking = marking self . transforms = transforms self . test = test self . alpha = 1.0 def __getitem__ ( self , index : int ) : image_id = self . image_ids [ index ] if self . test or random . random ( ) > 0.55 : image , boxes = self . load_image_and_boxes ( index ) else :
765	for size in msizes : markers . append ( plt . scatter ( [ ] , [ ] , s = 100 * size , label = f'{int(round(np.square(size) / 100) * 100)}' , color = 'lightgreen' , alpha = 0.6 , edgecolor = 'k' , linewidth = 1.5 ) )
420	bold ( '**ELECTRICITY THE MOST FREQUENT METER TYPE MEASURED**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) ax = sns . countplot ( data = train , x = 'meter' , palette = 'CMRmap' , alpha = 0.5 ) ax . set_ylabel ( 'Count' , fontsize = 20 ) ax . set_xlabel ( 'Meter Type' , fontsize = 20 ) plt . show ( )
1270	unique_colors = np . unique ( true_image ) for i , color in enumerate ( unique_colors ) : image = np . copy ( true_image ) if color == background : continue image [ image != color ] = background inx = np . where ( image == color ) obj = image [ np . min ( inx [ 0 ] ) : np . max ( inx [ 0 ] ) + 1 , np . min ( inx [ 1 ] ) : np . max ( inx [ 1 ] ) + 1 ]
677	df_train = df_train [ df_train [ 'outliers' ] == 0 ] target = df_train [ 'target' ] del df_train [ 'target' ] features = [ c for c in df_train . columns if c not in [ 'card_id' , 'first_active_month' , 'outliers' ] ] categorical_feats = [ c for c in features if 'feature_' in c ]
528	pipe1 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf1' , clf1 ) ] ) pipe3 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf3' , clf3 ) ] ) pipe4 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf4' , clf4 ) ] ) pipe5 = Pipeline ( [ ( 'std' , StandardScaler ( ) ) , ( 'clf5' , clf5 ) ] )
1543	score = median_model ( df_train . copy ( ) , df_actual . copy ( ) , 14 , review = True ) print ( "The SMAPE score is : %.5f" % score ) score = arima_model ( df_train . copy ( ) , df_actual . copy ( ) , 7 , 1 , 2 , review = True ) print ( "The SMAPE score is : %.5f" % score )
338	import matplotlib . pyplot as plt loss = history . history [ 'loss' ] val_loss = history . history [ 'val_loss' ] epochs = range ( 1 , len ( loss ) + 1 ) plt . legend ( ) plt . plot ( epochs , loss , 'bo' , label = 'Training loss' ) plt . plot ( epochs , val_loss , 'b' , label = 'Validation loss' ) plt . title ( 'Training and validation loss' ) plt . legend ( ) plt . show ( )
1402	exog_input_data = exog_array [ : first_n_samples , exog_inds , : ] [ : , 1 : , : ] else : exog_input_data = exog_array [ : first_n_samples , exog_inds , : ] [ : , : , : ]
593	for i in cat_feature : df [ i ] = pd . factorize ( df [ i ] ) [ 0 ] trn_cat = df [ cat_feature ] . values [ : 182080 ] tst_cat = df [ cat_feature ] . values [ 182080 : ]
1263	data_generator = create_datagen ( ) train_gen = create_flow ( data_generator , 'training' ) val_gen = create_flow ( data_generator , 'validation' ) test_gen = create_test_gen ( )
1640	dtypes = { 'ip' : 'uint32' , 'app' : 'uint16' , 'device' : 'uint16' , 'os' : 'uint16' , 'channel' : 'uint16' , 'is_attributed' : 'uint8' } train = dd . read_csv ( "../input/train.csv" , dtype = dtypes , parse_dates = [ 'click_time' , 'attributed_time' ] ) train . head ( )
859	rs = RandomizedSearchCV ( estimator , param_grid , n_jobs = - 1 , scoring = 'neg_mean_absolute_error' , cv = 3 , n_iter = 100 , verbose = 1 , random_state = RSEED )
38	train_data = observation . train train_data = train_data . set_index ( [ 'id' , 'timestamp' ] ) . sort_index ( ) train_data
110	cate_group . groupby ( "year" ) [ 'FOODS' , 'HOBBIES' , 'HOUSEHOLD' ] . sum ( ) . plot ( ) plt . title ( "Sales volume per year" ) plt . ylabel ( "Sales volume" ) ;
1131	valid_loader = data . DataLoader ( dataset_val , batch_size = 32 , shuffle = False , num_workers = 4 , pin_memory = True ) test_loader = data . DataLoader ( dataset_test , batch_size = 32 , shuffle = False , num_workers = 4 , pin_memory = True )
828	image_id = 'c14c1e300' image = cv2 . imread ( os . path . join ( BASE_DIR , 'train' , f'{image_id}.jpg' ) , cv2 . IMREAD_COLOR ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )
1164	if len ( VALIDATION_MISMATCHES_IDS ) > 0 : dataset = load_dataset ( TRAINING_FILENAMES , labeled = True ) dataset = dataset . filter ( lambda image , label , idnum : tf . reduce_sum ( tf . cast ( idnum == VALIDATION_MISMATCHES_IDS , tf . int32 ) ) > 0 ) dataset = dataset . map ( lambda image , label , idnum : [ image , label ] ) imgs = next ( iter ( dataset . batch ( len ( VALIDATION_MISMATCHES_IDS ) ) ) ) display_batch_of_images ( imgs )
688	bbox_draw . rectangle ( ( x , y , x + w , y + h ) , fill = ( 255 , 255 , 255 , 0 ) , outline = ( 255 , 0 , 0 , 255 ) ) imsource = Image . alpha_composite ( imsource , bbox_canvas ) imsource = imsource . convert ( "RGB" ) return np . asarray ( imsource )
808	X_train = features [ train_indices ] X_valid = features [ valid_indices ] y_train = labels [ train_indices ] y_valid = labels [ valid_indices ] start = timer ( )
1317	config = tf . ConfigProto ( device_count = { 'GPU' : 1 , 'CPU' : 2 } ) config . gpu_options . allow_growth = True config . gpu_options . per_process_gpu_memory_fraction = 0.6 sess = tf . Session ( config = config ) K . set_session ( sess )
1208	batch_size = 64 num_classes = 14 epochs = 30 val_split = 0.1 save_dir = os . path . join ( os . getcwd ( ) , 'models' ) model_name = 'keras_cnn_model.h5'
1282	train_label = train_df [ 'label' ] test_indices = test_df [ 'id' ] train_df = train_df . drop ( [ 'label' ] , axis = 1 ) test_df = test_df . drop ( [ 'id' ] , axis = 1 ) train_x = train_df . values train_y = train_label . values test_x = test_df . values print ( "shape of train_x :" , train_x . shape ) print ( "shape of train_y :" , train_y . shape ) print ( "shape of test_x :" , test_x . shape )
1388	target [ 'boxes' ] = torch . stack ( tuple ( map ( torch . tensor , zip ( * boxes ) ) ) ) . permute ( 1 , 0 ) else : sample = { 'image' : image , 'bbox' : target [ 'bbox' ] , 'cls' : target [ 'cls' ] } image = self . transforms ( ** sample ) [ 'image' ] return image , target def __len__ ( self ) -> int : return len ( self . image_ids )
58	sz = 128 bs = 32 tfms = get_transforms ( do_flip = False , flip_vert = False , max_rotate = 20 , max_zoom = 1.5 , max_lighting = 0.2 )
1301	test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv' ) test [ 'content' ] = test [ 'content' ] . apply ( lambda x : text_process ( x ) ) x_test = regular_encode ( test . content . values , tokenizer , maxlen = MAX_LEN ) lang_tag_test = np . array ( [ lang_embed ( row [ 'lang' ] , 'orig' ) for _ , row in test . iterrows ( ) ] )
1144	for key , value in dict . items ( ) : if key in [ "China" , 'Rest of China w/o Hubei' ] : pass else : growth_rate_over_time ( exp , value , 'Confirmed' , key + ' - Growth Rate Percentage for ' , )
1806	train = pd . read_csv ( '../input/train.csv' ) train = train . append ( pd . read_csv ( '../input/train_v2.csv' ) ) train . index = range ( len ( train ) ) test = pd . read_csv ( '../input/sample_submission_v2.csv' )
106	train_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv" ) calendar_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/calendar.csv" ) price_df = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv" ) sample = pd . read_csv ( "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv" )
882	train_set = lgb . Dataset ( train , label = train_labels ) hyperparameters = dict ( ** random_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS ) print ( 'The cross validation score on the full dataset for Random Search= {:.5f} with std: {:.5f}.' . format ( cv_results [ 'auc-mean' ] [ - 1 ] , cv_results [ 'auc-stdv' ] [ - 1 ] ) ) print ( 'Number of estimators = {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
1474	n_iter = 1000 start = datetime . datetime . now ( ) for i in range ( n_iter ) : batch_cutmix ( images , labels , PROBABILITY = 1.0 ) end = datetime . datetime . now ( ) timing = ( end - start ) . total_seconds ( ) / n_iter print ( f"batch_cutmix: {timing}" )
1105	labels_breed = pd . read_csv ( '../input/breed_labels.csv' ) labels_state = pd . read_csv ( '../input/color_labels.csv' ) labels_color = pd . read_csv ( '../input/state_labels.csv' )
151	plt . figure ( figsize = ( 8 , 8 ) ) data_to_plot2 [ "cat_DL" ] . value_counts ( ) . plot ( kind = 'pie' , autopct = '%1.0f%%' ) plt . title ( 'Does users download the app ?' , fontsize = 15 ) plt . ytitle = ''
137	clear_output ( wait = True ) print ( 'Done!' )
613	import random import warnings warnings . filterwarnings ( 'ignore' ) import matplotlib . pyplot as plt import seaborn as sns from sklearn . preprocessing import LabelEncoder from sklearn . preprocessing import RobustScaler , MinMaxScaler from tqdm import tqdm from sklearn . model_selection import train_test_split from sklearn . model_selection import KFold from sklearn . model_selection import StratifiedKFold from sklearn . model_selection import GridSearchCV from sklearn . utils import class_weight from sklearn . metrics import accuracy_score , make_scorer from sklearn . metrics import roc_curve , auc , accuracy_score , cohen_kappa_score from sklearn . metrics import mean_squared_error , f1_score , confusion_matrix
850	fld_dtype = fld . dtype if isinstance ( fld_dtype , pd . core . dtypes . dtypes . DatetimeTZDtype ) : fld_dtype = np . datetime64
1021	cols_seen . append ( key ) for x in value : if x == key : next else :
1757	r_applications_cc_balance = ft . Relationship ( es [ 'applications' ] [ 'SK_ID_CURR' ] , es [ 'cc_balance' ] [ 'SK_ID_CURR' ] ) es = es . add_relationship ( r_applications_cc_balance )
760	if heads_only : df = df . loc [ df [ 'parentesco1' ] == 1 ] . copy ( ) plt . figure ( figsize = ( 8 , 6 ) ) df [ col ] . value_counts ( ) . sort_index ( ) . plot . bar ( color = 'blue' , edgecolor = 'k' , linewidth = 2 ) plt . xlabel ( f'{col}' ) ; plt . title ( f'{col} Value Counts' ) ; plt . ylabel ( 'Count' ) plt . show ( ) ;
552	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
1220	X_train = train . drop ( [ 'isFraud' , 'TransactionDT' ] , axis = 1 ) X_test = test . drop ( [ 'TransactionDT' ] , axis = 1 ) del train , test X_train = X_train . fillna ( - 999 ) X_test = X_test . fillna ( - 999 )
349	threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style
1490	input_paths = tf . io . gfile . glob ( input_pattern ) final_dict = { } for input_path in input_paths : final_dict . update ( read_candidates_from_one_split ( input_path ) ) return final_dict def get_best_indexes ( logits , n_best_size , token_map = None ) :
1054	gc . enable ( ) del model , train_features , valid_features gc . collect ( )
1418	import numpy as np import pandas as pd from tqdm import tqdm import copy import multiprocessing import nltk import re import gensim . models . word2vec as w2v import matplotlib . pyplot as plt
1503	image = tf . image . random_flip_left_right ( image ) return image , label def get_validation_dataset ( dataset ) : dataset = dataset . batch ( BATCH_SIZE ) dataset = dataset . cache ( ) dataset = dataset . prefetch ( AUTO ) return dataset def get_test_dataset ( ordered = False ) : dataset = load_dataset ( TEST_FILENAMES , labeled = False , ordered = ordered ) dataset = dataset . batch ( BATCH_SIZE ) dataset = dataset . prefetch ( AUTO ) return dataset def count_data_items ( filenames ) :
1548	df [ 'date' ] = df [ 'pickup_datetime' ] . dt . date df [ 'hour' ] = df [ 'pickup_datetime' ] . dt . hour df = pd . merge ( left = df , right = weather , on = 'date' , how = 'left' )
640	positive_train [ 'temp_list' ] = positive_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) positive_train [ 'temp_list' ] = positive_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) positive_top = Counter ( [ item for sublist in positive_train [ 'temp_list' ] for item in sublist ] ) positive_temp = pd . DataFrame ( positive_top . most_common ( 20 ) ) positive_temp . columns = [ 'Common_words' , 'count' ] positive_temp . style . background_gradient ( cmap = 'Blues' )
222	lsvr = LinearSVR ( C = 0.05 , max_iter = 1000 ) . fit ( dfe , target_fe ) model = SelectFromModel ( lsvr , prefit = True ) X_new = model . transform ( dfe ) X_selected_df = pd . DataFrame ( X_new , columns = [ dfe . columns [ i ] for i in range ( len ( dfe . columns ) ) if model . get_support ( ) [ i ] ] ) X_selected_df . shape
350	lsvr = LinearSVR ( C = 0.05 , max_iter = 1000 ) . fit ( dfe , target_fe ) model = SelectFromModel ( lsvr , prefit = True ) X_new = model . transform ( dfe ) X_selected_df = pd . DataFrame ( X_new , columns = [ dfe . columns [ i ] for i in range ( len ( dfe . columns ) ) if model . get_support ( ) [ i ] ] ) X_selected_df . shape
632	df_population = pd . read_csv ( "../input/countries-of-the-world/countries of the world.csv" ) df_population
787	plt . figure ( figsize = ( 8 , 6 ) ) plt . plot ( list ( range ( len ( df ) ) ) , df [ 'cumulative_importance' ] , 'b-' ) plt . xlabel ( 'Number of Features' , size = 16 ) ; plt . ylabel ( 'Cumulative Importance' , size = 16 ) ; plt . title ( 'Cumulative Feature Importance' , size = 18 ) ;
71	imgs = np . array ( imgs ) msks = np . array ( msks ) return imgs , msks def on_epoch_end ( self ) : if self . shuffle : random . shuffle ( self . filenames ) def __len__ ( self ) : if self . predict :
542	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
129	sample = random . choice ( filenames ) image = load_img ( "../input/train/train/" + sample ) plt . imshow ( image )
510	if len ( img_batch ) > 0 : process_batch ( image_id_batch , img_batch , row_id , target , model , inv_tuple_map ) image_id_batch , img_batch = [ ] , [ ] sub_fn = 'submission.csv' sub = pd . DataFrame ( { 'row_id' : row_id , 'target' : target } ) sub . to_csv ( sub_fn , index = False ) print ( f'Done wrote to {sub_fn}' ) main ( )
425	bold ( '**MANUFACTURING REALLY BUCKED THE GENERAL TREND**' ) temp_df = train . groupby ( [ 'timestamp' , "primary_use" ] ) . meter_reading . sum ( ) . reset_index ( ) ax = sns . FacetGrid ( temp_df , col = "primary_use" , col_wrap = 2 , height = 4 , aspect = 2 , sharey = False ) ax . map ( sns . lineplot , 'timestamp' , 'meter_reading' , color = "teal" ) plt . subplots_adjust ( hspace = 0.45 ) plt . show ( )
1673	if parsed [ pid ] [ 'label' ] == 1 : parsed [ pid ] [ 'boxes' ] . append ( extract_box ( row ) ) return parsed df = pd . read_csv ( '../input/stage_1_train_labels.csv' ) patient_class = pd . read_csv ( '../input/stage_1_detailed_class_info.csv' , index_col = 0 ) parsed = parse_data ( df ) patientId = df [ 'patientId' ] [ 0 ] print ( 'Just a checking that everything is working fine...' ) print ( parsed [ patientId ] ) print ( patient_class . loc [ patientId ] ) def draw ( data ) :
638	def generate_word_cloud ( text ) : wordcloud = WordCloud ( width = 3000 , height = 2000 , background_color = 'black' ) . generate ( str ( text ) ) fig = plt . figure ( figsize = ( 40 , 30 ) , facecolor = 'k' , edgecolor = 'k' ) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . tight_layout ( pad = 0 ) plt . show ( )
1715	def seed_everything ( seed = 1029 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( )
1737	sample_df = pd . read_csv ( SAMPLE ) sample_list = list ( sample_df . Id ) pred_dic = dict ( ( key , value ) for ( key , value ) in zip ( learner . data . test_ds . fnames , pred_list ) ) pred_list_cor = [ pred_dic [ id ] for id in sample_list ] df = pd . DataFrame ( { 'Id' : sample_list , 'Predicted' : pred_list_cor } ) df . to_csv ( 'protein_classification.csv' , header = True , index = False )
232	all_data . loc [ all_data [ 'Date' ] >= '2020-03-19' , 'ConfirmedCases' ] = np . nan all_data . loc [ all_data [ 'Date' ] >= '2020-03-19' , 'Fatalities' ] = np . nan all_data [ 'Date' ] = pd . to_datetime ( all_data [ 'Date' ] )
661	public_df = test . query ( "seq_length == 107" ) . copy ( ) private_df = test . query ( "seq_length == 130" ) . copy ( ) public_inputs , public_adj = preprocess_inputs ( public_df ) private_inputs , private_adj = preprocess_inputs ( private_df ) public_inputs = torch . tensor ( public_inputs , dtype = torch . long ) private_inputs = torch . tensor ( private_inputs , dtype = torch . long ) public_adj = torch . tensor ( public_adj , dtype = torch . long ) private_adj = torch . tensor ( private_adj , dtype = torch . long )
459	start_time = timer ( None ) random_search . fit ( X , Y ) timer ( start_time )
769	heads [ 'roof' ] = np . argmax ( np . array ( heads [ [ 'etecho1' , 'etecho2' , 'etecho3' ] ] ) , axis = 1 ) heads = heads . drop ( columns = [ 'etecho1' , 'etecho2' , 'etecho3' ] )
538	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
1697	if image_list == [ ] : return [ ] return image_list
980	app_types [ 'REGION_RATING_CLIENT' ] = ft . variable_types . Ordinal app_types [ 'REGION_RATING_CLIENT_W_CITY' ] = ft . variable_types . Ordinal app_test_types = app_types . copy ( ) del app_test_types [ 'TARGET' ]
179	mean_price_2 = pd . DataFrame ( group . price . mean ( ) ) mean_price_2 . reset_index ( level = 0 , inplace = True ) plt . figure ( figsize = ( 12 , 7 ) ) sns . kdeplot ( mean_price_2 . price , shade = True ) plt . title ( 'Mean price by category distribution' , fontsize = 20 ) plt . xlabel ( 'Mean price of each category' , fontsize = 16 )
952	test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
1550	week_year_avg = train_info . groupby ( 'week_of_year' ) . trip_duration . mean ( ) week_year_avg = week_year_avg . reset_index ( ) week_year_avg . columns = [ 'week_of_year' , 'week_of_year_avg' ] df = pd . merge ( left = df , right = week_year_avg , on = 'week_of_year' , how = 'left' )
1682	def __init__ ( self ) : self . coef_ = 0 def _kappa_loss ( self , coef , X , y ) :
1557	external_a = ndimage . binary_dilation ( marker_internal , iterations = 10 ) external_b = ndimage . binary_dilation ( marker_internal , iterations = 55 ) marker_external = external_b ^ external_a
275	Dropout_new = 0.15 n_split = 5 lr = 3e-5
34	train = pd . read_csv ( "../input/siim-isic-melanoma-classification/train.csv" ) test = pd . read_csv ( "../input/siim-isic-melanoma-classification/test.csv" ) print ( "{} images in train set." . format ( train . shape [ 0 ] ) ) print ( "{} images in test set." . format ( test . shape [ 0 ] ) )
1378	labels4 = [ ] s = self . img_size xc , yc = [ int ( random . uniform ( s * 0.5 , s * 1.5 ) ) for _ in range ( 2 ) ] indices = [ index ] + [ random . randint ( 0 , len ( self . labels ) - 1 ) for _ in range ( 3 ) ] for i , index in enumerate ( indices ) :
229	ensemble_final = ensembles [ 0 ] . copy ( ) ensemble_final [ target_cols ] = 0 for ensemble in ensembles : ensemble_final [ target_cols ] += ensemble [ target_cols ] . values / len ( ensembles ) ensemble_final
78	sub = pd . DataFrame . from_dict ( submission_dict , orient = 'index' ) sub . index . names = [ 'patientId' ] sub . columns = [ 'PredictionString' ] sub . to_csv ( 'submission.csv' )
1148	image_size = ( 128 , 128 ) input_size = image_size + ( 3 , ) print ( image_size , input_size )
1791	train_flattened [ 'year' ] = train_flattened . date . dt . year train_flattened [ 'month' ] = train_flattened . date . dt . month train_flattened [ 'day' ] = train_flattened . date . dt . day
11	sta [ length_sta : ] = sta [ length_sta : ] - sta [ : - length_sta ] sta /= length_sta lta [ length_lta : ] = lta [ length_lta : ] - lta [ : - length_lta ] lta /= length_lta
388	def compute_histogram ( img , hist_size = 100 ) : hist = cv2 . calcHist ( [ img ] , [ 0 ] , mask = None , histSize = [ hist_size ] , ranges = ( 0 , 255 ) ) hist = cv2 . normalize ( hist , dst = hist ) return hist
641	negative_train [ 'temp_list' ] = negative_train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) negative_train [ 'temp_list' ] = negative_train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) ) negative_top = Counter ( [ item for sublist in negative_train [ 'temp_list' ] for item in sublist ] ) negative_temp = pd . DataFrame ( negative_top . most_common ( 20 ) ) negative_temp . columns = [ 'Common_words' , 'count' ] negative_temp . style . background_gradient ( cmap = 'Blues' )
72	def iou_loss ( y_true , y_pred ) : y_true = tf . reshape ( y_true , [ - 1 ] ) y_pred = tf . reshape ( y_pred , [ - 1 ] ) intersection = tf . reduce_sum ( y_true * y_pred ) score = ( intersection + 1. ) / ( tf . reduce_sum ( y_true ) + tf . reduce_sum ( y_pred ) - intersection + 1. ) return 1 - score
313	for col in categorical_columns : if col in train0 . columns : le = LabelEncoder ( ) le . fit ( list ( train0 [ col ] . astype ( str ) . values ) ) train0 [ col ] = le . transform ( list ( train0 [ col ] . astype ( str ) . values ) )
406	df = pd . DataFrame ( index = df_train . index ) df [ "ID_code" ] = df_train [ "ID_code" ] df [ 'new' + var ] = pd . cut ( df_train [ var ] , bins = bin_values , labels = labels ) df . set_index ( 'ID_code' )
955	es = es . entity_from_dataframe ( entity_id = 'app' , dataframe = app , index = 'SK_ID_CURR' ) es = es . entity_from_dataframe ( entity_id = 'bureau' , dataframe = bureau , index = 'SK_ID_BUREAU' ) es = es . entity_from_dataframe ( entity_id = 'previous' , dataframe = previous , index = 'SK_ID_PREV' )
1076	train [ 'len' ] = train [ 'comment_text' ] . str . len ( ) print ( 'Average comment length: %d' % train [ 'len' ] . mean ( ) ) print ( 'Median comment length: %d' % train [ 'len' ] . quantile ( .5 ) ) print ( '90th percentile comment length: %d' % train [ 'len' ] . quantile ( .9 ) )
341	new_names = [ 'conf_1' , 'x_1' , 'y_1' , 'width_1' , 'height_1' , 'conf_2' , 'x_2' , 'y_2' , 'width_2' , 'height_2' ] df_preds . columns = new_names
601	img2 = cv2 . bitwise_and ( image [ 400 : 600 , 200 : 400 ] , image [ 400 : 600 , 200 : 400 ] , mask = total_mask [ 400 : 600 , 200 : 400 ] . astype ( np . uint8 ) ) plt . figure ( figsize = ( 8 , 8 ) ) total_mask [ total_mask > 1 ] = 0 plt . title ( 'Masks over image' ) plt . imshow ( img2 ) plt . show ( )
1649	def calc_extra ( df_timeseries ) : gp = df_timeseries . groupby ( 'object_id' ) dfe = ( gp [ 'mjd' ] . max ( ) - gp [ 'mjd' ] . min ( ) ) . rename ( 'dmjd' ) . reset_index ( ) dfe [ 'dmjd' ] = dfe [ 'dmjd' ] / 1000 dfe [ 'std_flux' ] = gp . flux . std ( ) . reset_index ( ) . flux / 1000 return dfe
295	def binary_target ( x ) : if x != 0 : return 1 else : return x df_train [ 'binary_target' ] = df_train [ 'diagnosis' ] . apply ( binary_target )
1248	train = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/train.json' , lines = True ) test = pd . read_json ( '/kaggle/input/stanford-covid-vaccine/test.json' , lines = True ) sample_df = pd . read_csv ( '/kaggle/input/stanford-covid-vaccine/sample_submission.csv' )
566	from keras import backend as K def logloss ( y_true , y_pred ) : eps = K . epsilon ( ) class_weights = np . array ( [ 2. , 1. , 1. , 1. , 1. , 1. ] ) y_pred = K . clip ( y_pred , eps , 1.0 - eps )
1075	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) result = test [ [ 'id' ] ] . copy ( ) print ( train . head ( 3 ) )
46	grouped = pd . DataFrame ( dataset [ var_name_official ] . groupby ( dataset [ var_name_official ] . dt . month ) . count ( ) ) . rename ( columns = { var_name_official : 'Count' } )
1384	image = np . array ( image ) image = image / 255.0 image = image . astype ( np . float32 ) if self . mode != "test" : records = self . df [ self . df [ 'image_id' ] == image_id ] area = records [ "area" ] . values area = torch . as_tensor ( area , dtype = torch . float32 ) boxes = records [ [ "x" , "y" , "x2" , "y2" ] ] . values
234	data = all_data . copy ( ) features = [ 'Id' , 'ForecastId' , 'Country_Region' , 'Province_State' , 'ConfirmedCases' , 'Fatalities' , 'Day_num' ] data = data [ features ]
1553	day_week_avg = train_info . groupby ( 'day_week' ) . trip_duration . mean ( ) day_week_avg = day_week_avg . reset_index ( ) day_week_avg . columns = [ 'day_week' , 'day_week_avg' ] df = pd . merge ( left = df , right = day_week_avg , on = 'day_week' , how = 'left' )
1499	if ckpt_manager . latest_checkpoint : ckpt . restore ( ckpt_manager . latest_checkpoint ) last_epoch = int ( ckpt_manager . latest_checkpoint . split ( "-" ) [ - 1 ] ) print ( f'Latest BertNQ checkpoint restored -- Model trained for {last_epoch} epochs' ) else : print ( 'Checkpoint not found. Train BertNQ from scratch' ) last_epoch = 0
727	cheap = df [ df . price < 10 ] . category_name . value_counts ( ) . map ( lambda x : '{:.2f}%' . format ( x / df . shape [ 0 ] * 100 ) ) print ( 'Categories of items < 10 \u20BD (top 10)' ) cheap . head ( 10 )
887	original_features = list ( set ( previous_columns ) & set ( bureau_columns ) ) print ( 'There are %d original features.' % len ( original_features ) ) print ( 'There are %d bureau and bureau balance features.' % len ( bureau_features ) ) print ( 'There are %d previous Home Credit loan features.' % len ( previous_features ) )
3	weather_df = weather_df . set_index ( [ 'site_id' , 'day' , 'month' ] ) air_temperature_filler = pd . DataFrame ( weather_df . groupby ( [ 'site_id' , 'day' , 'month' ] ) [ 'air_temperature' ] . mean ( ) , columns = [ "air_temperature" ] ) weather_df . update ( air_temperature_filler , overwrite = False )
421	bold ( '**SUNDAYS HAVE THE LOWEST READINGS**' ) plt . rcParams [ 'figure.figsize' ] = ( 18 , 10 ) ax = sns . boxplot ( data = train , x = 'weekday_name' , y = 'meter_reading' , color = 'teal' , boxprops = dict ( alpha = .3 ) ) ax . set_ylabel ( 'Log(Meter Reading)' , fontsize = 20 ) ax . set_xlabel ( 'weekdays' , fontsize = 20 ) plt . show ( )
310	X = df_train_padded X_test = df_test_padded y_toxic = df_train [ 'toxic' ] y_severe_toxic = df_train [ 'severe_toxic' ] y_obscene = df_train [ 'obscene' ] y_threat = df_train [ 'threat' ] y_insult = df_train [ 'insult' ] y_identity_hate = df_train [ 'identity_hate' ]
1004	feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'mean' , 'max' , 'min' , 'trend' , 'mode' , 'count' , 'sum' , 'percent_true' , NormalizedModeCount , MostRecent , LongestSeq ] , trans_primitives = [ 'diff' , 'cum_sum' , 'cum_mean' , 'percentile' ] , where_primitives = [ 'mean' , 'sum' ] , seed_features = [ late_payment , past_due ] , max_depth = 2 , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
1705	for program in best_candidates . values ( ) : if is_solution ( program , task ) : return program
932	eval_results = objective ( hyperparameters , i ) results . loc [ i , : ] = eval_results i += 1
458	params = { 'min_child_weight' : [ 1 , 5 , 10 ] , 'gamma' : [ 0.5 , 1 , 1.5 , 2 , 5 ] , 'subsample' : [ 0.6 , 0.8 , 1.0 ] , 'colsample_bytree' : [ 0.6 , 0.8 , 1.0 ] , 'max_depth' : [ 3 , 5 , 7 , 10 ] , 'learning_rate' : [ 0.01 , 0.02 , 0.05 ] }
1634	h1_col = [ s for s in col if "h1_" in s ] d1_col = [ s for s in col if "d1_" in s ] h1d1 = list ( pd . Series ( h1_col ) . str . replace ( "h1_" , "" ) ) for c in h1d1 : All_df [ 'diff_' + c ] = All_df [ 'd1_' + c ] - All_df [ 'h1_' + c ]
1624	import pandas as pd , numpy as np from matplotlib import pyplot as plt import scipy . stats as stats pd . options . display . max_columns = 50
428	ax . bar ( bar_dir , rosedata [ c2 ] . values , width = bar_width , bottom = rosedata . cumsum ( axis = 1 ) [ c1 ] . values , color = palette [ n + 1 ] , edgecolor = 'none' , label = c2 , linewidth = 0 ) leg = ax . legend ( loc = ( 0.75 , 0.95 ) , ncol = 2 ) xtl = ax . set_xticklabels ( [ 'N' , 'NE' , 'E' , 'SE' , 'S' , 'SW' , 'W' , 'NW' ] ) return fig
1372	submission = pd . DataFrame ( predictions , columns = le2 . inverse_transform ( np . linspace ( 0 , 38 , 39 , dtype = 'int16' ) ) , index = test . index ) submission . to_csv ( 'LGBM_final.csv' , index_label = 'Id' )
172	model_json = model . to_json ( ) with open ( "model.json" , "w" ) as json_file : json_file . write ( model_json )
883	hyperparameters = dict ( ** bayes_results . loc [ 0 , 'hyperparameters' ] ) del hyperparameters [ 'n_estimators' ] cv_results = lgb . cv ( hyperparameters , train_set , num_boost_round = 10000 , early_stopping_rounds = 100 , metrics = 'auc' , nfold = N_FOLDS ) print ( 'The cross validation score on the full dataset for Bayesian optimization = {:.5f} with std: {:.5f}.' . format ( cv_results [ 'auc-mean' ] [ - 1 ] , cv_results [ 'auc-stdv' ] [ - 1 ] ) ) print ( 'Number of estimators = {}.' . format ( len ( cv_results [ 'auc-mean' ] ) ) )
111	out_df = pd . merge ( out_df , dept_day_lag , left_on = "dept_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , dept_day_year_lag , left_on = "dept_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , month_dept_lag , left_on = "dept_id" , right_index = True , how = "left" )
1803	img_cor_2_world_cor ( ) img_cor_points = img_cor_points . astype ( int ) img = draw_points ( img , img_cor_points ) img = draw_line ( img , img_cor_points ) img = Image . fromarray ( img ) plt . imshow ( img ) plt . show ( )
1347	start_mem = df . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( 'Memory usage of dataframe is {:.2f} MB' . format ( start_mem ) ) for col in df . columns : if is_datetime ( df [ col ] ) or is_categorical_dtype ( df [ col ] ) :
1393	for col in CATEGORICAL_COLS : df [ col ] = df [ col ] . astype ( 'cateogry' ) return df
1380	x = xy [ : , [ 0 , 2 , 4 , 6 ] ] y = xy [ : , [ 1 , 3 , 5 , 7 ] ] xy = np . concatenate ( ( x . min ( 1 ) , y . min ( 1 ) , x . max ( 1 ) , y . max ( 1 ) ) ) . reshape ( 4 , n ) . T
707	auc = roc_auc_score ( train [ 'target' ] , oof_QDA ) print ( 'QDA scores CV =' , round ( auc , 8 ) ) sub_QDA = pd . read_csv ( '../input/sample_submission.csv' ) sub_QDA [ 'target' ] = preds_QDA sub_QDA . to_csv ( 'submission_QDA.csv' , index = False ) oof_preds_QDA = train [ [ 'id' , 'target' ] ] . copy ( ) oof_preds_QDA [ 'target' ] = oof_QDA oof_preds_QDA . to_csv ( 'oof_preds_QDA.csv' , index = False )
1020	cols_to_remove = [ ] cols_seen = [ ] cols_to_remove_pair = [ ]
1710	from keras . models import Sequential from keras . layers import Dense , Dropout , Activation from keras . utils . np_utils import to_categorical
121	def check_current_coverage ( num = 50 ) : vocab = count_words_from ( train_df [ "comment_text" ] ) coverage = check_coverage_for ( vocab ) return coverage [ : num ]
271	from functools import partial import scipy as sp import time import datetime import gc import warnings warnings . simplefilter ( 'ignore' ) warnings . filterwarnings ( 'ignore' )
987	previous = previous . drop ( columns = [ 'DAYS_DECISION' , 'DAYS_FIRST_DRAWING' , 'DAYS_FIRST_DUE' , 'DAYS_LAST_DUE_1ST_VERSION' , 'DAYS_LAST_DUE' , 'DAYS_TERMINATION' ] ) plt . figure ( figsize = ( 8 , 6 ) ) example_client = previous [ previous [ 'SK_ID_CURR' ] == 100007 ] plt . plot ( example_client [ 'previous_decision_date' ] , example_client [ 'AMT_CREDIT' ] , 'ro' ) plt . title ( 'Client 100007 Previous Loan Amounts' ) ; plt . xlabel ( 'Date' ) ; plt . ylabel ( 'Credit Amount' ) ;
591	combined_aug = Compose ( transforms = [ GaussianTargetNoise ( p = 1.0 , gaus_std = 0.3 ) , TemporalFlip ( p = 1.0 ) ] )
1467	temp_test_df . fillna ( 0 , inplace = True ) temp_test_df [ col_freq ] = temp_test_df [ col_freq ] . astype ( np . int32 ) if result_train_df . shape [ 0 ] == 0 : result_train_df = temp_train_df result_test_df = temp_test_df else : result_train_df = pd . concat ( [ result_train_df , temp_train_df ] , axis = 1 ) result_test_df = pd . concat ( [ result_test_df , temp_test_df ] , axis = 1 ) return result_train_df , result_test_df
554	def auc ( y_true , y_pred ) : auc = tf . metrics . auc ( y_true , y_pred ) [ 1 ] K . get_session ( ) . run ( tf . local_variables_initializer ( ) ) return auc
1591	for i in range ( 8 ) : model . load_weights ( f'./model_sn{(i+1)*5}.h5' ) results = model . evaluate ( test_inputs , test_labels , batch_size = 8 , verbose = 0 ) print ( f'SN={(i+1)*0.5}: test_loss = {results}' )
548	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
1623	from sklearn . linear_model import LogisticRegression lr = LogisticRegression ( penalty = 'l2' , C = 1000000 , class_weight = "balanced" ) lr . fit ( X_train , y_train ) y_pred = lr . predict ( X_test ) print ( classification_report ( y_pred , y_test ) )
1544	if inplace : data_frame = input_data else : data_frame = input_data . copy ( )
842	corrs = data . corr ( ) corrs [ 'fare_amount' ] . plot . bar ( color = 'b' ) ; plt . title ( 'Correlation with Fare Amount' ) ;
675	param_search = list_params [ im ] tscv = TimeSeriesSplit ( n_splits = 5 ) gsearch = GridSearchCV ( estimator = model [ 1 ] , cv = tscv , param_grid = param_search , verbose = 1 , return_train_score = True , n_jobs = - 1 , scoring = 'neg_mean_squared_error' ) gsearch . fit ( X , y )
1636	test [ 'band' ] = np . where ( test [ 'click_hour' ] <= 6 , 0 , \ np . where ( test [ 'click_hour' ] <= 11 , 1 , \ np . where ( test [ 'click_hour' ] <= 15 , 2 , 3 ) ) )
434	import pandas as pd import numpy as np from scipy import stats
1272	checked_inx = [ ] for pair in checked_pairs : _ , inx = self . check_pairs ( current_object_pairs , pair , return_inx = True ) checked_inx . append ( inx [ 0 ] [ 0 ] ) unchecked_pairs = np . delete ( current_object_pairs , checked_inx , axis = 0 )
1754	r_applications_installment = ft . Relationship ( es [ 'applications' ] [ 'SK_ID_CURR' ] , es [ 'installments' ] [ 'SK_ID_CURR' ] ) es = es . add_relationship ( r_applications_installment )
158	print ( 'Original image shape: {}' . format ( im . shape ) ) from skimage . color import rgb2gray im_gray = rgb2gray ( im ) print ( 'New image shape: {}' . format ( im_gray . shape ) )
327	if parsed [ pid ] [ 'label' ] == 1 : parsed [ pid ] [ 'boxes' ] . append ( extract_box ( row ) ) return parsed
1298	yhat = [ 1 if y > 0.5 else 0 for y in yhat ] test_df [ 'label' ] = yhat label_map = dict ( ( v , k ) for k , v in train_generator . class_indices . items ( ) ) test_df [ 'label' ] = test_df [ 'label' ] . replace ( label_map ) test_df [ 'label' ] = test_df [ 'label' ] . replace ( { 'dog' : 1 , 'cat' : 0 } ) test_df . to_csv ( 'submission.csv' , index = False )
545	from sklearn . feature_selection import SelectPercentile , f_classif selector = SelectPercentile ( f_classif , percentile = 100 ) selector . fit ( model_predictions , y ) p_scores = ( selector . pvalues_ ) F_scores = ( selector . scores_ ) df_significance = pd . DataFrame ( { "Feature" : model_predictions . columns , "p_value" : p_scores , "F_score" : F_scores } ) df_significance
322	Voting_Reg = VotingRegressor ( estimators = [ ( 'lin' , linreg ) , ( 'ridge' , ridge ) , ( 'sgd' , sgd ) ] ) Voting_Reg . fit ( train , target ) acc_model ( 14 , Voting_Reg , train , test )
805	hyperparameters [ 'subsample' ] = subsample hyperparameters [ 'subsample_freq' ] = subsample_freq hyperparameters [ 'boosting_type' ] = boosting_type
103	X = train . Image . values del train [ 'Image' ] Y = train . values
1713	model = Sequential ( ) model . add ( Dense ( 512 , input_dim = 192 , init = 'uniform' , activation = 'relu' ) ) model . add ( Dropout ( 0.3 ) ) model . add ( Dense ( 256 , activation = 'sigmoid' ) ) model . add ( Dropout ( 0.3 ) ) model . add ( Dense ( 99 , activation = 'softmax' ) )
758	all_equal = train . groupby ( 'idhogar' ) [ 'Target' ] . apply ( lambda x : x . nunique ( ) == 1 ) not_equal = all_equal [ all_equal != True ] print ( 'There are {} households where the family members do not all have the same target.' . format ( len ( not_equal ) ) )
1482	if not os . path . isdir ( INPUT_DIR ) : IS_KAGGLE = False INPUT_DIR = "./" NQ_DIR = "./" MY_OWN_NQ_DIR = "./"
1788	axes = plt . axes ( [ 0.55 , 0.6 , 0.3 , 0.2 ] ) plt . title ( 'Peak frequency' ) plt . plot ( freqs [ : 8 ] , power [ : 8 ] ) plt . setp ( axes , yticks = [ ] )
590	class GaussianTargetNoise ( object ) : def __init__ ( self , p : float = 0.5 , gaus_std : float = 1.0 , ) : self . p = p self . gaus_std = gaus_std def __call__ ( self , x_arr , y_arr , atten_arr ) : if np . random . binomial ( n = 1 , p = self . p ) : y_arr = y_arr + np . random . normal ( scale = self . gaus_std , size = y_arr . shape ) return x_arr , y_arr , atten_arr
61	def get_sex ( x ) : x = str ( x ) if x . find ( 'Male' ) >= 0 : return 'male' if x . find ( 'Female' ) >= 0 : return 'female' return 'unknown' def get_neutered ( x ) : x = str ( x ) if x . find ( 'Spayed' ) >= 0 : return 'neutered' if x . find ( 'Neutered' ) >= 0 : return 'neutered' if x . find ( 'Intact' ) >= 0 : return 'intact' return 'unknown'
902	model . fit ( train_features , train_labels , eval_metric = 'auc' , eval_set = [ ( valid_features , valid_labels ) , ( train_features , train_labels ) ] , eval_names = [ 'valid' , 'train' ] , categorical_feature = cat_indices , early_stopping_rounds = 100 , verbose = 200 )
77	y , x , y2 , x2 = region . bbox height = y2 - y width = x2 - x
560	ax = plt . subplot ( 122 ) sns . barplot ( x = 'gain_score' , y = 'feature' , data = x . sort_values ( 'gain_score' , ascending = False ) , ax = ax ) ax . set_title ( 'Feature scores wrt gain importances' , fontweight = 'bold' , fontsize = 14 ) plt . tight_layout ( ) if return_df == True : return x . reset_index ( drop = True ) scores_df = plot_feature_imp_gain ( models = [ XGBGBDT , LGBGBDT ] , plot_all = True , return_df = True )
1664	import os import sys sys . path . append ( "../input/pystacknet/repository/h2oai-pystacknet-af571e0" ) import pystacknet
864	model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] baseline_auc = roc_auc_score ( test_labels , preds ) print ( 'The baseline model scores {:.5f} ROC AUC on the test set.' . format ( baseline_auc ) )
964	features_sample = pd . read_csv ( '../input/home-credit-default-risk-feature-tools/feature_matrix.csv' , nrows = 20000 ) features_sample = features_sample [ features_sample [ 'set' ] == 'train' ] features_sample . head ( )
1160	idx2 = K . dot ( m , tf . cast ( idx , dtype = 'float32' ) ) idx2 = K . cast ( idx2 , dtype = 'int32' ) idx2 = K . clip ( idx2 , - DIM // 2 + XDIM + 1 , DIM // 2 )
860	model . n_jobs = - 1 model . fit ( X_train [ features ] , y_train ) evaluate ( model , features , X_train , X_valid , y_train , y_valid )
732	img = cv2 . imread ( os . path . join ( dataset_path , img_name ) ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) kp , des = detector . detectAndCompute ( img , None ) return img , kp , des def draw_image_matches ( detector , img1_name , img2_name , nmatches = 10 ) :
1585	train_agg . fillna ( value = 0 , inplace = True ) test . fillna ( value = 0 , inplace = True ) print ( 'train shape:' , train_agg . shape , 'test shape:' , test . shape )
721	nom_cols = [ f'nom_{i}' for i in range ( 5 , 10 ) ] fig , ax = plt . subplots ( 5 , 1 , figsize = ( 22 , 17 ) ) for i , col in enumerate ( nom_cols ) : plt . subplot ( 5 , 1 , i + 1 ) sns . countplot ( raw_train [ col ] ) plt . show ( )
1031	import warnings warnings . filterwarnings ( 'ignore' ) plt . style . use ( 'fivethirtyeight' )
88	df_models = [ ] f2f_models = [ ] i = 0 while len ( df_models ) < kfolds : model = define_model ( ( 150 , 150 , 3 ) ) if i == 0 : model . summary ( )
414	import matplotlib . pyplot as plt import seaborn as sns sns . set_style ( "whitegrid" )
63	for col in contvar : sns . distplot ( train [ col ] ) plt . show ( )
501	fig , axarr = plt . subplots ( nrows = 4 , ncols = 4 , figsize = ( 10 , 10 ) ) i = 0 for row in range ( 4 ) : for col in range ( 4 ) : resized_img = cv2 . resize ( img [ i ] , ( 0 , 0 ) , fx = 0.1 , fy = 0.1 ) axarr [ row , col ] . imshow ( np . flipud ( resized_img ) , cmap = COLORMAP ) i += 1 print ( 'Done!' )
312	numerics = [ 'int8' , 'int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64' ] categorical_columns = [ ] features = train0 . columns . values . tolist ( ) for col in features : if train0 [ col ] . dtype in numerics : continue categorical_columns . append ( col )
1254	y_pred = model . predict_generator ( test_generator , workers = 2 , use_multiprocessing = True , verbose = 1 ) cat_test_df [ 'sirna' ] = y_pred . argmax ( axis = 1 ) output_df . append ( cat_test_df [ [ 'id_code' , 'sirna' ] ] )
293	b = a [ 1 ] . split ( '.' ) extracted_id = b [ 0 ] return extracted_id df_preds [ 'id' ] = df_preds [ 'file_names' ] . apply ( extract_id ) df_preds . head ( )
1140	shap . dependence_plot ( ( "returnsClosePrevRaw10_lag_3_mean" , "returnsClosePrevRaw10_lag_3_mean" ) , shap_interaction_values , X_interaction )
1264	model . load_weights ( 'model.h5' ) y_test = model . predict_generator ( test_gen , steps = len ( test_gen ) , verbose = 1 )
1285	model_members = [ ] check_points_path = [ ] for idx in range ( k_fold_split ) : check_points_path . append ( str ( idx ) + '_model.hdf5' ) for model_index , ( train_indices , valid_indices ) in enumerate ( kf . split ( train_x ) ) :
14	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train . target . values , bins = 200 ) plt . title ( 'Histogram target counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Target' ) plt . show ( )
119	fig = px . line ( train_df , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'SmokingStatus' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines+markers' )
25	train = pd . read_csv ( "../input/train.csv" ) test = pd . read_csv ( "../input/test.csv" ) print ( "{} observations and {} features in train set." . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( "{} observations and {} features in test set." . format ( test . shape [ 0 ] , test . shape [ 1 ] ) )
866	boosting_type = { 'boosting_type' : hp . choice ( 'boosting_type' , [ { 'boosting_type' : 'gbdt' , 'subsample' : hp . uniform ( 'subsample' , 0.5 , 1 ) } , { 'boosting_type' : 'dart' , 'subsample' : hp . uniform ( 'subsample' , 0.5 , 1 ) } , { 'boosting_type' : 'goss' , 'subsample' : 1.0 } ] ) }
1149	if model_touse == 'MobileNet' : preproc_func = mobilenet . preprocess_input elif model_touse == 'MobileNetV2' : preproc_func = mobilenet_v2 . preprocess_input elif model_touse == 'ResNet50' : preproc_func = preprocess_input train_steps_per_epoch = tr_df . shape [ 0 ] // batch_size
96	import matplotlib . pyplot as plt savePickleBZ ( 'before.pbz' , before ) savePickleBZ ( 'sets.pbz' , sets ) def showBefore ( before ) : before = np . array ( before , dtype = np . float ) before = before / ( before + np . transpose ( np . copy ( before ) ) + 1e-30 ) before *= 255 before = np . array ( before , dtype = np . uint8 ) plt . figure ( figsize = ( 16 , 16 ) ) plt . imshow ( before ) showBefore ( before )
1166	X_test = Parallel ( n_jobs = - 3 , verbose = 1 ) ( delayed ( processor . prepareSample ) ( test_root , sample_submission . iloc [ f , : ] , processor . createMel , CONFIG , TRAINING_CONFIG , test_mode = True , proc_mode = 'resize' , ) for f in range ( 100 ) ) X_test = np . array ( X_test ) print ( X_test . shape )
1579	for col1 in [ 'epared1' , 'epared2' , 'epared3' ] : for col2 in [ 'etecho1' , 'etecho2' , 'etecho3' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
1621	import seaborn as sns sns . set ( ) g = sns . FacetGrid ( pd . melt ( df [ [ 'bone_length' , 'rotting_flesh' , 'hair_length' , 'has_soul' , 'type' ] ] , id_vars = 'type' ) , col = 'type' ) g . map ( sns . boxplot , 'value' , 'variable' )
1811	print ( pd . crosstab ( y_test , preds , rownames = [ 'Actual' ] , colnames = [ 'Predicted' ] ) ) plot_roc_curve ( clf , X_test , y_test , preds , isRF = True ) return clf rf_clf = RF ( X_train , X_test , y_train , y_test )
1612	dev_df = df_train [ train_dates <= datetime . date ( 2017 , 5 , 31 ) ] val_df = df_train [ train_dates > datetime . date ( 2017 , 5 , 31 ) ] dev_y = np . log1p ( dev_df [ "totals_transactionRevenue" ] . values ) val_y = np . log1p ( val_df [ "totals_transactionRevenue" ] . values ) use_cols = [ col for col in df_train . columns if col not in not_use_cols ] dev_X = dev_df [ use_cols ] val_X = val_df [ use_cols ] test_X = df_test [ use_cols ]
42	grouped = pd . DataFrame ( dataset [ var_name_official ] . groupby ( [ dataset [ var_name_official ] . dt . year , dataset [ var_name_official ] . dt . month , dataset [ var_name_official ] . dt . day ] ) . count ( ) ) . rename ( columns = { var_name_official : 'Count' } )
214	Xtrain , Xval , Ztrain , Zval = train_test_split ( X , z , test_size = 0.2 , random_state = 0 ) train_set = lgbm . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgbm . Dataset ( Xval , Zval , silent = False )
1772	seed_everything ( ) x_test_cuda = torch . tensor ( x_test , dtype = torch . long ) . cuda ( ) test = torch . utils . data . TensorDataset ( x_test_cuda ) test_loader = torch . utils . data . DataLoader ( test , batch_size = batch_size , shuffle = False ) avg_losses_f = [ ] avg_val_losses_f = [ ] def sigmoid ( x ) : return 1 / ( 1 + np . exp ( - x ) )
1562	from sklearn . model_selection import train_test_split params = { 'n_estimators' : 200 , 'max_depth' : 5 , 'min_child_weight' : 100 , 'subsample' : .9 , 'gamma' : 1 , 'objective' : 'reg:linear' , 'colsample_bytree' : .8 , 'nthread' : 3 , 'silent' : 1 , 'seed' : 27 } train , test = train_test_split ( x_train , test_size = 0.2 ) predictors = df [ 'feature' ] [ df [ 'fscore' ] > 0.5 ] . tolist ( )
892	train_missing = train_missing . index [ train_missing > 0.75 ] test_missing = test_missing . index [ test_missing > 0.75 ] all_missing = list ( set ( set ( train_missing ) | set ( test_missing ) ) ) print ( 'There are %d columns with more than 75%% missing values' % len ( all_missing ) )
1333	if self . has_se : num_squeezed_channels = max ( 1 , int ( self . _block_args . input_filters * self . _block_args . se_ratio ) ) self . _se_reduce = Conv2d ( in_channels = oup , out_channels = num_squeezed_channels , kernel_size = 1 ) self . _se_expand = Conv2d ( in_channels = num_squeezed_channels , out_channels = oup , kernel_size = 1 )
1172	reader = vtk . vtkDICOMImageReader ( ) reader . SetDirectoryName ( datadir + patient ) reader . Update ( )
1094	input_dim = salt_parameters [ 'image_size' ] if salt_parameters [ 'grayscale' ] : input_dim = input_dim + ( 1 , ) else : input_dim = input_dim + ( 3 , )
801	predictions [ 'Target' ] = predictions [ [ 1 , 2 , 3 , 4 ] ] . idxmax ( axis = 1 ) predictions [ 'confidence' ] = predictions [ [ 1 , 2 , 3 , 4 ] ] . max ( axis = 1 ) predictions = predictions . drop ( columns = [ 'fold' ] )
1079	for word in words : if word in index2word_set : nwords = nwords + 1 featureVec = np . add ( featureVec , model [ word ] )
973	random [ 'set' ] = 'random' scores = random [ [ 'score' , 'iteration' , 'set' ] ] opt [ 'set' ] = 'opt' scores = scores . append ( opt [ [ 'set' , 'iteration' , 'score' ] ] , sort = True ) scores . head ( )
1779	f1 = open ( "eap.png" , "wb" ) f1 . write ( codecs . decode ( eap_64 , 'base64' ) ) f1 . close ( ) img1 = imread ( "eap.png" )
1404	print ( 'encode_input_first_day:' , cmp_enc_start . date ( ) ) print ( 'encode_input_last_day:' , cmp_enc_end . date ( ) ) columns = [ f"F{id}" for id in range ( 1 , 29 ) ] sumbmission_df = pd . DataFrame ( pred_series_transformed , columns = columns ) sumbmission_df [ "id" ] = ids del pred_series_transformed gc . collect ( )
1491	summary_dict = { } nq_pred_dict = { } for e in examples : all_summaries = compute_predictions ( e ) summary_dict [ e . example_id ] = all_summaries nq_pred_dict [ e . example_id ] = [ summary . predicted_label for summary in all_summaries ] if len ( nq_pred_dict ) % 100 == 0 : print ( "Examples processed: %d" % len ( nq_pred_dict ) ) return nq_pred_dict
314	train0b = train0 train_target0b = train0b [ target_name ] train0b = train0b . drop ( [ target_name ] , axis = 1 )
1555	d_train = lgb . Dataset ( X_train , y_train ) model_lgb = lgb . train ( lgb_params , d_train , feval = lgb_rmsle_score , num_boost_round = n_rounds )
851	df [ pre + 'Elapsed' ] = ( fld - start_ref ) . dt . total_seconds ( ) if drop : df = df . drop ( date_col , axis = 1 ) return df
672	list_models = [ ( 'ElasticNet' , ElasticNet ( ) ) , ( 'ElasticNet_wdiff' , ElasticNet ( ) ) , ( 'ElasticNet_wdiffwrolling' , ElasticNet ( ) ) , ]
485	conv1 = Conv2D ( 32 , kernel_size = 4 , activation = 'relu' ) ( visible ) pool1 = MaxPooling2D ( pool_size = ( 2 , 2 ) ) ( conv1 ) conv2 = Conv2D ( 16 , kernel_size = 4 , activation = 'relu' ) ( pool1 ) pool2 = MaxPooling2D ( pool_size = ( 2 , 2 ) ) ( conv2 )
671	dftrainall = dftrain . join ( pop , on = 'Country_Region' ) dftrainall [ 'Lockdown' ] = dftrainlockdown [ 'Lockdown' ] dftrainall = dftrainall . join ( flights , on = 'Country_Region' ) dftrainall [ 'Mortality' ] = dftrainall [ 'Fatalities' ] / dftrainall [ 'ConfirmedCases' ] dftrainall [ 'ConfirmedCases_by_pop' ] = dftrainall [ 'ConfirmedCases' ] / dftrainall [ 'Population (2020)' ] dftrainall [ 'ConfirmedCases_by_Km' ] = dftrainall [ 'ConfirmedCases' ] / dftrainall [ 'Land Area (Km)' ] dftrainall . tail ( )
139	from mmcv . ops import get_compiling_cuda_version , get_compiler_version print ( get_compiling_cuda_version ( ) ) print ( get_compiler_version ( ) )
93	def setBefore ( bef , aft ) : for b in bef : before [ b , aft ] += 1 return def valrow ( n ) : return values [ n ] _vset = [ ]
830	plt . figure ( figsize = ( 15 , 5 ) ) sns . countplot ( label [ 'surface' ] , order = label . surface . value_counts ( ) . index ) plt . show ( )
307	t = Tokenizer ( ) t . fit_on_texts ( docs_combined ) vocab_size = len ( t . word_index ) + 1
171	x = AveragePooling2D ( ( 8 , 8 ) ) ( x ) x = Flatten ( ) ( x ) outputs = Dense ( output_dim , activation = "softmax" ) ( x ) model = Model ( inputs = inputs , outputs = outputs ) return model
700	clf = QuadraticDiscriminantAnalysis ( reg_param = 0.5 ) clf . fit ( train3 [ train_index , : ] , train2 . loc [ train_index ] [ 'target' ] ) oof [ idx1 [ test_index ] ] = clf . predict_proba ( train3 [ test_index , : ] ) [ : , 1 ] preds [ idx2 ] += clf . predict_proba ( test3 ) [ : , 1 ] / skf . n_splits
1303	del model from keras import backend as K import gc K . clear_session ( ) gc . collect ( )
1462	data_dir = '../input/alaska2-image-steganalysis' folder_names = [ 'JMiPOD/' , 'JUNIWARD/' , 'UERD/' ] class_names = [ 'Normal' , 'JMiPOD_75' , 'JMiPOD_90' , 'JMiPOD_95' , 'JUNIWARD_75' , 'JUNIWARD_90' , 'JUNIWARD_95' , 'UERD_75' , 'UERD_90' , 'UERD_95' ] class_labels = { name : i for i , name in enumerate ( class_names ) }
387	item = bson . BSON . decode ( item_data ) IDS_MAPPING [ item [ '_id' ] ] = ( offset , length ) offset += length def get_item ( item_id ) : assert item_id in IDS_MAPPING with open ( os . path . join ( INPUT_PATH , 'train.bson' ) , 'rb' ) as f : offset , length = IDS_MAPPING [ item_id ] f . seek ( offset ) item_data = f . read ( length ) return bson . BSON . decode ( item_data )
1801	k = np . array ( [ [ 2304.5479 , 0 , 1686.2379 ] , [ 0 , 2305.8757 , 1354.9849 ] , [ 0 , 0 , 1 ] ] , dtype = np . float32 )
441	import mplleaflet plt . figure ( figsize = ( 10 , 10 ) ) map1 = df_train [ df_train [ 'City' ] == 'Atlanta' ] . groupby ( [ 'Latitude' , 'Longitude' ] ) [ 'RowId' ] . count ( ) . reset_index ( ) plt . scatter ( map1 [ 'Longitude' ] , map1 [ 'Latitude' ] , alpha = 0.5 ) mplleaflet . display ( )
142	concat_sub [ 'ieee_max' ] = concat_sub . iloc [ : , 1 : ncol ] . max ( axis = 1 ) concat_sub [ 'ieee_min' ] = concat_sub . iloc [ : , 1 : ncol ] . min ( axis = 1 ) concat_sub [ 'ieee_mean' ] = concat_sub . iloc [ : , 1 : ncol ] . mean ( axis = 1 ) concat_sub [ 'ieee_median' ] = concat_sub . iloc [ : , 1 : ncol ] . median ( axis = 1 )
804	subsample = hyperparameters [ 'boosting_type' ] . get ( 'subsample' , 1.0 ) subsample_freq = hyperparameters [ 'boosting_type' ] . get ( 'subsample_freq' , 0 ) boosting_type = hyperparameters [ 'boosting_type' ] [ 'boosting_type' ] if boosting_type == 'dart' : hyperparameters [ 'drop_rate' ] = hyperparameters [ 'boosting_type' ] [ 'drop_rate' ]
565	predicted = clf . predict ( data_sub ) print ( features ) print ( clf . feature_importances_ )
1685	import numpy as np import pandas as pd import itertools import random import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' training_tasks = sorted ( os . listdir ( training_path ) )
1228	train1 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv" ) train2 = pd . read_csv ( "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv" ) train2 . toxic = train2 . toxic . round ( ) . astype ( int ) valid = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv' ) test = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv' ) sub = pd . read_csv ( '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv' )
218	feature_score = pd . DataFrame ( preprocessing . MinMaxScaler ( ) . fit_transform ( feature_score ) , columns = feature_score . columns , index = feature_score . index )
816	preds_df [ 'prediction' ] = preds_df [ [ 1 , 2 , 3 , 4 ] ] . idxmax ( axis = 1 ) preds_df [ 'confidence' ] = preds_df [ [ 1 , 2 , 3 , 4 ] ] . max ( axis = 1 ) preds_df . head ( )
1559	ls = [ feat2 ] for method in methods : corr = df [ [ feat1 , feat2 ] ] . corr ( method = method ) ls . append ( corr [ feat1 ] [ 1 ] ) corr_df . loc [ len ( corr_df ) ] = ls
511	df_season_composite [ 'WINS' ] = df_season [ 'WTeamID' ] . groupby ( [ df_season [ 'Season' ] , df_season [ 'WTeamID' ] ] ) . count ( ) df_season_composite [ 'LOSSES' ] = df_season [ 'LTeamID' ] . groupby ( [ df_season [ 'Season' ] , df_season [ 'LTeamID' ] ] ) . count ( ) df_season_composite [ 'WINPCT' ] = df_season_composite [ 'WINS' ] / ( df_season_composite [ 'WINS' ] + df_season_composite [ 'LOSSES' ] )
1512	FIGSIZE = 13.0 SPACING = 0.1 subplot = ( rows , cols , 1 ) if rows < cols : plt . figure ( figsize = ( FIGSIZE , FIGSIZE / cols * rows ) ) else : plt . figure ( figsize = ( FIGSIZE / rows * cols , FIGSIZE ) )
1185	if is_test_set or ( win_n + loss_n ) > 0 : user_samples . append ( features ) counter += 1
493	print ( 'Applicatoin train shape before merge: ' , ap_train . shape ) ap_train = ap_train . merge ( br_data , left_on = 'SK_ID_CURR' , right_on = 'SK_ID_CURR' , how = 'inner' ) print ( 'Applicatoin train shape after merge: ' , ap_train . shape )
1798	trainPredictPlot = np . empty_like ( df_dl ) trainPredictPlot [ : , : ] = np . nan trainPredictPlot [ look_back : len ( trainPredict ) + look_back , : ] = trainPredict
879	for i , hyper in enumerate ( random_params . columns ) : if hyper not in [ 'class_weight' , 'n_estimators' , 'score' , 'is_unbalance' , 'boosting_type' , 'iteration' , 'subsample' , 'metric' , 'verbose' , 'loss' , 'learning_rate' ] : plt . figure ( figsize = ( 14 , 6 ) )
1611	cat_cols = [ "channelGrouping" , "device_browser" , "device_deviceCategory" , "device_operatingSystem" , "geoNetwork_city" , "geoNetwork_continent" , "geoNetwork_country" , "geoNetwork_metro" , "geoNetwork_networkDomain" , "geoNetwork_region" , "geoNetwork_subContinent" , "trafficSource_adContent" , "trafficSource_adwordsClickInfo.adNetworkType" , "trafficSource_adwordsClickInfo.gclId" , "trafficSource_adwordsClickInfo.page" , "trafficSource_adwordsClickInfo.slot" , "trafficSource_campaign" , "trafficSource_keyword" , "trafficSource_medium" , "trafficSource_referralPath" , "trafficSource_source" , 'trafficSource_adwordsClickInfo.isVideoAd' , 'trafficSource_isTrueDirect' , 'device_isMobile' ]
553	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
1670	seed = 42 random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . backends . cudnn . deterministic = True if torch . cuda . is_available ( ) : torch . cuda . manual_seed_all ( seed )
297	df_train , df_val = train_test_split ( df_data , test_size = 0.1 , random_state = 101 ) print ( df_train . shape ) print ( df_val . shape )
910	_ , idx = np . unique ( agg , axis = 1 , return_index = True ) agg = agg . iloc [ : , idx ] return agg
1195	sub_prediction = pd . Series ( data = gbm . predict ( test_X , num_iteration = model . best_iteration_ ) ) sub_prediction [ sub_prediction < 0 ] = 0 predictions_test [ 'Predictions_{}' . format ( k ) ] = sub_prediction . copy ( ) k += 1
115	learning_rate = [ 0.15 , 0.2 , 0.25 ] max_depth = [ 15 , 20 , 25 ] param_grid = { 'learning_rate' : learning_rate , 'max_depth' : max_depth }
1044	train = train . drop ( columns = missing_columns ) test = test . drop ( columns = missing_columns ) return train , test
412	import numpy as np import pandas as pd from scipy import stats import os , gc import matplotlib . pyplot as plt import seaborn as sns sns . set_style ( "whitegrid" ) import plotly . offline as py from plotly . offline import iplot , init_notebook_mode import plotly . graph_objs as go init_notebook_mode ( connected = True ) from IPython . display import Markdown def bold ( string ) : display ( Markdown ( string ) )
944	for i , hyp in enumerate ( results [ 'hyperparameters' ] ) : hyp_df = hyp_df . append ( pd . DataFrame ( hyp , index = [ 0 ] ) , ignore_index = True )
843	X_train , X_valid , y_train , y_valid = train_test_split ( data , np . array ( data [ 'fare_amount' ] ) , stratify = data [ 'fare-bin' ] , random_state = RSEED , test_size = 1_000_000 )
1566	test [ 'Min_week' ] = test . groupby ( 'Patient' ) [ 'Weeks' ] . transform ( 'min' ) base = test . loc [ test . Weeks == test . Min_week ] base = base [ [ 'Patient' , 'FVC' ] ] . copy ( ) base . columns = [ 'Patient' , 'Base_FVC' ] test = test . merge ( base , on = 'Patient' , how = 'left' ) test [ 'Base_week' ] = test [ 'Weeks' ] - test [ 'Min_week' ]
374	dtiny = np . finfo ( 0.0 ) . tiny idx = lta < dtiny lta [ idx ] = dtiny return sta / lta
520	logreg = LogisticRegression ( random_state = 0 ) params = { 'C' : np . logspace ( start = - 5 , stop = 3 , num = 9 ) } clf = GridSearchCV ( logreg , params , scoring = 'neg_log_loss' , refit = True , cv = 10 , ) clf . fit ( X_train , y_train ) print ( 'Best log_loss: {:.4}, with best C: {}' . format ( clf . best_score_ , clf . best_params_ [ 'C' ] ) )
1470	if scale : scaler = StandardScaler ( ) . fit ( x_train_kf . values ) x_train_kf_values = scaler . transform ( x_train_kf . values ) x_val_kf_values = scaler . transform ( x_val_kf . values ) x_test_values = scaler . transform ( x_test . values ) else : x_train_kf_values = x_train_kf . values x_val_kf_values = x_val_kf . values x_test_values = x_test . values
803	plt . figure ( figsize = ( 10 , 6 ) ) sns . boxplot ( x = 'Target' , y = 'confidence' , data = predictions ) ; plt . title ( 'Confidence by Target' ) ; plt . figure ( figsize = ( 10 , 6 ) ) sns . violinplot ( x = 'Target' , y = 'confidence' , data = predictions ) ; plt . title ( 'Confidence by Target' ) ;
1377	import matplotlib . pyplot as plt def plotImages ( images_arr ) : fig , axes = plt . subplots ( 1 , 5 , figsize = ( 20 , 20 ) ) axes = axes . flatten ( ) for img , ax in zip ( images_arr , axes ) : ax . imshow ( img ) plt . tight_layout ( ) plt . show ( ) augmented_images = [ train_generator [ 0 ] [ 0 ] [ 0 ] for i in range ( 5 ) ] plotImages ( augmented_images )
113	out_df = pd . merge ( out_df , store_day_lag , left_on = "store_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , store_day_year_lag , left_on = "store_id" , right_index = True , how = "left" ) out_df = pd . merge ( out_df , month_store_lag , left_on = "store_id" , right_index = True , how = "left" )
1753	r_applications_bureau = ft . Relationship ( es [ 'applications' ] [ 'SK_ID_CURR' ] , es [ 'bureau' ] [ 'SK_ID_CURR' ] ) es = es . add_relationship ( r_applications_bureau )
1161	idx3 = tf . stack ( [ DIM // 2 - idx2 [ 0 , ] , DIM // 2 - 1 + idx2 [ 1 , ] ] ) d = tf . gather_nd ( image , tf . transpose ( idx3 ) ) return tf . reshape ( d , [ DIM , DIM , 3 ] ) , label
777	grid . map_lower ( sns . kdeplot , cmap = plt . cm . OrRd_r ) ; grid = grid . add_legend ( ) plt . suptitle ( 'Feature Plots Colored By Target' , size = 32 , y = 1.05 ) ;
783	from sklearn . ensemble import RandomForestClassifier from sklearn . metrics import f1_score , make_scorer from sklearn . model_selection import cross_val_score from sklearn . preprocessing import Imputer from sklearn . preprocessing import MinMaxScaler from sklearn . pipeline import Pipeline scorer = make_scorer ( f1_score , greater_is_better = True , average = 'macro' )
445	road_encoding = { 'Road' : 1 , 'Street' : 2 , 'Avenue' : 2 , 'Drive' : 3 , 'Broad' : 3 , 'Boulevard' : 4 }
1073	ed = application . groupby ( [ 'TARGET' , 'CNT_CHILDREN' ] ) [ 'TARGET' ] . count ( ) . unstack ( 'TARGET' ) . fillna ( 0 ) ed . plot ( kind = 'bar' , stacked = True ) print ( ed )
586	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . boxplot ( x = "bedroomcnt" , y = "logerror" , data = mergedFiltered , ax = ax , color = " ax.set(ylabel='Log Error',xlabel=" Bedroom Count ",title=" Bedroom Count Vs Log Error " )
251	sgd = SGDRegressor ( ) sgd . fit ( train , target ) acc_model ( 4 , sgd , train , test )
19	nulls = np . sum ( train . isnull ( ) ) nullcols = nulls . loc [ ( nulls != 0 ) ] dtypes = train . dtypes dtypes2 = dtypes . loc [ ( nulls != 0 ) ] info = pd . concat ( [ nullcols , dtypes2 ] , axis = 1 ) . sort_values ( by = 0 , ascending = False )
1764	train_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . notnull ( ) ] . copy ( ) test_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . isnull ( ) ] . copy ( ) test_df . drop ( [ 'TARGET' ] , axis = 1 , inplace = True ) del feature_matrix , feature_defs , feature_matrix_enc gc . collect ( )
872	OUT_FILE = 'bayes_test.csv' of_connection = open ( OUT_FILE , 'w' ) writer = csv . writer ( of_connection ) ITERATION = 0
571	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) sn . countplot ( color = " ax.set(xlabel='Hour Of The Day',title=" Reorder Count " )
1784	sta [ length_sta : ] = sta [ length_sta : ] - sta [ : - length_sta ] sta /= length_sta lta [ length_lta : ] = lta [ length_lta : ] - lta [ : - length_lta ] lta /= length_lta
833	plt . figure ( figsize = ( 10 , 6 ) ) sns . distplot ( data [ 'fare_amount' ] ) ; plt . title ( 'Distribution of Fare' ) ;
45	fig . quad ( top = 'Count' , bottom = 'bottom' , left = 'left' , right = 'right' , source = ColumnDataSource ( grouped ) , color = bar_colors , legend = dataset_name . capitalize ( ) ) fig . legend . location = 'bottom_right' show ( fig )
1331	state_dict = model_zoo . load_url ( url_map [ model_name ] ) if load_fc : model . load_state_dict ( state_dict ) else : state_dict . pop ( '_fc.weight' ) state_dict . pop ( '_fc.bias' ) res = model . load_state_dict ( state_dict , strict = False ) assert str ( res . missing_keys ) == str ( [ '_fc.weight' , '_fc.bias' ] ) , 'issue loading pretrained weights' print ( 'Loaded pretrained weights for {}' . format ( model_name ) ) class MBConvBlock ( nn . Module ) :
408	tag_to_count_map tupl = dict ( tag_to_count_map . items ( ) ) word_cloud = WordCloud ( width = 1600 , height = 800 , ) . generate_from_frequencies ( tupl ) plt . figure ( figsize = ( 12 , 8 ) ) plt . imshow ( word_cloud ) plt . axis ( 'off' ) plt . tight_layout ( pad = 0 )
403	model = CatBoostRegressor ( iterations = 1000 , task_type = 'GPU' , verbose = 0 , loss_function = 'RMSE' , boosting_type = 'Plain' , depth = 8 , ) model . fit ( train_pool , eval_set = val_pool , plot = True ) del train_pool , val_pool gc . collect ( )
257	linreg . fit ( train0 , train_target0 ) Submission1 [ 'Predicted' ] = linreg . predict ( testn ) Submission1 . to_csv ( 'submission1.csv' , index = False ) Submission1 . head ( 3 )
44	norm = colors . Normalize ( vmax = grouped [ 'Count' ] . max ( ) , vmin = grouped [ 'Count' ] . min ( ) - 2 * grouped [ 'Count' ] . min ( ) ) bar_colors = [ colors . rgb2hex ( cmap ( norm ( c ) ) ) for c in grouped [ 'Count' ] ]
1590	data_dir = '/kaggle/input/stanford-covid-vaccine/' train = pd . read_json ( data_dir + 'train.json' , lines = True ) test = pd . read_json ( data_dir + 'test.json' , lines = True ) sample_df = pd . read_csv ( data_dir + 'sample_submission.csv' )
94	def vset ( n ) : while len ( _vset ) <= n : _vset . append ( None ) if _vset [ n ] is None : vs = set ( valrow ( n ) ) vs . remove ( 0 ) _vset [ n ] = vs return _vset [ n ]
1152	filenames_valid = valid_df . image_filename . values y_valid = valid_df . AdoptionSpeed . values y_valid = to_categorical ( y_valid ) print ( 'valid set shapes:' ) print ( filenames_valid . shape , y_valid . shape ) valid_parser = PetfinderImageParser ( preproc_func , image_size , valid_aug ) valid_datagen = PetfinderDataGenerator ( filenames_valid , y_valid , valid_parser , image_size = image_size , batch_size = batch_size )
244	country_name = "Andorra" shift = 0 day_start = 39 + shift dates_list2 = dates_list [ shift : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , shift )
807	features = np . array ( train_selected ) labels = np . array ( train_labels ) . reshape ( ( - 1 ) ) valid_scores = [ ] best_estimators = [ ] run_times = [ ] model = lgb . LGBMClassifier ( ** hyperparameters , class_weight = 'balanced' , n_jobs = - 1 , metric = 'None' , n_estimators = 10000 )
1645	lr_corse = np . array ( [ 1e-3 , 1e-3 , 1e-3 , 3e-4 , 3e-4 , 3e-4 , 1e-4 , 1e-4 , 1e-4 , 3e-4 , 3e-4 , 3e-4 , 1e-4 , 1e-4 , 1e-4 , 3e-5 , 3e-5 , 3e-5 , 1e-3 , 3e-4 , 1e-4 , 3e-5 , 3e-5 , 3e-5 , 1e-5 , 1e-5 , 1e-5 , 3e-6 , 3e-6 , 3e-6 , 1e-5 , 1e-5 , 1e-5 , 3e-6 , 3e-6 , 3e-6 , 1e-5 , 1e-5 , 1e-5 , 3e-6 , 3e-6 , 3e-6 , 3e-6 , 3e-6 , 3e-6 ] )
1538	actual = actual . fillna ( 0 ) data = pd . concat ( [ predict , actual ] , axis = 1 , keys = [ 'predict' , 'actual' ] ) data = data [ data . actual . notnull ( ) ] if debug : print ( 'debug' , data ) evals = abs ( data . predict - data . actual ) * 1.0 / ( abs ( data . predict ) + abs ( data . actual ) ) * 2 evals [ evals . isnull ( ) ] = 0
317	sgd = SGDRegressor ( ) sgd . fit ( train , target ) acc_model ( 4 , sgd , train , test )
1433	plt . figure ( figsize = ( 10 , 8 ) ) sns . heatmap ( link_count ) plt . show ( )
443	plt . figure ( figsize = ( 10 , 10 ) ) map3 = df_train [ df_train [ 'City' ] == 'Philadelphia' ] . groupby ( [ 'Latitude' , 'Longitude' ] ) [ 'RowId' ] . count ( ) . reset_index ( ) plt . scatter ( map3 [ 'Longitude' ] , map3 [ 'Latitude' ] , alpha = 0.5 ) mplleaflet . display ( )
723	full_data [ 'ord_3_by_ord' ] = full_data . ord_3 . map ( ord , na_action = 'ignore' ) map_ord3 = { key : value for value , key in enumerate ( sorted ( full_data . ord_3 . dropna ( ) . unique ( ) ) ) } full_data . ord_3 = full_data . ord_3 . map ( map_ord3 )
221	threshold = 0.9 def highlight ( value ) : if value > threshold : style = 'background-color: pink' else : style = 'background-color: palegreen' return style
990	example_credit = installments [ installments [ 'SK_ID_PREV' ] == 1369693 ] plt . plot ( ( example_credit [ 'installments_due_date' ] - example_credit [ 'installments_paid_date' ] ) . dt . days , example_credit [ 'AMT_INSTALMENT' ] , 'ro' ) ; plt . title ( 'Loan 1369693' ) ; plt . xlabel ( 'Days Paid Early' ) ; plt . ylabel ( 'Installment Amount' ) ;
534	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
1088	if self . pad_images : print ( 'Remove padding from images.' ) y_min_pad , y_max_pad , x_min_pad , x_max_pad = self . padding_pixels [ 0 ] , self . padding_pixels [ 1 ] , self . padding_pixels [ 2 ] , self . padding_pixels [ 3 ] y_pred_test = y_pred_test [ : , y_min_pad : - y_max_pad , x_min_pad : - x_max_pad , 0 ]
1661	order = [ ] with open ( 'lk.sol' , 'r' ) as fp : lines = fp . readlines ( ) order = [ int ( v . split ( ' ' ) [ 0 ] ) for v in lines [ 1 : ] ] + [ 0 ]
1396	epoch_datetime = pd . datetime ( 1900 , 1 , 1 ) trf_var_68_s = ( train_df [ 'var_68' ] * 10000 - 7000 + epoch_datetime . toordinal ( ) ) . astype ( int ) date_s = trf_var_68_s . map ( datetime . fromordinal ) train_df [ 'date' ] = date_s sorted_train_df = train_df . drop ( 'var_68' , axis = 1 ) . sort_values ( 'date' )
365	svr = SVR ( ) svr . fit ( train , target ) acc_model ( 1 , svr , train , test )
734	target_size = ( 224 , 224 ) if img . size != target_size : img = img . resize ( target_size ) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = pak . preprocess_input ( x ) preds = model . predict ( x ) return pak . decode_predictions ( preds , top = top_n ) [ 0 ] def plot_preds ( img , preds_arr ) :
1256	for folder in [ 'train' , 'test' ] : os . makedirs ( folder )
1527	raw_results_by_id = [ ( int ( res . unique_id ) , 1 , res ) for res in raw_results ] examples_by_id = [ ( int ( k ) , 0 , v ) for k , v in candidates_dict . items ( ) ] features_by_id = [ ( int ( d [ 'unique_id' ] ) , 2 , d ) for d in dev_features ]
465	bayesian_tr_idx , bayesian_val_idx = train_test_split ( train_df , test_size = 0.3 , random_state = 42 , stratify = train_df [ target ] ) bayesian_tr_idx = bayesian_tr_idx . index bayesian_val_idx = bayesian_val_idx . index
1413	label_map = dict ( labels [ [ 'attribute_id' , 'attribute_name' ] ] . values . tolist ( ) ) not_in_train_labels = set ( labels [ 'attribute_id' ] . astype ( str ) . values ) - set ( list ( cls_counts ) ) for _id in not_in_train_labels : label = label_map [ int ( _id ) ] print ( f'attribute_id: {_id} attribute_name: {label}' )
722	map_ord1 = { 'Novice' : 1 , 'Contributor' : 2 , 'Expert' : 4 , 'Master' : 5 , 'Grandmaster' : 6 } full_data . ord_1 = full_data . ord_1 . map ( map_ord1 )
1691	def lift ( fct ) : def lifted_function ( xs ) : list_of_results = [ fct ( x ) for x in xs ] return list ( itertools . chain ( * list_of_results ) ) import re lifted_function . __name__ = re . sub ( '_unlifted$' , '_lifted' , fct . __name__ ) return lifted_function cropToContent = lift ( cropToContent_unlifted ) groupByColor = lift ( groupByColor_unlifted ) splitH = lift ( splitH_unlifted ) negative = lift ( negative_unlifted )
681	d = { "unicode" : label_count [ : 10 ] . index . values , "str" : [ unicode_map [ l ] for l in label_count [ : 10 ] . index . values ] } pd . DataFrame ( d )
228	ensemble_final = ensembles [ 0 ] . copy ( ) ensemble_final [ target_cols ] = 0 for ensemble in ensembles : ensemble_final [ target_cols ] += ensemble [ target_cols ] . values / len ( ensembles ) ensemble_final
396	from sklearn . metrics import confusion_matrix confusion = confusion_matrix ( yt , yt_pred ) print ( confusion )
168	state [ 'exp_avg_sq' ] = torch . zeros_like ( p . data ) exp_avg , exp_avg_sq = state [ 'exp_avg' ] , state [ 'exp_avg_sq' ] beta1 , beta2 = group [ 'betas' ] state [ 'step' ] += 1
598	images_with_ship = masks . ImageId [ masks . EncodedPixels . isnull ( ) == False ] images_with_ship = np . unique ( images_with_ship . values ) print ( 'There are ' + str ( len ( images_with_ship ) ) + ' image files with masks' )
118	fig = px . line ( train_df , 'Weeks' , 'FVC' , line_group = 'Patient' , color = 'Sex' , title = 'Pulmonary Condition Progression by Sex' ) fig . update_traces ( mode = 'lines+markers' )
1180	return self . conf_mtx . assign_add ( new_conf_mtx ) def result ( self ) : nb_ratings = tf . shape ( self . conf_mtx ) [ 0 ] weight_mtx = tf . ones ( [ nb_ratings , nb_ratings ] , dtype = tf . int32 )
1537	import re def breakdown_topic ( str ) : m = re . search ( '(.*)\_(.*).wikipedia.org\_(.*)\_(.*)' , str ) if m is not None : return m . group ( 1 ) , m . group ( 2 ) , m . group ( 3 ) , m . group ( 4 ) else : return "" , "" , "" , "" print ( breakdown_topic ( ",___ru.wikipedia.org_all-access_spider" ) ) print ( breakdown_topic ( "_zh.wikipedia.org_all-access_spider" ) ) print ( breakdown_topic ( "File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents" ) )
452	import numpy as np import pandas as pd from PIL import Image from PIL import ImageColor from PIL import ImageDraw from PIL import ImageFont from PIL import ImageOps
1637	df = do_cumcount ( df , [ 'ip' , 'device' , 'os' ] , 'app' , 'cumcount_ip_dev_os' , show_max = True ) ; gc . collect ( ) print ( '*' * 30 ) print ( 'after data prep:' ) print ( df . head ( ) ) print ( 'finished feature generation' ) return ( df )
1338	self . _blocks . append ( MBConvBlock ( block_args , self . _global_params ) ) if block_args . num_repeat > 1 : block_args = block_args . _replace ( input_filters = block_args . output_filters , stride = 1 ) for _ in range ( block_args . num_repeat - 1 ) : self . _blocks . append ( MBConvBlock ( block_args , self . _global_params ) )
822	image_id = 'c14c1e300' image = cv2 . imread ( os . path . join ( BASE_DIR , 'train' , f'{image_id}.jpg' ) , cv2 . IMREAD_COLOR ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) . astype ( np . float32 ) image /= 255.0 plt . figure ( figsize = ( 10 , 10 ) ) plt . imshow ( image ) plt . show ( )
1249	trials_df = pd . DataFrame ( [ parse_trial_state ( t ) for t in tuner . oracle . trials . values ( ) ] ) trials_df . to_csv ( 'trials_table.csv' , index = False ) trials_df
255	etr = ExtraTreesRegressor ( ) etr . fit ( train , target ) acc_model ( 12 , etr , train , test )
1508	for image , labe in training_dataset : pass end = datetime . datetime . now ( ) elapsed = ( end - start ) . total_seconds ( ) average = elapsed / n_iter print ( "Average timing for 1 iteration = {}" . format ( average ) )
729	print ( 'Coefficient of variation (CV) for prices in different recognized image categories.' ) dfl = dff . merge ( all_image_labels , left_on = 'image' , right_index = True , how = 'left' ) dfd = dfl . groupby ( 'image_label' ) [ 'price' ] . apply ( lambda x : np . std ( x ) / np . mean ( x ) ) . sort_values ( ascending = False ) dfd . head ( 10 )
1362	import datetime START_DATE = '2017-12-01' startdate = datetime . datetime . strptime ( START_DATE , '%Y-%m-%d' ) df [ 'TransactionDT' ] = df [ 'TransactionDT' ] . apply ( lambda x : ( startdate + datetime . timedelta ( seconds = x ) ) ) df [ 'hour' ] = df [ 'TransactionDT' ] . dt . hour
1805	def memo ( df ) : mem = df . memory_usage ( index = True ) . sum ( ) print ( mem / 1024 ** 2 , " MB" )
917	previous = pd . read_csv ( '../input/previous_application.csv' ) . replace ( { 365243 : np . nan } ) previous = convert_types ( previous ) previous [ 'LOAN_RATE' ] = previous [ 'AMT_ANNUITY' ] / previous [ 'AMT_CREDIT' ] previous [ "AMT_DIFFERENCE" ] = previous [ 'AMT_CREDIT' ] - previous [ 'AMT_APPLICATION' ]
725	model = LogisticRegression ( C = 0.03 , max_iter = 300 ) model . fit ( encoded_train , raw_train . target ) test_pred = model . predict_proba ( encoded_test ) [ : , 1 ]
771	heads [ 'phones-per-capita' ] = heads [ 'qmobilephone' ] / heads [ 'tamviv' ] heads [ 'tablets-per-capita' ] = heads [ 'v18q1' ] / heads [ 'tamviv' ] heads [ 'rooms-per-capita' ] = heads [ 'rooms' ] / heads [ 'tamviv' ] heads [ 'rent-per-capita' ] = heads [ 'v2a1' ] / heads [ 'tamviv' ]
1032	for col in df : if col != parent_var and 'SK_ID' in col : df = df . drop ( columns = col )
1155	tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu )
278	def plot_word_cloud ( x , col ) : corpus = [ ] for k in x [ col ] . str . split ( ) : for i in k : corpus . append ( i ) plt . figure ( figsize = ( 12 , 8 ) ) word_cloud = WordCloud ( background_color = 'black' , max_font_size = 80 ) . generate ( " " . join ( corpus [ : 50 ] ) ) plt . imshow ( word_cloud ) plt . axis ( 'off' ) plt . show ( ) return corpus [ : 50 ]
1700	for index , fitness_function in enumerate ( fitness_functions ) : images = evaluate ( program , i ) if images == [ ] : score [ index ] += 500 else : score [ index ] = fitness_function ( images [ 0 ] , o ) return tuple ( score ) print ( "Fitness evaluation:" , evaluate_fitness ( [ groupByColor , cropToContent ] , task [ 'train' ] ) )
1552	day_year_avg = train_info . groupby ( 'day_of_year' ) . trip_duration . mean ( ) day_year_avg = day_year_avg . reset_index ( ) day_year_avg . columns = [ 'day_of_year' , 'day_of_year_avg' ] df = pd . merge ( left = df , right = day_year_avg , on = 'day_of_year' , how = 'left' )
1495	if not os . path . isdir ( INPUT_DIR ) : IS_KAGGLE = False INPUT_DIR = "./" NQ_DIR = "./" MY_OWN_NQ_DIR = "./"
1093	salt_parameters = { 'data_src' : '../input/' , 'image_size' : ( 128 , 128 ) , 'pad_images' : False , 'grayscale' : False , } salt_parser = SaltParser ( ** salt_parameters ) normalize = True save = False
792	submission = submission_base . merge ( predictions , on = 'idhogar' , how = 'left' ) . drop ( columns = [ 'idhogar' ] )
527	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
1809	feature_cols = [ col for col in train . columns if col not in [ 'is_churn' , 'msno' ] ] train [ feature_cols ] = train [ feature_cols ] . applymap ( lambda x : np . nan if np . isinf ( x ) else x ) test [ feature_cols ] = test [ feature_cols ] . applymap ( lambda x : np . nan if np . isinf ( x ) else x )
43	grouped [ 'left' ] = grouped [ 'date' ] - datetime . timedelta ( days = 0.5 ) grouped [ 'right' ] = grouped [ 'date' ] + datetime . timedelta ( days = 0.5 ) grouped [ 'bottom' ] = [ 0 ] * grouped . shape [ 0 ]
5	le = LabelEncoder ( ) df [ "primary_use" ] = le . fit_transform ( df [ "primary_use" ] ) return df
241	country_name = "Germany" march_day = 15 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
1599	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
1374	X_train = train . drop ( 'isFraud' , axis = 1 ) X_test = test . copy ( ) X_train = X_train . fillna ( - 999 ) X_test = X_test . fillna ( - 999 )
323	result = calculate_iou ( y_true , y_pred ) print ( result ) print ( type ( result ) )
37	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( np . log ( train_df [ columns_to_use ] . values . flatten ( ) + 1 ) , bins = 50 ) plt . title ( 'Log Histogram all train counts' ) plt . xlabel ( 'Count' ) plt . ylabel ( 'Log value' ) plt . show ( )
922	df . loc [ : n , : ] . plot . barh ( y = 'importance_normalized' , x = 'feature' , color = 'blue' , edgecolor = 'k' , figsize = ( 12 , 8 ) , legend = False ) plt . xlabel ( 'Normalized Importance' , size = 18 ) ; plt . ylabel ( '' ) ; plt . title ( f'Top {n} Most Important Features' , size = 18 ) plt . gca ( ) . invert_yaxis ( ) if threshold :
829	image_id = 'c14c1e300' image = cv2 . imread ( os . path . join ( BASE_DIR , 'train' , f'{image_id}.jpg' ) , cv2 . IMREAD_COLOR ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )
696	intersection = intersection [ 1 : , 1 : ] intersection [ intersection == 0 ] = 1e-9 union = union [ 1 : , 1 : ] union [ union == 0 ] = 1e-9
1708	import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt
1262	def load_img ( code , base , resize = True ) : path = f'{base}/{code}' img = cv2 . imread ( path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) if resize : img = cv2 . resize ( img , ( 256 , 256 ) ) return img def validate_path ( path ) : if not os . path . exists ( path ) : os . makedirs ( path )
108	state_group . groupby ( "year" ) [ 'CA' , 'TX' , 'WI' ] . sum ( ) . plot ( ) plt . title ( "Sales volume per year" ) plt . ylabel ( "Sales volume" ) ;
564	import re train [ 'int_desc' ] = train [ 'item_description' ] . apply ( lambda x : int ( bool ( re . search ( r'\d' , x ) ) ) ) test [ 'int_desc' ] = test [ 'item_description' ] . apply ( lambda x : int ( bool ( re . search ( r'\d' , x ) ) ) )
253	Xtrain , Xval , Ztrain , Zval = train_test_split ( trainb , targetb , test_size = 0.2 , random_state = 0 ) train_set = lgb . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgb . Dataset ( Xval , Zval , silent = False )
1304	del model from keras import backend as K import gc K . clear_session ( ) gc . collect ( )
839	for lh in leg . legendHandles : lh . set_alpha ( 1 ) leg . set_title ( 'Fare Bin' , prop = { 'size' : 28 } )
659	fagg_model_accuracies = perform_feature_agglomeration ( X_train , y_train , X_test , y_test ) print ( fagg_model_accuracies )
27	plt . figure ( figsize = ( 12 , 5 ) ) plt . hist ( train [ 'wheezy-copper-turtle-magic' ] . values , bins = 1000 ) plt . title ( 'Histogram muggy-smalt-axolotl-pembus counts' ) plt . xlabel ( 'Value' ) plt . ylabel ( 'Count' ) plt . show ( )
1080	if nwords == 0 : nwords = 1 featureVec = np . divide ( featureVec , nwords ) return featureVec def getAvgFeatureVecs ( reviews , model , num_features ) :
1622	print ( "Feature ranking:" ) for f in range ( df . shape [ 1 ] ) : print ( "%d. feature %s (%f)" % ( f + 1 , df . columns [ indices [ f ] ] , importances [ indices [ f ] ] ) )
1597	total = application_train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( application_train . isnull ( ) . sum ( ) / application_train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_application_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_application_train_data . head ( 20 )
1786	xs = pq . read_table ( '../input/train.parquet' , columns = [ str ( i ) for i in range ( 999 ) ] ) . to_pandas ( ) print ( ( xs . shape ) ) xs . head ( 2 )
164	im_id = im_path . parts [ - 3 ] im = imageio . imread ( str ( im_path ) ) im_gray = rgb2gray ( im )
567	if weights is not None : scl = K . sum ( weights ) weights = K . expand_dims ( weights , axis = 1 ) return K . sum ( K . dot ( arr , weights ) , axis = 1 ) / scl return K . mean ( arr , axis = 1 ) def weighted_loss ( y_true , y_pred ) :
308	df_train_padded = padded_docs_combined [ : train_len ] df_test_padded = padded_docs_combined [ train_len : ] print ( df_train_padded . shape ) print ( df_test_padded . shape )
1536	print ( "Check the number of records" ) print ( "Number of records: " , train . shape [ 0 ] , "\n" ) print ( "Null analysis" ) empty_sample = train [ train . isnull ( ) . any ( axis = 1 ) ] print ( "Number of records contain 1+ null: " , empty_sample . shape [ 0 ] , "\n" )
259	commits_df [ 'LB_score' ] = pd . to_numeric ( commits_df [ 'LB_score' ] ) commits_df [ 'max' ] = 0 commits_df . loc [ commits_df [ 'LB_score' ] . idxmax ( ) , 'max' ] = 1
901	model = lgb . LGBMClassifier ( n_estimators = 10000 , objective = 'binary' , boosting_type = 'goss' , class_weight = 'balanced' , learning_rate = 0.05 , reg_alpha = 0.1 , reg_lambda = 0.1 , n_jobs = - 1 , random_state = 50 )
508	def evaluate_threshold ( tpr , fpr , clf_threshold , threshold ) : print ( 'Sensitivity:' , tpr [ clf_threshold > threshold ] [ - 1 ] ) print ( 'Specificity:' , 1 - fpr [ clf_threshold > threshold ] [ - 1 ] )
1420	import nltk import re import gensim . models . word2vec as w2v import matplotlib . pyplot as plt
316	linear_svr = LinearSVR ( ) linear_svr . fit ( train , target ) acc_model ( 2 , linear_svr , train , test )
1349	ucf_root = Path ( '../input/ashrae-ucf-spider-and-eda-full-test-labels' ) leak0_df = pd . read_pickle ( ucf_root / 'site0.pkl' ) leak0_df [ 'meter_reading' ] = leak0_df . meter_reading_scraped leak0_df . drop ( [ 'meter_reading_original' , 'meter_reading_scraped' ] , axis = 1 , inplace = True ) leak0_df . fillna ( 0 , inplace = True ) leak0_df . loc [ leak0_df . meter_reading < 0 , 'meter_reading' ] = 0 leak0_df = leak0_df [ leak0_df . timestamp . dt . year > 2016 ] print ( len ( leak0_df ) )
1763	train_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . notnull ( ) ] . copy ( ) test_df = feature_matrix_enc [ feature_matrix_enc [ 'TARGET' ] . isnull ( ) ] . copy ( ) test_df . drop ( [ 'TARGET' ] , axis = 1 , inplace = True ) del feature_matrix , feature_defs , feature_matrix_enc gc . collect ( )
1729	leak = pd . read_csv ( '../input/breaking-lb-fresh-start-with-lag-selection/train_leak.csv' ) data [ 'leak' ] = leak [ 'compiled_leak' ] . values data [ 'log_leak' ] = np . log1p ( leak [ 'compiled_leak' ] . values )
1652	train = pd . read_csv ( "../input/train.csv" ) test = pd . read_csv ( '../input/test.csv' ) sample_submission = pd . read_csv ( '../input/sample_submission.csv' )
984	bureau = bureau . drop ( columns = [ 'DAYS_CREDIT' , 'DAYS_CREDIT_ENDDATE' , 'DAYS_ENDDATE_FACT' , 'DAYS_CREDIT_UPDATE' ] ) plt . figure ( figsize = ( 10 , 8 ) ) sns . distplot ( ( bureau [ 'bureau_credit_end_date' ] - bureau [ 'bureau_credit_application_date' ] ) . dropna ( ) . dt . days ) ; plt . xlabel ( 'Length of Loan (Days)' , size = 24 ) ; plt . ylabel ( 'Density' , size = 24 ) ; plt . title ( 'Loan Length' , size = 30 ) ;
1165	self . mel_mean = None self . mel_std = None self . mel_max = None self . mfcc_max = None def createMel ( self , filename , params , normalize = False ) :
204	labels = measure . label ( binary_image , background = 0 ) l_max = largest_label_volume ( labels , bg = 0 ) if l_max is not None : binary_image [ labels != l_max ] = 0 return binary_image
1116	if min_rating is None : min_rating = min ( ratings ) if max_rating is None : max_rating = max ( ratings ) num_ratings = int ( max_rating - min_rating + 1 ) hist_ratings = [ 0 for x in range ( num_ratings ) ] for r in ratings : hist_ratings [ r - min_rating ] += 1 return hist_ratings def quadratic_weighted_kappa ( y , y_pred ) :
1782	tf_vectorizer = LemmaCountVectorizer ( max_df = 0.95 , min_df = 2 , stop_words = 'english' , decode_error = 'ignore' ) tf = tf_vectorizer . fit_transform ( text ) print ( tf_vectorizer . get_feature_names ( ) )
1524	def __init__ ( self , example_id , candidates ) : self . example_id = example_id self . candidates = candidates self . results = { } self . features = { }
413	import numpy as np import pandas as pd from scipy import stats import os , gc
505	def plot_jointplot ( train_flat , cols , col_y ) : for col in cols : fig = plt . figure ( figsize = ( 15 , 15 ) ) sns . set_style ( "whitegrid" ) sns . jointplot ( col , col_y , data = train_flat ) plt . xlabel ( col ) plt . ylabel ( 'log of transaction revenue' ) fig . show ( ) col_y = train_flat [ 'totals.transactionRevenueLogNAN' ] cat_cols = [ 'totals.hits' , 'visitNumber' , 'totals.pageviews' , 'totals.bounces' , 'totals.newVisits' , 'visitStartTime' ]
1526	score = - 10000.0 short_span = Span ( - 1 , - 1 ) long_span = Span ( - 1 , - 1 ) summary = ScoreSummary ( ) if predictions : score , _ , summary , start_span , end_span = sorted ( predictions , reverse = True ) [ 0 ] short_span = Span ( start_span , end_span ) for c in example . candidates : start = short_span . start_token_idx end = short_span . end_token_idx
705	train2 = train [ train [ 'wheezy-copper-turtle-magic' ] == i ] test2 = test [ test [ 'wheezy-copper-turtle-magic' ] == i ] idx1 = train2 . index ; idx2 = test2 . index train2 . reset_index ( drop = True , inplace = True )
495	PARENT_DATA_DIR_PATH = '../input' METADATA_TRAIN_FILE_PATH = os . path . join ( PARENT_DATA_DIR_PATH , "metadata_train.csv" ) TRAIN_DATA_FILE_PATH = os . path . join ( PARENT_DATA_DIR_PATH , "train.parquet" )
562	params = { 'bagging_fraction' : 0.7982116702024386 , 'feature_fraction' : 0.1785051643813966 , 'max_depth' : int ( 49.17611603427576 ) , 'min_child_weight' : 3.2852905549011155 , 'min_data_in_leaf' : int ( 31.03480802715621 ) , 'n_estimators' : 5000 , 'num_leaves' : int ( 52.851307790411965 ) , 'reg_alpha' : 0.45963319421692145 , 'reg_lambda' : 0.6591286807489907 , 'metric' : 'auc' , 'boosting_type' : 'gbdt' , 'colsample_bytree' : .8 , 'subsample' : .9 , 'min_split_gain' : .01 , 'max_bin' : 127 , 'bagging_freq' : 5 , 'learning_rate' : 0.01 , 'early_stopping_rounds' : 100 }
535	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
748	checkpoint = torch . load ( checkpoint_path ) model . load_state_dict ( checkpoint [ 'model_state_dict' ] ) del checkpoint gc . collect ( ) model . eval ( ) ; return model . cuda ( ) net = load_net ( WEIGHTS_FILE )
1065	TRAIN_BATCH_SIZE = 32 EVAL_BATCH_SIZE = 8 LEARNING_RATE = 2e-5 NUM_TRAIN_EPOCHS = 4.0 WARMUP_PROPORTION = 0.1 MAX_SEQ_LENGTH = 128
1516	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
1167	import numpy as np import pandas as pd import time from sklearn . preprocessing import Imputer from sklearn . preprocessing import RobustScaler import seaborn as sns import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( "ignore" ) import xgboost as xgb from sklearn import preprocessing import os print ( os . listdir ( "../input" ) )
1620	total = train . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( train . isnull ( ) . sum ( ) / train . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] )
391	ix = 3 test_image = X_test [ ix ] . astype ( float ) imshow ( test_image ) plt . show ( )
345	def my_generator ( ) : while True : for i in range ( 0 , 4 ) : yield i infinity_gen = my_generator ( )
1529	input_paths = tf . io . gfile . glob ( input_pattern ) final_dict = { } for input_path in input_paths : final_dict . update ( read_candidates_from_one_split ( input_path ) ) return final_dict
1237	image_string_placeholder = tf . placeholder ( tf . string ) decoded_image = tf . image . decode_jpeg ( image_string_placeholder ) decoded_image_float = tf . image . convert_image_dtype ( image = decoded_image , dtype = tf . float32 )
1143	by_date [ 'prior_confirmed' ] = 0 by_date [ 'prior_deaths' ] = 0 by_date [ 'prior_recovered' ] = 0 by_date [ 'daily_confirmed' ] = 0 by_date [ 'daily_deaths' ] = 0 by_date [ 'daily_recovered' ] = 0 p_confirmed = 0 p_deaths = 0 p_recovered = 0 for i , row in by_date . iterrows ( ) :
1719	np . random . seed ( SEED ) trn_idx = np . random . permutation ( len ( train_X ) ) train_X = train_X [ trn_idx ] train_y = train_y [ trn_idx ] features = features [ trn_idx ] return train_X , test_X , train_y , features , test_features , tokenizer . word_index
65	if filename in pneumonia_locations : pneumonia_locations [ filename ] . append ( location ) else : pneumonia_locations [ filename ] = [ location ]
585	fig , ax1 = plt . subplots ( ) fig . set_size_inches ( 20 , 10 ) merged [ "yearbuilt" ] = merged [ "yearbuilt" ] . map ( lambda x : str ( x ) . split ( "." ) [ 0 ] ) yearMerged = merged . groupby ( [ 'yearbuilt' , 'numberofstories' ] ) [ "parcelid" ] . count ( ) . unstack ( 'numberofstories' ) . fillna ( 0 ) yearMerged . plot ( kind = 'bar' , stacked = True , ax = ax1 )
1286	import time blur_list = [ ] blur_list_id = [ ] start_time = time . time ( ) ; for i , image_id in enumerate ( tqdm ( train_df [ 'id_code' ] ) ) : img = preprocess_image ( f'../input/train_images/{image_id}.png' ) if ( not isClear ( img ) ) : blur_list . append ( i ) blur_list_id . append ( image_id ) train_df = train_df . drop ( blur_list ) print ( f'Cost: {time.time() - start_time}:.3% seconds' ) ;
1003	feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'app_train' , agg_primitives = [ 'mean' , 'max' , 'min' , 'trend' , 'mode' , 'count' , 'sum' , 'percent_true' , NormalizedModeCount , MostRecent , LongestSeq ] , trans_primitives = [ 'diff' , 'cum_sum' , 'cum_mean' , 'percentile' ] , where_primitives = [ 'mean' , 'sum' ] , seed_features = [ late_payment , past_due ] , max_depth = 2 , features_only = False , verbose = True , chunk_size = len ( app_train ) , ignore_entities = [ 'app_test' ] )
1213	tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu )
1441	N = test_df . shape [ 0 ] x_test = np . empty ( ( N , imSize * imSize ) , dtype = np . uint8 ) for i , Patient in enumerate ( tqdm ( test_df [ 'Patient' ] ) ) : x_test [ i , : ] = process_patient_images ( f'../input/osic-pulmonary-fibrosis-progression/train/{Patient}' )
1617	input_x = model . layers [ 0 ] . input output_layer = model . layers [ - 1 ] . input model1 = Model ( input_x , output_layer ) c = optimizers . adam ( lr = 0.001 )
709	preds_QDA_PL_label_slice = preds_QDA_PL_label [ list ( idx2 [ : n ] ) + list ( idx2 [ - n : ] ) ] skf = StratifiedKFold ( n_splits = 25 , random_state = 42 , shuffle = True ) for train_index , test_index in skf . split ( train3p , train2p [ 'target' ] ) : test_index3 = test_index [ test_index < len ( train3 ) ] test_index4 = test_index [ test_index >= len ( train3 ) ]
1028	gc . enable ( ) del model , train_features , valid_features gc . collect ( )
0	if 'ID00165637202237320314458' in image_file : image_data = pydicom . read_file ( image_file ) . pixel_array axes [ image_index ] . imshow ( image_data , cmap = plt . cm . bone ) image_name = '-' . join ( image_file . split ( '/' ) [ - 2 : ] ) axes [ image_index ] . set_title ( f'{image_name}' ) image_index += 1 train_image_path = '/kaggle/input/osic-pulmonary-fibrosis-progression/train' train_image_files = sorted ( glob . glob ( os . path . join ( train_image_path , '*' , '*.dcm' ) ) )
1258	train_resized_imgs = [ ] test_resized_imgs = [ ] for image_id in label_df [ 'id' ] : train_resized_imgs . append ( pad_and_resize ( image_id , 'train' ) ) for image_id in submission_df [ 'Id' ] : test_resized_imgs . append ( pad_and_resize ( image_id , 'test' ) )
606	precisions_for_samples_by_classes = np . zeros ( ( num_samples , num_classes ) ) for sample_num in range ( num_samples ) : pos_class_indices , precision_at_hits = ( _one_sample_positive_class_precisions ( scores [ sample_num , : ] , truth [ sample_num , : ] ) ) precisions_for_samples_by_classes [ sample_num , pos_class_indices ] = ( precision_at_hits ) labels_per_class = np . sum ( truth > 0 , axis = 0 ) weight_per_class = labels_per_class / float ( np . sum ( labels_per_class ) )
620	areas = [ ] for c in contours : areas . append ( cv2 . contourArea ( c ) )
1179	new_conf_mtx = tf . math . confusion_matrix ( labels = y_true , predictions = y_pred , num_classes = self . num_classes , weights = sample_weight )
1717	df_train = pd . read_csv ( "../input/train.csv" ) df_test = pd . read_csv ( "../input/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
1025	model = lgb . LGBMClassifier ( n_estimators = 10000 , objective = 'binary' , class_weight = 'balanced' , learning_rate = 0.05 , reg_alpha = 0.1 , reg_lambda = 0.1 , subsample = 0.8 , n_jobs = - 1 , random_state = 50 )
1316	for col in atoms . columns : if col . startswith ( 'atom_' ) : atoms [ col ] = atoms [ col ] . fillna ( 0 ) . astype ( 'int8' ) atoms [ 'molecule_index' ] = atoms [ 'molecule_index' ] . astype ( 'int32' ) full = add_atoms ( base , atoms ) add_distances ( full ) full . sort_values ( 'id' , inplace = True ) return full
1037	return round ( sys . getsizeof ( df ) / 1e9 , 2 ) def convert_types ( df , print_info = False ) : original_memory = df . memory_usage ( ) . sum ( )
1128	train_df [ 'coverage' ] = np . mean ( y_train / 255. , axis = ( 1 , 2 ) ) train_df [ 'coverage_class' ] = train_df . coverage . map ( cov_to_class )
636	fitting_model = pm . Deterministic ( 'seir2_model' , seir_ode_solver_wrapper ( theano . shared ( data_time ) , theano . shared ( infected_individuals ) , theano . shared ( np . array ( y0_seir ) ) , beta , gamma ) )
814	X_train , X_valid , y_train , y_valid = train_test_split ( train_selected , train_labels , test_size = 1000 , random_state = 10 )
924	plt . vlines ( importance_index + 1 , ymin = 0 , ymax = 1.2 , linestyles = '--' , colors = 'red' ) plt . show ( ) ; print ( '{} features required for {:.0f}% of cumulative importance.' . format ( importance_index + 1 , 100 * threshold ) ) return df
370	gradient_boosting = GradientBoostingRegressor ( ** params ) gradient_boosting . fit ( train , target ) acc_model ( 9 , gradient_boosting , train , test )
849	feature_importances = pd . DataFrame ( { 'feature' : features , 'importance' : model . feature_importances_ } ) . \ sort_values ( 'importance' , ascending = False ) . set_index ( 'feature' ) if return_model : return sub , feature_importances , model return sub , feature_importances
335	print ( df_train . shape ) print ( df_val . shape ) print ( df_test . shape )
730	logger . info ( ">> Extracting {} time calculation features..." . format ( agg_suffix ) ) GROUP_BY_NEXT_CLICKS = [ { 'groupby' : [ 'ip' , 'channel' ] } , { 'groupby' : [ 'ip' , 'os' ] } ]
592	train = pd . read_csv ( '../input/train.csv' , low_memory = False , index_col = 'id' ) test = pd . read_csv ( '../input/test.csv' , low_memory = False , index_col = 'id' ) res = pd . read_csv ( '../input/resources.csv' , low_memory = False , index_col = 'id' )
242	country_name = "Albania" march_day = 0 day_start = 39 + march_day dates_list2 = dates_list [ march_day : ] plot_rreg_basic_country ( data , country_name , dates_list2 , day_start , march_day )
216	train = pd . DataFrame ( preprocessing . MinMaxScaler ( ) . fit_transform ( train ) , columns = train . columns , index = train . index )
1176	img_list = [ ] for fi in range ( N ) : img_list . append ( mpimg . imread ( name + '/shot' + str ( fi ) + '.png' ) ) shutil . rmtree ( name ) imageio . mimsave ( name + ".gif" , img_list , duration = 0.5 )
1018	print ( "Your selected dataframe has " + str ( df . shape [ 1 ] ) + " columns.\n" "There are " + str ( mis_val_table_ren_columns . shape [ 0 ] ) + " columns that have missing values." )
934	model = lgb . LGBMClassifier ( ** grid_search_params , random_state = 42 ) model . fit ( train_features , train_labels ) preds = model . predict_proba ( test_features ) [ : , 1 ] print ( 'The best model from grid search scores {:.5f} ROC AUC on the test set.' . format ( roc_auc_score ( test_labels , preds ) ) )
1117	optR = OptimizedRounder ( ) optR . fit ( oof_train , X_train [ 'AdoptionSpeed' ] . values ) coefficients = optR . coefficients ( ) pred_test_y_k = optR . predict ( oof_train , coefficients ) print ( "\nValid Counts = " , Counter ( X_train [ 'AdoptionSpeed' ] . values ) ) print ( "Predicted Counts = " , Counter ( pred_test_y_k ) ) print ( "Coefficients = " , coefficients ) qwk = quadratic_weighted_kappa ( X_train [ 'AdoptionSpeed' ] . values , pred_test_y_k ) print ( "QWK = " , qwk )
1669	seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )
407	results = pool . map ( variable_to_bin , features ) pool . close ( ) pool . join ( )
1104	cred_card_bal = pd . read_csv ( "../input/credit_card_balance.csv" ) cred_card_bal = cred_card_bal . drop ( [ 'SK_ID_PREV' ] , axis = 1 ) cred_card_bal_dfs = feature_aggregator_on_df ( cred_card_bal , aggs_cat , aggs_num , [ 'SK_ID_CURR' ] , 'cred_card_balance' , 'basic' , save = False )
680	length = 5 labels = [ ] for label in df_train [ "labels" ] :
1194	sub_prediction_train = pd . Series ( data = gbm . predict ( train_X , num_iteration = model . best_iteration_ ) ) . copy ( ) sub_prediction_train [ sub_prediction_train < 0 ] = 0 predictions_train [ 'Predictions_{}' . format ( k ) ] = sub_prediction_train . values . copy ( ) print ( predictions_train . groupby ( 'fullVisitorId' ) . count ( ) . info ( ) )
890	threshold = 0.9 corr_matrix = train . corr ( ) . abs ( ) corr_matrix . head ( )
1335	if self . has_se : x_squeezed = F . adaptive_avg_pool2d ( x , 1 ) x_squeezed = self . _se_expand ( relu_fn ( self . _se_reduce ( x_squeezed ) ) ) x = torch . sigmoid ( x_squeezed ) * x x = self . _bn2 ( self . _project_conv ( x ) )
36	train_log_target = train_df [ [ 'target' ] ] train_log_target [ 'target' ] = np . log ( 1 + train_df [ 'target' ] . values ) train_log_target . describe ( )
827	image_id = 'c14c1e300' image = cv2 . imread ( os . path . join ( BASE_DIR , 'train' , f'{image_id}.jpg' ) , cv2 . IMREAD_COLOR ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )
49	fig . quad ( top = 'Count' , bottom = 'bottom' , left = 'left' , right = 'right' , source = ColumnDataSource ( grouped ) , color = bar_colors , legend = dataset_name . capitalize ( ) ) fig . legend . location = 'bottom_left' figs . append ( fig ) show ( row ( figs ) )
1569	lgbBO = BayesianOptimization ( lgb_eval , { 'num_leaves' : ( 22 , 50 ) , 'feature_fraction' : ( 0.01 , 0.9 ) , 'bagging_fraction' : ( 0.8 , 1 ) , 'min_split_gain' : ( 0.001 , 0.1 ) , 'min_child_weight' : ( 5 , 50 ) , 'learning_rate' : ( 0.001 , 0.01 ) , 'num_threads' : ( 6 , 10 ) , 'min_data_in_leaf' : ( 60 , 100 ) , 'min_sum_hessian_in_leaf' : ( 5.0 , 15.0 ) } , random_state = 0 )
600	mask_dir = df [ df . id == imid ] . masks . values [ 0 ] masks = os . listdir ( mask_dir ) masks [ : 10 ]
1466	model . eval ( ) with torch . no_grad ( ) : preds = np . empty ( 0 ) for x , _ in tqdm_notebook ( tloader ) : x = x . to ( device ) output = model ( x ) idx = output . max ( dim = - 1 ) [ 1 ] . cpu ( ) . numpy ( ) preds = np . append ( preds , idx , axis = 0 )
248	train0b = train0 train_target0b = train0b [ target_name ] train0b = train0b . drop ( [ target_name ] , axis = 1 )
1250	pickle . dump ( best_hp , open ( 'best_hp.pickle' , 'wb' ) ) best_model = tuner . get_best_models ( 1 ) [ 0 ] best_model . save ( 'best_model.h5' )
978	ax . barh ( list ( reversed ( list ( df . index [ : 15 ] ) ) ) , df [ 'importance_normalized' ] . head ( 15 ) , align = 'center' , edgecolor = 'k' )
1366	for filename in os . listdir ( './pd_data/' ) : if re . match ( ".+\.shp" , filename ) : pd_districts = gpd . read_file ( './pd_data/' + filename ) break
1577	continuous_features = [ col for col in df_train . columns if col not in binary_cat_features ] continuous_features = [ col for col in continuous_features if col not in features_object ] continuous_features = [ col for col in continuous_features if col not in [ 'Id' , 'Target' , 'idhogar' ] ]
1276	for i in range ( num_examples ) : this_input = np . array ( train_tasks [ self . task_num ] [ 'train' ] [ i ] [ 'input' ] ) this_output = np . array ( train_tasks [ self . task_num ] [ 'train' ] [ i ] [ 'output' ] )
724	imputer = SimpleImputer ( strategy = 'mean' ) retain_full = pd . DataFrame ( imputer . fit_transform ( full_data [ retain_cols ] ) , columns = retain_cols ) retain_full = retain_full / retain_full . max ( )
1247	truncated = hidden [ : , : self . pred_len ] out = L . Dense ( 5 , activation = 'linear' ) ( truncated ) model = tf . keras . Model ( inputs = inputs , outputs = out ) model . compile ( tf . keras . optimizers . Adam ( lr ) , loss = self . MCRMSE ) return model
404	nn_vec = np . ones ( N ) best = np . zeros ( N , dtype = float ) last = np . zeros ( N , dtype = int )
570	fig , ax = plt . subplots ( ) fig . set_size_inches ( 20 , 5 ) ordersDay = orders [ [ "order_dow" ] ] . replace ( { 0 : "Sunday" , 1 : "Monday" , 2 : "Tuesday" , 3 : "Wednesday" , 4 : "Thursday" , 5 : "Friday" , 6 : "Saturday" } ) sn . countplot ( color = " ax.set(xlabel='Day Of The Week',title=" Order Count Across Days Of The Week " )
1501	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
1730	tst_leak = pd . read_csv ( '../input/breaking-lb-fresh-start-with-lag-selection/test_leak.csv' ) test [ 'leak' ] = tst_leak [ 'compiled_leak' ] test [ 'log_leak' ] = np . log1p ( tst_leak [ 'compiled_leak' ] )
1240	sample_submission_df = pd . read_csv ( '../input/sample_submission.csv' ) image_ids = sample_submission_df [ 'ImageId' ] predictions = [ ] for image_id in tqdm ( image_ids ) : image_path = f'../input/test/{image_id}.jpg' with tf . gfile . Open ( image_path , "rb" ) as binfile : image_string = binfile . read ( ) result_out = sess . run ( detector_output , feed_dict = { image_string_placeholder : image_string } ) predictions . append ( format_prediction_string ( image_id , result_out ) ) sess . close ( )
518	df_wins = pd . DataFrame ( ) df_wins [ 'SeedDiff' ] = df_tourney [ 'SeedDiff' ] df_wins [ 'Result' ] = 1 df_losses = pd . DataFrame ( ) df_losses [ 'SeedDiff' ] = - df_tourney [ 'SeedDiff' ] df_losses [ 'Result' ] = 0 df_predictions = pd . concat ( ( df_wins , df_losses ) ) df_predictions . head ( )
1158	FIGSIZE = figsize SPACING = 0.1 subplot = ( rows , cols , 1 ) if rows < cols : plt . figure ( figsize = ( FIGSIZE , FIGSIZE / cols * rows ) ) else : plt . figure ( figsize = ( FIGSIZE / rows * cols , FIGSIZE ) )
615	def __init__ ( self ) : self . coef_ = 0 def _f1_loss ( self , coef , X , y ) :
1605	etc_ordianal_features = [ 'ps_ind_01' , 'ps_ind_03' , 'ps_ind_14' , 'ps_ind_15' , 'ps_reg_01' , 'ps_reg_02' , 'ps_car_11' , 'ps_calc_01' , 'ps_calc_02' , 'ps_calc_03' , 'ps_calc_04' , 'ps_calc_05' , 'ps_calc_06' , 'ps_calc_07' , 'ps_calc_08' , 'ps_calc_09' , 'ps_calc_10' , 'ps_calc_11' , 'ps_calc_12' , 'ps_calc_13' , 'ps_calc_14' ] etc_continuous_features = [ 'ps_reg_03' , 'ps_car_12' , 'ps_car_13' , 'ps_car_14' , 'ps_car_15' ] train_null_columns = train_null . columns test_null_columns = test_null . columns
526	best_algo = gcv best_algo . fit ( X_train , y_train ) train_acc = accuracy_score ( y_true = y_train , y_pred = best_algo . predict ( X_train ) ) test_acc = accuracy_score ( y_true = y_test , y_pred = best_algo . predict ( X_test ) ) print ( 'Training Accuracy: %.2f%%' % ( 100 * train_acc ) ) print ( 'Test Accuracy: %.2f%%' % ( 100 * test_acc ) ) print ( )
824	image = image . copy ( ) self . img_height , self . img_width , _ = image . shape for i in range ( self . number ) : cutout_arr , cutout_size , cutout_pos = self . _get_cutout ( self . img_height , self . img_width )
818	test_df [ '%s_c1' % name ] = test_reduction [ : , 0 ] test_df [ '%s_c2' % name ] = test_reduction [ : , 1 ] test_df [ '%s_c3' % name ] = test_reduction [ : , 2 ]
714	m , n = df . shape miss_count = [ ] for col in df . columns : x = df [ col ] . isnull ( ) . sum ( ) miss_count . append ( x ) miss_count_rate = np . array ( miss_count ) / m
1273	this_object += 1 return objectness def identify_object_by_color_isolation ( self , true_image , background = 0 ) :
390	IMG_WIDTH = 128 IMG_HEIGHT = 128 IMG_CHANNELS = 3 TRAIN_PATH = '../input/stage1_train/' TEST_PATH = '../input/stage1_test/' warnings . filterwarnings ( 'ignore' , category = UserWarning , module = 'skimage' ) seed = 42 random . seed = seed np . random . seed = seed
1511	input_layer = tf . keras . layers . InputLayer ( input_shape = ( IMAGE_SIZE [ 0 ] , IMAGE_SIZE [ 1 ] , 3 ) , name = 'input_layer' ) data_augmentation_layer = Data_Augmentation_Dummy ( ) with strategy . scope ( ) : model = tf . keras . Sequential ( [ input_layer , data_augmentation_layer ] )
1796	df_date_index = times_series_means [ [ 'date' , 'Visits' ] ] . set_index ( 'date' ) def test_stationarity ( timeseries ) : plt . figure ( figsize = ( 50 , 8 ) )
928	subsample = 1.0 if boosting_type == 'goss' else random . sample ( param_grid [ 'subsample' ] , 1 ) [ 0 ] print ( 'Boosting type: ' , boosting_type ) print ( 'Subsample ratio: ' , subsample )
1281	import tensorflow as tf from sklearn . metrics import confusion_matrix , accuracy_score , classification_report import seaborn as sn import albumentations as albu from sklearn . model_selection import train_test_split , KFold from tqdm import tqdm_notebook import gc import os import warnings warnings . filterwarnings ( 'ignore' ) main_dir = '../input/Kannada-MNIST/' tf . keras . __version__
858	plt . scatter ( xv , yv , s = 0.02 , c = 'r' , marker = '.' , label = 'Predicted' ) plt . scatter ( xtrue , ytrue , s = 0.02 , c = 'b' , marker = '.' , label = 'True' ) plt . title ( 'ECDF of Predicted and Actual Validation' ) plt . legend ( markerscale = 100 , prop = { 'size' : 20 } ) ;
418	bold ( '**Preview of building data**' ) display ( building . head ( 3 ) ) bold ( '**Preview of Weather Train Data:**' ) display ( weather_train . head ( 3 ) ) bold ( '**Preview of Weather Test Data:**' ) display ( weather_test . head ( 3 ) ) bold ( '**Preview of Train Data:**' ) display ( train . head ( 3 ) ) bold ( '**Preview of Test Data:**' ) display ( test . head ( 3 ) )
1077	words = letters_only . lower ( ) . split ( ) if remove_stopwords : stops = set ( stopwords . words ( "english" ) ) meaningful_words = [ w for w in words if not w in stops ] words = meaningful_words return words sentences_train = train [ 'comment_text' ] . apply ( text_to_words , remove_stopwords = False ) sentences_test = test [ 'comment_text' ] . apply ( text_to_words , remove_stopwords = False )
904	gc . enable ( ) del model , train_features , valid_features gc . collect ( )
40	train = pd . read_csv ( filepath_or_buffer = '../input/train.csv' , index_col = 'id' , parse_dates = [ 'pickup_datetime' , 'dropoff_datetime' ] , infer_datetime_format = True ) test = pd . read_csv ( filepath_or_buffer = '../input/test.csv' , index_col = 'id' , parse_dates = [ 'pickup_datetime' ] , infer_datetime_format = True )
1209	if not os . path . isdir ( save_dir ) : os . makedirs ( save_dir ) model_path = os . path . join ( save_dir , model_name ) model . save ( model_path ) print ( 'Saved trained model at %s ' % model_path )
942	results . sort_values ( 'score' , ascending = False , inplace = True ) results . reset_index ( inplace = True ) return results
595	device = torch . device ( "cuda:0" if torch . cuda . is_available ( ) else "cpu" ) fullModelArhitecture = ModelArhitecture ( ) clf = PandaAlgorithm ( model = fullModelArhitecture , fold = fold ) model = clf . train ( dataloaders , num_epochs = num_epochs ) fold += 1 torch . cuda . empty_cache ( ) gc . collect ( ) del model
854	for i , d in enumerate ( [ 'day' , 'week' , 'month' , 'year' ] ) : ax = axes [ i ] sns . regplot ( f'pickup_frac_{d}' , 'fare_amount' , data = data . sample ( 100000 , random_state = RSEED ) , fit_reg = False , scatter_kws = { 'alpha' : 0.05 } , marker = '.' , ax = ax , color = 'r' ) ax . set_title ( f'Fare Amount vs pickup_frac_{d}' )
539	predictions = best_algo . predict ( X_test ) probability = best_algo . predict_proba ( X_test ) print ( classification_report ( y_test , predictions ) ) print ( ) print ( confusion_matrix ( y_test , predictions ) ) print ( )
460	start_time = timer ( None ) random_search . fit ( X , Y ) timer ( start_time )
1760	def process_dataframe ( input_df , encoder_dict = None ) : print ( 'Label encoding categorical features...' ) categorical_feats = input_df . columns [ input_df . dtypes == 'object' ] for feat in categorical_feats : encoder = LabelEncoder ( ) input_df [ feat ] = encoder . fit_transform ( input_df [ feat ] . fillna ( 'NULL' ) ) print ( 'Label encoding complete.' ) return input_df , categorical_feats . tolist ( ) , encoder_dict
1357	N = 10 scores = np . zeros ( N , ) for i in range ( N ) : p = i * 1. / N v = p * leak_df [ 'pred1' ] . values + ( 1. - p ) * leak_df [ 'pred3' ] . values vl1p = np . log1p ( v ) scores [ i ] = np . sqrt ( mean_squared_error ( vl1p , leak_df . meter_reading_l1p ) )
975	hyp_df [ 'iteration' ] = results [ 'iteration' ] hyp_df [ 'score' ] = results [ 'score' ] return hyp_df
1568	qs = PINBALL_QUANTILE q = tf . constant ( np . array ( [ qs ] ) , dtype = tf . float32 ) e = y_true - y_pred v = tf . maximum ( q * e , ( q - 1 ) * e ) return K . backend . mean ( v )
474	subm = pd . DataFrame ( ) subm [ 'id' ] = ids subm [ 'target' ] = predictions subm . to_csv ( data_path + 'lgbm_submission.csv.gz' , compression = 'gzip' , index = False , float_format = '%.5f' ) print ( 'Done!' )
53	fig . quad ( top = 'Count' , bottom = 'bottom' , left = 'left' , right = 'right' , source = ColumnDataSource ( grouped ) , color = bar_colors , legend = dataset_name . capitalize ( ) ) fig . legend . location = 'top_left' show ( fig )
1703	best_candidates_items = list ( best_candidates . items ( ) ) for best_score , best_candidate in best_candidates_items : if product_less ( score , best_score ) :
1173	for n in range ( len ( slices ) ) : intercept = slices [ n ] . RescaleIntercept slope = slices [ n ] . RescaleSlope if slope != 1 : images [ n ] = slope * images [ n ] . astype ( np . float64 ) images [ n ] = images [ n ] . astype ( np . int16 ) images [ n ] += np . int16 ( intercept ) return np . array ( images , dtype = np . int16 )
650	filters = 250 kernel_size = 3 hidden_dims = 250
165	thresh_val = threshold_otsu ( im_gray ) mask = np . where ( im_gray > thresh_val , 1 , 0 ) if np . sum ( mask == 0 ) < np . sum ( mask == 1 ) : mask = np . where ( mask , 0 , 1 ) labels , nlabels = ndimage . label ( mask ) labels , nlabels = ndimage . label ( mask )
109	ax1 . plot ( data . index , data [ col ] , label = "price of {}" . format ( col ) , color = color , linewidth = 0.5 ) ax1 . plot ( data . index , data [ col ] . rolling ( lag // 12 ) . mean ( ) , label = "Rolling {}" . format ( lag // 12 ) , color = color , linewidth = 2 ) ax1 . set_xlabel ( "date" ) ax1 . set_ylabel ( "price" ) ax1 . set_title ( "raw data" ) ax1 . legend ( )
1648	validate_timeseries , validate_timeseries0 , validate_void , validate_meta , validate_switch , validate_y = \ calculate_inputs ( df_timeseries , df_validation_meta [ f ] , meta_cols , col_weights , real_targets , length = num_samples , aug = None , return_y = True ) tq = tqdm_notebook ( range ( training_trails ) , desc = 'full:' )
1740	plt . figure ( figsize = [ 10 , 6 ] ) df_train [ 'DBNOs' ] . value_counts ( ) . plot ( kind = 'bar' ) plt . title ( "Distribution of DBNOs" ) plt . ylabel ( "count" ) plt . show ( ) print ( df_train [ 'DBNOs' ] . value_counts ( ) )
869	OUT_FILE = 'bayes_test.csv' of_connection = open ( OUT_FILE , 'w' ) writer = csv . writer ( of_connection ) ITERATION = 0
1766	from sklearn . model_selection import StratifiedKFold from sklearn . metrics import f1_score from torch . optim . optimizer import Optimizer from unidecode import unidecode
673	dftrain2 = dflag [ masktrain ] . join ( dfcountry [ masktrain ] ) dfvalid2 = dflag [ ~ masktrain ] . join ( dfcountry [ ~ masktrain ] ) if ic > 0 : inpfeature += order_countries [ : ic ] for ic2 in range ( ic ) : dftrain2 = dftrain2 . join ( list_train2 [ ic2 ] ) dfvalid2 = dfvalid2 . join ( list_valid2 [ ic2 ] )
1261	def create_test_gen ( batch_size = 64 ) : return ImageDataGenerator ( rescale = 1 / 255. ) . flow_from_dataframe ( test_imgs , directory = '../input/severstal-steel-defect-detection/test_images' , x_col = 'ImageId' , class_mode = None , target_size = ( 256 , 256 ) , batch_size = batch_size , shuffle = False )
625	df_brazil_cases_by_day = df_grouped_brazil [ df_grouped_brazil . confirmed > 0 ] df_brazil_cases_by_day = df_brazil_cases_by_day . reset_index ( drop = True ) df_brazil_cases_by_day [ 'day' ] = df_brazil_cases_by_day . date . apply ( lambda x : ( x - df_brazil_cases_by_day . date . min ( ) ) . days ) reordered_columns = [ 'date' , 'day' , 'confirmed' , 'deaths' , 'confirmed_marker' , 'deaths_marker' ] df_brazil_cases_by_day = df_brazil_cases_by_day [ reordered_columns ] df_brazil_cases_by_day
506	def plot_box ( train_flat , cols , col_y ) : for col in cols : fig = plt . figure ( figsize = ( 150 , 25 ) ) sns . set_style ( "whitegrid" ) g = sns . boxplot ( col , col_y , data = train_flat ) plt . xlabel ( col ) plt . ylabel ( 'log of transaction revenue' ) for item in g . get_xticklabels ( ) : item . set_rotation ( 90 ) fig . show ( ) col_y = train_flat [ 'totals.transactionRevenueLog' ] cat_cols = [ 'geoNetwork.region' ]
500	df [ 'Subject' ] , df [ 'Zone' ] = df [ 'Id' ] . str . split ( '_' , 1 ) . str df = df [ [ 'Subject' , 'Zone' , 'Probability' ] ] threat_list = df . loc [ df [ 'Subject' ] == subject_id ] return threat_list
487	application_train = pd . read_csv ( '../input/application_train.csv' ) application_test = pd . read_csv ( '../input/application_test.csv' ) bureau = pd . read_csv ( '../input/bureau.csv' ) bureau_balance = pd . read_csv ( '../input/bureau_balance.csv' ) POS_CASH_balance = pd . read_csv ( '../input/POS_CASH_balance.csv' ) credit_card_balance = pd . read_csv ( '../input/credit_card_balance.csv' ) previous_application = pd . read_csv ( '../input/previous_application.csv' ) installments_payments = pd . read_csv ( '../input/installments_payments.csv' )
331	for j in range ( 0 , len ( batch ) ) : patientId = batch [ j ] path = \ '../input/rsna-pneumonia-detection-challenge/stage_1_train_images/%s.dcm' % patientId dcm_data = pydicom . read_file ( path )
269	train_dense_players = np . reshape ( train_dense_players , ( len ( train_dense_players ) , - 1 ) ) train_dense = np . hstack ( [ train_dense_players , train_dense_game ] ) train_cat_players = np . reshape ( train_cat_players , ( len ( train_cat_players ) , - 1 ) ) train_cat = np . hstack ( [ train_cat_players , train_cat_game ] ) train_x = np . hstack ( [ train_dense , train_cat ] )
75	folder = '../input/stage_1_train_images' train_gen = generator ( folder , train_filenames , pneumonia_locations , batch_size = 16 , image_size = 256 , shuffle = True , augment = True , predict = False ) valid_gen = generator ( folder , valid_filenames , pneumonia_locations , batch_size = 16 , image_size = 256 , shuffle = False , predict = False ) history = model . fit_generator ( train_gen , validation_data = valid_gen , callbacks = [ learning_rate ] , epochs = 20 , shuffle = True )
1101	gbm_params = { 'objective' : 'binary' , 'boosting_type' : 'gbdt' , 'nthread' : 6 , 'learning_rate' : 0.05 , 'num_leaves' : 20 , 'colsample_bytree' : 0.9497036 , 'subsample' : 0.8715623 , 'subsample_freq' : 1 , 'max_depth' : 8 , 'reg_alpha' : 0.041545473 , 'reg_lambda' : 0.0735294 , 'min_split_gain' : 0.0222415 , 'min_child_weight' : 60 , 'seed' : 0 , 'verbose' : - 1 , 'metric' : 'auc' , } oof_train , oof_test = run_kfold_lgbm ( X_train , y_train , X_test , gbm_params )
577	sample = self . df . loc [ idx , : ] site = sample . site row_id = sample . row_id name = sample . audio_id
521	df_model_winners = pd . merge ( left = df_tourney_list , right = df_tourney_final , how = 'left' , left_on = [ 'Season' , 'WTeamID' ] , right_on = [ 'Season' , 'TeamID' ] ) df_model_winners . drop ( labels = [ 'TeamID' ] , inplace = True , axis = 1 ) df_model_winners . head ( )
1350	start_mem = df . memory_usage ( ) . sum ( ) / 1024 ** 2 print ( 'Memory usage of dataframe is {:.2f} MB' . format ( start_mem ) ) for col in df . columns : if is_datetime ( df [ col ] ) or is_categorical_dtype ( df [ col ] ) :
959	default_agg_primitives = [ "sum" , "std" , "max" , "skew" , "min" , "mean" , "count" , "percent_true" , "num_unique" , "mode" ] default_trans_primitives = [ "day" , "year" , "month" , "weekday" , "haversine" , "numwords" , "characters" ] feature_names = ft . dfs ( entityset = es , target_entity = 'app' , trans_primitives = default_trans_primitives , agg_primitives = default_agg_primitives , max_depth = 2 , features_only = True ) print ( '%d Total Features' % len ( feature_names ) )
1586	train . drop ( columns = [ 'idhogar' , 'Id' , 'agesq' , 'hogar_adul' , 'SQBescolari' , 'SQBage' , 'SQBhogar_total' , 'SQBedjefe' , 'SQBhogar_nin' , 'SQBovercrowding' , 'SQBdependency' , 'SQBmeaned' ] , inplace = True ) test . drop ( columns = [ 'idhogar' , 'Id' , 'agesq' , 'hogar_adul' , 'SQBescolari' , 'SQBage' , 'SQBhogar_total' , 'SQBedjefe' , 'SQBhogar_nin' , 'SQBovercrowding' , 'SQBdependency' , 'SQBmeaned' ] , inplace = True ) correlation = train . corr ( ) correlation = correlation [ 'Target' ] . sort_values ( ascending = False )
657	cols = ret_test_df . filter ( like = '_bb' ) . columns train_X = ret_train_df . drop ( [ 'ID' , 'y' ] , axis = 1 ) train_Y = ret_train_df [ 'y' ] train_Y = train_Y . values test_X = ret_test_df . drop ( [ 'ID' ] , axis = 1 ) test_X = test_X . drop ( cols , axis = 1 )
1153	plt . rcParams [ 'figure.figsize' ] = ( 10 , 10 ) X_tr_temp , y_tr_temp = tr_datagen . __getitem__ ( 0 ) X_valid_temp , y_valid_temp = valid_datagen . __getitem__ ( 0 ) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . imshow ( X_tr_temp [ 0 ] ) ax [ 0 ] . set_title ( 'train:' ) ax [ 1 ] . imshow ( X_valid_temp [ 0 ] ) ax [ 1 ] . set_title ( 'valid:' )
1188	print ( 'Calculate average accuracy of each assessment' ) return { title2num [ title ] : group [ 'accuracy' ] . mean ( ) \ for ( title , group ) in train_labels . groupby ( 'title' , sort = False ) } def gen_assess_corr_rate ( train_labels , title2num ) :
948	bars = alt . Chart ( random_hyp , width = 400 ) . mark_bar ( ) . encode ( x = 'boosting_type' , y = alt . Y ( 'count()' , scale = alt . Scale ( domain = [ 0 , 400 ] ) ) ) bars . title = 'Boosting Type for Random Search' text = bars . mark_text ( align = 'center' , baseline = 'bottom' , size = 20 ) . encode ( text = 'count()' ) bars + text
302	dst = os . path . join ( val_dir , sub_folder , fname ) image = cv2 . imread ( src ) image = cv2 . resize ( image , ( IMAGE_HEIGHT , IMAGE_WIDTH ) ) cv2 . imwrite ( dst , image )
1461	use_cols = [ col for col in train . columns if col not in [ 'card_id' , 'first_active_month' ] ] train = train [ use_cols ] test = test [ use_cols ] features = list ( train [ use_cols ] . columns ) categorical_feats = [ col for col in features if 'feature_' in col ]
87	import random real = [ ] fake = [ ] for m , n in zip ( paths , y ) : if n == 0 : real . append ( m ) else : fake . append ( m ) fake = random . sample ( fake , len ( real ) ) paths , y = [ ] , [ ] for x in real : paths . append ( x ) y . append ( 0 ) for x in fake : paths . append ( x ) y . append ( 1 )
740	res_x [ idx ] += int ( scale * r1 * ratio * abs ( np . random . randn ( ) ) ) res_y [ idx ] += int ( scale * r2 * ratio * abs ( np . random . randn ( ) ) ) res_line = [ res_x , res_y ] return res_line
853	plt . figure ( figsize = ( 10 , 8 ) ) for d , grouped in data . groupby ( 'pickup_Dayofweek' ) : sns . kdeplot ( grouped [ 'fare_amount' ] , label = f'{d}' ) plt . title ( 'Fare Amount by Day of Week' ) ;
274	MAX_LEN = 96 PATH = '../input/tf-roberta/' tokenizer = tokenizers . ByteLevelBPETokenizer ( vocab_file = PATH + 'vocab-roberta-base.json' , merges_file = PATH + 'merges-roberta-base.txt' , lowercase = True , add_prefix_space = True ) EPOCHS = 3 BATCH_SIZE = 32 PAD_ID = 1 SEED = 88888 LABEL_SMOOTHING = 0.1 tf . random . set_seed ( SEED ) np . random . seed ( SEED ) sentiment_id = { 'positive' : 1313 , 'negative' : 2430 , 'neutral' : 7974 } train = pd . read_csv ( '../input/tweet-sentiment-extraction/train.csv' ) . fillna ( '' ) train . head ( )
1217	title_group = ( pd . get_dummies ( group_game_time . drop ( columns = [ 'game_session' , 'event_count' , 'game_time' ] ) , columns = [ 'title' , 'type' , 'world' ] ) . groupby ( [ 'installation_id' ] ) . sum ( ) ) event_game_time_group = ( group_game_time [ [ 'installation_id' , 'event_count' , 'game_time' ] ] . groupby ( [ 'installation_id' ] ) . agg ( [ np . sum , np . mean , np . std , np . min , np . max ] ) )
415	from IPython . display import Markdown def bold ( string ) : display ( Markdown ( string ) )
358	skin_mask = sieve ( skin_like_mask , skin_sieve_min_size ) kernel = np . ones ( ( skin_kernel_size , skin_kernel_size ) , dtype = np . int8 ) skin_mask = cv2 . morphologyEx ( skin_mask , cv2 . MORPH_CLOSE , kernel )
912	import gc def agg_child ( df , parent_var , df_name ) : df_agg = agg_numeric ( df , parent_var , df_name ) df_agg_cat = agg_categorical ( df , parent_var , df_name ) df_info = df_agg . merge ( df_agg_cat , on = parent_var , how = 'outer' ) _ , idx = np . unique ( df_info , axis = 1 , return_index = True ) df_info = df_info . iloc [ : , idx ] gc . enable ( ) del df_agg , df_agg_cat gc . collect ( ) return df_info
1277	arc . reset ( ) arc . identify_object ( image , method = 1 ) arc . plot_identified_objects ( arc . identified_objects , title = 'by color' )
1549	df = pd . merge ( left = df , right = weather_hour . drop_duplicates ( subset = [ 'date' , 'hour' ] ) , on = [ 'date' , 'hour' ] , how = 'left' ) df . drop ( [ 'date' ] , axis = 1 , inplace = True )
1454	inv_yhat = np . concatenate ( ( yhat , test_X [ : , 1 : ] ) , axis = 1 ) inv_yhat = scaler . inverse_transform ( inv_yhat ) inv_yhat = inv_yhat [ : , 0 ]
789	warnings . filterwarnings ( 'ignore' , category = ConvergenceWarning ) warnings . filterwarnings ( 'ignore' , category = DeprecationWarning ) warnings . filterwarnings ( 'ignore' , category = UserWarning )
687	img_path = self . img_paths [ index ] image = Image . open ( img_path ) if self . mode == "train" : labels , masks = self . get_label_and_mask ( img_path . stem )
674	X = dftrain2 [ inpfeature ] y = dftrain2 [ country ] X_valid = dfvalid2 [ inpfeature ] y_valid = dfvalid2 [ country ]
1558	marker_watershed = np . zeros ( ( image . shape [ 0 ] , image . shape [ 1 ] ) , dtype = np . int ) marker_watershed += marker_internal * 255 marker_watershed += marker_external * 128 return marker_internal , marker_external , marker_watershed maskwatershed = MaskWatershed ( min_hu = min ( clip_bounds ) , iterations = MASK_ITERATION )
847	lr . fit ( X_train [ [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] ] , y_train ) evaluate ( lr , [ 'haversine' , 'abs_lat_diff' , 'abs_lon_diff' , 'passenger_count' ] , X_train , X_valid , y_train , y_valid )
1376	import numpy as np import pandas as pd import efficientnet . tfkeras as efn from tensorflow . keras import backend as K import tensorflow_addons as tfa import tensorflow . keras . layers as layers import tensorflow as tf
101	image = cv2 . imread ( im_path + self . _data [ 'image_id' ] [ k ] + '.png' ) image = ( cv2 . resize ( image , self . _dim ) / 255.0 - stats [ 0 ] ) / stats [ 1 ] if self . transform is not None : randint = np . random . rand ( ) if randint <= 0.4 :
747	for name , parameter in backbone . named_parameters ( ) : if all ( [ not name . startswith ( layer ) for layer in layers_to_train ] ) : parameter . requires_grad_ ( False ) return_layers = { 'layer1' : '0' , 'layer2' : '1' , 'layer3' : '2' , 'layer4' : '3' } in_channels_stage2 = backbone . inplanes // 8 in_channels_list = [ in_channels_stage2 , in_channels_stage2 * 2 , in_channels_stage2 * 4 , in_channels_stage2 * 8 , ] out_channels = 256 return BackboneWithFPN ( backbone , return_layers , in_channels_list , out_channels )
1434	if unk == True : labels . append ( word2id [ 'unknown' ] ) else : labels . append ( word2id [ f . split ( '/' ) [ - 2 ] ] ) return data , labels , indexes
1785	dtiny = np . finfo ( 0.0 ) . tiny idx = lta < dtiny lta [ idx ] = dtiny return sta / lta
1439	train_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) test_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' ) sub_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' ) print ( train_df . shape ) print ( test_df . shape ) train_df . head ( )
1494	if FLAGS . do_predict : test_features = ( tf . train . Example . FromString ( r . numpy ( ) ) for r in tf . data . TFRecordDataset ( FLAGS . test_tf_record ) ) predictions_json = get_prediction_json ( mode = 'test' , max_nb_pos_logits = FLAGS . n_best_size )
634	has_to_plot_infection_peak = True if has_to_run_sir : crisis_day_sir = np . argmax ( I_predict_sir ) if has_to_run_sird : crisis_day_sird = np . argmax ( I_predict_sird ) if has_to_run_seir : crisis_day_seir = np . argmax ( I_predict_seir ) if has_to_run_seird : crisis_day_seird = np . argmax ( I_predict_seird ) if has_to_run_seirdq : crisis_day_seirdq = np . argmax ( I_predict_seirdq )
1351	root = Path ( '../input/ashrae-feather-format-for-fast-loading' ) train_df = pd . read_feather ( root / 'train.feather' ) weather_train_df = pd . read_feather ( root / 'weather_train.feather' ) weather_test_df = pd . read_feather ( root / 'weather_test.feather' ) building_meta_df = pd . read_feather ( root / 'building_metadata.feather' )
457	import pandas as pd import numpy as np import warnings warnings . filterwarnings ( 'ignore' ) from datetime import datetime from sklearn . model_selection import RandomizedSearchCV , GridSearchCV from sklearn import metrics from sklearn . metrics import roc_auc_score from sklearn . model_selection import StratifiedKFold from xgboost import XGBClassifier pd . set_option ( 'display.max_columns' , 200 )
319	Xtrain , Xval , Ztrain , Zval = train_test_split ( trainb , targetb , test_size = 0.2 , random_state = 0 ) train_set = lgb . Dataset ( Xtrain , Ztrain , silent = False ) valid_set = lgb . Dataset ( Xval , Zval , silent = False )
442	plt . figure ( figsize = ( 10 , 10 ) ) map2 = df_train [ df_train [ 'City' ] == 'Boston' ] . groupby ( [ 'Latitude' , 'Longitude' ] ) [ 'RowId' ] . count ( ) . reset_index ( ) plt . scatter ( map2 [ 'Longitude' ] , map2 [ 'Latitude' ] , alpha = 0.5 ) mplleaflet . display ( )
1308	[ x1 , x2 , x3 , x4 ] = sess . run ( [ num_frames , video_matrix , batch_labels , batch_frames ] ) [ z1 , z2 ] = sess . run ( [ labels , num_frames ] ) vid_byte = sess . run ( batch_video_ids ) vid = vid_byte [ 0 ] . decode ( ) print ( 'vid = %s' % vid )
1523	oversampled_training_dataset = get_training_dataset_with_oversample ( repeat_dataset = False , oversample = True , augumentation = False ) label_counter_2 = Counter ( ) for images , labels in oversampled_training_dataset : label_counter_2 . update ( labels . numpy ( ) ) del oversampled_training_dataset label_counting_sorted_2 = label_counter_2 . most_common ( ) NUM_TRAINING_IMAGES_OVERSAMPLED = sum ( [ x [ 1 ] for x in label_counting_sorted_2 ] ) print ( "number of examples in the oversampled training dataset: {}" . format ( NUM_TRAINING_IMAGES_OVERSAMPLED ) ) print ( "labels in the oversampled training dataset, sorted by occurrence" ) label_counting_sorted_2
1322	train_input = input_data [ train_index ] cv_input = input_data [ cv_index ] train_target = target_data [ train_index ] cv_target = target_data [ cv_index ] train_target_1 = target_data_1 [ train_index ] cv_target_1 = target_data_1 [ cv_index ]
951	train = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_train.csv' ) test = pd . read_csv ( '../input/home-credit-simple-featuers/simple_features_test.csv' ) test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
1671	train_csv = pd . read_csv ( '../input/train/train.csv' , low_memory = False ) test_csv = pd . read_csv ( '../input/test/test.csv' , low_memory = False ) def preprocess ( csv ) : csv [ 'Description_len' ] = [ len ( str ( tt ) ) for tt in csv [ 'Description' ] ] csv [ 'Name_len' ] = [ len ( str ( tt ) ) for tt in csv [ 'Name' ] ] return csv train_csv = preprocess ( train_csv ) test_csv = preprocess ( test_csv )
1034	_ , idx = np . unique ( agg , axis = 1 , return_index = True ) agg = agg . iloc [ : , idx ] return agg
471	f = plt . figure ( 1 ) fpr , tpr , t = roc_curve ( train_df . iloc [ valid_idx ] [ target ] . values , oof [ valid_idx ] ) tprs . append ( interp ( mean_fpr , fpr , tpr ) ) roc_auc = auc ( fpr , tpr ) aucs . append ( roc_auc ) plt . plot ( fpr , tpr , lw = 2 , alpha = 0.3 , label = 'ROC fold %d (AUC = %0.4f)' % ( i , roc_auc ) )
1271	all_pairs = np . array ( np . where ( image != background ) ) . T objectness = np . zeros ( image . shape ) this_object = 1 while len ( all_pairs ) >= 1 : init_pair = all_pairs [ 0 ] objectness = self . check_neighbors ( all_pairs , init_pair , objectness , this_object )
757	label_counts . plot . bar ( figsize = ( 8 , 6 ) , color = colors . values ( ) , edgecolor = 'k' , linewidth = 2 )
496	target0df = metadata_train [ metadata_train [ 'target' ] == 0 ] target1df = metadata_train [ metadata_train [ 'target' ] == 1 ] print ( "target0data shape:" , target0df . shape ) print ( "target1data shape:" , target1df . shape )
605	def pad_audio ( samples ) : if len ( samples ) >= L : return samples else : return np . pad ( samples , pad_width = ( L - len ( samples ) , 0 ) , mode = 'constant' , constant_values = ( 0 , 0 ) )
1747	app_both [ 'LOAN_INCOME_RATIO' ] = app_both [ 'AMT_CREDIT' ] / app_both [ 'AMT_INCOME_TOTAL' ] app_both [ 'ANNUITY_INCOME_RATIO' ] = app_both [ 'AMT_ANNUITY' ] / app_both [ 'AMT_INCOME_TOTAL' ] app_both [ 'ANNUITY LENGTH' ] = app_both [ 'AMT_CREDIT' ] / app_both [ 'AMT_ANNUITY' ] app_both [ 'WORKING_LIFE_RATIO' ] = app_both [ 'DAYS_EMPLOYED' ] / app_both [ 'DAYS_BIRTH' ] app_both [ 'INCOME_PER_FAM' ] = app_both [ 'AMT_INCOME_TOTAL' ] / app_both [ 'CNT_FAM_MEMBERS' ] app_both [ 'CHILDREN_RATIO' ] = app_both [ 'CNT_CHILDREN' ] / app_both [ 'CNT_FAM_MEMBERS' ]
1769	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
304	class_weights = { 0 : 1.0 , 1 : 1.0 , }
1283	train_label = train_df [ 'label' ] test_indices = test_df [ 'id' ] train_df = train_df . drop ( [ 'label' ] , axis = 1 ) test_df = test_df . drop ( [ 'id' ] , axis = 1 )
318	decision_tree = DecisionTreeRegressor ( ) decision_tree . fit ( train , target ) acc_model ( 5 , decision_tree , train , test )
268	eps = 1e-8 dense_game_features = train_dense . columns [ train_dense [ : 22 ] . std ( ) <= eps ] dense_player_features = train_dense . columns [ train_dense [ : 22 ] . std ( ) > eps ] cat_game_features = train_cat . columns [ train_cat [ : 22 ] . std ( ) <= eps ] cat_player_features = train_cat . columns [ train_cat [ : 22 ] . std ( ) > eps ]
1531	df_prev_agg = df_previous_app [ [ key ] + payment_cols ] . groupby ( key ) . agg ( [ 'sum' , 'mean' ] ) ; df_prev_agg . columns = prev_agg_cols df = pd . merge ( left = df , right = df_prev_agg , left_on = key , right_index = True , how = 'left' )
357	img_1 = get_image_data ( '1023' , 'Type_1' ) img_2 = get_image_data ( '531' , 'Type_1' ) img_3 = get_image_data ( '596' , 'Type_1' ) img_4 = get_image_data ( '1061' , 'Type_1' ) img_5 = get_image_data ( '1365' , 'Type_2' )
133	train_data , eval_data , train_label , eval_label = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( train_data . shape , eval_data . shape , train_label . shape , eval_label . shape )
70	img = np . expand_dims ( img , - 1 ) return img def __getitem__ ( self , index ) :
1311	spent = ( time . time ( ) - start ) / 60 dsp = 'Done with %4d of %4d (Spent %5.1f min)' % ( i + 1 , nb_runs , spent ) print ( dsp , end = '' , flush = True )
153	print ( ip_level [ 'DL_by_click_ratio' ] . describe ( ) ) plt . figure ( figsize = ( 6 , 4 ) ) sns . violinplot ( ip_level [ 'DL_by_click_ratio' ] ) plt . title ( 'Ratio : Download by click' , fontsize = 15 ) plt . xlabel ( 'Download by click' )
1126	mask = self . __load_image ( mask_path , mask = True ) if self . divide : image = image / 255. mask = mask / 255.
1315	ATOMIC_NUMBERS = { 'H' : 1 , 'C' : 6 , 'N' : 7 , 'O' : 8 , 'F' : 9 }
881	test_ids = test [ 'SK_ID_CURR' ] train_labels = np . array ( train [ 'TARGET' ] . astype ( np . int32 ) ) . reshape ( ( - 1 , ) ) train = train . drop ( columns = [ 'SK_ID_CURR' , 'TARGET' ] ) test = test . drop ( columns = [ 'SK_ID_CURR' ] ) print ( 'Training shape: ' , train . shape ) print ( 'Testing shape: ' , test . shape )
1118	coefficients_ = coefficients . copy ( ) coefficients_ [ 0 ] = 1.645 coefficients_ [ 1 ] = 2.115 coefficients_ [ 3 ] = 2.84 train_predictions = optR . predict ( oof_train , coefficients_ ) . astype ( int ) print ( 'train pred distribution: {}' . format ( Counter ( train_predictions ) ) ) test_predictions = optR . predict ( oof_test . mean ( axis = 1 ) , coefficients_ ) print ( 'test pred distribution: {}' . format ( Counter ( test_predictions ) ) )
1580	for col1 in [ 'epared1' , 'epared2' , 'epared3' ] : for col2 in [ 'eviv1' , 'eviv2' , 'eviv3' ] : new_col_name = 'new_{}_x_{}' . format ( col1 , col2 ) df_train [ new_col_name ] = df_train [ col1 ] * df_train [ col2 ] df_test [ new_col_name ] = df_test [ col1 ] * df_test [ col2 ]
195	final_image = cv2 . inpaint ( image_resize , threshold , 1 , cv2 . INPAINT_TELEA ) plt . subplot ( l , 5 , ( i * 5 ) + 5 ) plt . imshow ( cv2 . cvtColor ( final_image , cv2 . COLOR_BGR2RGB ) ) plt . axis ( 'off' ) plt . title ( 'final_image : ' + image_name ) plt . plot ( )
1242	result_out = sess . run ( detector_output , feed_dict = { image_string_placeholder : image_string } ) predictions . append ( format_prediction_string ( image_id , result_out ) ) sess . close ( )
637	import numpy as np import pandas as pd pd . set_option ( 'display.max_colwidth' , - 1 ) import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1565	from skimage . transform import resize from scipy . ndimage import zoom import scipy . ndimage as ndimage from skimage import measure , morphology , segmentation from math import ceil
1628	stats = [ ] for country in sorted ( full_table [ 'Country/Region' ] . unique ( ) ) : df = get_time_series ( country ) if len ( df ) == 0 or ( max ( df [ 'Confirmed' ] ) < 1000 ) : continue print ( '{} COVID-19 Prediction' . format ( country ) ) opt_display_model ( df , stats )
497	reducedtarget0sampleDF = pd . DataFrame ( ) for col in range ( target0sampledata . shape [ 1 ] ) : tmp_pdSeries = reduce_sample ( target0sampledata . iloc [ : , col ] ) reducedtarget0sampleDF [ str ( col ) ] = tmp_pdSeries reducedtarget0sampleDF . shape
666	df = ( pd . merge ( agg . reset_index ( ) , products , on = 'Producto_ID' , how = 'left' ) . groupby ( 'short_name' ) [ 'Demanda_uni_equil_sum' , 'Venta_uni_hoy_sum' , 'Dev_uni_proxima_sum' , 'Dev_uni_proxima_count' ] . sum ( ) . sort_values ( by = 'Demanda_uni_equil_sum' , ascending = False ) )
711	train2 = train [ train [ 'wheezy-copper-turtle-magic' ] == i ] test2 = test [ test [ 'wheezy-copper-turtle-magic' ] == i ] idx1 = train2 . index ; idx2 = test2 . index train2 . reset_index ( drop = True , inplace = True )
1603	import missingno as msno train_null = train train_null = train_null . replace ( - 1 , np . NaN ) msno . matrix ( df = train_null . iloc [ : , : ] , figsize = ( 20 , 14 ) , color = ( 0.8 , 0.5 , 0.2 ) )
753	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) train . head ( )
1820	if not nulls : news_df_expanded = news_df_expanded . loc [ news_valid_indecies ] def news_df_feats ( x ) : if x . name == 'headline' : return list ( x )
1588	pred_labels = predictions . reshape ( len ( np . unique ( truth ) ) , - 1 ) . argmax ( axis = 0 ) f1 = f1_score ( truth , pred_labels , average = 'macro' ) return ( 'macroF1' , f1 , True )
104	model . compile ( optimizer = 'adam' , loss = 'mean_squared_error' , metrics = [ 'mae' ] ) model . fit ( x , Y , batch_size = 64 , epochs = 100 ) model . fit ( x , Y , batch_size = 128 , epochs = 50 ) model . fit ( x , Y , batch_size = 256 , epochs = 50 )
619	areas = [ ] for c in contours : areas . append ( cv2 . contourArea ( c ) )
437	bold ( '**Preview of Train Data:**' ) display ( df_train . head ( ) ) bold ( '**Preview of Test Data:**' ) display ( df_test . head ( ) )
798	X_train = features [ train_indices ] X_valid = features [ valid_indices ] y_train = labels [ train_indices ] y_valid = labels [ valid_indices ]
264	tourney_win_result [ 'Seed_diff' ] = tourney_win_result [ 'Seed1' ] - tourney_win_result [ 'Seed2' ] tourney_win_result [ 'ScoreT_diff' ] = tourney_win_result [ 'ScoreT1' ] - tourney_win_result [ 'ScoreT2' ] tourney_lose_result [ 'Seed_diff' ] = tourney_lose_result [ 'Seed1' ] - tourney_lose_result [ 'Seed2' ] tourney_lose_result [ 'ScoreT_diff' ] = tourney_lose_result [ 'ScoreT1' ] - tourney_lose_result [ 'ScoreT2' ]
731	hits_sample = hits . sample ( 8000 ) sns . pairplot ( hits_sample , hue = 'volume_id' , size = 8 ) plt . show ( )
969	train = feature_matrix2 [ feature_matrix2 [ 'set' ] == 'train' ] test = feature_matrix2 [ feature_matrix2 [ 'set' ] == 'test' ] train = pd . get_dummies ( train ) test = pd . get_dummies ( test ) train , test = train . align ( test , join = 'inner' , axis = 1 ) test = test . drop ( columns = [ 'TARGET' ] ) print ( 'Final Training Shape: ' , train . shape ) print ( 'Final Testing Shape: ' , test . shape )
1296	train_datagen = tf . keras . preprocessing . image . ImageDataGenerator ( rotation_range = 40 , width_shift_range = 0.2 , height_shift_range = 0.2 , rescale = 1. / 255. , shear_range = 0.2 , zoom_range = 0.2 , horizontal_flip = True , fill_mode = 'nearest' )
136	model . save_model ( "cbmodel.cbm" , format = "cbm" , export_parameters = None , pool = None )
999	for element in x : if current_element == element : current_repeats += 1 else : current_element = element current_repeats = 1 if current_repeats > longest_repeats : longest_repeats = current_repeats longest_element = current_element return longest_element LongestSeq = make_agg_primitive ( function = longest_repetition , input_types = [ Discrete ] , return_type = Discrete )
737	i = test [ test [ 'key_id' ] == 9000052667981386 ] . iloc [ 0 ] [ 'drawing' ] img = draw_cv2 ( ast . literal_eval ( i ) , img_size = 256 ) plt . imshow ( img )
