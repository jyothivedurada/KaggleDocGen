282	Let 's see what 's the best score for a given commit number .
548	Bathroom Count Vs Log Error
1475	Import Required Libraries
808	Running the Algorithm
325	Inference
236	Let 's create a new column called 'dropout_model ' and 'hidden_dim
432	tag_to_count_map is a dictionary with key being the key word count and value being the value word count .
1590	TfidfVectorizer is a vectorizer that transforms a bag of words into a bag of words .
136	Unique Values
1301	Load Test Data
858	altair
1420	Let 's see if there are any outliers in China .
321	Let 's split the data into 0 and 1 .
1439	Load the Data
631	Univariate Analysis
1489	Increased Vascular Markings + Enlarged Heart
303	LightGBM Model
1464	Read the order file
1025	Loading the Data
1220	Predict
302	Prepare the Model
13	Define Train and Test Datasets
1182	Train and Validation
947	Listing Input Files
675	Let 's see the coefficient of variation ( CV ) for prices in different image categories .
510	Let 's create a function to get a single image
616	SVR
978	Let 's set the _should_scroll to true or false .
150	Create Test Generator
435	TfidfVectorizer
1324	Let 's add new features to train and test .
55	Let 's see how many zeros we have in the training data .
330	SGD Regressor
285	Let 's see what 's the best score for a given commit_num and Dropout model
771	Fare Amount
1546	Saving the Data
107	Let 's save before and see what happens
1535	Let 's calculate the distance between the first and the last point .
1417	Logistic Regression
506	Target 1 - Sample Data
880	Score Function of Learning Rate and Estimation
650	Let 's check the number of missing values per column .
945	extract different column types
527	Data preparation
176	Let 's check the memory usage of the dataframe .
252	Italy
1315	Let 's replace 'yes ' and 'no ' with 0 .
501	Correlations
1369	Let 's see the percent of target for numeric features
818	Create Submission File
135	Reading the Training Data
724	Let 's take a look at the range of target values .
587	Let 's calculate the number of infected and deaths per country .
789	Let 's create a list of features .
180	Let 's check how many different components / objects are detected .
1119	Sexupon Outcome
1423	Province - COVID-19 Prediction
1117	Preparation
343	Let 's check the data .
10	Let 's have a look at the numeric columns .
482	Loading Libraries
1518	Scaling with t-SNE
1411	One Hot Encoding
1235	Let 's split the data into train and test .
846	Hyper Parameters
227	Let 's see how many hidden dimensities we have in our dataset .
41	Loading the Data
1250	Let 's do a batch-mixup of the images and labels .
436	One Vs Classifier
1272	Number of Repetitions for each class
825	Dropping some of the missing values
1588	Let 's check the unknown assetCode .
392	Let 's look at the most frequent category
1153	Let 's create a function to compute the mean of the rolling values per store .
939	Make Submission File
197	We 'll use the neato package to render the images .
748	Saving the Trial Data
1232	Let 's validate the output of the LGBM model .
73	Import required libraries
459	We can see that most of the trips are Road , Road , Road , Road , Road , Road , Road , Road
1273	Let 's see how many examples in the oversampled training dataset are there
1184	Loading the Data
966	Growth Rate over time
856	Generate Output File
1253	cod_prov
1451	Let 's see how the Ratio is calculated .
439	Meter Type
71	Loading the Data
1540	Check for missing values
421	Confusion Matrix
826	One hot encoding
175	Loading the Data
598	Gini on perfect submission
357	Inspired by this [ kernel ] ( and [ this kernel
488	Hashing with Keras
77	Train the model
353	Create an EntitySet
1060	Predicting Test
1385	Let 's see the distribution of target for numeric features
1572	Let 's group the data by month and day and plot the number of visits per day .
902	Correlations
256	Let 's drop the Id ' , '3' , '4' , '5' , '6' , '7' from the training data
685	Target Distribution
500	Correlation of Features
428	Train the model
1140	Load Image
877	Let 's see what 's the best score
625	UNDERSTANDING FEATURE
306	Loading the Data
1014	Let 's compute the game time stats for each installation_id .
342	Loading the Data
168	How many clicks needed to download an app
1054	Filter Submission
1158	Train the model
450	Let 's see how the air temperature changes .
1106	Leak 0 - EDA
1149	Let 's see the distribution of var_68 over time .
138	Month Temperature
409	Duplication and Duplication
583	US - Cases by Day
130	Count the number of words in a series
1565	Hilbert Convolution
732	Train the model
89	Clean Training
674	Loading and Describing the Dataset
581	Spain Cases by Country
1493	Let 's load the data .
1129	Loading the Data
1053	Create Test Generator
85	Let 's calculate the age in years
461	One hot encoder
692	Combinations of TTA
422	Random Forest Classifier
1290	Train the model on train and predict on test
1145	Let 's create a mask from the pixel values
54	Let 's check the distribution of the test data .
684	Let 's have a look at the binary features
1073	Import Required Libraries
1582	Let 's take a look at the sample_data .
203	There is a problem with this approach .There is a problem with the CT scanners . It is often a good idea to zero centering . There are two methods to deal with this problem . One is to zero centering with a unit root ( e.g . 0 ) . The other is to zero center with a unit root ( e.g . 0 ) . There are two methods to zero centering with a unit root ( e.g . 0 ) . There are two methods to zero center
1259	Get prediction of validation set
1003	Generate Submission File
524	Predict
408	Exploring the Dataset
1205	Mode by Owners and Investment
1165	Detect TPUs
733	Import the Libraries
687	Let 's split the ID into train and test .
1389	Let 's look at for numeric features
1307	Random Forest Model
1028	Fitting the model
25	Predict on Test
247	Final Ensembles
1445	Let 's load the data .
66	Data preparation
1248	Plot of Weekly Sales vs Department
1241	Let 's have a look at the data
420	Confusion Matrix
614	Read Training Data
489	Tokenization
1365	numeric features
1007	Load the model
225	Let 's see how many hidden dimensities we have in our dataset .
1030	Now that we have our result , we can convert it into a prediction string
1206	Number of Rooms and price
843	Feature Importance
848	Learning Rate
84	Outcome Type
311	Let 's see how many examples we have in each class .
210	MinMax Scaling
60	Liste - Exploration
1171	Let 's get rid of stop words .
1310	LightGBM
1579	Plot of Model Loss
274	We can see that most of the total number of passengers is 8 , while most of the passengers are 3 , so we can see that most of the passengers are fant .
98	Load Test Data
1405	Let 's see what 's going on here . We 'll use a rolling window .
137	Let 's check the statistics of each column .
1199	Create X and Y Dataset
1573	Let 's split the time series data into a basic model and a prediction model
1286	Split the data into train and validation folds
875	Let 's look at the hyperparameters
1352	Remove Null Columns
555	Let 's scale the real features .
1386	Let 's see the distribution of target for numeric features
594	The most common words in selected_text
468	Import Required Libraries
1116	Leak Data
267	AdaBoost
246	Loading the Data
20	Muggy-Smalt-axolotl-configuration
827	Train the model
1015	Let 's create a mapping from title to mode .
1189	Let 's see how the data is distributed .
796	Test Fare Distribution
1539	Process the data
1212	Feature Engineering
815	Boosting Type
49	Let 's create a list of columns to use for training .
188	Brand name
905	Count Categorical Features
209	Linear Regression
1198	Let 's split the data into train and test .
1329	Let 's import the libraries we need
1571	Time Series Average
782	Random Forest
1538	Train the model
1033	Let 's check the output of the model .
762	Submission
1275	Feature Engineering - Previous Applications
291	As we can see that most of the total number of passengers is 20 , while most of the passengers are less than
845	LightGBM Model
74	Ensure determinism
1348	Applicatoin train data
544	Let 's check what type of data is present in the data .
355	Let 's train model with LV and select from model
446	Distribution of meter reading
1209	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category_1 : anonymized category_3 : anonymized category_4 : anonymized category_5 : anonymized category_5 : anonymized category_6 : anonymized category_7 : anonymized category_8 : anonymized category_9 : anonymized category
1076	Splitting Train and Test
1591	Let 's do the same for news data .
334	Split the data into train and validation
1175	Let 's have a look at the number of links and number of nodes .
658	Correlations
372	Decision Tree
358	Loading the Data
718	Correlation between the PCORRS and the scorrs
1061	Filter Submission
1152	Import Libraries
881	Plot of learning rate vs number of estimators
817	LightGBM Implementation
935	Let 's have a look at the data .
573	Now that we 've created a new feature called 'active ' , let 's create a new feature 'active ' .
1321	Let 's do the same for all three columns .
18	Read Train and Test Data
1340	Let 's see how many missing values we have in each column .
629	Let 's create a new feature called 'bookings ' .
86	Age Category
437	Loading Necessary Libraries
1499	Creating new features based on the newly created data
1056	KNN Classifier
351	Loading the Data
1435	Preparation and Feature Engineering
1193	Let 's preprocess the image .
463	Modelling of train and test
1360	numeric features
575	Confirmed and Deaths for a given COVID
986	We can see that there are a lot of missing values in the train_clean_rob dataset . Let 's convert them to labels .
223	We can see that most of the total number of passengers is 6 . And that most of the passengers are hidden . Let 's see how many passengers are hidden .
857	Evaluating the Hyper Parameters
1103	Preparation
1172	Let 's check the number of unique tokens and number of unique tokens
194	Let 's see the price of the most popular product
1040	Loading the Data
76	F1 score
702	Checking for Null values
1425	Predicting COVID-19 Countries
1121	Let 's take a look at the total number of animals .
1313	Checking for Null values
212	Loading the Data
454	Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding
1494	Function Lift
326	Identity toxic identity toxic and severe toxic
1433	Prepare the Model
1353	Categorical Features
339	Voting Regressors
481	LightGBM Model
975	Let 's take a look at the DICOM images
1185	Loading the Data
319	Let 's create the filename
292	Let 's see what 's the FVC weight for a given commit_num and dropout model .
621	Ridge Regression
467	Let 's see how long a trip took .
231	Let 's create a new column called 'dropout_model ' and 'hidden_dim
11	Outliers
1532	Let 's check the correlation with winPlacePerc
1470	Build the Model
1274	FEATURE ENGINEERING
301	Let 's split the data into train/val
891	Let 's see how many time features each entity has .
39	Let 's add sex column .
147	Learning Rate Reduction
1555	All Words
898	Late Payment and past due features
1094	Calculating SNR
249	Now that we have our model ready , let 's proceed to the next steps . First , let 's define a few functions that we 'll be using .
290	We can see that most of the total number of passengers is 19 followed by 28 .
1320	Let 's add some new features .
791	Feature Importance
708	We can see that the number of walls is n't n't n't n't n't n't n
628	Let 's take a look at the total number of bookings per product .
1029	Fitting model on validation set
423	Confusion Matrix
746	Create Submission File
1257	Get the validation and test datasets
1219	Define the learning rate
1270	Let 's run the model for 1 iteration .
115	Let 's see how many price items are in the store .
24	Bag of Words
1126	Simple Submission
985	Let 's see the distribution of TransactionAmt
1430	Loading Necessary Libraries
1568	Let 's take a look at the data
984	Loading the Data
1239	Examine the structure of train and test data
1479	Model
660	Day distribution
1334	Let 's split the data into train and test .
426	Loading the Data
1347	Non Loving Area
715	Let 's see how these correlations look like
823	One hot encoding
595	Let 's see which words are common
1134	Loading the Model
709	Words + Roof + Floor
1303	Check for Null values
281	We can see that most of the total number of passengers is 10 , while some of the passengers are 16 and some of the passengers are less than
458	Intersection
713	As we can see that some of the data is missing , let 's fix that .
1291	Let 's create a new feature called 'mo_ye ' .
366	Function to compute histogram
1125	Let 's see if there 's a change in addr
1463	Let 's have a look at the first 1000 cities .
427	There are a lot of missing values in the data . Let 's see how many missing values we have in the data .
777	Train the model
181	We can see that there are only two cells in the mask . We can see that there are only two cells in the mask . Let 's find the mask of the first cell and open it .
161	Let 's check the files .
451	Dew Temperature
1142	Train the model
770	Absolute latitude and Absolute Longitude Difference
394	Distribution of Categories and Images
840	Feature Engineering - Credit Card Balance
576	Let 's create a function to get the cases of a given country .
1215	Load Test Data
1164	Let 's have a look at the most common class
1354	Let 's look at one of the numeric features .
1318	Let 's replace missing values with 0 .
798	LightGBM
307	Prepare the Model
226	We can see that most of the total number of passengers is 7 , while most of the passengers are 288 and some of the passengers are
1027	Load the model
1203	Let 's sort the data by visit_date and look at the target .
1064	Function to load image
1544	Let 's learn on a example
413	Data generator
647	Loading the model
1251	Now that we 've trained the model , let 's run it 100 times .
874	Inspired by this [ kernel ] ( inspired by this kernel
1	Import Required Libraries
1450	Distribution of Target
1161	var_81 and var_108 are categorical variables . var_81 and var_108 are categorical variables
224	Let 's see how many hidden dimensities we have in our dataset .
636	Confirmed Cases by Population ( Km
1366	Let 's see the distribution of target for numeric features
1089	Import Required Libraries
855	LightGBM Classifier
918	Credit Card Balance
265	Bagging Regressor
331	Decision Tree
784	Let 's extract the date information from the test set .
707	Let 's have a look at the target for each area .
1075	Let 's split the data into train and test
1115	Fast Data loading
416	Unit Sales evolution
695	Let 's check the distribution of Unique Values in Integer Columns
157	Here we import the required libraries .
364	Type
518	Let 's create a base estimator and a custom classifier .
568	Selected Features
377	Bagging Regressor
569	Now that we have our model ready , let 's create our training and validation generators .
1247	Now that we 've created a new feature called 'Weekly_Sales ' and 'Weekly_Sales ' . We 've created a new feature 'Dept ' and 'Weekly_Sales ' .
531	Orders Hour of The Day
308	Word Cloud
605	Fixing public samples
997	Read Site
1022	Fitting the model
1037	Training History
264	Ridge CV
44	Let 's look at 25 most frequent words .
786	Fare Amount by Hour of Day
666	Concatenate OH with full data
407	Let 's take a look at one of the images
164	MinMax + Median Stacking
942	Bureau Balance
216	Let 's train model with LV and select from model
566	Load Test Data
476	Merging Identity and Identity
72	Let 's check the number of missing values in the training and testing datasets .
365	Let 's look at one of the training images .
1267	Let 's check the results .
757	Load the Data
1355	Let 's see the distribution of target for numeric features .
1048	Build New File
47	Let 's check the distribution of target values .
906	Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau Balance Cases Bureau - Bureau
652	Let 's get rid of high and low points .
30	Submission File
559	Let 's check the number of masks per image .
166	Number of different values
919	Splitting the Masks
933	Split the data into train and test
1529	headshotKills
1050	Let 's check what kind of images are available .
1553	Loading the Data
245	Let 's calculate the LB score for each commit .
1008	Loading the Data
56	Let 's see the distribution of the number of zeros in the training set .
1173	Setting up some basic model specs
673	Let 's see the coefficient of variation ( CV ) for prices in different categories .
1091	Light GBM Model
1192	Load the Data
119	Expected FVC
1482	Sample Patient 1 - Normal Image
667	Logistic Regression
1287	Loading the Data
924	Target Variable
1560	Vectorization
81	Let 's see if there 's a difference between the two animals .
1514	Let 's have a look at the distribution of the accent and acc_d .
565	Inference
230	Let 's create a new column called 'dropout_model ' and 'hidden_dim
1382	Let 's see the distribution of target for numeric features
1421	World COVID-19 Prediction
42	Correlations
624	Now that we have our model ready , let 's prepare the inputs and make predictions on the test set .
65	Split the data into train and test
719	Correlations
1441	Let 's check the size of the training data .
760	Let 's check the accuracy of the model .
1345	KDE for EXT_SOURCE
659	Correlations
1486	Sample Patient 4 - Ground Glass Opacities Sample Patient 5 - Consolidation
1166	Loading the Data
1424	Let 's see the COVID-19 model
788	Split the data into train and validation
443	Distribution Reading
1058	KNN logloss on longitude and latitude
1155	Prepare the Model
298	Prepare Training Data
417	Let 's load the meta data .
1087	Loading the Data
833	Let 's do the same for the child dataframe .
862	LightGBM Classifier
29	Calculate AUC and Gini
1263	BERT Distilbert Model
704	We covered every variable
921	Split the data into train and val
412	D4D34af4f7 - Visualization
337	ExtraTrees Regressor
1262	Loading the Data
121	Correlations
117	Let 's create a list of Xmas dates from the state group
577	We can see that most of the cases are China
993	Slicer Code
672	Let 's check the price variance of the parent categories .
957	Stacking Test Predictions
1547	GloVe Tweets
1534	Sieve of Eraten
867	We can see that most of the features are categorical . Let 's see how many of the features are present in the training set .
839	Feature Engineering - Cash
156	Clear Output
934	Predict the model
1444	Let 's load the data and convert it to a pandas dataframe
764	Fare
145	Let 's have a look at the data .
1085	Let 's load the model
183	Check for Null values
775	Linear Regression
234	We can see that most of the total number of passengers is 19 and that most of the total number of passengers is
58	Loading the Data
1178	Let 's check the folder structure .
1069	Cohen Kappa
1016	Simple XGBoost
1279	Check for Null values
916	Inspired by this kernel
1099	Let 's solve some of the tasks .
671	Let 's check the categories of items > 1M \u20B
890	Bureau Favity over Time
1130	Dropping V109_V1 and V
309	Let 's check the data .
963	ClosePrevRaw10 - Lag
804	Bayes Test
53	Let 's check the distribution of the nonzero values of the training data .
1508	Selecting the good features
1280	Breakdown Topic
821	Loading the Data
701	Let 's plot the value counts for each column .
67	Let 's import the libraries we need
1469	Let 's take a look at the data
578	Italy
341	Function for IoU
651	We can see that there are no missing values in the training data . Let 's remove those missing values .
269	Model
1390	Let 's look at one of the most important features
1395	Let 's see the percent of target for numeric features
1236	Cross Validating the Model
681	Loading the Data
490	Model
271	Let 's calculate the FVC weight for a given commit number .
802	Prepare the Model Parameters
1036	Inference
869	Load the feature matrix
45	Target distribution
254	Albania
1339	Let 's see how many missing values we have in each column .
1043	Preparation
1052	Unet model
100	Let 's generate a random sample from the real data and a random sample from the fake data
1278	First , we import the required libraries .
769	Zoom of NYC
956	Let 's plot some random masks and predictions .
178	Masking with Otsu
1400	Let 's look at one of the numeric features .
1190	Let 's define the learning rate
904	One hot encoder
1477	Ensure determinism
258	SVR Accuracy
783	Random Forest Prediction
287	Let 's see what 's the FVC weight for each commit .
557	Let 's have a look at the data .
106	Loading and Describing the Dataset
128	Now that we have a segmented data , we can perform analysis on the data . To do so , let 's create a function to perform analysis on the data .
491	Compile the model
464	Let 's load the data
275	We can see that most of the total number of passengers is 4 , while most of the total number of passengers is 9 . We can see that most of the passengers are fluorescent .
1346	KDE for EXT_SOURCE
756	Let 's have a look at the bounding boxes of the voxels of the wheat .
996	UCF Replaces
876	Bayesian Optimization Results
1010	Save Model
680	Inception V3 model
1380	Let 's see the distribution of target for numeric features
315	Delete the base directory
700	Check for Null values
61	Let 's see how many products are in the train data .
720	Correlations
338	AdaBoost
1266	Define the optimizer
989	Let 's see what 's going on here
127	Volume ( lung_volume
43	Let 's see the distribution of question_asker_intent_understanding
1558	Let 's remove some of the most important words from the corpus .
260	SGD Regressor
1437	Let 's create a new feature : click_time .
1111	Let 's have a look at the data
173	Number of clicks over the day
830	Feature Importance
781	Correlations
368	Linear Regression
206	Import the Libraries
332	Random Forest
354	Correlations
113	Loading the Data
1362	Let 's see the distribution of target for numeric features
642	Preparation
585	Italy cases by day S0 , E0 , I0 , R0 .
1563	LDA
741	Correlations
523	Predict
386	Build the model
1322	Let 's add some new features .
572	Let 's have a look at the distribution of the data .
424	Confusion Matrix
429	Let 's see the distribution of the bayesian blocks
1038	Build Model
976	Now that we 've loaded the DICOM files , we 'll be able to perform DICOM tagging .
1258	Train the model
991	Cylinder Detection
1157	Concatenate wins and losses
1151	Let 's see the distribution of var_91 for train and test .
1530	Let 's check the distribution of killPlace
1559	Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .Lemmatizer is an extension of Lemmatizer .
1497	Define a function to check product less than
1523	Prediction with Threshold
813	ROC AUC vs Iteration
1577	Add missing values to train and test
313	ROC AUC
729	Let 's import the required libraries .
1501	Ensure determinism
232	Let 's create a new column called 'dropout_model ' .
1408	Let 's check if there is a difference between train and test sets .
699	Let 's see if there are any families where the family members do not all have the same target
1505	Load Glove , and paragram embeddings
1585	Loading the Data
923	BanglaLekha child count
778	Baseline Validation
983	Prepare Test
219	We can see that most of the total number of passengers is 0 . Let 's see how many passengers we have in our model .
1509	Add leak to test
1378	Let 's look at 25 numeric features .
1000	Detect TPUs
1314	Let 's replace 'yes ' and 'no ' with 0 .
797	LightGBM
1109	Fast Data loading
832	We can see that there is a strong correlation between the PC and the target . Let 's see if that 's the case
1504	LOAD DATASET FROM DISK
721	Education Distribution by Target
350	Import Required Libraries
1467	Plotting sales over time
1293	Import Required Libraries
948	Let 's have a look at the data .
69	Let 's calculate the distance between the tour and the target .
148	Generate Generator
1154	Let 's convert ` train_date ` to ` test_date ` .
968	Curve for Cases
1034	Read sample submission file
202	Normalize and Zero Center
452	Wind Speed
1019	Loading the Data
604	Let 's create a submission with a random number of samples .
391	Let 's see if there are any outliers in the data
958	Generate Submission File
1238	Stacking Demonstration
1392	Let 's look at one of the numeric features .
1412	Categorization with Logarithm
174	Let 's see the download rate over the day .
1454	Let 's perform clustering on hits , stds and filters .
570	Inspired by this [ discussion ] ( and [ discussion
1057	Test Equation
599	Random Submission
363	There are duplicate clicks with different target values in train data .
619	Linear Regression
1526	Distribution of winPlacePerc
184	Top 10 Categories
460	Let 's see what 's the direction of the ship .
487	Let 's transform text to word sequence .
1112	LightGBM without leak
1200	Creating Train and Test
382	Import libraries
533	Hour Of The Day
676	Import the Data
344	Training and Validation Loss
547	Bedroom Count Vs Log Error
907	Bureau - Bureau
767	ECDF - EDA
1180	Load the Data
712	Bonus variable
1452	Calculate Extra Data
502	Applicatoin train data
329	Linear SVR
564	Test Submission File
1376	Let 's see the distribution of target for numeric features
217	Import Libraries
779	LR with Simple Submission
1072	Loading the Data
1492	Loading the Data
1001	Building the Model
1432	Diff of H1_ vs D1_
710	Let 's add a new column 'warning ' .
914	LightGBM
1137	Image Augmentation
1067	Load Test
882	Plot of learning rate vs number of estimators
1090	Reducing for validation
887	Region Rating client
1481	Predict on Test Data
1081	Visualization of Blurry Images
200	Hounsfield Units ( HU
965	Shap Importance
1455	Function to format prediction string
1208	feature_3 has only one value
1361	numeric features
1429	United States
79	Final Submission File
1576	Let 's load the data
99	Import Required Libraries
248	Import the Libraries
537	Pitches and magnitudes
1374	Let 's see the distribution of target for numeric features
401	Load the data
215	Correlations
312	Setting up the model
250	Spain
1271	Get Training Dataset
48	Let 's see the target distribution .
1254	Loading the Data
1543	As we can see that the signal has a positive correlation with the positive correlation and a negative correlation with the positive correlation .
1333	Concatenate Train and Test Data
469	Predict on test data
1107	Preparation
125	Let 's take a look at some patient scans .
970	Loading the Data
483	Now that we 've trained the model , we 'll be able to use the trained model to make predictions . To do this , we 'll need to transform the text to a vector . To do this , we 'll call ` transform ( ) ` and then call ` toarray ( ) ` on the transformed text .
1474	Select Plate Group
853	LightGBM Classifier
774	Correlation with Fare Amount
397	Inference
567	Read Train and Test Data
1327	Loading the Data
96	Load Training Data
1216	Prepare the Model
522	Classification Report
1044	Now that we have our model ready , we can submit it to Kaggle .
633	Loading the Data
445	meter reading
477	Build
1522	Let 's see the F1 score of the model
922	Keypoints
272	One of the most important things in the competition is the number of passengers in a commit . In this case , the number of passengers in a particular commit is 4 . In other words , the number of passengers in a particular commit is 0.36 .
496	We can see that most of the features are categorical , while other features are numerical .
606	Import Libraries
380	Voting Regressors
1460	Prepare Test Data
1233	Random Forest Classifier
1402	Load libraries and datasets
1120	Male , Spayed Female , Intact
1245	Scatter plot of weekly sales
725	Let 's create a new column with aggregated values
88	Let 's run the model 1000 times .
456	Preview of Train and Test Data
1104	Let 's have a look at the data
795	Trainig and Evaluate Model
961	Month Distribution
546	year built vs year built
1221	Loading the Data
1487	Sample Patient 6 - Normal Pleural Effortion
116	Lets check the price distribution of the whole data .
1513	Let 's convert categorical features to numerical ones
711	Target vs Warning Variable
1147	With Masks
263	Split the data into train and validation
950	Let 's see what types of features are available for the model .
1299	Let 's check if there are any missing values in the dataset .
1541	Let 's create a copy of the feature matrix and split it into train and test .
327	Linear Regression
1434	Split the data into train and test
1122	Inspired by this [ discussion ] ( and [ discussion
1093	Let 's plot first 10 variables
1294	Let 's convert the sample images to jpg
657	Loading the Data
977	Let 's have a look at the UIDs of the patients .
1230	Cross Validating XGB
1309	Inception ResNet
740	Ridge Regression
602	Let 's compare the distribution of public-private difference
513	Masking with opencv
1580	Function to find all occurance in text
515	Normalize and Zero Center
847	Boosting and Subsampling
378	ExtraTrees Regressor
1575	Train and Test Split
705	Let 's look at heads of household .
1453	Load the Data
1338	Let 's see how many missing values we have in each column .
1265	A list of all the trainable variables .
187	Let 's see the price of the categories .
1021	Build Model
177	Let 's convert the image to grayscale
126	Hounsfield Units
185	Let 's see the price of each category .
1316	Continuous Features
751	UMAP - UMAP - PCA ICA - ICA
1305	Let 's have a look at the number of unknowns in each category
1341	Let 's see how many missing values we have in each column .
1074	Configure hyper-parameters
1584	Let 's extract host and timestamp from the filename .
93	Dropping Gene and Varation
92	Class Distribution Over Entries
814	Boosting Type
645	Unique Label
1055	Loading the Data
1371	numeric features
1578	Let 's calculate the accuracy of the model .
9	Check for Null values
1006	Fitting Model
900	Let 's align the feature matrices with the target .
1225	Dropping calc columns
761	StratifiedKFold
1148	Loading the Data
201	Let 's resample our patient 's pixels to a resolution of 1 .
149	Load Test Data
1358	numeric features
340	Model
1468	Let 's have a look at the total sales per store .
441	Meter Reading
669	Most common ingredients
123	Pulmonary Condition Progression by Sex
883	Correlation Heatmap
893	Let 's look at the most interesting features .
162	Pushout + Median Stacking
691	Now that we have our outputs , we can proceed to create a function to process the outputs .
485	Vectorization
462	Normalize Latitude and Longtitude
750	Confusion Matrix
609	Build the model
280	We see that most of the total number of passengers is 15 and some of the total number of passengers is
262	Random Forest
410	Test Duplication
999	Evaluating the Model
1217	Create Supervised Model
472	Bayesian Validation
70	Let 's run the kopt with 2 options
457	Most commmon IntersectionId
728	Average Education by Target and Female Head of Household
146	Let 's take a look at a sample image
63	Let 's see what data is available for this transaction .
1114	Find Best Weight
1228	Logistic Regression
388	Let 's see how many images we have in the test set .
837	Feature Engineering - Installments
1051	We can see that most of the data is missing values . Let 's see how we can deal with missing values .
838	Balanced Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance Balance
865	Train the model
120	FVC Difference
346	Prediction
497	Bureau_balance
917	Cash Balance
316	Now that we 've trained our model on the test set , we 'll generate predictions for the test set .
1046	Building the Model
895	Late Payment
998	Site 4 - EDA
512	Spacing
1160	Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding Label Encoding
1515	We can see that most of the households are fantasy fantasy fantasy fantasy fantasy fantasy fantasy
952	Let 's split the data into train and test
1084	Build Model
352	Let 's create a new dataframe with only 10000 rows .
1525	Loading the Data
834	Feature Engineering
414	Function to compute histogram
690	Let 's read the DICOM files
1566	Make Submission File
12	Load the Data
1567	Let 's load the data .
1337	Let 's see how many missing values we have in each column .
828	Let 's have a look at the data .
1297	Let 's check the number of data per diagnosis .
1406	Loading the Data
1207	Investment or owner occupier product category
1326	Binary Categorification
603	Public Absolute Difference
765	Fare Amount
653	Random Forest
195	t-SNE with dimensionality reduction
809	Find Best Path
1331	Add new category
1527	Distribution of assists
1330	Let 's check the first 10 rows of the training data .
820	Import the Libraries
1302	There are some missing values in the test set . Let 's deal with them .
1086	Best Submission File
863	Add missing values
819	Bayesian Optimization
995	Predict
772	Let 's see the test data
967	Logistic Growth Curve
323	Setting up Training and Validation
1105	Fast Data loading
936	Bureau aggreagation
345	Predicting on Test
1478	Preparation
97	Load Test Data
716	Correlations
561	TTC G9-6362-01-DX
1517	Age vs Mean Eucution
261	Decision Tree
310	Read Train Labels
1288	Correlations
1282	Now that we have our model , we can plot the predictions and actual data .
112	Compile the model
336	Bagging Regressor
1170	Let 's see how many sentences we have in the training and test datasets .
141	Split data into train and test
824	Correlations
1240	Feature Engineering
1031	Visualization of Bounding Boxes
655	Save Model to File
26	LightGBM Features
62	Plot of Fraud vs Non-Fraud
322	Train and Validation Split
1485	Sample Patient 1 - Lung Opacity
1476	Inspired by this [ kernel ] ( inspired by this [ kernel
402	Let 's check the test data
270	Dropout Model
766	First , let 's create a function to calculate the ECC
1500	Inspired by this kernel
551	Noise
1528	DBNO Distribution
747	Write output to file
753	Exploring the Tree
167	Number of Click by IP Address
1045	Build Model
665	Retreasing the Full Dataset
519	Cross-validation
1413	Data generator
892	Trends in Credit Sum
634	Loading the Data
1524	Protein Classification
988	Let 's see what 's going on here
888	Replace Day Outliers
1289	Let 's split the data into train and test
1047	Create Train and Test folders
238	We can see that there are 19 hidden diminals and 19 hidden diminals . We can see that there are only 19 hidden diminals .
182	RLE Encoding
943	Cred Card Balance
64	T-SNE with 2 dimensions
191	No Descrip
859	Boosting Type for Random Search
723	Let 's create a new feature : age
745	Confidence by Fold and Target
1226	Let 's convert the probability to rank
1311	Loading the Data
608	Let 's limit the number of features and maximum text length .
193	Let 's see the length of the products
592	Data Visualization
730	Preparation
228	Let 's see what 's the best score for a given commit number
486	Building a Bag of Words
367	Let 's create a function to read an image and convert it to an array
1163	Let 's check which labels are not in train dataset .
1023	Fitting model on validation set
1447	Convert categorical variables to categorical categories
35	Import Required Libraries
842	Let 's create a copy of this app and reset the index .
543	Loading the Data
288	Let 's see what 's the LB score for a given commit number .
1461	Test data preparation
550	No Of Stores Vs Log Error
1243	Type and Size
478	LightGBM Model
442	Monthly readings
884	Correlation Heatmap
822	Feature Engineering
495	Loading the Data
1583	Let 's take a look at the data
1418	Inspiration
110	Build Model
1401	Let 's see the percent of target for numeric features .
1350	Check for Null values
1379	Let 's look at one of the numeric features
831	Principal Component Analysis ( PCA
68	Let 's start with the initial data .
484	Vectorization
1059	Function to load image
873	Final Training and Testing Shape
1223	Encode Categorical Features
466	Get image paths
586	Let 's see if it 's possible to run the model .
1304	There are missing values in the train and test data . Let 's fill missing values with the missing values .
1458	Create new features based on start and test positions
6	Let 's see the distribution of target values .
1284	Let 's calculate validation score for a proposed model .
1281	Function to extract series
51	Let 's take a look at the distribution of the training data .
1377	Let 's see the distribution of target for numeric features
453	Let 's add the year built feature .
91	Gene Frequency
1359	Let 's see the distribution of target for numeric features
1168	Loading Necessary Libraries
438	Let 's preview of building data
1328	Predict on Test
1561	Lemmatizer is an extension of WordNetLemmatizer . Lemmatizer is an extension of WordNetLemmatizer . Lemmatizer is an extension of WordNetLemmatizer .
390	Unique Categories
990	Cylinder
277	Let 's see what 's the best score for a given commit_num and Dropout model .
1510	Create Video File
1039	Now that we have our model ready , we can submit it to Kaggle .
1502	Loading the Data
369	SVR Accuracy
419	Decision Tree
133	Reducing the memory usage
105	Pickling with PyBZ
1495	Let 's create a function to get the description of the program .
946	adapted from
941	Loading the Data
1018	Loading the Data
1002	Let 's check the paths of the fake faces .
1569	Id Error Categories
534	Now let 's check the order count .
1082	Generate Submission File
643	Outliers and Target Variables
19	Target distribution
754	Non Exploring Model
34	Identity Hate
928	Let 's see the length of the comment text
1159	Predict on test data
33	Now that we 've trained our model , we 'll be using the TfidfVectorizer to make our predictions . We 'll be using the TfidfVectorizer for this .
541	Set hyper-parameters
1398	Let 's look at one of the numeric features .
949	Feature Engineering - Merchant
1542	Let 's see the distribution of acoustic data
1472	Let 's see how many plates there are in the training set .
799	AUC on the test set
1312	Augmented Dataset
552	Combining Augmentations
28	Let 's see the distribution of 0 .
682	Let 's see what 's the shape of the data
1213	Preparation
1531	Let 's check the distribution of kills
528	LightGBM Model Parameters
1414	Check for Null values
455	Predict for test
23	Vectorization
430	Label Encoding is the process of converting categorical variables to ordinal ones . For example , if we want to encode the ID_code and target , we can do that .
1466	This kernel uses the [ H2Oai Data Loader ] ( to load the data .
387	Let 's see how many examples we have in the training set
749	Train Model
927	Loading the Data
901	Feature Engineering
1246	Weekly Sales
480	LightGBM
425	Let 's convert the image to grayscale
1012	Now that we have our model ready , let 's do the same for the test and train datasets .
1436	Minute distribution
1394	Let 's see the percent of target for numeric features .
521	Evaluate Threshold
812	Calculate ROC AUC
1276	Feature Engineering
1503	Saving the Data
517	As we can see that there are a lot of NaNs in the dataset . Let 's remove them .
1343	Let 's see how many features are available for each integer value in application_train and application_test .
610	Let 's use only the first 250 images .
122	The Condition Progression by Sex
982	Check if there are any mismatched masks
179	Let 's check how many distinct components / objects are detected .
793	Validation Fares
688	Let 's convert the image ids to a filepath
305	Building Model
253	Germany
398	Inspired by this [ kernel ] ( inspired by this kernel
836	Installments
615	Check for Null values
1351	Group Battery Type
1285	Let 's calculate the square of the squared of the squared of the squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total squared of the total
1066	We split the data into train and val
1032	Let 's decode the image string to float
1139	Let 's look at the augmented images
1011	Reshape and Padding
703	Let 's see if there are any missing values for age and rez_esc
612	Define Model Parameters
800	Let 's see the distribution of the learning rate
158	Loading the Data
140	Label Encoding for continuous features
1068	Now that we have our tokenizer , we can generate the text and questions from the test data .
1113	LV score : 0 .
503	Let 's check the distribution of the missing values .
403	We can see that there are no missing values in the training data . Let 's see how many missing values we have in the training data .
1095	SN_filter ` SN_filter is one of the most informative SN_filter is one of the most informative SN_filter is one of the most informative SN_filter is one of the most informative SN_filter is one of the most informative
1150	Read Test Data
1201	Train the model
1592	Remove columns
593	Top Common words
415	Let 's see the test image
1551	Let 's take a look at one of the most important features
94	Let 's look at the most common words in the text
259	Linear SVR
776	Split the data into train and validation
696	Let 's create a mapping between dependency and edjefa
211	Prepare the Model
1537	Feature Engineering
742	Random Forest Classifier
1255	BERT Distilbert Model
1533	Let 's see the distribution of winPlacePerc over time .
539	Bedrooms
1214	Model
1110	Preparation
1100	Let 's visualize the model output .
520	Calibrated Classifier
235	Let 's see how many hidden dimensities we have in this dataset .
885	Feature Engineering
2	Prepare the Model
1020	Create dataset objects
620	Linear OLS
870	Feature Importance
644	Let 's split the training data into train and val
1373	Let 's look at the most important features
17	Predictions
134	Reducing the memory usage
1384	Let 's look at the most important features
580	China Cases by Day
124	Inspired by this [ discussion ] ( and [ discussion
872	Remove Low Information Features
1570	Import libraries
664	One Hot Encoding
1187	Now that we 've processed the patient images , we 'll generate the submission file .
530	Loading the Data
1409	Let 's have a look at missing values
289	Let 's see what 's the best score for a given commit_num and Dropout model
940	Let 's split the data into aggs
104	Let 's detect face in this frame .
504	Load Data
964	Lag Cepstral Coefficients
944	Loading the Data
1124	Let 's create a new column called 'addr
1242	Let 's take a look at the sizes of the stores .
1196	Number of annotators and comments
896	Let 's find the most recent value of x and y .
1456	Import Required Libraries
972	Let 's take a look at the DICOM files
601	Plot of public score vs private score
1261	Predict test features
153	F-B Score
87	Import Required Libraries
1108	Let 's have a look at the data
143	Set Seeds
383	Setting up the model
1041	Load the Trial Data
1005	DenseNet
590	Let 's look at the data
1516	Let 's see the mean of v2a
1101	Fast Data loading
204	Loading the Data
255	Andorra
300	Define XGB Parameters
805	Hyperopt
356	Embedding with Random Forest
237	Let 's create a new column called 'dropout_model ' and 'hidden_dim
526	Model ( OLS
600	Evaluption
810	Saving the Trial Data
320	Binary Target
929	We initialize the word2vec model .
1295	Plot of Accuracy and Validation Acc
854	Let 's create a dictionary with the parameters
698	Let 's see if there are any households without a head
639	Data Preparation
244	Let 's see what 's the best score for a given commit number .
1237	Logistic Regression
851	Let 's see how many combinations there are
1308	Loading the Data
1419	Now that we 've created a new column called ' Active ' , let 's create a new column called ' Confirmed ' and ' Deated ' .
1167	Compile the model
75	Let 's set the image size to 32x128 .
714	Corral Coefficients
1368	Let 's see the percent of target for numeric features .
159	Loading Necessary Libraries
1372	Let 's see the percent of target for numeric features .
384	Now that we have our filters , let 's do the same for the high pass filter and low pass filter
376	Ridge CV
913	Let 's remove the correlation values from the training and Testing datasets .
514	Cropping with opencv
82	Sex vs Outcome Type
299	LightGBM Model
649	RLE Encode
1364	Numeric Features
434	Train and Test Split
214	Create an EntitySet
932	Load the data and compute coverage
1446	Loading the Data
937	Feature Engineering
794	Random Forest Model
623	Performing Variance Thresholding
582	Iran Cases by Country
1179	Let 's check the test data
879	Score as Function of Reg Lambda and Alpha
1388	Let 's see the distribution of target for numeric features
1511	Create video
589	Let 's plot the infection peak for each species .
114	Copies the data
333	XGB Regressors
911	We can see that most of the correlations are correlated with other correlations . Let 's find the correlations that are correlated with other correlations .
1181	Let 's create a function that preprocesses an image
348	Generator
418	Test Clustering
538	Let 's see how many bathrooms a user has bathed .
1403	Majority and Minority
588	Differential Eolution
1102	Leak 0 - EDA
139	Let 's create a new feature called 'ord
5	Target distribution
807	Generate Submission File
736	KNN with 20 neighbors
646	Let 's split the training data into train and val
926	Inspired by this [ discussion ] ( and [ discussion
1428	Let 's take a look at the US Counties
101	Let 's check how many fake and real samples there are .
1169	Catagories & Occurrence
243	Let 's see how many hidden dimensities we have in this dataset .
908	Bureau Balance by LAN
7	Let 's check the distribution of feature_1 .
1138	Let 's create a new column called 'image_name
1459	Prepare Training and Test Datasets
399	Loading the Data
951	Joining new merchant_card_id_num to new merchant_card_id
1381	Let 's see the percent of target for numeric features .
473	Prepare the Model
304	Macro F1 Model
229	Let 's see how many hidden dimensities we have in our dataset .
1357	Numeric Features
1383	Let 's see the distribution of target for numeric features
1078	We 'll use a [ albu ] ( augmentation , such as rotation , zooming , zooming , etc . We 'll use [ albu.ShiftScaleRotate
632	Univariate Distribution
190	Shipping and price
1009	Preparation
499	Let 's check the distribution of val_p
979	Let 's get a list of patients
562	Let 's have a look at the masks for a given image .
144	Let 's create a list of unique values for each column .
864	Let 's see how the data is distributed .
1317	Preparation and Feature Engineering
1156	Let 's convert the seed to int
1519	t-SNE visualization in 3 dimensions
912	Let 's remove all columns with above threshold
1587	Highest Trading Volume
95	Word Distribution Over Whole Text
1498	Build the model
668	Let 's have a look at the top 10 labels .
638	Loading the Data
962	SHAP Interactions
395	Train Masks
1512	Loading the Data
1404	New features : close , close_26 , close
635	We can see that there are a lot of missing values in the training data . Let 's remove the missing values from the training data .
1186	Now let 's process the patient images .
955	Split the data into train and test
1222	Frequency encoding
16	Toxic Prediction
930	Model
835	previous_application.csv
1188	Now that we have our model ready , let 's do the same for the test set .
974	Let 's see how many keywords we have in our dictionary .
1484	Lung Nodules and Masses
1162	Let 's now look at the number of different attribute_ids .
1176	Let 's see how many links are there
431	There are no duplicate questions in this dataset . Let 's remove duplicates .
1132	Diff V319 V320 vs V321
108	TPU Configuration
283	Let 's see what 's the FVC weight for a given commit number .
1325	Let 's see which columns have only one value
324	Cohen Kappa
1070	Let 's see how the ARC works
1335	Loading the Data
554	Let 's factorize the features
1536	DAYS_LAST_DUE ` and ` DAYS_LAST_DUE
1550	Loading the Data
931	RLE Encode
286	We see that most of the total number of passengers is 15 , while most of the passengers are 21 and most of the passengers are 0.38 .
759	Let 's replace missing values with 0 .
129	Let 's check the memory usage of the training set
21	Muggy-Salt-Cpper-Turtle
1035	Loading the Data
637	Now that we have our features , let 's create a dictionary with the key being the shift index and the value being the number of zeros at that index .
1062	Concatenate test and null submission
994	Let 's take a look at one of the training images
361	Let 's see what 's going on here
278	We can see that there are only 7 commit_num , Dropout model , FVC weight , and LB score
630	Let 's create a new feature called 'hotel_cluster ' .
626	Let 's create a new feature called 'bookings ' , grouped by 'sum ' and grouping by 'level
1283	Load data from folder
1396	Let 's see the percent of target for numeric features .
1013	Applies Convolution to the signal
663	Let 's add some time features .
374	XGB Regressors
959	Loading the Data
607	Read Train and Test Data
553	Loading the Data
597	Perfect Submission
471	Merging Identity and Identity
981	Let 's take a look at the top-right corner of the image .
154	SAVE MODEL TO DISK
768	Let 's see the distribution of pickup and dropoff coordinates
1473	Create Model
694	Loading the Data
780	Training the Model
1194	Spliting Train and Validation
920	Load the model
878	Hyperparameters search hyperparameters
295	Submission
1397	Let 's see the percent of target for numeric features
868	Let 's load correlations data
811	Evaluability
109	Data augmentation
640	Gini Cohen Kappa
1399	Let 's see the percent of target for numeric features .
208	Let 's apply MinMax Scaling
1356	numeric features
155	Clear Output
83	Outcome Type
1393	Let 's look at one of the numeric features .
373	Random Forest
969	Loading the Data
1415	Let 's see the distribution of the target variable .
4	Load Train and Test Data
852	Let 's search for best hyperparameters .
841	Feature Engineering
743	Feature Selection Scores
1557	Tokenization
1083	Load Test Data
1131	Label Encoding
142	Let 's remove the target column from the training data .
1197	Let 's create a new column called 'distance2mys1' .
251	Let 's create a list of all the dates .
233	Let 's see how many hidden dimensities we have in this dataset .
118	Let 's check the number of data points .
111	Data preparation
1118	Let 's have a look at the data
160	Plot of Fraud vs Target
349	Now that we have a generator , let 's create a generator that yields the first 4 integers .
297	Import the Libraries
151	Train Test Split
773	Manhattan - Euclidean distance
172	Let 's see if there are any missing values in the train set .
3	Read input files
192	Word Cloud
1306	Splitting the data into training and validation sets
525	Mean Squared Error
763	Read Train Data
293	We can see that there are no missing values in this dataset . Let 's see if there are missing values in this dataset .
693	Inspired by this [ kernel ] ( inspired by this kernel
102	Now that we have a path , and a fake path , we need to make sure that the real path matches the fake path .
717	Spearman correlations
46	Target distribution
1071	Let 's create an ARC solver for a random task .
163	MinMax + Mean Stacking
1202	Inverse Scaling
803	Let 's prepare the data .
627	Let 's take a look at the total number of bookings per year .
960	Let 's check the test data
925	Income Cases
335	Ridge CV
165	Loading the Data
493	Build the Model
1252	Exploring the Data
221	Let 's see how many hidden dimensities we have in this dataset .
31	Let 's calculate the sum of squared distances for each cluster
871	Let 's create a list of all the available features .
785	Fare Amount
1319	Feature Engineering
738	Random Forest Model
584	Load the Data
622	Performing Feature-agglomeration
385	Build
381	Model
1562	Vectorization with Lemmatizer
574	Let 's replace 'mainland China ' with 'China
257	Linear Regression
886	Let 's check if there are any variables with more than 2 values .
1554	Load Train Data
1586	Let 's check the distribution of news and market data
1136	Loading the Data
505	Let 's check the shape of the target variable
80	Exploratory Data Analysis
196	Bulge Graph Visualization
1211	checking missing data for new_merchant
1332	Function to add new category
220	One of the most important things in the competition is the number of passengers in the competition . In this case , the number of passengers in the competition is 3 . In other words , the number of passengers in the competition is 0.25855 . In other words , the number of passengers in the competition is 0.25855 .
57	Mean Squared Error
1521	Predict with TTA
1367	Let 's see the distribution of target for numeric features
1049	Now that we have our model ready , let 's do the same for the test and train datasets .
532	Day Of The Week
1549	Create a new dataset
169	Divided by IP Address
465	Exploring Data
284	Let 's see what 's the FVC weight for a given commit number .
1231	Cross Validating XGB
1564	Let 's look at the distribution of each topic .
36	Read OOF files
1581	Reading the Data
1177	Let 's take a look at one of the dicom images
1088	Running the model
903	Correlations
1098	Let 's solve some basic tasks
449	Year built vs year_built
359	GP-ARN Model
492	Input layer
1363	Let 's plot 10 numeric features
1574	Time Series Analysis
1488	Sample Patient
103	Predict with Median Absolute Deviation
1292	FVC of FVC of FVC of FVC of FVC of Patient
241	Let 's see what 's the best score for a given commit number .
444	HOBBIES OF INDUSTRY HIGHEST READINGS ON WEEKDAYS
910	Let 's align the test and train datasets .
816	Simple Feature Import
980	Let 's take a look at the pixel data of the patient .
560	B Bounding Boxes
152	Train the model
1183	Create Data Generator
1249	Cut a batch of 1000 images
1491	Let 's take a look at a sample patient
1065	Predicting on Test
897	Late-payment and past-due features
328	SVR Accuracy
1128	SHAP explanation
689	Read DICOM files
32	Load the Data
406	Stage 1b
1244	Weekly Sales
535	Let 's see how librosa works .
792	Let 's create a list of all the available features
894	Average Term of Previous Credit
545	Correlations
866	Let 's see the feature names .
549	We can see that the total number of passengers is less than the total number of passengers . We can also see that the total number of passengers is less than the total number of passengers .
1096	Now let 's calculate the mean squared error of the filtered data
1256	Read NQ examples
563	Masks Over Image
737	ExtraTrees Classifier
1457	Ensure determinism
404	Loading the Data
199	We 'll use the neato package to render the images .
722	target : escolari/age
596	Read Training Data
656	Import Required Libraries
1079	Let 's take a look at one of the training images .
268	Voting Regressors
1520	Classification Report
448	Let 's transform square feet to log tranformation
1191	Train and Validation
1426	Let 's create a dataframe with the following columns
1556	Let 's see how the word clouds look like
1480	QWK Model
1548	Load Glove , and paragram embeddings
509	Let 's get the labels for a specific subject .
1375	Let 's see the distribution of target for numeric features
1146	Let 's take a look at a mask
317	Predicting with Keras
744	F1 metric
735	LightGBM Model
1264	Train the model
579	Balzil Cases by Day
27	Read input files
1141	Efficient Detection
850	Let 's have a look at the results of our models .
440	meter reading
1042	Save Best Model
529	Convolutional Neural Network
131	Let 's clean special characters
1422	Without China Data
1195	How many toxicity annotators are there
1442	Let 's see how many lines we have in a sample .
1268	Now that we 've trained our model on the training data , we 'll be able to make predictions on the test set . To do this , we 'll call ` get_training_dataset ( do_aug = False , advanced_aug = True , repeat = 1 , with labels
697	There are many family members who do not all have the same target . Let 's check them .
371	SGD Regressor
1133	Let 's see what kind of browser is used for this dataset .
207	Prepare the Model
1143	Let 's check the number of unique values for each column .
731	Random Forest Model
648	Train the model
806	Hyper-Param Optimization
556	Adding missing values to full_text
909	Load Test Data
1218	Training - EPOCH_COMPLETED
1077	Random Forest Model
1427	Visualizing COVID-19 Province/State
276	We can see that there are only 5 commit_num , Dropout model , FVC weight
379	AdaBoost
1080	Let 's do the same for the rest of the images
1438	Loading the Data
205	One hot encoder
475	Submission
1024	We 'll use Distilbert tokenizer to tokenize the text .
861	LightGBM Model
987	Let 's take a look at the patients
1440	Load Train Data
479	Submission
1277	Create a Random Forest Model
889	Bureau Credit
1342	Let 's see how many missing values we have in each column .
222	Let 's see how many hidden dimensities we have in our dataset .
1349	Let 's have a look at the overdue dates .
801	boosting_type = dart boosting_type = goss
294	Feature Engineering : LB Score
617	Ridge Regression
1545	Loading the Data
571	Let 's load the COVID
1298	Let 's see how many columns we have
433	Top 20 tags
679	Extracted Images
1391	Let 's look at one of the most important features
1465	Let 's create a new column called 'previous_visitStartTime ' .
899	Remove Low Information Features
1483	Sample Patient 2 - Lung Opacity
52	Let 's check the distribution of log of values of columns_to_use .
1431	Age vs Gender vs Hospital Death
494	Model
15	We can see that most of the questions are 40 words long . Let 's try having sequence length equal to max_len .
447	Correlations
1234	Logistic Regression
860	Simple Feature Import
171	Ratio : Download by click
1229	Bernoulli Model
1227	Let 's split the data into train and test
411	Let 's create a mask for training and a mask for testing .
1296	Plot of Loss vs Epoch
844	Load the Data
1123	Feature Engineering
38	Visualization of Melanoma Images
296	Prepare the Model
347	Create Submission File
50	Let 's check the distribution of the training data .
727	Merge the features
1507	Add train leak
470	Import Required Libraries
0	Target distribution
971	Let 's visualize the data .
613	CNN sentiment
683	Number of Features with all zero values
1471	Loading Necessary Libraries
1174	Adding \ 'PAD '' to each sequence
1026	Create dataset objects
396	Split test metadata into train and test
375	Split the data into train and validation
1210	merchant information
953	Initialize the Data
1407	Loading the Data
266	ExtraTrees Regressor
1127	LGBM Classifier
536	Mel-Frequency Cepstral Coefficients Cepstral Coefficients Cepstral Coefficients
611	Load word embeddings
170	Let 's see the download by click ratio .
14	Tokenization
393	Let 's take a look at the data
90	Loading the Data
1496	Evaluate
498	Group by
686	Let 's take a look at one of the images
8	Loading the Data
1552	Correlations
314	Classification Report
618	KNN Regressor
362	Ok , let 's see if it 's working
239	Let 's see how many hidden dimensities we have in this dataset .
1490	Let 's take a look at a sample patient
507	Reducing the target
1336	Let 's create a function to generate random colors
1448	Let 's convert the time to category .
1589	Let 's see how many columns we have in our dataset .
213	Sample 5000 samples from the training set
1260	Calculate F1 Score
1387	Let 's look at the first 36 numeric features
558	Masks
790	Linear Regression
40	LightGBM Features
360	Predict on test and train data
508	Loading the Data
370	Linear SVR
511	Converting to Grayscale
540	Correlations
739	Submission
1300	We can see that most of the columns are of type int8 and that most of the columns are of type int16 .
938	Running the Model
1097	Split the data into train and test
706	Correlations
1443	HHOURLY Ratio
1204	Build the model
132	Function to clean up text with all process
662	Ordinal Features
1224	Dropping calc columns
1506	Create a new dataset
992	Show a 3D image
273	We can see that most of the total number of passengers is 6 and that most of the passengers are fantasy .
915	Top Features created from the bureau data
1144	CATEGORICAL_COLS
829	Let 's remove features with cumulative importances below 0.95 .
755	Let 's take a look at the image .
726	Correlations
1462	ReadyOLV3 model
1416	Remove unwanted Columns
641	Prepare the Model
1410	Let 's create a list of all the features we want to predict .
37	Let 's see the distribution of ` age_approx ` .
591	Word Clouds
542	As we can see that there are a lot of birds in the training data . We want to know how many birds are present in the training data . We want to know how many birds are present in the training data . We want to know how many birds are present in the training data .
516	There are some missing values in the dataset . Let 's deal with them .
474	Set hyperparameters
189	Price of the item with a price of 0 .
22	Split the data into train and val
1344	Let 's check the age ( years ) for each trip .
734	Model
670	Categories of items < 10 \u20B ( top
654	Random Forest
758	Let 's check the distribution of surface .
1269	Define Model
1449	IP Address
59	D1 minus day
954	Data Preparation
973	Let 's check what 's the name of the patient .
1370	Let 's see the percent of target for numeric features .
186	Let 's extract the names of the categories .
787	Fare Amount by Day of Week
218	Dropout Model
1323	Let 's add some new features to the train and test datasets .
1063	There are some missing values in the training data . Let 's fix them .
400	Loading the Data
1004	Evaluition
661	Let 's check the distribution of nominal values .
1092	Feature Importance
242	Let 's see what 's the best score for this competition .
389	Let 's take a look at the images of the item .
678	Let 's see how particles look like
677	Hillside Detection
78	Unfreeze and record
1017	Let 's plot some random images
240	We can see that most of the total number of passengers is 21 . And that most of the passengers are hidden . Let 's see how many passengers are hidden .
318	Generate Submission File
198	Bulge Graph Visualization
752	Random Forest Model
405	stage_1_cv2 ( stage_1_cv2 ( filename
1135	Inspired by this [ discussion ] ( and [ discussion
279	We can see that most of the total number of passengers is around 14 and that most of the passengers are around
849	Let 's have a look at param_grid
