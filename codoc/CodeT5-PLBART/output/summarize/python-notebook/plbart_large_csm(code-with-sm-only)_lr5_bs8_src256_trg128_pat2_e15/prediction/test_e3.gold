567	The method for training is borrowed from
370	inspect datagen output
496	Get metrics for validation dataset
589	Replace infs and imputing missing values by mean
164	Loading the data
483	Make a Baseline model
352	Define helper functions
361	Set data loading parameters
382	First downsize all the images
499	Choose the model to use
412	Reducing Image Size
417	Comparing various kappa scoring
435	All train tasks predictions
506	Random Forest model
64	So , some categories are expensive , but most are cheap
302	Test Time Features
461	TPU Strategy and other configs
349	Income distribution and target value
63	Convert each labeled object to Run Line Encoding
537	Predict by Specify Country
6	Read data set
454	Rolling monthly and yearly store means
564	SAVE DATASET TO DISK
441	Find Best Weight
297	Create Training and Validation Set
207	Loading and preprocessing data
234	Load and Explore Data
105	Its also builds on kernel functions but is appropriate for unsupervised learning
541	Difference varialbes were created to describe the difference beween maximum and minimum value
290	Lets first check the Train Target Distribution
411	Create test generator
569	Add leak to test
92	Balance the target distribution
274	Read in Data and Look at Summary Information
210	Let us read the masks
58	Download rate by hour
161	Encoding Street Names
579	Feature primitives Basically which functions are we going to use to create features
368	Gaussian Approximation of Active Cases
318	Aggregate previous loans at Home Credit
244	Interactive booking , click , and percentage of booking trends with Bokeh
586	Reading in the data , as usual
388	Pad and resize all the images
160	Variable Description , Identification , and Correction
177	Function for find out Numerical and categeical Variables
199	Create final submission DF
5	Now there appears to be one feature that is not gaussian
145	Word map for most frequent Tags
171	Document Vectors with TfidfVectorizer
431	This plot shows summarized information about feature impact against shap output
540	Age distribution of male and female patients
165	Librairies and data
515	Check the dataset
478	Convert an array of values into a dataset matrix
90	How many images are in each folder
124	Apply skin segmentation on all training data and visualize the result
509	Best parameters are searched by GridSearchCV on my Laptop
125	Some stats using jpg exif
123	Basic skin detection
91	Create a Dataframe containing all images
220	Spain since first recorded case
267	Affected Surface Object
20	Data is still small enough for memory so read to memory using pandas
50	You can state below link to your copy of this MMDetection repo
448	Model Evaluation and Validation
112	Create the submission csv file
355	Loading the data
87	Code from notebook
377	OSIC training data Example
288	Recursive Feature Elimination with Random Forest
248	Read the csv files on the Johns Hopkins CSSE database on github
446	At first , I made Europe future
206	Composition of Augmentations
279	Per Capita Features
498	Hugging Face pretrained Bert model names
191	When Do People Generally Order
574	Lib and Load data
291	Filling missing and infinite data by zeroes
413	Save results as CSV files
572	You can choose many palettes , which makes the graphs visually nice
22	The data is not balanced
336	Properly Representing Variable Types
299	Use More Features
310	Random Search on the Full Dataset
347	Train and validate
24	The data is not balanced
593	We can safely store the two types into separate dataframes
77	FS with the Pearson correlation
530	Target , prediction process
423	Creating submission file
181	Merging the bureau dataset along with application train dataset to do more analysis
81	Implementing the SIR model
367	Exponential Growth Curves
14	The Shape of the Data
485	CNN Model for multiclass classification
152	Time of Day and Meter Reading
292	Submitting our Predictions
46	Split the data into train and validation parts
141	Confusion Matrix for Test Data Predictions
321	Aggregate Credit previous loans
566	The mean of the two is used as the final embedding matrix
479	Create dataset with look back
463	Loading and Visualization of Data
127	Thanks for the example of ensemling different models from
275	Addressing Wrong Labels
136	Using DecisionTree Classifier
323	Learning Rate Domain
316	Combined Aggregation Function
459	targets in train.csv
565	LOAD DATASET FROM DISK
369	load mapping dictionaries
344	Monthly Credit Data
235	Create the embedding layer
49	Saving the model
162	Encoding Cordinal Direction
568	Add train leak
354	Train the LGBM model
167	Loading the data
284	Next we can rename the columns to make it easier to keep track
303	Try with All Time Variables
75	Comparison of the all feature importance diagrams
149	Variable Description and Identification
289	Visualize Tree with No Maximum Depth
433	train solved tasks
276	Families without Heads of Household
168	MERGE , MISSING VALUE , FILL NA
250	Join all dataframes
61	Dealing with color
535	Add active column
214	For the same window we superimpose the masks above the image
309	Applied to Full Dataset
420	Other columns are the digital value of pixels of kannada mnist
88	My upgrade of parameters
393	Load and process data
330	Visualize Distribution of Correlated Variables
583	That is the size of one test example that we ought to predict
560	The evaluation method
19	Predicting for test data
591	Evaluate the Model
320	Aggregate Cash previous loans
134	Test on the data that is not seen by the network during training
407	Load Model into TPU
419	Import the modules
212	Let us load one image and its masks
429	Pick some frames to display
324	Grid Search Implementation
436	Fast data loading
34	FVC Progression by SmokingStatus
477	Split into train and test sets
475	process submission images
209	Text Processing of text data easily
552	At the scale of stores
260	Import necessary libraries
193	When Do People Generally Reorder
158	Encoding Categorical Variable
517	Make new features using continuous feature
31	Data loading and checking
121	Genetic program model , main code loop
99	Create a submission file
261	High cardinality features
328	DFS with Default Primitives
301	We can make the same plot by day of the week
294	Empirical Cumulative Distribution Function Plot
239	Feature Agglomeration Results
13	Getting to Know the Data
252	using outliers column as labels instead of target column
576	We can see above the counts of higher damageDealt smoothly decrease
587	Applying it on text
547	This notebook will deal with positive , negative and neutral samples independently
53	I updated importation for a faster version
578	NOTE Even tough it is automatic , we can incorporate some manual features
93	Set Up the Generators
403	Evaluate training history
457	Sampling the train data since too much data
133	Idea is to use clustering on images of one type to group data
307	Example of Sampling from the Domain
312	Standard imports for data science work
451	OUTPUT OF AUGMENTATED IMAGES
120	Set up the folds for cross validation
109	Plot the Loss Curves
553	Disable fastai randomness
132	Lets validate the test files
442	Fast data loading
89	Using my notebook
227	Full data Analysis
542	Here is a base model without parameter tuning .
563	LOAD PROCESSED TRAINING DATA FROM DISK
286	The data has no missing values and is scaled between zero and one
11	Maybe if we used the log plot things would be better
173	Document Vectors with hashing trick
60	We can now print the results
311	Bayesian Optimization on the Full Dataset
41	Prepare Traning Data
581	NaN imputation will be skipped in this tutorial
554	Load train and test dataframes and add length columns for Description and Name
474	process test images
438	Fast data loading
464	Making Vocabulary and Text Conversion
484	Create dataset for training and Validation
226	Lets gets started
360	Initialize train and test DataFrames to access IDs and depth information
106	Thanks for the example of ensemling different models from
32	Unique value counts
525	Check the dataset
183	Paths to data and metadata
192	At What Day Of The Week People Order
253	Check Unique Label
428	This is a simple modify from
397	Create fast tokenizer
516	Check null data
51	Make a simple restart of runtime at this point
43	See how our generator work
27	Loading data etc
424	Build the original and translated test data
437	Leak Data loading and concat
146	Featurization of Training Data
37	Convert to lower case Clean contractions Clean special charactor Convert small caps
443	Leak Data loading and concat
67	Does shipping depends of price
482	These have their kitchen area larger than the total area of the house
348	Granted applications per number of children
254	Non physical data augmentation
66	What are their top categories
217	Now a look at Italy
536	Predict by Specify Province
0	Seems like a very wide range of values , relatively spaking
434	evaluation solved tasks
494	Choose the model to use
373	Load Packages and Data
315	Drop Correlated Variables
308	Learning Rate Distribution
296	The test distribution seems to be similar to the training distribution
378	Predict Test Set and Submit Result
385	Creating and Training the Model
21	And finally lets look at the class distribution
556	What are Lung Opacities
62	Deriving individual masks for each object
364	Distribution of months in train and test
287	Comparing Model Performance
115	Thanks to Automatic FE The main code for basic FE
341	Aggregated Stats of Bureau Balance by Client
122	Get the dupplicate clicks with different target values
148	Importing Packages and Collecting Data
551	Import libraries and data , reduce memory usage
36	All contraction are known
140	Confusion Matrix for Train Data Predictions
213	Now we can read the masks for the specific image
47	Create and set up the model
7	Text preprosesing source
353	Predict validation and test set masks
176	Importing The Dataset
195	Visuallizing Interest Level Vs Bathroom
545	Preparing the data
527	Find Null data
264	Remove Extreme Prices
1	Imputations and Data Transformation
449	They are very similar to each other
524	Check null data
331	Remove Low Importance Features
201	Bedroom Count Vs Log Error
416	Infer using trained model
186	Rescaling the Image Most image preprocessing functions want the image as grayscale
257	y的异常值 drop samples which have exception value in y
337	Putting it all Together
39	Using LabelEncoding we just change string values to numbers
163	Seting X and Y
104	Define X and y
580	Label encoding Making it machine readable
402	Load and preprocess data
228	Seperating the data into different data frame based on the labels
295	Rides on Map of NYC
346	Prediction for one image
325	First , we need to put our data into a long format dataframe
170	Create Document Vectors
425	Predict with pure text models
30	Training on the complete Dataset now
570	Function which creates final video from list of images
48	Counting the metric score
189	Bounded region of parameter space
96	MAKE A TEST SET PREDICTION
249	Transpose the dataframes
151	Weekday and Meter Reading
577	Getting Prime Cities
514	Predictions class distribution
178	Types Of Features
59	Importation of a entire day data
539	Predict all province greater than
396	Load model into the TPU
268	All Zero Features
26	Ability to Detect Face
98	Make a prediction on the test images
4	No surprises , since this is all presumably artificial data
79	Parameters and LB score visualization
256	missing value statistics
154	Meter Readings over time
387	Padding process and resizing with OpenCV
282	For example , we can divide the years of schooling by the age
246	a quick check if demand distribution changes week to week
343	Monthly Cash Data
573	The competition metric relies only on the order of recods ignoring IDs
439	Leak Data loading and concat
174	Keras Tokenizer API
549	Write a problem file
138	Confusion Matrix for Test Data Predictions
118	FS with SelectFromModel and RandomForestRegressor
242	Bookings by year
532	It seems Goblins are a little similar to Ghouls
490	Compare timing for MixUp
16	Seed everything for reproducibility
38	Predict submission dates
263	Predict test set and make submission
400	Load model into the TPU
582	Let us split the variables one more time
313	Now we want to combine the data without creating any duplicate rows
197	Correlation Between Price and Other Features
35	Count occurance of words
502	An example usage
238	Random Forest Regressor
259	Save objects for next step
427	Ensemble with my historical best
144	CatBoost is RAM expensive so I prefer to utilize GPU
500	Get variables to apply weight decay in AdamW optimizer
410	Pad and resize all the images
476	we add some squared features for some model flexability
262	Ordinal features mapping
65	Prices of the first level of categories
327	Testing Results on Full Data
338	Correlations of Aggregated Values with Target
137	Confusion Matrix for Train Data Predictions
304	Visualize Validation Predicted Target
534	You only have two areas to work on
585	What should good EDA be capable of
399	Build datasets objects
216	Replacing Mainland china with just China
200	No Of Storey Over The Years
82	Its also builds on kernel functions but is appropriate for unsupervised learning
467	Visualize by heatmap
229	most important or common positive words
219	China scenario since first entry
430	Split Trian and Valid
426	Predict with mixed language models
153	Primary Use and Meter Reading
470	Number of Patients and Images in Test Images Folder
17	Defining DataBunch for FastAI
488	Prediction for test
280	In most cases , the values are very similar
409	Create new labels
507	Data transformation and helper functions
68	Can we get some informations out of the item description
182	analyzing the numerical features disturbion in previous application dataset
224	Select the models to run setting bool variables below
376	Add the actors to the renderer , set the background and size
508	Show influence of economical factors on housing prices
546	Use machine learning model
366	Get important features according to SHAP
281	Redundant Individual Variables
277	Redundant Household Variables
84	Parameters and LB score visualization
444	what already is known
52	Hist Graph of scores
8	Model Validation on train data set
526	Moving average is so simple
455	Train Our Linear Regression Model
169	Loading the data
495	Run on validation dataset
362	Perform check on randomly chosen mask and prediction
283	Feature Engineering through Aggregations
504	Define the number of repetitions for each class
450	Importing Library Files
247	Read the csv files from kaggle
223	Getting population for each country
381	Create fake filepaths dataframe
150	Meter Reading and Meter Type
71	Is there a correlation between description length and price
544	Conversions by Device
70	Can the length of the description give us some informations
432	Plotting errors for one sample
571	Loading libraries and data
175	Creating the Model
543	Explore ip counts
492	Hugging Face pretrained Bert model names
345	Split into training and validation groups
119	Load data files
462	Get Model into TPU
561	Solve the task
363	Create DFs imitating public and private test subsets
44	Prepare Testing Data
155	Meter Readings over time And Primary Use
592	Load CSV files
208	Preprocessing of features
285	Machine Learning Modeling
520	Mix region and education
245	expanding the aggregate
522	Feature selection using shap
57	Attributed time analysis
194	How many orders users generally made
468	View Single Image
237	Imputing missing values
265	Use Ad Image to Identify Item Category
558	If you like the content of this notebook , please consider upvoting it
510	Set Model for prediction
422	Display the dropped images
379	TPU Strategy and other configs
166	MERGE , MISSING VALUE , FILL NA
300	Explore Time Variables
190	Lets Read In Data Files
159	Prediction and Submission
389	Group and Reduce
143	Data load and process functions
501	Check saved checkpoints
204	No Of Storeys Vs Log Error
56	Does bots download the app
319	Aggregate Installments Data
380	Load Model into TPU
521	Remove feature with only one value
538	Predict all country greater than
73	Import Required Libraries
135	The number of samples in each cluster is the following
232	and target vector that correspond to the test data size
322	Hyperparameter Tuning Implementation
550	Plot the obtained tour
23	Apply Underbalancing Techinique
95	Create a Classification Report
529	Onehot encoding for categorical data
111	Process the Predictions
116	FS with the Pearson correlation
230	Most important or common negative words
326	Distribution of Search Values
493	Make TF record file for test dataset
221	Iran since first case
335	First we need to format the data and extract the labels
503	Get labels and their countings
452	Then transform to a datetime object supposing that it is an ordinal datetime
375	and background color definition
18	Unfreeze all layers and find best learning rate
236	Build the Model
358	extract different column types
359	Prepare for training
12	Look at the data types and some basic info about the different columns
486	Create Inference Dataset
390	Adding mode as feature
533	Logistic Regression seems to be a good classification algorithm for this dataset
472	Get Tabular Data
405	Unhide below to see all trials results
273	Combinations of TTA
453	Most of the dates overlap
418	Relevant attributes of identified objects are stored
102	Train Test Split
298	Train with Simple Features
147	Fitting Logistic Regression with OneVsRest Classifier
222	USA since first case
233	Import Packages and Functions
271	As a Neuroradiologist , this distribution looks pretty true to daily practice
94	What is the AUC Score
83	Thanks for the example of ensemling different models from
351	Check the typical length of a comment
491	Compare timing for GridMask
103	Set Up the Generators
519	combination using three features
421	Drop the blurry image
505	Check oversampled dataset
33	FVC Progression by Sex
414	Training the model
10	And it looks like a fairly nice distribution , albeit still fairly asymetrical
188	Load data and fit some models
266	Price Variance Within Identified Items
374	Outlier Analysis and Feature Scaling
278	Creating Ordinal Variables
139	Using RandomForest Classifier
270	Start by pivoting the DataFrame to explore the label distribution over slices
113	What is a python generator
497	Run on test dataset
398	Load text data into memory
86	Divide features into groups
42	See sample image
231	Most important or common words in neutral data
100	Create Binary Targets
131	Code from here and below is commented out because the kernel dies
117	FS with SelectFromModel and LinearSVR
415	Load Test dataframe
76	Thanks to Automatic FE The main code for basic FE
156	Square feet size is positively Skewed
408	Will need those folders later for storing our jpegs
28	Show and save column comparision matrix and save row sets
471	Create Image Augmentation Generator
465	Making Feature Matrices
590	RF for feature selection
473	process training images
356	and batch aggregations examples for the rest of the tables ..
29	Preparing the training data
196	Visualizing Interest Level Vs Bedrooms
584	Save some memory
447	There is a gap between them
130	Define some constants for data location
372	Process test data in parallel
258	y hist with defferent timestamps are similar
293	Data Exploration and Data Cleaning
269	Show Original Image
466	Count game trainsition
386	Save model and weights
225	Calculating the day when the number of infected individuals is max
342	Calculate Information for Testing Data
557	Opacities That Are Not Related to Pneumonia
392	Plotting some random images to check how cleaning works
305	Evaluate Best Model from Random Search
85	Prepare Training Data
9	And now we embed each chunk individually
255	Check if valid data looks all right
306	Now we can evaluate the baseline model on the testing data
394	Load text data into memory
54	How many different values does our categorial variables take
531	Null data check
187	Interpreting ROC Plot
211	So a unique operator will give us the unique filenames that contain ships
108	Initialize the generators
78	FS with SelectFromModel and LinearSVR
511	Test data preparation
202	Bathroom Count Vs Log Error
74	Apply Logistic Regression
513	Convert DCM to PNG
333	Distribution of Scores
480	Make prediction and apply invert scaling
114	How to make a generator run infinitely
184	Group signals metadata accroding to target
72	Graph Representation of RNA structure
588	Read and Explore
107	Load the pre processed data
340	Putting the Functions Together
445	make hour column from transactionDT
406	Save model and best hyperparams
317	Merge with the main dataframe
391	Refit and Submit
332	Align Train and Test Sets
440	Fast data loading
157	Imputing Missing variable
198	Main Config Variables
329	DFS with Selected Aggregation Primitives
365	SHAP Interaction Values
180	Checking the Correlation Between The Features for Application Train Dataset
15	Importing all Libraries
512	Data preparation for test
401	Inference on Test Set
45	Create Testing Generator
142	Converting the Input images to plot using plt
395	Build datasets objects
371	This images from validation data seem to be really strange labeled ...
334	Plots of Hyperparameters vs Score
203	Room Count Vs Log Error
339	Function to Handle Categorical Variables
548	For neutral samples , use original texts as they are
179	Analysis based Averages values
55	Zoom on this IP
469	Number of Patients and Images in Training Images Folder
110	Make a Prediction
562	Ensure determinism in the results
97	Set up the generator
487	Define dataset and model
460	These labels are not in train
80	Ensembling the solutions and submission
40	Using embedding in NN we can change dimensionality of categorical features
384	Load and freeze DenseNet
555	What Does a Normal Image Look Like
357	load mapping dictionaries
3	Detect and Correct Outliers
218	Comparison between Brazil and Italy
101	Balance the target distribution
481	Using all features for model training
241	Bookings per day of week
126	Its also builds on kernel functions but is appropriate for unsupervised learning
350	Read the data
69	What words do people use
128	Manager function to call the create features functions in multiple processes
456	Next , we will make prediction with our LR Model
272	Demonstration how it works
215	Prediction on test set
243	Bookings by month
518	Ratio feature can have infinite values
458	targets in labels.csv
314	Identify Correlated Variables
25	Take a look at predictions
129	How many different cars in train dataset
575	Explore distribution of single variable
528	Predict null data based on statistical method
2	Impute any values will significantly affect the RMSE score for test set
205	Gaussian Noise on Target
559	Composition of functions
185	Apply reduction on some samples and visualize the results
251	filtering out outliers
172	Document Vectors with HashingVectorizer
404	Load and preprocess data
489	Compare timing for CutMix
383	Create real file paths dataframe
523	Load and preprocess data
240	Predict on test set
