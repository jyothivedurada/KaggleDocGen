254	Load the Model
247	Loading the Data
493	Yield examples from jsonl files
52	Histogram of isFraud
316	Aggregate numeric and categorical values
335	Data preparation
344	Loading the Data
424	Loading the Test
458	Count the number of classes in the dataset
118	Embeded Random Forest
219	China cases by day
524	Percentage of missing values
547	Load the Data
249	Transpose lat and long
437	Loading the Leak
16	Seed everything
538	CovID-19 predictions
540	Age Gender Hospital Death and BMI
226	Loading the Data
413	Load model and predict generator
149	Preview of Training and Test Data
439	Loading the Leak
241	Aggregates the data
268	Number of binary features
368	Curve for Cases
269	Drawing the image
402	Loading the Data
577	Sieve Eatosthenes
333	Get the best scores for each set
116	Now , we will visualize the correlation matrix
207	Loading the Data
18	Unfreeze the model
440	Loading the Data
59	Loading the Data
518	Remove missing values
156	Distribution after log tranformation
86	Now , we will look at the features
546	Performing the clustering
589	Adding missing values
434	Plotting the predictions
211	Look at the images with missing pixels
120	Fold Importance
100	Binary Target
525	Missing Train Data
221	Iran cases by day
238	Random Forest Classifier
180	Pearson Correlation
183	Load the Data
40	Categorize categorical variables
58	Distribution of Download Rate over the day
352	Encodes the image
346	Load the Model
515	Loading the Data
126	Linear SVR Model
244	Aggregate the Data
427	Toxic Submission
587	Lemma CountVectorizer
378	Prediction
46	Split the training and validation sets
101	Data preparation
117	Select From Model
51	Torch and MMDet
246	Semana Distribution
500	List of decay variables
227	Generate WordCloud
45	Create the test generator
114	Generate a generator
448	LGBM Classifier
191	Hour Of The Day
517	Extracting continuous features
558	Import the Libraries
195	Bathrooms and Bathrooms
360	Initialize the dataset
272	Process Det
135	KMeans and KMeans
319	Loading the Data
331	Remove low information features
133	Computation of Histogram
505	Oversample and Oversample
27	Pickling the data
253	Split labels into list of labels
501	Look at the results
399	Load the Data
209	Adding Full Text Features
123	Get Image Data
186	Convert image to grayscale
444	Importing the Libraries
477	Scaling the data
342	Load test data
332	Final Training and Final Testing
473	Process the patient images
438	Loading the Data
134	The test image is a numpy array
77	Now , we will visualize the correlation matrix
385	Define the model
261	Number of nominals
284	Now , we need to split the data
137	Confusion Matrix
562	Seed everything
301	Fare Amount by Day of Week
112	Prediction String
445	Now , we need to create the dataframe
326	Boosting Type for Random Search
329	Computation of Feature
422	Display blurry samples
90	Loading the Data
330	Loading the Data
20	Load Training Text
396	Build the model
392	Plotting random images
411	Create Image Data Generator
492	BERT and DISTILBERT
590	Import the metrics
154	Meter Reading
421	Preprocess Images
354	Run Kfold LGBM
357	Loading the Data
451	Augmented Images
460	Look at the most frequent labels
504	Number of Repetitions for each class
591	Plotting the model loss
64	Mean price by category distribution
345	Split into train and validation masks
179	Distribution of Average Values
115	Import the Libraries
170	Vectorizer and Vectorizer
462	Create EfficientNet model
365	Exploring the Interactions
542	Spliting the Data
561	Build the model
57	Missing Values
503	Get the raw training dataset
381	Preparing the Data
292	Submission of surface
78	Select From Model
296	Correlation with Fare Amount
175	Create the model
146	Train Multilabel
417	Linear Weighted Kappa Score
305	Train the model
484	Loading the Data
475	Submission of patient images
583	Plots for Quaketime
33	Pulmonary Condition Progression by Sex
531	Missing Train Data
155	Manually BUCKED TRENDS
467	Plotting the number of links
560	Exploring Images
121	Check if the data is valid
164	Import the necessary libraries
569	Load test leak data
506	Create a Random Fore Classifier
431	Scatter plot of variable
277	Check for missing values
287	Random Forest Model
544	Count of clicks and proportion of downloads by device
449	Difference between V32 and V32
459	Look at the labels
405	Create Trials Table
468	DICOM Image
513	Converts Images
215	Load Test Data
370	Plotting train and validation datasets
93	Training and Validation
519	Exploring the Data
187	Evaluate threshold
323	Lets check the learning rate
520	Exploring the Data
482	Majority of rooms
192	Order Count Across Days Of Week
447	Detecting the change
469	Total number of images and patients
39	Encodes LabelEncoder
60	Memory usage before optimization and after optimization
553	Set the random seed
371	Loading the Dataset
228	Look at positive and negative
390	Create title mode
557	Pleural Effusion
216	Extracting mainland China and China
7	Vectorization with TfidfVectorizer
169	Import the necessary libraries
85	Merge the results into a single dataframe
217	We will now look at the data
355	Loading the Data
325	Loading the renderers
499	Pretrained Model
48	Computation of FB Score
485	Create the EfficientNet
320	Reading the Data
566	The mean of glove , paragram and fasttext embeddings
122	Duplicates with different target values
309	Loading the Data
582	Now , we will look at the target
0	Histogram of Targets
523	Loading the Data
15	Import the necessary libraries
199	Merge the results into a single dataframe
36	Remove special characters from text
267	Sample of hits
294	Exponential Exponentiation
564	Save the data
151	Distribution of Meter Reading
494	Pretrained Model
263	Log Regression Model
274	Loading the Data
152	Highest Reading
190	Loading the Data
362	Stacked Validation Index
168	Merge the Data
76	Import the necessary libraries
351	Average Comment Length
526	Mean and Standard Deviation
206	Compose the augmentation
166	Merge the Data
43	Exploring the Data
3	Outliers and outliers
400	Build the model
353	Prediction
11	Log Histogram of Log Values
68	Number of items have no description
214	Masks Over Image
495	Get predictions for validation
349	Distribution of Income Bins
62	Now , we will look at the labels
266	Coefficient of variation
293	Distribution of Fare
25	Mean Absolute Deviation
212	Cropping the image
584	Save the data
271	Convert image id to DNE filename
231	Most common words
474	Preparing test images
534	Import the necessary libraries
147	OneVsRestClassifier
490	Batch Mixture
87	Loading the Data
372	Prepare Sample Submission
150	ELECTRICITY THE MOST FREQUENT
158	LabelEncoder and LabelEncoder
91	Loading the Data
283	Computation for Range
273	TTA Combination
502	Create the model
314	Calculate the correlation matrix
541	Difference between H1 and D1
157	Adding year built features
83	Voting Regifier
313	Train and Test
508	Spearman correlation
171	Vectorize TfidfVectorizer
327	Loading the Data
189	Create a dictionary of parameters
470	Checking the test data
593	Lidar Data
576	Distribution of DBNOs
275	Check for the same target in all family members
408	Create train and test folders
453	Number of Train and Test
142	Convert image to numpy array
104	Data preparation
163	Updated train and test data
588	Loading the Data
527	Missing Values
423	Generate Submission File
398	Loading the Data
412	Load the image
201	Bedroom Count vs Log Error
276	Households without head
161	Turning the road into a dictionary
483	Train and Test
414	Create the Data Generator
514	Number of data per each diagnosis
127	Voting Regifier
128	Runs the build
50	Clean up the output
310	Cross Validation
436	Loading the Data
570	Create a video
185	Reducing the Data
111	Prepare Prediction
94	ROC AUC Score
586	Loading the Data
555	Sample Patient 1 Normal Image
455	Fit the logreg model
556	Sample Lung Opacity
539	CovID-19 Prediction
81	Function to calculate A and B
132	Load Data
160	preview of train and test data
73	Import the necessary libraries
102	Split training and validation sets
225	Findings and Plot Infections
281	Check for missing values
56	Quantiles and Quantiles
181	Applicatoin Dataset Merge
573	Protein Classification
537	CovID-19 Prediction
256	Calculate Miss Count
30	Train the model
252	Remove outliers
377	DICOM Image
533	Logistic Regression Model
24	Generate random paths and y
393	Loading the Data
363	Public and Private Test
425	Build model from pretrained model
488	Train the model
481	Preparing the Data
430	Reducing the dataset
66	Top 10 categories with a price of zero
31	Loading the Data
450	Import the necessary libraries
406	Save the best model
366	Calculates the importance matrix
489	Batch Cutting
386	Save the trained model
32	Check for unique values
554	Load the Data
14	Check for missing values
220	Spain Cases by Day
88	Setting up the model
2	Check for numeric types
391	Simple XGBoost Classifier
574	Import the Libraries
578	Calculate the ratio of the two datasets
397	Tokenization for DistilBERT
312	Import the necessary libraries
138	Confusion Matrix
229	Most common words
103	Training and Validation
248	Loading the Data
471	Create Image Data Generator
148	Import the necessary libraries
328	Feature Analysis
403	Training History
257	High and low quantile
5	Histogram of Histogram
61	Converts the image to grayscale
387	Resize Image
22	Generate a random sample
295	Zooms and Zooms
237	Check for missing values
585	Import the necessary libraries
223	Loading the Data
322	Train the LG model
348	Exploration of Target
550	Order of the solids
178	Grouping by two columns
334	Create a random hypothet
568	Load leak data
203	Room Count Vs Log Error
426	Clear session and garbage collector
69	Word Cloud
140	Confusion Matrix
279	Load the data into a dictionary
394	Loading the Data
108	Lets check the shape of the data
543	Count the number of IPs
567	Create a new dataset
159	Predicting the test
452	Now , we will sort the train data
9	Embeddings from Training Text
380	EfficientNetB3 model
361	Load the Data
416	Convert text and questions to sequences
177	Categorical features and numerical features
278	Most of the walls are categorical
435	Plotting the predictions
65	First level categories
198	Set the parameters
497	Prediction of test features
174	Tokenize the text
442	Loading the Data
522	Adding categorical features
131	Loading the Data
12	Data preparation for training
84	LB Scores
339	Count of categorical variables
367	Growth Rate Over Time
80	Finalize Ensembles
401	Submit predictions for each image
98	Prediction on test images
498	BERT and DISTILBERT
173	Hashing the text
364	Exploring Months
290	Count of surface value
202	Bathroom Count Vs Log Error
141	Confusion Matrix
509	Train and Test Split
432	Plotting the errors
72	Render a neato image
592	Loading the Data
218	Preparing the Data
456	Predicting the test set
44	Load test data
491	Batch Masks
23	Now lets check the number of train and validation sets
10	Adding the log target
6	Loading the Data
240	Preprocess the data
29	Data preparation
105	Linear SVR Model
486	Load Test Data
369	Loading the Data
109	Training and Validation Loss
130	Load the Data
466	Total number of links for each title
13	Loading the Data
54	Number of different values
464	Training and Test Sentences
465	Adding PAD to each sequence
143	Setting up the model
306	Calculate ROC AUC
472	Loading the Data
1	Adding missing values
463	Import the necessary libraries
19	Submit for Test Dataset
79	LB Score
28	Save before and after
300	Fare Amount vs Time Since Start of Records
289	Random Forest Classifier
47	Create a CatBoostClassifier
26	Checking for face detection
336	Checking for boolean variables
347	Split train and validation sets
507	Breakdown Topics
383	Loading the Data
49	Save the model
350	Loading the Data
454	Calculate rolling mean per store
551	Import Pystacknet
41	Loading the Data
410	Resizes and Resizes
516	Missing Values
37	Clean up text with all processes
172	Vectorize the text
420	Train and Test Data
129	Masks and Images
4	Histogram of Targets
457	Set the variables
575	Distribution of winPlacePerc
245	Demanda Unequil Sum
581	Missing Data
429	Runs the videos
303	Remove features from data
512	Merge the test data
124	Plotting Training Dataset
297	Split training and validation sets
510	Mean Squared Error
144	Train a CatBoostRegressor
113	Yielding the results
232	Perfect Submission
82	Linear SVR Model
251	Features with outliers
21	Class Distribution Over Entries
125	Load Image Data
194	Order Count
580	Label encoding for categorical features
288	Random Forest Classifier
395	Load the Data
521	Check for only one value
184	Prediction
496	Validate Prediction
409	Build the new dataset
299	Train the model
291	Fill missing values
193	Reorder Count
478	Create the dataset
75	Mean of the feature score
89	WordCloud WordCloud
200	Merge year built and numberofstories
415	Generate Test and Submit
110	Predicting the model
260	Import the necessary libraries
282	Plotting the violinplot
162	Create Directions
480	Prediction and Prediction
407	EfficientNetB7 model
324	Lets check the number of parameters
165	Import the necessary libraries
205	Gaussian Target Noise
404	Loading the Data
388	Resizes and Resizes
530	Logarithmic Logarithmic Correlation
107	Loading the Data
145	Generate WordCloud from Frequency
38	Loading the Data
167	Import the necessary libraries
307	Subsample and Boosting Type
374	Cleaning the Data
70	Comos Length
375	Visualize Bkg Color
535	Deaths and Deaths
528	Loading the Data
204	No Of Storeys Vs Log Error
536	Time Series Prediction
389	Compute Game Time Stats
315	Dropping the data
321	Reading the Data
552	Categorize the sales
106	Voting Regifier
443	Load Leak Data
239	Performing feature agglomeration
461	Run on TPU
182	We can see the distribution of the values
545	Exploring Time Series
549	Loading the Data
250	Exploring the Data
235	Embeddings and Dropout
196	Bar plot of bedrooms
441	Calculates the scores
262	Orders and Experts
92	Exploring the Data
286	Random Forest Classifier
136	Decision Tree Classifier
255	Encodes the image
302	Split training and validation sets
418	Analysis of the ARC
264	Categories of items < 10
428	Import the necessary libraries
382	Create the fake directory
74	Dropping missing values
563	Loading the Data
337	Feature Analysis
53	Loading the Data
35	Count the number of words in the series
317	Merge the bureau info
67	Shipping fee paid by seller
213	Load Masks
376	Add a cylinder actor
17	Setting up the model
511	Preprocess the test data
311	Cross Validation
234	Loading the Data
270	Extracting the ID and Submission
224	Check for Sir and Seird
304	Distribution of Validation Fares
341	Bureau balance by loan
529	One hot encoded features
97	Load Test Data
487	Set up the model
446	Check if the address is valid
565	Load the Data
242	Bar plot of the sum of all the dates
571	Import the necessary libraries
230	Most common words
55	Number of click by IP
340	Clearing the data
95	Classification Report
236	Set the kernel and hidden_dims
559	Lift the results
379	Run on TPU
356	Loading the Data
548	Extracting Sentiments
71	Coms length VS price
208	Preparing the categorical features
338	Correlation of Target
419	Import the necessary libraries
479	Create train and test datasets
384	Define the neural network
99	Create a submission file
476	Full and Sub Full
359	Remove the first active month
153	Monthly Reading ARE Highest Channels
358	Int , float , categorical columns
34	Pulmonary Condition Progression by Sex
63	RLE encoding of the mask
579	Analysis of feature definitions
96	Remove the base directory
308	Load Bayesian Results
343	Loading the Data
139	Random Forest Classifier
259	Save Preprocessing Model
243	Aggregate by date
258	Random Forest
197	Correction of Matt
298	Train the model
176	Loading the Data
373	Import the necessary libraries
285	Import the necessary modules
433	Plotting the predictions
233	Import the necessary libraries
8	Prediction for Identity Hate
222	Now , we will sort the results by day
265	Loading the Data
188	Data preparation
42	Load the image
119	Load the Data
532	We can also plot a boxplot
210	Load Masks
280	Plot the correlation matrix
572	Accent and Accent D
318	Reading the previous application
