131	Code from notebook
249	Keras Tokenizer API
783	Check the dataset
780	Credit day overdue
72	How many different values does our categorial variables take
833	What Does a Normal Image Look Like
472	Visualize Distribution of Correlated Variables
148	Create a submission file
392	Affected Surface Object
155	Define X and y
569	First downsize all the images
863	NOTE Even tough it is automatic , we can incorporate some manual features
108	Read the data
878	Replace infs and imputing missing values by mean
81	Last click from the IP analysis
659	Leak Data loading and concat
273	Bounded region of parameter space
287	Visualizing Interest Level Vs Hour
442	Learning Rate Distribution
578	Adding mode as feature
716	Make a Baseline model
250	Creating the Model
632	This plot shows summarized information about feature impact against shap output
337	It fills our perfect submission with n randomly distributed random answers i.e
605	Inference on test set
36	Take a look at predictions
798	Predict by Specify Province
564	A simple function from stack overflow that crops the image
159	Load the pre processed data
42	Data loading and checking
773	combination using three features
262	analyzing the numerical features disturbion in previous application dataset
328	Select the models to run setting bool variables below
478	Plots of Hyperparameters vs Score
37	Ability to Detect Face
218	Variable Description and Identification
555	Process train data in parallel
714	Apply invert scaling
257	Types Of Features
295	No Of Storey Over The Years
873	What should good EDA be capable of
618	Other columns are the digital value of pixels of kannada mnist
162	Plot the Loss Curves
122	Its also builds on kernel functions but is appropriate for unsupervised learning
727	Distribution of product among different genders
725	Data Loading and Cleaning
743	Define the number of repetitions for each class
193	Visualization of a single batch
769	Room per family features
877	Read and Explore
821	Plot the obtained tour
327	Getting population for each country
494	Calculate Information for Testing Data
140	Set Up the Generators
147	Make a prediction on the test images
276	At What Day Of The Week People Order
161	Set up the Model Architecture
366	Data Exploration The label distribution shows clear label imbalance
808	Explore ip counts
537	Examining the Growth Curves
804	Difference varialbes were created to describe the difference beween maximum and minimum value
822	Made a mask using the tour
226	Imputing Missing variable
383	High cardinality features
379	Save objects for next step
456	Aggregate Grandchild Data Tables
721	Prediction for test
150	Balance the target distribution
795	You only have two areas to work on
409	Creating Ordinal Variables
109	Test Input Pipeline
115	Parameters and LB score visualization
867	NaN imputation will be skipped in this tutorial
817	Start positions and end positions of selected texts in tokenized source texts
574	Padding process and resizing with OpenCV
745	A method to get oversampled training dataset
154	Plot the Training Curves
844	SAVE DATASET TO DISK
0	Load the metadata of each file
649	Adding some lag feature
803	Impact of bmi over old and young patients
548	Load Model into TPU
321	Now a look at Italy
23	Defining DataBunch for FastAI
602	Pad and resize all the images
188	Display some of similar cars
93	Which brands are most expensive
529	Run length encoding to reduce predictions size
313	Training and callbacs
99	What words do people use
451	Identify Correlated Variables
283	Visualizing Outliers In Data
856	You can choose many palettes , which makes the graphs visually nice
396	All Zero Features
631	Split Trian and Valid
244	Splitting train into training and validation set
241	MERGE , MISSING VALUE , FILL NA
390	Price Variance Within Identified Items
265	Apply reduction on some samples and visualize the results
305	Thx Stackoverflow for realization
756	Set Model for prediction
404	Read in Data and Look at Summary Information
812	Conversions by Channel
595	Unhide below to see all trials results
644	Train model by each meter type
579	Refit and Submit
663	what already is known
417	The data has no missing values and is scaled between zero and one
227	Encoding Categorical Variable
111	Comparison of the all feature importance diagrams
823	Import libraries and data , reduce memory usage
662	Replace to UCF data
841	Solve the task
274	Lets Read In Data Files
235	Seting X and Y
230	Variable Description , Identification , and Correction
862	Getting Prime Cities
312	Train a simple CNN architecture
700	Get Tabular Data
13	And now we embed each chunk individually
748	Results from my own training
876	Applying it on text
854	Predict on test
116	Ensembling the solutions and submission
25	Predicting for test data
394	Initial Position and Momentum
1	Testing Time Augmentation
776	Feature selection using shap
63	See predicted result with images
599	Will need those folders later for storing our jpegs
612	Infer using trained model
546	Custom LR schedule
614	Comparing various kappa scoring
293	Top Features Selection
137	Display a random sample of train images by class
252	Recurrent Neural Network
661	Train model by each meter type
800	Predict all country greater than
288	Bedrooms Vs Bathrooms Vs Interest Level
416	Machine Learning Modeling
83	Importation of a entire day data
285	Visuallizing Interest Level Vs Bathroom
489	Function for Numeric Aggregations
291	Resnext initialize functions
853	Create Magic features
343	Imputing missing values
408	Redundant Household Variables
585	Load model into the TPU
835	The Definition of Opacity
278	How many orders users generally made
352	Bookings by year
517	Train the LGBM model
621	Drop the blurry image
423	Submitting our Predictions
664	make hour column from transactionDT
872	Save some memory
479	Now for the next four hyperparameters versus the score
406	Addressing Wrong Labels
598	Load Model into TPU
151	Train Test Split
490	Function to Handle Categorical Variables
165	Create the submission csv file
669	They are very similar to each other
446	Random Search on the Full Dataset
225	Square feet size is positively Skewed
348	Dimensionality Reduction Techniques
369	using outliers column as labels instead of target column
317	removing columns where correlation is high
560	and background color definition
593	Evaluate training history
53	Predict submission dates
747	Common things for training
724	Compare timing for GridMask
319	Some —Åustom Loss functions
452	Drop Correlated Variables
762	Divide data into training and validation sets
372	Check if valid data looks all right
79	Attributed time analysis
492	Aggregated Stats of Bureau Balance by Client
813	Preparing the data
794	Logistic Regression seems to be a good classification algorithm for this dataset
344	Looks like there are no missing values in the dataframe
708	Convert an array of values into a dataset matrix
367	How much does this trust interval change over the real value for kappa
229	Importing Packages and Collecting Data
608	Save results as CSV files
554	Visualize test processed samples , resize
839	The evaluation method
357	Same series of charts but for returns
133	Using my notebook
33	Check Validation Log Loss
368	filtering out outliers
401	Examples of different hemorrhage subtypes
329	Calculating the day when the number of infected individuals is max
77	Does standard users download the app
651	Replace to UCF and UCL data
301	Composition of Augmentations
625	Predict with pure text models
641	Leak Data loading and concat
175	Get the dupplicate clicks with different target values
397	Here is the distribution of the maximum value for the remaining features
323	China scenario since first entry
740	Check saved checkpoints
266	Hit Rate Bar Chart
184	Display a single car with its mask
95	What are their top categories
141	Plot the Training Curves
484	Previous Credit and Cash
22	Seed everything for reproducibility
582	Load and process data
101	Is there a correlation between description length and price
117	Implementing the SIR model
793	It seems Goblins are a little similar to Ghouls
69	Make a simple restart of runtime at this point
851	Function which creates set of images for each axis
814	Calculate extra features for test set
349	Feature Agglomeration Results
722	Compare timing for CutMix
746	Check oversampled dataset
859	Lib and Load data
536	Get important features according to SHAP
829	Other special events
588	Build datasets objects
591	Inference on Test Set
259	Analysis based Averages values
685	targets in labels.csv
222	Primary Use and Meter Reading
34	Apply Underbalancing Techinique
719	Create Inference Dataset
120	Add country details
797	Add active column
751	Putting the principles in practice
9	Target varaible distribution
654	Adding some lag feature
342	We can finally define the CNN architecture
84	The magical function
656	Leak Data Validation
671	Binary Focal Loss
10	Text preprosesing source
405	Exploring Label Distribution
771	Tabulet per family features
94	More details on brands with a treemap
526	Set data loading parameters
827	At the scale of stores
170	FS with SelectFromModel and LinearSVR
848	Find final Thresshold
838	Composition of functions
703	process submission images
512	Choose and train a classifier
507	Granted applications per number of children
480	First we need to format the data and extract the labels
351	Bookings per day of week
705	we add some squared features for some model flexability
561	Add the actors to the renderer , set the background and size
774	Mix region and education
728	Hugging Face pretrained Bert model names
842	Ensure determinism in the results
495	Function to Aggregate Numeric Data
304	Text Processing of text data easily
686	targets in train.csv
325	Iran since first case
208	Use the images from our validation dataset to check the precdictions
744	Define the number of repetitions for each training example
869	Train the model , predict , etc
153	Set Up the Generators
231	City and Time features
562	OSIC training data Example
687	These labels are not in train
828	Perspective of Time
615	Relevant attributes of identified objects are stored
263	Paths to data and metadata
60	See how our generator work
586	Create fast tokenizer
766	roof and electricity
365	Show one Example of landmark images
373	missing value statistics
350	Predict on test set
799	Predict by Specify Country
533	differences between public and private test
857	Plot several examples of input images
757	Test data preparation
509	Read the data
75	What proportion of IP download the app
57	We should give CatBoost names of categorical features
882	We can safely store the two types into separate dataframes
547	This images from validation data seem to be really strange labeled ...
610	Training the model
465	First , we need to put our data into a long format dataframe
508	Income distribution and target value
861	We can see above the counts of higher damageDealt smoothly decrease
538	New Daily Cases
791	Show some examples of different mask
738	Get variables to apply weight decay in AdamW optimizer
49	Count occurance of words
786	Predict null data based on statistical method
439	The number of leaves on the other hand is a discrete uniform distribution
866	Label encoding Making it machine readable
540	Gaussian Approximation of Active Cases
855	Loading libraries and data
558	Exploratory Data Analysis
391	Price Variance in param
500	Function to Aggregate Stats at the Client Level
584	Build datasets objects
750	Data transformation and helper functions
523	Prepare for training
50	All contraction are known
677	Most of the dates overlap
658	Fast data loading
355	expanding the aggregate
682	Train Our Linear Regression Model
135	How many images are in each folder
239	MERGE , MISSING VALUE , FILL NA
103	Generating Graph Matrices from the Structures
113	FS with the Pearson correlation
737	Choose the model to use
802	Age distribution of male and female patients
645	Replace to Leak data
810	Conversions by OS
476	Score versus Iteration
212	Use of Bayesian Block in Santander Customer Transaction Prediction Challenge
545	Finally , we are ready to load the whole series into one matrix
422	Filling missing and infinite data by zeroes
6	No surprises , since this is all presumably artificial data
228	Prediction and Submission
31	Initialize Mobilenet Face Extractor
583	Load text data into memory
788	Prediction with xgboost
292	Create final submission DF
676	Then transform to a datetime object supposing that it is an ordinal datetime
297	Bathroom Count Vs Log Error
427	Calculate Distance between points using Haversine distance
834	What are Lung Opacities
247	Document Vectors with HashingVectorizer
816	Use machine learning model
277	When Do People Generally Reorder
864	Relationships betweeen the sets
185	How many different cars in train dataset
345	Exploratory Data Analysis
118	Join data , filter dates and clean missings
54	Using LabelEncoding we just change string values to numbers
690	Get Model into TPU
461	Aggregate Credit previous loans
26	Load pneumonia locations
518	Loading the data
557	Load Packages and Data
359	Read the csv files on the Johns Hopkins CSSE database on github
4	Impute any values will significantly affect the RMSE score for test set
463	Learning Rate Domain
48	FVC Progression by SmokingStatus
648	FIX Time Zone
375	The following is features with fine hist
477	Hyperparameters versus Iteration
30	And finally lets look at the class distribution
563	Predict Test Set and Submit Result
679	Rolling monthly and yearly store means
314	Prediction on test set
597	Create Dataset objects
145	MAKE A TEST SET PREDICTION
868	Let us split the variables one more time
755	Best parameters are searched by GridSearchCV on my Laptop
430	Train with Simple Features
215	Featurization of Training Data
139	Create a Directory Structure
308	Let us plot some bounding boxes right from the dictionary we just created
617	Import the modules
628	Ensemble with my historical best
764	Check the dataset
251	Convolutional Neural Network
85	We can now print the results
216	Fitting Logistic Regression with OneVsRest Classifier
650	Train model by each meter type
206	Converting the Input images to plot using plt
149	Create Binary Targets
553	Visualize processed samples , resize
559	Outlier Analysis and Feature Scaling
493	An important consideration is the missing values in the dataframe
92	Prices of the first level of categories
454	Categorical Aggregation Function
32	The data is not balanced
731	Try to load the latest checkpoint
487	Kernel Density Estimate Plots
552	Inspection of created samples , with normalization
694	Count game trainsition
142	What is the AUC Score
380	Import necessary libraries
570	Create real file paths dataframe
530	Map extracted timestamps to columns
203	Confusion Matrix for Train Data Predictions
473	Remove Low Importance Features
177	Basic skin detection
433	We can make the same plot by day of the week
190	Code from here and below is commented out because the kernel dies
549	Finding best alpha
98	Can we get some informations out of the item description
782	This parted was taken from the helpful kernel
504	Prediction for one image
97	Does shipping depends of price
419	Recursive Feature Elimination with Random Forest
770	BedRoom per family features
80	Download rate by hour
627	Blend and smooth for submission
429	Create Training and Validation Set
24	Unfreeze all layers and find best learning rate
436	Visualize Validation Predicted Target
336	and target vector that correspond to the test data size
363	Create variables of interest
428	The test distribution seems to be similar to the training distribution
499	Function to Calculate Missing Values
639	All train tasks predictions
692	Making Vocabulary and Text Conversion
46	Submission data prediction
883	Displaying sample image by host
640	Fast data loading
767	Make new features using continuous feature
778	Wiki News FastText Embeddings
785	Find Null data
29	Data is still small enough for memory so read to memory using pandas
761	Predictions class distribution
688	train valid split
445	Applied to Full Dataset
460	Aggregate Cash previous loans
733	Get metrics for validation dataset
187	Display some of similar cars
378	y hist with defferent timestamps are similar
338	Import Packages and Functions
603	Focal loss is good for unbalanced datasets , like this one
589	Load model into the TPU
453	Numeric Aggregation Function
219	Meter Reading and Meter Type
759	Convert DCM to PNG
86	Dealing with color
699	Create Image Augmentation Generator
281	Which item do people put into the cart first
41	Training on the complete Dataset now
224	Meter Readings over time And Primary Use
332	Seperating the data into different data frame based on the labels
879	RF for feature selection
194	Idea is to use clustering on images of one type to group data
381	Density plot of numeric features
333	most important or common positive words
129	Code from notebook
801	Predict all province greater than
104	Graph Representation of RNA structure
27	GPU version It is just function verification , not for performance evaluation
471	DFS with Selected Aggregation Primitives
387	Remove Extreme Prices
809	Conversions by App
413	For example , we can divide the years of schooling by the age
322	Comparison between Brazil and Italy
299	No Of Storeys Vs Log Error
119	Compute lags and trends
67	Saving the model
70	Hist Graph of scores
666	There is a gap between them
768	Ratio feature can have infinite values
510	Check the typical length of a comment
242	Loading the data
520	load mapping dictionaries
760	Split training set
753	Dealing with missed variables
534	Distribution of months in train and test
330	Lets gets started
811	Conversions by Device
466	Distribution of Search Values
696	View Single Image
296	Bedroom Count Vs Log Error
74	Zoom on this IP
143	Create a Confusion Matrix
156	Create a submission file
790	the difficuly of training different mask type is different
196	Newer products or products from newer stores are shorter than the oldest ones
420	Visualize Tree with No Maximum Depth
424	Data Exploration and Data Cleaning
434	Test Time Features
638	evaluation solved tasks
525	Initialize train and test DataFrames to access IDs and depth information
3	Imputations and Data Transformation
388	A good item price binning should be based on the price distribution
234	Encoding Cordinal Direction
221	Time of Day and Meter Reading
720	Define dataset and model
341	Build the Model
284	Visualizing Interest Level Vs Price
18	Getting to Know the Data
205	Transformations and Data Augmentation
524	KFold LGB model training
89	Combine it into a single function
191	Lets validate the test files
880	Evaluate the Model
693	Making Feature Matrices
630	Pick some frames to display
470	DFS with Default Primitives
197	Now , the clusters will be created using just the test set
556	Process test data in parallel
613	Reproducing simple baseline
167	How to make a generator run infinitely
781	Credit day prolong
637	train solved tasks
754	Starting importance variables evaluation
233	Encoding Street Names
157	Its also builds on kernel functions but is appropriate for unsupervised learning
837	If you like the content of this notebook , please consider upvoting it
789	Target , prediction process
96	What are the most expensive items
501	Monthly Cash Data
653	FIX Time Zone
531	Create DFs imitating public and private test subsets
850	Add leak to test
805	Here is a base model without parameter tuning .
207	Defining our Model
181	Thanks for the example of ensemling different models from
370	Check Unique Label
836	Opacities That Are Not Related to Pneumonia
182	Manager function to call the create features functions in multiple processes
673	Dense Added Model
726	Visualising the distribution of each product by age by boxplot
832	Load train and test dataframes and add length columns for Description and Name
571	Load and freeze DenseNet
61	Prepare Testing Data
635	Averaging over all instances with standard deviation as errorbar
88	Convert each labeled object to Run Line Encoding
717	Create dataset for training and Validation
784	Moving average is so simple
76	Does bots download the app
715	These have their kitchen area larger than the total area of the house
528	Perform check on randomly chosen mask and prediction
619	Check some augmentation effect
535	SHAP Interaction Values
590	Running the model on a Sample Image
282	Visualizing Distribution Of Price Before and After Removing Outliers
271	Visualize the feature interaction
469	Testing Results on Full Data
488	Correlations of Aggregated Values with Target
40	Preparing the training data
347	Random Forest Regressor
594	Load and preprocess data
426	Rides on Map of NYC
16	Look at the data types and some basic info about the different columns
415	Next we can rename the columns to make it easier to keep track
448	Standard imports for data science work
711	Using all features for model training
173	Set up the folds for cross validation
237	Loading the data
515	Define helper functions
238	Librairies and data
210	CatBoost Pool and Regression
268	Interpreting ROC Plot
418	Comparing Model Performance
258	Analysis Based on EXter Source Types
691	Loading and Visualization of Data
73	What proportion of click generate downloads
496	Function to Calculate Categorical Counts
232	Total Time Stopped
852	Function which creates final video from list of images
668	Model Evaluation and Validation
11	Model Validation on train data set
15	Maybe if we used the log plot things would be better
256	Checking the Imbalance of Target Variable
20	Visualizing Raw Variables
128	Showing Confusion Matrices
62	Create Testing Generator
361	Create dataframe listing the starting date of lockdowns
566	Create Dataset objects
843	LOAD PROCESSED TRAINING DATA FROM DISK
730	Choose the model to use
186	Display similar cars
519	and batch aggregations examples for the rest of the tables ..
616	Result of my new models
701	process training images
568	Create fake filepaths dataframe
110	Apply Logistic Regression
713	Split into train and test sets
712	Function to convert series to supervised learning
223	Meter Readings over time
815	Judge machine learning model
865	Feature primitives Basically which functions are we going to use to create features
702	process test images
303	Preprocessing of features
596	Save model and best hyperparams
107	Import Required Libraries
881	Load CSV files
522	extract different column types
132	Training prediction result visualization
261	Merging the bureau dataset along with application train dataset to do more analysis
443	Distribution of all Numeric Hyperparameters
695	Visualize by heatmap
399	Start by pivoting the DataFrame to explore the label distribution over slices
179	Some stats using jpg exif
138	Balance the target distribution
572	Creating and Training the Model
544	inspect datagen output
689	TPU Strategy and other configs
462	Hyperparameter Tuning Implementation
432	Explore Time Variables
260	Checking the Correlation Between The Features for Application Train Dataset
202	Using RandomForest Classifier
102	Data Info Method
680	Fitting the trend on the yearly time series
543	inspect validation samples
425	Empirical Cumulative Distribution Function Plot
171	FS with SelectFromModel and RandomForestRegressor
294	Dependent variable logerror follows nice normal distribution
414	Feature Engineering through Aggregations
374	Exception value processing
521	add breed mapping
160	Initialize the generators
698	Number of Patients and Images in Test Images Folder
220	Weekday and Meter Reading
831	Disable fastai randomness
254	Function for find out Numerical and categeical Variables
505	Train and validate
633	Plotting errors for one sample
91	Zoom on the second level
486	Putting it all Together
437	Evaluate Best Model from Random Search
174	Genetic program model , main code loop
316	Display train and test signals
68	You can state below link to your copy of this MMDetection repo
360	Transpose the dataframes
204	Confusion Matrix for Test Data Predictions
807	Only a small proportion of clicks were followed by a download
847	The method for training is borrowed from
819	For neutral samples , use original texts as they are
611	Load Test dataframe
125	Prepare Training Data
845	LOAD DATASET FROM DISK
267	Rescaling the Image Most image preprocessing functions want the image as grayscale
384	Ordinal features mapping
860	Explore distribution of single variable
709	Create dataset with look back
735	Hugging Face pretrained Bert model names
200	Confusion Matrix for Train Data Predictions
178	Apply skin segmentation on all training data and visualize the result
2	Seems like a very wide range of values , relatively spaking
189	Define some constants for data location
458	Aggregate previous loans at Home Credit
100	Can the length of the description give us some informations
551	Inspection of created samples , without normalization
421	Lets first check the Train Target Distribution
217	Importing Packages and Collecting Data
732	Run on validation dataset
248	Document Vectors with hashing trick
752	Show influence of economical factors on housing prices
447	Bayesian Optimization on the Full Dataset
214	Word map for most frequent Tags
826	cut down features to avoid the kernel die
198	The number of samples in each cluster is the following
665	At first , I made Europe future
609	Losses and Metrics
514	Now predict the result for each toxic level
354	Interactive booking , click , and percentage of booking trends with Bokeh
840	Is a program solution
158	Thanks for the example of ensemling different models from
818	This notebook will deal with positive , negative and neutral samples independently
779	Check null data
475	Distribution of Scores
114	FS with SelectFromModel and LinearSVR
364	Compare our forecast with actual evolution both for training and validation sets
243	Training the model on CPU
339	Load and Explore Data
655	Train model by each meter type
126	Showing Confusion Matrices
765	Check null data
58	Prepare Traning Data
78	How many times , each categories of clickers download the app
647	Leak Data loading and concat
411	In most cases , the values are very similar
168	Thanks to Automatic FE The main code for basic FE
636	Ploting one sample with the corresponding error as error bars
481	Read in Data and Create Smaller Datasets
601	Create new labels
830	Inspect the data at different time scale
604	Create test generator
474	Align Train and Test Sets
59	See sample image
134	Using my notebook
19	The Shape of the Data
527	Define data loading
718	CNN Model for multiclass classification
502	Monthly Credit Data
772	phone per family features
127	Divide features into groups
674	OUTPUT OF AUGMENTATED IMAGES
45	test prediction by LGBM and feature importance check
290	Main Config Variables
130	My upgrade of parameters
306	Let us read the masks
124	Parameters and LB score visualization
749	Random Forest model
440	Complete Bayesian Domain
575	Pad and resize all the images
624	Build the original and translated test data
532	train and test rows fraction intersection
646	Fast data loading
8	Read data set
47	FVC Progression by Sex
43	Unique value counts
55	We should split our dataset to create validation and train parts
431	Use More Features
742	Get labels and their countings
56	Using embedding in NN we can change dimensionality of categorical features
382	Let take some small visualizations with these bool features
71	I updated importation for a faster version
824	Some Feature Engineering
503	Split into training and validation groups
643	Adding some lag feature
335	Most important or common words in neutral data
516	Predict validation and test set masks
87	Deriving individual masks for each object
672	Efficient Net Architecture
39	Show and save column comparision matrix and save row sets
106	plotting the scan
35	The data is not balanced
642	FIX Time Zone
567	Load Model into TPU
275	When Do People Generally Order
513	Now train the models with a partial fit approach
657	Find Best Weight
112	Thanks to Automatic FE The main code for basic FE
144	Create a Classification Report
51	Convert to lower case Clean contractions Clean special charactor Convert small caps
858	The competition metric relies only on the order of recods ignoring IDs
412	Redundant Individual Variables
741	An example usage
279	Best Selling Products
580	Plotting some random images to check how cleaning works
82	Number of clicks from the IP during the last minute
573	Save model and weights
758	Data preparation for test
324	Spain since first recorded case
253	Importing The Dataset
146	Set up the generator
211	CatBoost is RAM expensive so I prefer to utilize GPU
269	Load data and fit some models
710	Make prediction and apply invert scaling
310	Now we can read the masks for the specific image
65	Create and set up the model
362	Join all dataframes
620	Validation on real world data
587	Load text data into memory
806	this graph is adapted from
318	Trivial segmentation stuff
199	Using DecisionTree Classifier
66	Counting the metric score
270	Fit an XGBoost classifier model
320	Replacing Mainland china with just China
485	Create Custom Feature Primitives
435	Try with All Time Variables
825	Stacking models and training
870	That is the size of one test example that we ought to predict
163	Make a Prediction
707	Split into train and test sets
385	Predict test set and make submission
393	Hit Rate and Charge Distribution
506	Locating a face within an image
298	Room Count Vs Log Error
389	Use Ad Image to Identify Item Category
38	Loading data etc
763	Now , we can downcast numeric columns in the same way
386	Draw network graph to see which ingredients are linked together
541	load mapping dictionaries
734	Run on test dataset
353	Bookings by month
286	Visualizing Interest Level Vs Bedrooms
192	Load tabular data and create new features
28	Define a Convolutional Neural Network following Yoon Kim model
21	Importing all Libraries
455	Combined Aggregation Function
581	The filtration step for RGB images may take a lot of time
683	Next , we will make prediction with our LR Model
90	So , some categories are expensive , but most are cheap
660	Adding some lag feature
169	FS with the Pearson correlation
684	Sampling the train data since too much data
172	Load data files
201	Confusion Matrix for Test Data Predictions
550	Mismatches on a validation data
623	Creating submission file
209	Data load and process functions
376	yÁöÑÂºÇÂ∏∏ÂÄº drop samples which have exception value in y
407	Families without Heads of Household
340	Create the embedding layer
736	Try the demo model
289	Correlation Between Price and Other Features
315	We can now create a submission file
17	Do some simple plots for exploration
302	Loading and preprocessing data
358	Read the csv files from kaggle
403	Combinations of TTA
52	Train on full data
498	Function to Convert Data Types
874	Reading in the data , as usual
183	Here is a plot of sorted category counts
670	Importing Library Files
311	For the same window we superimpose the masks above the image
309	Let us load one image and its masks
457	Merge with the main dataframe
400	As a Neuroradiologist , this distribution looks pretty true to daily practice
280	Top Reordered Products
577	Group and Reduce
402	Demonstration how it works
482	Properly Representing Variable Types
152	Key No Diabetic Retinopathy Has Diabetic Retinopathy
464	Grid Search Implementation
213	Importing necessary libraries
704	process tabular features
497	Function for KDE Plots of Variable
875	Now that we have dataset in desired form , i.e
777	Load and preprocess data
246	Document Vectors with TfidfVectorizer
371	Non physical data augmentation
7	Now there appears to be one feature that is not gaussian
136	Create a Dataframe containing all images
264	Group signals metadata accroding to target
245	Create Document Vectors
675	Construct the graph and plot it
739	Try to load the latest checkpoint
820	Write a problem file
377	Divide the test set and the training set
395	Correlation with Deal Probability
706	Random Search Confidence
884	After we impute it , we can observe standard fluctuation
176	Display all test images
44	Unique count of Categorical values
511	Split comments into array of words
356	a quick check if demand distribution changes week to week
600	Saving as JPEG
539	Exponential Growth Curves
775	Remove feature with only one value
195	Test on the data that is not seen by the network during training
483	Plot for a sanity check
450	Admit and Correct Mistakes
164	Process the Predictions
849	Add train leak
121	Ridge Regression for one country
678	Adding categorical features
180	Its also builds on kernel functions but is appropriate for unsupervised learning
444	Evolution of Search
796	Get data from
334	Most important or common negative words
236	Which Team Wins And Lost The Most Championships
626	Predict with mixed language models
592	Load and preprocess data
398	Show Original Image
723	Compare timing for MixUp
606	Interest based on geographical location
576	Load Model into TPU
729	Make TF record file for test dataset
410	Per Capita Features
12	Roc AUC curve
652	Fast data loading
787	Onehot encoding for categorical data
565	TPU Strategy and other configs
459	Aggregate Installments Data
792	Null data check
467	The following code repeats this plot for all the of the numeric hyperparameters
468	Sequence of Search Values
300	Gaussian Noise on Target
326	USA since first case
166	What is a python generator
240	Loading the data
331	Full data Analysis
449	Now we want to combine the data without creating any duplicate rows
634	Some correlation is indeed present
542	inspect training samples
697	Number of Patients and Images in Training Images Folder
64	Split the data into train and validation parts
5	Detect and Correct Outliers
667	We will also evaluate the position of the data points using the coordinates
607	Reducing Image Size
871	Let us proceede with further interesting EDA
629	This is a simple modify from
272	Visualize the default split and gains for all models
438	Now we can evaluate the baseline model on the testing data
346	Modeling and Prediction
14	And it looks like a fairly nice distribution , albeit still fairly asymetrical
846	The mean of the two is used as the final embedding matrix
123	Thanks for the example of ensemling different models from
491	Putting the Functions Together
441	Example of Sampling from the Domain
105	Loading the files
307	So a unique operator will give us the unique filenames that contain ships
622	Display the dropped images
255	Identifying Missing Value Present in Application Train Dataset
681	Merge seed for each team
