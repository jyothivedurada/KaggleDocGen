3244	def get_security_group ( security_group , flags = FLAGS . ALL , * * kwargs ) : result = registry . build_out ( flags , start_with = security_group , pass_datastructure = True , * * kwargs ) result . pop ( 'security_group_rules' , [ ] ) return result
3338	def is_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and childUri . rstrip ( "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
415	def save_dataset ( self , dataset = None , dataset_name = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) # print("[Database] Save params: {} SUCCESS, took: {}s".format(file_name, round(time.time()-s, 2))) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
3811	def get_request_header ( self ) : # resource is allowed to be null if it's not available yet (the Chrome # client does this for the first getentitybyid call) if self . _client_id is not None : self . _request_header . client_identifier . resource = self . _client_id return self . _request_header
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
13012	def pprint ( arr , columns = ( 'temperature' , 'luminosity' ) , names = ( 'Temperature (Kelvin)' , 'Luminosity (solar units)' ) , max_rows = 32 , precision = 2 ) : if max_rows is True : pd . set_option ( 'display.max_rows' , 1000 ) elif type ( max_rows ) is int : pd . set_option ( 'display.max_rows' , max_rows ) pd . set_option ( 'precision' , precision ) df = pd . DataFrame ( arr . flatten ( ) , index = arr [ 'id' ] . flatten ( ) , columns = columns ) df . columns = names return df . style . format ( { names [ 0 ] : '{:.0f}' , names [ 1 ] : '{:.2f}' } )
12427	def get_nginx_config ( self ) : if os . path . exists ( self . _nginx_config ) : return open ( self . _nginx_config , 'r' ) . read ( ) else : return None
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## Get counts on down edges. ## How to treat polytomies here? if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) ## everyone else lenul = tots - ( lendr + lendl + lenur ) ## return product return lendr * lendl * lenur * lenul
13639	def bind ( mod_path , with_path = None ) : if with_path : if os . path . isdir ( with_path ) : sys . path . insert ( 0 , with_path ) else : sys . path . insert ( 0 , with_path . rsplit ( '/' , 2 ) [ 0 ] ) pass # raise `ImportError` mod_path if not exist mod = importlib . import_module ( mod_path ) settings = Settings ( ) for v in dir ( mod ) : if v [ 0 ] == '_' or type ( getattr ( mod , v ) ) . __name__ == 'module' : continue setattr ( settings , v , getattr ( mod , v ) ) pass Settings . _path = mod_path Settings . _wrapped = settings return settings
12575	def apply_mask ( self , mask_img ) : self . set_mask ( mask_img ) return self . get_data ( masked = True , smoothed = True , safe_copy = True )
4291	def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
7611	def get_top_clans ( self , location_id = 'global' , * * params : keys ) : url = self . api . LOCATIONS + '/' + str ( location_id ) + '/rankings/clans' return self . _get_model ( url , PartialClan , * * params )
3334	def string_repr ( s ) : if compat . is_bytes ( s ) : res = "{!r}: " . format ( s ) for b in s : if type ( b ) is str : # Py2 b = ord ( b ) res += "%02x " % b return res return "{}" . format ( s )
11609	def multiply ( self , multiplier , axis = None ) : if self . finalized : if multiplier . ndim == 1 : if axis == 0 : # multiplier is np.array of length |haplotypes| raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : # multiplier is np.array of length |loci| sz = len ( multiplier ) multiplier_mat = lil_matrix ( ( sz , sz ) ) multiplier_mat . setdiag ( multiplier ) for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] * multiplier_mat elif axis == 2 : # multiplier is np.array of length |reads| for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices ] else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif multiplier . ndim == 2 : if axis == 0 : # multiplier is sp.sparse matrix of shape |reads| x |haplotypes| for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices , hid ] elif axis == 1 : # multiplier is sp.sparse matrix of shape |reads| x |loci| for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier ) elif axis == 2 : # multiplier is np.matrix of shape |haplotypes| x |loci| for hid in xrange ( self . shape [ 1 ] ) : multiplier_vec = multiplier [ hid , : ] multiplier_vec = multiplier_vec . ravel ( ) self . data [ hid ] . data *= multiplier_vec . repeat ( np . diff ( self . data [ hid ] . indptr ) ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif isinstance ( multiplier , Sparse3DMatrix ) : # multiplier is Sparse3DMatrix object for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier . data [ hid ] ) else : raise RuntimeError ( 'The multiplier should be 1, 2 dimensional numpy array or a Sparse3DMatrix object.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
697	def bestModelIdAndErrScore ( self , swarmId = None , genIdx = None ) : if swarmId is None : return ( self . _bestModelID , self . _bestResult ) else : if swarmId not in self . _swarmBestOverall : return ( None , numpy . inf ) # Get the best score, considering the appropriate generations genScores = self . _swarmBestOverall [ swarmId ] bestModelId = None bestScore = numpy . inf for ( i , ( modelId , errScore ) ) in enumerate ( genScores ) : if genIdx is not None and i > genIdx : break if errScore < bestScore : bestScore = errScore bestModelId = modelId return ( bestModelId , bestScore )
4679	def getAccountFromPrivateKey ( self , wif ) : pub = self . publickey_from_wif ( wif ) return self . getAccountFromPublicKey ( pub )
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , * * kwargs ) : # normal = direction / np.sqrt(np.dot(direction, direction)) normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , * * kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
9670	def normalize ( self , string ) : return '' . join ( [ self . _normalize . get ( x , x ) for x in nfd ( string ) ] )
160	def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
3860	def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : # Reserve the "-" and "--" namespace. # If the column has no leading "-", treat it as an environment variable col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
13763	def _wrap_color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" # Manage the format parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has_colors and self . colors_enabled : # Set brightness st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset_all' ] ) else : return text
8552	def update_image ( self , image_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/images/' + image_id , method = 'PATCH' , data = json . dumps ( data ) ) return response
9735	def get_3d_markers ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPosition , component_info , data , component_position )
4334	def norm ( self , db_level = - 3.0 ) : if not is_number ( db_level ) : raise ValueError ( 'db_level must be a number.' ) effect_args = [ 'norm' , '{:f}' . format ( db_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'norm' ) return self
7309	def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
10859	def update ( self , params , values ) : #1. Figure out if we're going to do a global update, in which # case we just draw from scratch. global_update , particles = self . _update_type ( params ) # if we are doing a global update, everything must change, so # starting fresh will be faster instead of add subtract if global_update : self . set_values ( params , values ) self . initialize ( ) return # otherwise, update individual particles. delete the current versions # of the particles update the particles, and redraw them anew at the # places given by (params, values) oldargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set_values ( params , values ) newargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )
13827	def get_doc ( doc_id , db_name , server_url = 'http://127.0.0.1:5984/' , rev = None ) : db = get_server ( server_url ) [ db_name ] if rev : headers , response = db . resource . get ( doc_id , rev = rev ) return couchdb . client . Document ( response ) return db [ doc_id ]
10818	def can_invite_others ( self , user ) : if self . is_managed : return False elif self . is_admin ( user ) : return True elif self . subscription_policy != SubscriptionPolicy . CLOSED : return True else : return False
13084	def add_tag ( ) : if len ( sys . argv ) > 1 : tag = sys . argv [ 1 ] doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : count = 0 for obj in doc_mapper . get_pipe ( ) : obj . add_tag ( tag ) obj . update ( tags = obj . tags ) count += 1 print_success ( "Added tag '{}' to {} object(s)" . format ( tag , count ) ) else : print_error ( "Please use this script with pipes" ) else : print_error ( "Usage: jk-add-tag <tag>" ) sys . exit ( )
11968	def _dec_to_bin ( ip ) : bits = [ ] while ip : bits . append ( _BYTES_TO_BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
5572	def is_valid_with_config ( self , config ) : validate_values ( config , [ ( "schema" , dict ) , ( "path" , str ) ] ) validate_values ( config [ "schema" ] , [ ( "properties" , dict ) , ( "geometry" , str ) ] ) if config [ "schema" ] [ "geometry" ] not in [ "Geometry" , "Point" , "MultiPoint" , "Line" , "MultiLine" , "Polygon" , "MultiPolygon" ] : raise TypeError ( "invalid geometry type" ) return True
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
5602	def create_app ( mapchete_files = None , zoom = None , bounds = None , single_input_file = None , mode = "continue" , debug = None ) : from flask import Flask , render_template_string app = Flask ( __name__ ) mapchete_processes = { os . path . splitext ( os . path . basename ( mapchete_file ) ) [ 0 ] : mapchete . open ( mapchete_file , zoom = zoom , bounds = bounds , single_input_file = single_input_file , mode = mode , with_cache = True , debug = debug ) for mapchete_file in mapchete_files } mp = next ( iter ( mapchete_processes . values ( ) ) ) pyramid_type = mp . config . process_pyramid . grid pyramid_srid = mp . config . process_pyramid . crs . to_epsg ( ) process_bounds = "," . join ( [ str ( i ) for i in mp . config . bounds_at_zoom ( ) ] ) grid = "g" if pyramid_srid == 3857 else "WGS84" web_pyramid = BufferedTilePyramid ( pyramid_type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : """Render and hosts the appropriate OpenLayers instance.""" return render_template_string ( pkgutil . get_data ( 'mapchete.static' , 'index.html' ) . decode ( "utf-8" ) , srid = pyramid_srid , process_bounds = process_bounds , is_mercator = ( pyramid_srid == 3857 ) , process_names = mapchete_processes . keys ( ) ) @ app . route ( "/" . join ( [ "" , "wmts_simple" , "1.0.0" , "<string:mp_name>" , "default" , grid , "<int:zoom>" , "<int:row>" , "<int:col>.<string:file_ext>" ] ) , methods = [ 'GET' ] ) def get ( mp_name , zoom , row , col , file_ext ) : """Return processed, empty or error (in pink color) tile.""" logger . debug ( "received tile (%s, %s, %s) for process %s" , zoom , row , col , mp_name ) # convert zoom, row, col into tile object using web pyramid return _tile_response ( mapchete_processes [ mp_name ] , web_pyramid . tile ( zoom , row , col ) , debug ) return app
12766	def distances ( self ) : distances = [ ] for label in self . labels : joint = self . joints . get ( label ) distances . append ( [ np . nan , np . nan , np . nan ] if joint is None else np . array ( joint . getAnchor ( ) ) - joint . getAnchor2 ( ) ) return np . array ( distances )
6124	def gaussian_prior_model_for_arguments ( self , arguments ) : new_model = copy . deepcopy ( self ) for key , value in filter ( lambda t : isinstance ( t [ 1 ] , pm . PriorModel ) , self . __dict__ . items ( ) ) : setattr ( new_model , key , value . gaussian_prior_model_for_arguments ( arguments ) ) return new_model
5556	def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ # order of operators is important: # prematurely return in cases of "<=" or ">=", otherwise # _strip_zoom() cannot parse config strings starting with "<" # or ">" ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
10691	def rgb_to_hex ( rgb ) : r , g , b = rgb return "#{0}{1}{2}" . format ( hex ( int ( r ) ) [ 2 : ] . zfill ( 2 ) , hex ( int ( g ) ) [ 2 : ] . zfill ( 2 ) , hex ( int ( b ) ) [ 2 : ] . zfill ( 2 ) )
9546	def add_record_check ( self , record_check , modulus = 1 ) : assert callable ( record_check ) , 'record check must be a callable function' t = record_check , modulus self . _record_checks . append ( t )
5988	def scaled_deflection_stack_from_plane_and_scaling_factor ( plane , scaling_factor ) : def scale ( grid ) : return np . multiply ( scaling_factor , grid ) if plane . deflection_stack is not None : return plane . deflection_stack . apply_function ( scale ) else : return None
12831	def validate_xml_text ( text ) : bad_chars = __INVALID_XML_CHARS & set ( text ) if bad_chars : for offset , c in enumerate ( text ) : if c in bad_chars : raise RuntimeError ( 'invalid XML character: ' + repr ( c ) + ' at offset ' + str ( offset ) )
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
7412	def run_tree_inference ( self , nexus , idx ) : ## create a tmpdir for this test tmpdir = tempfile . tempdir tmpfile = os . path . join ( tempfile . NamedTemporaryFile ( delete = False , prefix = str ( idx ) , dir = tmpdir , ) ) ## write nexus to tmpfile tmpfile . write ( nexus ) tmpfile . flush ( ) ## infer the tree rax = raxml ( name = str ( idx ) , data = tmpfile . name , workdir = tmpdir , N = 1 , T = 2 ) rax . run ( force = True , block = True , quiet = True ) ## clean up tmpfile . close ( ) ## return tree order order = get_order ( toytree . tree ( rax . trees . bestTree ) ) return "" . join ( order )
3685	def set_from_PT ( self , Vs ) : # All roots will have some imaginary component; ignore them if > 1E-9 good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : # Even in the case of three real roots, it is still the min/max that make sense self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
11657	def fit ( self , X , y = None ) : X = check_array ( X , copy = self . copy , dtype = [ np . float64 , np . float32 , np . float16 , np . float128 ] ) feature_range = self . feature_range if feature_range [ 0 ] >= feature_range [ 1 ] : raise ValueError ( "Minimum of desired feature range must be smaller" " than maximum. Got %s." % str ( feature_range ) ) if self . fit_feature_range is not None : fit_feature_range = self . fit_feature_range if fit_feature_range [ 0 ] >= fit_feature_range [ 1 ] : raise ValueError ( "Minimum of desired (fit) feature range must " "be smaller than maximum. Got %s." % str ( feature_range ) ) if ( fit_feature_range [ 0 ] < feature_range [ 0 ] or fit_feature_range [ 1 ] > feature_range [ 1 ] ) : raise ValueError ( "fit_feature_range must be a subset of " "feature_range. Got %s, fit %s." % ( str ( feature_range ) , str ( fit_feature_range ) ) ) feature_range = fit_feature_range data_min = np . min ( X , axis = 0 ) data_range = np . max ( X , axis = 0 ) - data_min # Do not scale constant features data_range [ data_range == 0.0 ] = 1.0 self . scale_ = ( feature_range [ 1 ] - feature_range [ 0 ] ) / data_range self . min_ = feature_range [ 0 ] - data_min * self . scale_ self . data_range = data_range self . data_min = data_min return self
13505	def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True
6074	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_ellipse_in_units ( major_axis = major_axis , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
5451	def convert_to_label_chars ( s ) : # We want the results to be user-friendly, not just functional. # So we can't base-64 encode it. # * If upper-case: lower-case it # * If the char is not a standard letter or digit. make it a dash # March 2019 note: underscores are now allowed in labels. # However, removing the conversion of underscores to dashes here would # create inconsistencies between old jobs and new jobs. # With existing code, $USER "jane_doe" has a user-id label of "jane-doe". # If we remove the conversion, the user-id label for new jobs is "jane_doe". # This makes looking up old jobs more complicated. accepted_characters = string . ascii_lowercase + string . digits + '-' def label_char_transform ( char ) : if char in accepted_characters : return char if char in string . ascii_uppercase : return char . lower ( ) return '-' return '' . join ( label_char_transform ( c ) for c in s )
1913	def locked_context ( self , key = None , default = dict ) : keys = [ 'policy' ] if key is not None : keys . append ( key ) with self . _executor . locked_context ( '.' . join ( keys ) , default ) as policy_context : yield policy_context
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
7736	def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
8892	def get_default ( self ) : if self . has_default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
2743	def load ( self ) : identifier = None if self . id : identifier = self . id elif self . fingerprint is not None : identifier = self . fingerprint data = self . get_data ( "account/keys/%s" % identifier , type = GET ) ssh_key = data [ 'ssh_key' ] # Setting the attribute values for attr in ssh_key . keys ( ) : setattr ( self , attr , ssh_key [ attr ] ) self . id = ssh_key [ 'id' ]
1038	def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) #TODO:explicitly delete all subfolders, star-delete doesn't work r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
7159	def ask ( self , error = None ) : q = self . next_question if q is None : return try : answer = q . prompter ( self . get_prompt ( q , error ) , * q . prompter_args , * * q . prompter_kwargs ) except QuestionnaireGoBack as e : steps = e . args [ 0 ] if e . args else 1 if steps == 0 : self . ask ( ) # user can redo current question even if `can_go_back` is `False` return self . go_back ( steps ) else : if q . _validate : error = q . _validate ( answer ) if error : self . ask ( error ) return if q . _transform : answer = q . _transform ( answer ) self . answers [ q . key ] = answer return answer
3588	def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
1874	def MOVHPD ( cpu , dest , src ) : if src . size == 128 : assert dest . size == 64 dest . write ( Operators . EXTRACT ( src . read ( ) , 64 , 64 ) ) else : assert src . size == 64 and dest . size == 128 value = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) # low part dest . write ( Operators . CONCAT ( 128 , src . read ( ) , value ) )
573	def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )
2277	def parse_generator_doubling ( config ) : start = 1 if 'start' in config : start = int ( config [ 'start' ] ) # We cannot simply use start as the variable, because of scoping # limitations def generator ( ) : val = start while ( True ) : yield val val = val * 2 return generator ( )
447	def _bias_scale ( x , b , data_format ) : if data_format == 'NHWC' : return x * b elif data_format == 'NCHW' : return x * _to_channel_first_bias ( b ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
4507	def get_device ( self , id = None ) : if id is None : if not self . devices : raise ValueError ( 'No default device for %s' % self . hardware_id ) id , ( device , version ) = sorted ( self . devices . items ( ) ) [ 0 ] elif id in self . devices : device , version = self . devices [ id ] else : error = 'Unable to find device with ID %s' % id log . error ( error ) raise ValueError ( error ) log . info ( "Using COM Port: %s, Device ID: %s, Device Ver: %s" , device , id , version ) return id , device , version
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , * * kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , * * kwargs )
5446	def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
6894	def parallel_starfeatures_lcdir ( lcdir , outdir , lc_catalog_pickle , neighbor_radius_arcsec , fileglob = None , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS , recursive = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob # now find the files LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting starfeatures...' % len ( matching ) ) return parallel_starfeatures ( matching , outdir , lc_catalog_pickle , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
11977	def get_bits ( self ) : return _convert ( self . _ip , notation = NM_BITS , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( * * jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
9759	def statuses ( ctx , job , page ) : def get_experiment_statuses ( ) : try : response = PolyaxonClient ( ) . experiment . get_statuses ( user , project_name , _experiment , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could get status for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for experiment `{}`.' . format ( _experiment ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for experiment `{}`.' . format ( _experiment ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'experiment' , None ) dict_tabulate ( objects , is_list_dict = True ) def get_experiment_job_statuses ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_statuses ( user , project_name , _experiment , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True ) page = page or 1 user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_statuses ( ) else : get_experiment_statuses ( )
7546	def make_chunks ( data , samples , lbview ) : ## first progress bar start = time . time ( ) printstr = " chunking clusters | {} | s5 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) ## send off samples to be chunked lasyncs = { } for sample in samples : lasyncs [ sample . name ] = lbview . apply ( chunk_clusters , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in lasyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures for sample in samples : if not lasyncs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , lasyncs [ sample . name ] . exception ( ) ) return lasyncs
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , * * kwargs ) : """Wrapper for methods decorated with `hold_exception`.""" # pylint: disable=W0703,W0212 try : return method ( self , * args , * * kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) # pylint: disable=W0212 main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) # Commit the configuration otherwise the jija2_env won't have # a `globals` assignment. config . commit ( ) # Place a few globals in the template environment. from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
5550	def get_zoom_levels ( process_zoom_levels = None , init_zoom_levels = None ) : process_zoom_levels = _validate_zooms ( process_zoom_levels ) if init_zoom_levels is None : return process_zoom_levels else : init_zoom_levels = _validate_zooms ( init_zoom_levels ) if not set ( init_zoom_levels ) . issubset ( set ( process_zoom_levels ) ) : raise MapcheteConfigError ( "init zooms must be a subset of process zoom" ) return init_zoom_levels
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
11065	def acl_show ( self , msg , args ) : name = args [ 0 ] if len ( args ) > 0 else None if name is None : return "%s: The following ACLs are defined: %s" % ( msg . user , ', ' . join ( self . _acl . keys ( ) ) ) if name not in self . _acl : return "Sorry, couldn't find an acl named '%s'" % name return '\n' . join ( [ "%s: ACL '%s' is defined as follows:" % ( msg . user , name ) , "allow: %s" % ', ' . join ( self . _acl [ name ] [ 'allow' ] ) , "deny: %s" % ', ' . join ( self . _acl [ name ] [ 'deny' ] ) ] )
4710	def get_chunk_information ( self , chk , lun , chunk_name ) : cmd = [ "nvm_cmd rprt_lun" , self . envs , "%d %d > %s" % ( chk , lun , chunk_name ) ] status , _ , _ = cij . ssh . command ( cmd , shell = True ) return status
8181	def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] # Remove all edges involving id and all links to it. for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
1491	def get_serializer ( context ) : cluster_config = context . get_cluster_config ( ) serializer_clsname = cluster_config . get ( constants . TOPOLOGY_SERIALIZER_CLASSNAME , None ) if serializer_clsname is None : return PythonSerializer ( ) else : try : topo_pex_path = context . get_topology_pex_path ( ) pex_loader . load_pex ( topo_pex_path ) serializer_cls = pex_loader . import_and_get_class ( topo_pex_path , serializer_clsname ) serializer = serializer_cls ( ) return serializer except Exception as e : raise RuntimeError ( "Error with loading custom serializer class: %s, with error message: %s" % ( serializer_clsname , str ( e ) ) )
11211	def normalized ( self ) : # Cascade remainders down (rounding each to roughly nearest # microsecond) days = int ( self . days ) hours_f = round ( self . hours + 24 * ( self . days - days ) , 11 ) hours = int ( hours_f ) minutes_f = round ( self . minutes + 60 * ( hours_f - hours ) , 10 ) minutes = int ( minutes_f ) seconds_f = round ( self . seconds + 60 * ( minutes_f - minutes ) , 8 ) seconds = int ( seconds_f ) microseconds = round ( self . microseconds + 1e6 * ( seconds_f - seconds ) ) # Constructor carries overflow back up with call to _fix() return self . __class__ ( years = self . years , months = self . months , days = days , hours = hours , minutes = minutes , seconds = seconds , microseconds = microseconds , leapdays = self . leapdays , year = self . year , month = self . month , day = self . day , weekday = self . weekday , hour = self . hour , minute = self . minute , second = self . second , microsecond = self . microsecond )
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
13326	def create ( name_or_path , config ) : if not name_or_path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv create my_env\n' ' cpenv create ./relative/path/to/my_env\n' ' cpenv create my_env --config ./relative/path/to/config\n' ' cpenv create my_env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name_or_path ) ) try : env = cpenv . create ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
950	def corruptVector ( v1 , noiseLevel , numActiveCols ) : size = len ( v1 ) v2 = np . zeros ( size , dtype = "uint32" ) bitsToSwap = int ( noiseLevel * numActiveCols ) # Copy the contents of v1 into v2 for i in range ( size ) : v2 [ i ] = v1 [ i ] for _ in range ( bitsToSwap ) : i = random . randrange ( size ) if v2 [ i ] == 1 : v2 [ i ] = 0 else : v2 [ i ] = 1 return v2
10866	def update ( self , params , values ) : # radparams = self.param_radii() params = listify ( params ) values = listify ( values ) for i , p in enumerate ( params ) : # if (p in radparams) & (values[i] < 0): if ( p [ - 2 : ] == '-a' ) and ( values [ i ] < 0 ) : values [ i ] = 0.0 super ( PlatonicSpheresCollection , self ) . update ( params , values )
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) # This is a cleverer way of doing # # for (u, v) in matches.items(): # P[u, v] = 1 # P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
6025	def geometry_from_grid ( self , grid , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer pixel_scales = ( float ( ( y_max - y_min ) / self . shape [ 0 ] ) , float ( ( x_max - x_min ) / self . shape [ 1 ] ) ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) pixel_neighbors , pixel_neighbors_size = self . neighbors_from_pixelization ( ) return self . Geometry ( shape = self . shape , pixel_scales = pixel_scales , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
11776	def WeightedMajority ( predictors , weights ) : def predict ( example ) : return weighted_mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
12245	def get_all_buckets ( self , * args , * * kwargs ) : if kwargs . pop ( 'force' , None ) : buckets = super ( S3Connection , self ) . get_all_buckets ( * args , * * kwargs ) for bucket in buckets : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return buckets return [ Bucket ( self , bucket ) for bucket in mimicdb . backend . smembers ( tpl . connection ) ]
2869	def setup ( self , pin , mode , pull_up_down = PUD_OFF ) : self . rpi_gpio . setup ( pin , self . _dir_mapping [ mode ] , pull_up_down = self . _pud_mapping [ pull_up_down ] )
2034	def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 1 ) self . _store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )
9637	def format ( self , record ) : data = record . _raw . copy ( ) # serialize the datetime date as utc string data [ 'time' ] = data [ 'time' ] . isoformat ( ) # stringify exception data if data . get ( 'traceback' ) : data [ 'traceback' ] = self . formatException ( data [ 'traceback' ] ) return json . dumps ( data )
10260	def remove_falsy_values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }
5805	def get_dh_params_length ( server_handshake_bytes ) : output = None dh_params_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0c' : dh_params_bytes = message_data break if dh_params_bytes : break if dh_params_bytes : output = int_from_bytes ( dh_params_bytes [ 0 : 2 ] ) * 8 return output
5286	def post ( self , request , * args , * * kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
650	def generateSequences ( nPatterns = 10 , patternLen = 500 , patternActivity = 50 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSimpleSequences = 50 , nHubSequences = 50 ) : # Create the input patterns patterns = generateCoincMatrix ( nCoinc = nPatterns , length = patternLen , activity = patternActivity ) # Create the raw sequences seqList = generateSimpleSequences ( nCoinc = nPatterns , seqLength = seqLength , nSeq = nSimpleSequences ) + generateHubSequences ( nCoinc = nPatterns , hubs = hubs , seqLength = seqLength , nSeq = nHubSequences ) # Return results return ( seqList , patterns )
6875	def _pyuncompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = sqlitecurve . replace ( '.gz' , '' ) try : if os . path . exists ( outfile ) and not force : return outfile else : with gzip . open ( sqlitecurve , 'rb' ) as infd : with open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) # do not remove the intput file yet if os . path . exists ( outfile ) : return outfile except Exception as e : return None
5684	def day_start_ut ( self , ut ) : # set timezone to the one of gtfs old_tz = self . set_current_process_time_zone ( ) ut = time . mktime ( time . localtime ( ut ) [ : 3 ] + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return ut
2435	def reset_creation_info ( self ) : # FIXME: this state does not make sense self . created_date_set = False self . creation_comment_set = False self . lics_list_ver_set = False
12172	def genPNGs ( folder , files = None ) : if files is None : files = glob . glob ( folder + "/*.*" ) new = [ ] for fname in files : ext = os . path . basename ( fname ) . split ( "." ) [ - 1 ] . lower ( ) if ext in [ 'tif' , 'tiff' ] : if not os . path . exists ( fname + ".png" ) : print ( " -- converting %s to PNG..." % os . path . basename ( fname ) ) cm . image_convert ( fname ) new . append ( fname ) #fancy burn-in of image data else : pass #print(" -- already converted %s to PNG..."%os.path.basename(fname)) return new
12198	def to_cldf ( self , dest , mdname = 'cldf-metadata.json' ) : dest = Path ( dest ) if not dest . exists ( ) : dest . mkdir ( ) data = self . read ( ) if data [ self . source_table_name ] : sources = Sources ( ) for src in data [ self . source_table_name ] : sources . add ( Source ( src [ 'genre' ] , src [ 'id' ] , * * { k : v for k , v in src . items ( ) if k not in [ 'id' , 'genre' ] } ) ) sources . write ( dest / self . dataset . properties . get ( 'dc:source' , 'sources.bib' ) ) for table_type , items in data . items ( ) : try : table = self . dataset [ table_type ] table . common_props [ 'dc:extent' ] = table . write ( [ self . retranslate ( table , item ) for item in items ] , base = dest ) except KeyError : assert table_type == self . source_table_name , table_type return self . dataset . write_metadata ( dest / mdname )
6387	def _sb_r2 ( self , term , r1_prefixes = None ) : r1_start = self . _sb_r1 ( term , r1_prefixes ) return r1_start + self . _sb_r1 ( term [ r1_start : ] )
12345	def stitch ( self , folder = None ) : debug ( 'stitching ' + self . __str__ ( ) ) if not folder : folder = self . path # create list of macros and files macros = [ ] files = [ ] for well in self . wells : f , m = stitch_macro ( well , folder ) macros . extend ( m ) files . extend ( f ) chopped_arguments = zip ( chop ( macros , _pools ) , chop ( files , _pools ) ) chopped_filenames = Parallel ( n_jobs = _pools ) ( delayed ( fijibin . macro . run ) ( macro = arg [ 0 ] , output_files = arg [ 1 ] ) for arg in chopped_arguments ) # flatten return [ f for list_ in chopped_filenames for f in list_ ]
1980	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
10104	def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
7864	def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP version checker' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add_argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) args = parser . parse_args ( ) settings = XMPPSettings ( ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . source ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . source = args . source . decode ( "utf-8" ) source = JID ( args . source ) if args . target : if sys . version_info . major < 3 : args . target = args . target . decode ( "utf-8" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basicConfig ( level = args . log_level ) checker = VersionChecker ( source , target , settings ) try : checker . run ( ) except KeyboardInterrupt : checker . disconnect ( )
13822	def update_config ( new_config ) : flask_app . base_config . update ( new_config ) # Check for changed working directory. if new_config . has_key ( 'working_directory' ) : wd = os . path . abspath ( new_config [ 'working_directory' ] ) if nbmanager . notebook_dir != wd : if not os . path . exists ( wd ) : raise IOError ( 'Path not found: %s' % wd ) nbmanager . notebook_dir = wd
5223	def ccy_pair ( local , base = 'USD' ) -> CurrencyPair : ccy_param = param . load_info ( cat = 'ccy' ) if f'{local}{base}' in ccy_param : info = ccy_param [ f'{local}{base}' ] elif f'{base}{local}' in ccy_param : info = ccy_param [ f'{base}{local}' ] info [ 'factor' ] = 1. / info . get ( 'factor' , 1. ) info [ 'power' ] = - info . get ( 'power' , 1 ) elif base . lower ( ) == local . lower ( ) : info = dict ( ticker = '' ) info [ 'factor' ] = 1. if base [ - 1 ] . lower ( ) == base [ - 1 ] : info [ 'factor' ] /= 100. if local [ - 1 ] . lower ( ) == local [ - 1 ] : info [ 'factor' ] *= 100. else : logger = logs . get_logger ( ccy_pair ) logger . error ( f'incorrect currency - local {local} / base {base}' ) return CurrencyPair ( ticker = '' , factor = 1. , power = 1 ) if 'factor' not in info : info [ 'factor' ] = 1. if 'power' not in info : info [ 'power' ] = 1 return CurrencyPair ( * * info )
7105	def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc_range ] for n in N : X = self . X [ : n ] y = self . y [ : n ] e = Experiment ( X , y , model . estimator , self . scores , self . validation_method ) e . log_folder = self . log_folder e . train ( )
335	def compute_consistency_score ( returns_test , preds ) : returns_test_cum = cum_returns ( returns_test , starting_value = 1. ) cum_preds = np . cumprod ( preds + 1 , 1 ) q = [ sp . stats . percentileofscore ( cum_preds [ : , i ] , returns_test_cum . iloc [ i ] , kind = 'weak' ) for i in range ( len ( returns_test_cum ) ) ] # normalize to be from 100 (perfect median line) to 0 (completely outside # of cone) return 100 - np . abs ( 50 - np . mean ( q ) ) / .5
13532	def ancestors_root ( self ) : if self . is_root ( ) : return [ ] ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors , True ) try : ancestors . remove ( self ) except KeyError : # we weren't ancestor of ourself, that's ok pass return list ( ancestors )
2814	def convert_shape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting shape ...' ) def target_layer ( x ) : import tensorflow as tf return tf . shape ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
539	def _finalize ( self ) : self . _logger . info ( "Finished: modelID=%r; %r records processed. Performing final activities" , self . _modelID , self . _currentRecordIndex + 1 ) # ========================================================================= # Dump the experiment metrics at the end of the task # ========================================================================= self . _updateModelDBResults ( ) # ========================================================================= # Check if the current model is the best. Create a milestone if necessary # If the model has been killed, it is not a candidate for "best model", # and its output cache should be destroyed # ========================================================================= if not self . _isKilled : self . __updateJobResults ( ) else : self . __deleteOutputCache ( self . _modelID ) # ========================================================================= # Close output stream, if necessary # ========================================================================= if self . _predictionLogger : self . _predictionLogger . close ( ) # ========================================================================= # Close input stream, if necessary # ========================================================================= if self . _inputSource : self . _inputSource . close ( )
6471	def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply_function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
4171	def enbw ( data ) : N = len ( data ) return N * np . sum ( data ** 2 ) / np . sum ( data ) ** 2
13394	def setting ( self , name_hyphen ) : if name_hyphen in self . _instance_settings : value = self . _instance_settings [ name_hyphen ] [ 1 ] else : msg = "No setting named '%s'" % name_hyphen raise UserFeedback ( msg ) if hasattr ( value , 'startswith' ) and value . startswith ( "$" ) : env_var = value . lstrip ( "$" ) if env_var in os . environ : return os . getenv ( env_var ) else : msg = "'%s' is not defined in your environment" % env_var raise UserFeedback ( msg ) elif hasattr ( value , 'startswith' ) and value . startswith ( "\$" ) : return value . replace ( "\$" , "$" ) else : return value
239	def create_position_tear_sheet ( returns , positions , show_and_plot_top_pos = 2 , hide_positions = False , return_fig = False , sector_mappings = None , transactions = None , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if hide_positions : show_and_plot_top_pos = 0 vertical_sections = 7 if sector_mappings is not None else 6 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_exposures = plt . subplot ( gs [ 0 , : ] ) ax_top_positions = plt . subplot ( gs [ 1 , : ] , sharex = ax_exposures ) ax_max_median_pos = plt . subplot ( gs [ 2 , : ] , sharex = ax_exposures ) ax_holdings = plt . subplot ( gs [ 3 , : ] , sharex = ax_exposures ) ax_long_short_holdings = plt . subplot ( gs [ 4 , : ] ) ax_gross_leverage = plt . subplot ( gs [ 5 , : ] , sharex = ax_exposures ) positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = show_and_plot_top_pos , hide_positions = hide_positions , ax = ax_top_positions ) plotting . plot_max_median_position_concentration ( positions , ax = ax_max_median_pos ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) plotting . plot_gross_leverage ( returns , positions , ax = ax_gross_leverage ) if sector_mappings is not None : sector_exposures = pos . get_sector_exposures ( positions , sector_mappings ) if len ( sector_exposures . columns ) > 1 : sector_alloc = pos . get_percent_alloc ( sector_exposures ) sector_alloc = sector_alloc . drop ( 'cash' , axis = 'columns' ) ax_sector_alloc = plt . subplot ( gs [ 6 , : ] , sharex = ax_exposures ) plotting . plot_sector_allocations ( returns , sector_alloc , ax = ax_sector_alloc ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig
4779	def is_in ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : for i in items : if self . val == i : return self self . _err ( 'Expected <%s> to be in %s, but was not.' % ( self . val , self . _fmt_items ( items ) ) )
10436	def verifypartialtablecell ( self , window_name , object_name , row_index , column_index , row_text ) : try : value = getcellvalue ( window_name , object_name , row_index , column_index ) if re . searchmatch ( row_text , value ) : return 1 except LdtpServerException : pass return 0
7860	def _request_tls ( self ) : self . requested = True element = ElementTree . Element ( STARTTLS_TAG ) self . stream . write_element ( element )
11287	def execute ( self , arg_str = '' , * * kwargs ) : cmd = "{} {} {}" . format ( self . cmd_prefix , self . script , arg_str ) expected_ret_code = kwargs . pop ( 'code' , 0 ) # any kwargs with all capital letters should be considered environment # variables environ = self . environ for k in list ( kwargs . keys ( ) ) : if k . isupper ( ) : environ [ k ] = kwargs . pop ( k ) # we will allow overriding of these values kwargs . setdefault ( "stderr" , subprocess . STDOUT ) # we will not allow these to be overridden via kwargs kwargs [ "shell" ] = True kwargs [ "stdout" ] = subprocess . PIPE kwargs [ "cwd" ] = self . cwd kwargs [ "env" ] = environ process = None self . buf = deque ( maxlen = self . bufsize ) try : process = subprocess . Popen ( cmd , * * kwargs ) # another round of links # http://stackoverflow.com/a/17413045/5006 (what I used) # http://stackoverflow.com/questions/2715847/ for line in iter ( process . stdout . readline , b"" ) : line = line . decode ( self . encoding ) self . buf . append ( line . rstrip ( ) ) yield line process . wait ( ) if process . returncode != expected_ret_code : if process . returncode > 0 : raise RuntimeError ( "{} returned {} with output: {}" . format ( cmd , process . returncode , self . output ) ) except subprocess . CalledProcessError as e : if e . returncode != expected_ret_code : raise RuntimeError ( "{} returned {} with output: {}" . format ( cmd , e . returncode , self . output ) ) finally : if process : process . stdout . close ( )
228	def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos
5486	def jsonify_status_code ( status_code , * args , * * kw ) : is_batch = kw . pop ( 'is_batch' , False ) if is_batch : response = flask_make_response ( json . dumps ( * args , * * kw ) ) response . mimetype = 'application/json' response . status_code = status_code return response response = jsonify ( * args , * * kw ) response . status_code = status_code return response
13565	def register ( app ) : # Pick a handler based on the requested format. Currently we assume the # caller wants JSON. error_handler = json . http_exception_error_handler @ app . errorhandler ( 400 ) def handle_bad_request ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 404 ) def handle_not_found ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 405 ) def handle_method_not_allowed ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 422 ) def handle_unprocessable_entity ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 500 ) def handle_internal_server_error ( exception ) : return error_handler ( exception )
3519	def matomo ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MatomoNode ( )
10061	def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
6143	def DSP_capture_add_samples_stereo ( self , new_data_left , new_data_right ) : self . capture_sample_count = self . capture_sample_count + len ( new_data_left ) + len ( new_data_right ) if self . Tcapture > 0 : self . data_capture_left = np . hstack ( ( self . data_capture_left , new_data_left ) ) self . data_capture_right = np . hstack ( ( self . data_capture_right , new_data_right ) ) if ( len ( self . data_capture_left ) > self . Ncapture ) : self . data_capture_left = self . data_capture_left [ - self . Ncapture : ] if ( len ( self . data_capture_right ) > self . Ncapture ) : self . data_capture_right = self . data_capture_right [ - self . Ncapture : ]
6216	def buffers_exist ( self ) : for buff in self . buffers : if not buff . is_separate_file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise FileNotFoundError ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
709	def runWithPermutationsScript ( permutationsFilePath , options , outputLabel , permWorkDir ) : global g_currentVerbosityLevel if "verbosityCount" in options : g_currentVerbosityLevel = options [ "verbosityCount" ] del options [ "verbosityCount" ] else : g_currentVerbosityLevel = 1 _setupInterruptHandling ( ) options [ "permutationsScriptPath" ] = permutationsFilePath options [ "outputLabel" ] = outputLabel options [ "outDir" ] = permWorkDir options [ "permWorkDir" ] = permWorkDir # Assume it's a permutations python script runOptions = _injectDefaultOptions ( options ) _validateOptions ( runOptions ) return _runAction ( runOptions )
1037	def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
11076	def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
926	def generateDataset ( aggregationInfo , inputFilename , outputFilename = None ) : # Create the input stream inputFullPath = resource_filename ( "nupic.datafiles" , inputFilename ) inputObj = FileRecordStream ( inputFullPath ) # Instantiate the aggregator aggregator = Aggregator ( aggregationInfo = aggregationInfo , inputFields = inputObj . getFields ( ) ) # Is it a null aggregation? If so, just return the input file unmodified if aggregator . isNullAggregation ( ) : return inputFullPath # ------------------------------------------------------------------------ # If we were not given an output filename, create one based on the # aggregation settings if outputFilename is None : outputFilename = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFullPath ) ) [ 0 ] timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if aggregationInfo . get ( k , 0 ) > 0 : outputFilename += '_%s_%d' % ( k , aggregationInfo [ k ] ) outputFilename += '.csv' outputFilename = os . path . join ( os . path . dirname ( inputFullPath ) , outputFilename ) # ------------------------------------------------------------------------ # If some other process already started creating this file, simply # wait for it to finish and return without doing anything lockFilePath = outputFilename + '.please_wait' if os . path . isfile ( outputFilename ) or os . path . isfile ( lockFilePath ) : while os . path . isfile ( lockFilePath ) : print 'Waiting for %s to be fully written by another process' % lockFilePath time . sleep ( 1 ) return outputFilename # Create the lock file lockFD = open ( lockFilePath , 'w' ) # ------------------------------------------------------------------------- # Create the output stream outputObj = FileRecordStream ( streamID = outputFilename , write = True , fields = inputObj . getFields ( ) ) # ------------------------------------------------------------------------- # Write all aggregated records to the output while True : inRecord = inputObj . getNextRecord ( ) ( aggRecord , aggBookmark ) = aggregator . next ( inRecord , None ) if aggRecord is None and inRecord is None : break if aggRecord is not None : outputObj . appendRecord ( aggRecord ) return outputFilename
13811	def GetTopLevelContainingType ( self ) : desc = self while desc . containing_type is not None : desc = desc . containing_type return desc
4842	def get_program_type_by_slug ( self , slug ) : return self . _load_data ( self . PROGRAM_TYPES_ENDPOINT , resource_id = slug , default = None , )
1161	def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( "%s.acquire(%s): blocked waiting, value=%s" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( "%s.acquire: success, value=%s" , self , self . __value ) rc = True return rc
10793	def separate_particles_into_groups ( s , region_size = 40 , bounds = None ) : imtile = ( s . oshape . translate ( - s . pad ) if bounds is None else util . Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) # does all particle including out of image, is that correct? region = util . Tile ( region_size , dim = s . dim ) trange = np . ceil ( imtile . shape . astype ( 'float' ) / region . shape ) translations = util . Tile ( trange ) . coords ( form = 'vector' ) translations = translations . reshape ( - 1 , translations . shape [ - 1 ] ) groups = [ ] positions = s . obj_get_positions ( ) for v in translations : tmptile = region . copy ( ) . translate ( region . shape * v - s . pad ) groups . append ( find_particles_in_tile ( positions , tmptile ) ) return [ g for g in groups if len ( g ) > 0 ]
1368	def register_on_message ( self , msg_builder ) : message = msg_builder ( ) Log . debug ( "In register_on_message(): %s" % message . DESCRIPTOR . full_name ) self . registered_message_map [ message . DESCRIPTOR . full_name ] = msg_builder
4356	def get_multiple_client_msgs ( self , * * kwargs ) : client_queue = self . client_queue msgs = [ client_queue . get ( * * kwargs ) ] while client_queue . qsize ( ) : msgs . append ( client_queue . get ( ) ) return msgs
8042	def parse ( self , filelike , filename ) : self . log = log self . source = filelike . readlines ( ) src = "" . join ( self . source ) # This may raise a SyntaxError: compile ( src , filename , "exec" ) self . stream = TokenStream ( StringIO ( src ) ) self . filename = filename self . all = None self . future_imports = set ( ) self . _accumulated_decorators = [ ] return self . parse_module ( )
4094	def AIC ( N , rho , k ) : from numpy import log , array #k+1 #todo check convention. agrees with octave res = N * log ( array ( rho ) ) + 2. * ( array ( k ) + 1 ) return res
1528	def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
7630	def values ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) if 'enum' not in __NAMESPACE__ [ ns_key ] [ 'value' ] : raise NamespaceError ( 'Namespace {:s} is not enumerated' . format ( ns_key ) ) return copy . copy ( __NAMESPACE__ [ ns_key ] [ 'value' ] [ 'enum' ] )
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( * * params )
8311	def draw_math ( str , x , y , alpha = 1.0 ) : try : from web import _ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = _ctx . imagesize ( img ) _ctx . image ( img , x , y , alpha = alpha ) return w , h
10832	def delete ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls . query . filter ( cls . admin == admin , cls . group == group ) . one ( ) db . session . delete ( obj )
4770	def is_length ( self , length ) : if type ( length ) is not int : raise TypeError ( 'given arg must be an int' ) if length < 0 : raise ValueError ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . _err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
1940	def get_func_signature ( self , hsh : bytes ) -> Optional [ str ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) return self . _function_signatures_by_selector . get ( hsh )
12769	def load_markers ( self , filename , attachments , max_frames = 1e100 ) : self . markers = Markers ( self ) fn = filename . lower ( ) if fn . endswith ( '.c3d' ) : self . markers . load_c3d ( filename , max_frames = max_frames ) elif fn . endswith ( '.csv' ) or fn . endswith ( '.csv.gz' ) : self . markers . load_csv ( filename , max_frames = max_frames ) else : logging . fatal ( '%s: not sure how to load markers!' , filename ) self . markers . load_attachments ( attachments , self . skeleton )
11406	def records_identical ( rec1 , rec2 , skip_005 = True , ignore_field_order = False , ignore_subfield_order = False , ignore_duplicate_subfields = False , ignore_duplicate_controlfields = False ) : rec1_keys = set ( rec1 . keys ( ) ) rec2_keys = set ( rec2 . keys ( ) ) if skip_005 : rec1_keys . discard ( "005" ) rec2_keys . discard ( "005" ) if rec1_keys != rec2_keys : return False for key in rec1_keys : if ignore_duplicate_controlfields and key . startswith ( '00' ) : if set ( field [ 3 ] for field in rec1 [ key ] ) != set ( field [ 3 ] for field in rec2 [ key ] ) : return False continue rec1_fields = rec1 [ key ] rec2_fields = rec2 [ key ] if len ( rec1_fields ) != len ( rec2_fields ) : # They already differs in length... return False if ignore_field_order : # We sort the fields, first by indicators and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) else : # We sort the fields, first by indicators, then by global position # and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) for field1 , field2 in zip ( rec1_fields , rec2_fields ) : if ignore_duplicate_subfields : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or set ( field1 [ 0 ] ) != set ( field2 [ 0 ] ) : return False elif ignore_subfield_order : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or sorted ( field1 [ 0 ] ) != sorted ( field2 [ 0 ] ) : return False elif field1 [ : 4 ] != field2 [ : 4 ] : return False return True
1095	def escape ( pattern ) : s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
1361	def get_argument_endtime ( self ) : try : endtime = self . get_argument ( constants . PARAM_ENDTIME ) return endtime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
10524	def login ( self , username = None , password = None , android_id = None ) : cls_name = type ( self ) . __name__ if username is None : username = input ( "Enter your Google username or email address: " ) if password is None : password = getpass . getpass ( "Enter your Google Music password: " ) if android_id is None : android_id = Mobileclient . FROM_MAC_ADDRESS try : self . api . login ( username , password , android_id ) except OSError : logger . exception ( "{} authentication failed." . format ( cls_name ) ) if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
4657	def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] # This makes sure that _is_constructed will return False afterwards self [ "expiration" ] = None dict . __init__ ( self , { } )
7875	def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : # pylint: disable=E1103 return ElementTree . tounicode ( "element" ) elif sys . version_info . major < 3 : return unicode ( ElementTree . tostring ( element ) ) else : return ElementTree . tostring ( element , encoding = "unicode" )
12522	def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
3561	def advertised ( self ) : uuids = [ ] # Get UUIDs property but wrap it in a try/except to catch if the property # doesn't exist as it is optional. try : uuids = self . _props . Get ( _INTERFACE , 'UUIDs' ) except dbus . exceptions . DBusException as ex : # Ignore error if device has no UUIDs property (i.e. might not be # a BLE device). if ex . get_dbus_name ( ) != 'org.freedesktop.DBus.Error.InvalidArgs' : raise ex return [ uuid . UUID ( str ( x ) ) for x in uuids ]
5617	def clean_geometry_type ( geometry , target_type , allow_multipart = True ) : multipart_geoms = { "Point" : MultiPoint , "LineString" : MultiLineString , "Polygon" : MultiPolygon , "MultiPoint" : MultiPoint , "MultiLineString" : MultiLineString , "MultiPolygon" : MultiPolygon } if target_type not in multipart_geoms . keys ( ) : raise TypeError ( "target type is not supported: %s" % target_type ) if geometry . geom_type == target_type : return geometry elif allow_multipart : target_multipart_type = multipart_geoms [ target_type ] if geometry . geom_type == "GeometryCollection" : return target_multipart_type ( [ clean_geometry_type ( g , target_type , allow_multipart ) for g in geometry ] ) elif any ( [ isinstance ( geometry , target_multipart_type ) , multipart_geoms [ geometry . geom_type ] == target_multipart_type ] ) : return geometry raise GeometryTypeError ( "geometry type does not match: %s, %s" % ( geometry . geom_type , target_type ) )
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] # Reap the args, taking care to stop before the next option or '.' gotDot = False for arg in parser . rargs : # Stop on --longname options if arg . startswith ( "--" ) and len ( arg ) > 2 : break # Stop on -b options if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] # Retrieve the existing arg accumulator, if any value = getattr ( parser . values , option . dest , [ ] ) #print "Previous value: %r" % value if value is None : value = [ ] # Append the new args to the existing ones and save to the parser value . extend ( newValues ) setattr ( parser . values , option . dest , value )
9771	def stop ( ctx , yes ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _job ) ) : click . echo ( 'Existing without stopping job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . job . stop ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job is being stopped." )
2913	def get_state_name ( self ) : state_name = [ ] for state , name in list ( self . state_names . items ( ) ) : if self . _has_state ( state ) : state_name . append ( name ) return '|' . join ( state_name )
8740	def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise # alexm: Notify from this method for consistency with _delete_flip billing . notify ( context , billing . IP_ASSOC , flip )
6855	def ismounted ( device ) : # Check filesystem with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'mount' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True # Check swap with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'swapon -s' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True return False
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow ## first figure is dag layout plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) ## second figure is times for steps pos = { } colors = { } for node in dag : #jobkey = "{}-{}".format(node, sample) mtd = results [ node ] . metadata start = date2num ( mtd . started ) #runtime = date2num(md.completed)# - start ## sample id to separate samples on x-axis _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) ## 1e6 to separate on y-axis pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id ## x just spaces out samples; ## y is start time of each job with edge leading to next job ## color is the engine that ran the job ## all jobs were submitted as 3 second wait times plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
1482	def get_commands_to_run ( self ) : # During shutdown the watch might get triggered with the empty packing plan if len ( self . packing_plan . container_plans ) == 0 : return { } if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : retval = { } retval [ 'heron-shell' ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval if self . shard == 0 : commands = self . _get_tmaster_processes ( ) else : self . _untar_if_needed ( ) commands = self . _get_streaming_processes ( ) # Attach daemon processes commands . update ( self . _get_heron_support_processes ( ) ) return commands
1932	def get_description ( self , name : str ) -> str : if name not in self . _vars : raise ConfigError ( f"{self.name}.{name} not defined." ) return self . _vars [ name ] . description
7120	def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDict ( obj ) elif isinstance ( obj , list ) : # must mutate and not just reassign, otherwise it will # just use original object mutable/immutable for i , item in enumerate ( obj ) : if isinstance ( item , dict ) and not isinstance ( item , DotDict ) : obj [ i ] = DotDict ( item ) return obj
5516	def append ( self , data , start ) : if self . _limit is not None and self . _limit > 0 : if self . _start is None : self . _start = start if start - self . _start > self . reset_rate : self . _sum -= round ( ( start - self . _start ) * self . _limit ) self . _start = start self . _sum += len ( data )
6088	def scaled_noise_map_from_hyper_galaxies_and_contribution_maps ( contribution_maps , hyper_galaxies , noise_map ) : scaled_noise_maps = list ( map ( lambda hyper_galaxy , contribution_map : hyper_galaxy . hyper_noise_from_contributions ( noise_map = noise_map , contributions = contribution_map ) , hyper_galaxies , contribution_maps ) ) return noise_map + sum ( scaled_noise_maps )
8634	def get_bids ( session , project_ids = [ ] , bid_ids = [ ] , limit = 10 , offset = 0 ) : get_bids_data = { } if bid_ids : get_bids_data [ 'bids[]' ] = bid_ids if project_ids : get_bids_data [ 'projects[]' ] = project_ids get_bids_data [ 'limit' ] = limit get_bids_data [ 'offset' ] = offset # GET /api/projects/0.1/bids/ response = make_get_request ( session , 'bids' , params_data = get_bids_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise BidsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9586	def write_numeric_array ( fd , header , array ) : # make a memory file for writing array data bd = BytesIO ( ) # write matrix header to memory file write_var_header ( bd , header ) if not isinstance ( array , basestring ) and header [ 'dims' ] [ 0 ] > 1 : # list array data in column major order array = list ( chain . from_iterable ( izip ( * array ) ) ) # write matrix data to memory file write_elements ( bd , header [ 'mtp' ] , array ) # write the variable to disk file data = bd . getvalue ( ) bd . close ( ) write_var_data ( fd , data )
6778	def get_component_order ( component_names ) : assert isinstance ( component_names , ( tuple , list ) ) component_dependences = { } for _name in component_names : deps = set ( manifest_deployers_befores . get ( _name , [ ] ) ) deps = deps . intersection ( component_names ) component_dependences [ _name ] = deps component_order = list ( topological_sort ( component_dependences . items ( ) ) ) return component_order
11928	def get_files_stat ( self ) : if not exists ( Post . src_dir ) : logger . error ( SourceDirectoryNotFound . __doc__ ) sys . exit ( SourceDirectoryNotFound . exit_code ) paths = [ ] for fn in ls ( Post . src_dir ) : if fn . endswith ( src_ext ) : paths . append ( join ( Post . src_dir , fn ) ) # config.toml if exists ( config . filepath ) : paths . append ( config . filepath ) # files: a <filepath to updated time> dict files = dict ( ( p , stat ( p ) . st_mtime ) for p in paths ) return files
13576	def submit ( course , tid = None , pastebin = False , review = False ) : if tid is not None : return submit_exercise ( Exercise . byid ( tid ) , pastebin = pastebin , request_review = review ) else : sel = Exercise . get_selected ( ) if not sel : raise NoExerciseSelected ( ) return submit_exercise ( sel , pastebin = pastebin , request_review = review )
3468	def _update_awareness ( self ) : for x in self . _metabolites : x . _reaction . add ( self ) for x in self . _genes : x . _reaction . add ( self )
9875	def aggregate_tree ( l_tree ) : def _aggregate_phase1 ( tree ) : # phase1 removes any supplied prefixes which are superfluous because # they are already included in another supplied prefix. For example, # 2001:67c:208c:10::/64 would be removed if 2001:67c:208c::/48 was # also supplied. n_tree = radix . Radix ( ) for prefix in tree . prefixes ( ) : if tree . search_worst ( prefix ) . prefix == prefix : n_tree . add ( prefix ) return n_tree def _aggregate_phase2 ( tree ) : # phase2 identifies adjacent prefixes that can be combined under a # single, shorter-length prefix. For example, 2001:67c:208c::/48 and # 2001:67c:208d::/48 can be combined into the single prefix # 2001:67c:208c::/47. n_tree = radix . Radix ( ) for rnode in tree : p = text ( ip_network ( text ( rnode . prefix ) ) . supernet ( ) ) r = tree . search_covered ( p ) if len ( r ) == 2 : if r [ 0 ] . prefixlen == r [ 1 ] . prefixlen == rnode . prefixlen : n_tree . add ( p ) else : n_tree . add ( rnode . prefix ) else : n_tree . add ( rnode . prefix ) return n_tree l_tree = _aggregate_phase1 ( l_tree ) if len ( l_tree . prefixes ( ) ) == 1 : return l_tree while True : r_tree = _aggregate_phase2 ( l_tree ) if l_tree . prefixes ( ) == r_tree . prefixes ( ) : break else : l_tree = r_tree del r_tree return l_tree
6026	def geometry_from_grid ( self , grid , pixel_centres , pixel_neighbors , pixel_neighbors_size , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer shape_arcsec = ( y_max - y_min , x_max - x_min ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) return self . Geometry ( shape_arcsec = shape_arcsec , pixel_centres = pixel_centres , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
12179	def detect ( self ) : self . log . info ( "initializing AP detection on all sweeps..." ) t1 = cm . timeit ( ) for sweep in range ( self . abf . sweeps ) : self . detectSweep ( sweep ) self . log . info ( "AP analysis of %d sweeps found %d APs (completed in %s)" , self . abf . sweeps , len ( self . APs ) , cm . timeit ( t1 ) )
9722	async def load ( self , filename ) : cmd = "load %s" % filename return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
12451	def deref ( self , data ) : # We have to make a deepcopy here to create a proper JSON # compatible object, otherwise `json.dumps` fails when it # hits jsonref.JsonRef objects. deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) # Write out JSON version because we might want this. self . write_template ( deref , filename = 'swagger.json' ) return deref
5628	def hook ( self , event_type = 'push' ) : def decorator ( func ) : self . _hooks [ event_type ] . append ( func ) return func return decorator
1916	def get ( self ) : # A shutdown has been requested if self . is_shutdown ( ) : return None # if not more states in the queue, let's wait for some forks while len ( self . _states ) == 0 : # if no worker is running, bail out if self . running == 0 : return None # if a shutdown has been requested, bail out if self . is_shutdown ( ) : return None # if there ares actually some workers running, wait for state forks logger . debug ( "Waiting for available states" ) self . _lock . wait ( ) state_id = self . _policy . choice ( list ( self . _states ) ) if state_id is None : return None del self . _states [ self . _states . index ( state_id ) ] return state_id
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
9174	def bake ( binder , recipe_id , publisher , message , cursor ) : recipe = _get_recipe ( recipe_id , cursor ) includes = _formatter_callback_factory ( ) binder = collate_models ( binder , ruleset = recipe , includes = includes ) def flatten_filter ( model ) : return ( isinstance ( model , cnxepub . CompositeDocument ) or ( isinstance ( model , cnxepub . Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) def only_documents_filter ( model ) : return isinstance ( model , cnxepub . Document ) and not isinstance ( model , cnxepub . CompositeDocument ) for doc in cnxepub . flatten_to ( binder , flatten_filter ) : publish_composite_model ( cursor , doc , binder , publisher , message ) for doc in cnxepub . flatten_to ( binder , only_documents_filter ) : publish_collated_document ( cursor , doc , binder ) tree = cnxepub . model_to_tree ( binder ) publish_collated_tree ( cursor , tree ) return [ ]
13863	def ts ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) )
13533	def descendents ( self ) : visited = set ( [ ] ) self . _depth_descend ( self , visited ) try : visited . remove ( self ) except KeyError : # we weren't descendent of ourself, that's ok pass return list ( visited )
12978	def deleteByPk ( self , pk ) : obj = self . mdl . objects . getOnlyIndexedFields ( pk ) if not obj : return 0 return self . deleteOne ( obj )
6037	def scaled_array_2d_from_array_1d ( self , array_1d ) : return scaled_array . ScaledSquarePixelArray ( array = self . array_2d_from_array_1d ( array_1d ) , pixel_scale = self . mask . pixel_scale , origin = self . mask . origin )
10253	def remove_highlight_edges ( graph : BELGraph , edges = None ) : for u , v , k , _ in graph . edges ( keys = True , data = True ) if edges is None else edges : if is_edge_highlighted ( graph , u , v , k ) : del graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ]
2405	def update_prompt ( self , prompt_text ) : if ( isinstance ( prompt_text , basestring ) ) : self . _prompt = util_functions . sub_chars ( prompt_text ) ret = self . _prompt else : raise util_functions . InputError ( prompt_text , "Invalid prompt. Need to enter a string value." ) return ret
9498	def parse_litezip ( path ) : struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( x ) for x in path . iterdir ( ) if x . is_dir ( ) and x . name . startswith ( 'm' ) ] ) return tuple ( sorted ( struct ) )
10644	def Sh ( L : float , h : float , D : float ) -> float : return h * L / D
7761	def xml_elements_equal ( element1 , element2 , ignore_level1_cdata = False ) : # pylint: disable-msg=R0911 if None in ( element1 , element2 ) or element1 . tag != element2 . tag : return False attrs1 = element1 . items ( ) attrs1 . sort ( ) attrs2 = element2 . items ( ) attrs2 . sort ( ) if not ignore_level1_cdata : if element1 . text != element2 . text : return False if attrs1 != attrs2 : return False if len ( element1 ) != len ( element2 ) : return False for child1 , child2 in zip ( element1 , element2 ) : if child1 . tag != child2 . tag : return False if not ignore_level1_cdata : if element1 . text != element2 . text : return False if not xml_elements_equal ( child1 , child2 ) : return False return True
5903	def glob_parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) # at least some rough sorting... return files
4394	def adsSyncReadReqEx2 ( port , address , index_group , index_offset , data_type , return_ctypes = False ) : # type: (int, AmsAddr, int, int, Type, bool) -> Any sync_read_request = _adsDLL . AdsSyncReadReqEx2 ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if data_type == PLCTYPE_STRING : data = ( STRING_BUFFER * PLCTYPE_STRING ) ( ) else : data = data_type ( ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . c_ulong ( ctypes . sizeof ( data ) ) bytes_read = ctypes . c_ulong ( ) bytes_read_pointer = ctypes . pointer ( bytes_read ) error_code = sync_read_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , bytes_read_pointer , ) if error_code : raise ADSError ( error_code ) # If we're reading a value of predetermined size (anything but a string), # validate that the correct number of bytes were read if data_type != PLCTYPE_STRING and bytes_read . value != data_length . value : raise RuntimeError ( "Insufficient data (expected {0} bytes, {1} were read)." . format ( data_length . value , bytes_read . value ) ) if return_ctypes : return data if data_type == PLCTYPE_STRING : return data . value . decode ( "utf-8" ) if type ( data_type ) . __name__ == "PyCArrayType" : return [ i for i in data ] if hasattr ( data , "value" ) : return data . value return data
8810	def delete_segment_allocation_range ( context , sa_id ) : LOG . info ( "delete_segment_allocation_range %s for tenant %s" % ( sa_id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : sa_range = db_api . segment_allocation_range_find ( context , id = sa_id , scope = db_api . ONE ) if not sa_range : raise q_exc . SegmentAllocationRangeNotFound ( segment_allocation_range_id = sa_id ) _delete_segment_allocation_range ( context , sa_range )
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : # first, lowercase, then _squeeze to single spaces stringelems = _squeeze ( filterstring ) . lower ( ) # replace shady characters stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) # split into words stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] # get rid of all numbers stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) # get rid of everything within quotes stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] # check the filterstring words against the allowed words wordset = set ( stringwords2 ) # generate the allowed word set for these LC columns allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) # if there are words left over, then this filter string is suspicious if len ( validatecheck ) > 0 : # check if validatecheck contains an elem with % in it LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
12814	def startProducing ( self , consumer ) : self . _consumer = consumer self . _current_deferred = defer . Deferred ( ) self . _sent = 0 self . _paused = False if not hasattr ( self , "_chunk_headers" ) : self . _build_chunk_headers ( ) if self . _data : block = "" for field in self . _data : block += self . _chunk_headers [ field ] block += self . _data [ field ] block += "\r\n" self . _send_to_consumer ( block ) if self . _files : self . _files_iterator = self . _files . iterkeys ( ) self . _files_sent = 0 self . _files_length = len ( self . _files ) self . _current_file_path = None self . _current_file_handle = None self . _current_file_length = None self . _current_file_sent = 0 result = self . _produce ( ) if result : return result else : return defer . succeed ( None ) return self . _current_deferred
8251	def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : _ctx . fill ( self ) _ctx . rect ( x , y , w , h , roundness )
12851	def _unlock_temporarily ( self ) : if not self . _is_locked : yield else : try : self . _is_locked = False yield finally : self . _is_locked = True
7368	async def read ( response , loads = loads , encoding = None ) : ctype = response . headers . get ( 'Content-Type' , "" ) . lower ( ) try : if "application/json" in ctype : logger . info ( "decoding data as json" ) return await response . json ( encoding = encoding , loads = loads ) if "text" in ctype : logger . info ( "decoding data as text" ) return await response . text ( encoding = encoding ) except ( UnicodeDecodeError , json . JSONDecodeError ) as exc : data = await response . read ( ) raise exceptions . PeonyDecodeError ( response = response , data = data , exception = exc ) return await response . read ( )
7387	def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group ] . index ( node )
3493	def production_envelope ( model , reactions , objective = None , carbon_sources = None , points = 20 , threshold = None ) : reactions = model . reactions . get_by_any ( reactions ) objective = model . solver . objective if objective is None else objective data = dict ( ) if carbon_sources is None : c_input = find_carbon_sources ( model ) else : c_input = model . reactions . get_by_any ( carbon_sources ) if c_input is None : data [ 'carbon_source' ] = None elif hasattr ( c_input , 'id' ) : data [ 'carbon_source' ] = c_input . id else : data [ 'carbon_source' ] = ', ' . join ( rxn . id for rxn in c_input ) threshold = normalize_cutoff ( model , threshold ) size = points ** len ( reactions ) for direction in ( 'minimum' , 'maximum' ) : data [ 'flux_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) data [ 'carbon_yield_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) data [ 'mass_yield_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) grid = pd . DataFrame ( data ) with model : model . objective = objective objective_reactions = list ( sutil . linear_reaction_coefficients ( model ) ) if len ( objective_reactions ) != 1 : raise ValueError ( 'cannot calculate yields for objectives with ' 'multiple reactions' ) c_output = objective_reactions [ 0 ] min_max = fva ( model , reactions , fraction_of_optimum = 0 ) min_max [ min_max . abs ( ) < threshold ] = 0.0 points = list ( product ( * [ linspace ( min_max . at [ rxn . id , "minimum" ] , min_max . at [ rxn . id , "maximum" ] , points , endpoint = True ) for rxn in reactions ] ) ) tmp = pd . DataFrame ( points , columns = [ rxn . id for rxn in reactions ] ) grid = pd . concat ( [ grid , tmp ] , axis = 1 , copy = False ) add_envelope ( model , reactions , grid , c_input , c_output , threshold ) return grid
10677	def Cp ( self , T ) : result = 0.0 for c , e in zip ( self . _coefficients , self . _exponents ) : result += c * T ** e return result
4977	def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
3968	def get_compose_dict ( assembled_specs , port_specs ) : compose_dict = _compose_dict_for_nginx ( port_specs ) for app_name in assembled_specs [ 'apps' ] . keys ( ) : compose_dict [ app_name ] = _composed_app_dict ( app_name , assembled_specs , port_specs ) for service_spec in assembled_specs [ 'services' ] . values ( ) : compose_dict [ service_spec . name ] = _composed_service_dict ( service_spec ) return compose_dict
5941	def transform_args ( self , * args , * * kwargs ) : newargs = self . _combineargs ( * args , * * kwargs ) return self . _build_arg_list ( * * newargs )
7885	def _emit_element ( self , element , level , declared_prefixes ) : declarations = { } declared_prefixes = dict ( declared_prefixes ) name = element . tag prefixed = self . _make_prefixed ( name , True , declared_prefixes , declarations ) start_tag = u"<{0}" . format ( prefixed ) end_tag = u"</{0}>" . format ( prefixed ) for name , value in element . items ( ) : prefixed = self . _make_prefixed ( name , False , declared_prefixes , declarations ) start_tag += u' {0}={1}' . format ( prefixed , quoteattr ( value ) ) declarations = self . _make_ns_declarations ( declarations , declared_prefixes ) if declarations : start_tag += u" " + declarations children = [ ] for child in element : children . append ( self . _emit_element ( child , level + 1 , declared_prefixes ) ) if not children and not element . text : start_tag += u"/>" end_tag = u"" text = u"" else : start_tag += u">" if level > 0 and element . text : text = escape ( element . text ) else : text = u"" if level > 1 and element . tail : tail = escape ( element . tail ) else : tail = u"" return start_tag + text + u'' . join ( children ) + end_tag + tail
4951	def get_required_query_params ( self , request ) : username = get_request_value ( request , self . REQUIRED_PARAM_USERNAME , '' ) course_id = get_request_value ( request , self . REQUIRED_PARAM_COURSE_ID , '' ) program_uuid = get_request_value ( request , self . REQUIRED_PARAM_PROGRAM_UUID , '' ) enterprise_customer_uuid = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER ) if not ( username and ( course_id or program_uuid ) and enterprise_customer_uuid ) : raise ConsentAPIRequestError ( self . get_missing_params_message ( [ ( "'username'" , bool ( username ) ) , ( "'enterprise_customer_uuid'" , bool ( enterprise_customer_uuid ) ) , ( "one of 'course_id' or 'program_uuid'" , bool ( course_id or program_uuid ) ) , ] ) ) return username , course_id , program_uuid , enterprise_customer_uuid
10165	def get_arrays ( self , lines , personalities = [ ] ) : ret = { } i = 0 while i < len ( lines ) : try : # First array line: get the md device md_device = self . get_md_device_name ( lines [ i ] ) except IndexError : # No array detected pass else : # Array detected if md_device is not None : # md device line ret [ md_device ] = self . get_md_device ( lines [ i ] , personalities ) # md config/status line i += 1 ret [ md_device ] . update ( self . get_md_status ( lines [ i ] ) ) i += 1 return ret
2838	def setup ( self , pin , value ) : self . _validate_pin ( pin ) # Set bit to 1 for input or 0 for output. if value == GPIO . IN : self . iodir [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) elif value == GPIO . OUT : self . iodir [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) else : raise ValueError ( 'Unexpected value. Must be GPIO.IN or GPIO.OUT.' ) self . write_iodir ( )
193	def OneOf ( children , name = None , deterministic = False , random_state = None ) : return SomeOf ( n = 1 , children = children , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
10647	def create_component ( self , name , description = None ) : new_comp = Component ( name , self . gl , description = description ) new_comp . set_parent_path ( self . path ) self . components . append ( new_comp ) return new_comp
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
11127	def update_file ( self , value , relativePath , name = None , description = False , klass = False , dump = False , pull = False , ACID = None , verbose = False ) : # check ACID if ACID is None : ACID = self . __ACID assert isinstance ( ACID , bool ) , "ACID must be boolean" # get relative path normalized relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' is not allowed as file name in main repository directory" assert name != '.pyrepstate' , "'.pyrepstate' is not allowed as file name in main repository directory" assert name != '.pyreplock' , "'.pyreplock' is not allowed as file name in main repository directory" if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) # get file info dict fileInfoDict , errorMessage = self . get_file_info ( relativePath , name ) assert fileInfoDict is not None , errorMessage # get real path realPath = os . path . join ( self . __path , relativePath ) # check if file exists if verbose : if not os . path . isfile ( os . path . join ( realPath , name ) ) : warnings . warn ( "file '%s' is in repository but does not exist in the system. It is therefore being recreated." % os . path . join ( realPath , name ) ) # convert dump and pull methods to strings if not dump : dump = fileInfoDict [ "dump" ] if not pull : pull = fileInfoDict [ "pull" ] # get savePath if ACID : savePath = os . path . join ( tempfile . gettempdir ( ) , name ) else : savePath = os . path . join ( realPath , name ) # dump file try : exec ( dump . replace ( "$FILE_PATH" , str ( savePath ) ) ) except Exception as e : message = "unable to dump the file (%s)" % e if 'pickle.dump(' in dump : message += '\nmore info: %s' % str ( get_pickling_errors ( value ) ) raise Exception ( message ) # copy if ACID if ACID : try : shutil . copyfile ( savePath , os . path . join ( realPath , name ) ) except Exception as e : os . remove ( savePath ) if verbose : warnings . warn ( e ) return os . remove ( savePath ) # update timestamp fileInfoDict [ "timestamp" ] = datetime . utcnow ( ) if description is not False : fileInfoDict [ "description" ] = description if klass is not False : assert inspect . isclass ( klass ) , "klass must be a class definition" fileInfoDict [ "class" ] = klass # save repository self . save ( )
1698	def union ( self , other_streamlet ) : from heronpy . streamlet . impl . unionbolt import UnionStreamlet union_streamlet = UnionStreamlet ( self , other_streamlet ) self . _add_child ( union_streamlet ) other_streamlet . _add_child ( union_streamlet ) return union_streamlet
10264	def collapse_orthologies_by_namespace ( graph : BELGraph , victim_namespace : Strings , survivor_namespace : str ) -> None : _collapse_edge_by_namespace ( graph , victim_namespace , survivor_namespace , ORTHOLOGOUS )
7153	def many ( prompt , * args , * * kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '✔' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
904	def read ( cls , proto ) : # pylint: disable=W0212 anomalyLikelihood = object . __new__ ( cls ) anomalyLikelihood . _iteration = proto . iteration anomalyLikelihood . _historicalScores = collections . deque ( maxlen = proto . historicWindowSize ) for i , score in enumerate ( proto . historicalScores ) : anomalyLikelihood . _historicalScores . append ( ( i , score . value , score . anomalyScore ) ) if proto . distribution . name : # is "" when there is no distribution. anomalyLikelihood . _distribution = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] [ "name" ] = proto . distribution . name anomalyLikelihood . _distribution [ 'distribution' ] [ "mean" ] = proto . distribution . mean anomalyLikelihood . _distribution [ 'distribution' ] [ "variance" ] = proto . distribution . variance anomalyLikelihood . _distribution [ 'distribution' ] [ "stdev" ] = proto . distribution . stdev anomalyLikelihood . _distribution [ "movingAverage" ] = { } anomalyLikelihood . _distribution [ "movingAverage" ] [ "windowSize" ] = proto . distribution . movingAverage . windowSize anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] = [ ] for value in proto . distribution . movingAverage . historicalValues : anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] . append ( value ) anomalyLikelihood . _distribution [ "movingAverage" ] [ "total" ] = proto . distribution . movingAverage . total anomalyLikelihood . _distribution [ "historicalLikelihoods" ] = [ ] for likelihood in proto . distribution . historicalLikelihoods : anomalyLikelihood . _distribution [ "historicalLikelihoods" ] . append ( likelihood ) else : anomalyLikelihood . _distribution = None anomalyLikelihood . _probationaryPeriod = proto . probationaryPeriod anomalyLikelihood . _learningPeriod = proto . learningPeriod anomalyLikelihood . _reestimationPeriod = proto . reestimationPeriod # pylint: enable=W0212 return anomalyLikelihood
2551	def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = _unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
1631	def CheckHeaderFileIncluded ( filename , include_state , error ) : # Do not check test files fileinfo = FileInfo ( filename ) if Search ( _TEST_FILE_SUFFIX , fileinfo . BaseName ( ) ) : return for ext in GetHeaderExtensions ( ) : basefilename = filename [ 0 : len ( filename ) - len ( fileinfo . Extension ( ) ) ] headerfile = basefilename + '.' + ext if not os . path . exists ( headerfile ) : continue headername = FileInfo ( headerfile ) . RepositoryName ( ) first_include = None for section_list in include_state . include_list : for f in section_list : if headername in f [ 0 ] or f [ 0 ] in headername : return if not first_include : first_include = f [ 1 ] error ( filename , first_include , 'build/include' , 5 , '%s should include its header file %s' % ( fileinfo . RepositoryName ( ) , headername ) )
10569	def filter_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : matched_songs = [ ] filtered_songs = [ ] for filepath in filepaths : try : song = _get_mutagen_metadata ( filepath ) except mutagen . MutagenError : filtered_songs . append ( filepath ) else : if include_filters or exclude_filters : if _check_filters ( song , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) : matched_songs . append ( filepath ) else : filtered_songs . append ( filepath ) else : matched_songs . append ( filepath ) return matched_songs , filtered_songs
3010	def _get_scopes ( self ) : if _credentials_from_request ( self . request ) : return ( self . _scopes | _credentials_from_request ( self . request ) . scopes ) else : return self . _scopes
6397	def sim ( self , src , tar , qval = 2 ) : if src == tar : return 1.0 if not src or not tar : return 0.0 q_src , q_tar = self . _get_qgrams ( src , tar , qval ) q_src_mag = sum ( q_src . values ( ) ) q_tar_mag = sum ( q_tar . values ( ) ) q_intersection_mag = sum ( ( q_src & q_tar ) . values ( ) ) return q_intersection_mag / sqrt ( q_src_mag * q_tar_mag )
10189	def consume ( self , event_type , no_ack = True , payload = True ) : assert event_type in self . events return current_queues . queues [ 'stats-{}' . format ( event_type ) ] . consume ( payload = payload )
3405	def _sample_chain ( args ) : n , idx = args # has to be this way to work in Python 2.7 center = sampler . center np . random . seed ( ( sampler . _seed + idx ) % np . iinfo ( np . int32 ) . max ) pi = np . random . randint ( sampler . n_warmup ) prev = sampler . warmup [ pi , ] prev = step ( sampler , center , prev - center , 0.95 ) n_samples = max ( sampler . n_samples , 1 ) samples = np . zeros ( ( n , center . shape [ 0 ] ) ) for i in range ( 1 , sampler . thinning * n + 1 ) : pi = np . random . randint ( sampler . n_warmup ) delta = sampler . warmup [ pi , ] - center prev = step ( sampler , prev , delta ) if sampler . problem . homogeneous and ( n_samples * sampler . thinning % sampler . nproj == 0 ) : prev = sampler . _reproject ( prev ) center = sampler . _reproject ( center ) if i % sampler . thinning == 0 : samples [ i // sampler . thinning - 1 , ] = prev center = ( ( n_samples * center ) / ( n_samples + 1 ) + prev / ( n_samples + 1 ) ) n_samples += 1 return ( sampler . retries , samples )
11393	def relative_to_full ( url , example_url ) : if re . match ( 'https?:\/\/' , url ) : return url domain = get_domain ( example_url ) if domain : return '%s%s' % ( domain , url ) return url
13547	def _setVirtualEnv ( ) : try : activate = options . virtualenv . activate_cmd except AttributeError : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL_ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate_cmd' , 'source %s' % activate )
12180	def get_author_and_version ( package ) : init_py = open ( os . path . join ( package , '__init__.py' ) ) . read ( ) author = re . search ( "__author__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) version = re . search ( "__version__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) return author , version
4495	def remove ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To remove a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . target ) store = project . storage ( storage ) for f in store . files : if norm_remote_path ( f . path ) == remote_path : f . remove ( )
6304	def find_effect_class ( self , path ) -> Type [ Effect ] : package_name , class_name = parse_package_string ( path ) if package_name : package = self . get_package ( package_name ) return package . find_effect_class ( class_name , raise_for_error = True ) for package in self . packages : effect_cls = package . find_effect_class ( class_name ) if effect_cls : return effect_cls raise EffectError ( "No effect class '{}' found in any packages" . format ( class_name ) )
11452	def _attach_fulltext ( self , rec , doi ) : url = os . path . join ( self . url_prefix , doi ) record_add_field ( rec , 'FFT' , subfields = [ ( 'a' , url ) , ( 't' , 'INSPIRE-PUBLIC' ) , ( 'd' , 'Fulltext' ) ] )
13839	def ConsumeFloat ( self ) : try : result = ParseFloat ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
6874	def _pycompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = '%s.gz' % sqlitecurve try : if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : with open ( sqlitecurve , 'rb' ) as infd : with gzip . open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : os . remove ( sqlitecurve ) return outfile except Exception as e : return None
10457	def getaccesskey ( self , window_name , object_name ) : # Used http://www.danrodney.com/mac/ as reference for # mapping keys with specific control # In Mac noticed (in accessibility inspector) only menu had access keys # so, get the menu_handle of given object and # return the access key menu_handle = self . _get_menu_handle ( window_name , object_name ) key = menu_handle . AXMenuItemCmdChar modifiers = menu_handle . AXMenuItemCmdModifiers glpyh = menu_handle . AXMenuItemCmdGlyph virtual_key = menu_handle . AXMenuItemCmdVirtualKey modifiers_type = "" if modifiers == 0 : modifiers_type = "<command>" elif modifiers == 1 : modifiers_type = "<shift><command>" elif modifiers == 2 : modifiers_type = "<option><command>" elif modifiers == 3 : modifiers_type = "<option><shift><command>" elif modifiers == 4 : modifiers_type = "<ctrl><command>" elif modifiers == 6 : modifiers_type = "<ctrl><option><command>" # Scroll up if virtual_key == 115 and glpyh == 102 : modifiers = "<option>" key = "<cursor_left>" # Scroll down elif virtual_key == 119 and glpyh == 105 : modifiers = "<option>" key = "<right>" # Page up elif virtual_key == 116 and glpyh == 98 : modifiers = "<option>" key = "<up>" # Page down elif virtual_key == 121 and glpyh == 107 : modifiers = "<option>" key = "<down>" # Line up elif virtual_key == 126 and glpyh == 104 : key = "<up>" # Line down elif virtual_key == 125 and glpyh == 106 : key = "<down>" # Noticed in Google Chrome navigating next tab elif virtual_key == 124 and glpyh == 101 : key = "<right>" # Noticed in Google Chrome navigating previous tab elif virtual_key == 123 and glpyh == 100 : key = "<left>" # List application in a window to Force Quit elif virtual_key == 53 and glpyh == 27 : key = "<escape>" # FIXME: # * Instruments Menu View->Run Browser # modifiers==12 virtual_key==48 glpyh==2 # * Terminal Menu Edit->Start Dictation # fn fn - glpyh==148 modifiers==24 # * Menu Chrome->Clear Browsing Data in Google Chrome # virtual_key==51 glpyh==23 [Delete Left (like Backspace on a PC)] if not key : raise LdtpServerException ( "No access key associated" ) return modifiers_type + key
6540	def read_file ( filepath ) : with _FILE_CACHE_LOCK : if filepath not in _FILE_CACHE : _FILE_CACHE [ filepath ] = _read_file ( filepath ) return _FILE_CACHE [ filepath ]
11169	def _add_positional_argument ( self , posarg ) : if self . positional_args : if self . positional_args [ - 1 ] . recurring : raise ValueError ( "recurring positional arguments must be last" ) if self . positional_args [ - 1 ] . optional and not posarg . optional : raise ValueError ( "required positional arguments must precede optional ones" ) self . positional_args . append ( posarg )
6256	def get_finder ( import_path ) : Finder = import_string ( import_path ) if not issubclass ( Finder , BaseFileSystemFinder ) : raise ImproperlyConfigured ( 'Finder {} is not a subclass of core.finders.FileSystemFinder' . format ( import_path ) ) return Finder ( )
304	def show_worst_drawdown_periods ( returns , top = 5 ) : drawdown_df = timeseries . gen_drawdown_table ( returns , top = top ) utils . print_table ( drawdown_df . sort_values ( 'Net drawdown in %' , ascending = False ) , name = 'Worst drawdown periods' , float_format = '{0:.2f}' . format , )
9432	def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
7098	def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ marker . __id__ ] marker . remove ( ) super ( AndroidMapItemBase , self ) . destroy ( )
2358	def registerDriver ( iface , driver , class_implements = [ ] ) : for class_item in class_implements : classImplements ( class_item , iface ) component . provideAdapter ( factory = driver , adapts = [ iface ] , provides = IDriver )
2224	def _convert_to_hashable ( data , types = True ) : # HANDLE MOST COMMON TYPES FIRST if data is None : hashable = b'NONE' prefix = b'NULL' elif isinstance ( data , six . binary_type ) : hashable = data prefix = b'TXT' elif isinstance ( data , six . text_type ) : # convert unicode into bytes hashable = data . encode ( 'utf-8' ) prefix = b'TXT' elif isinstance ( data , _intlike ) : # warnings.warn('Hashing ints is slow, numpy is prefered') hashable = _int_to_bytes ( data ) # hashable = data.to_bytes(8, byteorder='big') prefix = b'INT' elif isinstance ( data , float ) : a , b = float ( data ) . as_integer_ratio ( ) hashable = _int_to_bytes ( a ) + b'/' + _int_to_bytes ( b ) prefix = b'FLT' else : # Then dynamically look up any other type hash_func = _HASHABLE_EXTENSIONS . lookup ( data ) prefix , hashable = hash_func ( data ) if types : return prefix , hashable else : return b'' , hashable
13531	def ancestors ( self ) : ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors ) try : ancestors . remove ( self ) except KeyError : # we weren't ancestor of ourself, that's ok pass return list ( ancestors )
9361	def characters ( quantity = 10 ) : line = map ( _to_lower_alpha_only , '' . join ( random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ) ) return '' . join ( line ) [ : quantity ]
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) # "-bind-to core" is crucial for good performance args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
825	def expValue ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] return sum ( [ x * p for x , p in pred . items ( ) ] )
13665	def update_item ( filename , item , uuid ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # apply modifications to the JSON data wrt UUID # TODO: handle this in a neat way if 'products' in products_data [ - 1 ] : # handle orders object [ products_data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] else : # handle products object [ products_data [ i ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] # save the modified JSON data into the temp file json . dump ( products_data , temp_file ) return True
5987	def compute_deflections_at_next_plane ( plane_index , total_planes ) : if plane_index < total_planes - 1 : return True elif plane_index == total_planes - 1 : return False else : raise exc . RayTracingException ( 'A galaxy was not correctly allocated its previous / next redshifts' )
183	def to_segmentation_map ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : from . segmaps import SegmentationMapOnImage return SegmentationMapOnImage ( self . draw_mask ( image_shape , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
10703	def get_usage ( _id ) : url = USAGE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False try : return arequest . json ( ) except ValueError : _LOGGER . info ( "Failed to get usage. Not supported by unit?" ) return None
10473	def _sendKey ( self , keychr , modFlags = 0 , globally = False ) : escapedChrs = { '\n' : AXKeyCodeConstants . RETURN , '\r' : AXKeyCodeConstants . RETURN , '\t' : AXKeyCodeConstants . TAB , } if keychr in escapedChrs : keychr = escapedChrs [ keychr ] self . _addKeyToQueue ( keychr , modFlags , globally = globally ) self . _postQueuedEvents ( )
10508	def log ( self , message , level = logging . DEBUG ) : if _ldtp_debug : print ( message ) self . logger . log ( level , str ( message ) ) return 1
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
2504	def get_extr_license_ident ( self , extr_lic ) : identifier_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseId' ] , None ) ) ) if not identifier_tripples : self . error = True msg = 'Extracted license must have licenseId property.' self . logger . log ( msg ) return if len ( identifier_tripples ) > 1 : self . more_than_one_error ( 'extracted license identifier_tripples' ) return identifier_tripple = identifier_tripples [ 0 ] _s , _p , identifier = identifier_tripple return identifier
943	def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , checkpointLabel + g_defaultCheckpointExtension ) checkpointDir = os . path . abspath ( checkpointDir ) return checkpointDir
1497	def parse_query_string ( self , query ) : if not query : return None # Just braces do not matter if query [ 0 ] == '(' : index = self . find_closing_braces ( query ) # This must be the last index, since this was an NOP starting brace if index != len ( query ) - 1 : raise Exception ( "Invalid syntax" ) else : return self . parse_query_string ( query [ 1 : - 1 ] ) start_index = query . find ( "(" ) # There must be a ( in the query if start_index < 0 : # Otherwise it must be a constant try : constant = float ( query ) return constant except ValueError : raise Exception ( "Invalid syntax" ) token = query [ : start_index ] if token not in self . operators : raise Exception ( "Invalid token: " + token ) # Get sub components rest_of_the_query = query [ start_index : ] braces_end_index = self . find_closing_braces ( rest_of_the_query ) if braces_end_index != len ( rest_of_the_query ) - 1 : raise Exception ( "Invalid syntax" ) parts = self . get_sub_parts ( rest_of_the_query [ 1 : - 1 ] ) # parts are simple strings in this case if token == "TS" : # This will raise exception if parts are not syntactically correct return self . operators [ token ] ( parts ) children = [ ] for part in parts : children . append ( self . parse_query_string ( part ) ) # Make a node for the current token node = self . operators [ token ] ( children ) return node
775	def _abbreviate ( text , threshold ) : if text is not None and len ( text ) > threshold : text = text [ : threshold ] + "..." return text
4816	def create_n_gram_df ( df , n_pad ) : n_pad_2 = int ( ( n_pad - 1 ) / 2 ) for i in range ( n_pad_2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n_pad_2 : - n_pad_2 ]
9796	def get ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : response = PolyaxonClient ( ) . experiment_group . get_experiment_group ( user , project_name , _group ) cache . cache ( config_manager = GroupManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_group_details ( response )
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : # code out of range return L_ANY if ( code & ( code - 1 ) ) != 0 : # choice was more than one language; use any return L_ANY return code
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
10545	def delete_task ( task_id ) : #: :arg task: A task try : res = _pybossa_req ( 'delete' , 'task' , task_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
10769	def matlab_formatter ( level , vertices , codes = None ) : vertices = numpy_formatter ( level , vertices , codes ) if codes is not None : level = level [ 0 ] headers = np . vstack ( ( [ v . shape [ 0 ] for v in vertices ] , [ level ] * len ( vertices ) ) ) . T vertices = np . vstack ( list ( it . __next__ ( ) for it in itertools . cycle ( ( iter ( headers ) , iter ( vertices ) ) ) ) ) return vertices
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
10206	def run ( self , start_date = None , end_date = None , * * kwargs ) : start_date = self . extract_date ( start_date ) if start_date else None end_date = self . extract_date ( end_date ) if end_date else None self . validate_arguments ( start_date , end_date , * * kwargs ) agg_query = self . build_query ( start_date , end_date , * * kwargs ) query_result = agg_query . execute ( ) . to_dict ( ) res = self . process_query_result ( query_result , start_date , end_date ) return res
6618	def _expand_tuple ( path_cfg , alias_dict , overriding_kargs ) : # e.g., # path_cfg = ('ev : {low} <= ev.var[0] < {high}', {'low': 10, 'high': 200}) # overriding_kargs = {'alias': 'var_cut', 'name': 'var_cut25', 'low': 25} new_path_cfg = path_cfg [ 0 ] # e.g., 'ev : {low} <= ev.var[0] < {high}' new_overriding_kargs = path_cfg [ 1 ] . copy ( ) # e.g., {'low': 10, 'high': 200} new_overriding_kargs . update ( overriding_kargs ) # e.g., {'low': 25, 'high': 200, 'alias': 'var_cut', 'name': 'var_cut25'} return expand_path_cfg ( new_path_cfg , overriding_kargs = new_overriding_kargs , alias_dict = alias_dict )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
3585	def get_all ( self , cbobjects ) : try : with self . _lock : return [ self . _metadata [ x ] for x in cbobjects ] except KeyError : # Note that if this error gets thrown then the assumption that OSX # will pass back to callbacks the exact CoreBluetooth objects that # were used previously is broken! (i.e. the CoreBluetooth objects # are not stateless) raise RuntimeError ( 'Failed to find expected metadata for CoreBluetooth object!' )
2947	def deserialize ( cls , serializer , wf_spec , s_state , * * kwargs ) : return serializer . deserialize_trigger ( wf_spec , s_state , * * kwargs )
10923	def finish ( s , desc = 'finish' , n_loop = 4 , max_mem = 1e9 , separate_psf = True , fractol = 1e-7 , errtol = 1e-3 , dowarn = True ) : values = [ np . copy ( s . state [ s . params ] ) ] remove_params = s . get ( 'psf' ) . params if separate_psf else None # FIXME explicit params global_params = name_globals ( s , remove_params = remove_params ) #FIXME this could be done much better, since much of the globals such #as the ilm are local. Could be done with sparse matrices and/or taking #nearby globals in a group and using the update tile only as the slicer, #rather than the full residuals. gs = np . floor ( max_mem / s . residuals . nbytes ) . astype ( 'int' ) groups = [ global_params [ a : a + gs ] for a in range ( 0 , len ( global_params ) , gs ) ] CLOG . info ( 'Start ``finish``:\t{}' . format ( s . error ) ) for a in range ( n_loop ) : start_err = s . error #1. Min globals: for g in groups : do_levmarq ( s , g , damping = 0.1 , decrease_damp_factor = 20. , max_iter = 1 , max_mem = max_mem , eig_update = False ) if separate_psf : do_levmarq ( s , remove_params , max_mem = max_mem , max_iter = 4 , eig_update = False ) CLOG . info ( 'Globals, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) #2. Min particles do_levmarq_all_particle_groups ( s , max_iter = 1 , max_mem = max_mem ) CLOG . info ( 'Particles, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) #3. Append vals, line min: values . append ( np . copy ( s . state [ s . params ] ) ) # dv = (np.array(values[1:]) - np.array(values[0]))[-3:] # do_levmarq_n_directions(s, dv, damping=1e-2, max_iter=2, errtol=3e-4) # CLOG.info('Line min., loop {}:\t{}'.format(a, s.error)) # if desc is not None: # states.save(s, desc=desc) #4. terminate? new_err = s . error derr = start_err - new_err dobreak = ( derr / new_err < fractol ) or ( derr < errtol ) if dobreak : break if dowarn and ( not dobreak ) : CLOG . warn ( 'finish() did not converge; consider re-running' ) return { 'converged' : dobreak , 'loop_values' : np . array ( values ) }
1722	def is_lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER_START : return False return all ( e in IDENTIFIER_PART for e in i )
8946	def run ( self , cmd , * args , * * kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , * * kwargs )
6788	def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get_satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : # If we want to confirm the deployment with the user, and we're at the first server, # then run the preview. if self . genv . host_string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify_pre_deployment ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) service . pre_deploy ( ) for func_name , plan_func in plan_funcs : print ( 'Executing %s...' % func_name ) plan_func ( ) self . fake ( components = components ) service . post_deploy ( ) notifier . notify_post_deployment ( ) finally : self . unlock ( )
2112	def parse_requirements ( filename ) : reqs = [ ] version_spec_in_play = None # Iterate over each line in the requirements file. for line in open ( filename , 'r' ) . read ( ) . strip ( ) . split ( '\n' ) : # Sanity check: Is this an empty line? # If so, do nothing. if not line . strip ( ) : continue # If this is just a plain requirement (not a comment), then # add it to the requirements list. if not line . startswith ( '#' ) : reqs . append ( line ) continue # "Header" comments take the form of "=== Python {op} {version} ===", # and make the requirement only matter for those versions. # If this line is a header comment, parse it. match = re . search ( r'^# === [Pp]ython (?P<op>[<>=]{1,2}) ' r'(?P<major>[\d])\.(?P<minor>[\d]+) ===[\s]*$' , line ) if match : version_spec_in_play = match . groupdict ( ) for key in ( 'major' , 'minor' ) : version_spec_in_play [ key ] = int ( version_spec_in_play [ key ] ) continue # If this is a comment that otherwise looks like a package, then it # should be a package applying only to the current version spec. # # We can identify something that looks like a package by a lack # of any spaces. if ' ' not in line [ 1 : ] . strip ( ) and version_spec_in_play : package = line [ 1 : ] . strip ( ) # Sanity check: Is our version of Python one of the ones currently # in play? op = version_spec_in_play [ 'op' ] vspec = ( version_spec_in_play [ 'major' ] , version_spec_in_play [ 'minor' ] ) if '=' in op and sys . version_info [ 0 : 2 ] == vspec : reqs . append ( package ) elif '>' in op and sys . version_info [ 0 : 2 ] > vspec : reqs . append ( package ) elif '<' in op and sys . version_info [ 0 : 2 ] < vspec : reqs . append ( package ) # Okay, we should have an entire list of requirements now. return reqs
13174	def next ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index + 1 , len ( self . parent ) ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
8618	def find_item_by_name ( list_ , namegetter , name ) : matching_items = [ i for i in list_ if namegetter ( i ) == name ] if not matching_items : prog = re . compile ( re . escape ( name ) + '$' , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) ) matching_items = [ i for i in list_ if prog . search ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . search ( namegetter ( i ) ) ] return matching_items
5228	def load_info ( cat ) : res = _load_yaml_ ( f'{PKG_PATH}/markets/{cat}.yml' ) root = os . environ . get ( 'BBG_ROOT' , '' ) . replace ( '\\' , '/' ) if not root : return res for cat , ovrd in _load_yaml_ ( f'{root}/markets/{cat}.yml' ) . items ( ) : if isinstance ( ovrd , dict ) : if cat in res : res [ cat ] . update ( ovrd ) else : res [ cat ] = ovrd if isinstance ( ovrd , list ) and isinstance ( res [ cat ] , list ) : res [ cat ] += ovrd return res
4011	def get_dusty_containers ( services , include_exited = False ) : client = get_docker_client ( ) if services : containers = [ get_container_for_app_or_service ( service , include_exited = include_exited ) for service in services ] return [ container for container in containers if container ] else : return [ container for container in client . containers ( all = include_exited ) if any ( name . startswith ( '/dusty' ) for name in container . get ( 'Names' , [ ] ) ) ]
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
12810	def connectionMade ( self ) : headers = [ "GET %s HTTP/1.1" % ( "/room/%s/live.json" % self . factory . get_stream ( ) . get_room_id ( ) ) ] connection_headers = self . factory . get_stream ( ) . get_connection ( ) . get_headers ( ) for header in connection_headers : headers . append ( "%s: %s" % ( header , connection_headers [ header ] ) ) headers . append ( "Host: streaming.campfirenow.com" ) self . transport . write ( "\r\n" . join ( headers ) + "\r\n\r\n" ) self . factory . get_stream ( ) . set_protocol ( self )
7302	def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = "{0}.mongoadmin" . format ( app_name ) try : module = import_module ( mongoadmin ) except ImportError as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app_store = AppStore ( module ) apps . append ( dict ( app_name = app_name , obj = app_store ) ) return apps
9149	def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
9496	def parse_module ( path , excludes = None ) : file = path / MODULE_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == MODULE_FILENAME , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Module ( id , file , resources )
8465	def run ( self ) : filename = ".DS_Store" command = "find {path} -type f -name \"{filename}\" " . format ( path = self . path , filename = filename ) cmd = CommandHelper ( command ) cmd . execute ( ) files = cmd . output . split ( "\n" ) for f in files : if not f . endswith ( filename ) : continue # Ignore paths excluded rel_path = f . replace ( self . path , "" ) if rel_path . startswith ( tuple ( self . CONFIG [ 'exclude_paths' ] ) ) : continue issue = Issue ( ) issue . name = "File .DS_Store detected" issue . potential = False issue . severity = Issue . SEVERITY_LOW # Get only relative path issue . file = rel_path self . saveIssue ( issue )
4992	def transmit_content_metadata ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Transmitting content metadata to integrated channel using configuration: [%s]' , integrated_channel ) try : integrated_channel . transmit_content_metadata ( api_user ) except Exception : # pylint: disable=broad-except LOGGER . exception ( 'Transmission of content metadata failed for user [%s] and for integrated ' 'channel with code [%s] and id [%s].' , username , channel_code , channel_pk ) duration = time . time ( ) - start LOGGER . info ( 'Content metadata transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
8025	def multiglob_compile ( globs , prefix = False ) : if not globs : # An empty globs list should only match empty strings return re . compile ( '^$' ) elif prefix : globs = [ x + '*' for x in globs ] return re . compile ( '|' . join ( fnmatch . translate ( x ) for x in globs ) )
7034	def import_apikey ( lcc_server , apikey_text_json ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) respdict = json . loads ( apikey_text_json ) # # now that we have an API key dict, get the API key out of it and write it # to the APIKEYFILE # apikey = respdict [ 'apikey' ] expires = respdict [ 'expires' ] # write this to the apikey file if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) # chmod it to the correct value os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
1329	def has_gradient ( self ) : try : self . __model . gradient self . __model . predictions_and_gradient except AttributeError : return False else : return True
330	def model_returns_normal ( data , samples = 500 , progressbar = True ) : with pm . Model ( ) as model : mu = pm . Normal ( 'mean returns' , mu = 0 , sd = .01 , testval = data . mean ( ) ) sigma = pm . HalfCauchy ( 'volatility' , beta = 1 , testval = data . std ( ) ) returns = pm . Normal ( 'returns' , mu = mu , sd = sigma , observed = data ) pm . Deterministic ( 'annual volatility' , returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'sharpe' , returns . distribution . mean / returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
12317	def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . _run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } # print(result) return result
11251	def get_percentage ( a , b , i = False , r = False ) : # Round to the second decimal if i is False and r is True : percentage = round ( 100.0 * ( float ( a ) / b ) , 2 ) # Round to the nearest whole number elif ( i is True and r is True ) or ( i is True and r is False ) : percentage = int ( round ( 100 * ( float ( a ) / b ) ) ) # A rounded number and an integer were requested if r is False : warnings . warn ( "If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments." ) # A precise unrounded decimal else : percentage = 100.0 * ( float ( a ) / b ) return percentage
12877	def many_until ( these , term ) : results = [ ] while True : stop , result = choice ( _tag ( True , term ) , _tag ( False , these ) ) if stop : return results , result else : results . append ( result )
12338	def stitch_macro ( path , output_folder = None ) : output_folder = output_folder or path debug ( 'stitching ' + path + ' to ' + output_folder ) fields = glob ( _pattern ( path , _field ) ) # assume we have rectangle of fields xs = [ attribute ( field , 'X' ) for field in fields ] ys = [ attribute ( field , 'Y' ) for field in fields ] x_min , x_max = min ( xs ) , max ( xs ) y_min , y_max = min ( ys ) , max ( ys ) fields_column = len ( set ( xs ) ) fields_row = len ( set ( ys ) ) # assume all fields are the same # and get properties from images in first field images = glob ( _pattern ( fields [ 0 ] , _image ) ) # assume attributes are the same on all images attr = attributes ( images [ 0 ] ) # find all channels and z-stacks channels = [ ] z_stacks = [ ] for image in images : channel = attribute_as_str ( image , 'C' ) if channel not in channels : channels . append ( channel ) z = attribute_as_str ( image , 'Z' ) if z not in z_stacks : z_stacks . append ( z ) debug ( 'channels ' + str ( channels ) ) debug ( 'z-stacks ' + str ( z_stacks ) ) # create macro _ , extension = os . path . splitext ( images [ - 1 ] ) if extension == '.tif' : # assume .ome.tif extension = '.ome.tif' macros = [ ] output_files = [ ] for Z in z_stacks : for C in channels : filenames = os . path . join ( _field + '--X{xx}--Y{yy}' , _image + '--L' + attr . L + '--S' + attr . S + '--U' + attr . U + '--V' + attr . V + '--J' + attr . J + '--E' + attr . E + '--O' + attr . O + '--X{xx}--Y{yy}' + '--T' + attr . T + '--Z' + Z + '--C' + C + extension ) debug ( 'filenames ' + filenames ) cur_attr = attributes ( filenames ) . _asdict ( ) f = 'stitched--U{U}--V{V}--C{C}--Z{Z}.png' . format ( * * cur_attr ) output = os . path . join ( output_folder , f ) debug ( 'output ' + output ) output_files . append ( output ) if os . path . isfile ( output ) : # file already exists print ( 'leicaexperiment stitched file already' ' exists {}' . format ( output ) ) continue macros . append ( fijibin . macro . stitch ( path , filenames , fields_column , fields_row , output_filename = output , x_start = x_min , y_start = y_min ) ) return ( output_files , macros )
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) # Don't lock down if django-lockdown is disabled altogether. if settings . ENABLED is False : return None # Don't lock down if the client REMOTE_ADDR matched and is part of the # exception list. if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : # If forwarding proxies are used they must be listed as trusted trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : # If REMOTE_ADDR is a trusted proxy check x-forwarded-for x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None # Don't lock down if the URL matches an exception pattern. if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None # Don't lock down if the URL resolves to a whitelisted view. try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None # Don't lock down if outside of the lockdown dates. if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , * * self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) # Don't lock down if the user is already authorized for previewing. if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
3057	def _load_credentials_file ( credentials_file ) : try : credentials_file . seek ( 0 ) data = json . load ( credentials_file ) except Exception : logger . warning ( 'Credentials file could not be loaded, will ignore and ' 'overwrite.' ) return { } if data . get ( 'file_version' ) != 2 : logger . warning ( 'Credentials file is not version 2, will ignore and ' 'overwrite.' ) return { } credentials = { } for key , encoded_credential in iteritems ( data . get ( 'credentials' , { } ) ) : try : credential_json = base64 . b64decode ( encoded_credential ) credential = client . Credentials . new_from_json ( credential_json ) credentials [ key ] = credential except : logger . warning ( 'Invalid credential {0} in file, ignoring.' . format ( key ) ) return credentials
1589	def set_topology_context ( self , metrics_collector ) : Log . debug ( "Setting topology context" ) cluster_config = self . get_topology_config ( ) cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) task_to_component_map = self . _get_task_to_comp_map ( ) self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , self . my_task_id , metrics_collector , self . topology_pex_abs_path )
1574	def add_tracker_url ( parser ) : parser . add_argument ( '--tracker_url' , metavar = '(tracker url; default: "' + DEFAULT_TRACKER_URL + '")' , type = str , default = DEFAULT_TRACKER_URL ) return parser
12623	def dir_match ( regex , wd = os . curdir ) : ls = os . listdir ( wd ) filt = re . compile ( regex ) . match return filter_list ( ls , filt )
8812	def get_used_ips ( session , * * kwargs ) : LOG . debug ( "Getting used IPs..." ) with session . begin ( ) : query = session . query ( models . Subnet . segment_id , func . count ( models . IPAddress . address ) ) query = query . group_by ( models . Subnet . segment_id ) query = _filter ( query , * * kwargs ) reuse_window = timeutils . utcnow ( ) - datetime . timedelta ( seconds = cfg . CONF . QUARK . ipam_reuse_after ) # NOTE(asadoughi): This is an outer join instead of a regular join # to include subnets with zero IP addresses in the database. query = query . outerjoin ( models . IPAddress , and_ ( models . Subnet . id == models . IPAddress . subnet_id , or_ ( not_ ( models . IPAddress . lock_id . is_ ( None ) ) , models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPAddress . deallocated_at > reuse_window ) ) ) query = query . outerjoin ( models . IPPolicyCIDR , and_ ( models . Subnet . ip_policy_id == models . IPPolicyCIDR . ip_policy_id , models . IPAddress . address >= models . IPPolicyCIDR . first_ip , models . IPAddress . address <= models . IPPolicyCIDR . last_ip ) ) # NOTE(asadoughi): (address is allocated) OR # (address is deallocated and not inside subnet's IP policy) query = query . filter ( or_ ( models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPPolicyCIDR . id . is_ ( None ) ) ) ret = ( ( segment_id , address_count ) for segment_id , address_count in query . all ( ) ) return dict ( ret )
10085	def delete ( self , force = True , pid = None ) : pid = pid or self . pid if self [ '_deposit' ] . get ( 'pid' ) : raise PIDInvalidAction ( ) if pid : pid . delete ( ) return super ( Deposit , self ) . delete ( force = force )
425	def delete_tasks ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Task . delete_many ( kwargs ) logging . info ( "[Database] Delete Task SUCCESS" )
4802	def is_child_of ( self , parent ) : self . is_file ( ) if not isinstance ( parent , str_types ) : raise TypeError ( 'given parent directory arg must be a path' ) val_abspath = os . path . abspath ( self . val ) parent_abspath = os . path . abspath ( parent ) if not val_abspath . startswith ( parent_abspath ) : self . _err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val_abspath , parent_abspath ) ) return self
8755	def run ( ) : groups_client = sg_cli . SecurityGroupsClient ( ) xapi_client = xapi . XapiClient ( ) interfaces = set ( ) while True : try : interfaces = xapi_client . get_interfaces ( ) except Exception : LOG . exception ( "Unable to get instances/interfaces from xapi" ) _sleep ( ) continue try : sg_states = groups_client . get_security_group_states ( interfaces ) new_sg , updated_sg , removed_sg = partition_vifs ( xapi_client , interfaces , sg_states ) xapi_client . update_interfaces ( new_sg , updated_sg , removed_sg ) groups_to_ack = [ v for v in new_sg + updated_sg if v . success ] # NOTE(quade): This solves a race condition where a security group # rule may have changed between the time the sg_states were called # and when they were officially ack'd. It functions as a compare # and set. This is a fix until we get onto a proper messaging # queue. NCP-2287 sg_sts_curr = groups_client . get_security_group_states ( interfaces ) groups_to_ack = get_groups_to_ack ( groups_to_ack , sg_states , sg_sts_curr ) # This list will contain all the security group rules that do not # match ack_groups ( groups_client , groups_to_ack ) except Exception : LOG . exception ( "Unable to get security groups from registry and " "apply them to xapi" ) _sleep ( ) continue _sleep ( )
12824	def _exec ( self , globals_dict = None ) : globals_dict = globals_dict or { } globals_dict . setdefault ( '__builtins__' , { } ) exec ( self . _code , globals_dict ) return globals_dict
3028	def _get_application_default_credential_from_file ( filename ) : # read the credentials from the file with open ( filename ) as file_obj : client_credentials = json . load ( file_obj ) credentials_type = client_credentials . get ( 'type' ) if credentials_type == AUTHORIZED_USER : required_fields = set ( [ 'client_id' , 'client_secret' , 'refresh_token' ] ) elif credentials_type == SERVICE_ACCOUNT : required_fields = set ( [ 'client_id' , 'client_email' , 'private_key_id' , 'private_key' ] ) else : raise ApplicationDefaultCredentialsError ( "'type' field should be defined (and have one of the '" + AUTHORIZED_USER + "' or '" + SERVICE_ACCOUNT + "' values)" ) missing_fields = required_fields . difference ( client_credentials . keys ( ) ) if missing_fields : _raise_exception_for_missing_fields ( missing_fields ) if client_credentials [ 'type' ] == AUTHORIZED_USER : return GoogleCredentials ( access_token = None , client_id = client_credentials [ 'client_id' ] , client_secret = client_credentials [ 'client_secret' ] , refresh_token = client_credentials [ 'refresh_token' ] , token_expiry = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , user_agent = 'Python client library' ) else : # client_credentials['type'] == SERVICE_ACCOUNT from oauth2client import service_account return service_account . _JWTAccessCredentials . from_json_keyfile_dict ( client_credentials )
7953	def handle_write ( self ) : with self . lock : logger . debug ( "handle_write: queue: {0!r}" . format ( self . _write_queue ) ) try : job = self . _write_queue . popleft ( ) except IndexError : return if isinstance ( job , WriteData ) : self . _do_write ( job . data ) # pylint: disable=E1101 elif isinstance ( job , ContinueConnect ) : self . _continue_connect ( ) elif isinstance ( job , StartTLS ) : self . _initiate_starttls ( * * job . kwargs ) elif isinstance ( job , TLSHandshake ) : self . _continue_tls_handshake ( ) else : raise ValueError ( "Unrecognized job in the write queue: " "{0!r}" . format ( job ) )
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
8471	def _debug ( message , color = None , attrs = None ) : if attrs is None : attrs = [ ] if color is not None : print colored ( message , color , attrs = attrs ) else : if len ( attrs ) > 0 : print colored ( message , "white" , attrs = attrs ) else : print message
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
4454	def group_by ( self , fields , * reducers ) : group = Group ( fields , reducers ) self . _groups . append ( group ) return self
1071	def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '"' : aslist . append ( '"%s"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )
1996	def cmp_regs ( cpu , should_print = False ) : differing = False gdb_regs = gdb . getCanonicalRegisters ( ) for name in sorted ( gdb_regs ) : vg = gdb_regs [ name ] if name . endswith ( 'psr' ) : name = 'apsr' v = cpu . read_register ( name . upper ( ) ) if should_print : logger . debug ( f'{name} gdb:{vg:x} mcore:{v:x}' ) if vg != v : if should_print : logger . warning ( '^^ unequal' ) differing = True if differing : logger . debug ( qemu . correspond ( None ) ) return differing
11633	def get_orthographies ( self , _library = library ) : results = [ ] for charset in _library . charsets : if self . _charsets : cn = getattr ( charset , 'common_name' , False ) abbr = getattr ( charset , 'abbreviation' , False ) nn = getattr ( charset , 'short_name' , False ) naive = getattr ( charset , 'native_name' , False ) if cn and cn . lower ( ) in self . _charsets : results . append ( charset ) elif nn and nn . lower ( ) in self . _charsets : results . append ( charset ) elif naive and naive . lower ( ) in self . _charsets : results . append ( charset ) elif abbr and abbr . lower ( ) in self . _charsets : results . append ( charset ) else : results . append ( charset ) for result in results : yield CharsetInfo ( self , result )
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
4427	async def _seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) seconds = time_rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track_time = player . position + seconds await player . seek ( track_time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format_time(track_time)}**' )
11719	def pipelines ( self ) : if not self . response : return set ( ) elif self . _pipelines is None and self . response : self . _pipelines = set ( ) for group in self . response . payload : for pipeline in group [ 'pipelines' ] : self . _pipelines . add ( pipeline [ 'name' ] ) return self . _pipelines
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
1397	def extract_execution_state ( self , topology ) : execution_state = topology . execution_state executionState = { "cluster" : execution_state . cluster , "environ" : execution_state . environ , "role" : execution_state . role , "jobname" : topology . name , "submission_time" : execution_state . submission_time , "submission_user" : execution_state . submission_user , "release_username" : execution_state . release_state . release_username , "release_tag" : execution_state . release_state . release_tag , "release_version" : execution_state . release_state . release_version , "has_physical_plan" : None , "has_tmaster_location" : None , "has_scheduler_location" : None , "extra_links" : [ ] , } for extra_link in self . config . extra_links : link = extra_link . copy ( ) link [ "url" ] = self . config . get_formatted_url ( executionState , link [ EXTRA_LINK_FORMATTER_KEY ] ) executionState [ "extra_links" ] . append ( link ) return executionState
1868	def PSRLDQ ( cpu , dest , src ) : # TODO(yan): Verify the correctness of truncating SRC like this ( tests # use '-1' as the value temp = Operators . EXTRACT ( src . read ( ) , 0 , 8 ) temp = Operators . ITEBV ( src . size , temp > 15 , 16 , temp ) dest . write ( dest . read ( ) >> ( temp * 8 ) )
13490	def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = MapRouletteTaskCollection . from_server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] # reconcile the new tasks with the existing tasks: for task in self . tasks : # if the task exists on the server... if task . identifier in [ existing_task . identifier for existing_task in existing . tasks ] : # and they are equal... if task == existing . get_by_identifier ( task . identifier ) : # add to 'same' list same . append ( task ) # if they are not equal, add to 'changed' list else : changed . append ( task ) # if the task does not exist on the server, add to 'new' list else : new . append ( task ) # next, check for tasks on the server that don't exist in the new collection... for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : # ... and add those to the 'deleted' list. deleted . append ( task ) # update the server with new, changed, and deleted tasks if new : newCollection = MapRouletteTaskCollection ( self . challenge , tasks = new ) newCollection . create ( server ) if changed : changedCollection = MapRouletteTaskCollection ( self . challenge , tasks = changed ) changedCollection . update ( server ) if deleted : deletedCollection = MapRouletteTaskCollection ( self . challenge , tasks = deleted ) for task in deletedCollection . tasks : task . status = 'deleted' deletedCollection . update ( server ) # return same, new, changed and deleted tasks return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
11964	def _hex_to_dec ( ip , check = True ) : if check and not is_hex ( ip ) : raise ValueError ( '_hex_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = hex ( ip ) return int ( str ( ip ) , 16 )
2472	def reset_file_stat ( self ) : # FIXME: this state does not make sense self . file_spdx_id_set = False self . file_comment_set = False self . file_type_set = False self . file_chksum_set = False self . file_conc_lics_set = False self . file_license_comment_set = False self . file_notice_set = False self . file_copytext_set = False
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
11896	def _create_index_file ( root_dir , location , image_files , dirs , force_no_processing = False ) : # Put together HTML as a list of the lines we'll want to include # Issue #2 exists to do this better than HTML in-code header_text = 'imageMe: ' + location + ' [' + str ( len ( image_files ) ) + ' image(s)]' html = [ '<!DOCTYPE html>' , '<html>' , ' <head>' , ' <title>imageMe</title>' ' <style>' , ' html, body {margin: 0;padding: 0;}' , ' .header {text-align: right;}' , ' .content {' , ' padding: 3em;' , ' padding-left: 4em;' , ' padding-right: 4em;' , ' }' , ' .image {max-width: 100%; border-radius: 0.3em;}' , ' td {width: ' + str ( 100.0 / IMAGES_PER_ROW ) + '%;}' , ' </style>' , ' </head>' , ' <body>' , ' <div class="content">' , ' <h2 class="header">' + header_text + '</h2>' ] # Populate the present subdirectories - this includes '..' unless we're at # the top level directories = [ ] if root_dir != location : directories = [ '..' ] directories += dirs if len ( directories ) > 0 : html . append ( '<hr>' ) # For each subdirectory, include a link to its index file for directory in directories : link = directory + '/' + INDEX_FILE_NAME html += [ ' <h3 class="header">' , ' <a href="' + link + '">' + directory + '</a>' , ' </h3>' ] # Populate the image gallery table # Counter to cycle down through table rows table_row_count = 1 html += [ '<hr>' , '<table>' ] # For each image file, potentially create a new <tr> and create a new <td> for image_file in image_files : if table_row_count == 1 : html . append ( '<tr>' ) img_src = _get_thumbnail_src_from_file ( location , image_file , force_no_processing ) link_target = _get_image_link_target_from_file ( location , image_file , force_no_processing ) html += [ ' <td>' , ' <a href="' + link_target + '">' , ' <img class="image" src="' + img_src + '">' , ' </a>' , ' </td>' ] if table_row_count == IMAGES_PER_ROW : table_row_count = 0 html . append ( '</tr>' ) table_row_count += 1 html += [ '</tr>' , '</table>' ] html += [ ' </div>' , ' </body>' , '</html>' ] # Actually create the file, now we've put together the HTML content index_file_path = _get_index_file_path ( location ) print ( 'Creating index file %s' % index_file_path ) index_file = open ( index_file_path , 'w' ) index_file . write ( '\n' . join ( html ) ) index_file . close ( ) # Return the path for cleaning up later return index_file_path
120	def terminate ( self ) : if not self . join_signal . is_set ( ) : self . join_signal . set ( ) # give minimal time to put generated batches in queue and gracefully shut down time . sleep ( 0.01 ) if self . main_worker_thread . is_alive ( ) : self . main_worker_thread . join ( ) if self . threaded : for worker in self . workers : if worker . is_alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) worker . join ( ) # wait until all workers are fully terminated while not self . all_finished ( ) : time . sleep ( 0.001 ) # empty queue until at least one element can be added and place None as signal that BL finished if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) # clean the queue, this reportedly prevents hanging threads while True : try : self . _queue_internal . get ( timeout = 0.005 ) except QueueEmpty : break if not self . _queue_internal . _closed : self . _queue_internal . close ( ) if not self . queue . _closed : self . queue . close ( ) self . _queue_internal . join_thread ( ) self . queue . join_thread ( ) time . sleep ( 0.025 )
487	def release ( self ) : self . _logger . debug ( "Releasing: %r" , self ) # Discard self from set of outstanding instances if self . _addedToInstanceSet : try : self . _clsOutstandingInstances . remove ( self ) except : self . _logger . exception ( "Failed to remove self from _clsOutstandingInstances: %r;" , self ) raise self . _releaser ( dbConn = self . dbConn , cursor = self . cursor ) self . __class__ . _clsNumOutstanding -= 1 assert self . _clsNumOutstanding >= 0 , "_clsNumOutstanding=%r" % ( self . _clsNumOutstanding , ) self . _releaser = None self . cursor = None self . dbConn = None self . _creationTracebackString = None self . _addedToInstanceSet = False self . _logger = None return
3612	def do_filter ( qs , keywords , exclude = False ) : and_q = Q ( ) for keyword , value in iteritems ( keywords ) : try : values = value . split ( "," ) if len ( values ) > 0 : or_q = Q ( ) for value in values : or_q |= Q ( * * { keyword : value } ) and_q &= or_q except AttributeError : # value can be a bool and_q &= Q ( * * { keyword : value } ) if exclude : qs = qs . exclude ( and_q ) else : qs = qs . filter ( and_q ) return qs
13673	def add_file ( self , * args ) : for file_path in args : self . files . append ( FilePath ( file_path , self ) )
8794	def set_all ( self , model , * * tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except TagValidationError as e : raise n_exc . BadRequest ( resource = "tags" , msg = "%s" % ( e . message ) )
6999	def parallel_cp_pfdir ( pfpickledir , outdir , lcbasedir , pfpickleglob = 'periodfinding-*.pkl*' , lclistpkl = None , cprenorm = False , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , makeneighborlcs = True , fast_mode = False , gaia_max_timeout = 60.0 , gaia_mirror = None , xmatchinfo = None , xmatchradiusarcsec = 3.0 , minobservations = 99 , sigclip = 10.0 , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , skipdone = False , done_callback = None , done_callback_args = None , done_callback_kwargs = None , maxobjects = None , nworkers = 32 ) : pfpicklelist = sorted ( glob . glob ( os . path . join ( pfpickledir , pfpickleglob ) ) ) LOGINFO ( 'found %s period-finding pickles, running cp...' % len ( pfpicklelist ) ) return parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = fast_mode , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , maxnumneighbors = maxnumneighbors , makeneighborlcs = makeneighborlcs , xmatchinfo = xmatchinfo , xmatchradiusarcsec = xmatchradiusarcsec , sigclip = sigclip , minobservations = minobservations , cprenorm = cprenorm , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , timecols = timecols , magcols = magcols , errcols = errcols , skipdone = skipdone , nworkers = nworkers , done_callback = done_callback , done_callback_args = done_callback_args , done_callback_kwargs = done_callback_kwargs )
3274	def get_user_info ( self ) : if self . value in ERROR_DESCRIPTIONS : s = "{}" . format ( ERROR_DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context_info : s += ": {}" . format ( self . context_info ) elif self . value in ERROR_RESPONSES : s += ": {}" . format ( ERROR_RESPONSES [ self . value ] ) if self . src_exception : s += "\n Source exception: '{}'" . format ( self . src_exception ) if self . err_condition : s += "\n Error condition: '{}'" . format ( self . err_condition ) return s
5250	def start ( self ) : # flush event queue in defensive way logger = _get_logger ( self . debug ) started = self . _session . start ( ) if started : ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) else : ev = self . _session . nextEvent ( self . timeout ) if ev . eventType ( ) == blpapi . Event . SESSION_STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise ConnectionError ( 'Could not start blpapi.Session' ) self . _init_services ( ) return self
471	def read_analogies_file ( eval_file = 'questions-words.txt' , word2id = None ) : if word2id is None : word2id = { } questions = [ ] questions_skipped = 0 with open ( eval_file , "rb" ) as analogy_f : for line in analogy_f : if line . startswith ( b":" ) : # Skip comments. continue words = line . strip ( ) . lower ( ) . split ( b" " ) # lowercase ids = [ word2id . get ( w . strip ( ) ) for w in words ] if None in ids or len ( ids ) != 4 : questions_skipped += 1 else : questions . append ( np . array ( ids ) ) tl . logging . info ( "Eval analogy file: %s" % eval_file ) tl . logging . info ( "Questions: %d" , len ( questions ) ) tl . logging . info ( "Skipped: %d" , questions_skipped ) analogy_questions = np . array ( questions , dtype = np . int32 ) return analogy_questions
7754	def process_stanza ( self , stanza ) : self . fix_in_stanza ( stanza ) to_jid = stanza . to_jid if not self . process_all_stanzas and to_jid and ( to_jid != self . me and to_jid . bare ( ) != self . me . bare ( ) ) : return self . route_stanza ( stanza ) try : if isinstance ( stanza , Iq ) : if self . process_iq ( stanza ) : return True elif isinstance ( stanza , Message ) : if self . process_message ( stanza ) : return True elif isinstance ( stanza , Presence ) : if self . process_presence ( stanza ) : return True except ProtocolError , err : typ = stanza . stanza_type if typ != 'error' and ( typ != 'result' or stanza . stanza_type != 'iq' ) : response = stanza . make_error_response ( err . xmpp_name ) self . send ( response ) err . log_reported ( ) else : err . log_ignored ( ) return logger . debug ( "Unhandled %r stanza: %r" % ( stanza . stanza_type , stanza . serialize ( ) ) ) return False
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( * * _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( * * _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) # TODO: Test that re-encryption does not occur on similar # passphrases keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
9985	def _reload ( self , module = None ) : if self . module is None : raise RuntimeError elif module is None : import importlib module = ModuleSource ( importlib . reload ( module ) ) elif module . name != self . module : raise RuntimeError if self . name in module . funcs : func = module . funcs [ self . name ] self . __init__ ( func = func ) else : self . __init__ ( func = NULL_FORMULA ) return self
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
5959	def ma ( self ) : a = self . array return numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) )
13363	def echo_via_pager ( text , color = None ) : color = resolve_color_default ( color ) if not isinstance ( text , string_types ) : text = text_type ( text ) from . _termui_impl import pager return pager ( text + '\n' , color )
13518	def maximum_deck_area ( self , water_plane_coef = 0.88 ) : AD = self . beam * self . length * water_plane_coef return AD
4820	def refresh_token ( func ) : @ wraps ( func ) def inner ( self , * args , * * kwargs ) : """ Before calling the wrapped function, we check if the JWT token is expired, and if so, re-connect. """ if self . token_expired ( ) : self . connect ( ) return func ( self , * args , * * kwargs ) return inner
7371	def main ( args_list = None ) : args = parse_args ( args_list ) binding_predictions = run_predictor ( args ) df = binding_predictions . to_dataframe ( ) logger . info ( '\n%s' , df ) if args . output_csv : df . to_csv ( args . output_csv , index = False ) print ( "Wrote: %s" % args . output_csv )
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) # report in the original locus order cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
3639	def squad ( self , squad_id = 0 , persona_id = None ) : method = 'GET' url = 'squad/%s/user/%s' % ( squad_id , persona_id or self . persona_id ) # pinEvents events = [ self . pin . event ( 'page_view' , 'Hub - Squads' ) ] self . pin . send ( events ) # TODO: ability to return other info than players only rc = self . __request__ ( method , url ) # pinEvents events = [ self . pin . event ( 'page_view' , 'Squad Details' ) , self . pin . event ( 'page_view' , 'Squads - Squad Overview' ) ] self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'players' , ( ) ) ]
3807	def nested_formula_parser ( formula , check = True ) : formula = formula . replace ( '[' , '' ) . replace ( ']' , '' ) charge_splits = bracketed_charge_re . split ( formula ) if len ( charge_splits ) > 1 : formula = charge_splits [ 0 ] else : formula = formula . split ( '+' ) [ 0 ] . split ( '-' ) [ 0 ] stack = [ [ ] ] last = stack [ 0 ] tokens = formula_token_matcher_rational . findall ( formula ) # The set of letters in the tokens should match the set of letters if check : token_letters = set ( [ j for i in tokens for j in i if j in letter_set ] ) formula_letters = set ( i for i in formula if i in letter_set ) if formula_letters != token_letters : raise Exception ( 'Input may not be a formula; extra letters were detected' ) for token in tokens : if token == "(" : stack . append ( [ ] ) last = stack [ - 1 ] elif token == ")" : temp_dict = { } for d in last : for ele , count in d . items ( ) : if ele in temp_dict : temp_dict [ ele ] = temp_dict [ ele ] + count else : temp_dict [ ele ] = count stack . pop ( ) last = stack [ - 1 ] last . append ( temp_dict ) elif token . isalpha ( ) : last . append ( { token : 1 } ) else : v = float ( token ) v_int = int ( v ) if v_int == v : v = v_int last [ - 1 ] = { ele : count * v for ele , count in last [ - 1 ] . items ( ) } ans = { } for d in last : for ele , count in d . items ( ) : if ele in ans : ans [ ele ] = ans [ ele ] + count else : ans [ ele ] = count return ans
4182	def window_blackman_nuttall ( N ) : a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
9933	def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . __dict__ , self . obj , self . seen , self . _ignore ) # Ignore the calling frame, its builtins, globals and locals self . ignore_caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . _gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
10520	def oneup ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
8770	def _lswitch_select_open ( self , context , switches = None , * * kwargs ) : if switches is not None : for res in switches [ "results" ] : count = res [ "_relations" ] [ "LogicalSwitchStatus" ] [ "lport_count" ] if ( self . limits [ 'max_ports_per_switch' ] == 0 or count < self . limits [ 'max_ports_per_switch' ] ) : return res [ "uuid" ] return None
1025	def quote ( c ) : i = ord ( c ) return ESCAPE + HEX [ i // 16 ] + HEX [ i % 16 ]
8297	def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
5694	def create_table ( self , conn ) : # Make cursor cur = conn . cursor ( ) # Drop table if it already exists, to be recreated. This # could in the future abort if table already exists, and not # recreate it from scratch. #cur.execute('''DROP TABLE IF EXISTS %s'''%self.table) #conn.commit() if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : # "normal" table creation. cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : # When tabledef contains the full CREATE statement (for # virtual tables). cur . execute ( self . tabledef ) conn . commit ( )
4332	def noiseprof ( self , input_filepath , profile_path ) : if os . path . isdir ( profile_path ) : raise ValueError ( "profile_path {} is a directory." . format ( profile_path ) ) if os . path . dirname ( profile_path ) == '' and profile_path != '' : _abs_profile_path = os . path . join ( os . getcwd ( ) , profile_path ) else : _abs_profile_path = profile_path if not os . access ( os . path . dirname ( _abs_profile_path ) , os . W_OK ) : raise IOError ( "profile_path {} is not writeable." . format ( _abs_profile_path ) ) effect_args = [ 'noiseprof' , profile_path ] self . build ( input_filepath , None , extra_args = effect_args ) return None
9084	def upload_backend ( index = 'dev' , user = None ) : get_vars ( ) use_devpi ( index = index ) with fab . lcd ( '../application' ) : fab . local ( 'make upload' )
11988	async def connect ( self ) : if not self . _consumer : waiter = self . _waiter = asyncio . Future ( ) try : address = self . _websocket_host ( ) self . logger . info ( 'Connect to %s' , address ) self . _consumer = await self . http . get ( address ) if self . _consumer . status_code != 101 : raise PusherError ( "Could not connect to websocket" ) except Exception as exc : waiter . set_exception ( exc ) raise else : await waiter return self . _consumer
13768	def collect_files ( self ) : self . files = [ ] for bundle in self . bundles : bundle . init_build ( self , self . builder ) bundle_files = bundle . prepare ( ) self . files . extend ( bundle_files ) return self
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
1720	def trans ( ele , standard = False ) : try : node = globals ( ) . get ( ele [ 'type' ] ) if not node : raise NotImplementedError ( '%s is not supported!' % ele [ 'type' ] ) if standard : node = node . __dict__ [ 'standard' ] if 'standard' in node . __dict__ else node return node ( * * ele ) except : #print ele raise
11969	def _bits_to_dec ( nm , check = True ) : if check and not is_bits_nm ( nm ) : raise ValueError ( '_bits_to_dec: invalid netmask: "%s"' % nm ) bits = int ( str ( nm ) ) return VALID_NETMASKS [ bits ]
5651	def _scan_footpaths_to_departure_stop ( self , connection_dep_stop , connection_dep_time , arrival_time_target ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ connection_dep_stop ] , data = True ) : d_walk = data [ 'd_walk' ] neighbor_dep_time = connection_dep_time - d_walk / self . _walk_speed pt = LabelTimeSimple ( departure_time = neighbor_dep_time , arrival_time_target = arrival_time_target ) self . _stop_profiles [ neighbor ] . update_pareto_optimal_tuples ( pt )
2942	def _add_notify ( self , task_spec ) : if task_spec . name in self . task_specs : raise KeyError ( 'Duplicate task spec name: ' + task_spec . name ) self . task_specs [ task_spec . name ] = task_spec task_spec . id = len ( self . task_specs )
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
10480	def _performAction ( self , action ) : try : _a11y . AXUIElement . _performAction ( self , 'AX%s' % action ) except _a11y . ErrorUnsupported as e : sierra_ver = '10.12' if mac_ver ( ) [ 0 ] < sierra_ver : raise e else : pass
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
2147	def _configuration ( self , kwargs , config_item ) : if 'notification_configuration' not in config_item : if 'notification_type' not in kwargs : return nc = kwargs [ 'notification_configuration' ] = { } for field in Resource . configuration [ kwargs [ 'notification_type' ] ] : if field not in config_item : raise exc . TowerCLIError ( 'Required config field %s not' ' provided.' % field ) else : nc [ field ] = config_item [ field ] else : kwargs [ 'notification_configuration' ] = config_item [ 'notification_configuration' ]
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
477	def moses_multi_bleu ( hypotheses , references , lowercase = False ) : if np . size ( hypotheses ) == 0 : return np . float32 ( 0.0 ) # Get MOSES multi-bleu script try : multi_bleu_path , _ = urllib . request . urlretrieve ( "https://raw.githubusercontent.com/moses-smt/mosesdecoder/" "master/scripts/generic/multi-bleu.perl" ) os . chmod ( multi_bleu_path , 0o755 ) except Exception : # pylint: disable=W0702 tl . logging . info ( "Unable to fetch multi-bleu.perl script, using local." ) metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) bin_dir = os . path . abspath ( os . path . join ( metrics_dir , ".." , ".." , "bin" ) ) multi_bleu_path = os . path . join ( bin_dir , "tools/multi-bleu.perl" ) # Dump hypotheses and references to tempfiles hypothesis_file = tempfile . NamedTemporaryFile ( ) hypothesis_file . write ( "\n" . join ( hypotheses ) . encode ( "utf-8" ) ) hypothesis_file . write ( b"\n" ) hypothesis_file . flush ( ) reference_file = tempfile . NamedTemporaryFile ( ) reference_file . write ( "\n" . join ( references ) . encode ( "utf-8" ) ) reference_file . write ( b"\n" ) reference_file . flush ( ) # Calculate BLEU using multi-bleu script with open ( hypothesis_file . name , "r" ) as read_pred : bleu_cmd = [ multi_bleu_path ] if lowercase : bleu_cmd += [ "-lc" ] bleu_cmd += [ reference_file . name ] try : bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) bleu_out = bleu_out . decode ( "utf-8" ) bleu_score = re . search ( r"BLEU = (.+?)," , bleu_out ) . group ( 1 ) bleu_score = float ( bleu_score ) except subprocess . CalledProcessError as error : if error . output is not None : tl . logging . warning ( "multi-bleu.perl script returned non-zero exit code" ) tl . logging . warning ( error . output ) bleu_score = np . float32 ( 0.0 ) # Close temp files hypothesis_file . close ( ) reference_file . close ( ) return np . float32 ( bleu_score )
740	def coordinateForPosition ( self , longitude , latitude , altitude = None ) : coords = PROJ ( longitude , latitude ) if altitude is not None : coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) coordinate = numpy . array ( coords ) coordinate = coordinate / self . scale return coordinate . astype ( int )
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
2225	def _update_hasher ( hasher , data , types = True ) : # Determine if the data should be hashed directly or iterated through if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : # Denote that we are hashing over an iterable # Multiple structure bytes makes it harder accidently make conflicts SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) # first, try to nest quickly without recursive calls # (this works if all data in the sequence is a non-iterable) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : # need to use recursive calls # Update based on current item _update_hasher ( hasher , item , types ) for item in iter_ : # Ensure the items have a spacer between them _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
7755	def set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : # pylint: disable-msg=R0913 self . lock . acquire ( ) try : self . _set_response_handlers ( stanza , res_handler , err_handler , timeout_handler , timeout ) finally : self . lock . release ( )
11487	def _search_folder_for_item_or_folder ( name , folder_id ) : session . token = verify_credentials ( ) children = session . communicator . folder_children ( session . token , folder_id ) for folder in children [ 'folders' ] : if folder [ 'name' ] == name : return False , folder [ 'folder_id' ] # Found a folder for item in children [ 'items' ] : if item [ 'name' ] == name : return True , item [ 'item_id' ] # Found an item return False , - 1
8114	def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )
10501	def waitForWindowToDisappear ( self , winName , timeout = 10 ) : callback = AXCallbacks . elemDisappearedCallback retelem = None args = ( retelem , self ) # For some reason for the AXUIElementDestroyed notification to fire, # we need to have a reference to it first win = self . findFirst ( AXRole = 'AXWindow' , AXTitle = winName ) return self . waitFor ( timeout , 'AXUIElementDestroyed' , callback = callback , args = args , AXRole = 'AXWindow' , AXTitle = winName )
9728	def get_analog ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDevice , data , component_position ) if device . sample_count > 0 : component_position , sample_number = QRTPacket . _get_exact ( RTSampleNumber , data , component_position ) RTAnalogChannel . format = struct . Struct ( RTAnalogChannel . format_str % device . sample_count ) for _ in range ( device . channel_count ) : component_position , channel = QRTPacket . _get_tuple ( RTAnalogChannel , data , component_position ) append_components ( ( device , sample_number , channel ) ) return components
7974	def stop ( self , join = False , timeout = None ) : logger . debug ( "Closing the io handlers..." ) for handler in self . io_handlers : handler . close ( ) if self . event_thread and self . event_thread . is_alive ( ) : logger . debug ( "Sending the QUIT signal" ) self . event_queue . put ( QUIT ) logger . debug ( " sent" ) threads = self . io_threads + self . timeout_threads for thread in threads : logger . debug ( "Stopping thread: {0!r}" . format ( thread ) ) thread . stop ( ) if not join : return if self . event_thread : threads . append ( self . event_thread ) if timeout is None : for thread in threads : thread . join ( ) else : timeout1 = ( timeout * 0.01 ) / len ( threads ) threads_left = [ ] for thread in threads : logger . debug ( "Quick-joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout1 ) if thread . is_alive ( ) : logger . debug ( " thread still alive" . format ( thread ) ) threads_left . append ( thread ) if threads_left : timeout2 = ( timeout * 0.99 ) / len ( threads_left ) for thread in threads_left : logger . debug ( "Joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout2 ) self . io_threads = [ ] self . event_thread = None
4475	def sample_clip_indices ( filename , n_samples , sr ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : # Measure required length of fragment n_target = int ( np . ceil ( n_samples * soundf . samplerate / float ( sr ) ) ) # Raise exception if source is too short if len ( soundf ) < n_target : raise RuntimeError ( 'Source {} (length={})' . format ( filename , len ( soundf ) ) + ' must be at least the length of the input ({})' . format ( n_target ) ) # Draw a starting point at random in the background waveform start = np . random . randint ( 0 , 1 + len ( soundf ) - n_target ) stop = start + n_target return start , stop
8076	def rectmode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . rectmode = mode return self . rectmode elif mode is None : return self . rectmode else : raise ShoebotError ( _ ( "rectmode: invalid input" ) )
13644	def append_arguments ( klass , sub_parsers , default_epilog , general_arguments ) : entry_name = hump_to_underscore ( klass . __name__ ) . replace ( '_component' , '' ) # set sub command document epilog = default_epilog if default_epilog else 'This tool generate by `cliez` ' 'https://www.github.com/wangwenpei/cliez' sub_parser = sub_parsers . add_parser ( entry_name , help = klass . __doc__ , epilog = epilog ) sub_parser . description = klass . add_arguments . __doc__ # add slot arguments if hasattr ( klass , 'add_slot_args' ) : slot_args = klass . add_slot_args ( ) or [ ] for v in slot_args : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) sub_parser . description = klass . add_slot_args . __doc__ pass user_arguments = klass . add_arguments ( ) or [ ] for v in user_arguments : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) if not klass . exclude_global_option : for v in general_arguments : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) return sub_parser
10451	def grabfocus ( self , window_name , object_name = None ) : if not object_name : handle , name , app = self . _get_window_handle ( window_name ) else : handle = self . _get_object_handle ( window_name , object_name ) return self . _grabfocus ( handle )
4407	async def listen ( self ) : while not self . _shutdown : try : data = json . loads ( await self . _ws . recv ( ) ) except websockets . ConnectionClosed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . _lavalink . players . _players . copy ( ) . keys ( ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( g ) ) await ws . voice_state ( int ( g ) , None ) self . _lavalink . players . clear ( ) if self . _shutdown : break if await self . _attempt_reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received WebSocket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received WebSocket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . _lavalink . players [ int ( data [ 'guildId' ] ) ] event = None if data [ 'type' ] == 'TrackEndEvent' : event = TrackEndEvent ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'TrackExceptionEvent' : event = TrackExceptionEvent ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'TrackStuckEvent' : event = TrackStuckEvent ( player , data [ 'track' ] , data [ 'thresholdMs' ] ) if event : await self . _lavalink . dispatch_event ( event ) elif op == 'playerUpdate' : await self . _lavalink . update_state ( data ) elif op == 'stats' : self . _lavalink . stats . _update ( data ) await self . _lavalink . dispatch_event ( StatsUpdateEvent ( self . _lavalink . stats ) ) log . debug ( 'Closing WebSocket...' ) await self . _ws . close ( )
13262	def task ( func , * * config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
8400	def rescale ( x , to = ( 0 , 1 ) , _from = None ) : if _from is None : _from = np . min ( x ) , np . max ( x ) return np . interp ( x , _from , to )
2768	def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
5226	def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
5511	def register_memory ( ) : # XXX How to get a reliable representation of memory being used is # not clear. (rss - shared) seems kind of ok but we might also use # the private working set via get_memory_maps().private*. def get_mem ( proc ) : if os . name == 'posix' : mem = proc . memory_info_ex ( ) counter = mem . rss if 'shared' in mem . _fields : counter -= mem . shared return counter else : # TODO figure out what to do on Windows return proc . get_memory_info ( ) . rss if SERVER_PROC is not None : mem = get_mem ( SERVER_PROC ) for child in SERVER_PROC . children ( ) : mem += get_mem ( child ) server_memory . append ( bytes2human ( mem ) )
2065	def inverse_transform ( self , X_in ) : X = X_in . copy ( deep = True ) # first check the type X = util . convert_input ( X ) if self . _dim is None : raise ValueError ( 'Must train encoder before it can be used to inverse_transform data' ) # then make sure that it is the right size if X . shape [ 1 ] != self . _dim : if self . drop_invariant : raise ValueError ( "Unexpected input dimension %d, the attribute drop_invariant should " "set as False when transform data" % ( X . shape [ 1 ] , ) ) else : raise ValueError ( 'Unexpected input dimension %d, expected %d' % ( X . shape [ 1 ] , self . _dim , ) ) if not self . cols : return X if self . return_df else X . values if self . handle_unknown == 'value' : for col in self . cols : if any ( X [ col ] == - 1 ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category -1 when encode %s" % ( col , ) ) if self . handle_unknown == 'return_nan' and self . handle_missing == 'return_nan' : for col in self . cols : if X [ col ] . isnull ( ) . any ( ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category nan when encode %s" % ( col , ) ) for switch in self . mapping : column_mapping = switch . get ( 'mapping' ) inverse = pd . Series ( data = column_mapping . index , index = column_mapping . get_values ( ) ) X [ switch . get ( 'col' ) ] = X [ switch . get ( 'col' ) ] . map ( inverse ) . astype ( switch . get ( 'data_type' ) ) return X if self . return_df else X . values
8422	def _tidyup_labels ( self , labels ) : def remove_zeroes ( s ) : """ Remove unnecessary zeros for float string s """ tup = s . split ( 'e' ) if len ( tup ) == 2 : mantissa = tup [ 0 ] . rstrip ( '0' ) . rstrip ( '.' ) exponent = int ( tup [ 1 ] ) if exponent : s = '%se%d' % ( mantissa , exponent ) else : s = mantissa return s def as_exp ( s ) : """ Float string s as in exponential format """ return s if 'e' in s else '{:1.0e}' . format ( float ( s ) ) # If any are in exponential format, make all of # them expontential has_e = np . array ( [ 'e' in x for x in labels ] ) if not np . all ( has_e ) and not np . all ( ~ has_e ) : labels = [ as_exp ( x ) for x in labels ] labels = [ remove_zeroes ( x ) for x in labels ] return labels
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
8056	def do_escape_nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape_nl = False else : self . escape_nl = True
3275	def handle_delete ( self ) : # DELETE is only supported for the '/by_tag/' collection if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) # path must be '/by_tag/<tag>/<resname>' catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) return True
3983	def get_same_container_repos_from_spec ( app_or_library_spec ) : repos = set ( ) app_or_lib_repo = get_repo_of_app_or_library ( app_or_library_spec . name ) if app_or_lib_repo is not None : repos . add ( app_or_lib_repo ) for dependent_name in app_or_library_spec [ 'depends' ] [ 'libs' ] : repos . add ( get_repo_of_app_or_library ( dependent_name ) ) return repos
6123	def instance_for_arguments ( self , arguments ) : profiles = { * * { key : value . instance_for_arguments ( arguments ) for key , value in self . profile_prior_model_dict . items ( ) } , * * self . constant_profile_dict } try : redshift = self . redshift . instance_for_arguments ( arguments ) except AttributeError : redshift = self . redshift pixelization = self . pixelization . instance_for_arguments ( arguments ) if isinstance ( self . pixelization , pm . PriorModel ) else self . pixelization regularization = self . regularization . instance_for_arguments ( arguments ) if isinstance ( self . regularization , pm . PriorModel ) else self . regularization hyper_galaxy = self . hyper_galaxy . instance_for_arguments ( arguments ) if isinstance ( self . hyper_galaxy , pm . PriorModel ) else self . hyper_galaxy return galaxy . Galaxy ( redshift = redshift , pixelization = pixelization , regularization = regularization , hyper_galaxy = hyper_galaxy , * * profiles )
394	def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )
7234	def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , image = None , image_bounds = None , index = "vector-user-provided" , name = "GBDX_Task_Output" , * * kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX_API_KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorTileLayer ( url , source_name = name , styles = styles , * * kwargs ) image_layer = self . _build_image_layer ( image , image_bounds ) template = BaseTemplate ( map_id , * * { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : self . gbdx_connection . access_token } ) template . inject ( )
6671	def is_file ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isfile ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
10945	def _do_run ( self , mode = '1' ) : for a in range ( len ( self . particle_groups ) ) : group = self . particle_groups [ a ] lp = LMParticles ( self . state , group , * * self . _kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . _dif_tile = self . _load_j_diftile ( a ) if mode == '1' : lp . do_run_1 ( ) if mode == '2' : lp . do_run_2 ( ) if mode == 'internal' : lp . do_internal_run ( ) self . stats . append ( lp . get_termination_stats ( get_cos = self . get_cos ) ) if self . save_J and ( mode != 'internal' ) : self . _dump_j_diftile ( a , lp . J , lp . _dif_tile ) self . _has_saved_J [ a ] = True
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , * * self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad_func
8686	def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
1099	def _count_leading ( line , ch ) : i , n = 0 , len ( line ) while i < n and line [ i ] == ch : i += 1 return i
11238	def imap_send ( func , gen ) : gen = iter ( gen ) assert _is_just_started ( gen ) yielder = yield_from ( gen ) for item in yielder : with yielder : yielder . send ( func ( ( yield item ) ) ) return_ ( yielder . result )
11118	def get_parent_directory_info ( self , relativePath ) : relativePath = os . path . normpath ( relativePath ) # if root directory if relativePath in ( '' , '.' ) : return self , "relativePath is empty pointing to the repostitory itself." # split path parentDirPath , _ = os . path . split ( relativePath ) # get parent directory info return self . get_directory_info ( parentDirPath )
6269	def on_resize ( self , width , height ) : self . width , self . height = width , height self . buffer_width , self . buffer_height = width , height self . resize ( width , height )
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
2895	def _on_complete_hook ( self , my_task ) : # Find all matching conditions. outputs = [ ] for condition , output in self . cond_task_specs : if self . choice is not None and output not in self . choice : continue if condition is None : outputs . append ( self . _wf_spec . get_task_spec_from_name ( output ) ) continue if not condition . _matches ( my_task ) : continue outputs . append ( self . _wf_spec . get_task_spec_from_name ( output ) ) my_task . _sync_children ( outputs , Task . FUTURE ) for child in my_task . children : child . task_spec . _update ( child )
10904	def compare_data_model_residuals ( s , tile , data_vmin = 'calc' , data_vmax = 'calc' , res_vmin = - 0.1 , res_vmax = 0.1 , edgepts = 'calc' , do_imshow = True , data_cmap = plt . cm . bone , res_cmap = plt . cm . RdBu ) : # This could be modified to alpha the borderline... or to embiggen # the image and slice it more finely residuals = s . residuals [ tile . slicer ] . squeeze ( ) data = s . data [ tile . slicer ] . squeeze ( ) model = s . model [ tile . slicer ] . squeeze ( ) if data . ndim != 2 : raise ValueError ( 'tile does not give a 2D slice' ) im = np . zeros ( [ data . shape [ 0 ] , data . shape [ 1 ] , 4 ] ) if data_vmin == 'calc' : data_vmin = 0.5 * ( data . min ( ) + model . min ( ) ) if data_vmax == 'calc' : data_vmax = 0.5 * ( data . max ( ) + model . max ( ) ) #1. Get masks: upper_mask , center_mask , lower_mask = trisect_image ( im . shape , edgepts ) #2. Get colorbar'd images gm = data_cmap ( center_data ( model , data_vmin , data_vmax ) ) dt = data_cmap ( center_data ( data , data_vmin , data_vmax ) ) rs = res_cmap ( center_data ( residuals , res_vmin , res_vmax ) ) for a in range ( 4 ) : im [ : , : , a ] [ upper_mask ] = rs [ : , : , a ] [ upper_mask ] im [ : , : , a ] [ center_mask ] = gm [ : , : , a ] [ center_mask ] im [ : , : , a ] [ lower_mask ] = dt [ : , : , a ] [ lower_mask ] if do_imshow : return plt . imshow ( im ) else : return im
1222	def processed_shape ( self , shape ) : for processor in self . preprocessors : shape = processor . processed_shape ( shape = shape ) return shape
4202	def aryule ( X , order , norm = 'biased' , allow_singularity = True ) : assert norm in [ 'biased' , 'unbiased' ] r = CORRELATION ( X , maxlags = order , norm = norm ) A , P , k = LEVINSON ( r , allow_singularity = allow_singularity ) return A , P , k
13332	def localize ( name ) : env = cpenv . get_active_env ( ) if not env : click . echo ( 'You need to activate an environment first.' ) return try : r = cpenv . resolve ( name ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) module = r . resolved [ 0 ] if isinstance ( module , cpenv . VirtualEnvironment ) : click . echo ( '\nCan only localize a module not an environment' ) return active_modules = cpenv . get_active_modules ( ) if module in active_modules : click . echo ( '\nCan not localize an active module.' ) return if module in env . get_modules ( ) : click . echo ( '\n{} is already local to {}' . format ( module . name , env . name ) ) return if click . confirm ( '\nAdd {} to env {}?' . format ( module . name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : module = env . add_module ( module . name , module . path ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) click . echo ( '\nActivate the localize module:' ) click . echo ( ' cpenv activate {} {}' . format ( env . name , module . name ) )
1340	def binarize ( x , values , threshold = None , included_in = 'upper' ) : lower , upper = values if threshold is None : threshold = ( lower + upper ) / 2. x = x . copy ( ) if included_in == 'lower' : x [ x <= threshold ] = lower x [ x > threshold ] = upper elif included_in == 'upper' : x [ x < threshold ] = lower x [ x >= threshold ] = upper else : raise ValueError ( 'included_in must be "lower" or "upper"' ) return x
5580	def extract_contours ( array , tile , interval = 100 , field = 'elev' , base = 0 ) : import matplotlib . pyplot as plt levels = _get_contour_values ( array . min ( ) , array . max ( ) , interval = interval , base = base ) if not levels : return [ ] contours = plt . contour ( array , levels ) index = 0 out_contours = [ ] for level in range ( len ( contours . collections ) ) : elevation = levels [ index ] index += 1 paths = contours . collections [ level ] . get_paths ( ) for path in paths : out_coords = [ ( tile . left + ( y * tile . pixel_x_size ) , tile . top - ( x * tile . pixel_y_size ) , ) for x , y in zip ( path . vertices [ : , 1 ] , path . vertices [ : , 0 ] ) ] if len ( out_coords ) >= 2 : out_contours . append ( dict ( properties = { field : elevation } , geometry = mapping ( LineString ( out_coords ) ) ) ) return out_contours
12341	def _set_path ( self , path ) : import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
12095	def indexImages ( folder , fname = "index.html" ) : #TODO: REMOVE html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( " " , os . path . abspath ( folder + "/" + fname ) ) return
7756	def _set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : # pylint: disable-msg=R0913 self . fix_out_stanza ( stanza ) to_jid = stanza . to_jid if to_jid : to_jid = unicode ( to_jid ) if timeout_handler : def callback ( dummy1 , dummy2 ) : """Wrapper for the timeout handler to make it compatible with the `ExpiringDictionary` """ timeout_handler ( ) self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout , callback ) else : self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout )
7781	def rfc2426 ( self ) : ret = "begin:VCARD\r\n" ret += "version:3.0\r\n" for _unused , value in self . content . items ( ) : if value is None : continue if type ( value ) is list : for v in value : ret += v . rfc2426 ( ) else : v = value . rfc2426 ( ) ret += v return ret + "end:VCARD\r\n"
1442	def next_tuple ( self , latency_in_ns ) : self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) self . update_count ( self . NEXT_TUPLE_COUNT )
13660	def subroute ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , subroute ( * components ) ) return f return _factory
6453	def dist ( self , src , tar ) : if tar == src : return 0.0 if not src or not tar : return 1.0 max_length = max ( len ( src ) , len ( tar ) ) return self . dist_abs ( src , tar ) / max_length
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
9557	def _apply_record_predicates ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for predicate , code , message , modulus in self . _record_predicates : if i % modulus == 0 : # support sampling rdict = self . _as_dict ( r ) try : valid = predicate ( rdict ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . __name__ , predicate . __doc__ ) if context is not None : p [ 'context' ] = context yield p
4396	def adsSyncWriteByNameEx ( port , address , data_name , value , data_type ) : # type: (int, AmsAddr, str, Any, Type) -> None # Get the handle of the PLC-variable handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) # Write the value of a PLC-variable, via handle adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_VALBYHND , handle , value , data_type ) # Release the handle of the PLC-variable adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT )
2926	def create_package ( self ) : # Check that all files exist (and calculate the longest shared path # prefix): self . input_path_prefix = None for filename in self . input_files : if not os . path . isfile ( filename ) : raise ValueError ( '%s does not exist or is not a file' % filename ) if self . input_path_prefix : full = os . path . abspath ( os . path . dirname ( filename ) ) while not ( full . startswith ( self . input_path_prefix ) and self . input_path_prefix ) : self . input_path_prefix = self . input_path_prefix [ : - 1 ] else : self . input_path_prefix = os . path . abspath ( os . path . dirname ( filename ) ) # Parse all of the XML: self . bpmn = { } for filename in self . input_files : bpmn = ET . parse ( filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn # Now run through pre-parsing and validation: for filename , bpmn in list ( self . bpmn . items ( ) ) : bpmn = self . pre_parse_and_validate ( bpmn , filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn # Now check that we can parse it fine: for filename , bpmn in list ( self . bpmn . items ( ) ) : self . parser . add_bpmn_xml ( bpmn , filename = filename ) self . wf_spec = self . parser . get_spec ( self . entry_point_process ) # Now package everything: self . package_zip = zipfile . ZipFile ( self . package_file , "w" , compression = zipfile . ZIP_DEFLATED ) done_files = set ( ) for spec in self . wf_spec . get_specs_depth_first ( ) : filename = spec . file if filename not in done_files : done_files . add ( filename ) bpmn = self . bpmn [ os . path . abspath ( filename ) ] self . write_to_package_zip ( "%s.bpmn" % spec . name , ET . tostring ( bpmn . getroot ( ) ) ) self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( filename ) , filename ) self . _call_editor_hook ( 'package_for_editor' , spec , filename ) self . write_meta_data ( ) self . write_manifest ( ) self . package_zip . close ( )
8589	def stop_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/stop' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
6365	def to_dict ( self ) : return { 'tp' : self . _tp , 'tn' : self . _tn , 'fp' : self . _fp , 'fn' : self . _fn }
9866	def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
5342	def __get_dash_menu ( self , kibiter_major ) : # omenu = OrderedDict() omenu = [ ] # Start with Overview omenu . append ( self . menu_panels_common [ 'Overview' ] ) # Now the data _getsources ds_menu = self . __get_menu_entries ( kibiter_major ) # Remove the kafka and community menus, they will be included at the end kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu # If kafka and community are present add them before the Data Status and About if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) # At the end Data Status, About omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
12246	def get_bucket ( self , bucket_name , validate = True , headers = None , force = None ) : if force : bucket = super ( S3Connection , self ) . get_bucket ( bucket_name , validate , headers ) mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket if mimicdb . backend . sismember ( tpl . connection , bucket_name ) : return Bucket ( self , bucket_name ) else : if validate : raise S3ResponseError ( 404 , 'NoSuchBucket' ) else : return Bucket ( self , bucket_name )
8190	def nodes_by_betweenness ( self , treshold = 0.0 ) : nodes = [ ( n . betweenness , n ) for n in self . nodes if n . betweenness > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
11381	def do_autodiscover ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 2 : raise template . TemplateSyntaxError ( '%s takes an object as its parameter.' % args [ 0 ] ) else : obj = args [ 1 ] return OEmbedAutodiscoverNode ( obj )
3775	def solve_prop ( self , goal , reset_method = True ) : if self . Tmin is None or self . Tmax is None : raise Exception ( 'Both a minimum and a maximum value are not present indicating there is not enough data for temperature dependency.' ) if not self . test_property_validity ( goal ) : raise Exception ( 'Input property is not considered plausible; no method would calculate it.' ) def error ( T ) : if reset_method : self . method = None return self . T_dependent_property ( T ) - goal try : return brenth ( error , self . Tmin , self . Tmax ) except ValueError : raise Exception ( 'To within the implemented temperature range, it is not possible to calculate the desired value.' )
12770	def step ( self , substeps = 2 ) : # by default we step by following our loaded marker data. self . frame_no += 1 try : next ( self . follower ) except ( AttributeError , StopIteration ) as err : self . reset ( )
11063	def push ( self , message ) : if self . _ignore_event ( message ) : return None , None args = self . _parse_message ( message ) self . log . debug ( "Searching for command using chunks: %s" , args ) cmd , msg_args = self . _find_longest_prefix_command ( args ) if cmd is not None : if message . user is None : self . log . debug ( "Discarded message with no originating user: %s" , message ) return None , None sender = message . user . username if message . channel is not None : sender = "#%s/%s" % ( message . channel . name , sender ) self . log . info ( "Received from %s: %s, args %s" , sender , cmd , msg_args ) f = self . _get_command ( cmd , message . user ) if f : if self . _is_channel_ignored ( f , message . channel ) : self . log . info ( "Channel %s is ignored, discarding command %s" , message . channel , cmd ) return '_ignored_' , "" return cmd , f . execute ( message , msg_args ) return '_unauthorized_' , "Sorry, you are not authorized to run %s" % cmd return None , None
3094	def oauth_aware ( self , method ) : def setup_oauth ( request_handler , * args , * * kwargs ) : if self . _in_error : self . _display_error_message ( request_handler ) return user = users . get_current_user ( ) # Don't use @login_decorator as this could be used in a # POST request. if not user : request_handler . redirect ( users . create_login_url ( request_handler . request . uri ) ) return self . _create_flow ( request_handler ) self . flow . params [ 'state' ] = _build_state_value ( request_handler , user ) self . credentials = self . _storage_class ( self . _credentials_class , None , self . _credentials_property_name , user = user ) . get ( ) try : resp = method ( request_handler , * args , * * kwargs ) finally : self . credentials = None return resp return setup_oauth
13239	def intervals ( self , range_start = datetime . datetime . min , range_end = datetime . datetime . max ) : # At the moment the algorithm works on periods split by calendar day, one at a time, # merging them if they're continuous; to avoid looping infinitely for infinitely long # periods, it splits periods as soon as they reach 60 days. # This algorithm could likely be improved to get rid of this restriction and improve # efficiency, so code should not rely on this behaviour. current_period = None max_continuous_days = 60 range_start = self . to_timezone ( range_start ) range_end = self . to_timezone ( range_end ) for period in self . _daily_periods ( range_start . date ( ) , range_end . date ( ) ) : if period . end < range_start or period . start > range_end : continue if current_period is None : current_period = period else : if ( ( ( period . start < current_period . end ) or ( period . start - current_period . end ) <= datetime . timedelta ( minutes = 1 ) ) and ( current_period . end - current_period . start ) < datetime . timedelta ( days = max_continuous_days ) ) : # Merge current_period = Period ( current_period . start , period . end ) else : yield current_period current_period = period if current_period : yield current_period
7966	def end ( self , tag ) : self . _level -= 1 if self . _level < 0 : self . _handler . stream_parse_error ( u"Unexpected end tag for: {0!r}" . format ( tag ) ) return if self . _level == 0 : if tag != self . _root . tag : self . _handler . stream_parse_error ( u"Unexpected end tag for:" " {0!r} (stream end tag expected)" . format ( tag ) ) return self . _handler . stream_end ( ) return element = self . _builder . end ( tag ) if self . _level == 1 : self . _handler . stream_element ( element )
13177	def get_cache_key ( prefix , * args , * * kwargs ) : hash_args_kwargs = hash ( tuple ( kwargs . iteritems ( ) ) + args ) return '{}_{}' . format ( prefix , hash_args_kwargs )
7321	def convert_markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( "text/markdown" ) del message [ 'Content-Type' ] # Convert the text from markdown and then make the message multipart message = make_message_multipart ( message ) for payload_item in set ( message . get_payload ( ) ) : # Assume the plaintext item is formatted with markdown. # Add corresponding HTML version of the item as the last part of # the multipart message (as per RFC 2046) if payload_item [ 'Content-Type' ] . startswith ( 'text/plain' ) : original_text = payload_item . get_payload ( ) html_text = markdown . markdown ( original_text ) html_payload = future . backports . email . mime . text . MIMEText ( "<html><body>{}</body></html>" . format ( html_text ) , "html" , ) message . attach ( html_payload ) return message
13823	def end_timing ( self ) : if self . _callback != None : elapsed = time . clock ( ) * 1000 - self . _start self . _callback . end_timing ( self . _counter , elapsed )
7074	def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_range = ( 1.0 , 20.0 ) , iqr_stdev_range = ( 1.0 , 20.0 ) , ngridpoints = 32 , ngridworkers = None ) : # make the output directory where all the pkls from the variability # threshold runs will go outdir = os . path . join ( simbasedir , 'recvar-threshold-pkls' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) # get the info from the simbasedir with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # get the magbinmedians to use for the recovery processing magbinmedians = siminfo [ 'magrms' ] [ magcols [ 0 ] ] [ 'binned_sdssr_median' ] # generate the grids for stetson and inveta stetson_grid = np . linspace ( stetson_stdev_range [ 0 ] , stetson_stdev_range [ 1 ] , num = ngridpoints ) inveta_grid = np . linspace ( inveta_stdev_range [ 0 ] , inveta_stdev_range [ 1 ] , num = ngridpoints ) iqr_grid = np . linspace ( iqr_stdev_range [ 0 ] , iqr_stdev_range [ 1 ] , num = ngridpoints ) # generate the grid stet_inveta_iqr_grid = [ ] for stet in stetson_grid : for inveta in inveta_grid : for iqr in iqr_grid : grid_point = [ stet , inveta , iqr ] stet_inveta_iqr_grid . append ( grid_point ) # the output dict grid_results = { 'stetson_grid' : stetson_grid , 'inveta_grid' : inveta_grid , 'iqr_grid' : iqr_grid , 'stet_inveta_iqr_grid' : stet_inveta_iqr_grid , 'magbinmedians' : magbinmedians , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'simbasedir' : os . path . abspath ( simbasedir ) , 'recovery' : [ ] } # set up the pool pool = mp . Pool ( ngridworkers ) # run the grid search per magbinmedian for magbinmedian in magbinmedians : LOGINFO ( 'running stetson J-inveta grid-search ' 'for magbinmedian = %.3f...' % magbinmedian ) tasks = [ ( simbasedir , gp , magbinmedian ) for gp in stet_inveta_iqr_grid ] thisbin_results = pool . map ( magbin_varind_gridsearch_worker , tasks ) grid_results [ 'recovery' ] . append ( thisbin_results ) pool . close ( ) pool . join ( ) LOGINFO ( 'done.' ) with open ( os . path . join ( simbasedir , 'fakevar-recovery-per-magbin.pkl' ) , 'wb' ) as outfd : pickle . dump ( grid_results , outfd , pickle . HIGHEST_PROTOCOL ) return grid_results
10953	def set_model ( self , mdl ) : self . mdl = mdl self . mdl . check_inputs ( self . comps ) for c in self . comps : setattr ( self , '_comp_' + c . category , c )
12761	def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
1433	def custom ( cls , customgrouper ) : if customgrouper is None : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) if not isinstance ( customgrouper , ICustomGrouping ) and not isinstance ( customgrouper , str ) : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) serialized = default_serializer . serialize ( customgrouper ) return cls . custom_serialized ( serialized , is_java = False )
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
9942	def clear_dir ( self , path ) : dirs , files = self . storage . listdir ( path ) for f in files : fpath = os . path . join ( path , f ) if self . dry_run : self . log ( "Pretending to delete '%s'" % smart_text ( fpath ) , level = 1 ) else : self . log ( "Deleting '%s'" % smart_text ( fpath ) , level = 1 ) self . storage . delete ( fpath ) for d in dirs : self . clear_dir ( os . path . join ( path , d ) )
371	def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : # x = np.asarray(x).swapaxes(axis, 0) # x = x[::-1, ...] # x = x.swapaxes(0, axis) # return x results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : # x = np.asarray(x).swapaxes(axis, 0) # x = x[::-1, ...] # x = x.swapaxes(0, axis) # return x results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )
7358	def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self . allow_X_in_peptides check_lower = not self . allow_lowercase_in_peptides check_min_length = self . min_peptide_length is not None min_length = self . min_peptide_length check_max_length = self . max_peptide_length is not None max_length = self . max_peptide_length for p in peptides : if not p . isalpha ( ) : raise ValueError ( "Invalid characters in peptide '%s'" % p ) elif check_X and "X" in p : raise ValueError ( "Invalid character 'X' in peptide '%s'" % p ) elif check_lower and not p . isupper ( ) : raise ValueError ( "Invalid lowercase letters in peptide '%s'" % p ) elif check_min_length and len ( p ) < min_length : raise ValueError ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min_length ) ) elif check_max_length and len ( p ) > max_length : raise ValueError ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max_length ) )
6909	def xieta_from_radecl ( inra , indecl , incenterra , incenterdecl , deg = True ) : if deg : ra = np . radians ( inra ) decl = np . radians ( indecl ) centerra = np . radians ( incenterra ) centerdecl = np . radians ( incenterdecl ) else : ra = inra decl = indecl centerra = incenterra centerdecl = incenterdecl cdecc = np . cos ( centerdecl ) sdecc = np . sin ( centerdecl ) crac = np . cos ( centerra ) srac = np . sin ( centerra ) uu = np . cos ( decl ) * np . cos ( ra ) vv = np . cos ( decl ) * np . sin ( ra ) ww = np . sin ( decl ) uun = uu * cdecc * crac + vv * cdecc * srac + ww * sdecc vvn = - uu * srac + vv * crac wwn = - uu * sdecc * crac - vv * sdecc * srac + ww * cdecc denom = vvn * vvn + wwn * wwn aunn = np . zeros_like ( uun ) aunn [ uun >= 1.0 ] = 0.0 aunn [ uun < 1.0 ] = np . arccos ( uun ) xi , eta = np . zeros_like ( aunn ) , np . zeros_like ( aunn ) xi [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 eta [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 sdenom = np . sqrt ( denom ) xi [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * vvn / sdenom eta [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * wwn / sdenom if deg : return np . degrees ( xi ) , np . degrees ( eta ) else : return xi , eta
3646	def sendToWatchlist ( self , trade_id ) : method = 'PUT' url = 'watchlist' data = { 'auctionInfo' : [ { 'id' : trade_id } ] } return self . __request__ ( method , url , data = json . dumps ( data ) )
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train # build trainer params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) # update monitoring dataset(s) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) # run main loop self . trainer . main_loop ( )
10095	def create_template ( self , name , subject , html , text = '' , timeout = None ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } return self . _api_request ( self . TEMPLATES_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
9595	def execute_async_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_ASYNC_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) # Windows and OS X if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
3434	def _populate_solver ( self , reaction_list , metabolite_list = None ) : constraint_terms = AutoVivification ( ) to_add = [ ] if metabolite_list is not None : for met in metabolite_list : to_add += [ self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) ] self . add_cons_vars ( to_add ) for reaction in reaction_list : if reaction . id not in self . variables : forward_variable = self . problem . Variable ( reaction . id ) reverse_variable = self . problem . Variable ( reaction . reverse_id ) self . add_cons_vars ( [ forward_variable , reverse_variable ] ) else : reaction = self . reactions . get_by_id ( reaction . id ) forward_variable = reaction . forward_variable reverse_variable = reaction . reverse_variable for metabolite , coeff in six . iteritems ( reaction . metabolites ) : if metabolite . id in self . constraints : constraint = self . constraints [ metabolite . id ] else : constraint = self . problem . Constraint ( Zero , name = metabolite . id , lb = 0 , ub = 0 ) self . add_cons_vars ( constraint , sloppy = True ) constraint_terms [ constraint ] [ forward_variable ] = coeff constraint_terms [ constraint ] [ reverse_variable ] = - coeff self . solver . update ( ) for reaction in reaction_list : reaction = self . reactions . get_by_id ( reaction . id ) reaction . update_variable_bounds ( ) for constraint , terms in six . iteritems ( constraint_terms ) : constraint . set_linear_coefficients ( terms )
11683	def _readblock ( self ) : block = '' while not self . _stop : line = self . _readline ( ) if line == '.' : break block += line return block
10840	def publish ( self ) : url = PATHS [ 'PUBLISH' ] % self . id return self . api . post ( url = url )
3642	def sell ( self , item_id , bid , buy_now , duration = 3600 , fast = False ) : method = 'POST' url = 'auctionhouse' # TODO: auto send to tradepile data = { 'buyNowPrice' : buy_now , 'startingBid' : bid , 'duration' : duration , 'itemData' : { 'id' : item_id } } rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } ) if not fast : # tradeStatus check like webapp do self . tradeStatus ( rc [ 'id' ] ) return rc [ 'id' ]
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) # Coefficients estimated via the covariance method # Here we use lstsq rathre than solve function because Xc is not square matrix a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) # Estimate the input white noise variance Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) # ignore imag part that should be small return a , e
1284	def footnote_ref ( self , key , index ) : html = ( '<sup class="footnote-ref" id="fnref-%s">' '<a href="#fn-%s">%d</a></sup>' ) % ( escape ( key ) , escape ( key ) , index ) return html
13286	def get_dataframe_from_variable ( nc , data_var ) : time_var = nc . get_variables_by_attributes ( standard_name = 'time' ) [ 0 ] depth_vars = nc . get_variables_by_attributes ( axis = lambda v : v is not None and v . lower ( ) == 'z' ) depth_vars += nc . get_variables_by_attributes ( standard_name = lambda v : v in [ 'height' , 'depth' 'surface_altitude' ] , positive = lambda x : x is not None ) # Find the correct depth variable depth_var = None for d in depth_vars : try : if d . _name in data_var . coordinates . split ( " " ) or d . _name in data_var . dimensions : depth_var = d break except AttributeError : continue times = netCDF4 . num2date ( time_var [ : ] , units = time_var . units , calendar = getattr ( time_var , 'calendar' , 'standard' ) ) original_times_size = times . size if depth_var is None and hasattr ( data_var , 'sensor_depth' ) : depth_type = get_type ( data_var . sensor_depth ) depths = np . asarray ( [ data_var . sensor_depth ] * len ( times ) ) . flatten ( ) values = data_var [ : ] . flatten ( ) elif depth_var is None : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) depth_type = get_type ( depths ) values = data_var [ : ] . flatten ( ) else : depths = depth_var [ : ] depth_type = get_type ( depths ) if len ( data_var . shape ) > 1 : times = np . repeat ( times , depths . size ) depths = np . tile ( depths , original_times_size ) values = data_var [ : , : ] . flatten ( ) else : values = data_var [ : ] . flatten ( ) if getattr ( depth_var , 'positive' , 'down' ) . lower ( ) == 'up' : logger . warning ( "Converting depths to positive down before returning the DataFrame" ) depths = depths * - 1 # https://github.com/numpy/numpy/issues/4595 # We can't call astype on a MaskedConstant if ( isinstance ( depths , np . ma . core . MaskedConstant ) or ( hasattr ( depths , 'mask' ) and depths . mask . all ( ) ) ) : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) df = pd . DataFrame ( { 'time' : times , 'value' : values . astype ( data_var . dtype ) , 'unit' : data_var . units if hasattr ( data_var , 'units' ) else np . nan , 'depth' : depths . astype ( depth_type ) } ) df . set_index ( [ pd . DatetimeIndex ( df [ 'time' ] ) , pd . Float64Index ( df [ 'depth' ] ) ] , inplace = True ) return df
3442	def to_json ( model , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , * * kwargs )
10439	def stopprocessmonitor ( self , process_name ) : if process_name in self . _process_stats : # Stop monitoring process self . _process_stats [ process_name ] . stop ( ) return 1
6425	def tanimoto_coeff ( self , src , tar , qval = 2 ) : coeff = self . sim ( src , tar , qval ) if coeff != 0 : return log ( coeff , 2 ) return float ( '-inf' )
854	def appendRecords ( self , records , progressCB = None ) : for record in records : self . appendRecord ( record ) if progressCB is not None : progressCB ( )
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
3576	def initialize ( self ) : # Setup the central manager and its delegate. self . _central_manager = CBCentralManager . alloc ( ) self . _central_manager . initWithDelegate_queue_options_ ( self . _central_delegate , None , None )
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : # pylint: disable=R0913 item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
3	def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )
12098	def show ( self , args , file_handle = None , * * kwargs ) : full_string = '' info = { 'root_directory' : '<root_directory>' , 'batch_name' : '<batch_name>' , 'batch_tag' : '<batch_tag>' , 'batch_description' : '<batch_description>' , 'launcher' : '<launcher>' , 'timestamp_format' : '<timestamp_format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying_keys' : args . varying_keys , 'constant_keys' : args . constant_keys , 'constant_items' : args . constant_items } quoted_cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . _formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd_lines = [ '%d: %s\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted_cmds ) ] full_string += '' . join ( cmd_lines ) if file_handle : file_handle . write ( full_string ) file_handle . flush ( ) else : print ( full_string )
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
2990	def cross_origin ( * args , * * kwargs ) : _options = kwargs def decorator ( f ) : LOG . debug ( "Enabling %s for cross_origin using options:%s" , f , _options ) # If True, intercept OPTIONS requests by modifying the view function, # replicating Flask's default behavior, and wrapping the response with # CORS headers. # # If f.provide_automatic_options is unset or True, Flask's route # decorator (which is actually wraps the function object we return) # intercepts OPTIONS handling, and requests will not have CORS headers if _options . get ( 'automatic_options' , True ) : f . required_methods = getattr ( f , 'required_methods' , set ( ) ) f . required_methods . add ( 'OPTIONS' ) f . provide_automatic_options = False def wrapped_function ( * args , * * kwargs ) : # Handle setting of Flask-Cors parameters options = get_cors_options ( current_app , _options ) if options . get ( 'automatic_options' ) and request . method == 'OPTIONS' : resp = current_app . make_default_options_response ( ) else : resp = make_response ( f ( * args , * * kwargs ) ) set_cors_headers ( resp , options ) setattr ( resp , FLASK_CORS_EVALUATED , True ) return resp return update_wrapper ( wrapped_function , f ) return decorator
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
13438	def _extendrange ( self , start , end ) : range_positions = [ ] for i in range ( start , end ) : if i != 0 : range_positions . append ( str ( i ) ) if i < end : range_positions . append ( self . separator ) return range_positions
9480	def _arg_parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . ArgumentParser ( description = description ) verbose_group = parser . add_mutually_exclusive_group ( ) verbose_group . add_argument ( '-v' , '--verbose' , action = 'store_true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose_group . add_argument ( '-q' , '--quiet' , action = 'store_false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add_argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
10125	def flip_y ( self , center = None ) : if center is None : self . poly . flop ( ) else : self . poly . flop ( center [ 1 ] ) return self
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
11511	def set_item_metadata ( self , token , item_id , element , value , qualifier = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id parameters [ 'element' ] = element parameters [ 'value' ] = value if qualifier : parameters [ 'qualifier' ] = qualifier response = self . request ( 'midas.item.setmetadata' , parameters ) return response
3754	def Skin ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : _Skin = ( _OntarioExposureLimits [ CASRN ] [ "Skin" ] ) elif Method == NONE : _Skin = None else : raise Exception ( 'Failure in in function' ) return _Skin
13046	def f_hierarchical_passages ( reffs , citation ) : d = OrderedDict ( ) levels = [ x for x in citation ] for cit , name in reffs : ref = cit . split ( '-' ) [ 0 ] levs = [ '%{}|{}%' . format ( levels [ i ] . name , v ) for i , v in enumerate ( ref . split ( '.' ) ) ] getFromDict ( d , levs [ : - 1 ] ) [ name ] = cit return d
11685	def cli ( id ) : ch = Analyse ( id ) ch . full_analysis ( ) click . echo ( 'Created: %s. Modified: %s. Deleted: %s' % ( ch . create , ch . modify , ch . delete ) ) if ch . is_suspect : click . echo ( 'The changeset {} is suspect! Reasons: {}' . format ( id , ', ' . join ( ch . suspicion_reasons ) ) ) else : click . echo ( 'The changeset %s is not suspect!' % id )
4464	def load_jam_audio ( jam_in , audio_file , validate = True , strict = True , fmt = 'auto' , * * kwargs ) : if isinstance ( jam_in , jams . JAMS ) : jam = jam_in else : jam = jams . load ( jam_in , validate = validate , strict = strict , fmt = fmt ) y , sr = librosa . load ( audio_file , * * kwargs ) if jam . file_metadata . duration is None : jam . file_metadata . duration = librosa . get_duration ( y = y , sr = sr ) return jam_pack ( jam , _audio = dict ( y = y , sr = sr ) )
11298	def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
6880	def _parse_csv_header ( header ) : # first, break into lines headerlines = header . split ( '\n' ) headerlines = [ x . lstrip ( '# ' ) for x in headerlines ] # next, find the indices of the metadata sections objectstart = headerlines . index ( 'OBJECT' ) metadatastart = headerlines . index ( 'METADATA' ) camfilterstart = headerlines . index ( 'CAMFILTERS' ) photaperturestart = headerlines . index ( 'PHOTAPERTURES' ) columnstart = headerlines . index ( 'COLUMNS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) # get the lines for the header sections objectinfo = headerlines [ objectstart + 1 : metadatastart - 1 ] metadatainfo = headerlines [ metadatastart + 1 : camfilterstart - 1 ] camfilterinfo = headerlines [ camfilterstart + 1 : photaperturestart - 1 ] photapertureinfo = headerlines [ photaperturestart + 1 : columnstart - 1 ] columninfo = headerlines [ columnstart + 1 : lcstart - 1 ] # parse the header sections and insert the appropriate key-val pairs into # the lcdict metadict = { 'objectinfo' : { } } # first, the objectinfo section objectinfo = [ x . split ( ';' ) for x in objectinfo ] for elem in objectinfo : for kvelem in elem : key , val = kvelem . split ( ' = ' , 1 ) metadict [ 'objectinfo' ] [ key . strip ( ) ] = ( _smartcast ( val , METAKEYS [ key . strip ( ) ] ) ) # the objectid belongs at the top level metadict [ 'objectid' ] = metadict [ 'objectinfo' ] [ 'objectid' ] [ : ] del metadict [ 'objectinfo' ] [ 'objectid' ] # get the lightcurve metadata metadatainfo = [ x . split ( ';' ) for x in metadatainfo ] for elem in metadatainfo : for kvelem in elem : try : key , val = kvelem . split ( ' = ' , 1 ) # get the lcbestaperture into a dict again if key . strip ( ) == 'lcbestaperture' : val = json . loads ( val ) # get the lcversion and datarelease as integers if key . strip ( ) in ( 'datarelease' , 'lcversion' ) : val = int ( val ) # get the lastupdated as a float if key . strip ( ) == 'lastupdated' : val = float ( val ) # put the key-val into the dict metadict [ key . strip ( ) ] = val except Exception as e : LOGWARNING ( 'could not understand header element "%s",' ' skipped.' % kvelem ) # get the camera filters metadict [ 'filters' ] = [ ] for row in camfilterinfo : filterid , filtername , filterdesc = row . split ( ' - ' ) metadict [ 'filters' ] . append ( ( int ( filterid ) , filtername , filterdesc ) ) # get the photometric apertures metadict [ 'lcapertures' ] = { } for row in photapertureinfo : apnum , appix = row . split ( ' - ' ) appix = float ( appix . rstrip ( ' px' ) ) metadict [ 'lcapertures' ] [ apnum . strip ( ) ] = appix # get the columns metadict [ 'columns' ] = [ ] for row in columninfo : colnum , colname , coldesc = row . split ( ' - ' ) metadict [ 'columns' ] . append ( colname ) return metadict
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
1102	def restore ( delta , which ) : try : tag = { 1 : "- " , 2 : "+ " } [ int ( which ) ] except KeyError : raise ValueError , ( 'unknown delta choice (must be 1 or 2): %r' % which ) prefixes = ( " " , tag ) for line in delta : if line [ : 2 ] in prefixes : yield line [ 2 : ]
11934	def auto_widget ( field ) : # Auto-detect info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( * * info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
13628	def parse ( expected , query ) : return dict ( ( key , parser ( query . get ( key , [ ] ) ) ) for key , parser in expected . items ( ) )
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : """Pass only for nodes that have the enclosed function and enclosed namespace.""" if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : """Pass only for nodes that have the enclosed function and namespace in the enclose set.""" if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
3367	def linear_reaction_coefficients ( model , reactions = None ) : linear_coefficients = { } reactions = model . reactions if not reactions else reactions try : objective_expression = model . solver . objective . expression coefficients = objective_expression . as_coefficients_dict ( ) except AttributeError : return linear_coefficients for rxn in reactions : forward_coefficient = coefficients . get ( rxn . forward_variable , 0 ) reverse_coefficient = coefficients . get ( rxn . reverse_variable , 0 ) if forward_coefficient != 0 : if forward_coefficient == - reverse_coefficient : linear_coefficients [ rxn ] = float ( forward_coefficient ) return linear_coefficients
6887	def parallel_epd_lclist ( lclist , externalparams , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # override the default timecols, magcols, and errcols # using the ones provided to the function if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols outdict = { } # run by magcol for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , externalparams , lcformat , lcformatdir , epdsmooth_sigclip , epdsmooth_windowsize , epdsmooth_func , epdsmooth_extraparams ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_epd_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
889	def _leastUsedCell ( cls , random , cells , connections ) : leastUsedCells = [ ] minNumSegments = float ( "inf" ) for cell in cells : numSegments = connections . numSegments ( cell ) if numSegments < minNumSegments : minNumSegments = numSegments leastUsedCells = [ ] if numSegments == minNumSegments : leastUsedCells . append ( cell ) i = random . getUInt32 ( len ( leastUsedCells ) ) return leastUsedCells [ i ]
12419	def capture_stderr ( ) : stderr = sys . stderr try : capture_out = StringIO ( ) sys . stderr = capture_out yield capture_out finally : sys . stderr = stderr
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
13175	def prev ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index - 1 , - 1 , - 1 ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
107	def max_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . max , cval = cval , preserve_dtype = preserve_dtype )
11358	def fix_journal_name ( journal , knowledge_base ) : if not journal : return '' , '' if not knowledge_base : return journal , '' if len ( journal ) < 2 : return journal , '' volume = '' if ( journal [ - 1 ] <= 'Z' and journal [ - 1 ] >= 'A' ) and ( journal [ - 2 ] == '.' or journal [ - 2 ] == ' ' ) : volume += journal [ - 1 ] journal = journal [ : - 1 ] journal = journal . strip ( ) if journal . upper ( ) in knowledge_base : journal = knowledge_base [ journal . upper ( ) ] . strip ( ) elif journal in knowledge_base : journal = knowledge_base [ journal ] . strip ( ) elif '.' in journal : journalnodots = journal . replace ( '. ' , ' ' ) journalnodots = journalnodots . replace ( '.' , ' ' ) . strip ( ) . upper ( ) if journalnodots in knowledge_base : journal = knowledge_base [ journalnodots ] . strip ( ) journal = journal . replace ( '. ' , '.' ) return journal , volume
2233	def _register_builtin_class_extensions ( self ) : @ self . register ( uuid . UUID ) def _hash_uuid ( data ) : hashable = data . bytes prefix = b'UUID' return prefix , hashable @ self . register ( OrderedDict ) def _hash_ordered_dict ( data ) : """ Note, we should not be hashing dicts because they are unordered """ hashable = b'' . join ( _hashable_sequence ( list ( data . items ( ) ) ) ) prefix = b'ODICT' return prefix , hashable
1331	def batch_predictions ( self , images , greedy = False , strict = True , return_details = False ) : if strict : in_bounds = self . in_bounds ( images ) assert in_bounds self . _total_prediction_calls += len ( images ) predictions = self . __model . batch_predictions ( images ) assert predictions . ndim == 2 assert predictions . shape [ 0 ] == images . shape [ 0 ] if return_details : assert greedy adversarials = [ ] for i in range ( len ( predictions ) ) : if strict : in_bounds_i = True else : in_bounds_i = self . in_bounds ( images [ i ] ) is_adversarial , is_best , distance = self . __is_adversarial ( images [ i ] , predictions [ i ] , in_bounds_i ) if is_adversarial and greedy : if return_details : return predictions , is_adversarial , i , is_best , distance else : return predictions , is_adversarial , i adversarials . append ( is_adversarial ) if greedy : # pragma: no cover # no adversarial found if return_details : return predictions , False , None , False , None else : return predictions , False , None is_adversarial = np . array ( adversarials ) assert is_adversarial . ndim == 1 assert is_adversarial . shape [ 0 ] == images . shape [ 0 ] return predictions , is_adversarial
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
9335	def copy ( a ) : shared = anonymousmemmap ( a . shape , dtype = a . dtype ) shared [ : ] = a [ : ] return shared
3040	def from_json ( cls , json_data ) : data = json . loads ( _helpers . _from_bytes ( json_data ) ) if ( data . get ( 'token_expiry' ) and not isinstance ( data [ 'token_expiry' ] , datetime . datetime ) ) : try : data [ 'token_expiry' ] = datetime . datetime . strptime ( data [ 'token_expiry' ] , EXPIRY_FORMAT ) except ValueError : data [ 'token_expiry' ] = None retval = cls ( data [ 'access_token' ] , data [ 'client_id' ] , data [ 'client_secret' ] , data [ 'refresh_token' ] , data [ 'token_expiry' ] , data [ 'token_uri' ] , data [ 'user_agent' ] , revoke_uri = data . get ( 'revoke_uri' , None ) , id_token = data . get ( 'id_token' , None ) , id_token_jwt = data . get ( 'id_token_jwt' , None ) , token_response = data . get ( 'token_response' , None ) , scopes = data . get ( 'scopes' , None ) , token_info_uri = data . get ( 'token_info_uri' , None ) ) retval . invalid = data [ 'invalid' ] return retval
9592	def set_window_position ( self , x , y , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_POSITION , { 'x' : int ( x ) , 'y' : int ( y ) , 'window_handle' : window_handle } )
13884	def ListFiles ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) # Handle local if _UrlIsLocal ( directory_url ) : if not os . path . isdir ( directory ) : return None return os . listdir ( directory ) # Handle FTP elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
3945	def _decode_field ( message , field , value ) : if field . type == FieldDescriptor . TYPE_MESSAGE : decode ( getattr ( message , field . name ) , value ) else : try : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) setattr ( message , field . name , value ) except ( ValueError , TypeError ) as e : # ValueError: invalid enum value, negative unsigned int value, or # invalid base64 # TypeError: mismatched type logger . warning ( 'Message %r ignoring field %s: %s' , message . __class__ . __name__ , field . name , e )
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
1844	def JO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . OF , target . read ( ) , cpu . PC )
9230	def fetch_repo_creation_date ( self ) : gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . get ( ) if rc == 200 : return REPO_CREATED_TAG_NAME , data [ "created_at" ] else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) return None , None
4211	def compatible_staticpath ( path ) : if VERSION >= ( 1 , 10 ) : # Since Django 1.10, forms.Media automatically invoke static # lazily on the path if it is relative. return path try : # >= 1.4 from django . templatetags . static import static return static ( path ) except ImportError : pass try : # >= 1.3 return '%s/%s' % ( settings . STATIC_URL . rstrip ( '/' ) , path ) except AttributeError : pass try : return '%s/%s' % ( settings . PAGEDOWN_URL . rstrip ( '/' ) , path ) except AttributeError : pass return '%s/%s' % ( settings . MEDIA_URL . rstrip ( '/' ) , path )
5668	def stop_to_stop_network_for_route_type ( gtfs , route_type , link_attributes = None , start_time_ut = None , end_time_ut = None ) : if link_attributes is None : link_attributes = DEFAULT_STOP_TO_STOP_LINK_ATTRIBUTES assert ( route_type in route_types . TRANSIT_ROUTE_TYPES ) stops_dataframe = gtfs . get_stops_for_route_type ( route_type ) net = networkx . DiGraph ( ) _add_stops_to_net ( net , stops_dataframe ) events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) if len ( net . nodes ( ) ) < 2 : assert events_df . shape [ 0 ] == 0 # group events by links, and loop over them (i.e. each link): link_event_groups = events_df . groupby ( [ 'from_stop_I' , 'to_stop_I' ] , sort = False ) for key , link_events in link_event_groups : from_stop_I , to_stop_I = key assert isinstance ( link_events , pd . DataFrame ) # 'dep_time_ut' 'arr_time_ut' 'shape_id' 'route_type' 'trip_I' 'duration' 'from_seq' 'to_seq' if link_attributes is None : net . add_edge ( from_stop_I , to_stop_I ) else : link_data = { } if "duration_min" in link_attributes : link_data [ 'duration_min' ] = float ( link_events [ 'duration' ] . min ( ) ) if "duration_max" in link_attributes : link_data [ 'duration_max' ] = float ( link_events [ 'duration' ] . max ( ) ) if "duration_median" in link_attributes : link_data [ 'duration_median' ] = float ( link_events [ 'duration' ] . median ( ) ) if "duration_avg" in link_attributes : link_data [ 'duration_avg' ] = float ( link_events [ 'duration' ] . mean ( ) ) # statistics on numbers of vehicles: if "n_vehicles" in link_attributes : link_data [ 'n_vehicles' ] = int ( link_events . shape [ 0 ] ) if "capacity_estimate" in link_attributes : link_data [ 'capacity_estimate' ] = route_types . ROUTE_TYPE_TO_APPROXIMATE_CAPACITY [ route_type ] * int ( link_events . shape [ 0 ] ) if "d" in link_attributes : from_lat = net . node [ from_stop_I ] [ 'lat' ] from_lon = net . node [ from_stop_I ] [ 'lon' ] to_lat = net . node [ to_stop_I ] [ 'lat' ] to_lon = net . node [ to_stop_I ] [ 'lon' ] distance = wgs84_distance ( from_lat , from_lon , to_lat , to_lon ) link_data [ 'd' ] = int ( distance ) if "distance_shape" in link_attributes : assert "shape_id" in link_events . columns . values found = None for i , shape_id in enumerate ( link_events [ "shape_id" ] . values ) : if shape_id is not None : found = i break if found is None : link_data [ "distance_shape" ] = None else : link_event = link_events . iloc [ found ] distance = gtfs . get_shape_distance_between_stops ( link_event [ "trip_I" ] , int ( link_event [ "from_seq" ] ) , int ( link_event [ "to_seq" ] ) ) link_data [ 'distance_shape' ] = distance if "route_I_counts" in link_attributes : link_data [ "route_I_counts" ] = link_events . groupby ( "route_I" ) . size ( ) . to_dict ( ) net . add_edge ( from_stop_I , to_stop_I , attr_dict = link_data ) return net
10882	def aN ( a , dim = 3 , dtype = 'int' ) : if not hasattr ( a , '__iter__' ) : return np . array ( [ a ] * dim , dtype = dtype ) return np . array ( a ) . astype ( dtype )
5780	def _obtain_credentials ( self ) : protocol_values = { 'SSLv3' : Secur32Const . SP_PROT_SSL3_CLIENT , 'TLSv1' : Secur32Const . SP_PROT_TLS1_CLIENT , 'TLSv1.1' : Secur32Const . SP_PROT_TLS1_1_CLIENT , 'TLSv1.2' : Secur32Const . SP_PROT_TLS1_2_CLIENT , } protocol_bit_mask = 0 for key , value in protocol_values . items ( ) : if key in self . _protocols : protocol_bit_mask |= value algs = [ Secur32Const . CALG_AES_128 , Secur32Const . CALG_AES_256 , Secur32Const . CALG_3DES , Secur32Const . CALG_SHA1 , Secur32Const . CALG_ECDHE , Secur32Const . CALG_DH_EPHEM , Secur32Const . CALG_RSA_KEYX , Secur32Const . CALG_RSA_SIGN , Secur32Const . CALG_ECDSA , Secur32Const . CALG_DSS_SIGN , ] if 'TLSv1.2' in self . _protocols : algs . extend ( [ Secur32Const . CALG_SHA512 , Secur32Const . CALG_SHA384 , Secur32Const . CALG_SHA256 , ] ) alg_array = new ( secur32 , 'ALG_ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg_array [ index ] = alg flags = Secur32Const . SCH_USE_STRONG_CRYPTO | Secur32Const . SCH_CRED_NO_DEFAULT_CREDS if not self . _manual_validation and not self . _extra_trust_roots : flags |= Secur32Const . SCH_CRED_AUTO_CRED_VALIDATION else : flags |= Secur32Const . SCH_CRED_MANUAL_CRED_VALIDATION schannel_cred_pointer = struct ( secur32 , 'SCHANNEL_CRED' ) schannel_cred = unwrap ( schannel_cred_pointer ) schannel_cred . dwVersion = Secur32Const . SCHANNEL_CRED_VERSION schannel_cred . cCreds = 0 schannel_cred . paCred = null ( ) schannel_cred . hRootStore = null ( ) schannel_cred . cMappers = 0 schannel_cred . aphMappers = null ( ) schannel_cred . cSupportedAlgs = len ( alg_array ) schannel_cred . palgSupportedAlgs = alg_array schannel_cred . grbitEnabledProtocols = protocol_bit_mask schannel_cred . dwMinimumCipherStrength = 0 schannel_cred . dwMaximumCipherStrength = 0 # Default session lifetime is 10 hours schannel_cred . dwSessionLifespan = 0 schannel_cred . dwFlags = flags schannel_cred . dwCredFormat = 0 cred_handle_pointer = new ( secur32 , 'CredHandle *' ) result = secur32 . AcquireCredentialsHandleW ( null ( ) , Secur32Const . UNISP_NAME , Secur32Const . SECPKG_CRED_OUTBOUND , null ( ) , schannel_cred_pointer , null ( ) , null ( ) , cred_handle_pointer , null ( ) ) handle_error ( result ) self . _credentials_handle = cred_handle_pointer
7270	def use ( plugin ) : log . debug ( 'register new plugin: {}' . format ( plugin ) ) if inspect . isfunction ( plugin ) : return plugin ( Engine ) if plugin and hasattr ( plugin , 'register' ) : return plugin . register ( Engine ) raise ValueError ( 'invalid plugin: must be a function or ' 'implement register() method' )
6760	def has_changes ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] if tracker . is_changed ( last_thumbprint ) : return True return False
6214	def load_gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTFMeta ( self . path , json . load ( fd ) )
3329	def acquire_read ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : # If we are the writer, grant a new read lock, always. self . __writercount += 1 return while True : if self . __writer is None : # Only test anything if there is no current writer. if self . __upgradewritercount or self . __pendingwriters : if me in self . __readers : # Only grant a read lock if we already have one # in case writers are waiting for their turn. # This means that writers can't easily get starved # (but see below, readers can). self . __readers [ me ] += 1 return # No, we aren't a reader (yet), wait for our turn. else : # Grant a new read lock, always, in case there are # no pending writers (and no writer). self . __readers [ me ] = self . __readers . get ( me , 0 ) + 1 return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : # Timeout has expired, signal caller of this. raise RuntimeError ( "Acquiring read lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
5210	def bdp ( tickers , flds , * * kwargs ) : logger = logs . get_logger ( bdp , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( * * kwargs ) logger . info ( f'loading reference data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . ref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for r , snap in data . iterrows ( ) : subset = [ r ] data_file = storage . ref_file ( ticker = snap . ticker , fld = snap . field , ext = 'pkl' , * * kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( data . iloc [ subset ] ) files . create_folder ( data_file , is_file = True ) data . iloc [ subset ] . to_pickle ( data_file ) return qry_data
2876	def add_bpmn_xml ( self , bpmn , svg = None , filename = None ) : xpath = xpath_eval ( bpmn ) processes = xpath ( './/bpmn:process' ) for process in processes : process_parser = self . PROCESS_PARSER_CLASS ( self , process , svg , filename = filename , doc_xpath = xpath ) if process_parser . get_id ( ) in self . process_parsers : raise ValidationException ( 'Duplicate process ID' , node = process , filename = filename ) if process_parser . get_name ( ) in self . process_parsers_by_name : raise ValidationException ( 'Duplicate process name' , node = process , filename = filename ) self . process_parsers [ process_parser . get_id ( ) ] = process_parser self . process_parsers_by_name [ process_parser . get_name ( ) ] = process_parser
2427	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True if validations . validate_doc_comment ( comment ) : doc . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'Document::Comment' ) else : raise CardinalityError ( 'Document::Comment' )
7523	def reftrick ( iseq , consdict ) : altrefs = np . zeros ( ( iseq . shape [ 1 ] , 4 ) , dtype = np . uint8 ) altrefs [ : , 1 ] = 46 for col in xrange ( iseq . shape [ 1 ] ) : ## expand colums with ambigs and remove N- fcounts = np . zeros ( 111 , dtype = np . int64 ) counts = np . bincount ( iseq [ : , col ] ) #, minlength=90) fcounts [ : counts . shape [ 0 ] ] = counts ## set N and - to zero, wish numba supported minlen arg fcounts [ 78 ] = 0 fcounts [ 45 ] = 0 ## add ambig counts to true bases for aidx in xrange ( consdict . shape [ 0 ] ) : nbases = fcounts [ consdict [ aidx , 0 ] ] for _ in xrange ( nbases ) : fcounts [ consdict [ aidx , 1 ] ] += 1 fcounts [ consdict [ aidx , 2 ] ] += 1 fcounts [ consdict [ aidx , 0 ] ] = 0 ## now get counts from the modified counts arr who = np . argmax ( fcounts ) altrefs [ col , 0 ] = who fcounts [ who ] = 0 ## if an alt allele fill over the "." placeholder who = np . argmax ( fcounts ) if who : altrefs [ col , 1 ] = who fcounts [ who ] = 0 ## if 3rd or 4th alleles observed then add to arr who = np . argmax ( fcounts ) altrefs [ col , 2 ] = who fcounts [ who ] = 0 ## if 3rd or 4th alleles observed then add to arr who = np . argmax ( fcounts ) altrefs [ col , 3 ] = who return altrefs
456	def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { "Sign" : "Identity" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
4400	def _generate_ranges ( start_date , end_date ) : range_start = start_date while range_start < end_date : range_end = range_start + timedelta ( days = 60 ) yield ( range_start . strftime ( "%d/%m/%Y" ) , range_end . strftime ( "%d/%m/%Y" ) ) range_start += timedelta ( days = 30 )
13775	def includeme ( config ) : settings = config . get_settings ( ) should_create = asbool ( settings . get ( 'baka_model.should_create_all' , False ) ) should_drop = asbool ( settings . get ( 'baka_model.should_drop_all' , False ) ) # Configure the transaction manager to support retrying retryable # exceptions. We also register the session factory with the thread-local # transaction manager, so that all sessions it creates are registered. # "tm.attempts": 3, config . add_settings ( { "retry.attempts" : 3 , "tm.activate_hook" : tm_activate_hook , "tm.annotate_user" : False , } ) # use pyramid_retry couse pyramid_tm disabled it config . include ( 'pyramid_retry' ) # use pyramid_tm to hook the transaction lifecycle to the request config . include ( 'pyramid_tm' ) engine = get_engine ( settings ) session_factory = get_session_factory ( engine ) config . registry [ 'db_session_factory' ] = session_factory # make request.db available for use in Pyramid config . add_request_method ( # r.tm is the transaction manager used by pyramid_tm lambda r : get_tm_session ( session_factory , r . tm ) , 'db' , reify = True ) # service model factory config . include ( '.service' ) # Register a deferred action to bind the engine when the configuration is # committed. Deferring the action means that this module can be included # before model modules without ill effect. config . action ( None , bind_engine , ( engine , ) , { 'should_create' : should_create , 'should_drop' : should_drop } , order = 10 )
6058	def bin_up_array_2d_using_mean ( array_2d , bin_up_factor ) : padded_array_2d = pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = array_2d , bin_up_factor = bin_up_factor ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = 0.0 for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 value += padded_array_2d [ padded_y , padded_x ] binned_array_2d [ y , x ] = value / ( bin_up_factor ** 2.0 ) return binned_array_2d
7099	def child_added ( self , child ) : if child . widget : # TODO: Should we keep count and remove the adapter if not all # markers request it? self . parent ( ) . init_info_window_adapter ( ) super ( AndroidMapMarker , self ) . child_added ( child )
2053	def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . _compute_writeback ( dest , offset ) cpu . write_int ( dest . address ( ) , val1 , 32 ) cpu . write_int ( dest . address ( ) + 4 , val2 , 32 ) cpu . _cs_hack_ldr_str_writeback ( dest , offset , writeback )
5608	def bounds_to_ranges ( out_bounds = None , in_affine = None , in_shape = None ) : return itertools . chain ( * from_bounds ( * out_bounds , transform = in_affine , height = in_shape [ - 2 ] , width = in_shape [ - 1 ] ) . round_lengths ( pixel_precision = 0 ) . round_offsets ( pixel_precision = 0 ) . toranges ( ) )
5414	def get_provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . GoogleJobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'google-v2' : return google_v2 . GoogleV2JobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'local' : return local . LocalJobProvider ( resources ) elif provider == 'test-fails' : return test_fails . FailsJobProvider ( ) else : raise ValueError ( 'Unknown provider: ' + provider )
10845	def shuffle ( self , count = None , utc = None ) : url = PATHS [ 'SHUFFLE' ] % self . profile_id post_data = '' if count : post_data += 'count=%s&' % count if utc : post_data += 'utc=%s' % utc return self . api . post ( url = url , data = post_data )
10867	def rmatrix ( self ) : t = self . param_dict [ self . lbl_theta ] r0 = np . array ( [ [ np . cos ( t ) , - np . sin ( t ) , 0 ] , [ np . sin ( t ) , np . cos ( t ) , 0 ] , [ 0 , 0 , 1 ] ] ) p = self . param_dict [ self . lbl_phi ] r1 = np . array ( [ [ np . cos ( p ) , 0 , np . sin ( p ) ] , [ 0 , 1 , 0 ] , [ - np . sin ( p ) , 0 , np . cos ( p ) ] ] ) return np . dot ( r1 , r0 )
13072	def r_passage ( self , objectId , subreference , lang = None ) : collection = self . get_collection ( objectId ) if isinstance ( collection , CtsWorkMetadata ) : editions = [ t for t in collection . children . values ( ) if isinstance ( t , CtsEditionMetadata ) ] if len ( editions ) == 0 : raise UnknownCollection ( "This work has no default edition" ) return redirect ( url_for ( ".r_passage" , objectId = str ( editions [ 0 ] . id ) , subreference = subreference ) ) text = self . get_passage ( objectId = objectId , subreference = subreference ) passage = self . transform ( text , text . export ( Mimetypes . PYTHON . ETREE ) , objectId ) prev , next = self . get_siblings ( objectId , subreference , text ) return { "template" : "main::text.html" , "objectId" : objectId , "subreference" : subreference , "collections" : { "current" : { "label" : collection . get_label ( lang ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , "author" : text . get_creator ( lang ) , "title" : text . get_title ( lang ) , "description" : text . get_description ( lang ) , "citation" : collection . citation , "coins" : self . make_coins ( collection , text , subreference , lang = lang ) } , "parents" : self . make_parents ( collection , lang = lang ) } , "text_passage" : Markup ( passage ) , "prev" : prev , "next" : next }
7747	def _process_handler_result ( self , response ) : if response is None or response is False : return False if isinstance ( response , Stanza ) : self . send ( response ) return True try : response = iter ( response ) except TypeError : return bool ( response ) for stanza in response : if isinstance ( stanza , Stanza ) : self . send ( stanza ) else : logger . warning ( u"Unexpected object in stanza handler result:" u" {0!r}" . format ( stanza ) ) return True
5956	def load_v4_tools ( ) : logger . debug ( "Loading v4 tools..." ) names = config . get_tool_names ( ) if len ( names ) == 0 and 'GMXBIN' in os . environ : names = find_executables ( os . environ [ 'GMXBIN' ] ) if len ( names ) == 0 or len ( names ) > len ( V4TOOLS ) * 4 : names = list ( V4TOOLS ) names . extend ( config . get_extra_tool_names ( ) ) tools = { } for name in names : fancy = make_valid_identifier ( name ) tools [ fancy ] = tool_factory ( fancy , name , None ) if not tools : errmsg = "Failed to load v4 tools" logger . debug ( errmsg ) raise GromacsToolLoadingError ( errmsg ) logger . debug ( "Loaded {0} v4 tools successfully!" . format ( len ( tools ) ) ) return tools
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
13567	def linspacestep ( start , stop , step = 1 ) : # Find an integer number of steps numsteps = _np . int ( ( stop - start ) / step ) # Do a linspace over the new range # that has the correct endpoint return _np . linspace ( start , start + step * numsteps , numsteps + 1 )
3570	def centralManager_didConnectPeripheral_ ( self , manager , peripheral ) : logger . debug ( 'centralManager_didConnectPeripheral called' ) # Setup peripheral delegate and kick off service discovery. For now just # assume all services need to be discovered. peripheral . setDelegate_ ( self ) peripheral . discoverServices_ ( None ) # Fire connected event for device. device = device_list ( ) . get ( peripheral ) if device is not None : device . _set_connected ( )
11610	def update_probability_at_read_level ( self , model = 3 ) : self . probability . reset ( ) # reset to alignment incidence matrix if model == 1 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . HAPLOGROUP , grouping_mat = self . t2t_mat ) haplogroup_sum_mat = self . allelic_expression * self . t2t_mat self . probability . multiply ( haplogroup_sum_mat , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( haplogroup_sum_mat . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 2 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . LOCUS ) self . probability . multiply ( self . allelic_expression . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 3 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 4 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . READ ) else : raise RuntimeError ( 'The read normalization model should be 1, 2, 3, or 4.' )
6423	def dist ( self , src , tar , word_approx_min = 0.3 , char_approx_min = 0.73 , tests = 2 ** 12 - 1 , ) : return ( synoname ( src , tar , word_approx_min , char_approx_min , tests , False ) / 14 )
12626	def recursive_find_search ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in files if re . search ( regex , f ) ] ) return outlist
851	def rewind ( self ) : # Superclass rewind super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) # Skip header rows self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) # Reset record count, etc. self . _recordCount = 0
4556	def all_named_colors ( ) : yield from _TO_COLOR_USER . items ( ) for name , color in _TO_COLOR . items ( ) : if name not in _TO_COLOR_USER : yield name , color
13221	def breakfast ( self , message = "Breakfast is ready" , shout : bool = False ) : return self . helper . output ( message , shout )
7429	def draw ( self , axes ) : ## create a toytree object from the treemix tree result tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) ## get coords for admix in self . results . admixture : ## parse admix event pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) ## add line for admixture edge mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) ## add points at admixture sink axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) ## add scale bar for edge lengths axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
9466	def conference_record_stop ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStop/' method = 'POST' return self . request ( path , method , call_params )
7228	def paint ( self ) : snippet = { 'fill-extrusion-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-extrusion-color' : VectorStyle . get_style_value ( self . color ) , 'fill-extrusion-base' : VectorStyle . get_style_value ( self . base ) , 'fill-extrusion-height' : VectorStyle . get_style_value ( self . height ) } if self . translate : snippet [ 'fill-extrusion-translate' ] = self . translate return snippet
7502	def fill_boot ( seqarr , newboot , newmap , spans , loci ) : ## column index cidx = 0 ## resample each locus for i in xrange ( loci . shape [ 0 ] ) : ## grab a random locus's columns x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] ## randomize columns within colsq cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] ## fill bootarr with n columns from seqarr ## the required length was already measured newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols ## fill bootmap with new map info newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 ## advance column index cidx += cols . shape [ 1 ] ## return the concatenated cols return newboot , newmap
6796	def manage_async ( self , command = '' , name = 'process' , site = ALL , exclude_sites = '' , end_message = '' , recipients = '' ) : exclude_sites = exclude_sites . split ( ':' ) r = self . local_renderer for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : if _site in exclude_sites : continue r . env . SITE = _site r . env . command = command r . env . end_email_command = '' r . env . recipients = recipients or '' r . env . end_email_command = '' if end_message : end_message = end_message + ' for ' + _site end_message = end_message . replace ( ' ' , '_' ) r . env . end_message = end_message r . env . end_email_command = r . format ( '{manage_cmd} send_mail --subject={end_message} --recipients={recipients}' ) r . env . name = name . format ( * * r . genv ) r . run ( 'screen -dmS {name} bash -c "export SITE={SITE}; ' 'export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} {command} --traceback; {end_email_command}"; sleep 3;' )
4789	def is_digit ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isdigit ( ) : self . _err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
10198	def anonymize_user ( doc ) : ip = doc . pop ( 'ip_address' , None ) if ip : doc . update ( { 'country' : get_geoip ( ip ) } ) user_id = doc . pop ( 'user_id' , '' ) session_id = doc . pop ( 'session_id' , '' ) user_agent = doc . pop ( 'user_agent' , '' ) # A 'User Session' is defined as activity by a user in a period of # one hour. timeslice represents the hour of the day in which # the event has been generated and together with user info it determines # the 'User Session' timestamp = arrow . get ( doc . get ( 'timestamp' ) ) timeslice = timestamp . strftime ( '%Y%m%d%H' ) salt = get_anonymization_salt ( timestamp ) visitor_id = hashlib . sha224 ( salt . encode ( 'utf-8' ) ) # TODO: include random salt here, that changes once a day. # m.update(random_salt) if user_id : visitor_id . update ( user_id . encode ( 'utf-8' ) ) elif session_id : visitor_id . update ( session_id . encode ( 'utf-8' ) ) elif ip and user_agent : vid = '{}|{}|{}' . format ( ip , user_agent , timeslice ) visitor_id . update ( vid . encode ( 'utf-8' ) ) else : # TODO: add random data? pass unique_session_id = hashlib . sha224 ( salt . encode ( 'utf-8' ) ) if user_id : sid = '{}|{}' . format ( user_id , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) elif session_id : sid = '{}|{}' . format ( session_id , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) elif ip and user_agent : sid = '{}|{}|{}' . format ( ip , user_agent , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) doc . update ( dict ( visitor_id = visitor_id . hexdigest ( ) , unique_session_id = unique_session_id . hexdigest ( ) ) ) return doc
2544	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True self . file ( doc ) . comment = text return True else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
10513	def registerkbevent ( self , keys , modifiers , fn_name , * args ) : event_name = "kbevent%s%s" % ( keys , modifiers ) self . _pollEvents . _callback [ event_name ] = [ event_name , fn_name , args ] return self . _remote_registerkbevent ( keys , modifiers )
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : """ Allow the passing of parameters to require_at_least_one_query_parameter. """ @ wraps ( view ) def wrapper ( request , * args , * * kwargs ) : """ Checks for the existence of the specified query parameters, raises a ValidationError if none of them were included in the request. """ requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , * * kwargs ) return wrapper return outer_wrapper
7293	def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return self . form
3043	def _expires_in ( self ) : if self . token_expiry : now = _UTCNOW ( ) if self . token_expiry > now : time_delta = self . token_expiry - now # TODO(orestica): return time_delta.total_seconds() # once dropping support for Python 2.6 return time_delta . days * 86400 + time_delta . seconds else : return 0
922	def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False # None of the field filters triggered, accept the record as a good one return True
331	def model_best ( y1 , y2 , samples = 1000 , progressbar = True ) : y = np . concatenate ( ( y1 , y2 ) ) mu_m = np . mean ( y ) mu_p = 0.000001 * 1 / np . std ( y ) ** 2 sigma_low = np . std ( y ) / 1000 sigma_high = np . std ( y ) * 1000 with pm . Model ( ) as model : group1_mean = pm . Normal ( 'group1_mean' , mu = mu_m , tau = mu_p , testval = y1 . mean ( ) ) group2_mean = pm . Normal ( 'group2_mean' , mu = mu_m , tau = mu_p , testval = y2 . mean ( ) ) group1_std = pm . Uniform ( 'group1_std' , lower = sigma_low , upper = sigma_high , testval = y1 . std ( ) ) group2_std = pm . Uniform ( 'group2_std' , lower = sigma_low , upper = sigma_high , testval = y2 . std ( ) ) nu = pm . Exponential ( 'nu_minus_two' , 1 / 29. , testval = 4. ) + 2. returns_group1 = pm . StudentT ( 'group1' , nu = nu , mu = group1_mean , lam = group1_std ** - 2 , observed = y1 ) returns_group2 = pm . StudentT ( 'group2' , nu = nu , mu = group2_mean , lam = group2_std ** - 2 , observed = y2 ) diff_of_means = pm . Deterministic ( 'difference of means' , group2_mean - group1_mean ) pm . Deterministic ( 'difference of stds' , group2_std - group1_std ) pm . Deterministic ( 'effect size' , diff_of_means / pm . math . sqrt ( ( group1_std ** 2 + group2_std ** 2 ) / 2 ) ) pm . Deterministic ( 'group1_annual_volatility' , returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_annual_volatility' , returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group1_sharpe' , returns_group1 . distribution . mean / returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_sharpe' , returns_group2 . distribution . mean / returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
1832	def JC ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF , target . read ( ) , cpu . PC )
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : # I already have the inverted fields I need anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : # I am creating the inverted fields then...need output file path: output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) # setup the commands to be called invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) # add fsl_sub before the commands if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd # create the inverse fields if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to anatomical space if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to functional space if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
10478	def _waitFor ( self , timeout , notification , * * kwargs ) : callback = self . _matchOther retelem = None callbackArgs = None callbackKwargs = None # Allow customization of the callback, though by default use the basic # _match() method if 'callback' in kwargs : callback = kwargs [ 'callback' ] del kwargs [ 'callback' ] # Deal with these only if callback is provided: if 'args' in kwargs : if not isinstance ( kwargs [ 'args' ] , tuple ) : errStr = 'Notification callback args not given as a tuple' raise TypeError ( errStr ) # If args are given, notification will pass back the returned # element in the first positional arg callbackArgs = kwargs [ 'args' ] del kwargs [ 'args' ] if 'kwargs' in kwargs : if not isinstance ( kwargs [ 'kwargs' ] , dict ) : errStr = 'Notification callback kwargs not given as a dict' raise TypeError ( errStr ) callbackKwargs = kwargs [ 'kwargs' ] del kwargs [ 'kwargs' ] # If kwargs are not given as a dictionary but individually listed # need to update the callbackKwargs dict with the remaining items in # kwargs if kwargs : if callbackKwargs : callbackKwargs . update ( kwargs ) else : callbackKwargs = kwargs else : callbackArgs = ( retelem , ) # Pass the kwargs to the default callback callbackKwargs = kwargs return self . _setNotification ( timeout , notification , callback , callbackArgs , callbackKwargs )
7433	def _read_sample_names ( fname ) : try : with open ( fname , 'r' ) as infile : subsamples = [ x . split ( ) [ 0 ] for x in infile . readlines ( ) if x . strip ( ) ] except Exception as inst : print ( "Failed to read input file with sample names.\n{}" . format ( inst ) ) raise inst return subsamples
13305	def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
10597	def h_L ( self , L , theta , Ts , * * statef ) : Nu_L = self . Nu_L ( L , theta , Ts , * * statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_L * k / L
776	def __getDBNameForVersion ( cls , dbVersion ) : # DB Name prefix for the given version prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) # DB Name suffix suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) # Replace dash and dot with underscore (e.g. 'ec2-user' or ec2.user will break SQL) suffix = suffix . replace ( "-" , "_" ) suffix = suffix . replace ( "." , "_" ) # Create the name of the database for the given DB version dbName = '%s_%s' % ( prefix , suffix ) return dbName
2403	def gen_feats ( self , e_set ) : bag_feats = self . gen_bag_feats ( e_set ) length_feats = self . gen_length_feats ( e_set ) prompt_feats = self . gen_prompt_feats ( e_set ) overall_feats = numpy . concatenate ( ( length_feats , prompt_feats , bag_feats ) , axis = 1 ) overall_feats = overall_feats . copy ( ) return overall_feats
1098	def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( "n must be > 0: %r" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( "cutoff must be in [0.0, 1.0]: %r" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) # Move the best scorers to head of list result = heapq . nlargest ( n , result ) # Strip scores for the best n matches return [ x for score , x in result ]
9561	def _apply_check_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
2978	def cmd_partition ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . random : if opts . partitions : raise BlockadeError ( "Either specify individual partitions " "or --random, but not both" ) b . random_partition ( ) else : partitions = [ ] for partition in opts . partitions : names = [ ] for name in partition . split ( "," ) : name = name . strip ( ) if name : names . append ( name ) partitions . append ( names ) if not partitions : raise BlockadeError ( "Either specify individual partitions " "or random" ) b . partition ( partitions )
10606	def prepare_to_run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare_to_run ( self . clock , self . period_count )
10343	def overlay_type_data ( graph : BELGraph , data : Mapping [ str , float ] , func : str , namespace : str , label : Optional [ str ] = None , overwrite : bool = False , impute : Optional [ float ] = None , ) -> None : new_data = { node : data . get ( node [ NAME ] , impute ) for node in filter_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) ) } overlay_data ( graph , new_data , label = label , overwrite = overwrite )
10349	def lint_file ( in_file , out_file = None ) : for line in in_file : print ( line . strip ( ) , file = out_file )
2165	def format_commands ( self , ctx , formatter ) : self . format_command_subsection ( ctx , formatter , self . list_misc_commands ( ) , 'Commands' ) self . format_command_subsection ( ctx , formatter , self . list_resource_commands ( ) , 'Resources' )
13289	def get_content_commit_date ( extensions , acceptance_callback = None , root_dir = '.' ) : logger = logging . getLogger ( __name__ ) def _null_callback ( _ ) : return True if acceptance_callback is None : acceptance_callback = _null_callback # Cache the repo object for each query root_dir = os . path . abspath ( root_dir ) repo = git . repo . base . Repo ( path = root_dir , search_parent_directories = True ) # Iterate over all files with all file extensions, looking for the # newest commit datetime. newest_datetime = None iters = [ _iter_filepaths_with_extension ( ext , root_dir = root_dir ) for ext in extensions ] for content_path in itertools . chain ( * iters ) : content_path = os . path . abspath ( os . path . join ( root_dir , content_path ) ) if acceptance_callback ( content_path ) : logger . debug ( 'Found content path %r' , content_path ) try : commit_datetime = read_git_commit_timestamp_for_file ( content_path , repo = repo ) logger . debug ( 'Commit timestamp of %r is %s' , content_path , commit_datetime ) except IOError : logger . warning ( 'Count not get commit for %r, skipping' , content_path ) continue if not newest_datetime or commit_datetime > newest_datetime : # Seed initial newest_datetime # or set a newer newest_datetime newest_datetime = commit_datetime logger . debug ( 'Newest commit timestamp is %s' , newest_datetime ) logger . debug ( 'Final commit timestamp is %s' , newest_datetime ) if newest_datetime is None : raise RuntimeError ( 'No content files found in {}' . format ( root_dir ) ) return newest_datetime
8314	def parse ( self , light = False ) : markup = self . markup self . disambiguation = self . parse_disambiguation ( markup ) self . categories = self . parse_categories ( markup ) self . links = self . parse_links ( markup ) if not light : # Conversion of HTML markup to Wikipedia markup. markup = self . convert_pre ( markup ) markup = self . convert_li ( markup ) markup = self . convert_table ( markup ) markup = replace_entities ( markup ) # Harvest references from the markup # and replace them by footnotes. markup = markup . replace ( "{{Cite" , "{{cite" ) markup = re . sub ( "\{\{ {1,2}cite" , "{{cite" , markup ) self . references , markup = self . parse_references ( markup ) # Make sure there are no legend linebreaks in image links. # Then harvest images and strip them from the markup. markup = re . sub ( "\n+(\{\{legend)" , "\\1" , markup ) self . images , markup = self . parse_images ( markup ) self . images . extend ( self . parse_gallery_images ( markup ) ) self . paragraphs = self . parse_paragraphs ( markup ) self . tables = self . parse_tables ( markup ) self . translations = self . parse_translations ( markup ) self . important = self . parse_important ( markup )
8037	def get_summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp_summ self . summarizers [ name ] = mcp_summ . summarize return self . summarizers [ name ]
62	def is_fully_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] return self . x1 >= 0 and self . x2 < width and self . y1 >= 0 and self . y2 < height
2347	def phrase_to_filename ( self , phrase ) : # remove non-word characters name = re . sub ( r"[^\w\s\.]" , '' , phrase . strip ( ) . lower ( ) ) # replace whitespace with underscores name = re . sub ( r"\s+" , '_' , name ) return name + '.png'
10705	def get_locations ( ) : arequest = requests . get ( LOCATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
8973	def can_connect_to ( self , other ) : assert other . is_mesh ( ) disconnected = not other . is_connected ( ) and not self . is_connected ( ) types_differ = self . _is_consumed_mesh ( ) != other . _is_consumed_mesh ( ) return disconnected and types_differ
730	def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) # Generate a new key pair key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) # Generate a certificate request using that key-pair cert_request = crypto . X509Req ( ) # Create public key object cert_request . set_pubkey ( key_pair ) # Add the public key to the request cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) # Build the OAuth session object token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
6584	def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
8605	def delete_user ( self , user_id ) : response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'DELETE' ) return response
6651	def build ( self , builddir , component , args , release_build = False , build_args = None , targets = None , release_no_debug_info_build = False ) : if build_args is None : build_args = [ ] if targets is None : targets = [ ] # in the future this may be specified in the target description, but # for now we only support cmake, so everything is simple: if release_no_debug_info_build : build_type = 'Release' elif release_build : build_type = 'RelWithDebInfo' else : build_type = 'Debug' cmd = [ 'cmake' , '-D' , 'CMAKE_BUILD_TYPE=%s' % build_type , '-G' , args . cmake_generator , '.' ] res = self . exec_helper ( cmd , builddir ) if res is not None : return res # work-around various yotta-specific issues with the generated # Ninja/project files: from yotta . lib import cmake_fixups cmake_fixups . applyFixupsForFenerator ( args . cmake_generator , builddir , component ) build_command = self . overrideBuildCommand ( args . cmake_generator , targets = targets ) if build_command : cmd = build_command + build_args else : cmd = [ 'cmake' , '--build' , builddir ] if len ( targets ) : # !!! FIXME: support multiple targets with the default CMake # build command cmd += [ '--target' , targets [ 0 ] ] cmd += build_args res = self . exec_helper ( cmd , builddir ) if res is not None : return res hint = self . hintForCMakeGenerator ( args . cmake_generator , component ) if hint : logger . info ( hint )
12742	def get_ISBNs ( self ) : invalid_isbns = set ( self . get_invalid_ISBNs ( ) ) valid_isbns = [ self . _clean_isbn ( isbn ) for isbn in self [ "020a" ] if self . _clean_isbn ( isbn ) not in invalid_isbns ] if valid_isbns : return valid_isbns # this is used sometimes in czech national library return [ self . _clean_isbn ( isbn ) for isbn in self [ "901i" ] ]
3715	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeSolids ] return mixing_simple ( zs , Vms ) else : raise Exception ( 'Method not valid' )
11878	def getProcessOwner ( pid ) : try : ownerUid = os . stat ( '/proc/' + str ( pid ) ) . st_uid except : return None try : ownerName = pwd . getpwuid ( ownerUid ) . pw_name except : ownerName = None return { 'uid' : ownerUid , 'name' : ownerName }
6901	def _parse_xmatch_catalog_header ( xc , xk ) : catdef = [ ] # read in this catalog and transparently handle gzipped files if xc . endswith ( '.gz' ) : infd = gzip . open ( xc , 'rb' ) else : infd = open ( xc , 'rb' ) # read in the defs for line in infd : if line . decode ( ) . startswith ( '#' ) : catdef . append ( line . decode ( ) . replace ( '#' , '' ) . strip ( ) . rstrip ( '\n' ) ) if not line . decode ( ) . startswith ( '#' ) : break if not len ( catdef ) > 0 : LOGERROR ( "catalog definition not parseable " "for catalog: %s, skipping..." % xc ) return None catdef = ' ' . join ( catdef ) catdefdict = json . loads ( catdef ) catdefkeys = [ x [ 'key' ] for x in catdefdict [ 'columns' ] ] catdefdtypes = [ x [ 'dtype' ] for x in catdefdict [ 'columns' ] ] catdefnames = [ x [ 'name' ] for x in catdefdict [ 'columns' ] ] catdefunits = [ x [ 'unit' ] for x in catdefdict [ 'columns' ] ] # get the correct column indices and dtypes for the requested columns # from the catdefdict catcolinds = [ ] catcoldtypes = [ ] catcolnames = [ ] catcolunits = [ ] for xkcol in xk : if xkcol in catdefkeys : xkcolind = catdefkeys . index ( xkcol ) catcolinds . append ( xkcolind ) catcoldtypes . append ( catdefdtypes [ xkcolind ] ) catcolnames . append ( catdefnames [ xkcolind ] ) catcolunits . append ( catdefunits [ xkcolind ] ) return ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits )
1603	def to_table ( metrics ) : all_queries = tracker_access . metric_queries ( ) m = tracker_access . queries_map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all_queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except KeyError : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all_queries if k in metrics . keys ( ) ] return stats , header
3888	def remove_observer ( self , callback ) : if callback not in self . _observers : raise ValueError ( '{} is not an observer of {}' . format ( callback , self ) ) self . _observers . remove ( callback )
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) # TODO: this process could use a timeout object like the disconnect # timeout thing, and ONLY send packets when none are sent! # We would do that by calling timeout.set() for a "sending" # timeout. If we're sending 100 messages a second, there is # no need to push some heartbeats in there also. self . put_client_msg ( "2::" )
2629	def scale_out ( self , blocks = 1 , block_size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block_id = len ( self . blocks ) self . blocks [ block_id ] = [ ] for instance_id in range ( 0 , block_size ) : instances = self . server_manager . create ( 'parsl-{0}-{1}' . format ( block_id , instance_id ) , # Name self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , # Image_id self . client . flavors . list ( ) [ 0 ] , min_count = 1 , max_count = 1 , userdata = setup_script . format ( engine_config = self . engine_config ) , key_name = 'TG-MCB090174-api-key' , security_groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block_id ] . extend ( [ instances ] ) count += 1 return count
9126	def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
11233	def get_inner_template ( self , language , template_type , indentation , key , val ) : #Language specific inner templates inner_templates = { 'php' : { 'iterable' : '%s%s => array \n%s( \n%s%s),\n' % ( indentation , key , indentation , val , indentation ) , 'singular' : '%s%s => %s, \n' % ( indentation , key , val ) } , 'javascript' : { 'iterable' : '%s%s : {\n%s\n%s},\n' % ( indentation , key , val , indentation ) , 'singular' : '%s%s: %s,\n' % ( indentation , key , val ) } , 'ocaml' : { 'iterable' : '%s[| (%s, (\n%s\n%s))|] ;;\n' % ( indentation , key , val , indentation ) , 'singular' : '%s(%s, %s);\n' % ( indentation , key , val ) } } return inner_templates [ language ] [ template_type ]
7378	def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance ( k , str ) : msg = "%s: key must be a string" % self . __class__ . __name__ raise ValueError ( msg ) if not k . startswith ( self . prefix ) : k = self . prefix + k return func ( self , k , * args ) return decorated
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
10241	def count_authors_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , typing . Counter [ str ] ] : authors = group_as_dict ( _iter_authors_by_annotation ( graph , annotation = annotation ) ) return count_defaultdict ( authors )
5978	def mask_blurring_from_mask_and_psf_shape ( mask , psf_shape ) : blurring_mask = np . full ( mask . shape , True ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : for y1 in range ( ( - psf_shape [ 0 ] + 1 ) // 2 , ( psf_shape [ 0 ] + 1 ) // 2 ) : for x1 in range ( ( - psf_shape [ 1 ] + 1 ) // 2 , ( psf_shape [ 1 ] + 1 ) // 2 ) : if 0 <= x + x1 <= mask . shape [ 1 ] - 1 and 0 <= y + y1 <= mask . shape [ 0 ] - 1 : if mask [ y + y1 , x + x1 ] : blurring_mask [ y + y1 , x + x1 ] = False else : raise exc . MaskException ( "setup_blurring_mask extends beyond the sub_grid_size of the masks - pad the " "datas array before masking" ) return blurring_mask
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] # sort them by size len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False # get the basename and the rest of the files fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] # check if the basename is in the basename of the rest of the files for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
11154	def auto_complete_choices ( self , case_sensitive = False ) : self_basename = self . basename self_basename_lower = self . basename . lower ( ) if case_sensitive : # pragma: no cover def match ( basename ) : return basename . startswith ( self_basename ) else : def match ( basename ) : return basename . lower ( ) . startswith ( self_basename_lower ) choices = list ( ) if self . is_dir ( ) : choices . append ( self ) for p in self . sort_by_abspath ( self . select ( recursive = False ) ) : choices . append ( p ) else : p_parent = self . parent if p_parent . is_dir ( ) : for p in self . sort_by_abspath ( p_parent . select ( recursive = False ) ) : if match ( p . basename ) : choices . append ( p ) else : # pragma: no cover raise ValueError ( "'%s' directory does not exist!" % p_parent ) return choices
8248	def rotate_ryb ( self , angle = 180 ) : h = self . h * 360 angle = angle % 360 # Approximation of Itten's RYB color wheel. # In HSB, colors hues range from 0-360. # However, on the artistic color wheel these are not evenly distributed. # The second tuple value contains the actual distribution. wheel = [ ( 0 , 0 ) , ( 15 , 8 ) , ( 30 , 17 ) , ( 45 , 26 ) , ( 60 , 34 ) , ( 75 , 41 ) , ( 90 , 48 ) , ( 105 , 54 ) , ( 120 , 60 ) , ( 135 , 81 ) , ( 150 , 103 ) , ( 165 , 123 ) , ( 180 , 138 ) , ( 195 , 155 ) , ( 210 , 171 ) , ( 225 , 187 ) , ( 240 , 204 ) , ( 255 , 219 ) , ( 270 , 234 ) , ( 285 , 251 ) , ( 300 , 267 ) , ( 315 , 282 ) , ( 330 , 298 ) , ( 345 , 329 ) , ( 360 , 0 ) ] # Given a hue, find out under what angle it is # located on the artistic color wheel. for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if y0 <= h <= y1 : a = 1.0 * x0 + ( x1 - x0 ) * ( h - y0 ) / ( y1 - y0 ) break # And the user-given angle (e.g. complement). a = ( a + angle ) % 360 # For the given angle, find out what hue is # located there on the artistic color wheel. for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if x0 <= a <= x1 : h = 1.0 * y0 + ( y1 - y0 ) * ( a - x0 ) / ( x1 - x0 ) break h = h % 360 return Color ( h / 360 , self . s , self . brightness , self . a , mode = "hsb" , name = "" )
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
10807	def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING_ADMIN , cls . PENDING_USER ]
10275	def generate_mechanism ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None ) -> BELGraph : subgraph = get_upstream_causal_subgraph ( graph , node ) expand_upstream_causal ( graph , subgraph ) remove_inconsistent_edges ( subgraph ) collapse_consistent_edges ( subgraph ) if key is not None : # FIXME when is it not pruned? prune_mechanism_by_data ( subgraph , key ) return subgraph
8352	def handle_charref ( self , ref ) : if self . convertEntities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle_data ( data )
9530	def encrypt ( base_field , key = None , ttl = None ) : if not isinstance ( base_field , models . Field ) : assert key is None assert ttl is None return get_encrypted_field ( base_field ) name , path , args , kwargs = base_field . deconstruct ( ) kwargs . update ( { 'key' : key , 'ttl' : ttl } ) return get_encrypted_field ( base_field . __class__ ) ( * args , * * kwargs )
5925	def check_setup ( ) : if "GROMACSWRAPPER_SUPPRESS_SETUP_CHECK" in os . environ : return True missing = [ d for d in config_directories if not os . path . exists ( d ) ] if len ( missing ) > 0 : print ( "NOTE: Some configuration directories are not set up yet: " ) print ( "\t{0!s}" . format ( '\n\t' . join ( missing ) ) ) print ( "NOTE: You can create the configuration file and directories with:" ) print ( "\t>>> import gromacs" ) print ( "\t>>> gromacs.config.setup()" ) return False return True
6667	def check_version ( ) : global CHECK_VERSION if not CHECK_VERSION : return # Ensure we only check once in this process. CHECK_VERSION = 0 # Lookup most recent remote version. from six . moves . urllib . request import urlopen try : response = urlopen ( "https://pypi.org/pypi/burlap/json" ) data = json . loads ( response . read ( ) . decode ( ) ) remote_release = sorted ( tuple ( map ( int , _ . split ( '.' ) ) ) for _ in data [ 'releases' ] . keys ( ) ) [ - 1 ] remote_release_str = '.' . join ( map ( str , remote_release ) ) local_release = VERSION local_release_str = '.' . join ( map ( str , local_release ) ) # Display warning. if remote_release > local_release : print ( '\033[93m' ) print ( "You are using burlap version %s, however version %s is available." % ( local_release_str , remote_release_str ) ) print ( "You should consider upgrading via the 'pip install --upgrade burlap' command." ) print ( '\033[0m' ) except Exception as exc : print ( '\033[93m' ) print ( "Unable to check for updated burlap version: %s" % exc ) print ( '\033[0m' )
11569	def run ( self ) : while not self . is_stopped ( ) : # we can get an OSError: [Errno9] Bad file descriptor when shutting down # just ignore it try : if self . arduino . inWaiting ( ) : c = self . arduino . read ( ) self . command_deque . append ( ord ( c ) ) else : time . sleep ( .1 ) except OSError : pass except IOError : self . stop ( ) self . close ( )
4293	def _manage_args ( parser , args ) : for item in data . CONFIGURABLE_OPTIONS : action = parser . _option_string_actions [ item ] choices = default = '' input_value = getattr ( args , action . dest ) new_val = None # cannot count this until we find a way to test input if not args . noinput : # pragma: no cover if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input_value : if type ( input_value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input_value ) ) else : default = ' [default {0}]' . format ( input_value ) while not new_val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new_val = utils . query_yes_no ( prompt ) else : new_val = compat . input ( prompt ) new_val = compat . clean ( new_val ) if not new_val and input_value : new_val = input_value if new_val and action . dest == 'templates' : if new_val != 'no' and not os . path . isdir ( new_val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new_val = False if new_val and action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) else : if not input_value and action . required : raise ValueError ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new_val = input_value if action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new_val == 'no' or not os . path . isdir ( new_val ) ) : new_val = False if action . dest in ( 'bootstrap' , 'starting_page' ) : new_val = ( new_val == 'yes' ) setattr ( args , action . dest , new_val ) return args
5112	def animate ( self , out = None , t = None , line_kwargs = None , scatter_kwargs = None , * * kwargs ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if not HAS_MATPLOTLIB : msg = "Matplotlib is necessary to animate a simulation." raise ImportError ( msg ) self . _update_all_colors ( ) kwargs . setdefault ( 'bgcolor' , self . colors [ 'bgcolor' ] ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_args , scat_args = self . g . lines_scatter_args ( * * mpl_kwargs ) lines = LineCollection ( * * line_args ) lines = ax . add_collection ( lines ) scatt = ax . scatter ( * * scat_args ) t = np . infty if t is None else t now = self . _t def update ( frame_number ) : if t is not None : if self . _t > now + t : return False self . _simulate_next_event ( slow = True ) lines . set_color ( line_args [ 'colors' ] ) scatt . set_edgecolors ( scat_args [ 'edgecolors' ] ) scatt . set_facecolor ( scat_args [ 'c' ] ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs [ 'bgcolor' ] ) else : ax . set_axis_bgcolor ( kwargs [ 'bgcolor' ] ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) animation_args = { 'fargs' : None , 'event_source' : None , 'init_func' : None , 'frames' : None , 'blit' : False , 'interval' : 10 , 'repeat' : None , 'func' : update , 'repeat_delay' : None , 'fig' : fig , 'save_count' : None , } for key , value in kwargs . items ( ) : if key in animation_args : animation_args [ key ] = value animation = FuncAnimation ( * * animation_args ) if 'filename' not in kwargs : plt . ioff ( ) plt . show ( ) else : save_args = { 'filename' : None , 'writer' : None , 'fps' : None , 'dpi' : None , 'codec' : None , 'bitrate' : None , 'extra_args' : None , 'metadata' : None , 'extra_anim' : None , 'savefig_kwargs' : None } for key , value in kwargs . items ( ) : if key in save_args : save_args [ key ] = value animation . save ( * * save_args )
12145	def doStuff ( ABFfolder , analyze = False , convert = False , index = True , overwrite = True , launch = True ) : IN = INDEX ( ABFfolder ) if analyze : IN . analyzeAll ( ) if convert : IN . convertImages ( )
1728	def to_arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
13854	def append_main_thread ( self ) : thread = MainThread ( main_queue = self . main_queue , main_spider = self . main_spider , branch_spider = self . branch_spider ) thread . daemon = True thread . start ( )
437	def data_to_tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( "%s exists" % filename ) return print ( "Converting data into %s ..." % filename ) # cwd = os.getcwd() writer = tf . python_io . TFRecordWriter ( filename ) for index , img in enumerate ( images ) : img_raw = img . tobytes ( ) # Visualize a image # tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236) label = int ( labels [ index ] ) # print(label) # Convert the bytes back to image as follow: # image = Image.frombytes('RGB', (32, 32), img_raw) # image = np.fromstring(img_raw, np.float32) # image = image.reshape([32, 32, 3]) # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236) example = tf . train . Example ( features = tf . train . Features ( feature = { "label" : tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ label ] ) ) , 'img_raw' : tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ img_raw ] ) ) , } ) ) writer . write ( example . SerializeToString ( ) ) # Serialize To String writer . close ( )
12178	def ensureDetection ( self ) : if self . APs == False : self . log . debug ( "analysis attempted before event detection..." ) self . detect ( )
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
13066	def make_members ( self , collection , lang = None ) : objects = sorted ( [ self . expose_ancestors_or_children ( member , collection , lang = lang ) for member in collection . members if member . get_label ( ) ] , key = itemgetter ( "label" ) ) return objects
5181	def _url ( self , endpoint , path = None ) : log . debug ( '_url called with endpoint: {0} and path: {1}' . format ( endpoint , path ) ) try : endpoint = ENDPOINTS [ endpoint ] except KeyError : # If we reach this we're trying to query an endpoint that doesn't # exist. This shouldn't happen unless someone made a booboo. raise APIError url = '{base_url}/{endpoint}' . format ( base_url = self . base_url , endpoint = endpoint , ) if path is not None : url = '{0}/{1}' . format ( url , quote ( path ) ) return url
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
7414	def plot_pairwise_dist ( self , labels = None , ax = None , cmap = None , cdict = None , metric = "euclidean" ) : allele_counts = self . genotypes . to_n_alt ( ) dist = allel . pairwise_distance ( allele_counts , metric = metric ) if not ax : fig = plt . figure ( figsize = ( 5 , 5 ) ) ax = fig . add_subplot ( 1 , 1 , 1 ) if isinstance ( labels , bool ) : if labels : labels = list ( self . samples_vcforder ) elif isinstance ( labels , type ( None ) ) : pass else : ## If not bool or None (default), then check to make sure the list passed in ## is the right length if not len ( labels ) == len ( self . samples_vcforder ) : raise IPyradError ( LABELS_LENGTH_ERROR . format ( len ( labels ) , len ( self . samples_vcforder ) ) ) allel . plot . pairwise_distance ( dist , labels = labels , ax = ax , colorbar = False )
1475	def _get_streaming_processes ( self ) : retval = { } instance_plans = self . _get_instance_plans ( self . packing_plan , self . shard ) instance_info = [ ] for instance_plan in instance_plans : global_task_id = instance_plan . task_id component_index = instance_plan . component_index component_name = instance_plan . component_name instance_id = "container_%s_%s_%d" % ( str ( self . shard ) , component_name , global_task_id ) instance_info . append ( ( instance_id , component_name , global_task_id , component_index ) ) stmgr_cmd_lst = [ self . stmgr_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--topologydefn_file=%s' % self . topology_defn_file , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--stmgr_id=%s' % self . stmgr_ids [ self . shard ] , '--instance_ids=%s' % ',' . join ( map ( lambda x : x [ 0 ] , instance_info ) ) , '--myhost=%s' % self . master_host , '--data_port=%s' % str ( self . master_port ) , '--local_data_port=%s' % str ( self . tmaster_controller_port ) , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--shell_port=%s' % str ( self . shell_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) , '--ckptmgr_id=%s' % self . ckptmgr_ids [ self . shard ] , '--metricscachemgr_mode=%s' % self . metricscache_manager_mode . lower ( ) ] stmgr_env = self . shell_env . copy ( ) if self . shell_env is not None else { } stmgr_cmd = Command ( stmgr_cmd_lst , stmgr_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : stmgr_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ self . stmgr_ids [ self . shard ] ] = stmgr_cmd # metricsmgr_metrics_sink_config_file = 'metrics_sinks.yaml' retval [ self . metricsmgr_ids [ self . shard ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ self . shard ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) if self . pkg_type == 'jar' or self . pkg_type == 'tar' : retval . update ( self . _get_java_instance_cmd ( instance_info ) ) elif self . pkg_type == 'pex' : retval . update ( self . _get_python_instance_cmd ( instance_info ) ) elif self . pkg_type == 'so' : retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) elif self . pkg_type == 'dylib' : retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) else : raise ValueError ( "Unrecognized package type: %s" % self . pkg_type ) return retval
13793	def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return # This tests to see if the function has been decorated with the view # server synchronisation decorator (``decorate_view``). if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) # The decorator gets called with the logger function. else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
9696	def validate_token ( self , request , consumer , token ) : oauth_server , oauth_request = oauth_provider . utils . initialize_server_request ( request ) oauth_server . verify_request ( oauth_request , consumer , token )
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
13886	def ReplaceInFile ( filename , old , new , encoding = None ) : contents = GetFileContents ( filename , encoding = encoding ) contents = contents . replace ( old , new ) CreateFile ( filename , contents , encoding = encoding ) return contents
7675	def pprint_jobject ( obj , * * kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , * * kwargs ) # Suppress braces and quotes string = re . sub ( r'[{}"]' , '' , string ) # Kill trailing commas string = re . sub ( r',\n' , '\n' , string ) # Kill blank lines string = re . sub ( r'^\s*$' , '' , string ) return string
4126	def spectrum_data ( filename ) : import os import pkg_resources info = pkg_resources . get_distribution ( 'spectrum' ) location = info . location # first try develop mode share = os . sep . join ( [ location , "spectrum" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )
4009	def _start_http_server ( ) : logging . info ( 'Starting HTTP server at {}:{}' . format ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread = threading . Thread ( target = http_server . app . run , args = ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread . daemon = True thread . start ( )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
6137	def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
5653	def execute ( cur , * args ) : stmt = args [ 0 ] if len ( args ) > 1 : stmt = stmt . replace ( '%' , '%%' ) . replace ( '?' , '%r' ) print ( stmt % ( args [ 1 ] ) ) return cur . execute ( * args )
13668	def slinky ( filename , seconds_available , bucket_name , aws_key , aws_secret ) : if not os . environ . get ( 'AWS_ACCESS_KEY_ID' ) and os . environ . get ( 'AWS_SECRET_ACCESS_KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create_temp_s3_link ( filename , seconds_available , bucket_name )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
6310	def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
5186	def aggregate_event_counts ( self , summarize_by , query = None , count_by = None , count_filter = None ) : return self . _query ( 'aggregate-event-counts' , query = query , summarize_by = summarize_by , count_by = count_by , count_filter = count_filter )
9524	def sort_by_size ( infile , outfile , smallest_first = False ) : seqs = { } file_to_dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest_first ) fout = utils . open_file_write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
5400	def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
7016	def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True ) : concatlcd = concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = aperture , sortby = sortby , normalize = normalize , recursive = recursive ) if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) outfpath = os . path . join ( outdir , '%s-%s-pklc.pkl' % ( concatlcd [ 'objectid' ] , aperture ) ) pklc = lcdict_to_pickle ( concatlcd , outfile = outfpath ) return pklc
11069	def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
5378	def _build_pipeline_docker_command ( cls , script_name , inputs , outputs , envs ) : # We upload the user script as an environment argument # and write it to SCRIPT_DIR (preserving its local file name). # # The docker_command: # * writes the script body to a file # * installs gcloud if there are recursive copies to do # * sets environment variables for inputs with wildcards # * sets environment variables for recursive input directories # * recursively copies input directories # * creates output directories # * sets environment variables for recursive output directories # * sets the DATA_ROOT environment variable to /mnt/data # * sets the working directory to ${DATA_ROOT} # * executes the user script # * recursively copies output directories recursive_input_dirs = [ var for var in inputs if var . recursive and var . value ] recursive_output_dirs = [ var for var in outputs if var . recursive and var . value ] install_cloud_sdk = '' if recursive_input_dirs or recursive_output_dirs : install_cloud_sdk = INSTALL_CLOUD_SDK export_input_dirs = '' copy_input_dirs = '' if recursive_input_dirs : export_input_dirs = providers_util . build_recursive_localize_env ( providers_util . DATA_MOUNT_POINT , inputs ) copy_input_dirs = providers_util . build_recursive_localize_command ( providers_util . DATA_MOUNT_POINT , inputs , job_model . P_GCS ) export_output_dirs = '' copy_output_dirs = '' if recursive_output_dirs : export_output_dirs = providers_util . build_recursive_gcs_delocalize_env ( providers_util . DATA_MOUNT_POINT , outputs ) copy_output_dirs = providers_util . build_recursive_delocalize_command ( providers_util . DATA_MOUNT_POINT , outputs , job_model . P_GCS ) docker_paths = [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in outputs if var . value ] mkdirs = '\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers_util . DATA_MOUNT_POINT , path ) for path in docker_paths ] ) inputs_with_wildcards = [ var for var in inputs if not var . recursive and var . docker_path and '*' in os . path . basename ( var . docker_path ) ] export_inputs_with_wildcards = '\n' . join ( [ 'export {0}="{1}/{2}"' . format ( var . name , providers_util . DATA_MOUNT_POINT , var . docker_path ) for var in inputs_with_wildcards ] ) export_empty_envs = '\n' . join ( [ 'export {0}=""' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER_COMMAND . format ( mk_runtime_dirs = MK_RUNTIME_DIRS_COMMAND , script_path = '%s/%s' % ( providers_util . SCRIPT_DIR , script_name ) , install_cloud_sdk = install_cloud_sdk , export_inputs_with_wildcards = export_inputs_with_wildcards , export_input_dirs = export_input_dirs , copy_input_dirs = copy_input_dirs , mk_output_dirs = mkdirs , export_output_dirs = export_output_dirs , export_empty_envs = export_empty_envs , tmpdir = providers_util . TMP_DIR , working_dir = providers_util . WORKING_DIR , copy_output_dirs = copy_output_dirs )
2657	def notify ( self , event_id ) : self . _event_buffer . extend ( [ event_id ] ) self . _event_count += 1 if self . _event_count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make_callback ( kind = "event" )
6398	def dist_monge_elkan ( src , tar , sim_func = sim_levenshtein , symmetric = False ) : return MongeElkan ( ) . dist ( src , tar , sim_func , symmetric )
2259	def dzip ( items1 , items2 , cls = dict ) : try : len ( items1 ) except TypeError : items1 = list ( items1 ) try : len ( items2 ) except TypeError : items2 = list ( items2 ) if len ( items1 ) == 0 and len ( items2 ) == 1 : # Corner case: # allow the first list to be empty and the second list to broadcast a # value. This means that the equality check wont work for the case # where items1 and items2 are supposed to correspond, but the length of # items2 is 1. items2 = [ ] if len ( items2 ) == 1 and len ( items1 ) > 1 : items2 = items2 * len ( items1 ) if len ( items1 ) != len ( items2 ) : raise ValueError ( 'out of alignment len(items1)=%r, len(items2)=%r' % ( len ( items1 ) , len ( items2 ) ) ) return cls ( zip ( items1 , items2 ) )
10470	def terminateAppByBundleId ( bundleID ) : ra = AppKit . NSRunningApplication if getattr ( ra , "runningApplicationsWithBundleIdentifier_" ) : appList = ra . runningApplicationsWithBundleIdentifier_ ( bundleID ) if appList and len ( appList ) > 0 : app = appList [ 0 ] return app and app . terminate ( ) and True or False return False
12315	def _run_generic_command ( self , repo , cmd ) : result = None with cd ( repo . rootdir ) : # Dont use sh. It is not collecting the stdout of all # child processes. output = self . _run ( cmd ) try : result = { 'cmd' : cmd , 'status' : 'success' , 'message' : output , } except Exception as e : result = { 'cmd' : cmd , 'status' : 'error' , 'message' : str ( e ) } return result
7870	def _decode_error ( self ) : error_qname = self . _ns_prefix + "error" for child in self . _element : if child . tag == error_qname : self . _error = StanzaErrorElement ( child ) return raise BadRequestProtocolError ( "Error element missing in" " an error stanza" )
3061	def positional ( max_positional_args ) : def positional_decorator ( wrapped ) : @ functools . wraps ( wrapped ) def positional_wrapper ( * args , * * kwargs ) : if len ( args ) > max_positional_args : plural_s = '' if max_positional_args != 1 : plural_s = 's' message = ( '{function}() takes at most {args_max} positional ' 'argument{plural} ({args_given} given)' . format ( function = wrapped . __name__ , args_max = max_positional_args , args_given = len ( args ) , plural = plural_s ) ) if positional_parameters_enforcement == POSITIONAL_EXCEPTION : raise TypeError ( message ) elif positional_parameters_enforcement == POSITIONAL_WARNING : logger . warning ( message ) return wrapped ( * args , * * kwargs ) return positional_wrapper if isinstance ( max_positional_args , six . integer_types ) : return positional_decorator else : args , _ , _ , defaults = inspect . getargspec ( max_positional_args ) return positional ( len ( args ) - len ( defaults ) ) ( max_positional_args )
4189	def window_poisson ( N , alpha = 2 ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) # Docker container names must match: [a-zA-Z0-9][a-zA-Z0-9_.-] # So 1) prefix it with "dsub-" and 2) change all invalid characters to "-". return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
10032	def execute ( helper , config , args ) : # check to see if the application exists if not helper . application_exists ( ) : helper . create_application ( get ( config , 'app.description' ) ) else : out ( "Application " + get ( config , 'app.app_name' ) + " exists" ) # create environments environment_names = [ ] environments_to_wait_for_green = [ ] for env_name , env_config in list ( get ( config , 'app.environments' ) . items ( ) ) : environment_names . append ( env_name ) env_config = parse_env_config ( config , env_name ) if not helper . environment_exists ( env_name ) : option_settings = parse_option_settings ( env_config . get ( 'option_settings' , { } ) ) helper . create_environment ( env_name , solution_stack_name = env_config . get ( 'solution_stack_name' ) , cname_prefix = env_config . get ( 'cname_prefix' , None ) , description = env_config . get ( 'description' , None ) , option_settings = option_settings , tier_name = env_config . get ( 'tier_name' ) , tier_type = env_config . get ( 'tier_type' ) , tier_version = env_config . get ( 'tier_version' ) , version_label = args . version_label ) environments_to_wait_for_green . append ( env_name ) else : out ( "Environment " + env_name ) # get the environments environments_to_wait_for_term = [ ] if args . delete : environments = helper . get_environments ( ) for env in environments : if env [ 'EnvironmentName' ] not in environment_names : if env [ 'Status' ] != 'Ready' : out ( "Unable to delete " + env [ 'EnvironmentName' ] + " because it's not in status Ready (" + env [ 'Status' ] + ")" ) else : out ( "Deleting environment: " + env [ 'EnvironmentName' ] ) helper . delete_environment ( env [ 'EnvironmentName' ] ) environments_to_wait_for_term . append ( env [ 'EnvironmentName' ] ) # wait if not args . dont_wait and len ( environments_to_wait_for_green ) > 0 : helper . wait_for_environments ( environments_to_wait_for_green , status = 'Ready' , include_deleted = False ) if not args . dont_wait and len ( environments_to_wait_for_term ) > 0 : helper . wait_for_environments ( environments_to_wait_for_term , status = 'Terminated' , include_deleted = False ) out ( "Application initialized" ) return 0
8645	def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset # GET /api/projects/0.1/tracks/{track_id}/ response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12278	def run_executable ( repo , args , includes ) : # Get platform information mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform_metadata = repomgr . get_metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find_executable_commitpath ( repo , args ) # Create a local directory tmpdir = tempfile . mkdtemp ( ) # Construct the strace command print ( "Running the command" ) strace_filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace_filename , "-s" , "1024" , "-q" , "--" ] + args # Run the command p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) # Capture the stdout/stderr stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) # Check the strace output files = extract_files ( strace_filename , includes ) # Now insert the execution metadata execution_metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution_metadata . update ( platform_metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution_metadata' ] = execution_metadata return files
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
2597	def can ( obj ) : import_needed = False for cls , canner in iteritems ( can_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import_needed : # perform can_map imports, then try again # this will usually only happen once _import_mapping ( can_map , _original_can_map ) return can ( obj ) return obj
8065	def drawdaisy ( x , y , color = '#fefefe' ) : # save location, size etc _ctx . push ( ) # save fill and stroke _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 # draw stalk _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) # draw flower _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) # draw petals _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) # draw centre _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) # restore fill and stroke _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) # restore location, size etc _ctx . pop ( )
2332	def uniform_noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : # Awkward, but makes modifier-key-only combinations possible # (since sendKeyWithModifiers() calls this) if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) # Press the key keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) # Release the key keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) # Set modflags on keyDown (default None): Quartz . CGEventSetFlags ( keyDown , modFlags ) # Set modflags on keyUp: Quartz . CGEventSetFlags ( keyUp , modFlags ) # Post the event to the given app if not globally : # To direct output to the correct application need the PSN (macOS <=10.10) or PID(macOS > 10.10): macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
12113	def file_supported ( cls , filename ) : if not isinstance ( filename , str ) : return False ( _ , ext ) = os . path . splitext ( filename ) if ext not in cls . extensions : return False else : return True
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) # this follows a similar pattern as docker-compose uses parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : # if we can't get a valid name from CWD, use "default" blockade_id = "default" return blockade_id
2222	def _rectify_base ( base ) : if base is NoParam or base == 'default' : return DEFAULT_ALPHABET elif base in [ 26 , 'abc' , 'alpha' ] : return _ALPHABET_26 elif base in [ 16 , 'hex' ] : return _ALPHABET_16 elif base in [ 10 , 'dec' ] : return _ALPHABET_10 else : if not isinstance ( base , ( list , tuple ) ) : raise TypeError ( 'Argument `base` must be a key, list, or tuple; not {}' . format ( type ( base ) ) ) return base
4290	def write ( self , album , media_group ) : from sigal import __url__ as sigal_link file_path = os . path . join ( album . dst_path , media_group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media_group [ 0 ] , 'previous_media' : media_group [ - 1 ] , 'next_media' : media_group [ 1 ] , 'index_title' : self . index_title , 'settings' : self . settings , 'sigal_link' : sigal_link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url_from_path ( os . path . relpath ( self . theme_path , album . dst_path ) ) } , } ) output_file = "%s.html" % file_path with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , * * kwargs ) : # Taken from Nodebox path = self . BezierPath ( * * kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
4337	def pad ( self , start_duration = 0.0 , end_duration = 0.0 ) : if not is_number ( start_duration ) or start_duration < 0 : raise ValueError ( "Start duration must be a positive number." ) if not is_number ( end_duration ) or end_duration < 0 : raise ValueError ( "End duration must be positive." ) effect_args = [ 'pad' , '{:f}' . format ( start_duration ) , '{:f}' . format ( end_duration ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'pad' ) return self
11652	def get_version ( self ) : if ( self . name is not None and self . version is not None and self . version . startswith ( ":versiontools:" ) ) : return ( self . __get_live_version ( ) or self . __get_frozen_version ( ) or self . __fail_to_get_any_version ( ) ) else : return self . __base . get_version ( self )
4618	def parse_time ( block_time ) : return datetime . strptime ( block_time , timeFormat ) . replace ( tzinfo = timezone . utc )
10087	def update ( self , * args , * * kwargs ) : super ( Deposit , self ) . update ( * args , * * kwargs )
9877	def _ratio_metric ( v1 , v2 , * * _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
2046	def get_storage_items ( self , address ) : storage = self . _world_state [ address ] [ 'storage' ] items = [ ] array = storage . array while not isinstance ( array , ArrayVariable ) : items . append ( ( array . index , array . value ) ) array = array . array return items
7231	def create_from_wkt ( self , wkt , item_type , ingest_source , * * attributes ) : # verify the "depth" of the attributes is single layer geojson = load_wkt ( wkt ) . __geo_interface__ vector = { 'type' : "Feature" , 'geometry' : geojson , 'properties' : { 'item_type' : item_type , 'ingest_source' : ingest_source , 'attributes' : attributes } } return self . create ( vector ) [ 0 ]
6661	def generate_self_signed_certificate ( self , domain = '' , r = None ) : r = self . local_renderer r . env . domain = domain or r . env . domain assert r . env . domain , 'No SSL domain defined.' role = r or self . genv . ROLE or ALL ssl_dst = 'roles/%s/ssl' % ( role , ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) r . env . base_dst = '%s/%s' % ( ssl_dst , r . env . domain ) r . local ( 'openssl req -new -newkey rsa:{ssl_length} ' '-days {ssl_days} -nodes -x509 ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.key -out {ssl_base_dst}.crt' )
10329	def get_path_effect ( graph , path , relationship_dict ) : causal_effect = [ ] for predecessor , successor in pairwise ( path ) : if pair_has_contradiction ( graph , predecessor , successor ) : return Effect . ambiguous edges = graph . get_edge_data ( predecessor , successor ) edge_key , edge_relation , _ = rank_edges ( edges ) relation = graph [ predecessor ] [ successor ] [ edge_key ] [ RELATION ] # Returns Effect.no_effect if there is a non causal edge in path if relation not in relationship_dict or relationship_dict [ relation ] == 0 : return Effect . no_effect causal_effect . append ( relationship_dict [ relation ] ) final_effect = reduce ( lambda x , y : x * y , causal_effect ) return Effect . activation if final_effect == 1 else Effect . inhibition
10038	def pick_coda_from_letter ( letter ) : try : __ , __ , coda = split_phonemes ( letter , onset = False , nucleus = False , coda = True ) except ValueError : return None else : return coda
8246	def morguefile ( query , n = 10 , top = 10 ) : from web import morguefile images = morguefile . search ( query ) [ : top ] path = choice ( images ) . download ( thumbnail = True , wait = 10 ) return ColorList ( path , n , name = query )
13061	def get_passage ( self , objectId , subreference ) : passage = self . resolver . getTextualNode ( textId = objectId , subreference = subreference , metadata = True ) return passage
10666	def stoichiometry_coefficients ( compound , elements ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return [ stoichiometry [ element ] for element in elements ]
1226	def tf_discounted_cumulative_reward ( self , terminal , reward , discount = None , final_reward = 0.0 , horizon = 0 ) : # By default -> take Model's gamma value if discount is None : discount = self . discount # Accumulates discounted (n-step) reward (start new if terminal) def cumulate ( cumulative , reward_terminal_horizon_subtract ) : rew , is_terminal , is_over_horizon , sub = reward_terminal_horizon_subtract return tf . where ( # If terminal, start new cumulation. condition = is_terminal , x = rew , y = tf . where ( # If we are above the horizon length (H) -> subtract discounted value from H steps back. condition = is_over_horizon , x = ( rew + cumulative * discount - sub ) , y = ( rew + cumulative * discount ) ) ) # Accumulates length of episodes (starts new if terminal) def len_ ( cumulative , term ) : return tf . where ( condition = term , # Start counting from 1 after is-terminal signal x = tf . ones ( shape = ( ) , dtype = tf . int32 ) , # Otherwise, increase length by 1 y = cumulative + 1 ) # Reverse, since reward cumulation is calculated right-to-left, but tf.scan only works left-to-right. reward = tf . reverse ( tensor = reward , axis = ( 0 , ) ) # e.g. -1.0 1.0 0.5 0.0 1.0 2.0 terminal = tf . reverse ( tensor = terminal , axis = ( 0 , ) ) # e.g. F T F F F F # Store the steps until end of the episode(s) determined by the input terminal signals (True starts new count). lengths = tf . scan ( fn = len_ , elems = terminal , initializer = 0 ) # e.g. 1 1 2 3 4 5 off_horizon = tf . greater ( lengths , tf . fill ( dims = tf . shape ( lengths ) , value = horizon ) ) # e.g. F F F F T T # Calculate the horizon-subtraction value for each step. if horizon > 0 : horizon_subtractions = tf . map_fn ( lambda x : ( discount ** horizon ) * x , reward , dtype = tf . float32 ) # Shift right by size of horizon (fill rest with 0.0). horizon_subtractions = tf . concat ( [ np . zeros ( shape = ( horizon , ) ) , horizon_subtractions ] , axis = 0 ) horizon_subtractions = tf . slice ( horizon_subtractions , begin = ( 0 , ) , size = tf . shape ( reward ) ) # e.g. 0.0, 0.0, 0.0, -1.0*g^3, 1.0*g^3, 0.5*g^3 # all 0.0 if infinite horizon (special case: horizon=0) else : horizon_subtractions = tf . zeros ( shape = tf . shape ( reward ) ) # Now do the scan, each time summing up the previous step (discounted by gamma) and # subtracting the respective `horizon_subtraction`. reward = tf . scan ( fn = cumulate , elems = ( reward , terminal , off_horizon , horizon_subtractions ) , initializer = final_reward if horizon != 1 else 0.0 ) # Re-reverse again to match input sequences. return tf . reverse ( tensor = reward , axis = ( 0 , ) )
8604	def update_user ( self , user_id , * * kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps ( data ) ) return response
1421	def load ( file_object ) : marshaller = JavaObjectUnmarshaller ( file_object ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
10249	def highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None , color : Optional [ str ] = None ) : color = color or NODE_HIGHLIGHT_DEFAULT_COLOR for node in nodes if nodes is not None else graph : graph . node [ node ] [ NODE_HIGHLIGHT ] = color
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
13602	def warn_message ( self , message , fh = None , prefix = "[warn]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stdout if fh is sys . stdout : termcolor . cprint ( msg , color = "yellow" ) else : fh . write ( msg ) pass
2690	def new_compiler ( * args , * * kwargs ) : make_silent = kwargs . pop ( 'silent' , True ) cc = _new_compiler ( * args , * * kwargs ) # If MSVC10, initialize the compiler here and add /MANIFEST to linker flags. # See Python issue 4431 (https://bugs.python.org/issue4431) if is_msvc ( cc ) : from distutils . msvc9compiler import get_build_version if get_build_version ( ) == 10 : cc . initialize ( ) for ldflags in [ cc . ldflags_shared , cc . ldflags_shared_debug ] : unique_extend ( ldflags , [ '/MANIFEST' ] ) # If MSVC14, do not silence. As msvc14 requires some custom # steps before the process is spawned, we can't monkey-patch this. elif get_build_version ( ) == 14 : make_silent = False # monkey-patch compiler to suppress stdout and stderr. if make_silent : cc . spawn = _CCompiler_spawn_silent return cc
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
2151	def list ( self , all_pages = False , * * kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . list ( all_pages = all_pages , * * kwargs )
3112	def locked_put ( self , credentials ) : serialized = credentials . to_json ( ) self . _dictionary [ self . _key ] = serialized
8298	def readLong ( data ) : high , low = struct . unpack ( ">ll" , data [ 0 : 8 ] ) big = ( long ( high ) << 32 ) + low rest = data [ 8 : ] return ( big , rest )
11700	def spawn ( self , generations ) : egg_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\nGENERATION %d\n" % ( i + 1 ) ) gen_xx = [ ] gen_xy = [ ] for egg_donor in egg_donors : sperm_donor = random . choice ( sperm_donors ) brood = self . breed ( egg_donor , sperm_donor ) for child in brood : if child . divinity > human : # divine offspring join the Pantheon self . add_god ( child ) if child . chromosomes == 'XX' : gen_xx . append ( child ) else : gen_xy . append ( child ) # elder gods leave the breeding pool egg_donors = [ ed for ed in egg_donors if ed . generation > ( i - 2 ) ] sperm_donors = [ sd for sd in sperm_donors if sd . generation > ( i - 3 ) ] # mature offspring join the breeding pool egg_donors += gen_xx sperm_donors += gen_xy
7485	def concat_reads ( data , subsamples , ipyclient ) : ## concatenate reads if they come from merged assemblies. if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : ## run on single engine for now start = time . time ( ) printstr = " concatenating inputs | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs ## wait for all to finish while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break ## collect results, which are concat file handles. for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) #exception() LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : ## just copy fastqs handles to concat attribute sample . files . concat = sample . files . fastqs return subsamples
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
2956	def load ( self ) : try : with open ( self . _state_file ) as f : state = yaml . safe_load ( f ) self . _containers = state [ 'containers' ] except ( IOError , OSError ) as err : if err . errno == errno . ENOENT : raise NotInitializedError ( "No blockade exists in this context" ) raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) ) except Exception as err : raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) )
3318	def get ( self , token ) : self . _lock . acquire_read ( ) try : lock = self . _dict . get ( token ) if lock is None : # Lock not found: purge dangling URL2TOKEN entries _logger . debug ( "Lock purged dangling: {}" . format ( token ) ) self . delete ( token ) return None expire = float ( lock [ "expire" ] ) if expire >= 0 and expire < time . time ( ) : _logger . debug ( "Lock timed-out({}): {}" . format ( expire , lock_string ( lock ) ) ) self . delete ( token ) return None return lock finally : self . _lock . release ( )
7954	def starttls ( self , * * kwargs ) : with self . lock : self . event ( TLSConnectingEvent ( ) ) self . _write_queue . append ( StartTLS ( * * kwargs ) ) self . _write_queue_cond . notify ( )
1880	def PSRLQ ( cpu , dest , src ) : count = src . read ( ) count = Operators . ITEBV ( src . size , Operators . UGT ( count , 63 ) , 64 , count ) count = Operators . EXTRACT ( count , 0 , 64 ) if dest . size == 64 : dest . write ( dest . read ( ) >> count ) else : hi = Operators . EXTRACT ( dest . read ( ) , 64 , 64 ) >> count low = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) >> count dest . write ( Operators . CONCAT ( 128 , hi , low ) )
12858	def from_date ( datetime_date ) : return BusinessDate . from_ymd ( datetime_date . year , datetime_date . month , datetime_date . day )
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
12557	def cli ( ) : return VersionedCLI ( cli_name = SF_CLI_NAME , config_dir = SF_CLI_CONFIG_DIR , config_env_var_prefix = SF_CLI_ENV_VAR_PREFIX , commands_loader_cls = SFCommandLoader , help_cls = SFCommandHelp )
5513	def bytes_per_second ( ftp , retr = True ) : tot_bytes = 0 if retr : def request_file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( "retr " + TESTFN ) return conn with contextlib . closing ( request_file ( ) ) as conn : register_memory ( ) stop_at = time . time ( ) + 1.0 while stop_at > time . time ( ) : chunk = conn . recv ( BUFFER_LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request_file ( ) stop_at += time . time ( ) - a tot_bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER_LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error_temp , ftplib . error_perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( "STOR " + TESTFN ) ) as conn : register_memory ( ) chunk = b'x' * BUFFER_LEN stop_at = time . time ( ) + 1 while stop_at > time . time ( ) : tot_bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot_bytes
12	def smooth ( y , radius , mode = 'two_sided' , valid_only = False ) : assert mode in ( 'two_sided' , 'causal' ) if len ( y ) < 2 * radius + 1 : return np . ones_like ( y ) * y . mean ( ) elif mode == 'two_sided' : convkernel = np . ones ( 2 * radius + 1 ) out = np . convolve ( y , convkernel , mode = 'same' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'same' ) if valid_only : out [ : radius ] = out [ - radius : ] = np . nan elif mode == 'causal' : convkernel = np . ones ( radius ) out = np . convolve ( y , convkernel , mode = 'full' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'full' ) out = out [ : - radius + 1 ] if valid_only : out [ : radius ] = np . nan return out
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
4389	def adsGetLocalAddressEx ( port ) : # type: (int) -> AmsAddr get_local_address_ex = _adsDLL . AdsGetLocalAddressEx ams_address_struct = SAmsAddr ( ) error_code = get_local_address_ex ( port , ctypes . pointer ( ams_address_struct ) ) if error_code : raise ADSError ( error_code ) local_ams_address = AmsAddr ( ) local_ams_address . _ams_addr = ams_address_struct return local_ams_address
7753	def route_stanza ( self , stanza ) : if stanza . stanza_type not in ( "error" , "result" ) : response = stanza . make_error_response ( u"recipient-unavailable" ) self . send ( response ) return True
12927	def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
2263	def dict_take ( dict_ , keys , default = util_const . NoParam ) : if default is util_const . NoParam : for key in keys : yield dict_ [ key ] else : for key in keys : yield dict_ . get ( key , default )
9929	def authenticate ( username , password , service = 'login' , encoding = 'utf-8' , resetcred = True ) : if sys . version_info >= ( 3 , ) : if isinstance ( username , str ) : username = username . encode ( encoding ) if isinstance ( password , str ) : password = password . encode ( encoding ) if isinstance ( service , str ) : service = service . encode ( encoding ) @ conv_func def my_conv ( n_messages , messages , p_response , app_data ) : """Simple conversation function that responds to any prompt where the echo is off with the supplied password""" # Create an array of n_messages response objects addr = calloc ( n_messages , sizeof ( PamResponse ) ) p_response [ 0 ] = cast ( addr , POINTER ( PamResponse ) ) for i in range ( n_messages ) : if messages [ i ] . contents . msg_style == PAM_PROMPT_ECHO_OFF : pw_copy = strdup ( password ) p_response . contents [ i ] . resp = cast ( pw_copy , c_char_p ) p_response . contents [ i ] . resp_retcode = 0 return 0 handle = PamHandle ( ) conv = PamConv ( my_conv , 0 ) retval = pam_start ( service , username , byref ( conv ) , byref ( handle ) ) if retval != 0 : # TODO: This is not an authentication error, something # has gone wrong starting up PAM return False retval = pam_authenticate ( handle , 0 ) auth_success = ( retval == 0 ) # Re-initialize credentials (for Kerberos users, etc) # Don't check return code of pam_setcred(), it shouldn't matter # if this fails if auth_success and resetcred : retval = pam_setcred ( handle , PAM_REINITIALIZE_CRED ) pam_end ( handle , retval ) return auth_success
11091	def select_file ( self , filters = all_true , recursive = True ) : for p in self . select ( filters , recursive ) : if p . is_file ( ) : yield p
2959	def _state_delete ( self ) : try : os . remove ( self . _state_file ) except OSError as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . _state_dir ) except OSError as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise
12793	def get ( self , url = None , parse_data = True , key = None , parameters = None ) : return self . _fetch ( "GET" , url , post_data = None , parse_data = parse_data , key = key , parameters = parameters )
5700	def _distribution ( gtfs , table , column ) : cur = gtfs . conn . cursor ( ) cur . execute ( 'SELECT {column}, count(*) ' 'FROM {table} GROUP BY {column} ' 'ORDER BY {column}' . format ( column = column , table = table ) ) return ' ' . join ( '%s:%s' % ( t , c ) for t , c in cur )
1708	def connect ( command , data = None , env = None , cwd = None ) : # TODO: support piped commands command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
3316	def get_domain_realm ( self , path_info , environ ) : realm = self . _calc_realm_from_path_provider ( path_info , environ ) return realm
7297	def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model_field , ObjectIdField ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
10313	def calculate_betweenness_centality ( graph : BELGraph , number_samples : int = CENTRALITY_SAMPLES ) -> Counter : try : res = nx . betweenness_centrality ( graph , k = number_samples ) except Exception : res = nx . betweenness_centrality ( graph ) return Counter ( res )
12503	def _smooth_array ( arr , affine , fwhm = None , ensure_finite = True , copy = True , * * kwargs ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : # We don't need crazy precision arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) if ensure_finite : # SPM tends to put NaNs in the data outside the brain arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 if fwhm == 'fast' : arr = _fast_smooth_array ( arr ) elif fwhm is not None : # Keep only the scale part. affine = affine [ : 3 , : 3 ] # Convert from a FWHM to a sigma: fwhm_over_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_over_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n , * * kwargs ) return arr
8901	def authenticate_credentials ( self , userargs , password , request = None ) : credentials = { 'password' : password } if "=" not in userargs : # if it doesn't seem to be in querystring format, just use it as the username credentials [ get_user_model ( ) . USERNAME_FIELD ] = userargs else : # parse out the user args from querystring format into the credentials dict for arg in userargs . split ( "&" ) : key , val = arg . split ( "=" ) credentials [ key ] = val # authenticate the user via Django's auth backends user = authenticate ( * * credentials ) if user is None : raise exceptions . AuthenticationFailed ( 'Invalid credentials.' ) if not user . is_active : raise exceptions . AuthenticationFailed ( 'User inactive or deleted.' ) return ( user , None )
10786	def add_subtract ( st , max_iter = 7 , max_npart = 'calc' , max_mem = 2e8 , always_check_remove = False , * * kwargs ) : if max_npart == 'calc' : max_npart = 0.05 * st . obj_get_positions ( ) . shape [ 0 ] total_changed = 0 _change_since_opt = 0 removed_poses = [ ] added_poses0 = [ ] added_poses = [ ] nr = 1 # Check removal on the first loop for _ in range ( max_iter ) : if ( nr != 0 ) or ( always_check_remove ) : nr , rposes = remove_bad_particles ( st , * * kwargs ) na , aposes = add_missing_particles ( st , * * kwargs ) current_changed = na + nr removed_poses . extend ( rposes ) added_poses0 . extend ( aposes ) total_changed += current_changed _change_since_opt += current_changed if current_changed == 0 : break elif _change_since_opt > max_npart : _change_since_opt *= 0 CLOG . info ( 'Start add_subtract optimization.' ) opt . do_levmarq ( st , opt . name_globals ( st , remove_params = st . get ( 'psf' ) . params ) , max_iter = 1 , run_length = 4 , num_eig_dirs = 3 , max_mem = max_mem , eig_update_frequency = 2 , rz_order = 0 , use_accel = True ) CLOG . info ( 'After optimization:\t{:.6}' . format ( st . error ) ) # Optimize the added particles' radii: for p in added_poses0 : i = st . obj_closest_particle ( p ) opt . do_levmarq_particles ( st , np . array ( [ i ] ) , max_iter = 2 , damping = 0.3 ) added_poses . append ( st . obj_get_positions ( ) [ i ] ) return total_changed , np . array ( removed_poses ) , np . array ( added_poses )
2271	def _win32_symlink ( path , link , verbose = 0 ) : from ubelt import util_cmd if os . path . isdir ( path ) : # directory symbolic link if verbose : print ( '... as directory symlink' ) command = 'mklink /D "{}" "{}"' . format ( link , path ) # Using the win32 API seems to result in privilege errors # but using shell commands does not have this problem. Weird. # jwfs.symlink(path, link, target_is_directory=True) # TODO: what do we need to do to use the windows api instead of shell? else : # file symbolic link if verbose : print ( '... as file symlink' ) command = 'mklink "{}" "{}"' . format ( link , path ) if command is not None : info = util_cmd . cmd ( command , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format permission_msg = 'You do not have sufficient privledges' if permission_msg not in info [ 'err' ] : print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) return link
5067	def get_cache_key ( * * kwargs ) : key = '__' . join ( [ '{}:{}' . format ( item , value ) for item , value in iteritems ( kwargs ) ] ) return hashlib . md5 ( key . encode ( 'utf-8' ) ) . hexdigest ( )
8163	def _set_mode ( self , mode ) : if mode == CENTER : self . _call_transform_mode = self . _center_transform elif mode == CORNER : self . _call_transform_mode = self . _corner_transform else : raise ValueError ( 'mode must be CENTER or CORNER' )
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) # create space for all features and read from subjects try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
6008	def load_noise_map ( noise_map_path , noise_map_hdu , pixel_scale , image , background_noise_map , exposure_time_map , convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map , convert_from_electrons , gain , convert_from_adus ) : noise_map_options = sum ( [ convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map ] ) if noise_map_options > 1 : raise exc . DataException ( 'You have specified more than one method to load the noise_map map, e.g.:' 'convert_noise_map_from_weight_map | ' 'convert_noise_map_from_inverse_noise_map |' 'noise_map_from_image_and_background_noise_map' ) if noise_map_options == 0 and noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = noise_map_path , hdu = noise_map_hdu , pixel_scale = pixel_scale ) elif convert_noise_map_from_weight_map and noise_map_path is not None : weight_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_noise_map_from_inverse_noise_map and noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) elif noise_map_from_image_and_background_noise_map : if background_noise_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a ' 'background noise_map map is not supplied.' ) if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a' 'gain is not supplied to convert from adus' ) return NoiseMap . from_image_and_background_noise_map ( pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) else : raise exc . DataException ( 'A noise_map map was not loaded, specify a noise_map_path or option to compute a noise_map map.' )
10708	def delete_vacation ( _id ) : arequest = requests . delete ( VACATIONS_URL + "/" + _id , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "Failed to delete vacation. " + status_code ) return False return True
9575	def read_elements ( fd , endian , mtps , is_name = False ) : mtpn , num_bytes , data = read_element_tag ( fd , endian ) if mtps and mtpn not in [ etypes [ mtp ] [ 'n' ] for mtp in mtps ] : raise ParseError ( 'Got type {}, expected {}' . format ( mtpn , ' / ' . join ( '{} ({})' . format ( etypes [ mtp ] [ 'n' ] , mtp ) for mtp in mtps ) ) ) if not data : # full format, read data data = fd . read ( num_bytes ) # Seek to next 64-bit boundary mod8 = num_bytes % 8 if mod8 : fd . seek ( 8 - mod8 , 1 ) # parse data and return values if is_name : # names are stored as miINT8 bytes fmt = 's' val = [ unpack ( endian , fmt , s ) for s in data . split ( b'\0' ) if s ] if len ( val ) == 0 : val = '' elif len ( val ) == 1 : val = asstr ( val [ 0 ] ) else : val = [ asstr ( s ) for s in val ] else : fmt = etypes [ inv_etypes [ mtpn ] ] [ 'fmt' ] val = unpack ( endian , fmt , data ) return val
12828	def log_error ( self , text : str ) -> None : if self . log_errors : with self . _log_fp . open ( 'a+' ) as log_file : log_file . write ( f'{text}\n' )
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
7071	def recall ( ntp , nfn ) : if ( ntp + nfn ) > 0 : return ntp / ( ntp + nfn ) else : return np . nan
4607	def blacklist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
1314	def ControlFromPoint2 ( x : int , y : int ) -> Control : return Control . CreateControlFromElement ( _AutomationClient . instance ( ) . IUIAutomation . ElementFromHandle ( WindowFromPoint ( x , y ) ) )
5014	def filter_queryset ( self , request , queryset , view ) : if not request . user . is_staff : filter_kwargs = { view . USER_ID_FILTER : request . user . id } queryset = queryset . filter ( * * filter_kwargs ) return queryset
2462	def set_file_spdx_id ( self , doc , spdx_id ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_spdx_id_set : self . file_spdx_id_set = True if validations . validate_file_spdx_id ( spdx_id ) : self . file ( doc ) . spdx_id = spdx_id return True else : raise SPDXValueError ( 'File::SPDXID' ) else : raise CardinalityError ( 'File::SPDXID' ) else : raise OrderError ( 'File::SPDXID' )
10178	def list_bookmarks ( self , start_date = None , end_date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : query = query . filter ( 'range' , date = range_args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )
5370	def _load_file_from_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get_media ( bucket = bucket_name , object = object_name ) file_handle = io . BytesIO ( ) downloader = MediaIoBaseDownload ( file_handle , request , chunksize = 1024 * 1024 ) done = False while not done : _ , done = _downloader_next_chunk ( downloader ) filevalue = file_handle . getvalue ( ) if not isinstance ( filevalue , six . string_types ) : filevalue = filevalue . decode ( ) return six . StringIO ( filevalue )
10502	def waitForValueToChange ( self , timeout = 10 ) : # Want to identify that the element whose value changes matches this # object's. Unique identifiers considered include role and position # This seems to work best if you set the notification at the application # level callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXValueChanged' , callback = callback , args = ( retelem , ) )
7489	def importvcf ( vcffile , locifile ) : try : ## Get names of all individuals in the vcf with open ( invcffile , 'r' ) as invcf : for line in invcf : if line . split ( ) [ 0 ] == "#CHROM" : ## This is maybe a little clever. The names in the vcf are everything after ## the "FORMAT" column, so find that index, then slice everything after it. names_col = line . split ( ) . index ( "FORMAT" ) + 1 names = line . split ( ) [ names_col : ] LOGGER . debug ( "Got names - %s" , names ) break print ( "wat" ) ## Get the column to start reading at except Exception : print ( "wat" )
1290	def tf_step ( self , time , variables , * * kwargs ) : fn_loss = kwargs [ "fn_loss" ] if variables is None : variables = tf . trainable_variables return tf . gradients ( fn_loss , variables )
6935	def add_cmds_cpdir ( cpdir , cmdpkl , cpfileglob = 'checkplot*.pkl*' , require_cmd_magcolor = True , save_cmd_pngs = False ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
11368	def punctuate_authorname ( an ) : name = an . strip ( ) parts = [ x for x in name . split ( ',' ) if x != '' ] ret_str = '' for idx , part in enumerate ( parts ) : subparts = part . strip ( ) . split ( ' ' ) for sidx , substr in enumerate ( subparts ) : ret_str += substr if len ( substr ) == 1 : ret_str += '.' if sidx < ( len ( subparts ) - 1 ) : ret_str += ' ' if idx < ( len ( parts ) - 1 ) : ret_str += ', ' return ret_str . strip ( )
6267	def set_time ( self , value : float ) : if value < 0 : value = 0 self . offset += self . get_time ( ) - value
11212	def _hash ( secret : bytes , data : bytes , alg : str ) -> bytes : algorithm = get_algorithm ( alg ) return hmac . new ( secret , msg = data , digestmod = algorithm ) . digest ( )
10002	def add_path ( self , nodes , * * attr ) : if nx . __version__ [ 0 ] == "1" : return super ( ) . add_path ( nodes , * * attr ) else : return nx . add_path ( self , nodes , * * attr )
1824	def SETZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
6883	def read_csvlc ( lcfile ) : # read in the file and split by lines if '.gz' in os . path . basename ( lcfile ) : LOGINFO ( 'reading gzipped HATLC: %s' % lcfile ) infd = gzip . open ( lcfile , 'rb' ) else : LOGINFO ( 'reading HATLC: %s' % lcfile ) infd = open ( lcfile , 'rb' ) # this transparently reads LCC CSVLCs lcformat_check = infd . read ( 12 ) . decode ( ) if 'LCC-CSVLC' in lcformat_check : infd . close ( ) return read_lcc_csvlc ( lcfile ) else : infd . seek ( 0 ) # below is reading the HATLC v2 CSV LCs lctext = infd . read ( ) . decode ( ) # argh Python 3 infd . close ( ) # figure out the header and get the LC columns lcstart = lctext . index ( '# LIGHTCURVE\n' ) lcheader = lctext [ : lcstart + 12 ] lccolumns = lctext [ lcstart + 13 : ] . split ( '\n' ) lccolumns = [ x for x in lccolumns if len ( x ) > 0 ] # initialize the lcdict and parse the CSV header lcdict = _parse_csv_header ( lcheader ) # tranpose the LC rows into columns lccolumns = [ x . split ( ',' ) for x in lccolumns ] lccolumns = list ( zip ( * lccolumns ) ) # argh more Python 3 # write the columns to the dict for colind , col in enumerate ( lcdict [ 'columns' ] ) : if ( col . split ( '_' ) [ 0 ] in LC_MAG_COLUMNS or col . split ( '_' ) [ 0 ] in LC_ERR_COLUMNS or col . split ( '_' ) [ 0 ] in LC_FLAG_COLUMNS ) : lcdict [ col ] = np . array ( [ _smartcast ( x , COLUMNDEFS [ col . split ( '_' ) [ 0 ] ] [ 2 ] ) for x in lccolumns [ colind ] ] ) elif col in COLUMNDEFS : lcdict [ col ] = np . array ( [ _smartcast ( x , COLUMNDEFS [ col ] [ 2 ] ) for x in lccolumns [ colind ] ] ) else : LOGWARNING ( 'lcdict col %s has no formatter available' % col ) continue return lcdict
3239	def get_group ( group_name , users = True , client = None , * * kwargs ) : # First, make the initial call to get the details for the group: result = client . get_group ( GroupName = group_name , * * kwargs ) # If we care about the user details, then fetch them: if users : if result . get ( 'IsTruncated' ) : kwargs_to_send = { 'GroupName' : group_name } kwargs_to_send . update ( kwargs ) user_list = result [ 'Users' ] kwargs_to_send [ 'Marker' ] = result [ 'Marker' ] result [ 'Users' ] = user_list + _get_users_for_group ( client , * * kwargs_to_send ) else : result . pop ( 'Users' , None ) result . pop ( 'IsTruncated' , None ) result . pop ( 'Marker' , None ) return result
13722	def log ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if re . match ( "file://" , url ) : self . log_file ( url ) elif re . match ( "https://" , url ) or re . match ( "http://" , url ) : self . log_post ( url , credentials , do_verify_certificate ) else : self . log_stdout ( )
3236	def list_objects_in_bucket ( * * kwargs ) : bucket = get_bucket ( * * kwargs ) if bucket : return [ o for o in bucket . list_blobs ( ) ] else : return None
12111	def save ( self , filename , metadata = { } , * * data ) : intersection = set ( metadata . keys ( ) ) & set ( data . keys ( ) ) if intersection : msg = 'Key(s) overlap between data and metadata: %s' raise Exception ( msg % ',' . join ( intersection ) )
6009	def load_background_noise_map ( background_noise_map_path , background_noise_map_hdu , pixel_scale , convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ) : background_noise_map_options = sum ( [ convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ] ) if background_noise_map_options == 0 and background_noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = background_noise_map_path , hdu = background_noise_map_hdu , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_weight_map and background_noise_map_path is not None : weight_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_inverse_noise_map and background_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
6233	def points_random_3d ( count , range_x = ( - 10.0 , 10.0 ) , range_y = ( - 10.0 , 10.0 ) , range_z = ( - 10.0 , 10.0 ) , seed = None ) -> VAO : random . seed ( seed ) def gen ( ) : for _ in range ( count ) : yield random . uniform ( * range_x ) yield random . uniform ( * range_y ) yield random . uniform ( * range_z ) data = numpy . fromiter ( gen ( ) , count = count * 3 , dtype = numpy . float32 ) vao = VAO ( "geometry:points_random_3d" , mode = moderngl . POINTS ) vao . buffer ( data , '3f' , [ 'in_position' ] ) return vao
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
12107	def cross_check_launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root_directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root_directory = launcher . get_root_directory ( ) if os . path . isdir ( root_directory ) : raise Exception ( "Root directory already exists: %r" % root_directory ) if root_directory in root_directories : raise Exception ( "Each launcher requires a unique root directory" ) root_directories . append ( root_directory )
13231	def get_def_macros ( tex_source ) : macros = { } for match in DEF_PATTERN . finditer ( tex_source ) : macros [ match . group ( 'name' ) ] = match . group ( 'content' ) return macros
1001	def printComputeEnd ( self , output , learn = False ) : if self . verbosity >= 3 : print "----- computeEnd summary: " print "learn:" , learn print "numBurstingCols: %s, " % ( self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , print "curPredScore2: %s, " % ( self . _internalStats [ 'curPredictionScore2' ] ) , print "curFalsePosScore: %s, " % ( self . _internalStats [ 'curFalsePositiveScore' ] ) , print "1-curFalseNegScore: %s, " % ( 1 - self . _internalStats [ 'curFalseNegativeScore' ] ) print "numSegments: " , self . getNumSegments ( ) , print "avgLearnedSeqLength: " , self . avgLearnedSeqLength print "----- infActiveState (%d on) ------" % ( self . infActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infActiveState [ 't' ] ) print "----- infPredictedState (%d on)-----" % ( self . infPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infPredictedState [ 't' ] ) print "----- lrnActiveState (%d on) ------" % ( self . lrnActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnActiveState [ 't' ] ) print "----- lrnPredictedState (%d on)-----" % ( self . lrnPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnPredictedState [ 't' ] ) print "----- cellConfidence -----" self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) if self . verbosity >= 6 : self . printConfidence ( self . cellConfidence [ 't' ] ) print "----- colConfidence -----" self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) print "----- cellConfidence[t-1] for currently active cells -----" cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] self . printActiveIndices ( cc , andValues = True ) if self . verbosity == 4 : print "Cells, predicted segments only:" self . printCells ( predictedOnly = True ) elif self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) print elif self . verbosity >= 1 : print "TM: learn:" , learn print "TM: active outputs(%d):" % len ( output . nonzero ( ) [ 0 ] ) , self . printActiveIndices ( output . reshape ( self . numberOfCols , self . cellsPerColumn ) )
9897	def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
5998	def plot_grid ( grid_arcsec , array , units , kpc_per_arcsec , pointsize , zoom_offset_arcsec ) : if grid_arcsec is not None : if zoom_offset_arcsec is not None : grid_arcsec -= zoom_offset_arcsec grid_units = convert_grid_units ( grid_arcsec = grid_arcsec , array = array , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = np . asarray ( grid_units [ : , 0 ] ) , x = np . asarray ( grid_units [ : , 1 ] ) , s = pointsize , c = 'k' )
11276	def disown ( debug ) : # Get the current PID pid = os . getpid ( ) cgroup_file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup_file , "r" ) except IOError : print ( "Could not open cgroup file: " , cgroup_file ) return False # Read each line for line in infile : # Check if the line contains "ardexa.service" if line . find ( "ardexa.service" ) == - 1 : continue # if the lines contains "name=", replace it with nothing line = line . replace ( "name=" , "" ) # Split the line by commas items_list = line . split ( ':' ) accounts = items_list [ 1 ] dir_str = accounts + "/ardexa.disown" # If accounts is empty, continue if not accounts : continue # Create the dir and all subdirs full_dir = "/sys/fs/cgroup/" + dir_str if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) if debug >= 1 : print ( "Making directory: " , full_dir ) else : if debug >= 1 : print ( "Directory already exists: " , full_dir ) # Add the PID to the file full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) # If this item contains a comma, then separate it, and reverse # some OSes will need cpuacct,cpu reversed to actually work if accounts . find ( "," ) != - 1 : acct_list = accounts . split ( ',' ) accounts = acct_list [ 1 ] + "," + acct_list [ 0 ] dir_str = accounts + "/ardexa.disown" # Create the dir and all subdirs. But it may not work. So use a TRY full_dir = "/sys/fs/cgroup/" + dir_str try : if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) except : continue # Add the PID to the file full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) infile . close ( ) # For debug purposes only if debug >= 1 : prog_list = [ "cat" , cgroup_file ] run_program ( prog_list , debug , False ) # If there are any "ardexa.service" in the proc file. If so, exit with error prog_list = [ "grep" , "-q" , "ardexa.service" , cgroup_file ] if run_program ( prog_list , debug , False ) : # There are entries still left in the file return False return True
3394	def find_gene_knockout_reactions ( cobra_model , gene_list , compiled_gene_reaction_rules = None ) : potential_reactions = set ( ) for gene in gene_list : if isinstance ( gene , string_types ) : gene = cobra_model . genes . get_by_id ( gene ) potential_reactions . update ( gene . _reaction ) gene_set = { str ( i ) for i in gene_list } if compiled_gene_reaction_rules is None : compiled_gene_reaction_rules = { r : parse_gpr ( r . gene_reaction_rule ) [ 0 ] for r in potential_reactions } return [ r for r in potential_reactions if not eval_gpr ( compiled_gene_reaction_rules [ r ] , gene_set ) ]
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name # Pooling type AveragePooling2D pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
7068	def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
7018	def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_concat_worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
10424	def infer_missing_two_way_edges ( graph ) : for u , v , k , d in graph . edges ( data = True , keys = True ) : if d [ RELATION ] in TWO_WAY_RELATIONS : infer_missing_backwards_edge ( graph , u , v , k )
7668	def trim ( self , start_time , end_time , strict = False ) : trimmed_array = AnnotationArray ( ) for ann in self : trimmed_array . append ( ann . trim ( start_time , end_time , strict = strict ) ) return trimmed_array
2248	def memoize ( func ) : cache = { } @ functools . wraps ( func ) def memoizer ( * args , * * kwargs ) : key = _make_signature_key ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , * * kwargs ) return cache [ key ] memoizer . cache = cache return memoizer
1420	def _get_scheduler_location_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_scheduler_location_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) # pylint: disable=unused-variable, unused-argument @ self . client . DataWatch ( path ) def watch_scheduler_location ( data , stats ) : """ invoke callback to watch scheduler location """ if data : scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) callback ( scheduler_location ) else : callback ( None ) # Returning False will result in no future watches # being triggered. If isWatching is True, then # the future watches will be triggered. return isWatching
842	def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( "index out of bounds" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId
921	def log ( self , level , msg , * args , * * kwargs ) : self . _baseLogger . log ( self , level , self . getExtendedMsg ( msg ) , * args , * * kwargs )
8172	def limit ( self , max = 30 ) : if abs ( self . vx ) > max : self . vx = self . vx / abs ( self . vx ) * max if abs ( self . vy ) > max : self . vy = self . vy / abs ( self . vy ) * max if abs ( self . vz ) > max : self . vz = self . vz / abs ( self . vz ) * max
7111	def predict ( self , X ) : if isinstance ( X [ 0 ] , list ) : return [ self . estimator . tag ( x ) for x in X ] return self . estimator . tag ( X )
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
1976	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : data = '' if count != 0 : if not self . _is_open ( fd ) : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EBADF" ) return Decree . CGC_EBADF # TODO check count bytes from buf if buf not in cpu . memory : # or not buf+count in cpu.memory: logger . info ( "RECEIVE: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT #import random #count = random.randint(1,count) if fd > 2 and self . files [ fd ] . is_empty ( ) : cpu . PC -= cpu . instruction . size self . wait ( [ fd ] , [ ] , None ) raise RestartSyscall ( ) # get some potential delay # if random.randint(5) == 0 and count > 1: # count = count/2 # Read the data and put it in memory data = self . files [ fd ] . receive ( count ) self . syscall_trace . append ( ( "_receive" , fd , data ) ) cpu . write_bytes ( buf , data ) self . signal_receive ( fd ) # TODO check 4 bytes from rx_bytes if rx_bytes : if rx_bytes not in cpu . memory : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rx_bytes , len ( data ) , 32 ) logger . info ( "RECEIVE(%d, 0x%08x, %d, 0x%08x) -> <%s> (size:%d)" % ( fd , buf , count , rx_bytes , repr ( data ) [ : min ( count , 10 ) ] , len ( data ) ) ) return 0
6441	def sim_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . sim ( src , tar , qval , alphabet )
4578	def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : # For now allow only left and right mouse buttons: mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) # Press the button buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) # Set modflags (default None) on button down: Quartz . CGEventSetFlags ( buttonDown , modFlags ) # Set the click count on button down: Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : # Drag and release the button buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) # Set modflags on the button dragged: Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : # Release the button buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) # Set modflags on the button up: Quartz . CGEventSetFlags ( buttonUp , modFlags ) # Set the click count on button up: Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) # Queue the events self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
4466	def __reconstruct ( params ) : if isinstance ( params , dict ) : if '__class__' in params : cls = params [ '__class__' ] data = __reconstruct ( params [ 'params' ] ) return cls ( * * data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = __reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ __reconstruct ( v ) for v in params ] else : return params
6729	def deploy_code ( self ) : assert self . genv . SITE , 'Site unspecified.' assert self . genv . ROLE , 'Role unspecified.' r = self . local_renderer if self . env . exclusions : r . env . exclusions_str = ' ' . join ( "--exclude='%s'" % _ for _ in self . env . exclusions ) r . local ( r . env . rsync_command ) r . sudo ( 'chown -R {rsync_chown_user}:{rsync_chown_group} {rsync_dst_dir}' )
4685	def unlock_wallet ( self , * args , * * kwargs ) : self . blockchain . wallet . unlock ( * args , * * kwargs ) return self
11363	def download_file ( from_url , to_filename = None , chunk_size = 1024 * 8 , retry_count = 3 ) : if not to_filename : to_filename = get_temporary_file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTPAdapter ( max_retries = retry_count ) session . mount ( from_url , adapter ) response = session . get ( from_url , stream = True ) with open ( to_filename , 'wb' ) as fd : for chunk in response . iter_content ( chunk_size ) : fd . write ( chunk ) return to_filename
5614	def segmentize_geometry ( geometry , segmentize_value ) : if geometry . geom_type != "Polygon" : raise TypeError ( "segmentize geometry type must be Polygon" ) return Polygon ( LinearRing ( [ p # pick polygon linestrings for l in map ( lambda x : LineString ( [ x [ 0 ] , x [ 1 ] ] ) , zip ( geometry . exterior . coords [ : - 1 ] , geometry . exterior . coords [ 1 : ] ) ) # interpolate additional points in between and don't forget end point for p in [ l . interpolate ( segmentize_value * i ) . coords [ 0 ] for i in range ( int ( l . length / segmentize_value ) ) ] + [ l . coords [ 1 ] ] ] ) )
10495	def clickMouseButtonRightWithMods ( self , coord , modifiers ) : modFlags = self . _pressModifiers ( modifiers ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _releaseModifiers ( modifiers ) self . _postQueuedEvents ( )
12176	def plot_shaded_data ( X , Y , variances , varianceX ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) nChunks = int ( len ( Y ) / CHUNK_POINTS ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) varianceIsAboveMin = np . where ( variances >= varLimitLow ) [ 0 ] varianceIsBelowMax = np . where ( variances <= varLimitHigh ) [ 0 ] varianceIsRange = [ chunkNumber for chunkNumber in range ( nChunks ) if chunkNumber in varianceIsAboveMin and chunkNumber in varianceIsBelowMax ] for chunkNumber in varianceIsRange : t1 = chunkNumber * CHUNK_POINTS / POINTS_PER_SEC t2 = t1 + CHUNK_POINTS / POINTS_PER_SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )
2718	def remove_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] # Extracting data from the Droplet object resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __remove_resources ( resources ) return False
2525	def get_reviewer ( self , r_term ) : reviewer_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewer' ] , None ) ) ) if len ( reviewer_list ) != 1 : self . error = True msg = 'Review must have exactly one reviewer' self . logger . log ( msg ) return try : return self . builder . create_entity ( self . doc , six . text_type ( reviewer_list [ 0 ] [ 2 ] ) ) except SPDXValueError : self . value_error ( 'REVIEWER_VALUE' , reviewer_list [ 0 ] [ 2 ] )
2314	def anm_score ( self , x , y ) : gp = GaussianProcessRegressor ( ) . fit ( x , y ) y_predict = gp . predict ( x ) indepscore = normalized_hsic ( y_predict - y , x ) return indepscore
4191	def window_cauchy ( N , alpha = 3 ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = 1. / ( 1. + ( alpha * n / ( N / 2. ) ) ** 2 ) return w
5334	def get_params_parser ( ) : parser = argparse . ArgumentParser ( add_help = False ) parser . add_argument ( '-g' , '--debug' , dest = 'debug' , action = 'store_true' , help = argparse . SUPPRESS ) parser . add_argument ( "--arthur" , action = 'store_true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add_argument ( "--raw" , action = 'store_true' , dest = 'raw' , help = "Activate raw task" ) parser . add_argument ( "--enrich" , action = 'store_true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add_argument ( "--identities" , action = 'store_true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add_argument ( "--panels" , action = 'store_true' , dest = 'panels' , help = "Activate panels task" ) parser . add_argument ( "--cfg" , dest = 'cfg_path' , help = "Configuration file path" ) parser . add_argument ( "--backends" , dest = 'backend_sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) return parser
972	def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] )
4439	async def _playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is_playing : return await ctx . invoke ( self . _play , query = query ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'loadType' ] == 'PLAYLIST_LOADED' : for _track in tracks : player . add ( requester = ctx . author . id , track = _track ) await player . play_now ( requester = ctx . author . id , track = track )
10575	def get_local_playlist_songs ( playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None ) : logger . info ( "Loading local playlist songs..." ) if os . name == 'nt' and CYGPATH_RE . match ( playlist ) : playlist = convert_cygwin_path ( playlist ) filepaths = [ ] base_filepath = os . path . dirname ( os . path . abspath ( playlist ) ) with open ( playlist ) as local_playlist : for line in local_playlist . readlines ( ) : line = line . strip ( ) if line . lower ( ) . endswith ( SUPPORTED_SONG_FORMATS ) : path = line if not os . path . isabs ( path ) : path = os . path . join ( base_filepath , path ) if os . path . isfile ( path ) : filepaths . append ( path ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local playlist songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
10395	def iter_leaves ( self ) -> Iterable [ BaseEntity ] : for node in self . graph : if self . tag in self . graph . nodes [ node ] : continue if not any ( self . tag not in self . graph . nodes [ p ] for p in self . graph . predecessors ( node ) ) : yield node
13803	def _validate_request_code ( self , code , client_id , callback ) : nonce = yield Task ( self . data_store . fetch , 'nonce_codes' , code = code ) if not nonce : raise Proauth2Error ( 'access_denied' , 'invalid request code: %s' % code ) if client_id != nonce [ 'client_id' ] : raise Proauth2Error ( 'access_denied' , 'invalid request code: %s' % code ) user_id = nonce [ 'user_id' ] expires = nonce [ 'expires' ] yield Task ( self . data_store . remove , 'nonce_codes' , code = code , client_id = client_id , user_id = user_id ) if time ( ) > expires : raise Proauth2Error ( 'access_denied' , 'request code %s expired' % code ) callback ( user_id )
12977	def deleteOne ( self , obj , conn = None ) : if not getattr ( obj , '_id' , None ) : return 0 if conn is None : conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) executeAfter = True else : pipeline = conn # In this case, we are inheriting a pipeline executeAfter = False pipeline . delete ( self . _get_key_for_id ( obj . _id ) ) self . _rem_id_from_keys ( obj . _id , pipeline ) for indexedFieldName in self . indexedFields : self . _rem_id_from_index ( indexedFieldName , obj . _id , obj . _origData [ indexedFieldName ] , pipeline ) obj . _id = None if executeAfter is True : pipeline . execute ( ) return 1
29	def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess
10429	def getrowcount ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) return len ( object_handle . AXRows )
1819	def SETO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , 1 , 0 ) )
3688	def solve_T ( self , P , V , quick = True ) : Tc , a , b , kappa0 , kappa1 = self . Tc , self . a , self . b , self . kappa0 , self . kappa1 if quick : x0 = V - b R_x0 = R / x0 x3 = ( 100. * ( V * ( V + b ) + b * x0 ) ) x4 = 10. * kappa0 kappa110 = kappa1 * 10. kappa17 = kappa1 * 7. def to_solve ( T ) : x1 = T / Tc x2 = x1 ** 0.5 return ( T * R_x0 - a * ( ( x4 - ( kappa110 * x1 - kappa17 ) * ( x2 + 1. ) ) * ( x2 - 1. ) - 10. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( ( kappa0 + kappa1 * ( sqrt ( T / Tc ) + 1 ) * ( - T / Tc + 7 / 10 ) ) * ( - sqrt ( T / Tc ) + 1 ) + 1 ) ** 2 / ( V * ( V + b ) + b * ( V - b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
13002	def _filter_cluster_data ( self ) : min_temp = self . temperature_range_slider . value [ 0 ] max_temp = self . temperature_range_slider . value [ 1 ] temp_mask = np . logical_and ( self . cluster . catalog [ 'temperature' ] >= min_temp , self . cluster . catalog [ 'temperature' ] <= max_temp ) min_lum = self . luminosity_range_slider . value [ 0 ] max_lum = self . luminosity_range_slider . value [ 1 ] lum_mask = np . logical_and ( self . cluster . catalog [ 'luminosity' ] >= min_lum , self . cluster . catalog [ 'luminosity' ] <= max_lum ) selected_mask = np . isin ( self . cluster . catalog [ 'id' ] , self . selection_ids ) filter_mask = temp_mask & lum_mask & selected_mask self . filtered_data = self . cluster . catalog [ filter_mask ] . data self . source . data = { 'id' : list ( self . filtered_data [ 'id' ] ) , 'temperature' : list ( self . filtered_data [ 'temperature' ] ) , 'luminosity' : list ( self . filtered_data [ 'luminosity' ] ) , 'color' : list ( self . filtered_data [ 'color' ] ) } logging . debug ( "Selected data is now: %s" , self . filtered_data )
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
5592	def tiles_from_bbox ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_bbox ( geometry , zoom ) : yield self . tile ( * tile . id )
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
3586	def add ( self , cbobject , metadata ) : with self . _lock : if cbobject not in self . _metadata : self . _metadata [ cbobject ] = metadata return self . _metadata [ cbobject ]
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
6480	def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
5951	def strftime ( self , fmt = "%d:%H:%M:%S" ) : substitutions = { "%d" : str ( self . days ) , "%H" : "{0:02d}" . format ( self . dhours ) , "%h" : str ( 24 * self . days + self . dhours ) , "%M" : "{0:02d}" . format ( self . dminutes ) , "%S" : "{0:02d}" . format ( self . dseconds ) , } s = fmt for search , replacement in substitutions . items ( ) : s = s . replace ( search , replacement ) return s
11485	def upload ( file_pattern , destination = 'Private' , leaf_folders_as_items = False , reuse_existing = False ) : session . token = verify_credentials ( ) # Logic for finding the proper folder to place the files in. parent_folder_id = None user_folders = session . communicator . list_user_folders ( session . token ) if destination . startswith ( '/' ) : parent_folder_id = _find_resource_id_from_path ( destination ) else : for cur_folder in user_folders : if cur_folder [ 'name' ] == destination : parent_folder_id = cur_folder [ 'folder_id' ] if parent_folder_id is None : print ( 'Unable to locate specified destination. Defaulting to {0}.' . format ( user_folders [ 0 ] [ 'name' ] ) ) parent_folder_id = user_folders [ 0 ] [ 'folder_id' ] for current_file in glob . iglob ( file_pattern ) : current_file = os . path . normpath ( current_file ) if os . path . isfile ( current_file ) : print ( 'Uploading item from {0}' . format ( current_file ) ) _upload_as_item ( os . path . basename ( current_file ) , parent_folder_id , current_file , reuse_existing ) else : _upload_folder_recursive ( current_file , parent_folder_id , leaf_folders_as_items , reuse_existing )
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
450	def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
647	def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) seqList = [ ] for i in xrange ( nSeq ) : if max ( seqLength ) <= nCoinc : seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) else : len = random . choice ( seqLength ) seq = [ ] for x in xrange ( len ) : seq . append ( random . choice ( coincList ) ) seqList . append ( seq ) return seqList
13711	def invalidate_ip ( self , ip ) : if self . _use_cache : key = self . _make_cache_key ( ip ) self . _cache . delete ( key , version = self . _cache_version )
10005	def clear_descendants ( self , source , clear_source = True ) : removed = self . cellgraph . clear_descendants ( source , clear_source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
11490	def _download_item ( item_id , path = '.' , item = None ) : session . token = verify_credentials ( ) filename , content_iter = session . communicator . download_item ( item_id , session . token ) item_path = os . path . join ( path , filename ) print ( 'Creating file at {0}' . format ( item_path ) ) out_file = open ( item_path , 'wb' ) for block in content_iter : out_file . write ( block ) out_file . close ( ) for callback in session . item_download_callbacks : if not item : item = session . communicator . item_get ( session . token , item_id ) callback ( session . communicator , session . token , item , item_path )
1672	def ProcessFile ( filename , vlevel , extra_check_functions = None ) : _SetVerboseLevel ( vlevel ) _BackupFilters ( ) if not ProcessConfigOverrides ( filename ) : _RestoreFilters ( ) return lf_lines = [ ] crlf_lines = [ ] try : # Support the UNIX convention of using "-" for stdin. Note that # we are not opening the file with universal newline support # (which codecs doesn't support anyway), so the resulting lines do # contain trailing '\r' characters if we are reading a file that # has CRLF endings. # If after the split a trailing '\r' is present, it is removed # below. if filename == '-' : lines = codecs . StreamReaderWriter ( sys . stdin , codecs . getreader ( 'utf8' ) , codecs . getwriter ( 'utf8' ) , 'replace' ) . read ( ) . split ( '\n' ) else : lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\n' ) # Remove trailing '\r'. # The -1 accounts for the extra trailing blank line we get from split() for linenum in range ( len ( lines ) - 1 ) : if lines [ linenum ] . endswith ( '\r' ) : lines [ linenum ] = lines [ linenum ] . rstrip ( '\r' ) crlf_lines . append ( linenum + 1 ) else : lf_lines . append ( linenum + 1 ) except IOError : _cpplint_state . PrintError ( "Skipping input '%s': Can't open for reading\n" % filename ) _RestoreFilters ( ) return # Note, if no dot is found, this will give the entire filename as the ext. file_extension = filename [ filename . rfind ( '.' ) + 1 : ] # When reading from stdin, the extension is unknown, so no cpplint tests # should rely on the extension. if filename != '-' and file_extension not in GetAllExtensions ( ) : # bazel 0.5.1> uses four distinct generated files that gives a warning # we suppress the warning for these files bazel_gen_files = set ( [ "external/local_config_cc/libtool" , "external/local_config_cc/make_hashed_objlist.py" , "external/local_config_cc/wrapped_ar" , "external/local_config_cc/wrapped_clang" , "external/local_config_cc/xcrunwrapper.sh" , ] ) if not filename in bazel_gen_files : _cpplint_state . PrintError ( 'Ignoring %s; not a valid file name ' '(%s)\n' % ( filename , ', ' . join ( GetAllExtensions ( ) ) ) ) else : ProcessFileData ( filename , file_extension , lines , Error , extra_check_functions ) # If end-of-line sequences are a mix of LF and CR-LF, issue # warnings on the lines with CR. # # Don't issue any warnings if all lines are uniformly LF or CR-LF, # since critique can handle these just fine, and the style guide # doesn't dictate a particular end of line sequence. # # We can't depend on os.linesep to determine what the desired # end-of-line sequence should be, since that will return the # server-side end-of-line sequence. if lf_lines and crlf_lines : # Warn on every line with CR. An alternative approach might be to # check whether the file is mostly CRLF or just LF, and warn on the # minority, we bias toward LF here since most tools prefer LF. for linenum in crlf_lines : Error ( filename , linenum , 'whitespace/newline' , 1 , 'Unexpected \\r (^M) found; better to use only \\n' ) _RestoreFilters ( )
9131	def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = _make_session ( ) actions = session . query ( cls ) . order_by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions
11078	def start_timer ( self , duration , func , * args ) : t = threading . Timer ( duration , self . _timer_callback , ( func , args ) ) self . _timer_callbacks [ func ] = t t . start ( ) self . log . info ( "Scheduled call to %s in %ds" , func . __name__ , duration )
13753	def prepare_path ( path ) : if type ( path ) == list : return os . path . join ( * path ) return path
3747	def calculate ( self , T , P , zs , ws , method ) : if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
2829	def convert_upsample_bilinear ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) output_size = params [ 'output_size' ] align_corners = params [ 'align_corners' ] > 0 def target_layer ( x , size = output_size , align_corners = align_corners ) : import tensorflow as tf x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ] ) x = tf . image . resize_images ( x , size , align_corners = align_corners ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ] ) return x lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
551	def __checkMaturity ( self ) : if self . _currentRecordIndex + 1 < self . _MIN_RECORDS_TO_BE_BEST : return # If we are already mature, don't need to check anything if self . _isMature : return metric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _metricRegression . addPoint ( x = self . _currentRecordIndex , y = metric ) # Perform a linear regression to see if the error is leveled off #pctChange = self._metricRegression.getPctChange() #if pctChange is not None and abs(pctChange ) <= self._MATURITY_MAX_CHANGE: pctChange , absPctChange = self . _metricRegression . getPctChanges ( ) if pctChange is not None and absPctChange <= self . _MATURITY_MAX_CHANGE : self . _jobsDAO . modelSetFields ( self . _modelID , { 'engMatured' : True } ) # TODO: Don't stop if we are currently the best model. Also, if we # are still running after maturity, we have to periodically check to # see if we are still the best model. As soon we lose to some other # model, then we should stop at that point. self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isMature = True self . _logger . info ( "Model %d has matured (pctChange=%s, n=%d). \n" "Scores = %s\n" "Stopping execution" , self . _modelID , pctChange , self . _MATURITY_NUM_POINTS , self . _metricRegression . _window )
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
10465	def _getRunningApps ( cls ) : def runLoopAndExit ( ) : AppHelper . stopEventLoop ( ) AppHelper . callLater ( 1 , runLoopAndExit ) AppHelper . runConsoleEventLoop ( ) # Get a list of running applications ws = AppKit . NSWorkspace . sharedWorkspace ( ) apps = ws . runningApplications ( ) return apps
6197	def numeric_params ( self ) : nparams = dict ( D = ( self . diffusion_coeff . mean ( ) , 'Diffusion coefficient (m^2/s)' ) , np = ( self . num_particles , 'Number of simulated particles' ) , t_step = ( self . t_step , 'Simulation time-step (s)' ) , t_max = ( self . t_max , 'Simulation total time (s)' ) , ID = ( self . ID , 'Simulation ID (int)' ) , EID = ( self . EID , 'IPython Engine ID (int)' ) , pico_mol = ( self . concentration ( ) * 1e12 , 'Particles concentration (pM)' ) ) return nparams
12394	def try_delegation ( method ) : @ functools . wraps ( method ) def delegator ( self , * args , * * kwargs ) : if self . try_delegation : # Try to dispatch to the instance's implementation. inst = getattr ( self , 'inst' , None ) if inst is not None : method_name = ( self . delegator_prefix or '' ) + method . __name__ func = getattr ( inst , method_name , None ) if func is not None : return func ( * args , * * kwargs ) # Otherwise run the decorated func. return method ( self , * args , * * kwargs ) return delegator
3381	def shared_np_array ( shape , data = None , integer = False ) : size = np . prod ( shape ) if integer : array = Array ( ctypes . c_int64 , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) , dtype = "int64" ) else : array = Array ( ctypes . c_double , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) ) np_array = np_array . reshape ( shape ) if data is not None : if len ( shape ) != len ( data . shape ) : raise ValueError ( "`data` must have the same dimensions" "as the created array." ) same = all ( x == y for x , y in zip ( shape , data . shape ) ) if not same : raise ValueError ( "`data` must have the same shape" "as the created array." ) np_array [ : ] = data return np_array
4279	def reduce_opacity ( im , opacity ) : assert opacity >= 0 and opacity <= 1 if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) else : im = im . copy ( ) alpha = im . split ( ) [ 3 ] alpha = ImageEnhance . Brightness ( alpha ) . enhance ( opacity ) im . putalpha ( alpha ) return im
6596	def receive_one ( self ) : if self . nruns == 0 : return None ret = self . communicationChannel . receive_one ( ) if ret is not None : self . nruns -= 1 return ret
10773	def add_node ( self , node , offset ) : # calculate x,y from offset considering axis start and end points width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
8206	def overlap ( self , x1 , y1 , x2 , y2 , r = 5 ) : if abs ( x2 - x1 ) < r and abs ( y2 - y1 ) < r : return True else : return False
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
9931	def get_repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . __module__ ) + "." + objtype . __name__ prettytype = typename . replace ( "__builtin__." , "" ) name = getattr ( obj , "__name__" , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get_refkey ( obj , referent ) url = reverse ( 'dowser_trace_object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get_repr ( obj , 100 ) ) )
5011	def _call_post_with_session ( self , url , payload ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : # Create a new session with a valid token self . session . close ( ) self . _create_session ( ) response = self . session . post ( url , data = payload ) return response . status_code , response . text
2171	def unified_job_template_options ( method ) : jt_dec = click . option ( '--job-template' , type = types . Related ( 'job_template' ) , help = 'Use this job template as unified_job_template field' ) prj_dec = click . option ( '--project' , type = types . Related ( 'project' ) , help = 'Use this project as unified_job_template field' ) inv_src_dec = click . option ( '--inventory-source' , type = types . Related ( 'inventory_source' ) , help = 'Use this inventory source as unified_job_template field' ) def ujt_translation ( _method ) : def _ujt_translation ( * args , * * kwargs ) : for fd in [ 'job_template' , 'project' , 'inventory_source' ] : if fd in kwargs and kwargs [ fd ] is not None : kwargs [ 'unified_job_template' ] = kwargs . pop ( fd ) return _method ( * args , * * kwargs ) return functools . wraps ( _method ) ( _ujt_translation ) return ujt_translation ( inv_src_dec ( prj_dec ( jt_dec ( method ) ) ) )
11008	def get_project_slug ( self , bet ) : if bet . get ( 'form_params' ) : params = json . loads ( bet [ 'form_params' ] ) return params . get ( 'project' ) return None
11793	def forward_checking ( csp , var , value , assignment , removals ) : for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr_domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr_domains [ B ] : return False return True
5517	def limit ( self , value ) : self . _limit = value self . _start = None self . _sum = 0
7750	def __try_handlers ( self , handler_list , stanza , stanza_type = None ) : # pylint: disable=W0212 if stanza_type is None : stanza_type = stanza . stanza_type payload = stanza . get_all_payload ( ) classes = [ p . __class__ for p in payload ] keys = [ ( p . __class__ , p . handler_key ) for p in payload ] for handler in handler_list : type_filter = handler . _pyxmpp_stanza_handled [ 1 ] class_filter = handler . _pyxmpp_payload_class_handled extra_filter = handler . _pyxmpp_payload_key if type_filter != stanza_type : continue if class_filter : if extra_filter is None and class_filter not in classes : continue if extra_filter and ( class_filter , extra_filter ) not in keys : continue response = handler ( stanza ) if self . _process_handler_result ( response ) : return True return False
7884	def _make_ns_declarations ( declarations , declared_prefixes ) : result = [ ] for namespace , prefix in declarations . items ( ) : if prefix : result . append ( u' xmlns:{0}={1}' . format ( prefix , quoteattr ( namespace ) ) ) else : result . append ( u' xmlns={1}' . format ( prefix , quoteattr ( namespace ) ) ) for d_namespace , d_prefix in declared_prefixes . items ( ) : if ( not prefix and not d_prefix ) or d_prefix == prefix : if namespace != d_namespace : del declared_prefixes [ d_namespace ] return u" " . join ( result )
1074	def getphraselist ( self ) : plist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '"' : plist . append ( self . getquote ( ) ) elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] in self . phraseends : break else : plist . append ( self . getatom ( self . phraseends ) ) return plist
10918	def find_best_step ( err_vals ) : if np . all ( np . isnan ( err_vals ) ) : raise ValueError ( 'All err_vals are nans!' ) return np . nanargmin ( err_vals )
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) # pylint: disable=deprecated-lambda def on_topologies_watch ( state_manager , topologies ) : """watch topologies""" Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : # The callback function with the bound # state_manager as first variable. onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
13495	def new ( self , mode ) : dw = DigitWord ( wordtype = mode . digit_type ) dw . random ( mode . digits ) self . _key = str ( uuid . uuid4 ( ) ) self . _status = "" self . _ttl = 3600 self . _answer = dw self . _mode = mode self . _guesses_remaining = mode . guesses_allowed self . _guesses_made = 0
12757	def add_torques ( self , torques ) : j = 0 for joint in self . joints : joint . add_torques ( list ( torques [ j : j + joint . ADOF ] ) + [ 0 ] * ( 3 - joint . ADOF ) ) j += joint . ADOF
11688	def get_changeset ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}/download' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content )
3967	def _compose_dict_for_nginx ( port_specs ) : spec = { 'image' : constants . NGINX_IMAGE , 'volumes' : [ '{}:{}' . format ( constants . NGINX_CONFIG_DIR_IN_VM , constants . NGINX_CONFIG_DIR_IN_CONTAINER ) ] , 'command' : 'nginx -g "daemon off;" -c /etc/nginx/conf.d/nginx.primary' , 'container_name' : 'dusty_{}_1' . format ( constants . DUSTY_NGINX_NAME ) } all_host_ports = set ( [ nginx_spec [ 'host_port' ] for nginx_spec in port_specs [ 'nginx' ] ] ) if all_host_ports : spec [ 'ports' ] = [ ] for port in all_host_ports : spec [ 'ports' ] . append ( '{0}:{0}' . format ( port ) ) return { constants . DUSTY_NGINX_NAME : spec }
4090	def _process_event ( self , key , mask ) : self . _logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT_READ and reader is not None : if reader . _cancelled : self . remove_reader ( fileobj ) else : self . _logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . _run ( ) if mask & selectors . EVENT_WRITE and writer is not None : if writer . _cancelled : self . remove_writer ( fileobj ) else : self . _logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . _run ( )
10067	def json_files_serializer ( objs , status = None ) : files = [ file_serializer ( obj ) for obj in objs ] return make_response ( json . dumps ( files ) , status )
10583	def create_account ( self , name , number = None , description = None ) : new_account = GeneralLedgerAccount ( name , description , number , self . account_type ) new_account . set_parent_path ( self . path ) self . accounts . append ( new_account ) return new_account
4592	def colors_no_palette ( colors = None , * * kwds ) : if isinstance ( colors , str ) : colors = _split_colors ( colors ) else : colors = to_triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , * * kwds )
4870	def to_internal_value ( self , data ) : if not isinstance ( data , list ) : message = self . error_messages [ 'not_a_list' ] . format ( input_type = type ( data ) . __name__ ) raise serializers . ValidationError ( { api_settings . NON_FIELD_ERRORS_KEY : [ message ] } ) ret = [ ] for item in data : try : validated = self . child . run_validation ( item ) except serializers . ValidationError as exc : ret . append ( exc . detail ) else : ret . append ( validated ) return ret
1553	def _get_comp_config ( self ) : proto_config = topology_pb2 . Config ( ) # first add parallelism key = proto_config . kvs . add ( ) key . key = TOPOLOGY_COMPONENT_PARALLELISM key . value = str ( self . parallelism ) key . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) # iterate through self.custom_config if self . custom_config is not None : sanitized = self . _sanitize_config ( self . custom_config ) for key , value in sanitized . items ( ) : if isinstance ( value , str ) : kvs = proto_config . kvs . add ( ) kvs . key = key kvs . value = value kvs . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) else : # need to serialize kvs = proto_config . kvs . add ( ) kvs . key = key kvs . serialized_value = default_serializer . serialize ( value ) kvs . type = topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) return proto_config
9374	def download_file ( url ) : try : ( local_file , headers ) = urllib . urlretrieve ( url ) except : sys . exit ( "ERROR: Problem downloading config file. Please check the URL (" + url + "). Exiting..." ) return local_file
324	def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
9242	def detect_actual_closed_dates ( self , issues , kind ) : if self . options . verbose : print ( "Fetching closed dates for {} {}..." . format ( len ( issues ) , kind ) ) all_issues = copy . deepcopy ( issues ) for issue in all_issues : if self . options . verbose > 2 : print ( "." , end = "" ) if not issues . index ( issue ) % 30 : print ( "" ) self . find_closed_date_by_commit ( issue ) if not issue . get ( 'actual_date' , False ) : if issue . get ( 'closed_at' , False ) : print ( "Skipping closed non-merged issue: #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) ) all_issues . remove ( issue ) if self . options . verbose > 2 : print ( "." ) return all_issues
1030	def b32encode ( s ) : parts = [ ] quanta , leftover = divmod ( len ( s ) , 5 ) # Pad the last quantum with zero bits if necessary if leftover : s += ( '\0' * ( 5 - leftover ) ) quanta += 1 for i in range ( quanta ) : # c1 and c2 are 16 bits wide, c3 is 8 bits wide. The intent of this # code is to process the 40 bits in units of 5 bits. So we take the 1 # leftover bit of c1 and tack it onto c2. Then we take the 2 leftover # bits of c2 and tack them onto c3. The shifts and masks are intended # to give us values of exactly 5 bits in width. c1 , c2 , c3 = struct . unpack ( '!HHB' , s [ i * 5 : ( i + 1 ) * 5 ] ) c2 += ( c1 & 1 ) << 16 # 17 bits wide c3 += ( c2 & 3 ) << 8 # 10 bits wide parts . extend ( [ _b32tab [ c1 >> 11 ] , # bits 1 - 5 _b32tab [ ( c1 >> 6 ) & 0x1f ] , # bits 6 - 10 _b32tab [ ( c1 >> 1 ) & 0x1f ] , # bits 11 - 15 _b32tab [ c2 >> 12 ] , # bits 16 - 20 (1 - 5) _b32tab [ ( c2 >> 7 ) & 0x1f ] , # bits 21 - 25 (6 - 10) _b32tab [ ( c2 >> 2 ) & 0x1f ] , # bits 26 - 30 (11 - 15) _b32tab [ c3 >> 5 ] , # bits 31 - 35 (1 - 5) _b32tab [ c3 & 0x1f ] , # bits 36 - 40 (1 - 5) ] ) encoded = EMPTYSTRING . join ( parts ) # Adjust for any leftover partial quanta if leftover == 1 : return encoded [ : - 6 ] + '======' elif leftover == 2 : return encoded [ : - 4 ] + '====' elif leftover == 3 : return encoded [ : - 3 ] + '===' elif leftover == 4 : return encoded [ : - 1 ] + '=' return encoded
4174	def window_kaiser ( N , beta = 8.6 , method = 'numpy' ) : if N == 1 : return ones ( 1 ) if method == 'numpy' : from numpy import kaiser return kaiser ( N , beta ) else : return _kaiser ( N , beta )
752	def setResultsPerChoice ( self , resultsPerChoice ) : # Keep track of the results obtained for each choice. self . _resultsPerChoice = [ [ ] ] * len ( self . choices ) for ( choiceValue , values ) in resultsPerChoice : choiceIndex = self . choices . index ( choiceValue ) self . _resultsPerChoice [ choiceIndex ] = list ( values )
51	def deepcopy ( self , x = None , y = None ) : x = self . x if x is None else x y = self . y if y is None else y return Keypoint ( x = x , y = y )
1768	def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
9042	def eigh ( self ) : from numpy . linalg import svd if self . _cache [ "eig" ] is not None : return self . _cache [ "eig" ] U , S = svd ( self . L ) [ : 2 ] S *= S S += self . _epsilon self . _cache [ "eig" ] = S , U return self . _cache [ "eig" ]
4127	def plot ( self , * * kargs ) : from pylab import plot , linspace , xlabel , ylabel , grid time = linspace ( 1 * self . dt , self . N * self . dt , self . N ) plot ( time , self . data , * * kargs ) xlabel ( 'Time' ) ylabel ( 'Amplitude' ) grid ( True )
3290	def get_preferred_path ( self ) : if self . path in ( "" , "/" ) : return "/" # Append '/' for collections if self . is_collection and not self . path . endswith ( "/" ) : return self . path + "/" # TODO: handle case-sensitivity, depending on OS # (FileSystemProvider could do this with os.path: # (?) on unix we can assume that the path already matches exactly the case of filepath # on windows we could use path.lower() or get the real case from the # file system return self . path
13580	def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
10577	def get_assay ( self ) : masses_sum = sum ( self . compound_masses ) return [ m / masses_sum for m in self . compound_masses ]
7677	def hierarchy ( annotation , * * kwargs ) : htimes , hlabels = hierarchy_flatten ( annotation ) htimes = [ np . asarray ( _ ) for _ in htimes ] return mir_eval . display . hierarchy ( htimes , hlabels , * * kwargs )
9409	def _extract ( data , session = None ) : # Extract each item of a list. if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] # Ignore leaf objects. if not isinstance ( data , np . ndarray ) : return data # Extract user defined classes. if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) # Extract struct data. if data . dtype . names : # Singular struct if data . size == 1 : return _create_struct ( data , session ) # Struct array return StructArray ( data , session ) # Extract cells. if data . dtype . kind == 'O' : return Cell ( data , session ) # Compress singleton values. if data . size == 1 : return data . item ( ) # Compress empty values. if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] # Return standard array. return data
2353	def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
1894	def _send ( self , cmd : str ) : logger . debug ( '>%s' , cmd ) try : self . _proc . stdout . flush ( ) self . _proc . stdin . write ( f'{cmd}\n' ) except IOError as e : raise SolverError ( str ( e ) )
10863	def add_particle ( self , pos , rad ) : rad = listify ( rad ) # add some zero mass particles to the list (same as not having these # particles in the image, which is true at this moment) inds = np . arange ( self . N , self . N + len ( rad ) ) self . pos = np . vstack ( [ self . pos , pos ] ) self . rad = np . hstack ( [ self . rad , np . zeros ( len ( rad ) ) ] ) # update the parameters globally self . setup_variables ( ) self . trigger_parameter_change ( ) # now request a drawing of the particle plz params = self . param_particle_rad ( inds ) self . trigger_update ( params , rad ) return inds
555	def getAllSwarms ( self , sprintIdx ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'sprintIdx' ] == sprintIdx : swarmIds . append ( swarmId ) return swarmIds
11854	def scanner ( self , j , word ) : for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
1111	def _qformat ( self , aline , bline , atags , btags ) : # Can hurt, but will probably help most of the time. common = min ( _count_leading ( aline , "\t" ) , _count_leading ( bline , "\t" ) ) common = min ( common , _count_leading ( atags [ : common ] , " " ) ) common = min ( common , _count_leading ( btags [ : common ] , " " ) ) atags = atags [ common : ] . rstrip ( ) btags = btags [ common : ] . rstrip ( ) yield "- " + aline if atags : yield "? %s%s\n" % ( "\t" * common , atags ) yield "+ " + bline if btags : yield "? %s%s\n" % ( "\t" * common , btags )
4268	def generate_thumbnail ( source , outname , box , fit = True , options = None , thumb_fit_centering = ( 0.5 , 0.5 ) ) : logger = logging . getLogger ( __name__ ) img = _read_image ( source ) original_format = img . format if fit : img = ImageOps . fit ( img , box , PILImage . ANTIALIAS , centering = thumb_fit_centering ) else : img . thumbnail ( box , PILImage . ANTIALIAS ) outformat = img . format or original_format or 'JPEG' logger . debug ( 'Save thumnail image: %s (%s)' , outname , outformat ) save_image ( img , outname , outformat , options = options , autoconvert = True )
5303	def parse_json_color_file ( path ) : with open ( path , "r" ) as color_file : color_list = json . load ( color_file ) # transform raw color list into color dict color_dict = { c [ "name" ] : c [ "hex" ] for c in color_list } return color_dict
1690	def UpdatePreprocessor ( self , line ) : if Match ( r'^\s*#\s*(if|ifdef|ifndef)\b' , line ) : # Beginning of #if block, save the nesting stack here. The saved # stack will allow us to restore the parsing state in the #else case. self . pp_stack . append ( _PreprocessorInfo ( copy . deepcopy ( self . stack ) ) ) elif Match ( r'^\s*#\s*(else|elif)\b' , line ) : # Beginning of #else block if self . pp_stack : if not self . pp_stack [ - 1 ] . seen_else : # This is the first #else or #elif block. Remember the # whole nesting stack up to this point. This is what we # keep after the #endif. self . pp_stack [ - 1 ] . seen_else = True self . pp_stack [ - 1 ] . stack_before_else = copy . deepcopy ( self . stack ) # Restore the stack to how it was before the #if self . stack = copy . deepcopy ( self . pp_stack [ - 1 ] . stack_before_if ) else : # TODO(unknown): unexpected #else, issue warning? pass elif Match ( r'^\s*#\s*endif\b' , line ) : # End of #if or #else blocks. if self . pp_stack : # If we saw an #else, we will need to restore the nesting # stack to its former state before the #else, otherwise we # will just continue from where we left off. if self . pp_stack [ - 1 ] . seen_else : # Here we can just use a shallow copy since we are the last # reference to it. self . stack = self . pp_stack [ - 1 ] . stack_before_else # Drop the corresponding #if self . pp_stack . pop ( ) else : # TODO(unknown): unexpected #endif, issue warning? pass
11342	def load_config ( filename = None , section_option_dict = { } ) : config = ConfigParser ( ) config . read ( filename ) working_dict = _prepare_working_dict ( config , section_option_dict ) tmp_dict = { } for section , options in working_dict . iteritems ( ) : tmp_dict [ section ] = { } for option in options : tmp_dict [ section ] [ option ] = config . get ( section , option ) return Bunch ( tmp_dict )
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
5077	def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) # course runs with no start date should be considered last. never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
12561	def create_rois_mask ( roislist , filelist ) : roifiles = [ ] for roi in roislist : try : roi_file = search_list ( roi , filelist ) [ 0 ] except Exception as exc : raise Exception ( 'Error creating list of roi files. \n {}' . format ( str ( exc ) ) ) else : roifiles . append ( roi_file ) return binarise ( roifiles )
948	def run ( self ) : self . __logger . debug ( "run(): Starting task <%s>" , self . __task [ 'taskLabel' ] ) # Set up the task # Create our main loop-control iterator if self . __cmdOptions . privateOptions [ 'testMode' ] : numIters = 10 else : numIters = self . __task [ 'iterationCount' ] if numIters >= 0 : iterTracker = iter ( xrange ( numIters ) ) else : iterTracker = iter ( itertools . count ( ) ) # Initialize periodic activities periodic = PeriodicActivityMgr ( requestedActivities = self . _createPeriodicActivities ( ) ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPFTaskDriver.setup(), instead? Is it always # desired in Nupic? self . __model . resetSequenceStates ( ) # Have Task Driver perform its initial setup activities, including setup # callbacks self . __taskDriver . setup ( ) # Run it! while True : # Check controlling iterator first try : next ( iterTracker ) except StopIteration : break # Read next input record try : inputRecord = self . __datasetReader . next ( ) except StopIteration : break # Process input record result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) self . __predictionLogger . writeRecord ( result ) # Run periodic activities periodic . tick ( ) # Dump the experiment metrics at the end of the task self . _getAndEmitExperimentMetrics ( final = True ) # Have Task Driver perform its final activities self . __taskDriver . finalize ( ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPFTaskDriver.setup(), instead? Is it always # desired in Nupic? self . __model . resetSequenceStates ( )
11162	def size ( self ) : try : return self . _stat . st_size except : # pragma: no cover self . _stat = self . stat ( ) return self . size
2343	def predict_proba ( self , a , b , nb_runs = 6 , nb_jobs = None , gpu = None , idx = 0 , verbose = None , ttest_threshold = 0.01 , nb_max_runs = 16 , train_epochs = 1000 , test_epochs = 1000 ) : Nb_jobs , verbose , gpu = SETTINGS . get_default ( ( 'nb_jobs' , nb_jobs ) , ( 'verbose' , verbose ) , ( 'gpu' , gpu ) ) x = np . stack ( [ a . ravel ( ) , b . ravel ( ) ] , 1 ) ttest_criterion = TTestCriterion ( max_iter = nb_max_runs , runs_per_iter = nb_runs , threshold = ttest_threshold ) AB = [ ] BA = [ ] while ttest_criterion . loop ( AB , BA ) : if nb_jobs != 1 : result_pair = Parallel ( n_jobs = nb_jobs ) ( delayed ( GNN_instance ) ( x , idx = idx , device = 'cuda:{}' . format ( run % gpu ) if gpu else 'cpu' , verbose = verbose , train_epochs = train_epochs , test_epochs = test_epochs ) for run in range ( ttest_criterion . iter , ttest_criterion . iter + nb_runs ) ) else : result_pair = [ GNN_instance ( x , idx = idx , device = 'cuda:0' if gpu else 'cpu' , verbose = verbose , train_epochs = train_epochs , test_epochs = test_epochs ) for run in range ( ttest_criterion . iter , ttest_criterion . iter + nb_runs ) ] AB . extend ( [ runpair [ 0 ] for runpair in result_pair ] ) BA . extend ( [ runpair [ 1 ] for runpair in result_pair ] ) if verbose : print ( "P-value after {} runs : {}" . format ( ttest_criterion . iter , ttest_criterion . p_value ) ) score_AB = np . mean ( AB ) score_BA = np . mean ( BA ) return ( score_BA - score_AB ) / ( score_BA + score_AB )
8039	def is_public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( "_" )
10574	def get_local_playlists ( filepaths , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local playlists..." ) included_playlists = [ ] excluded_playlists = [ ] supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_PLAYLIST_FORMATS , max_depth = max_depth ) included_playlists , excluded_playlists = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) logger . info ( "Excluded {0} local playlists" . format ( len ( excluded_playlists ) ) ) logger . info ( "Loaded {0} local playlists" . format ( len ( included_playlists ) ) ) return included_playlists , excluded_playlists
5759	def get_jenkins_job_urls ( rosdistro_name , jenkins_url , release_build_name , targets ) : urls = { } for target in targets : view_name = get_release_view_name ( rosdistro_name , release_build_name , target . os_name , target . os_code_name , target . arch ) base_url = jenkins_url + '/view/%s/job/%s__{pkg}__' % ( view_name , view_name ) if target . arch == 'source' : urls [ target ] = base_url + '%s_%s__source' % ( target . os_name , target . os_code_name ) else : urls [ target ] = base_url + '%s_%s_%s__binary' % ( target . os_name , target . os_code_name , target . arch ) return urls
1374	def parse_override_config_and_write_file ( namespace ) : overrides = parse_override_config ( namespace ) try : tmp_dir = tempfile . mkdtemp ( ) override_config_file = os . path . join ( tmp_dir , OVERRIDE_YAML ) with open ( override_config_file , 'w' ) as f : f . write ( yaml . dump ( overrides ) ) return override_config_file except Exception as e : raise Exception ( "Failed to parse override config: %s" % str ( e ) )
483	def getSwarmModelParams ( modelID ) : # TODO: the use of nupic.frameworks.opf.helpers.loadExperimentDescriptionScriptFromDir when # retrieving module params results in a leakage of pf_base_descriptionNN and # pf_descriptionNN module imports for every call to getSwarmModelParams, so # the leakage is unlimited when getSwarmModelParams is called by a # long-running process. An alternate solution is to execute the guts of # this function's logic in a seprate process (via multiprocessing module). cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) # Construct a directory with base.py and description.py for loading model # params, and use nupic.frameworks.opf.helpers to extract model params from # those files descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
2999	def sectorPerformanceDF ( token = '' , version = '' ) : df = pd . DataFrame ( sectorPerformance ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'name' ) return df
964	def export ( self ) : graph = nx . MultiDiGraph ( ) # Add regions to graph as nodes, annotated by name regions = self . network . getRegions ( ) for idx in xrange ( regions . getCount ( ) ) : regionPair = regions . getByIndex ( idx ) regionName = regionPair [ 0 ] graph . add_node ( regionName , label = regionName ) # Add links between regions to graph as edges, annotate by input-output # name pairs for linkName , link in self . network . getLinks ( ) : graph . add_edge ( link . getSrcRegionName ( ) , link . getDestRegionName ( ) , src = link . getSrcOutputName ( ) , dest = link . getDestInputName ( ) ) return graph
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
5561	def init_bounds ( self ) : if self . _raw [ "init_bounds" ] is None : return self . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "init_bounds" ] ) )
4150	def plot ( self , filename = None , norm = False , ylim = None , sides = None , * * kargs ) : import pylab from pylab import ylim as plt_ylim #First, check that psd attribute is up-to-date # just to get the PSD to be recomputed if needed _ = self . psd # check that the input sides parameter is correct if provided if sides is not None : if sides not in self . _sides_choices : raise errors . SpectrumChoiceError ( sides , self . _sides_choices ) # if sides is provided but identical to the current psd, nothing to do. # if sides not provided, let us use self.sides if sides is None or sides == self . sides : frequencies = self . frequencies ( ) psd = self . psd sides = self . sides elif sides is not None : # if sides argument is different from the attribute, we need to # create a new PSD/Freq ; indeed we do not want to change the # attribute itself # if data is complex, one-sided is wrong in any case. if self . datatype == 'complex' : if sides == 'onesided' : raise ValueError ( "sides cannot be one-sided with complex data" ) logging . debug ( "sides is different from the one provided. Converting PSD" ) frequencies = self . frequencies ( sides = sides ) psd = self . get_converted_psd ( sides ) if len ( psd ) != len ( frequencies ) : raise ValueError ( "PSD length is %s and freq length is %s" % ( len ( psd ) , len ( frequencies ) ) ) if 'ax' in list ( kargs . keys ( ) ) : save_ax = pylab . gca ( ) pylab . sca ( kargs [ 'ax' ] ) rollback = True del kargs [ 'ax' ] else : rollback = False if norm : pylab . plot ( frequencies , 10 * stools . log10 ( psd / max ( psd ) ) , * * kargs ) else : pylab . plot ( frequencies , 10 * stools . log10 ( psd ) , * * kargs ) pylab . xlabel ( 'Frequency' ) pylab . ylabel ( 'Power (dB)' ) pylab . grid ( True ) if ylim : plt_ylim ( ylim ) if sides == 'onesided' : pylab . xlim ( 0 , self . sampling / 2. ) elif sides == 'twosided' : pylab . xlim ( 0 , self . sampling ) elif sides == 'centerdc' : pylab . xlim ( - self . sampling / 2. , self . sampling / 2. ) if filename : pylab . savefig ( filename ) if rollback : pylab . sca ( save_ax ) del psd , frequencies
4746	def pkill ( ) : if env ( ) : return 1 cmd = [ "ps -aux | grep fio | grep -v grep" ] status , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) if not status : status , _ , _ = cij . ssh . command ( [ "pkill -f fio" ] , shell = True ) if status : return 1 return 0
11031	def get_json_field ( self , field , * * kwargs ) : d = self . request ( 'GET' , headers = { 'Accept' : 'application/json' } , * * kwargs ) d . addCallback ( raise_for_status ) d . addCallback ( raise_for_header , 'Content-Type' , 'application/json' ) d . addCallback ( json_content ) d . addCallback ( self . _get_json_field , field ) return d
13242	def period ( self ) : start_time = self . root . findtext ( 'daily_start_time' ) if start_time : return Period ( text_to_time ( start_time ) , text_to_time ( self . root . findtext ( 'daily_end_time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
9118	def _add_admin ( self , app , * * kwargs ) : from flask_admin import Admin from flask_admin . contrib . sqla import ModelView admin = Admin ( app , * * kwargs ) for flask_admin_model in self . flask_admin_models : if isinstance ( flask_admin_model , tuple ) : # assume its a 2 tuple if len ( flask_admin_model ) != 2 : raise TypeError model , view = flask_admin_model admin . add_view ( view ( model , self . session ) ) else : admin . add_view ( ModelView ( flask_admin_model , self . session ) ) return admin
561	def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )
11606	def condense_ranges ( cls , ranges ) : result = [ ] if ranges : ranges . sort ( key = lambda tup : tup [ 0 ] ) result . append ( ranges [ 0 ] ) for i in range ( 1 , len ( ranges ) ) : if result [ - 1 ] [ 1 ] + 1 >= ranges [ i ] [ 0 ] : result [ - 1 ] = ( result [ - 1 ] [ 0 ] , max ( result [ - 1 ] [ 1 ] , ranges [ i ] [ 1 ] ) ) else : result . append ( ranges [ i ] ) return result
4355	def _pop_ack_callback ( self , msgid ) : if msgid not in self . ack_callbacks : return None return self . ack_callbacks . pop ( msgid )
9027	def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
6139	def get_is_sim_running ( self ) : sim_info = self . simulation_info ( ) try : progress_info = sim_info [ 'simulation_info_progress' ] ret = progress_info [ 'simulation_progress_is_running' ] except KeyError : # Simulation has not been created. ret = False return ret
5175	def facts ( self , * * kwargs ) : return self . __api . facts ( query = EqualsOperator ( "certname" , self . name ) , * * kwargs )
7552	def _getbins ( ) : # Return error if system is 32-bit arch. # This is straight from the python docs: # https://docs.python.org/2/library/platform.html#cross-platform if not _sys . maxsize > 2 ** 32 : _sys . exit ( "ipyrad requires 64bit architecture" ) ## get platform mac or linux _platform = _sys . platform ## get current location if 'VIRTUAL_ENV' in _os . environ : ipyrad_path = _os . environ [ 'VIRTUAL_ENV' ] else : path = _os . path . abspath ( _os . path . dirname ( __file__ ) ) ipyrad_path = _os . path . dirname ( path ) ## find bin directory ipyrad_path = _os . path . dirname ( path ) bin_path = _os . path . join ( ipyrad_path , "bin" ) ## get the correct binaries if 'linux' in _platform : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-linux-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-linux-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-linux-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-linux-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-linux-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-linux-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-linux-x86_64" ) else : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-osx-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-osx-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-osx-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-osx-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-osx-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-osx-x86_64" ) ## only one compiled version available, works for all? qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-osx-x86_64" ) # Test for existence of binaries assert _cmd_exists ( muscle ) , "muscle not found here: " + muscle assert _cmd_exists ( vsearch ) , "vsearch not found here: " + vsearch assert _cmd_exists ( smalt ) , "smalt not found here: " + smalt assert _cmd_exists ( bwa ) , "bwa not found here: " + bwa assert _cmd_exists ( samtools ) , "samtools not found here: " + samtools assert _cmd_exists ( bedtools ) , "bedtools not found here: " + bedtools #assert _cmd_exists(qmc), "wQMC not found here: "+qmc return vsearch , muscle , smalt , bwa , samtools , bedtools , qmc
11180	def exchange_token ( self , code ) : access_token_url = OAUTH_ROOT + '/access_token' params = { 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'redirect_uri' : self . redirect_uri , 'code' : code , } resp = requests . get ( access_token_url , params = params ) if not resp . ok : raise MixcloudOauthError ( "Could not get access token." ) return resp . json ( ) [ 'access_token' ]
13587	def add_formatted_field ( cls , field , format_string , title = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = field . capitalize ( ) # python scoping is a bit weird with default values, if it isn't # referenced the inner function won't see it, so assign it for use _format_string = format_string def _ref ( self , obj ) : return _format_string % getattr ( obj , field ) _ref . short_description = title _ref . allow_tags = True _ref . admin_order_field = field setattr ( cls , fn_name , _ref )
736	def _sortChunk ( records , key , chunkIndex , fields ) : title ( additional = '(key=%s, chunkIndex=%d)' % ( str ( key ) , chunkIndex ) ) assert len ( records ) > 0 # Sort the current records records . sort ( key = itemgetter ( * key ) ) # Write to a chunk file if chunkIndex is not None : filename = 'chunk_%d.csv' % chunkIndex with FileRecordStream ( filename , write = True , fields = fields ) as o : for r in records : o . appendRecord ( r ) assert os . path . getsize ( filename ) > 0 return records
1446	def poll ( self ) : try : # non-blocking ret = self . _buffer . get ( block = False ) if self . _producer_callback is not None : self . _producer_callback ( ) return ret except Queue . Empty : Log . debug ( "%s: Empty in poll()" % str ( self ) ) raise Queue . Empty
8929	def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : # Check bit flags within pylint return code if exc . result . return_code & 32 : # Usage error (internal error in this code) notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
12507	def voxspace_to_mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm_coords = nib . affines . apply_affine ( affine , coords ) return mm_coords
5975	def anumb_to_atom ( self , anumb ) : assert isinstance ( anumb , int ) , "anumb must be integer" if not self . _anumb_to_atom : # empty dictionary if self . atoms : for atom in self . atoms : self . _anumb_to_atom [ atom . number ] = atom return self . _anumb_to_atom [ anumb ] else : self . logger ( "no atoms in the molecule" ) return False else : if anumb in self . _anumb_to_atom : return self . _anumb_to_atom [ anumb ] else : self . logger ( "no such atom number ({0:d}) in the molecule" . format ( anumb ) ) return False
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
1857	def BT ( cpu , dest , src ) : if dest . type == 'register' : cpu . CF = ( ( dest . read ( ) >> ( src . read ( ) % dest . size ) ) & 1 ) != 0 elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) value = cpu . read_int ( addr + base , 8 ) cpu . CF = Operators . EXTRACT ( value , pos , 1 ) == 1 else : raise NotImplementedError ( f"Unknown operand for BT: {dest.type}" )
4528	def _receive ( self , msg ) : msg = self . _convert ( msg ) if msg is None : return str_msg = self . verbose and self . _msg_to_str ( msg ) if self . verbose and log . is_debug ( ) : log . debug ( 'Message %s' , str_msg ) if self . pre_routing : self . pre_routing . receive ( msg ) receiver , msg = self . routing . receive ( msg ) if receiver : receiver . receive ( msg ) if self . verbose : log . info ( 'Routed message %s (%s) to %s' , str_msg [ : 128 ] , msg , repr ( receiver ) )
3970	def _get_build_path ( app_spec ) : if os . path . isabs ( app_spec [ 'build' ] ) : return app_spec [ 'build' ] return os . path . join ( Repo ( app_spec [ 'repo' ] ) . local_path , app_spec [ 'build' ] )
5900	def commandline ( self , * * mpiargs ) : cmd = self . MDRUN . commandline ( ) if self . mpiexec : cmd = self . mpicommand ( * * mpiargs ) + cmd return cmd
6194	def datafile_from_hash ( hash_ , prefix , path ) : pattern = '%s_%s*.h*' % ( prefix , hash_ ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise NoMatchError ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise MultipleMatchesError ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
231	def compute_sector_exposures ( positions , sectors , sector_dict = SECTORS ) : sector_ids = sector_dict . keys ( ) long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) for sector_id in sector_ids : in_sector = positions_wo_cash [ sectors == sector_id ] long_sector = in_sector [ in_sector > 0 ] . sum ( axis = 'columns' ) . divide ( long_exposure ) short_sector = in_sector [ in_sector < 0 ] . sum ( axis = 'columns' ) . divide ( short_exposure ) gross_sector = in_sector . abs ( ) . sum ( axis = 'columns' ) . divide ( gross_exposure ) net_sector = long_sector . subtract ( short_sector ) long_exposures . append ( long_sector ) short_exposures . append ( short_sector ) gross_exposures . append ( gross_sector ) net_exposures . append ( net_sector ) return long_exposures , short_exposures , gross_exposures , net_exposures
10330	def rank_edges ( edges , edge_ranking = None ) : edge_ranking = default_edge_ranking if edge_ranking is None else edge_ranking edges_scores = [ ( edge_id , edge_data [ RELATION ] , edge_ranking [ edge_data [ RELATION ] ] ) for edge_id , edge_data in edges . items ( ) ] return max ( edges_scores , key = itemgetter ( 2 ) )
7746	def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if tag . endswith ( "}iq" ) or tag == "iq" : return Iq ( element , return_path = return_path , language = language ) if tag . endswith ( "}message" ) or tag == "message" : return Message ( element , return_path = return_path , language = language ) if tag . endswith ( "}presence" ) or tag == "presence" : return Presence ( element , return_path = return_path , language = language ) else : return Stanza ( element , return_path = return_path , language = language )
3308	def _run_flup ( app , config , mode ) : # http://trac.saddi.com/flup/wiki/FlupServers if mode == "flup-fcgi" : from flup . server . fcgi import WSGIServer , __version__ as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi_fork import WSGIServer , __version__ as flupver else : raise ValueError _logger . info ( "Running WsgiDAV/{} {}/{}..." . format ( __version__ , WSGIServer . __module__ , flupver ) ) server = WSGIServer ( app , bindAddress = ( config [ "host" ] , config [ "port" ] ) , # debug=True, ) try : server . run ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
8327	def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except ValueError : pass #Find the two elements that would be next to each other if #this element (and any children) hadn't been parsed. Connect #the two. lastChild = self . _lastRecursiveChild ( ) nextElement = lastChild . next if self . previous : self . previous . next = nextElement if nextElement : nextElement . previous = self . previous self . previous = None lastChild . next = None self . parent = None if self . previousSibling : self . previousSibling . nextSibling = self . nextSibling if self . nextSibling : self . nextSibling . previousSibling = self . previousSibling self . previousSibling = self . nextSibling = None return self
11110	def walk_directory_directories_relative_path ( self , relativePath = "" ) : # get directory info dict errorMessage = "" relativePath = os . path . normpath ( relativePath ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage for dname in dict . __getitem__ ( dirInfoDict , "directories" ) : yield os . path . join ( relativePath , dname )
9948	def new_space_from_excel ( self , book , range_ , sheet = None , name = None , names_row = None , param_cols = None , space_param_order = None , cells_param_order = None , transpose = False , names_col = None , param_rows = None , ) : space = self . _impl . new_space_from_excel ( book , range_ , sheet , name , names_row , param_cols , space_param_order , cells_param_order , transpose , names_col , param_rows , ) return get_interfaces ( space )
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
10908	def circles ( st , layer , axis , ax = None , talpha = 1.0 , cedge = 'white' , cface = 'white' ) : pos = st . obj_get_positions ( ) rad = st . obj_get_radii ( ) shape = st . ishape . shape . tolist ( ) shape . pop ( axis ) #shape is now the shape of the image if ax is None : fig = plt . figure ( ) axisbg = 'white' if cface == 'black' else 'black' sx , sy = ( ( 1 , shape [ 1 ] / float ( shape [ 0 ] ) ) if shape [ 0 ] > shape [ 1 ] else ( shape [ 0 ] / float ( shape [ 1 ] ) , 1 ) ) ax = fig . add_axes ( ( 0 , 0 , sx , sy ) , axisbg = axisbg ) # get the index of the particles we want to include particles = np . arange ( len ( pos ) ) [ np . abs ( pos [ : , axis ] - layer ) < rad ] # for each of these particles display the effective radius # in the proper place scale = 1.0 #np.max(shape).astype('float') for i in particles : p = pos [ i ] . copy ( ) r = 2 * np . sqrt ( rad [ i ] ** 2 - ( p [ axis ] - layer ) ** 2 ) #CIRCLE IS IN FIGURE COORDINATES!!! if axis == 0 : ix = 1 iy = 2 elif axis == 1 : ix = 0 iy = 2 elif axis == 2 : ix = 0 iy = 1 c = Circle ( ( p [ ix ] / scale , p [ iy ] / scale ) , radius = r / 2 / scale , fc = cface , ec = cedge , alpha = talpha ) ax . add_patch ( c ) # plt.axis([0,1,0,1]) plt . axis ( 'equal' ) #circles not ellipses return ax
2567	def check_tracking_enabled ( self ) : track = True # By default we track usage test = False # By default we are not in testing mode testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
8154	def create ( self , name , overwrite = True ) : self . _name = name . rstrip ( ".db" ) from os import unlink if overwrite : try : unlink ( self . _name + ".db" ) except : pass self . _con = sqlite . connect ( self . _name + ".db" ) self . _cur = self . _con . cursor ( )
11263	def wildcard ( prev , pattern , * args , * * kw ) : import fnmatch inv = 'inv' in kw and kw . pop ( 'inv' ) pattern_obj = re . compile ( fnmatch . translate ( pattern ) , * args , * * kw ) if not inv : for data in prev : if pattern_obj . match ( data ) : yield data else : for data in prev : if not pattern_obj . match ( data ) : yield data
11522	def add_condor_job ( self , token , batchmaketaskid , jobdefinitionfilename , outputfilename , errorfilename , logfilename , postfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'jobdefinitionfilename' ] = jobdefinitionfilename parameters [ 'outputfilename' ] = outputfilename parameters [ 'errorfilename' ] = errorfilename parameters [ 'logfilename' ] = logfilename parameters [ 'postfilename' ] = postfilename response = self . request ( 'midas.batchmake.add.condor.job' , parameters ) return response
4119	def twosided_2_onesided ( data ) : assert len ( data ) % 2 == 0 N = len ( data ) psd = np . array ( data [ 0 : N // 2 + 1 ] ) * 2. psd [ 0 ] /= 2. psd [ - 1 ] = data [ - 1 ] return psd
9452	def conference_mute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceMute/' method = 'POST' return self . request ( path , method , call_params )
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
12023	def check_phase ( self ) : plus_minus = set ( [ '+' , '-' ] ) for k , g in groupby ( sorted ( [ line for line in self . lines if line [ 'line_type' ] == 'feature' and line [ 'type' ] == 'CDS' and 'Parent' in line [ 'attributes' ] ] , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) : cds_list = list ( g ) strand_set = list ( set ( [ line [ 'strand' ] for line in cds_list ] ) ) if len ( strand_set ) != 1 : for line in cds_list : self . add_line_error ( line , { 'message' : 'Inconsistent CDS strand with parent: {0:s}' . format ( k ) , 'error_type' : 'STRAND' } ) continue if len ( cds_list ) == 1 : if cds_list [ 0 ] [ 'phase' ] != 0 : self . add_line_error ( cds_list [ 0 ] , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( cds_list [ 0 ] [ 'phase' ] , 0 ) , 'error_type' : 'PHASE' } ) continue strand = strand_set [ 0 ] if strand not in plus_minus : # don't process unknown strands continue if strand == '-' : # sort end descending sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'end' ] , reverse = True ) else : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'start' ] ) phase = 0 for line in sorted_cds_list : if line [ 'phase' ] != phase : self . add_line_error ( line , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( line [ 'phase' ] , phase ) , 'error_type' : 'PHASE' } ) phase = ( 3 - ( ( line [ 'end' ] - line [ 'start' ] + 1 - phase ) % 3 ) ) % 3
12554	def sav_to_pandas_rpy2 ( input_file ) : import pandas . rpy . common as com w = com . robj . r ( 'foreign::read.spss("%s", to.data.frame=TRUE)' % input_file ) return com . convert_robj ( w )
2461	def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) # A file name marks the start of a new file instance. # The builder must be reset # FIXME: this state does not make sense self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
9829	def edges ( self ) : return [ self . delta [ d , d ] * numpy . arange ( self . shape [ d ] + 1 ) + self . origin [ d ] - 0.5 * self . delta [ d , d ] for d in range ( self . rank ) ]
13300	def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip_path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip_path , 'install' , package )
12981	def string ( html , start_on = None , ignore = ( ) , use_short = True , * * queries ) : if use_short : html = grow_short ( html ) return _to_template ( fromstring ( html ) , start_on = start_on , ignore = ignore , * * queries )
9220	def parse_args ( self , args , scope ) : arguments = list ( zip ( args , [ ' ' ] * len ( args ) ) ) if args and args [ 0 ] else None zl = itertools . zip_longest if sys . version_info [ 0 ] == 3 else itertools . izip_longest if self . args : parsed = [ v if hasattr ( v , 'parse' ) else v for v in copy . copy ( self . args ) ] args = args if isinstance ( args , list ) else [ args ] vars = [ self . _parse_arg ( var , arg , scope ) for arg , var in zl ( [ a for a in args ] , parsed ) ] for var in vars : if var : var . parse ( scope ) if not arguments : arguments = [ v . value for v in vars if v ] if not arguments : arguments = '' Variable ( [ '@arguments' , None , arguments ] ) . parse ( scope )
4226	def _data_root_Linux ( ) : fallback = os . path . expanduser ( '~/.local/share' ) root = os . environ . get ( 'XDG_DATA_HOME' , None ) or fallback return os . path . join ( root , 'python_keyring' )
13513	def reynolds_number ( length , speed , temperature = 25 ) : kinematic_viscosity = interpolate . interp1d ( [ 0 , 10 , 20 , 25 , 30 , 40 ] , np . array ( [ 18.54 , 13.60 , 10.50 , 9.37 , 8.42 , 6.95 ] ) / 10 ** 7 ) # Data from http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf Re = length * speed / kinematic_viscosity ( temperature ) return Re
6952	def aov_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) ndets = phases . size binnedphaseinds = npdigitize ( phases , bins ) bin_s1_tops = [ ] bin_s2_tops = [ ] binndets = [ ] goodbins = 0 all_xbar = npmedian ( pmags ) for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_ndet = thisbin_mags . size thisbin_xbar = npmedian ( thisbin_mags ) # get s1 thisbin_s1_top = ( thisbin_ndet * ( thisbin_xbar - all_xbar ) * ( thisbin_xbar - all_xbar ) ) # get s2 thisbin_s2_top = npsum ( ( thisbin_mags - all_xbar ) * ( thisbin_mags - all_xbar ) ) bin_s1_tops . append ( thisbin_s1_top ) bin_s2_tops . append ( thisbin_s2_top ) binndets . append ( thisbin_ndet ) goodbins = goodbins + 1 # turn the quantities into arrays bin_s1_tops = nparray ( bin_s1_tops ) bin_s2_tops = nparray ( bin_s2_tops ) binndets = nparray ( binndets ) # calculate s1 first s1 = npsum ( bin_s1_tops ) / ( goodbins - 1.0 ) # then calculate s2 s2 = npsum ( bin_s2_tops ) / ( ndets - goodbins ) theta_aov = s1 / s2 return theta_aov
13781	def FindExtensionByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) message_name , _ , extension_name = full_name . rpartition ( '.' ) try : # Most extensions are nested inside a message. scope = self . FindMessageTypeByName ( message_name ) except KeyError : # Some extensions are defined at file scope. scope = self . FindFileContainingSymbol ( full_name ) return scope . extensions_by_name [ extension_name ]
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) # else command is intended for the entire house code, not a single unit number # TODO normalize/validate state return self . _x10_command ( house_code , unit_number , state )
7324	def create_sample_input_files ( template_filename , database_filename , config_filename ) : print ( "Creating sample template email {}" . format ( template_filename ) ) if os . path . exists ( template_filename ) : print ( "Error: file exists: " + template_filename ) sys . exit ( 1 ) with io . open ( template_filename , "w" ) as template_file : template_file . write ( u"TO: {{email}}\n" u"SUBJECT: Testing mailmerge\n" u"FROM: My Self <myself@mydomain.com>\n" u"\n" u"Hi, {{name}},\n" u"\n" u"Your number is {{number}}.\n" ) print ( "Creating sample database {}" . format ( database_filename ) ) if os . path . exists ( database_filename ) : print ( "Error: file exists: " + database_filename ) sys . exit ( 1 ) with io . open ( database_filename , "w" ) as database_file : database_file . write ( u'email,name,number\n' u'myself@mydomain.com,"Myself",17\n' u'bob@bobdomain.com,"Bob",42\n' ) print ( "Creating sample config file {}" . format ( config_filename ) ) if os . path . exists ( config_filename ) : print ( "Error: file exists: " + config_filename ) sys . exit ( 1 ) with io . open ( config_filename , "w" ) as config_file : config_file . write ( u"# Example: GMail\n" u"[smtp_server]\n" u"host = smtp.gmail.com\n" u"port = 465\n" u"security = SSL/TLS\n" u"username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: Wide open\n" u"# [smtp_server]\n" u"# host = open-smtp.example.com\n" u"# port = 25\n" u"# security = Never\n" u"# username = None\n" u"#\n" u"# Example: University of Michigan\n" u"# [smtp_server]\n" u"# host = smtp.mail.umich.edu\n" u"# port = 465\n" u"# security = SSL/TLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with STARTTLS security\n" # noqa: E501 u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = STARTTLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with no encryption\n" # noqa: E501 u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = Never\n" u"# username = YOUR_USERNAME_HERE\n" ) print ( "Edit these files, and then run mailmerge again" )
1904	def _find_zero ( cpu , constrs , ptr ) : offset = 0 while True : byt = cpu . read_int ( ptr + offset , 8 ) if issymbolic ( byt ) : if not solver . can_be_true ( constrs , byt != 0 ) : break else : if byt == 0 : break offset += 1 return offset
8356	def _toUnicode ( self , data , encoding ) : # strip Byte Order Mark (if present) if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
3557	def find_device ( cls , timeout_sec = TIMEOUT_SEC ) : return get_provider ( ) . find_device ( service_uuids = cls . ADVERTISED , timeout_sec = timeout_sec )
4715	def trun_emph ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] > 1 : # Print environment variables cij . emph ( "rnr:CONF {" ) for cvar in sorted ( trun [ "conf" ] . keys ( ) ) : cij . emph ( " % 16s: %r" % ( cvar , trun [ "conf" ] [ cvar ] ) ) cij . emph ( "}" ) if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:INFO {" ) cij . emph ( " OUTPUT: %r" % trun [ "conf" ] [ "OUTPUT" ] ) cij . emph ( " yml_fpath: %r" % yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) ) cij . emph ( "}" )
4965	def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
13049	def check_service ( service ) : # Try HTTP service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : # Try HTTPS try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
13443	def path ( self , a_hash , b_hash ) : def _path ( a , b ) : if a is b : return [ a ] else : assert len ( a . children ) == 1 return [ a ] + _path ( a . children [ 0 ] , b ) a = self . nodes [ a_hash ] b = self . nodes [ b_hash ] return _path ( a , b ) [ 1 : ]
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
5963	def plot ( self , * * kwargs ) : columns = kwargs . pop ( 'columns' , Ellipsis ) # slice for everything maxpoints = kwargs . pop ( 'maxpoints' , self . maxpoints_default ) transform = kwargs . pop ( 'transform' , lambda x : x ) # default is identity transformation method = kwargs . pop ( 'method' , "mean" ) ax = kwargs . pop ( 'ax' , None ) if columns is Ellipsis or columns is None : columns = numpy . arange ( self . array . shape [ 0 ] ) if len ( columns ) == 0 : raise MissingDataError ( "plot() needs at least one column of data" ) if len ( self . array . shape ) == 1 or self . array . shape [ 0 ] == 1 : # special case: plot against index; plot would do this automatically but # we'll just produce our own xdata and pretend that this was X all along a = numpy . ravel ( self . array ) X = numpy . arange ( len ( a ) ) a = numpy . vstack ( ( X , a ) ) columns = [ 0 ] + [ c + 1 for c in columns ] else : a = self . array color = kwargs . pop ( 'color' , self . default_color_cycle ) try : cmap = matplotlib . cm . get_cmap ( color ) colors = cmap ( matplotlib . colors . Normalize ( ) ( numpy . arange ( len ( columns [ 1 : ] ) , dtype = float ) ) ) except TypeError : colors = cycle ( utilities . asiterable ( color ) ) if ax is None : ax = plt . gca ( ) # (decimate/smooth o slice o transform)(array) a = self . decimate ( method , numpy . asarray ( transform ( a ) ) [ columns ] , maxpoints = maxpoints ) # now deal with infs, nans etc AFTER all transformations (needed for plotting across inf/nan) ma = numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) ) # finally plot (each column separately to catch empty sets) for column , color in zip ( range ( 1 , len ( columns ) ) , colors ) : if len ( ma [ column ] ) == 0 : warnings . warn ( "No data to plot for column {column:d}" . format ( * * vars ( ) ) , category = MissingDataWarning ) kwargs [ 'color' ] = color ax . plot ( ma [ 0 ] , ma [ column ] , * * kwargs ) # plot all other columns in parallel return ax
5278	def preprocess_constraints ( ml , cl , n ) : # Represent the graphs using adjacency-lists ml_graph , cl_graph = { } , { } for i in range ( n ) : ml_graph [ i ] = set ( ) cl_graph [ i ] = set ( ) def add_both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml_graph [ i ] . add ( j ) ml_graph [ j ] . add ( i ) for ( i , j ) in cl : cl_graph [ i ] . add ( j ) cl_graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) # Run DFS from each node to get all the graph's components # and add an edge for each pair of nodes in the component (create a complete graph) # See http://www.techiedelight.com/transitive-closure-graph/ for more details visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml_graph [ i ] : component = [ ] dfs ( i , ml_graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml_graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml_graph [ i ] : add_both ( cl_graph , x , j ) for y in ml_graph [ j ] : add_both ( cl_graph , i , y ) for x in ml_graph [ i ] : for y in ml_graph [ j ] : add_both ( cl_graph , x , y ) for i in ml_graph : for j in ml_graph [ i ] : if j != i and j in cl_graph [ i ] : raise InconsistentConstraintsException ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml_graph , cl_graph , neighborhoods
5549	def get_hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
10400	def done_chomping ( self ) -> bool : return self . tag in self . graph . nodes [ self . target_node ]
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
12940	def getRedisPool ( params ) : global RedisPools global _defaultRedisConnectionParams global _redisManagedConnectionParams if not params : params = _defaultRedisConnectionParams isDefaultParams = True else : isDefaultParams = bool ( params is _defaultRedisConnectionParams ) if 'connection_pool' in params : return params [ 'connection_pool' ] hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] # Copy the params, so that we don't modify the original dict if not isDefaultParams : origParams = params params = copy . copy ( params ) else : origParams = params checkAgain = False if 'host' not in params : if not isDefaultParams and 'host' in _defaultRedisConnectionParams : params [ 'host' ] = _defaultRedisConnectionParams [ 'host' ] else : params [ 'host' ] = '127.0.0.1' checkAgain = True if 'port' not in params : if not isDefaultParams and 'port' in _defaultRedisConnectionParams : params [ 'port' ] = _defaultRedisConnectionParams [ 'port' ] else : params [ 'port' ] = 6379 checkAgain = True if 'db' not in params : if not isDefaultParams and 'db' in _defaultRedisConnectionParams : params [ 'db' ] = _defaultRedisConnectionParams [ 'db' ] else : params [ 'db' ] = 0 checkAgain = True if not isDefaultParams : otherGlobalKeys = set ( _defaultRedisConnectionParams . keys ( ) ) - set ( params . keys ( ) ) for otherKey in otherGlobalKeys : if otherKey == 'connection_pool' : continue params [ otherKey ] = _defaultRedisConnectionParams [ otherKey ] checkAgain = True if checkAgain : hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] connectionPool = redis . ConnectionPool ( * * params ) origParams [ 'connection_pool' ] = params [ 'connection_pool' ] = connectionPool RedisPools [ hashValue ] = connectionPool # Add the original as a "managed" redis connection (they did not provide their own pool) # such that if the defaults change, we make sure to re-inherit any keys, and can disconnect # from clearRedisPools origParamsHash = hashDictOneLevel ( origParams ) if origParamsHash not in _redisManagedConnectionParams : _redisManagedConnectionParams [ origParamsHash ] = [ origParams ] elif origParams not in _redisManagedConnectionParams [ origParamsHash ] : _redisManagedConnectionParams [ origParamsHash ] . append ( origParams ) return connectionPool
117	def imap_batches ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) # TODO change this to 'yield from' once switched to 3.3+ gen = self . pool . imap ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
13384	def get_store_env_tmp ( ) : tempdir = tempfile . gettempdir ( ) temp_name = 'envstore{0:0>3d}' temp_path = unipath ( tempdir , temp_name . format ( random . getrandbits ( 9 ) ) ) if not os . path . exists ( temp_path ) : return temp_path else : return get_store_env_tmp ( )
6581	def play ( self , song ) : self . _callbacks . play ( song ) self . _load_track ( song ) time . sleep ( 2 ) # Give the backend time to load the track while True : try : self . _callbacks . pre_poll ( ) self . _ensure_started ( ) self . _loop_hook ( ) readers , _ , _ = select . select ( self . _get_select_readers ( ) , [ ] , [ ] , 1 ) for handle in readers : if handle . fileno ( ) == self . _control_fd : self . _callbacks . input ( handle . readline ( ) . strip ( ) , song ) else : value = self . _read_from_process ( handle ) if self . _player_stopped ( value ) : return finally : self . _callbacks . post_poll ( )
12133	def extract_log ( log_path , dict_type = dict ) : log_path = ( log_path if os . path . isfile ( log_path ) else os . path . join ( os . getcwd ( ) , log_path ) ) with open ( log_path , 'r' ) as log : splits = ( line . split ( ) for line in log ) uzipped = ( ( int ( split [ 0 ] ) , json . loads ( " " . join ( split [ 1 : ] ) ) ) for split in splits ) szipped = [ ( i , dict ( ( str ( k ) , v ) for ( k , v ) in d . items ( ) ) ) for ( i , d ) in uzipped ] return dict_type ( szipped )
13834	def PrintMessage ( self , message ) : fields = message . ListFields ( ) if self . use_index_order : fields . sort ( key = lambda x : x [ 0 ] . index ) for field , value in fields : if _IsMapEntry ( field ) : for key in sorted ( value ) : # This is slow for maps with submessage entires because it copies the # entire tree. Unfortunately this would take significant refactoring # of this file to work around. # # TODO(haberman): refactor and optimize if this becomes an issue. entry_submsg = field . message_type . _concrete_class ( key = key , value = value [ key ] ) self . PrintField ( field , entry_submsg ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : for element in value : self . PrintField ( field , element ) else : self . PrintField ( field , value )
4730	def start ( self ) : self . __thread = Thread ( target = self . __run , args = ( True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
5382	def _build_pipeline_request ( self , task_view ) : job_metadata = task_view . job_metadata job_params = task_view . job_params job_resources = task_view . job_resources task_metadata = task_view . task_descriptors [ 0 ] . task_metadata task_params = task_view . task_descriptors [ 0 ] . task_params task_resources = task_view . task_descriptors [ 0 ] . task_resources script = task_view . job_metadata [ 'script' ] reserved_labels = google_base . build_pipeline_labels ( job_metadata , task_metadata , task_id_pattern = 'task-%d' ) # Build the ephemeralPipeline for this job. # The ephemeralPipeline definition changes for each job because file # parameters localCopy.path changes based on the remote_uri. pipeline = _Pipelines . build_pipeline ( project = self . _project , zones = job_resources . zones , min_cores = job_resources . min_cores , min_ram = job_resources . min_ram , disk_size = job_resources . disk_size , boot_disk_size = job_resources . boot_disk_size , preemptible = job_resources . preemptible , accelerator_type = job_resources . accelerator_type , accelerator_count = job_resources . accelerator_count , image = job_resources . image , script_name = script . name , envs = job_params [ 'envs' ] | task_params [ 'envs' ] , inputs = job_params [ 'inputs' ] | task_params [ 'inputs' ] , outputs = job_params [ 'outputs' ] | task_params [ 'outputs' ] , pipeline_name = job_metadata [ 'pipeline-name' ] ) # Build the pipelineArgs for this job. logging_uri = task_resources . logging_path . uri scopes = job_resources . scopes or google_base . DEFAULT_SCOPES pipeline . update ( _Pipelines . build_pipeline_args ( self . _project , script . value , job_params , task_params , reserved_labels , job_resources . preemptible , logging_uri , scopes , job_resources . keep_alive ) ) return pipeline
4283	def generate_video ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) # Don't transcode if source is in the required format and # has fitting datedimensions, copy instead. converter = settings [ 'video_converter' ] w_src , h_src = video_size ( source , converter = converter ) w_dst , h_dst = settings [ 'video_size' ] logger . debug ( 'Video size: %i, %i -> %i, %i' , w_src , h_src , w_dst , h_dst ) base , src_ext = splitext ( source ) base , dst_ext = splitext ( outname ) if dst_ext == src_ext and w_src <= w_dst and h_src <= h_dst : logger . debug ( 'Video is smaller than the max size, copying it instead' ) shutil . copy ( source , outname ) return # http://stackoverflow.com/questions/8218363/maintaining-ffmpeg-aspect-ratio # + I made a drawing on paper to figure this out if h_dst * w_src < h_src * w_dst : # biggest fitting dimension is height resize_opt = [ '-vf' , "scale=trunc(oh*a/2)*2:%i" % h_dst ] else : # biggest fitting dimension is width resize_opt = [ '-vf' , "scale=%i:trunc(ow/a/2)*2" % w_dst ] # do not resize if input dimensions are smaller than output dimensions if w_src <= w_dst and h_src <= h_dst : resize_opt = [ ] # Encoding options improved, thanks to # http://ffmpeg.org/trac/ffmpeg/wiki/vpxEncodingGuide cmd = [ converter , '-i' , source , '-y' ] # -y to overwrite output files if options is not None : cmd += options cmd += resize_opt + [ outname ] logger . debug ( 'Processing video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname )
6822	def configure_modrpaf ( self ) : r = self . local_renderer if r . env . modrpaf_enabled : self . install_packages ( ) self . enable_mod ( 'rpaf' ) else : if self . last_manifest . modrpaf_enabled : self . disable_mod ( 'mod_rpaf' )
6995	def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_bucket = None , result_queue = None , result_bucket = None , pfresult_list = None , runcp_kwargs = None , process_list_slice = None , download_when_done = True , purge_queues_when_done = True , save_state_when_done = True , delete_queues_when_done = False , s3_client = None , sqs_client = None ) : if use_saved_state is not None and os . path . exists ( use_saved_state ) : with open ( use_saved_state , 'rb' ) as infd : saved_state = pickle . load ( infd ) # run the producer loop using the saved state's todo list return runcp_producer_loop ( saved_state [ 'in_progress' ] , saved_state [ 'args' ] [ 1 ] , saved_state [ 'args' ] [ 2 ] , saved_state [ 'args' ] [ 3 ] , saved_state [ 'args' ] [ 4 ] , * * saved_state [ 'kwargs' ] ) else : return runcp_producer_loop ( lightcurve_list , input_queue , input_bucket , result_queue , result_bucket , pfresult_list = pfresult_list , runcp_kwargs = runcp_kwargs , process_list_slice = process_list_slice , download_when_done = download_when_done , purge_queues_when_done = purge_queues_when_done , save_state_when_done = save_state_when_done , delete_queues_when_done = delete_queues_when_done , s3_client = s3_client , sqs_client = sqs_client )
11859	def sum_out ( var , factors , bn ) : result , var_factors = [ ] , [ ] for f in factors : ( var_factors if var in f . vars else result ) . append ( f ) result . append ( pointwise_product ( var_factors , bn ) . sum_out ( var , bn ) ) return result
4209	def pascal ( n ) : errors . is_positive_integer ( n ) result = numpy . zeros ( ( n , n ) ) #fill the first row and column for i in range ( 0 , n ) : result [ i , 0 ] = 1 result [ 0 , i ] = 1 if n > 1 : for i in range ( 1 , n ) : for j in range ( 1 , n ) : result [ i , j ] = result [ i - 1 , j ] + result [ i , j - 1 ] return result
5390	def _get_task_from_task_dir ( self , job_id , user_id , task_id , task_attempt ) : # We need to be very careful about how we read and interpret the contents # of the task directory. The directory could be changing because a new # task is being created. The directory could be changing because a task # is ending. # # If the meta.yaml does not exist, the task does not yet exist. # If the meta.yaml exists, it means the task is scheduled. It does not mean # it is yet running. # If the task.pid file exists, it means that the runner.sh was started. task_dir = self . _task_directory ( job_id , task_id , task_attempt ) job_descriptor = self . _read_task_metadata ( task_dir ) if not job_descriptor : return None # If we read up an old task, the user-id will not be in the job_descriptor. if not job_descriptor . job_metadata . get ( 'user-id' ) : job_descriptor . job_metadata [ 'user-id' ] = user_id # Get the pid of the runner pid = - 1 try : with open ( os . path . join ( task_dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IOError , OSError ) : pass # Get the script contents script = None script_name = job_descriptor . job_metadata . get ( 'script-name' ) if script_name : script = self . _read_script ( task_dir , script_name ) # Read the files written by the runner.sh. # For new tasks, these may not have been written yet. end_time = self . _get_end_time_from_task_dir ( task_dir ) last_update = self . _get_last_update_time_from_task_dir ( task_dir ) events = self . _get_events_from_task_dir ( task_dir ) status = self . _get_status_from_task_dir ( task_dir ) log_detail = self . _get_log_detail_from_task_dir ( task_dir ) # If the status file is not yet written, then mark the task as pending if not status : status = 'RUNNING' log_detail = [ 'Pending' ] return LocalTask ( task_status = status , events = events , log_detail = log_detail , job_descriptor = job_descriptor , end_time = end_time , last_update = last_update , pid = pid , script = script )
2133	def _workflow_node_structure ( node_results ) : # Build list address translation, and create backlink lists node_list_pos = { } for i , node_result in enumerate ( node_results ) : for rel in [ 'success' , 'failure' , 'always' ] : node_result [ '{0}_backlinks' . format ( rel ) ] = [ ] node_list_pos [ node_result [ 'id' ] ] = i # Populate backlink lists for node_result in node_results : for rel in [ 'success' , 'failure' , 'always' ] : for sub_node_id in node_result [ '{0}_nodes' . format ( rel ) ] : j = node_list_pos [ sub_node_id ] node_results [ j ] [ '{0}_backlinks' . format ( rel ) ] . append ( node_result [ 'id' ] ) # Find the root nodes root_nodes = [ ] for node_result in node_results : is_root = True for rel in [ 'success' , 'failure' , 'always' ] : if node_result [ '{0}_backlinks' . format ( rel ) ] != [ ] : is_root = False break if is_root : root_nodes . append ( node_result [ 'id' ] ) # Create network dictionary recursively from root nodes def branch_schema ( node_id ) : i = node_list_pos [ node_id ] node_dict = node_results [ i ] ret_dict = { "id" : node_id } for fd in NODE_STANDARD_FIELDS : val = node_dict . get ( fd , None ) if val is not None : if fd == 'unified_job_template' : job_type = node_dict [ 'summary_fields' ] [ 'unified_job_template' ] [ 'unified_job_type' ] ujt_key = JOB_TYPES [ job_type ] ret_dict [ ujt_key ] = val else : ret_dict [ fd ] = val for rel in [ 'success' , 'failure' , 'always' ] : sub_node_id_list = node_dict [ '{0}_nodes' . format ( rel ) ] if len ( sub_node_id_list ) == 0 : continue relationship_name = '{0}_nodes' . format ( rel ) ret_dict [ relationship_name ] = [ ] for sub_node_id in sub_node_id_list : ret_dict [ relationship_name ] . append ( branch_schema ( sub_node_id ) ) return ret_dict schema_dict = [ ] for root_node_id in root_nodes : schema_dict . append ( branch_schema ( root_node_id ) ) return schema_dict
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : #Bind to signature. May throw its own TypeError bound = sig . bind ( * args , * * kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
187	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : # TODO improve efficiency here by copying only once for ls in self . line_strings : image = ls . draw_on_image ( image , color = color , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return image
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
2640	def runner ( incoming_q , outgoing_q ) : logger . debug ( "[RUNNER] Starting" ) def execute_task ( bufs ) : """Deserialize the buffer and execute the task. Returns the serialized result or exception. """ user_ns = locals ( ) user_ns . update ( { '__builtins__' : __builtins__ } ) f , args , kwargs = unpack_apply_message ( bufs , user_ns , copy = False ) fname = getattr ( f , '__name__' , 'f' ) prefix = "parsl_" fname = prefix + "f" argname = prefix + "args" kwargname = prefix + "kwargs" resultname = prefix + "result" user_ns . update ( { fname : f , argname : args , kwargname : kwargs , resultname : resultname } ) code = "{0} = {1}(*{2}, **{3})" . format ( resultname , fname , argname , kwargname ) try : logger . debug ( "[RUNNER] Executing: {0}" . format ( code ) ) exec ( code , user_ns , user_ns ) except Exception as e : logger . warning ( "Caught exception; will raise it: {}" . format ( e ) ) raise e else : logger . debug ( "[RUNNER] Result: {0}" . format ( user_ns . get ( resultname ) ) ) return user_ns . get ( resultname ) while True : try : # Blocking wait on the queue msg = incoming_q . get ( block = True , timeout = 10 ) except queue . Empty : # Handle case where no items were in the queue logger . debug ( "[RUNNER] Queue is empty" ) except IOError as e : logger . debug ( "[RUNNER] Broken pipe: {}" . format ( e ) ) try : # Attempt to send a stop notification to the management thread outgoing_q . put ( None ) except Exception : pass break except Exception as e : logger . debug ( "[RUNNER] Caught unknown exception: {}" . format ( e ) ) else : # Handle received message if not msg : # Empty message is a die request logger . debug ( "[RUNNER] Received exit request" ) outgoing_q . put ( None ) break else : # Received a valid message, handle it logger . debug ( "[RUNNER] Got a valid task with ID {}" . format ( msg [ "task_id" ] ) ) try : response_obj = execute_task ( msg [ 'buffer' ] ) response = { "task_id" : msg [ "task_id" ] , "result" : serialize_object ( response_obj ) } logger . debug ( "[RUNNER] Returing result: {}" . format ( deserialize_object ( response [ "result" ] ) ) ) except Exception as e : logger . debug ( "[RUNNER] Caught task exception: {}" . format ( e ) ) response = { "task_id" : msg [ "task_id" ] , "exception" : serialize_object ( e ) } outgoing_q . put ( response ) logger . debug ( "[RUNNER] Terminating" )
5179	def reports ( self , * * kwargs ) : return self . __api . reports ( query = EqualsOperator ( "certname" , self . name ) , * * kwargs )
994	def _generateRangeDescription ( self , ranges ) : desc = "" numRanges = len ( ranges ) for i in xrange ( numRanges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( ranges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += ", " return desc
9207	def add_prefix ( multicodec , bytes_ ) : prefix = get_prefix ( multicodec ) return b'' . join ( [ prefix , bytes_ ] )
7003	def collect_nonperiodic_features ( featuresdir , magcol , outfile , pklglob = 'varfeatures-*.pkl' , featurestouse = NONPERIODIC_FEATURES_TO_COLLECT , maxobjects = None , labeldict = None , labeltype = 'binary' , ) : # list of input pickles generated by varfeatures in lcproc.py pklist = glob . glob ( os . path . join ( featuresdir , pklglob ) ) if maxobjects : pklist = pklist [ : maxobjects ] # fancy progress bar with tqdm if present if TQDM : listiterator = tqdm ( pklist ) else : listiterator = pklist # go through all the varfeatures arrays feature_dict = { 'objectids' : [ ] , 'magcol' : magcol , 'availablefeatures' : [ ] } LOGINFO ( 'collecting features for magcol: %s' % magcol ) for pkl in listiterator : with open ( pkl , 'rb' ) as infd : varf = pickle . load ( infd ) # update the objectid list objectid = varf [ 'objectid' ] if objectid not in feature_dict [ 'objectids' ] : feature_dict [ 'objectids' ] . append ( objectid ) thisfeatures = varf [ magcol ] if featurestouse and len ( featurestouse ) > 0 : featurestoget = featurestouse else : featurestoget = NONPERIODIC_FEATURES_TO_COLLECT # collect all the features for this magcol/objectid combination for feature in featurestoget : # update the global feature list if necessary if ( ( feature not in feature_dict [ 'availablefeatures' ] ) and ( feature in thisfeatures ) ) : feature_dict [ 'availablefeatures' ] . append ( feature ) feature_dict [ feature ] = [ ] if feature in thisfeatures : feature_dict [ feature ] . append ( thisfeatures [ feature ] ) # now that we've collected all the objects and their features, turn the list # into arrays, and then concatenate them for feat in feature_dict [ 'availablefeatures' ] : feature_dict [ feat ] = np . array ( feature_dict [ feat ] ) feature_dict [ 'objectids' ] = np . array ( feature_dict [ 'objectids' ] ) feature_array = np . column_stack ( [ feature_dict [ feat ] for feat in feature_dict [ 'availablefeatures' ] ] ) feature_dict [ 'features_array' ] = feature_array # if there's a labeldict available, use it to generate a label array. this # feature collection is now a training set. if isinstance ( labeldict , dict ) : labelarray = np . zeros ( feature_dict [ 'objectids' ] . size , dtype = np . int64 ) # populate the labels for each object in the training set for ind , objectid in enumerate ( feature_dict [ 'objectids' ] ) : if objectid in labeldict : # if this is a binary classifier training set, convert bools to # ones and zeros if labeltype == 'binary' : if labeldict [ objectid ] : labelarray [ ind ] = 1 # otherwise, use the actual class label integer elif labeltype == 'classes' : labelarray [ ind ] = labeldict [ objectid ] feature_dict [ 'labels_array' ] = labelarray feature_dict [ 'kwargs' ] = { 'pklglob' : pklglob , 'featurestouse' : featurestouse , 'maxobjects' : maxobjects , 'labeltype' : labeltype } # write the info to the output pickle with open ( outfile , 'wb' ) as outfd : pickle . dump ( feature_dict , outfd , pickle . HIGHEST_PROTOCOL ) # return the feature_dict return feature_dict
4564	def fill ( strip , item , start = 0 , stop = None , step = 1 ) : if stop is None : stop = len ( strip ) for i in range ( start , stop , step ) : strip [ i ] = item
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : # pylint: disable=dangerous-default-value conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
4421	async def set_pause ( self , pause : bool ) : await self . _lavalink . ws . send ( op = 'pause' , guildId = self . guild_id , pause = pause ) self . paused = pause
923	def _aggr_sum ( inList ) : aggrMean = _aggr_mean ( inList ) if aggrMean == None : return None aggrSum = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem else : aggrSum += aggrMean return aggrSum
1496	def get_sub_parts ( self , query ) : parts = [ ] num_open_braces = 0 delimiter = ',' last_starting_index = 0 for i in range ( len ( query ) ) : if query [ i ] == '(' : num_open_braces += 1 elif query [ i ] == ')' : num_open_braces -= 1 elif query [ i ] == delimiter and num_open_braces == 0 : parts . append ( query [ last_starting_index : i ] . strip ( ) ) last_starting_index = i + 1 parts . append ( query [ last_starting_index : ] . strip ( ) ) return parts
3835	async def set_active_client ( self , set_active_client_request ) : response = hangouts_pb2 . SetActiveClientResponse ( ) await self . _pb_request ( 'clients/setactiveclient' , set_active_client_request , response ) return response
1017	def _adaptSegment ( self , segUpdate ) : # This will be set to True if detect that any syapses were decremented to # 0 trimSegment = False # segUpdate.segment is None when creating a new segment c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment # update.activeSynapses can be empty. # If not, it can contain either or both integers and tuples. # The integers are indices of synapses to update. # The tuples represent new synapses to create (src col, src cell in col). # We pre-process to separate these various element types. # synToCreate is not empty only if positiveReinforcement is True. # NOTE: the synapse indices start at *1* to skip the segment flags. activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) # Modify an existing segment if segment is not None : if self . verbosity >= 4 : print "Reinforcing segment #%d for cell[%d,%d]" % ( segment . segID , c , i ) print " before:" , segment . debugPrint ( ) # Mark it as recently useful segment . lastActiveIteration = self . lrnIterationIdx # Update frequency and positiveActivations segment . positiveActivations += 1 # positiveActivations += 1 segment . dutyCycle ( active = True ) # First, decrement synapses that are not active # s is a synapse *index*, with index 0 in the segment being the tuple # (segId, sequence segment flag). See below, creation of segments. lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) # Now, increment active synapses activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) # Finally, create new synapses if needed # syn is now a tuple (src col, src cell) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] # If we have fixed resources, get rid of some old syns if necessary if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print " after:" , segment . debugPrint ( ) # Create a new segment else : # (segID, sequenceSegment flag, frequency, positiveActivations, # totalActivations, lastActiveIteration) newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) # numpy.float32 important so that we can match with C++ for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print "New segment #%d for cell[%d,%d]" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment
10023	def environment_name_for_cname ( self , env_cname ) : envs = self . get_environments ( ) for env in envs : if env [ 'Status' ] != 'Terminated' and 'CNAME' in env and env [ 'CNAME' ] and env [ 'CNAME' ] . lower ( ) . startswith ( env_cname . lower ( ) + '.' ) : return env [ 'EnvironmentName' ] return None
9364	def domain_name ( ) : result = random . choice ( get_dictionary ( 'company_names' ) ) . strip ( ) result += '.' + top_level_domain ( ) return result . lower ( )
2882	def get_timer_event_definition ( self , timerEventDefinition ) : timeDate = first ( self . xpath ( './/bpmn:timeDate' ) ) return TimerEventDefinition ( self . node . get ( 'name' , timeDate . text ) , self . parser . parse_condition ( timeDate . text , None , None , None , None , self ) )
4641	def reset_counter ( self ) : self . _cnt_retries = 0 for i in self . _url_counter : self . _url_counter [ i ] = 0
8077	def ellipsemode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . ellipsemode = mode return self . ellipsemode elif mode is None : return self . ellipsemode else : raise ShoebotError ( _ ( "ellipsemode: invalid input" ) )
7323	def sendmail ( message , sender , recipients , config_filename ) : # Read config file from disk to get SMTP server host, port, username if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) # Prompt for password if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) # Connect to SMTP server if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) # Send credentials if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) # Send message. Note that we can't use the elegant # "smtp.send_message(message)" because that's python3 only smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , * * kwargs )
6493	def get_mappings ( cls , index_name , doc_type ) : return cache . get ( cls . get_cache_item_name ( index_name , doc_type ) , { } )
7817	def remove_handler ( self , handler ) : with self . lock : if handler in self . handlers : self . handlers . remove ( handler ) self . _update_handlers ( )
754	def setLoggedMetrics ( self , metricNames ) : if metricNames is None : self . __metricNames = set ( [ ] ) else : self . __metricNames = set ( metricNames )
2497	def handle_package_has_file_helper ( self , pkg_file ) : nodes = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( pkg_file . name ) ) ) ) if len ( nodes ) == 1 : return nodes [ 0 ] [ 0 ] else : raise InvalidDocumentError ( 'handle_package_has_file_helper could not' + ' find file node for file: {0}' . format ( pkg_file . name ) )
9276	def apply_exclude_tags ( self , all_tags ) : filtered = copy . deepcopy ( all_tags ) for tag in all_tags : if tag [ "name" ] not in self . options . exclude_tags : self . warn_if_tag_not_found ( tag , "exclude-tags" ) else : filtered . remove ( tag ) return filtered
11203	def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6), # Because 7 % 7 = 0 weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
10437	def getapplist ( self ) : app_list = [ ] # Update apps list, before parsing the list self . _update_apps ( ) for gui in self . _running_apps : name = gui . localizedName ( ) # default type was objc.pyobjc_unicode # convert to Unicode, else exception is thrown # TypeError: "cannot marshal <type 'objc.pyobjc_unicode'> objects" try : name = unicode ( name ) except NameError : name = str ( name ) except UnicodeEncodeError : pass app_list . append ( name ) # Return unique application list return list ( set ( app_list ) )
9568	def get_chat_id ( self , message ) : if message . chat . type == 'private' : return message . user . id return message . chat . id
3007	def _redirect_with_params ( url_name , * args , * * kwargs ) : url = urlresolvers . reverse ( url_name , args = args ) params = parse . urlencode ( kwargs , True ) return "{0}?{1}" . format ( url , params )
727	def numbersForBit ( self , bit ) : if bit >= self . _n : raise IndexError ( "Invalid bit" ) numbers = set ( ) for index , pattern in self . _patterns . iteritems ( ) : if bit in pattern : numbers . add ( index ) return numbers
2723	def take_snapshot ( self , snapshot_name , return_dict = True , power_off = False ) : if power_off is True and self . status != "off" : action = self . power_off ( return_dict = False ) action . wait ( ) self . load ( ) return self . _perform_action ( { "type" : "snapshot" , "name" : snapshot_name } , return_dict )
11249	def average ( numbers , numtype = 'float' ) : if type == 'decimal' : return Decimal ( sum ( numbers ) ) / len ( numbers ) else : return float ( sum ( numbers ) ) / len ( numbers )
2539	def set_pkg_excl_file ( self , doc , filename ) : self . assert_package_exists ( ) doc . package . add_exc_file ( filename )
8183	def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
8782	def select_ipam_strategy ( self , network_id , network_strategy , * * kwargs ) : LOG . info ( "Selecting IPAM strategy for network_id:%s " "network_strategy:%s" % ( network_id , network_strategy ) ) net_type = "tenant" if STRATEGY . is_provider_network ( network_id ) : net_type = "provider" strategy = self . _ipam_strategies . get ( net_type , { } ) default = strategy . get ( "default" ) overrides = strategy . get ( "overrides" , { } ) # If we override a particular strategy explicitly, we use it. if network_strategy in overrides : LOG . info ( "Selected overridden IPAM strategy: %s" % ( overrides [ network_strategy ] ) ) return overrides [ network_strategy ] # Otherwise, we are free to use an explicit default. if default : LOG . info ( "Selected default IPAM strategy for tenant " "network: %s" % ( default ) ) return default # Fallback to the network-specified IPAM strategy LOG . info ( "Selected network strategy for tenant " "network: %s" % ( network_strategy ) ) return network_strategy
7390	def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
3626	def pad_to ( unpadded , target_len ) : under = target_len - len ( unpadded ) if under <= 0 : return unpadded return unpadded + ( ' ' * under )
8468	def parseConfig ( cls , value ) : if 'enabled' in value : value [ 'enabled' ] = bool ( value [ 'enabled' ] ) if 'exclude_paths' in value : value [ 'exclude_paths' ] = [ n . strip ( ) for n in ast . literal_eval ( value [ 'exclude_paths' ] ) ] return value
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
9678	def read_info_string ( self ) : infostring = [ ] # Send the command byte and sleep for 9 ms self . cnxn . xfer ( [ 0x3F ] ) sleep ( 9e-3 ) # Read the info string by sending 60 empty bytes for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] infostring . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( infostring )
13010	def print_line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) except ValueError : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IOError : sys . exit ( 0 )
10121	def from_dict ( cls , spec ) : spec = spec . copy ( ) center = spec . pop ( 'center' , None ) radius = spec . pop ( 'radius' , None ) if center and radius : return cls . circle ( center , radius , * * spec ) vertices = spec . pop ( 'vertices' ) if len ( vertices ) == 2 : return cls . rectangle ( vertices , * * spec ) return cls ( vertices , * * spec )
10346	def merge_namespaces ( input_locations , output_path , namespace_name , namespace_keyword , namespace_domain , author_name , citation_name , namespace_description = None , namespace_species = None , namespace_version = None , namespace_query_url = None , namespace_created = None , author_contact = None , author_copyright = None , citation_description = None , citation_url = None , citation_version = None , citation_date = None , case_sensitive = True , delimiter = '|' , cacheable = True , functions = None , value_prefix = '' , sort_key = None , check_keywords = True ) : results = get_merged_namespace_names ( input_locations , check_keywords = check_keywords ) with open ( output_path , 'w' ) as file : write_namespace ( namespace_name = namespace_name , namespace_keyword = namespace_keyword , namespace_domain = namespace_domain , author_name = author_name , citation_name = citation_name , values = results , namespace_species = namespace_species , namespace_description = namespace_description , namespace_query_url = namespace_query_url , namespace_version = namespace_version , namespace_created = namespace_created , author_contact = author_contact , author_copyright = author_copyright , citation_description = citation_description , citation_url = citation_url , citation_version = citation_version , citation_date = citation_date , case_sensitive = case_sensitive , delimiter = delimiter , cacheable = cacheable , functions = functions , value_prefix = value_prefix , sort_key = sort_key , file = file )
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
3631	def md_table ( table , * , padding = DEFAULT_PADDING , divider = '|' , header_div = '-' ) : table = normalize_cols ( table ) table = pad_cells ( table ) header = table [ 0 ] body = table [ 1 : ] col_widths = [ len ( cell ) for cell in header ] horiz = horiz_div ( col_widths , header_div , divider , padding ) header = add_dividers ( header , divider , padding ) body = [ add_dividers ( row , divider , padding ) for row in body ] table = [ header , horiz ] table . extend ( body ) table = [ row . rstrip ( ) for row in table ] return '\n' . join ( table )
5724	def verify_valid_gdb_subprocess ( self ) : if not self . gdb_process : raise NoGdbProcessError ( "gdb process is not attached" ) elif self . gdb_process . poll ( ) is not None : raise NoGdbProcessError ( "gdb process has already finished with return code: %s" % str ( self . gdb_process . poll ( ) ) )
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) # or IS_REQUIREMENTS_RE.match(line) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
806	def enableTap ( self , tapPath ) : self . _tapFileIn = open ( tapPath + '.in' , 'w' ) self . _tapFileOut = open ( tapPath + '.out' , 'w' )
13112	def get_configured_dns ( ) : ips = [ ] try : output = subprocess . check_output ( [ 'nmcli' , 'device' , 'show' ] ) output = output . decode ( 'utf-8' ) for line in output . split ( '\n' ) : if 'DNS' in line : pattern = r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" for hit in re . findall ( pattern , line ) : ips . append ( hit ) except FileNotFoundError : pass return ips
9170	def declare_api_routes ( config ) : add_route = config . add_route add_route ( 'get-content' , '/contents/{ident_hash}' ) add_route ( 'get-resource' , '/resources/{hash}' ) # User actions API add_route ( 'license-request' , '/contents/{uuid}/licensors' ) add_route ( 'roles-request' , '/contents/{uuid}/roles' ) add_route ( 'acl-request' , '/contents/{uuid}/permissions' ) # Publishing API add_route ( 'publications' , '/publications' ) add_route ( 'get-publication' , '/publications/{id}' ) add_route ( 'publication-license-acceptance' , '/publications/{id}/license-acceptances/{uid}' ) add_route ( 'publication-role-acceptance' , '/publications/{id}/role-acceptances/{uid}' ) # TODO (8-May-12017) Remove because the term collate is being phased out. add_route ( 'collate-content' , '/contents/{ident_hash}/collate-content' ) add_route ( 'bake-content' , '/contents/{ident_hash}/baked' ) # Moderation routes add_route ( 'moderation' , '/moderations' ) add_route ( 'moderate' , '/moderations/{id}' ) add_route ( 'moderation-rss' , '/feeds/moderations.rss' ) # API Key routes add_route ( 'api-keys' , '/api-keys' ) add_route ( 'api-key' , '/api-keys/{id}' )
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
4702	def create ( ) : if env ( ) : cij . err ( "cij.lnvm.create: Invalid LNVM ENV" ) return 1 nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) lnvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cij . emph ( "lnvm.create: LNVM_DEV_NAME: %s" % lnvm [ "DEV_NAME" ] ) cmd = [ "nvme lnvm create -d %s -n %s -t %s -b %s -e %s -f" % ( nvme [ "DEV_NAME" ] , lnvm [ "DEV_NAME" ] , lnvm [ "DEV_TYPE" ] , lnvm [ "BGN" ] , lnvm [ "END" ] ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) if rcode : cij . err ( "cij.lnvm.create: FAILED" ) return 1 return 0
834	def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 #activeArray[column]=1 if column is active after spatial pooling self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
5431	def _get_filtered_mounts ( mounts , mount_param_type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount_param_type ) ] )
1626	def ReverseCloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if line [ pos ] not in ')}]>' : return ( line , 0 , - 1 ) # Check last line ( start_pos , stack ) = FindStartOfExpressionInLine ( line , pos , [ ] ) if start_pos > - 1 : return ( line , linenum , start_pos ) # Continue scanning backward while stack and linenum > 0 : linenum -= 1 line = clean_lines . elided [ linenum ] ( start_pos , stack ) = FindStartOfExpressionInLine ( line , len ( line ) - 1 , stack ) if start_pos > - 1 : return ( line , linenum , start_pos ) # Did not find start of expression before beginning of file, give up return ( line , 0 , - 1 )
12617	def get_data ( img ) : if hasattr ( img , '_data_cache' ) and img . _data_cache is None : # Copy locally the nifti_image to avoid the side effect of data # loading img = copy . deepcopy ( img ) # force garbage collector gc . collect ( ) return img . get_data ( )
3332	def init_logging ( config ) : verbose = config . get ( "verbose" , 3 ) enable_loggers = config . get ( "enable_loggers" , [ ] ) if enable_loggers is None : enable_loggers = [ ] logger_date_format = config . get ( "logger_date_format" , "%Y-%m-%d %H:%M:%S" ) logger_format = config . get ( "logger_format" , "%(asctime)s.%(msecs)03d - <%(thread)d> %(name)-27s %(levelname)-8s: %(message)s" , ) formatter = logging . Formatter ( logger_format , logger_date_format ) # Define handlers consoleHandler = logging . StreamHandler ( sys . stdout ) # consoleHandler = logging.StreamHandler(sys.stderr) consoleHandler . setFormatter ( formatter ) # consoleHandler.setLevel(logging.DEBUG) # Add the handlers to the base logger logger = logging . getLogger ( BASE_LOGGER_NAME ) if verbose >= 4 : # --verbose logger . setLevel ( logging . DEBUG ) elif verbose == 3 : # default logger . setLevel ( logging . INFO ) elif verbose == 2 : # --quiet logger . setLevel ( logging . WARN ) # consoleHandler.setLevel(logging.WARN) elif verbose == 1 : # -qq logger . setLevel ( logging . ERROR ) # consoleHandler.setLevel(logging.WARN) else : # -qqq logger . setLevel ( logging . CRITICAL ) # consoleHandler.setLevel(logging.ERROR) # Don't call the root's handlers after our custom handlers logger . propagate = False # Remove previous handlers for hdlr in logger . handlers [ : ] : # Must iterate an array copy try : hdlr . flush ( ) hdlr . close ( ) except Exception : pass logger . removeHandler ( hdlr ) logger . addHandler ( consoleHandler ) if verbose >= 3 : for e in enable_loggers : if not e . startswith ( BASE_LOGGER_NAME + "." ) : e = BASE_LOGGER_NAME + "." + e lg = logging . getLogger ( e . strip ( ) ) lg . setLevel ( logging . DEBUG )
4166	def tf2zp ( b , a ) : from numpy import roots assert len ( b ) == len ( a ) , "length of the vectors a and b must be identical. fill with zeros if needed." g = b [ 0 ] / a [ 0 ] z = roots ( b ) p = roots ( a ) return z , p , g
5842	def get_design_run_status ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_status ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) status = response [ "data" ] return ProcessStatus ( result = status . get ( "result" ) , progress = status . get ( "progress" ) , status = status . get ( "status" ) , messages = status . get ( "messages" ) )
9284	def connect ( self , blocking = False , retry = 30 ) : if self . _connected : return while True : try : self . _connect ( ) if not self . skip_login : self . _send_login ( ) break except ( LoginError , ConnectionError ) : if not blocking : raise self . logger . info ( "Retrying connection is %d seconds." % retry ) time . sleep ( retry )
9645	def _get_detail_value ( var , attr ) : value = getattr ( var , attr ) # Rename common Django class names kls = getattr ( getattr ( value , '__class__' , '' ) , '__name__' , '' ) if kls in ( 'ManyRelatedManager' , 'RelatedManager' , 'EmptyManager' ) : return kls if callable ( value ) : return 'routine' return value
11143	def is_name_allowed ( self , path ) : assert isinstance ( path , basestring ) , "given path must be a string" name = os . path . basename ( path ) if not len ( name ) : return False , "empty name is not allowed" # exact match for em in [ self . __repoLock , self . __repoFile , self . __dirInfo , self . __dirLock ] : if name == em : return False , "name '%s' is reserved for pyrep internal usage" % em # pattern match for pm in [ self . __fileInfo , self . __fileLock ] : #,self.__objectDir]: if name == pm or ( name . endswith ( pm [ 3 : ] ) and name . startswith ( '.' ) ) : return False , "name pattern '%s' is not allowed as result may be reserved for pyrep internal usage" % pm # name is ok return True , None
9058	def gradient ( self ) : self . _update_approx ( ) g = self . _ep . lml_derivatives ( self . _X ) ed = exp ( - self . logitdelta ) es = exp ( self . logscale ) grad = dict ( ) grad [ "logitdelta" ] = g [ "delta" ] * ( ed / ( 1 + ed ) ) / ( 1 + ed ) grad [ "logscale" ] = g [ "scale" ] * es grad [ "beta" ] = g [ "mean" ] return grad
11591	def _rc_sunion ( self , src , * args ) : args = list_or_args ( src , args ) src_set = self . smembers ( args . pop ( 0 ) ) if src_set is not set ( [ ] ) : for key in args : src_set . update ( self . smembers ( key ) ) return src_set
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
2324	def predict ( self , x , * args , * * kwargs ) : if len ( args ) > 0 : if type ( args [ 0 ] ) == nx . Graph or type ( args [ 0 ] ) == nx . DiGraph : return self . orient_graph ( x , * args , * * kwargs ) else : return self . predict_proba ( x , * args , * * kwargs ) elif type ( x ) == DataFrame : return self . predict_dataset ( x , * args , * * kwargs ) elif type ( x ) == Series : return self . predict_proba ( x . iloc [ 0 ] , x . iloc [ 1 ] , * args , * * kwargs )
10954	def model_to_data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set_image ( util . NullImage ( image = im ) )
8456	def up_to_date ( version = None ) : temple . check . in_git_repo ( ) temple . check . is_temple_project ( ) temple_config = temple . utils . read_temple_config ( ) old_template_version = temple_config [ '_version' ] new_template_version = version or _get_latest_template_version ( temple_config [ '_template' ] ) return new_template_version == old_template_version
540	def __createModelCheckpoint ( self ) : if self . _model is None or self . _modelCheckpointGUID is None : return # Create an output store, if one doesn't exist already if self . _predictionLogger is None : self . _createPredictionLogger ( ) predictions = StringIO . StringIO ( ) self . _predictionLogger . checkpoint ( checkpointSink = predictions , maxRows = int ( Configuration . get ( 'nupic.model.checkpoint.maxPredictionRows' ) ) ) self . _model . save ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : str ( self . _modelCheckpointGUID ) } , ignoreUnchanged = True ) self . _logger . info ( "Checkpointed Hypersearch Model: modelID: %r, " "checkpointID: %r" , self . _modelID , checkpointID ) return
8920	def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
9299	def apply_filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )
10612	def _calculate_T ( self , H ) : # Create the initial guesses for temperature. x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) # Evaluate the enthalpy for the initial guesses. y = list ( ) y . append ( self . _calculate_H ( x [ 0 ] ) - H ) y . append ( self . _calculate_H ( x [ 1 ] ) - H ) # Solve for temperature. for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_H ( x [ i ] ) - H ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
7562	def _run_qmc ( self , boot ) : ## build command self . _tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] ## run it proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise IPyradWarningExit ( res [ 1 ] ) ## parse tmp file written by qmc into a tree and rename it with open ( self . _tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) ## save the tree to file if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) ## save the file self . _save ( )
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
6550	def from_func ( cls , func , variables , vartype , name = None ) : variables = tuple ( variables ) configurations = frozenset ( config for config in itertools . product ( vartype . value , repeat = len ( variables ) ) if func ( * config ) ) return cls ( func , configurations , variables , vartype , name )
2110	def send ( source = None , prevent = None , exclude = None , secret_management = 'default' , no_color = False ) : from tower_cli . cli . transfer . send import Sender sender = Sender ( no_color ) sender . send ( source , prevent , exclude , secret_management )
4640	def find_next ( self ) : if int ( self . num_retries ) < 0 : # pragma: no cover self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( # Only provide URLS if num_retries is bigger equal 0, # i.e. we want to do reconnects at all int ( self . num_retries ) >= 0 # the counter for this host/endpoint should be smaller than # num_retries and v <= self . num_retries # let's not retry with the same URL *if* we have others # available and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
7904	def join ( self , room , nick , handler , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if not room . node or room . resource : raise ValueError ( "Invalid room JID" ) room_jid = JID ( room . node , room . domain , nick ) cur_rs = self . rooms . get ( room_jid . bare ( ) . as_unicode ( ) ) if cur_rs and cur_rs . joined : raise RuntimeError ( "Room already joined" ) rs = MucRoomState ( self , self . stream . me , room_jid , handler ) self . rooms [ room_jid . bare ( ) . as_unicode ( ) ] = rs rs . join ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) return rs
1817	def SETNS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , 1 , 0 ) )
4018	def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
12060	def TK_ask ( title , msg ) : root = tkinter . Tk ( ) root . attributes ( "-topmost" , True ) #always on top root . withdraw ( ) #hide tk window result = tkinter . messagebox . askyesno ( title , msg ) root . destroy ( ) return result
9061	def beta_covariance ( self ) : from numpy_sugar . linalg import ddot tX = self . _X [ "tX" ] Q = concatenate ( self . _QS [ 0 ] , axis = 1 ) S0 = self . _QS [ 1 ] D = self . v0 * S0 + self . v1 D = D . tolist ( ) + [ self . v1 ] * ( len ( self . _y ) - len ( D ) ) D = asarray ( D ) A = inv ( tX . T @ ( Q @ ddot ( 1 / D , Q . T @ tX ) ) ) VT = self . _X [ "VT" ] H = lstsq ( VT , A , rcond = None ) [ 0 ] return lstsq ( VT , H . T , rcond = None ) [ 0 ]
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) # Note that we convert positions to percentages *after* the checks # above, since get_turnover() expects positions in dollars. positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
1419	def get_scheduler_location ( self , topologyName , callback = None ) : isWatching = False # Temp dict used to return result # if callback is not provided. ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : """ Custom callback to get the scheduler location right now. """ ret [ "result" ] = data self . _get_scheduler_location_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
1719	def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . _emit_statement_list ( what ) else : return getattr ( self , what [ 'type' ] ) ( * * what )
951	def showPredictions ( ) : for k in range ( 6 ) : tm . reset ( ) print "--- " + "ABCDXY" [ k ] + " ---" tm . compute ( set ( seqT [ k ] [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = False ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] print ( "Active cols: " + str ( np . nonzero ( currentColumns ) [ 0 ] ) ) print ( "Predicted cols: " + str ( np . nonzero ( predictedColumns ) [ 0 ] ) ) print ""
5722	def _restore_resources ( resources ) : resources = deepcopy ( resources ) for resource in resources : schema = resource [ 'schema' ] for fk in schema . get ( 'foreignKeys' , [ ] ) : _ , name = _restore_path ( fk [ 'reference' ] [ 'resource' ] ) fk [ 'reference' ] [ 'resource' ] = name return resources
11029	def sse_content ( response , handler , * * sse_kwargs ) : # An SSE response must be 200/OK and have content-type 'text/event-stream' raise_for_not_ok_status ( response ) raise_for_header ( response , 'Content-Type' , 'text/event-stream' ) finished , _ = _sse_content_with_protocol ( response , handler , * * sse_kwargs ) return finished
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
6884	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = [ ( lctimes [ x ] - lctimes [ x - 1 ] ) for x in range ( 1 , len ( lctimes ) ) ] lc_time_diffs = np . array ( lc_time_diffs ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) # at the end, add the slice for the last group to the end of the times # array group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) # if there's no large gap in the LC, then there's only one group to worry # about else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
7738	def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
5636	def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec # headline head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] # main sections if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) # API section md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
13578	def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get_courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . DoesNotExist : old = None if old : old . details_url = course [ "details_url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details_url = course [ "details_url" ] ) else : selected = Course . get_selected ( ) # with Spinner.context(msg="Updated exercise metadata.", # waitmsg="Updating exercise metadata."): print ( "Updating exercise data." ) for exercise in api . get_exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . DoesNotExist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is_attempted = exercise [ "attempted" ] old . is_completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is_downloaded = os . path . isdir ( old . path ( ) ) old . return_url = exercise [ "return_url" ] old . zip_url = exercise [ "zip_url" ] old . submissions_url = exercise [ "exercise_submissions_url" ] old . save ( ) download_exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is_attempted = exercise [ "attempted" ] , is_completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return_url = exercise [ "return_url" ] , zip_url = exercise [ "zip_url" ] , submissions_url = exercise [ ( "exercise_" "submissions_" "url" ) ] ) ex . is_downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
9640	def require_template_debug ( f ) : def _ ( * args , * * kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , * * kwargs ) if TEMPLATE_DEBUG else '' return _
12685	def pods ( self ) : # Return empty list if xml_tree is not defined (error Result object) if not self . xml_tree : return [ ] # Create a Pod object for every pod group in xml return [ Pod ( elem ) for elem in self . xml_tree . findall ( 'pod' ) ]
1522	def is_self ( addr ) : ips = [ ] for i in netifaces . interfaces ( ) : entry = netifaces . ifaddresses ( i ) if netifaces . AF_INET in entry : for ipv4 in entry [ netifaces . AF_INET ] : if "addr" in ipv4 : ips . append ( ipv4 [ "addr" ] ) return addr in ips or addr == get_self_hostname ( )
5200	def Select ( self , command , index ) : OutstationApplication . process_point_value ( 'Select' , command , index , None ) return opendnp3 . CommandStatus . SUCCESS
2762	def get_all_certificates ( self ) : data = self . get_data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( * * jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
12347	def field_metadata ( self , well_row = 0 , well_column = 0 , field_row = 0 , field_column = 0 ) : def condition ( path ) : attrs = attributes ( path ) return ( attrs . u == well_column and attrs . v == well_row and attrs . x == field_column and attrs . y == field_row ) field = [ f for f in self . fields if condition ( f ) ] if field : field = field [ 0 ] filename = _pattern ( field , 'metadata' , _image , extension = '*.ome.xml' ) filename = glob ( filename ) [ 0 ] # resolve, assume found return objectify . parse ( filename ) . getroot ( )
2419	def write_extracted_licenses ( lics , out ) : write_value ( 'LicenseID' , lics . identifier , out ) if lics . full_name is not None : write_value ( 'LicenseName' , lics . full_name , out ) if lics . comment is not None : write_text_value ( 'LicenseComment' , lics . comment , out ) for xref in sorted ( lics . cross_ref ) : write_value ( 'LicenseCrossReference' , xref , out ) write_text_value ( 'ExtractedText' , lics . text , out )
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
4667	def refresh ( self ) : dict . __init__ ( self , self . blockchain . rpc . get_object ( self . identifier ) , blockchain_instance = self . blockchain , )
10469	def launchAppByBundlePath ( bundlePath , arguments = None ) : if arguments is None : arguments = [ ] bundleUrl = NSURL . fileURLWithPath_ ( bundlePath ) workspace = AppKit . NSWorkspace . sharedWorkspace ( ) arguments_strings = list ( map ( lambda a : NSString . stringWithString_ ( str ( a ) ) , arguments ) ) arguments = NSDictionary . dictionaryWithDictionary_ ( { AppKit . NSWorkspaceLaunchConfigurationArguments : NSArray . arrayWithArray_ ( arguments_strings ) } ) return workspace . launchApplicationAtURL_options_configuration_error_ ( bundleUrl , AppKit . NSWorkspaceLaunchAllowingClassicStartup , arguments , None )
10380	def calculate_concordance_probability_by_annotation ( graph , annotation , key , cutoff = None , permutations = None , percentage = None , use_ambiguous = False ) : result = [ ( value , calculate_concordance_probability ( subgraph , key , cutoff = cutoff , permutations = permutations , percentage = percentage , use_ambiguous = use_ambiguous , ) ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) ] return dict ( result )
9623	def maybe_decode_header ( header ) : value , encoding = decode_header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value
7961	def disconnect ( self ) : logger . debug ( "TCPTransport.disconnect()" ) with self . lock : if self . _socket is None : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) return if self . _hup or not self . _serializer : self . _close ( ) else : self . send_stream_tail ( )
10772	def filled_contour ( self , min = None , max = None ) : # pylint: disable=redefined-builtin,redefined-outer-name # Get the contour vertices. if min is None : min = np . finfo ( np . float64 ) . min if max is None : max = np . finfo ( np . float64 ) . max vertices , codes = ( self . _contour_generator . create_filled_contour ( min , max ) ) return self . formatter ( ( min , max ) , vertices , codes )
10830	def create ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls ( group = group , admin = admin , ) db . session . add ( obj ) return obj
13460	def event_all_comments_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) comments = event . all_comments page = int ( request . GET . get ( 'page' , 99999 ) ) # feed empty page by default to push to last page is_paginated = False if comments : paginator = Paginator ( comments , 50 ) # Show 50 comments per page try : comments = paginator . page ( page ) except EmptyPage : # If page is out of range (e.g. 9999), deliver last page of results. comments = paginator . page ( paginator . num_pages ) is_paginated = comments . has_other_pages ( ) return render ( request , 'happenings/event_comments.html' , { "event" : event , "comment_list" : comments , "object_list" : comments , "page_obj" : comments , "page" : page , "is_paginated" : is_paginated , "key" : key } )
3838	async def set_group_link_sharing_enabled ( self , set_group_link_sharing_enabled_request ) : response = hangouts_pb2 . SetGroupLinkSharingEnabledResponse ( ) await self . _pb_request ( 'conversations/setgrouplinksharingenabled' , set_group_link_sharing_enabled_request , response ) return response
11876	def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
10686	def Cp ( self , phase , T ) : if phase not in self . _phases : raise Exception ( "The phase '%s' was not found in compound '%s'." % ( phase , self . formula ) ) return self . _phases [ phase ] . Cp ( T )
3748	def calculate ( self , T , method ) : if method == GHARAGHEIZI : mu = Gharagheizi_gas_viscosity ( T , self . Tc , self . Pc , self . MW ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'g' ) elif method == DIPPR_PERRY_8E : mu = EQ102 ( T , * self . Perrys2_312_coeffs ) elif method == VDI_PPDS : mu = horner ( self . VDI_PPDS_coeffs , T ) elif method == YOON_THODOS : mu = Yoon_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == STIEL_THODOS : mu = Stiel_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == LUCAS_GAS : mu = lucas_gas ( T , self . Tc , self . Pc , self . Zc , self . MW , self . dipole , CASRN = self . CASRN ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
6699	def get_selections ( ) : with settings ( hide ( 'stdout' ) ) : res = run_as_root ( 'dpkg --get-selections' ) selections = dict ( ) for line in res . splitlines ( ) : package , status = line . split ( ) selections . setdefault ( status , list ( ) ) . append ( package ) return selections
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
13416	def linkcode_resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except : return None try : fn = inspect . getsourcefile ( obj ) except : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except : lineno = None if lineno : linespec = "#L%d-L%d" % ( lineno , lineno + len ( source ) - 1 ) else : linespec = "" fn = relpath ( fn , start = dirname ( scisalt . __file__ ) ) if 'dev' in scisalt . __version__ : return "http://github.com/joelfrederico/SciSalt/blob/master/scisalt/%s%s" % ( fn , linespec ) else : return "http://github.com/joelfrederico/SciSalt/blob/v%s/scisalt/%s%s" % ( scisalt . __version__ , fn , linespec )
6552	def check ( self , solution ) : return self . func ( * ( solution [ v ] for v in self . variables ) )
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
893	def cellsForColumn ( self , column ) : self . _validateColumn ( column ) start = self . cellsPerColumn * column end = start + self . cellsPerColumn return range ( start , end )
9900	def data ( self , data ) : if self . is_caching : self . cache = data else : fcontents = self . file_contents with open ( self . path , "w" ) as f : try : # Write the file. Keep user settings about indentation, etc indent = self . indent if self . pretty else None json . dump ( data , f , sort_keys = self . sort_keys , indent = indent ) except Exception as e : # Rollback to prevent data loss f . seek ( 0 ) f . truncate ( ) f . write ( fcontents ) # And re-raise the exception raise e self . _updateType ( )
6138	def set_default_sim_param ( self , * args , * * kwargs ) : if len ( args ) is 1 and isinstance ( args [ 0 ] , SimulationParameter ) : self . __default_param = args [ 0 ] else : self . __default_param = SimulationParameter ( * args , * * kwargs ) return
4073	def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
10812	def query_by_user ( cls , user , with_pending = False , eager = False ) : q1 = Group . query . join ( Membership ) . filter_by ( user_id = user . get_id ( ) ) if not with_pending : q1 = q1 . filter_by ( state = MembershipState . ACTIVE ) if eager : q1 = q1 . options ( joinedload ( Group . members ) ) q2 = Group . query . join ( GroupAdmin ) . filter_by ( admin_id = user . get_id ( ) , admin_type = resolve_admin_type ( user ) ) if eager : q2 = q2 . options ( joinedload ( Group . members ) ) query = q1 . union ( q2 ) . with_entities ( Group . id ) return Group . query . filter ( Group . id . in_ ( query ) )
2192	def renew ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = { 'timestamp' : util_time . timestamp ( ) , 'product' : products , } if products is not None : if not all ( map ( os . path . exists , products ) ) : raise IOError ( 'The stamped product must exist: {}' . format ( products ) ) certificate [ 'product_file_hash' ] = self . _product_file_hash ( products ) self . cacher . save ( certificate , cfgstr = cfgstr ) return certificate
4074	def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
12776	def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
3158	def _post ( self , url , data = None ) : url = urljoin ( self . base_url , url ) try : r = self . _make_request ( * * dict ( method = 'POST' , url = url , json = data , auth = self . auth , timeout = self . timeout , hooks = self . request_hooks , headers = self . request_headers ) ) except requests . exceptions . RequestException as e : raise e else : if r . status_code >= 400 : # in case of a 500 error, the response might not be a JSON try : error_data = r . json ( ) except ValueError : error_data = { "response" : r } raise MailChimpError ( error_data ) if r . status_code == 204 : return None return r . json ( )
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 # Get the distance and angle between point and light source. d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 # If no angle is defined, # light is emitted evenly in all directions # and carries as far as the defined radius # (e.g. like a radial gradient). if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 # Normalize the light's direction and spread # between 0 and 360. angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 # Objects that fall within the spreaded direction # of the light are illuminated. d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 # Wrapping from 0 to 360: # a light source with a direction of 10 degrees # and a spread of 45 degrees illuminates # objects between 0 and 35 degrees and 350 and 360 degrees. if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 # Wrapping from 360 to 0. if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
8571	def get_nic ( self , datacenter_id , server_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s?depth=%s' % ( datacenter_id , server_id , nic_id , str ( depth ) ) ) return response
13285	def clone ( src , dst_path , skip_globals , skip_dimensions , skip_variables ) : if os . path . exists ( dst_path ) : os . unlink ( dst_path ) dst = netCDF4 . Dataset ( dst_path , 'w' ) # Global attributes for attname in src . ncattrs ( ) : if attname not in skip_globals : setattr ( dst , attname , getattr ( src , attname ) ) # Dimensions unlimdim = None unlimdimname = False for dimname , dim in src . dimensions . items ( ) : # Skip what we need to if dimname in skip_dimensions : continue if dim . isunlimited ( ) : unlimdim = dim unlimdimname = dimname dst . createDimension ( dimname , None ) else : dst . createDimension ( dimname , len ( dim ) ) # Variables for varname , ncvar in src . variables . items ( ) : # Skip what we need to if varname in skip_variables : continue hasunlimdim = False if unlimdimname and unlimdimname in ncvar . dimensions : hasunlimdim = True filler = None if hasattr ( ncvar , '_FillValue' ) : filler = ncvar . _FillValue if ncvar . chunking == "contiguous" : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler ) else : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler , chunksizes = ncvar . chunking ( ) ) # Attributes for attname in ncvar . ncattrs ( ) : if attname == '_FillValue' : continue else : setattr ( var , attname , getattr ( ncvar , attname ) ) # Data nchunk = 1000 if hasunlimdim : if nchunk : start = 0 stop = len ( unlimdim ) step = nchunk if step < 1 : step = 1 for n in range ( start , stop , step ) : nmax = n + nchunk if nmax > len ( unlimdim ) : nmax = len ( unlimdim ) idata = ncvar [ n : nmax ] var [ n : nmax ] = idata else : idata = ncvar [ : ] var [ 0 : len ( unlimdim ) ] = idata else : idata = ncvar [ : ] var [ : ] = idata dst . sync ( ) src . close ( ) dst . close ( )
3843	def from_timestamp ( microsecond_timestamp ) : # Create datetime without losing precision from floating point (yes, this # is actually needed): return datetime . datetime . fromtimestamp ( microsecond_timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond_timestamp % 1000000 ) )
1467	def process ( self , tup ) : curtime = int ( time . time ( ) ) self . current_tuples . append ( ( tup , curtime ) ) self . _expire ( curtime )
2904	def new_workflow ( self , workflow_spec , read_only = False , * * kwargs ) : return BpmnWorkflow ( workflow_spec , read_only = read_only , * * kwargs )
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
194	def AssertLambda ( func_images = None , func_heatmaps = None , func_keypoints = None , func_polygons = None , name = None , deterministic = False , random_state = None ) : def func_images_assert ( images , random_state , parents , hooks ) : ia . do_assert ( func_images ( images , random_state , parents , hooks ) , "Input images did not fulfill user-defined assertion in AssertLambda." ) return images def func_heatmaps_assert ( heatmaps , random_state , parents , hooks ) : ia . do_assert ( func_heatmaps ( heatmaps , random_state , parents , hooks ) , "Input heatmaps did not fulfill user-defined assertion in AssertLambda." ) return heatmaps def func_keypoints_assert ( keypoints_on_images , random_state , parents , hooks ) : ia . do_assert ( func_keypoints ( keypoints_on_images , random_state , parents , hooks ) , "Input keypoints did not fulfill user-defined assertion in AssertLambda." ) return keypoints_on_images def func_polygons_assert ( polygons_on_images , random_state , parents , hooks ) : ia . do_assert ( func_polygons ( polygons_on_images , random_state , parents , hooks ) , "Input polygons did not fulfill user-defined assertion in AssertLambda." ) return polygons_on_images if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Lambda ( func_images_assert if func_images is not None else None , func_heatmaps_assert if func_heatmaps is not None else None , func_keypoints_assert if func_keypoints is not None else None , func_polygons_assert if func_polygons is not None else None , name = name , deterministic = deterministic , random_state = random_state )
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( * * filter_kwargs ) : yield integrated_channel
90	def copy_random_state ( random_state , force_copy = False ) : if random_state == np . random and not force_copy : return random_state else : rs_copy = dummy_random_state ( ) orig_state = random_state . get_state ( ) rs_copy . set_state ( orig_state ) return rs_copy
726	def addNoise ( self , bits , amount ) : newBits = set ( ) for bit in bits : if self . _random . getReal64 ( ) < amount : newBits . add ( self . _random . getUInt32 ( self . _n ) ) else : newBits . add ( bit ) return newBits
1697	def reduce_by_window ( self , window_config , reduce_function ) : from heronpy . streamlet . impl . reducebywindowbolt import ReduceByWindowStreamlet reduce_streamlet = ReduceByWindowStreamlet ( window_config , reduce_function , self ) self . _add_child ( reduce_streamlet ) return reduce_streamlet
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
5427	def _wait_for_any_job ( provider , job_ids , poll_interval ) : if not job_ids : return while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = job_ids ) running_jobs = set ( ) failed_jobs = set ( ) for t in tasks : status = t . get_field ( 'task-status' ) job_id = t . get_field ( 'job-id' ) if status in [ 'FAILURE' , 'CANCELED' ] : failed_jobs . add ( job_id ) if status == 'RUNNING' : running_jobs . add ( job_id ) remaining_jobs = running_jobs . difference ( failed_jobs ) if failed_jobs or len ( remaining_jobs ) != len ( job_ids ) : return remaining_jobs SLEEP_FUNCTION ( poll_interval )
12850	def _remove_from_world ( self ) : self . on_remove_from_world ( ) self . _extensions = { } self . _disable_forum_observation ( ) self . _world = None self . _id = None
10296	def get_incorrect_names_by_namespace ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) and exc . namespace == namespace }
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : # get set of tile columns tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) # if tile columns are an unbroken sequence, tiles are connected and are not # passing the Antimeridian if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : # look at column gaps and try to determine the smallest distance def gen_groups ( items ) : """Groups tile columns by sequence.""" j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : # item is next in expected sequence if i == j + 1 : group . append ( i ) # gap occured, so yield existing group and create new one else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) # in case there is only one group, don't shift if len ( groups ) == 1 : return False # distance between first column of first group and last column of last group normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] # distance between last column of first group and last column of first group # but crossing the antimeridian antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] # return whether distance over antimeridian is shorter return antimeridian_distance < normal_distance else : return False
13664	def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # check if UUID already exists uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : # add the new item to the JSON file products_data . append ( item ) # save the new JSON to the temp file json . dump ( products_data , temp_file ) return True return None
444	def roi_pooling ( input , rois , pool_height , pool_width ) : # TODO(maciek): ops scope out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
9648	def parse_log_messages ( self , text ) : regex = r"commit ([0-9a-f]+)\nAuthor: (.*?)\n\n(.*?)(?:\n\n|$)" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r"\s*<.*?>" , "" , author ) , # Remove email address if present message . strip ( ) ) ) return parsed
3728	def Zc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ COMBINED ] ) : def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Zc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Zc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Zc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Zc' ] ) : methods . append ( PSRK ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Zc' ] ) : methods . append ( YAWS ) if Tc ( CASRN ) and Vc ( CASRN ) and Pc ( CASRN ) : methods . append ( COMBINED ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == IUPAC : _Zc = float ( _crit_IUPAC . at [ CASRN , 'Zc' ] ) elif Method == PSRK : _Zc = float ( _crit_PSRKR4 . at [ CASRN , 'Zc' ] ) elif Method == MATTHEWS : _Zc = float ( _crit_Matthews . at [ CASRN , 'Zc' ] ) elif Method == CRC : _Zc = float ( _crit_CRC . at [ CASRN , 'Zc' ] ) elif Method == YAWS : _Zc = float ( _crit_Yaws . at [ CASRN , 'Zc' ] ) elif Method == COMBINED : _Zc = Vc ( CASRN ) * Pc ( CASRN ) / Tc ( CASRN ) / R elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Zc
9324	def refresh_collections ( self , accept = MEDIA_TYPE_TAXII_V20 ) : url = self . url + "collections/" response = self . _conn . get ( url , headers = { "Accept" : accept } ) self . _collections = [ ] for item in response . get ( "collections" , [ ] ) : # optional collection_url = url + item [ "id" ] + "/" collection = Collection ( collection_url , conn = self . _conn , collection_info = item ) self . _collections . append ( collection ) self . _loaded_collections = True
3079	def get ( http , path , root = METADATA_ROOT , recursive = None ) : url = urlparse . urljoin ( root , path ) url = _helpers . _add_query_parameter ( url , 'recursive' , recursive ) response , content = transport . request ( http , url , headers = METADATA_HEADERS ) if response . status == http_client . OK : decoded = _helpers . _from_bytes ( content ) if response [ 'content-type' ] == 'application/json' : return json . loads ( decoded ) else : return decoded else : raise http_client . HTTPException ( 'Failed to retrieve {0} from the Google Compute Engine' 'metadata service. Response:\n{1}' . format ( url , response ) )
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
6313	def print ( self ) : print ( "---[ START {} ]---" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( "{}: {}" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( "---[ END {} ]---" . format ( self . name ) )
12190	def _format_message ( self , channel , text ) : payload = { 'type' : 'message' , 'id' : next ( self . _msg_ids ) } payload . update ( channel = channel , text = text ) return json . dumps ( payload )
9365	def email_address ( user = None ) : if not user : user = user_name ( ) else : user = user . strip ( ) . replace ( ' ' , '_' ) . lower ( ) return user + '@' + domain_name ( )
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
1809	def SETE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
9937	def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : # check if storage location exists for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
12410	def all_package_versions ( package ) : info = PyPI . package_info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
7450	def get_barcode_func ( data , longbar ) : ## build func for finding barcode if longbar [ 1 ] == 'same' : if data . paramsdict [ "datatype" ] == '2brad' : def getbarcode ( cutters , read1 , longbar ) : """ find barcode for 2bRAD data """ return read1 [ 1 ] [ : - ( len ( cutters [ 0 ] [ 0 ] ) + 1 ) ] [ - longbar [ 0 ] : ] else : def getbarcode ( _ , read1 , longbar ) : """ finds barcode for invariable length barcode data """ return read1 [ 1 ] [ : longbar [ 0 ] ] else : def getbarcode ( cutters , read1 , longbar ) : """ finds barcode for variable barcode lengths""" return findbcode ( cutters , longbar , read1 ) return getbarcode
3128	def delete ( self , template_id ) : self . template_id = template_id return self . _mc_client . _delete ( url = self . _build_path ( template_id ) )
7796	def _register_client_authenticator ( klass , name ) : # pylint: disable-msg=W0212 CLIENT_MECHANISMS_D [ name ] = klass items = sorted ( CLIENT_MECHANISMS_D . items ( ) , key = _key_func , reverse = True ) CLIENT_MECHANISMS [ : ] = [ k for ( k , v ) in items ] SECURE_CLIENT_MECHANISMS [ : ] = [ k for ( k , v ) in items if v . _pyxmpp_sasl_secure ]
3457	def find_bump ( target , tag ) : tmp = tag . split ( "." ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , "[0-9]*.md" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return "major" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return "minor" else : return "patch"
13514	def froude_number ( speed , length ) : g = 9.80665 # conventional standard value m/s^2 Fr = speed / np . sqrt ( g * length ) return Fr
8455	def _apply_template ( template , target , * , checkout , extra_context ) : with tempfile . TemporaryDirectory ( ) as tempdir : repo_dir = cc_main . cookiecutter ( template , checkout = checkout , no_input = True , output_dir = tempdir , extra_context = extra_context ) for item in os . listdir ( repo_dir ) : src = os . path . join ( repo_dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
4531	def construct ( cls , project , * * desc ) : return cls ( project . drivers , maker = project . maker , * * desc )
1383	def unregister_watch ( self , uid ) : # Do not raise an error if UUID is # not present in the watches. Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
13699	def make_seekable ( fileobj ) : if sys . version_info < ( 3 , 0 ) and isinstance ( fileobj , file ) : filename = fileobj . name fileobj = io . FileIO ( fileobj . fileno ( ) , closefd = False ) fileobj . name = filename assert isinstance ( fileobj , io . IOBase ) , "fileobj must be an instance of io.IOBase or a file, got %s" % type ( fileobj ) return fileobj if fileobj . seekable ( ) else ArchiveTemp ( fileobj )
11545	def set_pwm_frequency ( self , frequency , pin = None ) : if pin is None : self . _set_pwm_frequency ( frequency , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_pwm_frequency ( frequency , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7089	def jd_corr ( jd , ra , dec , obslon = None , obslat = None , obsalt = None , jd_type = 'bjd' ) : if not HAVEKERNEL : LOGERROR ( 'no JPL kernel available, can\'t continue!' ) return # Source unit-vector ## Assume coordinates in ICRS ## Set distance to unit (kilometers) # convert the angles to degrees rarad = np . radians ( ra ) decrad = np . radians ( dec ) cosra = np . cos ( rarad ) sinra = np . sin ( rarad ) cosdec = np . cos ( decrad ) sindec = np . sin ( decrad ) # this assumes that the target is very far away src_unitvector = np . array ( [ cosdec * cosra , cosdec * sinra , sindec ] ) # Convert epochs to astropy.time.Time ## Assume JD(UTC) if ( obslon is None ) or ( obslat is None ) or ( obsalt is None ) : t = astime . Time ( jd , scale = 'utc' , format = 'jd' ) else : t = astime . Time ( jd , scale = 'utc' , format = 'jd' , location = ( '%.5fd' % obslon , '%.5fd' % obslat , obsalt ) ) # Get Earth-Moon barycenter position ## NB: jplephem uses Barycentric Dynamical Time, e.g. JD(TDB) ## and gives positions relative to solar system barycenter barycenter_earthmoon = jplkernel [ 0 , 3 ] . compute ( t . tdb . jd ) # Get Moon position vectors from the center of Earth to the Moon # this means we get the following vectors from the ephemerides # Earth Barycenter (3) -> Moon (301) # Earth Barycenter (3) -> Earth (399) # so the final vector is [3,301] - [3,399] # units are in km moonvector = ( jplkernel [ 3 , 301 ] . compute ( t . tdb . jd ) - jplkernel [ 3 , 399 ] . compute ( t . tdb . jd ) ) # Compute Earth position vectors (this is for the center of the earth with # respect to the solar system barycenter) # all these units are in km pos_earth = ( barycenter_earthmoon - moonvector * 1.0 / ( 1.0 + EMRAT ) ) if jd_type == 'bjd' : # Compute BJD correction ## Assume source vectors parallel at Earth and Solar System ## Barycenter ## i.e. source is at infinity # the romer_delay correction is (r.dot.n)/c where: # r is the vector from SSB to earth center # n is the unit vector from correction_seconds = np . dot ( pos_earth . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY elif jd_type == 'hjd' : # Compute HJD correction via Sun ephemeris # this is the position vector of the center of the sun in km # Solar System Barycenter (0) -> Sun (10) pos_sun = jplkernel [ 0 , 10 ] . compute ( t . tdb . jd ) # this is the vector from the center of the sun to the center of the # earth sun_earth_vec = pos_earth - pos_sun # calculate the heliocentric correction correction_seconds = np . dot ( sun_earth_vec . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY # TDB is the appropriate time scale for these ephemerides new_jd = t . tdb . jd + correction_days return new_jd
2763	def get_snapshot ( self , snapshot_id ) : return Snapshot . get_object ( api_token = self . token , snapshot_id = snapshot_id )
7538	def getassembly ( args , parsedict ) : ## Creating an assembly with a full path in the name will "work" ## but it is potentially dangerous, so here we have assembly_name ## and assembly_file, name is used for creating new in cwd, file is ## used for loading existing. ## ## Be nice if the user includes the extension. #project_dir = ip.core.assembly._expander(parsedict['1']) #assembly_name = parsedict['0'] project_dir = ip . core . assembly . _expander ( parsedict [ 'project_dir' ] ) assembly_name = parsedict [ 'assembly_name' ] assembly_file = os . path . join ( project_dir , assembly_name ) ## Assembly creation will handle error checking on ## the format of the assembly_name ## make sure the working directory exists. if not os . path . exists ( project_dir ) : os . mkdir ( project_dir ) try : ## If 1 and force then go ahead and create a new assembly if ( '1' in args . steps ) and args . force : data = ip . Assembly ( assembly_name , cli = True ) else : data = ip . load_json ( assembly_file , cli = True ) data . _cli = True except IPyradWarningExit as _ : ## if no assembly is found then go ahead and make one if '1' not in args . steps : raise IPyradWarningExit ( " Error: You must first run step 1 on the assembly: {}" . format ( assembly_file ) ) else : ## create a new assembly object data = ip . Assembly ( assembly_name , cli = True ) ## for entering some params... for param in parsedict : ## trap assignment of assembly_name since it is immutable. if param == "assembly_name" : ## Raise error if user tried to change assembly name if parsedict [ param ] != data . name : data . set_params ( param , parsedict [ param ] ) else : ## all other params should be handled by set_params try : data . set_params ( param , parsedict [ param ] ) except IndexError as _ : print ( " Malformed params file: {}" . format ( args . params ) ) print ( " Bad parameter {} - {}" . format ( param , parsedict [ param ] ) ) sys . exit ( - 1 ) return data
6957	def _log_prior_transit ( theta , priorbounds ) : # priorbounds contains the input priors, and because of how we previously # sorted theta, its sorted keys tell us which parts of theta correspond to # which physical quantities. allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf
6704	def togroups ( self , user , groups ) : r = self . local_renderer if isinstance ( groups , six . string_types ) : groups = [ _ . strip ( ) for _ in groups . split ( ',' ) if _ . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : # Under an existing coefficient set - assume Tmax will come from another set self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return # Must be appended to end instead self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
5909	def parse_groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
10732	def to_float ( option , value ) : if type ( value ) is str : try : value = float ( value ) except ValueError : pass return ( option , value )
11131	def stop ( self ) : with self . _status_lock : if self . _running : assert self . _observer is not None self . _observer . stop ( ) self . _running = False self . _origin_mapped_data = dict ( )
1176	def copy ( self ) : if 0 : # set this to 1 to make the flow space crash return copy . deepcopy ( self ) clone = self . __class__ ( ) clone . length = self . length clone . count = [ ] + self . count [ : ] clone . input = [ ] + self . input clone . A = self . A clone . B = self . B clone . C = self . C clone . D = self . D return clone
9846	def resample_factor ( self , factor ) : # new number of edges N' = (N-1)*f + 1 newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
9500	def _get_instructions_bytes ( code , varnames = None , names = None , constants = None , cells = None , linestarts = None , line_offset = 0 ) : labels = dis . findlabels ( code ) extended_arg = 0 starts_line = None free = None # enumerate() is not an option, since we sometimes process # multiple elements on a single pass through the loop n = len ( code ) i = 0 while i < n : op = code [ i ] offset = i if linestarts is not None : starts_line = linestarts . get ( i , None ) if starts_line is not None : starts_line += line_offset is_jump_target = i in labels i = i + 1 arg = None argval = None argrepr = '' if op >= dis . HAVE_ARGUMENT : arg = code [ i ] + code [ i + 1 ] * 256 + extended_arg extended_arg = 0 i = i + 2 if op == dis . EXTENDED_ARG : extended_arg = arg * 65536 # Set argval to the dereferenced value of the argument when # availabe, and argrepr to the string representation of argval. # _disassemble_bytes needs the string repr of the # raw name index for LOAD_GLOBAL, LOAD_CONST, etc. argval = arg if op in dis . hasconst : argval , argrepr = dis . _get_const_info ( arg , constants ) elif op in dis . hasname : argval , argrepr = dis . _get_name_info ( arg , names ) elif op in dis . hasjrel : argval = i + arg argrepr = "to " + repr ( argval ) elif op in dis . haslocal : argval , argrepr = dis . _get_name_info ( arg , varnames ) elif op in dis . hascompare : argval = dis . cmp_op [ arg ] argrepr = argval elif op in dis . hasfree : argval , argrepr = dis . _get_name_info ( arg , cells ) elif op in dis . hasnargs : argrepr = "%d positional, %d keyword pair" % ( code [ i - 2 ] , code [ i - 1 ] ) yield dis . Instruction ( dis . opname [ op ] , op , arg , argval , argrepr , offset , starts_line , is_jump_target )
73	def Emboss ( alpha = 0 , strength = 1 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) strength_param = iap . handle_continuous_param ( strength , "strength" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) strength_sample = strength_param . draw_sample ( random_state = random_state_func ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ - 1 - strength_sample , 0 - strength_sample , 0 ] , [ 0 - strength_sample , 1 , 0 + strength_sample ] , [ 0 , 0 + strength_sample , 1 + strength_sample ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
7344	def get_data ( self , response ) : if self . _response_list : return response elif self . _response_key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , "__getitem__" ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . _response_key = key return data else : self . _response_list = True return response else : return response [ self . _response_key ] raise NoDataFound ( response = response , url = self . request . get_url ( ) )
5642	def compute_pseudo_connections ( transit_connections , start_time_dep , end_time_dep , transfer_margin , walk_network , walk_speed ) : # A pseudo-connection should be created after (each) arrival to a transit_connection's arrival stop. pseudo_connection_set = set ( ) # use a set to ignore possible duplicates for c in transit_connections : if start_time_dep <= c . departure_time <= end_time_dep : walk_arr_stop = c . departure_stop walk_arr_time = c . departure_time - transfer_margin for _ , walk_dep_stop , data in walk_network . edges ( nbunch = [ walk_arr_stop ] , data = True ) : walk_dep_time = walk_arr_time - data [ 'd_walk' ] / float ( walk_speed ) if walk_dep_time > end_time_dep or walk_dep_time < start_time_dep : continue pseudo_connection = Connection ( walk_dep_stop , walk_arr_stop , walk_dep_time , walk_arr_time , Connection . WALK_TRIP_ID , Connection . WALK_SEQ , is_walk = True ) pseudo_connection_set . add ( pseudo_connection ) return pseudo_connection_set
2583	def load ( cls , config : Optional [ Config ] = None ) : if cls . _dfk is not None : raise RuntimeError ( 'Config has already been loaded' ) if config is None : cls . _dfk = DataFlowKernel ( Config ( ) ) else : cls . _dfk = DataFlowKernel ( config ) return cls . _dfk
4276	def get_albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get_albums ( subdir ) : yield subname , self . albums [ subdir ]
3088	def _delete_entity ( self ) : if self . _is_ndb ( ) : _NDB_KEY ( self . _model , self . _key_name ) . delete ( ) else : entity_key = db . Key . from_path ( self . _model . kind ( ) , self . _key_name ) db . delete ( entity_key )
7260	def get_data_location ( self , catalog_id ) : try : record = self . get ( catalog_id ) except : return None # Handle Landsat8 if 'Landsat8' in record [ 'type' ] and 'LandsatAcquisition' in record [ 'type' ] : bucket = record [ 'properties' ] [ 'bucketName' ] prefix = record [ 'properties' ] [ 'bucketPrefix' ] return 's3://' + bucket + '/' + prefix # Handle DG Acquisition if 'DigitalGlobeAcquisition' in record [ 'type' ] : o = Ordering ( ) res = o . location ( [ catalog_id ] ) return res [ 'acquisitions' ] [ 0 ] [ 'location' ] return None
653	def spDiff ( SP1 , SP2 ) : if ( len ( SP1 . _masterConnectedM ) != len ( SP2 . _masterConnectedM ) ) : print "Connected synapse matrices are different sizes" return False if ( len ( SP1 . _masterPotentialM ) != len ( SP2 . _masterPotentialM ) ) : print "Potential synapse matrices are different sizes" return False if ( len ( SP1 . _masterPermanenceM ) != len ( SP2 . _masterPermanenceM ) ) : print "Permanence matrices are different sizes" return False #iterate over cells for i in range ( 0 , len ( SP1 . _masterConnectedM ) ) : #grab the Coincidence Matrices and compare them connected1 = SP1 . _masterConnectedM [ i ] connected2 = SP2 . _masterConnectedM [ i ] if ( connected1 != connected2 ) : print "Connected Matrices for cell %d different" % ( i ) return False #grab permanence Matrices and compare them permanences1 = SP1 . _masterPermanenceM [ i ] permanences2 = SP2 . _masterPermanenceM [ i ] if ( permanences1 != permanences2 ) : print "Permanence Matrices for cell %d different" % ( i ) return False #grab the potential connection Matrices and compare them potential1 = SP1 . _masterPotentialM [ i ] potential2 = SP2 . _masterPotentialM [ i ] if ( potential1 != potential2 ) : print "Potential Matrices for cell %d different" % ( i ) return False #Check firing boosts if ( not numpy . array_equal ( SP1 . _firingBoostFactors , SP2 . _firingBoostFactors ) ) : print "Firing boost factors are different between spatial poolers" return False #Check duty cycles after inhibiton if ( not numpy . array_equal ( SP1 . _dutyCycleAfterInh , SP2 . _dutyCycleAfterInh ) ) : print "Duty cycles after inhibition are different between spatial poolers" return False #Check duty cycles before inhibition if ( not numpy . array_equal ( SP1 . _dutyCycleBeforeInh , SP2 . _dutyCycleBeforeInh ) ) : print "Duty cycles before inhibition are different between spatial poolers" return False print ( "Spatial Poolers are equivalent" ) return True
1065	def getfirstmatchingheader ( self , name ) : name = name . lower ( ) + ':' n = len ( name ) lst = [ ] hit = 0 for line in self . headers : if hit : if not line [ : 1 ] . isspace ( ) : break elif line [ : n ] . lower ( ) == name : hit = 1 if hit : lst . append ( line ) return lst
7490	def get_targets ( ipyclient ) : ## fill hosts with async[gethostname] hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( socket . gethostname ) ) ## capture results of asyncs hosts = [ i . get ( ) for i in hosts ] hostset = set ( hosts ) hostzip = zip ( hosts , ipyclient . ids ) hostdict = { host : [ i [ 1 ] for i in hostzip if i [ 0 ] == host ] for host in hostset } targets = list ( itertools . chain ( * [ hostdict [ i ] [ : 2 ] for i in hostdict ] ) ) ## return first two engines from each host return targets
6371	def fallout ( self ) : if self . _fp + self . _tn == 0 : return float ( 'NaN' ) return self . _fp / ( self . _fp + self . _tn )
169	def find_intersections_with ( self , other ) : import shapely . geometry geom = _convert_var_to_shapely_geometry ( other ) result = [ ] for p_start , p_end in zip ( self . coords [ : - 1 ] , self . coords [ 1 : ] ) : ls = shapely . geometry . LineString ( [ p_start , p_end ] ) intersections = ls . intersection ( geom ) intersections = list ( _flatten_shapely_collection ( intersections ) ) intersections_points = [ ] for inter in intersections : if isinstance ( inter , shapely . geometry . linestring . LineString ) : inter_start = ( inter . coords [ 0 ] [ 0 ] , inter . coords [ 0 ] [ 1 ] ) inter_end = ( inter . coords [ - 1 ] [ 0 ] , inter . coords [ - 1 ] [ 1 ] ) intersections_points . extend ( [ inter_start , inter_end ] ) else : assert isinstance ( inter , shapely . geometry . point . Point ) , ( "Expected to find shapely.geometry.point.Point or " "shapely.geometry.linestring.LineString intersection, " "actually found %s." % ( type ( inter ) , ) ) intersections_points . append ( ( inter . x , inter . y ) ) # sort by distance to start point, this makes it later on easier # to remove duplicate points inter_sorted = sorted ( intersections_points , key = lambda p : np . linalg . norm ( np . float32 ( p ) - p_start ) ) result . append ( inter_sorted ) return result
3258	def get_resources ( self , names = None , stores = None , workspaces = None ) : stores = self . get_stores ( names = stores , workspaces = workspaces ) resources = [ ] for s in stores : try : resources . extend ( s . get_resources ( ) ) except FailedRequestError : continue if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if resources and names : return ( [ resource for resource in resources if resource . name in names ] ) return resources
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
82	def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
11252	def get_datetime_string ( datetime_obj ) : if isinstance ( datetime_obj , datetime ) : dft = DTFormat ( ) return datetime_obj . strftime ( dft . datetime_format ) return None
11120	def get_file_relative_path_by_id ( self , id ) : for path , info in self . walk_files_info ( ) : if info [ 'id' ] == id : return path # none was found return None
1791	def IMUL ( cpu , * operands ) : dest = operands [ 0 ] OperandSize = dest . size reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ OperandSize ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ OperandSize ] arg0 = dest . read ( ) arg1 = None arg2 = None res = None if len ( operands ) == 1 : arg1 = cpu . read_register ( reg_name_l ) temp = ( Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( temp , OperandSize , OperandSize ) ) res = Operators . EXTRACT ( temp , 0 , OperandSize ) elif len ( operands ) == 2 : arg1 = operands [ 1 ] . read ( ) arg1 = Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) temp = Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * arg1 temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) else : arg1 = operands [ 1 ] . read ( ) arg2 = operands [ 2 ] . read ( ) temp = ( Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg2 , operands [ 2 ] . size , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . CF = ( Operators . SEXTEND ( res , OperandSize , OperandSize * 2 ) != temp ) cpu . OF = cpu . CF
13898	def IterHashes ( iterator_size , hash_length = 7 ) : if not isinstance ( iterator_size , int ) : raise TypeError ( 'iterator_size must be integer.' ) count = 0 while count != iterator_size : count += 1 yield GetRandomHash ( hash_length )
10295	def get_undefined_namespaces ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) }
6565	def xor_gate ( variables , vartype = dimod . BINARY , name = 'XOR' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configs = frozenset ( [ ( 0 , 0 , 0 ) , ( 0 , 1 , 1 ) , ( 1 , 0 , 1 ) , ( 1 , 1 , 0 ) ] ) def func ( in1 , in2 , out ) : return ( in1 != in2 ) == out else : # SPIN, vartype is checked by the decorator configs = frozenset ( [ ( - 1 , - 1 , - 1 ) , ( - 1 , + 1 , + 1 ) , ( + 1 , - 1 , + 1 ) , ( + 1 , + 1 , - 1 ) ] ) def func ( in1 , in2 , out ) : return ( ( in1 > 0 ) != ( in2 > 0 ) ) == ( out > 0 ) return Constraint ( func , configs , variables , vartype = vartype , name = name )
6716	def bootstrap ( self , force = 0 ) : force = int ( force ) if self . has_pip ( ) and not force : return r = self . local_renderer if r . env . bootstrap_method == GET_PIP : r . sudo ( 'curl --silent --show-error --retry 5 https://bootstrap.pypa.io/get-pip.py | python' ) elif r . env . bootstrap_method == EZ_SETUP : r . run ( 'wget http://peak.telecommunity.com/dist/ez_setup.py -O /tmp/ez_setup.py' ) with self . settings ( warn_only = True ) : r . sudo ( 'python /tmp/ez_setup.py -U setuptools' ) r . sudo ( 'easy_install -U pip' ) elif r . env . bootstrap_method == PYTHON_PIP : r . sudo ( 'apt-get install -y python-pip' ) else : raise NotImplementedError ( 'Unknown pip bootstrap method: %s' % r . env . bootstrap_method ) r . sudo ( 'pip {quiet_flag} install --upgrade pip' ) r . sudo ( 'pip {quiet_flag} install --upgrade virtualenv' )
3981	def _get_expanded_active_specs ( specs ) : _filter_active ( constants . CONFIG_BUNDLES_KEY , specs ) _filter_active ( 'apps' , specs ) _expand_libs_in_apps ( specs ) _filter_active ( 'libs' , specs ) _filter_active ( 'services' , specs ) _add_active_assets ( specs )
2027	def SHA3 ( self , start , size ) : # read memory from start to end # http://gavwood.com/paper.pdf data = self . try_simplify_to_constant ( self . read_buffer ( start , size ) ) if issymbolic ( data ) : known_sha3 = { } # Broadcast the signal self . _publish ( 'on_symbolic_sha3' , data , known_sha3 ) # This updates the local copy of sha3 with the pairs we need to explore value = 0 # never used known_hashes_cond = False for key , hsh in known_sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known_hashes_cond = Operators . OR ( cond , known_hashes_cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak_256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . _publish ( 'on_concrete_sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
13318	def launch ( module_name , * args , * * kwargs ) : r = resolve ( module_name ) r . activate ( ) mod = r . resolved [ 0 ] mod . launch ( * args , * * kwargs )
3592	def encryptPassword ( self , login , passwd ) : # structure of the binary key: # # *-------------------------------------------------------* # | modulus_length | modulus | exponent_length | exponent | # *-------------------------------------------------------* # # modulus_length and exponent_length are uint32 binaryKey = b64decode ( config . GOOGLE_PUBKEY ) # modulus i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) # exponent j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) # calculate SHA1 of the pub key digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] # generate a public key der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) # encrypt email and password using pubkey to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
1831	def JBE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . CF , cpu . ZF ) , target . read ( ) , cpu . PC )
13907	def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , * * kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
1431	def getInstancePid ( topology_info , instance_id ) : try : http_client = tornado . httpclient . AsyncHTTPClient ( ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/pid/%s" % ( endpoint , instance_id ) Log . debug ( "HTTP call for url: %s" , url ) response = yield http_client . fetch ( url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
9733	def get_6d_euler ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , euler = QRTPacket . _get_exact ( RT6DBodyEuler , data , component_position ) append_components ( ( position , euler ) ) return components
8633	def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } # POST /api/projects/0.1/bids/ response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11922	def paginate_dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate_dataframe ( dataframe , self . request , view = self )
4208	def lpc ( x , N = None ) : m = len ( x ) if N is None : N = m - 1 #default value if N is not provided elif N > m - 1 : #disp('Warning: zero-padding short input sequence') x . resize ( N + 1 ) #todo: check this zero-padding. X = fft ( x , 2 ** nextpow2 ( 2. * len ( x ) - 1 ) ) R = real ( ifft ( abs ( X ) ** 2 ) ) R = R / ( m - 1. ) #Biased autocorrelation estimate a , e , ref = LEVINSON ( R , N ) return a , e
13859	def contents ( self , f , text ) : text += self . _read ( f . abs_path ) + "\r\n" return text
7900	def process_configuration_success ( self , stanza ) : _unused = stanza self . configured = True self . handler . room_configured ( )
10811	def query_by_names ( cls , names ) : assert isinstance ( names , list ) return cls . query . filter ( cls . name . in_ ( names ) )
7969	def _remove_io_handler ( self , handler ) : if handler not in self . io_handlers : return self . io_handlers . remove ( handler ) for thread in self . io_threads : if thread . io_handler is handler : thread . stop ( )
7221	def ingest_vectors ( self , output_port_value ) : # append two tasks to self['definition']['tasks'] ingest_task = Task ( 'IngestItemJsonToVectorServices' ) ingest_task . inputs . items = output_port_value ingest_task . impersonation_allowed = True stage_task = Task ( 'StageDataToS3' ) stage_task . inputs . destination = 's3://{vector_ingest_bucket}/{recipe_id}/{run_id}/{task_name}' stage_task . inputs . data = ingest_task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest_task . generate_task_workflow_json ( ) ) self . definition [ 'tasks' ] . append ( stage_task . generate_task_workflow_json ( ) )
11568	def open ( self , verbose ) : # open a serial port if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : # in case the port is already open, let's close it and then # reopen it self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : # opened failed - will report back to caller raise
7658	def append_records ( self , records ) : for obs in records : if isinstance ( obs , Observation ) : self . append ( * * obs . _asdict ( ) ) else : self . append ( * * obs )
1006	def _learnBacktrack ( self ) : # How much input history have we accumulated? # The current input is always at the end of self._prevInfPatterns (at # index -1), and is not a valid startingOffset to evaluate. numPrevPatterns = len ( self . _prevLrnPatterns ) - 1 if numPrevPatterns <= 0 : if self . verbosity >= 3 : print "lrnBacktrack: No available history to backtrack from" return False # We will record which previous input patterns did not generate predictions # up to the current time step and remove all the ones at the head of the # input history queue so that we don't waste time evaluating them again at # a later time step. badPatterns = [ ] # Let's go back in time and replay the recent inputs from start cells and # see if we can lock onto this current set of inputs that way. # # Start the farthest back and work our way forward. For each starting point, # See if firing on start cells at that point would predict the current # input. # # We want to pick the point farthest in the past that has continuity # up to the current time step inSequence = False for startOffset in range ( 0 , numPrevPatterns ) : # Can we backtrack from startOffset? inSequence = self . _learnBacktrackFrom ( startOffset , readOnly = True ) # Done playing through the sequence from starting point startOffset # Break out as soon as we find a good path if inSequence : break # Take this bad starting point out of our input history so we don't # try it again later. badPatterns . append ( startOffset ) # If we failed to lock on at any starting point, return failure. The caller # will start over again on start cells if not inSequence : if self . verbosity >= 3 : print ( "Failed to lock on. Falling back to start cells on current " "time step." ) # Nothing in our input history was a valid starting point, so get rid # of it so we don't try any of them again at a later iteration self . _prevLrnPatterns = [ ] return False # We did find a valid starting point in the past. Now, we need to # re-enforce all segments that became active when following this path. if self . verbosity >= 3 : print ( "Discovered path to current input by using start cells from %d " "steps ago:" % ( numPrevPatterns - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) self . _learnBacktrackFrom ( startOffset , readOnly = False ) # Remove any useless patterns at the head of the input pattern history # queue. for i in range ( numPrevPatterns ) : if i in badPatterns or i <= startOffset : if self . verbosity >= 3 : print ( "Removing useless pattern from history:" , self . _prevLrnPatterns [ 0 ] ) self . _prevLrnPatterns . pop ( 0 ) else : break return numPrevPatterns - startOffset
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
5264	def pathcase ( string ) : string = snakecase ( string ) if not string : return string return re . sub ( r"_" , "/" , string )
11048	def dataReceived ( self , data ) : self . resetTimeout ( ) lines = ( self . _buffer + data ) . splitlines ( ) # str.splitlines() doesn't split the string after a trailing newline # character so we must check if there is a trailing newline and, if so, # clear the buffer as the line is "complete". Else, the line is # incomplete and we keep the last line in the buffer. if data . endswith ( b'\n' ) or data . endswith ( b'\r' ) : self . _buffer = b'' else : self . _buffer = lines . pop ( - 1 ) for line in lines : if self . transport . disconnecting : # this is necessary because the transport may be told to lose # the connection by a line within a larger packet, and it is # important to disregard all the lines in that packet following # the one that told it to close. return if len ( line ) > self . _max_length : self . lineLengthExceeded ( line ) return else : self . lineReceived ( line ) if len ( self . _buffer ) > self . _max_length : self . lineLengthExceeded ( self . _buffer ) return
13402	def selectedLogs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . logMenus ) ) : logType = self . logMenus [ i ] . selectedType ( ) log = self . logMenus [ i ] . selectedProgram ( ) if logType == "MCC" : if log not in mcclogs : mcclogs . append ( log ) elif logType == "Physics" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs
13591	def n_p ( self ) : return 2 * _sltr . GeV2joule ( self . E ) * _spc . epsilon_0 / ( self . beta * _spc . elementary_charge ) ** 2
364	def threading_data ( data = None , fn = None , thread_count = None , * * kwargs ) : def apply_fn ( results , i , data , kwargs ) : results [ i ] = fn ( data , * * kwargs ) if thread_count is None : results = [ None ] * len ( data ) threads = [ ] # for i in range(len(data)): # t = threading.Thread(name='threading_and_return', target=apply_fn, args=(results, i, data[i], kwargs)) for i , d in enumerate ( data ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , d , kwargs ) ) t . start ( ) threads . append ( t ) else : divs = np . linspace ( 0 , len ( data ) , thread_count + 1 ) divs = np . round ( divs ) . astype ( int ) results = [ None ] * thread_count threads = [ ] for i in range ( thread_count ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , data [ divs [ i ] : divs [ i + 1 ] ] , kwargs ) ) t . start ( ) threads . append ( t ) for t in threads : t . join ( ) if thread_count is None : try : return np . asarray ( results ) except Exception : return results else : return np . concatenate ( results )
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) # append values of p as an additional field ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) # calculate sample mean ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes # calculate sample standard deviation array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes # calculate standard normal confidence interval array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
12983	def filename ( file_name , start_on = None , ignore = ( ) , use_short = True , * * queries ) : with open ( file_name ) as template_file : return file ( template_file , start_on = start_on , ignore = ignore , use_short = use_short , * * queries )
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
1171	def format_option_strings ( self , option ) : if option . takes_value ( ) : metavar = option . metavar or option . dest . upper ( ) short_opts = [ self . _short_opt_fmt % ( sopt , metavar ) for sopt in option . _short_opts ] long_opts = [ self . _long_opt_fmt % ( lopt , metavar ) for lopt in option . _long_opts ] else : short_opts = option . _short_opts long_opts = option . _long_opts if self . short_first : opts = short_opts + long_opts else : opts = long_opts + short_opts return ", " . join ( opts )
6564	def and_gate ( variables , vartype = dimod . BINARY , name = 'AND' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configurations = frozenset ( [ ( 0 , 0 , 0 ) , ( 0 , 1 , 0 ) , ( 1 , 0 , 0 ) , ( 1 , 1 , 1 ) ] ) def func ( in1 , in2 , out ) : return ( in1 and in2 ) == out else : # SPIN, vartype is checked by the decorator configurations = frozenset ( [ ( - 1 , - 1 , - 1 ) , ( - 1 , + 1 , - 1 ) , ( + 1 , - 1 , - 1 ) , ( + 1 , + 1 , + 1 ) ] ) def func ( in1 , in2 , out ) : return ( ( in1 > 0 ) and ( in2 > 0 ) ) == ( out > 0 ) return Constraint ( func , configurations , variables , vartype = vartype , name = name )
5387	def _convert_suffix_to_docker_chars ( suffix ) : # Docker container names must match: [a-zA-Z0-9][a-zA-Z0-9_.-] accepted_characters = string . ascii_letters + string . digits + '_.-' def label_char_transform ( char ) : if char in accepted_characters : return char return '-' return '' . join ( label_char_transform ( c ) for c in suffix )
6764	def drop_database ( self , name ) : with settings ( warn_only = True ) : self . sudo ( 'dropdb %s' % ( name , ) , user = 'postgres' )
2158	def _echo_method ( self , method ) : @ functools . wraps ( method ) def func ( * args , * * kwargs ) : # Echo warning if this method is deprecated. if getattr ( method , 'deprecated' , False ) : debug . log ( 'This method is deprecated in Tower 3.0.' , header = 'warning' ) result = method ( * args , * * kwargs ) # If this was a request that could result in a modification # of data, print it in Ansible coloring. color_info = { } if isinstance ( result , dict ) and 'changed' in result : if result [ 'changed' ] : color_info [ 'fg' ] = 'yellow' else : color_info [ 'fg' ] = 'green' # Piece together the result into the proper format. format = getattr ( self , '_format_%s' % ( getattr ( method , 'format_freezer' , None ) or settings . format ) ) output = format ( result ) # Perform the echo. secho ( output , * * color_info ) return func
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
4266	def set_meta ( target , keys , overwrite = False ) : if not os . path . exists ( target ) : sys . stderr . write ( "The target {} does not exist.\n" . format ( target ) ) sys . exit ( 1 ) if len ( keys ) < 2 or len ( keys ) % 2 > 0 : sys . stderr . write ( "Need an even number of arguments.\n" ) sys . exit ( 1 ) if os . path . isdir ( target ) : descfile = os . path . join ( target , 'index.md' ) else : descfile = os . path . splitext ( target ) [ 0 ] + '.md' if os . path . exists ( descfile ) and not overwrite : sys . stderr . write ( "Description file '{}' already exists. " "Use --overwrite to overwrite it.\n" . format ( descfile ) ) sys . exit ( 2 ) with open ( descfile , "w" ) as fp : for i in range ( len ( keys ) // 2 ) : k , v = keys [ i * 2 : ( i + 1 ) * 2 ] fp . write ( "{}: {}\n" . format ( k . capitalize ( ) , v ) ) print ( "{} metadata key(s) written to {}" . format ( len ( keys ) // 2 , descfile ) )
7539	def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH ## calculate probs bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , estE ) homob = scipy . stats . binom . pmf ( base1 , bsum , estE ) ## calculate probs hetprob *= prior_hete homoa *= prior_homo homob *= prior_homo ## final probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) ## return if hetprob > homoa : return True , bestprob else : return False , bestprob
1385	def set_physical_plan ( self , physical_plan ) : if not physical_plan : self . physical_plan = None self . id = None else : self . physical_plan = physical_plan self . id = physical_plan . topology . id self . trigger_watches ( )
2313	def predict_proba ( self , a , b , * * kwargs ) : a = scale ( a ) . reshape ( ( - 1 , 1 ) ) b = scale ( b ) . reshape ( ( - 1 , 1 ) ) return self . anm_score ( b , a ) - self . anm_score ( a , b )
886	def punishPredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells ) : self . _punishPredictedColumn ( self . connections , columnMatchingSegments , prevActiveCells , self . predictedSegmentDecrement )
8999	def _dump_knitting_pattern ( self , file ) : knitting_pattern_set = self . __on_dump ( ) knitting_pattern = knitting_pattern_set . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) builder = AYABPNGBuilder ( * layout . bounding_box ) builder . set_colors_in_grid ( layout . walk_instructions ( ) ) builder . write_to_file ( file )
8877	def allele_expectation ( bgen , variant_idx ) : geno = bgen [ "genotype" ] [ variant_idx ] . compute ( ) if geno [ "phased" ] : raise ValueError ( "Allele expectation is define for unphased genotypes only." ) nalleles = bgen [ "variants" ] . loc [ variant_idx , "nalleles" ] . compute ( ) . item ( ) genotypes = get_genotypes ( geno [ "ploidy" ] , nalleles ) expec = [ ] for i in range ( len ( genotypes ) ) : count = asarray ( genotypes_to_allele_counts ( genotypes [ i ] ) , float ) n = count . shape [ 0 ] expec . append ( ( count . T * geno [ "probs" ] [ i , : n ] ) . sum ( 1 ) ) return stack ( expec , axis = 0 )
10130	def timezone ( haystack_tz , version = LATEST_VER ) : tz_map = get_tz_map ( version = version ) try : tz_name = tz_map [ haystack_tz ] except KeyError : raise ValueError ( '%s is not a recognised timezone on this host' % haystack_tz ) return pytz . timezone ( tz_name )
4140	def execute_script ( code_block , example_globals , image_path , fig_count , src_file , gallery_conf ) : time_elapsed = 0 stdout = '' # We need to execute the code print ( 'plotting code blocks in %s' % src_file ) plt . close ( 'all' ) cwd = os . getcwd ( ) # Redirect output to stdout and orig_stdout = sys . stdout try : # First cd in the original example dir, so that any file # created by the example get created in this directory os . chdir ( os . path . dirname ( src_file ) ) my_buffer = StringIO ( ) my_stdout = Tee ( sys . stdout , my_buffer ) sys . stdout = my_stdout t_start = time ( ) exec ( code_block , example_globals ) time_elapsed = time ( ) - t_start sys . stdout = orig_stdout my_stdout = my_buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my_stdout : stdout = CODE_OUTPUT . format ( indent ( my_stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure_list = save_figures ( image_path , fig_count , gallery_conf ) # Depending on whether we have one or more figures, we're using a # horizontal list or a single rst call to 'image'. image_list = "" if len ( figure_list ) == 1 : figure_name = figure_list [ 0 ] image_list = SINGLE_IMAGE % figure_name . lstrip ( '/' ) elif len ( figure_list ) > 1 : image_list = HLIST_HEADER for figure_name in figure_list : image_list += HLIST_IMAGE_TEMPLATE % figure_name . lstrip ( '/' ) except Exception : formatted_exception = traceback . format_exc ( ) print ( 80 * '_' ) print ( '%s is not compiling:' % src_file ) print ( formatted_exception ) print ( 80 * '_' ) figure_list = [ ] image_list = codestr2rst ( formatted_exception , lang = 'pytb' ) # Overrides the output thumbnail in the gallery for easy identification broken_img = os . path . join ( glr_path_static ( ) , 'broken_example.png' ) shutil . copyfile ( broken_img , os . path . join ( cwd , image_path . format ( 1 ) ) ) fig_count += 1 # raise count to avoid overwriting image # Breaks build on first example error if gallery_conf [ 'abort_on_example_error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig_stdout print ( " - time elapsed : %.2g sec" % time_elapsed ) code_output = "\n{0}\n\n{1}\n\n" . format ( image_list , stdout ) return code_output , time_elapsed , fig_count + len ( figure_list )
11512	def share_item ( self , token , item_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.share' , parameters ) return response
1937	def get_abi ( self , hsh : bytes ) -> Dict [ str , Any ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) if sig is not None : return dict ( self . _function_abi_items_by_signature [ sig ] ) item = self . _fallback_function_abi_item if item is not None : return dict ( item ) # An item describing the default fallback function. return { 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'fallback' }
770	def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )
12420	def urls ( cls ) : return urls . patterns ( '' , urls . url ( r'^{}(?:$|(?P<path>[/:(.].*))' . format ( cls . meta . name ) , cls . view , name = 'armet-api-{}' . format ( cls . meta . name ) , kwargs = { 'resource' : cls . meta . name } ) )
6678	def uncommented_lines ( self , filename , use_sudo = False ) : func = run_as_root if use_sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
9133	def get_data_dir ( module_name : str ) -> str : module_name = module_name . lower ( ) data_dir = os . path . join ( BIO2BEL_DIR , module_name ) os . makedirs ( data_dir , exist_ok = True ) return data_dir
1933	def function_signature_for_name_and_inputs ( name : str , inputs : Sequence [ Mapping [ str , Any ] ] ) -> str : return name + SolidityMetadata . tuple_signature_for_components ( inputs )
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
2628	def teardown ( self ) : self . shut_down_instance ( self . instances ) self . instances = [ ] try : self . client . delete_internet_gateway ( InternetGatewayId = self . internet_gateway ) self . internet_gateway = None self . client . delete_route_table ( RouteTableId = self . route_table ) self . route_table = None for subnet in list ( self . sn_ids ) : # Cast to list ensures that this is a copy # Which is important because it means that # the length of the list won't change during iteration self . client . delete_subnet ( SubnetId = subnet ) self . sn_ids . remove ( subnet ) self . client . delete_security_group ( GroupId = self . sg_id ) self . sg_id = None self . client . delete_vpc ( VpcId = self . vpc_id ) self . vpc_id = None except Exception as e : logger . error ( "{}" . format ( e ) ) raise e self . show_summary ( ) os . remove ( self . config [ 'state_file_path' ] )
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
7443	def _step1func ( self , force , ipyclient ) : ## check input data files sfiles = self . paramsdict [ "sorted_fastq_path" ] rfiles = self . paramsdict [ "raw_fastq_path" ] ## do not allow both a sorted_fastq_path and a raw_fastq if sfiles and rfiles : raise IPyradWarningExit ( NOT_TWO_PATHS ) ## but also require that at least one exists if not ( sfiles or rfiles ) : raise IPyradWarningExit ( NO_SEQ_PATH_FOUND ) ## print headers if self . _headers : if sfiles : print ( "\n{}Step 1: Loading sorted fastq data to Samples" . format ( self . _spacer ) ) else : print ( "\n{}Step 1: Demultiplexing fastq data to Samples" . format ( self . _spacer ) ) ## if Samples already exist then no demultiplexing if self . samples : if not force : print ( SAMPLES_EXIST . format ( len ( self . samples ) , self . name ) ) else : ## overwrite existing data else do demux if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) ## Creating new Samples else : ## first check if demultiplexed files exist in sorted path if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient ) ## otherwise do the demultiplexing else : assemble . demultiplex . run2 ( self , ipyclient , force )
211	def to_uint8 ( self ) : # TODO this always returns (H,W,C), even if input ndarray was originall (H,W) # does it make sense here to also return (H,W) if self.arr_was_2d? arr_0to255 = np . clip ( np . round ( self . arr_0to1 * 255 ) , 0 , 255 ) arr_uint8 = arr_0to255 . astype ( np . uint8 ) return arr_uint8
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
9636	def _getCallingContext ( ) : frames = inspect . stack ( ) if len ( frames ) > 4 : context = frames [ 5 ] else : context = frames [ 0 ] modname = context [ 1 ] lineno = context [ 2 ] if context [ 3 ] : funcname = context [ 3 ] else : funcname = "" # python docs say you don't want references to # frames lying around. Bad things can happen. del context del frames return modname , funcname , lineno
13579	def determine_type ( x ) : types = ( int , float , str ) _type = filter ( lambda a : is_type ( a , x ) , types ) [ 0 ] return _type ( x )
5922	def create ( logger_name , logfile = 'gromacs.log' ) : logger = logging . getLogger ( logger_name ) logger . setLevel ( logging . DEBUG ) logfile = logging . FileHandler ( logfile ) logfile_formatter = logging . Formatter ( '%(asctime)s %(name)-12s %(levelname)-8s %(message)s' ) logfile . setFormatter ( logfile_formatter ) logger . addHandler ( logfile ) # define a Handler which writes INFO messages or higher to the sys.stderr console = logging . StreamHandler ( ) console . setLevel ( logging . INFO ) # set a format which is simpler for console use formatter = logging . Formatter ( '%(name)-12s: %(levelname)-8s %(message)s' ) console . setFormatter ( formatter ) logger . addHandler ( console ) return logger
8186	def draw ( self , dx = 0 , dy = 0 , weighted = False , directed = False , highlight = [ ] , traffic = None ) : self . update ( ) # Draw the graph background. s = self . styles . default s . graph_background ( s ) # Center the graph on the canvas. _ctx . push ( ) _ctx . translate ( self . x + dx , self . y + dy ) # Indicate betweenness centrality. if traffic : if isinstance ( traffic , bool ) : traffic = 5 for n in self . nodes_by_betweenness ( ) [ : traffic ] : try : s = self . styles [ n . style ] except : s = self . styles . default if s . graph_traffic : s . graph_traffic ( s , n , self . alpha ) # Draw the edges and their labels. s = self . styles . default if s . edges : s . edges ( s , self . edges , self . alpha , weighted , directed ) # Draw each node in the graph. # Apply individual style to each node (or default). for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node : s . node ( s , n , self . alpha ) # Highlight the given shortest path. try : s = self . styles . highlight except : s = self . styles . default if s . path : s . path ( s , self , highlight ) # Draw node id's as labels on each node. for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node_label : s . node_label ( s , n , self . alpha ) # Events for clicked and dragged nodes. # Nodes will resist being dragged by attraction and repulsion, # put the event listener on top to get more direct feedback. #self.events.update() _ctx . pop ( )
11334	def table ( * columns , * * kwargs ) : ret = [ ] prefix = kwargs . get ( 'prefix' , '' ) buf_count = kwargs . get ( 'buf_count' , 2 ) if len ( columns ) == 1 : columns = list ( columns [ 0 ] ) else : # without the list the zip iterator gets spent, I'm sure I can make this # better columns = list ( zip ( * columns ) ) headers = kwargs . get ( "headers" , [ ] ) if headers : columns . insert ( 0 , headers ) # we have to go through all the rows and calculate the length of each # column of each row widths = kwargs . get ( "widths" , [ ] ) row_counts = Counter ( ) for i in range ( len ( widths ) ) : row_counts [ i ] = int ( widths [ i ] ) width = int ( kwargs . get ( "width" , 0 ) ) for row in columns : for i , c in enumerate ( row ) : if isinstance ( c , basestring ) : cl = len ( c ) else : cl = len ( str ( c ) ) if cl > row_counts [ i ] : row_counts [ i ] = cl width = int ( kwargs . get ( "width" , 0 ) ) if width : for i in row_counts : if row_counts [ i ] < width : row_counts [ i ] = width # actually go through and format each row def colstr ( c ) : if isinstance ( c , basestring ) : return c return str ( c ) def rowstr ( row , prefix , row_counts ) : row_format = prefix cols = list ( map ( colstr , row ) ) for i in range ( len ( row_counts ) ) : c = cols [ i ] # build the format string for each row, we use the row_counts found # above to decide how much padding each column should get # https://stackoverflow.com/a/9536084/5006 if re . match ( r"^\d+(?:\.\d+)?$" , c ) : if i == 0 : row_format += "{:>" + str ( row_counts [ i ] ) + "}" else : row_format += "{:>" + str ( row_counts [ i ] + buf_count ) + "}" else : row_format += "{:<" + str ( row_counts [ i ] + buf_count ) + "}" return row_format . format ( * cols ) for row in columns : ret . append ( rowstr ( row , prefix , row_counts ) ) out ( os . linesep . join ( ret ) )
4484	def copyfileobj ( fsrc , fdst , total , length = 16 * 1024 ) : with tqdm ( unit = 'bytes' , total = total , unit_scale = True ) as pbar : while 1 : buf = fsrc . read ( length ) if not buf : break fdst . write ( buf ) pbar . update ( len ( buf ) )
12163	def _check_limit ( self , event ) : if self . count ( event ) > self . max_listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , ResourceWarning , )
12531	def rename_file_group_to_serial_nums ( file_lst ) : file_lst . sort ( ) c = 1 for f in file_lst : dirname = get_abspath ( f . dirname ( ) ) fdest = f . joinpath ( dirname , "{0:04d}" . format ( c ) + OUTPUT_DICOM_EXTENSION ) log . info ( 'Renaming {0} to {1}' . format ( f , fdest ) ) f . rename ( fdest ) c += 1
1499	def ack ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in ack()" ) return if self . acking_enabled : ack_tuple = tuple_pb2 . AckTuple ( ) ack_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = ack_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns )
10876	def calculate_linescan_ilm_psf ( y , z , polar_angle = 0. , nlpts = 1 , pinhole_width = 1 , use_laggauss = False , * * kwargs ) : if use_laggauss : x_vals , wts = calc_pts_lag ( ) else : x_vals , wts = calc_pts_hg ( ) #I'm assuming that y,z are already some sort of meshgrid xg , yg , zg = [ np . zeros ( list ( y . shape ) + [ x_vals . size ] ) for a in range ( 3 ) ] hilm = np . zeros ( xg . shape ) for a in range ( x_vals . size ) : xg [ ... , a ] = x_vals [ a ] yg [ ... , a ] = y . copy ( ) zg [ ... , a ] = z . copy ( ) y_pinhole , wts_pinhole = np . polynomial . hermite . hermgauss ( nlpts ) y_pinhole *= np . sqrt ( 2 ) * pinhole_width wts_pinhole /= np . sqrt ( np . pi ) #Pinhole hermgauss first: for yp , wp in zip ( y_pinhole , wts_pinhole ) : rho = np . sqrt ( xg * xg + ( yg - yp ) * ( yg - yp ) ) phi = np . arctan2 ( yg , xg ) hsym , hasym = get_hsym_asym ( rho , zg , get_hdet = False , * * kwargs ) hilm += wp * ( hsym + np . cos ( 2 * ( phi - polar_angle ) ) * hasym ) #Now line hermgauss for a in range ( x_vals . size ) : hilm [ ... , a ] *= wts [ a ] return hilm . sum ( axis = - 1 ) * 2.
8560	def get_lan_members ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s/nics?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
7009	def _fourier_residual ( fourierparams , phase , mags ) : f = _fourier_func ( fourierparams , phase , mags ) residual = mags - f return residual
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
3208	def _reformat_policy ( policy ) : policy_name = policy [ 'PolicyName' ] ret = { } ret [ 'type' ] = policy [ 'PolicyTypeName' ] attrs = policy [ 'PolicyAttributeDescriptions' ] if ret [ 'type' ] != 'SSLNegotiationPolicyType' : return policy_name , ret attributes = dict ( ) for attr in attrs : attributes [ attr [ 'AttributeName' ] ] = attr [ 'AttributeValue' ] ret [ 'protocols' ] = dict ( ) ret [ 'protocols' ] [ 'sslv2' ] = bool ( attributes . get ( 'Protocol-SSLv2' ) ) ret [ 'protocols' ] [ 'sslv3' ] = bool ( attributes . get ( 'Protocol-SSLv3' ) ) ret [ 'protocols' ] [ 'tlsv1' ] = bool ( attributes . get ( 'Protocol-TLSv1' ) ) ret [ 'protocols' ] [ 'tlsv1_1' ] = bool ( attributes . get ( 'Protocol-TLSv1.1' ) ) ret [ 'protocols' ] [ 'tlsv1_2' ] = bool ( attributes . get ( 'Protocol-TLSv1.2' ) ) ret [ 'server_defined_cipher_order' ] = bool ( attributes . get ( 'Server-Defined-Cipher-Order' ) ) ret [ 'reference_security_policy' ] = attributes . get ( 'Reference-Security-Policy' , None ) non_ciphers = [ 'Server-Defined-Cipher-Order' , 'Protocol-SSLv2' , 'Protocol-SSLv3' , 'Protocol-TLSv1' , 'Protocol-TLSv1.1' , 'Protocol-TLSv1.2' , 'Reference-Security-Policy' ] ciphers = [ ] for cipher in attributes : if attributes [ cipher ] == 'true' and cipher not in non_ciphers : ciphers . append ( cipher ) ciphers . sort ( ) ret [ 'supported_ciphers' ] = ciphers return policy_name , ret
10799	def _eval_firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist_between_points = self . _distance_matrix ( rvecs , self . x ) gaussian_weights = self . _weight ( dist_between_points , sigma = sigma ) return gaussian_weights . dot ( data ) / gaussian_weights . sum ( axis = 1 ) else : # Now rather than calculating the distance matrix all at once, # we do it in chunks over rvecs ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . _distance_matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . _weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans
12963	def getPrimaryKeys ( self , sortByAge = False ) : conn = self . _get_connection ( ) # Apply filters, and return object numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : # No filters, get all. conn = self . _get_connection ( ) matchedKeys = conn . smembers ( self . _get_ids_key ( ) ) elif numNotFilters == 0 : # Only Inclusive if numFilters == 1 : # Only one filter, get members of that index key ( filterFieldName , filterValue ) = self . filters [ 0 ] matchedKeys = conn . smembers ( self . _get_key_for_index ( filterFieldName , filterValue ) ) else : # Several filters, intersect the index keys indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] matchedKeys = conn . sinter ( indexKeys ) else : # Some negative filters present notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : # Only negative, diff against all keys matchedKeys = conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) else : # Negative and positive. Use pipeline, find all positive intersections, and remove negative matches indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) matchedKeys = pipeline . execute ( ) [ 1 ] # sdiff matchedKeys = [ int ( _key ) for _key in matchedKeys ] if sortByAge is False : return list ( matchedKeys ) else : matchedKeys = list ( matchedKeys ) matchedKeys . sort ( ) return matchedKeys
4774	def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] # Index of the parser in the source running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] # Find the opening bracket. element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : # No starting bracket on the line. if element [ 'required' ] is True : # Try to parse a single single-word token after the # command, like '\input file' content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : # Give up on finding an optional element break # Handle cases when the opening bracket is never found. if element_start is None and element [ 'required' ] is False : # Optional element not found. Continue to next element, # not advancing the running_index of the parser. continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) # Find the closing bracket, keeping track of the number of times # the same type of bracket was opened and closed. balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) # Package the parsed element's content. element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
3254	def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) # DELETE /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules/<granule_id>.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) # maybe return a list of all granules? return None
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return # check for security if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return # get repo if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self # delete files if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) # delete directories if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) # protect from wiping out the system if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) # delete repository os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) # remove main directory if empty if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) # reset repository repo . __reset_repository ( )
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) # call get service with headers and params with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
1674	def ParseArguments ( args ) : try : ( opts , filenames ) = getopt . getopt ( args , '' , [ 'help' , 'output=' , 'verbose=' , 'counting=' , 'filter=' , 'root=' , 'repository=' , 'linelength=' , 'extensions=' , 'exclude=' , 'headers=' , 'quiet' , 'recursive' ] ) except getopt . GetoptError : PrintUsage ( 'Invalid arguments.' ) verbosity = _VerboseLevel ( ) output_format = _OutputFormat ( ) filters = '' counting_style = '' recursive = False for ( opt , val ) in opts : if opt == '--help' : PrintUsage ( None ) elif opt == '--output' : if val not in ( 'emacs' , 'vs7' , 'eclipse' , 'junit' ) : PrintUsage ( 'The only allowed output formats are emacs, vs7, eclipse ' 'and junit.' ) output_format = val elif opt == '--verbose' : verbosity = int ( val ) elif opt == '--filter' : filters = val if not filters : PrintCategories ( ) elif opt == '--counting' : if val not in ( 'total' , 'toplevel' , 'detailed' ) : PrintUsage ( 'Valid counting options are total, toplevel, and detailed' ) counting_style = val elif opt == '--root' : global _root _root = val elif opt == '--repository' : global _repository _repository = val elif opt == '--linelength' : global _line_length try : _line_length = int ( val ) except ValueError : PrintUsage ( 'Line length must be digits.' ) elif opt == '--exclude' : global _excludes if not _excludes : _excludes = set ( ) _excludes . update ( glob . glob ( val ) ) elif opt == '--extensions' : global _valid_extensions try : _valid_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--headers' : global _header_extensions try : _header_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--recursive' : recursive = True elif opt == '--quiet' : global _quiet _quiet = True if not filenames : PrintUsage ( 'No files were specified.' ) if recursive : filenames = _ExpandDirectories ( filenames ) if _excludes : filenames = _FilterExcludedFiles ( filenames ) _SetOutputFormat ( output_format ) _SetVerboseLevel ( verbosity ) _SetFilters ( filters ) _SetCountingStyle ( counting_style ) return filenames
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params # skip tags without parameters if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
10798	def _weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter_size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
5996	def plot_border ( mask , should_plot_border , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if should_plot_border and mask is not None : plt . gca ( ) border_pixels = mask . masked_grid_index_to_pixel [ mask . border_pixels ] if zoom_offset_pixels is not None : border_pixels -= zoom_offset_pixels border_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = border_pixels ) border_units = convert_grid_units ( array = mask , grid_arcsec = border_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = border_units [ : , 0 ] , x = border_units [ : , 1 ] , s = pointsize , c = 'y' )
10895	def set_filter ( self , slices , values ) : self . filters = [ [ sl , values [ sl ] ] for sl in slices ]
12528	def upload ( ctx , repo ) : artifacts = ' ' . join ( shlex . quote ( str ( n ) ) for n in ROOT . joinpath ( 'dist' ) . glob ( 'pipfile[-_]cli-*' ) ) ctx . run ( f'twine upload --repository="{repo}" {artifacts}' )
9342	def kill_all ( self ) : for pid in self . children : try : os . kill ( pid , signal . SIGTRAP ) except OSError : continue self . join ( )
1315	def DeleteLog ( ) -> None : if os . path . exists ( Logger . FileName ) : os . remove ( Logger . FileName )
5645	def make_views ( cls , conn ) : conn . execute ( 'DROP VIEW IF EXISTS main.day_trips' ) conn . execute ( 'CREATE VIEW day_trips AS ' 'SELECT day_trips2.*, trips.* ' #'days.day_start_ut+trips.start_time_ds AS start_time_ut, ' #'days.day_start_ut+trips.end_time_ds AS end_time_ut ' 'FROM day_trips2 JOIN trips USING (trip_I);' ) conn . commit ( ) conn . execute ( 'DROP VIEW IF EXISTS main.day_stop_times' ) conn . execute ( 'CREATE VIEW day_stop_times AS ' 'SELECT day_trips2.*, trips.*, stop_times.*, ' #'days.day_start_ut+trips.start_time_ds AS start_time_ut, ' #'days.day_start_ut+trips.end_time_ds AS end_time_ut, ' 'day_trips2.day_start_ut+stop_times.arr_time_ds AS arr_time_ut, ' 'day_trips2.day_start_ut+stop_times.dep_time_ds AS dep_time_ut ' 'FROM day_trips2 ' 'JOIN trips USING (trip_I) ' 'JOIN stop_times USING (trip_I)' ) conn . commit ( )
2925	def _on_ready ( self , my_task ) : assert my_task is not None self . test ( ) # Acquire locks, if any. for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) if not mutex . testandset ( ) : return # Assign variables, if so requested. for assignment in self . pre_assign : assignment . assign ( my_task , my_task ) # Run task-specific code. self . _on_ready_before_hook ( my_task ) self . reached_event . emit ( my_task . workflow , my_task ) self . _on_ready_hook ( my_task ) # Run user code, if any. if self . ready_event . emit ( my_task . workflow , my_task ) : # Assign variables, if so requested. for assignment in self . post_assign : assignment . assign ( my_task , my_task ) # Release locks, if any. for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) mutex . unlock ( ) self . finished_event . emit ( my_task . workflow , my_task )
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
5762	def write_groovy_script_and_configs ( filename , content , job_configs , view_configs = None ) : with open ( filename , 'w' ) as h : h . write ( content ) if view_configs : view_config_dir = os . path . join ( os . path . dirname ( filename ) , 'view_configs' ) if not os . path . isdir ( view_config_dir ) : os . makedirs ( view_config_dir ) for config_name , config_body in view_configs . items ( ) : config_filename = os . path . join ( view_config_dir , config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body ) job_config_dir = os . path . join ( os . path . dirname ( filename ) , 'job_configs' ) if not os . path . isdir ( job_config_dir ) : os . makedirs ( job_config_dir ) # prefix each config file with a serial number to maintain order format_str = '%0' + str ( len ( str ( len ( job_configs ) ) ) ) + 'd' i = 0 for config_name , config_body in job_configs . items ( ) : i += 1 config_filename = os . path . join ( job_config_dir , format_str % i + ' ' + config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body )
356	def save_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] ckpt_file = os . path . join ( save_dir , mode_name ) if var_list == [ ] : var_list = tf . global_variables ( ) logging . info ( "[*] save %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) saver = tf . train . Saver ( var_list ) saver . save ( sess , ckpt_file , global_step = global_step )
10442	def getobjectlist ( self , window_name ) : try : window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) except atomac . _a11y . ErrorInvalidUIElement : # During the test, when the window closed and reopened # ErrorInvalidUIElement exception will be thrown self . _windows = { } # Call the method again, after updating apps window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) return object_list . keys ( )
13207	def _parse_author ( self ) : command = LatexCommand ( 'author' , { 'name' : 'authors' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return try : content = parsed [ 'authors' ] except KeyError : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return # Clean content content = content . replace ( '\n' , ' ' ) content = content . replace ( '~' , ' ' ) content = content . strip ( ) # Split content into list of individual authors authors = [ ] for part in content . split ( ',' ) : part = part . strip ( ) for split_part in part . split ( 'and ' ) : split_part = split_part . strip ( ) if len ( split_part ) > 0 : authors . append ( split_part ) self . _authors = authors
9418	def document_func_view ( serializer_class = None , response_serializer_class = None , filter_backends = None , permission_classes = None , authentication_classes = None , doc_format_args = list ( ) , doc_format_kwargs = dict ( ) ) : def decorator ( func ) : if serializer_class : func . cls . serializer_class = func . view_class . serializer_class = serializer_class if response_serializer_class : func . cls . response_serializer_class = func . view_class . response_serializer_class = response_serializer_class if filter_backends : func . cls . filter_backends = func . view_class . filter_backends = filter_backends if permission_classes : func . cls . permission_classes = func . view_class . permission_classes = permission_classes if authentication_classes : func . cls . authentication_classes = func . view_class . authentication_classes = authentication_classes if doc_format_args or doc_format_kwargs : func . cls . __doc__ = func . view_class . __doc__ = getdoc ( func ) . format ( * doc_format_args , * * doc_format_kwargs ) return func return decorator
9049	def B ( self ) : return unvec ( self . _vecB . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )
13694	def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse_int ( argd [ '--width' ] or DEFAULT_WIDTH ) or 1 indent = parse_int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : # Smart indent, change max width based on indention. width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : # Smart indent, change max width based on prepended text. width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : # Try each argument as a file name. argd [ 'WORDS' ] = ( ( try_read_file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : # No text/filenames provided, use stdin for input. words = read_stdin ( ) block = FormatBlock ( words ) . iter_format_block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip_first = argd [ '--stripfirst' ] , append = userappend , strip_last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : # Current line number format supports up to 999 lines before # messing up. Who would format 1000 lines like this anyway? print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
457	def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : # Make sure that we keep our distance from the main body if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
7786	def timeout ( self ) : if not self . active : return if not self . _try_backup_item ( ) : if self . _timeout_handler : self . _timeout_handler ( self . address ) else : self . _error_handler ( self . address , None ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
9428	def printdir ( self ) : print ( "%-46s %19s %12s" % ( "File Name" , "Modified " , "Size" ) ) for rarinfo in self . filelist : date = "%d-%02d-%02d %02d:%02d:%02d" % rarinfo . date_time [ : 6 ] print ( "%-46s %s %12d" % ( rarinfo . filename , date , rarinfo . file_size ) )
13086	def get ( self , section , key ) : try : return self . config . get ( section , key ) except configparser . NoSectionError : pass except configparser . NoOptionError : pass return self . defaults [ section ] [ key ]
171	def draw_lines_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , antialiased = True , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_lines_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
12852	def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
8345	def find ( self , name = None , attrs = { } , recursive = True , text = None , * * kwargs ) : r = None l = self . findAll ( name , attrs , recursive , text , 1 , * * kwargs ) if l : r = l [ 0 ] return r
4997	def assign_enterprise_learner_role ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if kwargs [ 'created' ] and instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) SystemWideEnterpriseUserRoleAssignment . objects . get_or_create ( user = instance . user , role = enterprise_learner_role )
12902	def _set_range ( self , start , stop , value , value_len ) : assert stop >= start and value_len >= 0 range_len = stop - start if range_len < value_len : self . _insert_zeros ( stop , stop + value_len - range_len ) self . _copy_to_range ( start , value , value_len ) elif range_len > value_len : self . _del_range ( stop - ( range_len - value_len ) , stop ) self . _copy_to_range ( start , value , value_len ) else : self . _copy_to_range ( start , value , value_len )
6108	def yticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 0 ] ) , np . amax ( self . grid_stack . regular [ : , 0 ] ) , 4 )
9552	def _apply_value_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , check , code , message , modulus in self . _value_checks : if i % modulus == 0 : # support sampling fi = self . _field_names . index ( field_name ) if fi < len ( r ) : # only apply checks if there is a value value = r [ fi ] try : check ( value ) except ValueError : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
77	def project_coords ( coords , from_shape , to_shape ) : from_shape = normalize_shape ( from_shape ) to_shape = normalize_shape ( to_shape ) if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return coords from_height , from_width = from_shape [ 0 : 2 ] to_height , to_width = to_shape [ 0 : 2 ] assert all ( [ v > 0 for v in [ from_height , from_width , to_height , to_width ] ] ) # make sure to not just call np.float32(coords) here as the following lines # perform in-place changes and np.float32(.) only copies if the input # was *not* a float32 array coords_proj = np . array ( coords ) . astype ( np . float32 ) coords_proj [ : , 0 ] = ( coords_proj [ : , 0 ] / from_width ) * to_width coords_proj [ : , 1 ] = ( coords_proj [ : , 1 ] / from_height ) * to_height return coords_proj
11279	def _encode_ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
13113	def zone_transfer ( address , dns_name ) : ips = [ ] try : print_notification ( "Attempting dns zone transfer for {} on {}" . format ( dns_name , address ) ) z = dns . zone . from_xfr ( dns . query . xfr ( address , dns_name ) ) except dns . exception . FormError : print_notification ( "Zone transfer not allowed" ) return ips names = z . nodes . keys ( ) print_success ( "Zone transfer successfull for {}, found {} entries" . format ( address , len ( names ) ) ) for n in names : node = z [ n ] data = node . get_rdataset ( dns . rdataclass . IN , dns . rdatatype . A ) if data : # TODO add hostnames to entries. # hostname = n.to_text() for item in data . items : address = item . address ips . append ( address ) return ips
12956	def _rem_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_key_for_index ( indexedField , val ) , pk )
13593	def check_environment ( target , label ) : if not git . exists ( ) : click . secho ( 'You must have git installed to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not os . path . isdir ( '.git' ) : click . secho ( 'You must cd into a git repository to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not git . is_committed ( ) : click . secho ( 'You must commit or stash your work before proceeding.' , fg = 'red' ) sys . exit ( 1 ) if target is None and label is None : click . secho ( 'You must specify either a target or a label.' , fg = 'red' ) sys . exit ( 1 )
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] ## set generators qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice ## print progress update 1 to the engine stdout print ( self . _chunksize ) ## set to store rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) ## print progress bar update to engine stdout if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break ## store into database fillsets [ : ] = tmpr del tmpr
9832	def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )
10853	def harris_feature ( im , region_size = 5 , to_return = 'harris' , scale = 0.05 ) : ndim = im . ndim #1. Gradient of image grads = [ nd . sobel ( im , axis = i ) for i in range ( ndim ) ] #2. Corner response matrix matrix = np . zeros ( ( ndim , ndim ) + im . shape ) for a in range ( ndim ) : for b in range ( ndim ) : matrix [ a , b ] = nd . filters . gaussian_filter ( grads [ a ] * grads [ b ] , region_size ) if to_return == 'matrix' : return matrix #3. Trace, determinant trc = np . trace ( matrix , axis1 = 0 , axis2 = 1 ) det = np . linalg . det ( matrix . T ) . T if to_return == 'trace-determinant' : return trc , det else : #4. Harris detector: harris = det - scale * trc * trc return harris
6034	def padded_grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_padded_grid = PaddedRegularGrid . padded_grid_from_shape_psf_shape_and_pixel_scale ( shape = mask . shape , psf_shape = psf_shape , pixel_scale = mask . pixel_scale ) sub_padded_grid = PaddedSubGrid . padded_grid_from_mask_sub_grid_size_and_psf_shape ( mask = mask , sub_grid_size = sub_grid_size , psf_shape = psf_shape ) # TODO : The blurring grid is not used when the grid mapper is called, the 0.0 0.0 stops errors inr ayT_racing # TODO : implement a more explicit solution return GridStack ( regular = regular_padded_grid , sub = sub_padded_grid , blurring = np . array ( [ [ 0.0 , 0.0 ] ] ) )
12857	def with_peer ( events ) : stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : stack . append ( obj ) yield obj , None elif obj [ 'type' ] == EXIT : yield obj , stack . pop ( ) else : yield obj , None
12755	def enable_motors ( self , max_force ) : for joint in self . joints : amotor = getattr ( joint , 'amotor' , joint ) amotor . max_forces = max_force if max_force > 0 : amotor . enable_feedback ( ) else : amotor . disable_feedback ( )
4875	def validate_tpa_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : tpa_client = ThirdPartyAuthApiClient ( ) username = tpa_client . get_username_from_remote_id ( enterprise_customer . identity_provider , value ) user = User . objects . get ( username = username ) return models . EnterpriseCustomerUser . objects . get ( user_id = user . id , enterprise_customer = enterprise_customer ) except ( models . EnterpriseCustomerUser . DoesNotExist , User . DoesNotExist ) : pass return None
9252	def generate_unreleased_section ( self ) : if not self . filtered_tags : return "" now = datetime . datetime . utcnow ( ) now = now . replace ( tzinfo = dateutil . tz . tzutc ( ) ) head_tag = { "name" : self . options . unreleased_label } self . tag_times_dict [ head_tag [ "name" ] ] = now unreleased_log = self . generate_log_between_tags ( self . filtered_tags [ 0 ] , head_tag ) return unreleased_log
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
2054	def ADR ( cpu , dest , src ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc dest . write ( aligned_pc + src . read ( ) )
12326	def init ( globalvars = None , show = False ) : global config profileini = getprofileini ( ) if os . path . exists ( profileini ) : config = configparser . ConfigParser ( ) config . read ( profileini ) mgr = plugins_get_mgr ( ) mgr . update_configs ( config ) if show : for source in config : print ( "[%s] :" % ( source ) ) for k in config [ source ] : print ( " %s : %s" % ( k , config [ source ] [ k ] ) ) else : print ( "Profile does not exist. So creating one" ) if not show : update ( globalvars ) print ( "Complete init" )
4717	def tsuite_exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
12354	def change_kernel ( self , kernel_id , wait = True ) : return self . _action ( 'change_kernel' , kernel = kernel_id , wait = wait )
5518	def clone ( self ) : return StreamThrottle ( read = self . read . clone ( ) , write = self . write . clone ( ) )
9017	def _pattern ( self , base ) : rows = self . _rows ( base . get ( ROWS , [ ] ) ) self . _finish_inheritance ( ) self . _finish_instructions ( ) self . _connect_rows ( base . get ( CONNECTIONS , [ ] ) ) id_ = self . _to_id ( base [ ID ] ) name = base [ NAME ] return self . new_pattern ( id_ , name , rows )
508	def match ( self , record ) : for field , meta in self . filterDict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : # Record might be blank, handle this if not record : continue if record [ index ] . find ( category ) != - 1 : ''' This field contains the string we're searching for so we'll keep the records ''' return True # None of the categories were found in this record return False
8523	def add_float ( self , name , min , max , warp = None ) : min , max = map ( float , ( min , max ) ) if not min < max : raise ValueError ( 'variable %s: min >= max error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = FloatVariable ( name , min , max , warp )
5357	def _add_to_conf ( self , new_conf ) : for section in new_conf : if section not in self . conf : self . conf [ section ] = new_conf [ section ] else : for param in new_conf [ section ] : self . conf [ section ] [ param ] = new_conf [ section ] [ param ]
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
1280	def hard_wrap ( self ) : self . linebreak = re . compile ( r'^ *\n(?!\s*$)' ) self . text = re . compile ( r'^[\s\S]+?(?=[\\<!\[_*`~]|https?://| *\n|$)' )
3599	def delivery ( self , packageName , versionCode = None , offerType = 1 , downloadToken = None , expansion_files = False ) : if versionCode is None : # pick up latest version versionCode = self . details ( packageName ) . get ( 'versionCode' ) params = { 'ot' : str ( offerType ) , 'doc' : packageName , 'vc' : str ( versionCode ) } headers = self . getHeaders ( ) if downloadToken is not None : params [ 'dtok' ] = downloadToken response = requests . get ( DELIVERY_URL , headers = headers , params = params , verify = ssl_verify , timeout = 60 , proxies = self . proxies_config ) response = googleplay_pb2 . ResponseWrapper . FromString ( response . content ) if response . commands . displayErrorMessage != "" : raise RequestError ( response . commands . displayErrorMessage ) elif response . payload . deliveryResponse . appDeliveryData . downloadUrl == "" : raise RequestError ( 'App not purchased' ) else : result = { } result [ 'docId' ] = packageName result [ 'additionalData' ] = [ ] downloadUrl = response . payload . deliveryResponse . appDeliveryData . downloadUrl cookie = response . payload . deliveryResponse . appDeliveryData . downloadAuthCookie [ 0 ] cookies = { str ( cookie . name ) : str ( cookie . value ) } result [ 'file' ] = self . _deliver_data ( downloadUrl , cookies ) if not expansion_files : return result for obb in response . payload . deliveryResponse . appDeliveryData . additionalFile : a = { } # fileType == 0 -> main # fileType == 1 -> patch if obb . fileType == 0 : obbType = 'main' else : obbType = 'patch' a [ 'type' ] = obbType a [ 'versionCode' ] = obb . versionCode a [ 'file' ] = self . _deliver_data ( obb . downloadUrl , None ) result [ 'additionalData' ] . append ( a ) return result
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
12343	def image ( self , well_row , well_column , field_row , field_column ) : return next ( ( i for i in self . images if attribute ( i , 'u' ) == well_column and attribute ( i , 'v' ) == well_row and attribute ( i , 'x' ) == field_column and attribute ( i , 'y' ) == field_row ) , '' )
6479	def _normalised_python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
4823	def _sort_course_modes ( self , modes ) : def slug_weight ( mode ) : """ Assign a weight to the course mode dictionary based on the position of its slug in the sorting list. """ sorting_slugs = COURSE_MODE_SORT_ORDER sorting_slugs_size = len ( sorting_slugs ) if mode [ 'slug' ] in sorting_slugs : return sorting_slugs_size - sorting_slugs . index ( mode [ 'slug' ] ) return 0 # Sort slug weights in descending order return sorted ( modes , key = slug_weight , reverse = True )
3709	def calculate ( self , T , method ) : if method == RACKETT : Vm = Rackett ( T , self . Tc , self . Pc , self . Zc ) elif method == YAMADA_GUNN : Vm = Yamada_Gunn ( T , self . Tc , self . Pc , self . omega ) elif method == BHIRUD_NORMAL : Vm = Bhirud_normal ( T , self . Tc , self . Pc , self . omega ) elif method == TOWNSEND_HALES : Vm = Townsend_Hales ( T , self . Tc , self . Vc , self . omega ) elif method == HTCOSTALD : Vm = COSTALD ( T , self . Tc , self . Vc , self . omega ) elif method == YEN_WOODS_SAT : Vm = Yen_Woods_saturation ( T , self . Tc , self . Vc , self . Zc ) elif method == MMSNM0 : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega ) elif method == MMSNM0FIT : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega , self . SNM0_delta_SRK ) elif method == CAMPBELL_THODOS : Vm = Campbell_Thodos ( T , self . Tb , self . Tc , self . Pc , self . MW , self . dipole ) elif method == HTCOSTALDFIT : Vm = COSTALD ( T , self . Tc , self . COSTALD_Vchar , self . COSTALD_omega_SRK ) elif method == RACKETTFIT : Vm = Rackett ( T , self . Tc , self . Pc , self . RACKETT_Z_RA ) elif method == PERRYDIPPR : A , B , C , D = self . DIPPR_coeffs Vm = 1. / EQ105 ( T , A , B , C , D ) elif method == CRC_INORG_L : rho = CRC_inorganic ( T , self . CRC_INORG_L_rho , self . CRC_INORG_L_k , self . CRC_INORG_L_Tm ) Vm = rho_to_Vm ( rho , self . CRC_INORG_L_MW ) elif method == VDI_PPDS : A , B , C , D = self . VDI_PPDS_coeffs tau = 1. - T / self . VDI_PPDS_Tc rho = self . VDI_PPDS_rhoc + A * tau ** 0.35 + B * tau ** ( 2 / 3. ) + C * tau + D * tau ** ( 4 / 3. ) Vm = rho_to_Vm ( rho , self . VDI_PPDS_MW ) elif method == CRC_INORG_L_CONST : Vm = self . CRC_INORG_L_CONST_Vm elif method == COOLPROP : Vm = 1. / CoolProp_T_dependent_property ( T , self . CASRN , 'DMOLAR' , 'l' ) elif method in self . tabular_data : Vm = self . interpolate ( T , method ) return Vm
7532	def trackjobs ( func , results , spacer ) : ## TODO: try to insert a better way to break on KBD here. LOGGER . info ( "inside trackjobs of %s" , func ) ## get just the jobs from results that are relevant to this func asyncs = [ ( i , results [ i ] ) for i in results if i . split ( "-" , 2 ) [ 0 ] == func ] ## progress bar start = time . time ( ) while 1 : ## how many of this func have finished so far ready = [ i [ 1 ] . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {} | {} | s3 |" . format ( PRINTSTR [ func ] , elapsed ) progressbar ( len ( ready ) , sum ( ready ) , printstr , spacer = spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break sfails = [ ] errmsgs = [ ] for job in asyncs : if not job [ 1 ] . successful ( ) : sfails . append ( job [ 0 ] ) errmsgs . append ( job [ 1 ] . result ( ) ) return func , sfails , errmsgs
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
11655	def transform ( self , X , * * params ) : X = as_features ( X , stack = True ) X_new = self . transformer . transform ( X . stacked_features , * * params ) return self . _gather_outputs ( X , X_new )
9825	def groups ( ctx , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiment_groups ( username = user , project_name = project_name , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment groups for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiment groups for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiment groups found for project `{}/{}`.' . format ( user , project_name ) ) objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiment groups:" ) objects . pop ( 'project' , None ) objects . pop ( 'user' , None ) dict_tabulate ( objects , is_list_dict = True )
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] # also write it into the digital response table self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val # send data through callback if there is a callback function for the pin if sonar_pin_entry [ 0 ] is not None : # check if value changed since last reading if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) # update the data in the table with latest value sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
5326	def __create_arthur_json ( self , repo , backend_args ) : backend_args = self . _compose_arthur_params ( self . backend_section , repo ) if self . backend_section == 'git' : backend_args [ 'gitpath' ] = os . path . join ( self . REPOSITORY_DIR , repo ) backend_args [ 'tag' ] = self . backend_tag ( repo ) ajson = { "tasks" : [ { } ] } # This is the perceval tag ajson [ "tasks" ] [ 0 ] [ 'task_id' ] = self . backend_tag ( repo ) ajson [ "tasks" ] [ 0 ] [ 'backend' ] = self . backend_section . split ( ":" ) [ 0 ] ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] = backend_args ajson [ "tasks" ] [ 0 ] [ 'category' ] = backend_args [ 'category' ] ajson [ "tasks" ] [ 0 ] [ 'archive' ] = { } ajson [ "tasks" ] [ 0 ] [ 'scheduler' ] = { "delay" : self . ARTHUR_TASK_DELAY } # from-date or offset param must be added es_col_url = self . _get_collection_url ( ) es_index = self . conf [ self . backend_section ] [ 'raw_index' ] # Get the last activity for the data source es = ElasticSearch ( es_col_url , es_index ) connector = get_connector_from_name ( self . backend_section ) klass = connector [ 0 ] # Backend for the connector signature = inspect . signature ( klass . fetch ) last_activity = None filter_ = { "name" : "tag" , "value" : backend_args [ 'tag' ] } if 'from_date' in signature . parameters : last_activity = es . get_last_item_field ( 'metadata__updated_on' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'from_date' ] = last_activity . isoformat ( ) elif 'offset' in signature . parameters : last_activity = es . get_last_item_field ( 'offset' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'offset' ] = last_activity if last_activity : logging . info ( "Getting raw item with arthur since %s" , last_activity ) return ( ajson )
6937	def parallel_update_objectinfo_cplist ( cplist , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : # work around the Darwin segfault after fork if no network activity in # main thread bug: https://bugs.python.org/issue30385#msg293958 if sys . platform == 'darwin' : import requests requests . get ( 'http://captive.apple.com/hotspot-detect.html' ) # handle the start and end indices if ( liststartindex is not None ) and ( maxobjects is None ) : cplist = cplist [ liststartindex : ] elif ( liststartindex is None ) and ( maxobjects is not None ) : cplist = cplist [ : maxobjects ] elif ( liststartindex is not None ) and ( maxobjects is not None ) : cplist = ( cplist [ liststartindex : liststartindex + maxobjects ] ) tasks = [ ( x , { 'fast_mode' : fast_mode , 'findercmap' : findercmap , 'finderconvolve' : finderconvolve , 'deredden_object' : deredden_object , 'custom_bandpasses' : custom_bandpasses , 'gaia_submit_timeout' : gaia_submit_timeout , 'gaia_submit_tries' : gaia_submit_tries , 'gaia_max_timeout' : gaia_max_timeout , 'gaia_mirror' : gaia_mirror , 'complete_query_later' : complete_query_later , 'lclistpkl' : lclistpkl , 'nbrradiusarcsec' : nbrradiusarcsec , 'maxnumneighbors' : maxnumneighbors , 'plotdpi' : plotdpi , 'findercachedir' : findercachedir , 'verbose' : verbose } ) for x in cplist ] resultfutures = [ ] results = [ ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( cp_objectinfo_worker , tasks ) results = [ x for x in resultfutures ] executor . shutdown ( ) return results
4523	def color_scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
8998	def string ( self , string ) : object_ = json . loads ( string ) return self . object ( object_ )
4165	def _get_link ( self , cobj ) : fname_idx = None full_name = cobj [ 'module_short' ] + '.' + cobj [ 'name' ] if full_name in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ full_name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname_idx = value [ 0 ] elif cobj [ 'module_short' ] in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ cobj [ 'module_short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname_idx = value [ cobj [ 'name' ] ] [ 0 ] if fname_idx is not None : fname = self . _searchindex [ 'filenames' ] [ fname_idx ] + '.html' if self . _is_windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc_url , fname ) else : link = posixpath . join ( self . doc_url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . _page_cache : html = self . _page_cache [ link ] else : html = get_data ( link , self . gallery_dir ) self . _page_cache [ link ] = html # test if cobj appears in page comb_names = [ cobj [ 'module_short' ] + '.' + cobj [ 'name' ] ] if self . extra_modules_test is not None : for mod in self . extra_modules_test : comb_names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : # Decode bytes under Python 3 html = html . decode ( 'utf-8' , 'replace' ) for comb_name in comb_names : if hasattr ( comb_name , 'decode' ) : # Decode bytes under Python 3 comb_name = comb_name . decode ( 'utf-8' , 'replace' ) if comb_name in html : url = link + u'#' + comb_name link = url else : link = False return link
5116	def draw ( self , update_colors = True , line_kwargs = None , scatter_kwargs = None , * * kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "matplotlib is necessary to draw the network." ) if update_colors : self . _update_all_colors ( ) if 'bgcolor' not in kwargs : kwargs [ 'bgcolor' ] = self . colors [ 'bgcolor' ] self . g . draw_graph ( line_kwargs = line_kwargs , scatter_kwargs = scatter_kwargs , * * kwargs )
6006	def load_ccd_data_from_fits ( image_path , pixel_scale , image_hdu = 0 , resized_ccd_shape = None , resized_ccd_origin_pixels = None , resized_ccd_origin_arcsec = None , psf_path = None , psf_hdu = 0 , resized_psf_shape = None , renormalize_psf = True , noise_map_path = None , noise_map_hdu = 0 , noise_map_from_image_and_background_noise_map = False , convert_noise_map_from_weight_map = False , convert_noise_map_from_inverse_noise_map = False , background_noise_map_path = None , background_noise_map_hdu = 0 , convert_background_noise_map_from_weight_map = False , convert_background_noise_map_from_inverse_noise_map = False , poisson_noise_map_path = None , poisson_noise_map_hdu = 0 , poisson_noise_map_from_image = False , convert_poisson_noise_map_from_weight_map = False , convert_poisson_noise_map_from_inverse_noise_map = False , exposure_time_map_path = None , exposure_time_map_hdu = 0 , exposure_time_map_from_single_value = None , exposure_time_map_from_inverse_noise_map = False , background_sky_map_path = None , background_sky_map_hdu = 0 , convert_from_electrons = False , gain = None , convert_from_adus = False , lens_name = None ) : image = load_image ( image_path = image_path , image_hdu = image_hdu , pixel_scale = pixel_scale ) background_noise_map = load_background_noise_map ( background_noise_map_path = background_noise_map_path , background_noise_map_hdu = background_noise_map_hdu , pixel_scale = pixel_scale , convert_background_noise_map_from_weight_map = convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map = convert_background_noise_map_from_inverse_noise_map ) if background_noise_map is not None : inverse_noise_map = 1.0 / background_noise_map else : inverse_noise_map = None exposure_time_map = load_exposure_time_map ( exposure_time_map_path = exposure_time_map_path , exposure_time_map_hdu = exposure_time_map_hdu , pixel_scale = pixel_scale , shape = image . shape , exposure_time = exposure_time_map_from_single_value , exposure_time_map_from_inverse_noise_map = exposure_time_map_from_inverse_noise_map , inverse_noise_map = inverse_noise_map ) poisson_noise_map = load_poisson_noise_map ( poisson_noise_map_path = poisson_noise_map_path , poisson_noise_map_hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale , convert_poisson_noise_map_from_weight_map = convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map = convert_poisson_noise_map_from_inverse_noise_map , image = image , exposure_time_map = exposure_time_map , poisson_noise_map_from_image = poisson_noise_map_from_image , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) noise_map = load_noise_map ( noise_map_path = noise_map_path , noise_map_hdu = noise_map_hdu , pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_noise_map_from_weight_map = convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map = convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map = noise_map_from_image_and_background_noise_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) psf = load_psf ( psf_path = psf_path , psf_hdu = psf_hdu , pixel_scale = pixel_scale , renormalize = renormalize_psf ) background_sky_map = load_background_sky_map ( background_sky_map_path = background_sky_map_path , background_sky_map_hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) image = CCDData ( image = image , pixel_scale = pixel_scale , psf = psf , noise_map = noise_map , background_noise_map = background_noise_map , poisson_noise_map = poisson_noise_map , exposure_time_map = exposure_time_map , background_sky_map = background_sky_map , gain = gain , name = lens_name ) if resized_ccd_shape is not None : image = image . new_ccd_data_with_resized_arrays ( new_shape = resized_ccd_shape , new_centre_pixels = resized_ccd_origin_pixels , new_centre_arcsec = resized_ccd_origin_arcsec ) if resized_psf_shape is not None : image = image . new_ccd_data_with_resized_psf ( new_shape = resized_psf_shape ) if convert_from_electrons : image = image . new_ccd_data_converted_from_electrons ( ) elif convert_from_adus : image = image . new_ccd_data_converted_from_adus ( gain = gain ) return image
11965	def _oct_to_dec ( ip , check = True ) : if check and not is_oct ( ip ) : raise ValueError ( '_oct_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = oct ( ip ) return int ( str ( ip ) , 8 )
753	def _translateMetricsToJSON ( self , metrics , label ) : # Transcode the MetricValueElement values into JSON-compatible # structure metricsDict = metrics # Convert the structure to a display-friendly JSON string def _mapNumpyValues ( obj ) : """ """ import numpy if isinstance ( obj , numpy . float32 ) : return float ( obj ) elif isinstance ( obj , numpy . bool_ ) : return bool ( obj ) elif isinstance ( obj , numpy . ndarray ) : return obj . tolist ( ) else : raise TypeError ( "UNEXPECTED OBJ: %s; class=%s" % ( obj , obj . __class__ ) ) jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) return jsonString
2351	def register ( ) : registerDriver ( ISelenium , Selenium , class_implements = [ Firefox , Chrome , Ie , Edge , Opera , Safari , BlackBerry , PhantomJS , Android , Remote , EventFiringWebDriver , ] , )
5236	def file_modified_time ( file_name ) -> pd . Timestamp : return pd . to_datetime ( time . ctime ( os . path . getmtime ( filename = file_name ) ) )
3004	def _get_oauth2_client_id_and_secret ( settings_instance ) : secret_json = getattr ( settings_instance , 'GOOGLE_OAUTH2_CLIENT_SECRETS_JSON' , None ) if secret_json is not None : return _load_client_secrets ( secret_json ) else : client_id = getattr ( settings_instance , "GOOGLE_OAUTH2_CLIENT_ID" , None ) client_secret = getattr ( settings_instance , "GOOGLE_OAUTH2_CLIENT_SECRET" , None ) if client_id is not None and client_secret is not None : return client_id , client_secret else : raise exceptions . ImproperlyConfigured ( "Must specify either GOOGLE_OAUTH2_CLIENT_SECRETS_JSON, or " "both GOOGLE_OAUTH2_CLIENT_ID and " "GOOGLE_OAUTH2_CLIENT_SECRET in settings.py" )
12906	def objHasUnsavedChanges ( self ) : if not self . obj : return False return self . obj . hasUnsavedChanges ( cascadeObjects = True )
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( * * filters ) entity = query . first ( ) if not entity : entity = self . model_class ( * * filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
10407	def bond_initialize_canonical_averages ( canonical_statistics , * * kwargs ) : # initialize return array spanning_cluster = ( 'percolation_probability' in canonical_statistics . dtype . names ) # array should have the same size as the input array ret = np . empty_like ( canonical_statistics , dtype = canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) ret [ 'number_of_runs' ] = 1 # initialize percolation probability mean and sum of squared differences if spanning_cluster : ret [ 'percolation_probability_mean' ] = ( canonical_statistics [ 'percolation_probability' ] ) ret [ 'percolation_probability_m2' ] = 0.0 # initialize maximum cluster size mean and sum of squared differences ret [ 'max_cluster_size_mean' ] = ( canonical_statistics [ 'max_cluster_size' ] ) ret [ 'max_cluster_size_m2' ] = 0.0 # initialize moments means and sums of squared differences ret [ 'moments_mean' ] = canonical_statistics [ 'moments' ] ret [ 'moments_m2' ] = 0.0 return ret
12344	def well_images ( self , well_row , well_column ) : return list ( i for i in self . images if attribute ( i , 'u' ) == well_column and attribute ( i , 'v' ) == well_row )
5917	def check_output ( self , make_ndx_output , message = None , err = None ) : if message is None : message = "" else : message = '\n' + message def format ( output , w = 60 ) : hrule = "====[ GromacsError (diagnostic output) ]" . ljust ( w , "=" ) return hrule + '\n' + str ( output ) + hrule rc = True if self . _is_empty_group ( make_ndx_output ) : warnings . warn ( "Selection produced empty group.{message!s}" . format ( * * vars ( ) ) , category = GromacsValueWarning ) rc = False if self . _has_syntax_error ( make_ndx_output ) : rc = False out_formatted = format ( make_ndx_output ) raise GromacsError ( "make_ndx encountered a Syntax Error, " "%(message)s\noutput:\n%(out_formatted)s" % vars ( ) ) if make_ndx_output . strip ( ) == "" : rc = False out_formatted = format ( err ) raise GromacsError ( "make_ndx produced no output, " "%(message)s\nerror output:\n%(out_formatted)s" % vars ( ) ) return rc
3499	def assess ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True else : results = dict ( ) results [ 'precursors' ] = assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff ) results [ 'products' ] = assess_component ( model , reaction , 'products' , flux_coefficient_cutoff ) return results
12339	def compress ( images , delete_tif = False , folder = None ) : if type ( images ) == str : # only one image return [ compress_blocking ( images , delete_tif , folder ) ] filenames = copy ( images ) # as images property will change when looping return Parallel ( n_jobs = _pools ) ( delayed ( compress_blocking ) ( image = image , delete_tif = delete_tif , folder = folder ) for image in filenames )
8521	def plot_4 ( data , * args ) : params = nonconstant_parameters ( data ) scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p_list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( "error making plot4 for '%s'" % key ) continue p_list . append ( build_scatter_tooltip ( x = x , y = y , radius = radius , add_line = False , tt = params , xlabel = key , title = 'Score vs %s' % key ) ) return p_list
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : # This can happen if the relative path is a URL continue # # Prepare the target path targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass # print(sourcepath," => ", targetpath) print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) # get cache path to put the file cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path # make HEAD request to check ETag response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : # dropbox return code 301, so we ignore this error pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) # add ETag to filename if it exists # etag = response.headers.get("ETag") if not cache_path . exists ( ) : # Download to temporary file, then copy to cache dir once finished. # Otherwise you get corrupt cache entries if the download gets interrupted. fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) # GET file object req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : # filter out keep-alive new chunks progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
10544	def update_task ( task ) : try : task_id = task . id task = _forbidden_attributes ( task ) res = _pybossa_req ( 'put' , 'task' , task_id , payload = task . data ) if res . get ( 'id' ) : return Task ( res ) else : return res except : # pragma: no cover raise
699	def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , matured = None , lastDescendent = False ) : # The indexes of all the models in this swarm. This list excludes hidden # (orphaned) models. if swarmId is not None : entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) else : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) # Get the particles of interest particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] # If this entry is hidden (i.e. it was an orphaned model), it should # not be in this list if swarmId is not None : assert ( not entry [ 'hidden' ] ) # Get info on this model modelParams = entry [ 'modelParams' ] isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue if completed is not None and ( completed != isCompleted ) : continue if matured is not None and ( matured != isMatured ) : continue if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : continue # Incorporate into return values particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
8733	def parse_timedelta ( str ) : deltas = ( _parse_timedelta_part ( part . strip ( ) ) for part in str . split ( ',' ) ) return sum ( deltas , datetime . timedelta ( ) )
7606	def search_clans ( self , * * params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , * * params )
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
8364	def _output_file ( self , frame ) : if self . buff : return self . buff elif self . multifile : return self . file_root + "_%03d" % frame + self . file_ext else : return self . filename
13516	def dimension ( self , length , draught , beam , speed , slenderness_coefficient , prismatic_coefficient ) : self . length = length self . draught = draught self . beam = beam self . speed = speed self . slenderness_coefficient = slenderness_coefficient self . prismatic_coefficient = prismatic_coefficient self . displacement = ( self . length / self . slenderness_coefficient ) ** 3 self . surface_area = 1.025 * ( 1.7 * self . length * self . draught + self . displacement / self . draught )
2318	def predict ( self , data , alpha = 0.01 , max_iter = 2000 , * * kwargs ) : edge_model = GraphLasso ( alpha = alpha , max_iter = max_iter ) edge_model . fit ( data . values ) return nx . relabel_nodes ( nx . DiGraph ( edge_model . get_precision ( ) ) , { idx : i for idx , i in enumerate ( data . columns ) } )
281	def plot_monthly_returns_dist ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) ax . hist ( 100 * monthly_ret_table , color = 'orangered' , alpha = 0.80 , bins = 20 , * * kwargs ) ax . axvline ( 100 * monthly_ret_table . mean ( ) , color = 'gold' , linestyle = '--' , lw = 4 , alpha = 1.0 ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 , alpha = 0.75 ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Number of months' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Distribution of monthly returns" ) return ax
4972	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerReportingConfigAdminForm , self ) . clean ( ) report_customer = cleaned_data . get ( 'enterprise_customer' ) # Check that any selected catalogs are tied to the selected enterprise. invalid_catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned_data . get ( 'enterprise_customer_catalogs' ) if catalog . enterprise_customer != report_customer ] if invalid_catalogs : message = _ ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise_customer}: {invalid_catalogs}' , ) . format ( enterprise_customer = report_customer , invalid_catalogs = invalid_catalogs , ) self . add_error ( 'enterprise_customer_catalogs' , message )
1022	def createTMs ( includeCPP = True , includePy = True , numCols = 100 , cellsPerCol = 4 , activationThreshold = 3 , minThreshold = 3 , newSynapseCount = 3 , initialPerm = 0.6 , permanenceInc = 0.1 , permanenceDec = 0.0 , globalDecay = 0.0 , pamLength = 0 , checkSynapseConsistency = True , maxInfBacktrack = 0 , maxLrnBacktrack = 0 , * * kwargs ) : # Keep these fixed: connectedPerm = 0.5 tms = dict ( ) if includeCPP : if VERBOSITY >= 2 : print "Creating BacktrackingTMCPP instance" cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , checkSynapseConsistency = checkSynapseConsistency , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) # Ensure we are copying over learning states for TMDiff cpp_tm . retrieveLearningStates = True tms [ 'CPP' ] = cpp_tm if includePy : if VERBOSITY >= 2 : print "Creating PY TM instance" py_tm = BacktrackingTM ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) tms [ 'PY ' ] = py_tm return tms
12989	def setup_notebook ( debug = False ) : output_notebook ( INLINE , hide_banner = True ) if debug : _setup_logging ( logging . DEBUG ) logging . debug ( 'Running notebook in debug mode.' ) else : _setup_logging ( logging . WARNING ) # If JUPYTERHUB_SERVICE_PREFIX environment variable isn't set, # this means that you're running JupyterHub not with Hub in k8s, # and not using run_local.sh (which sets it to empty). if 'JUPYTERHUB_SERVICE_PREFIX' not in os . environ : global jupyter_proxy_url jupyter_proxy_url = 'localhost:8888' logging . info ( 'Setting jupyter proxy to local mode.' )
6218	def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
10408	def bond_reduce ( row_a , row_b ) : spanning_cluster = ( 'percolation_probability_mean' in row_a . dtype . names and 'percolation_probability_mean' in row_b . dtype . names and 'percolation_probability_m2' in row_a . dtype . names and 'percolation_probability_m2' in row_b . dtype . names ) # initialize return array ret = np . empty_like ( row_a ) def _reducer ( key , transpose = False ) : mean_key = '{}_mean' . format ( key ) m2_key = '{}_m2' . format ( key ) res = simoa . stats . online_variance ( * [ ( row [ 'number_of_runs' ] , row [ mean_key ] . T if transpose else row [ mean_key ] , row [ m2_key ] . T if transpose else row [ m2_key ] , ) for row in [ row_a , row_b ] ] ) ( ret [ mean_key ] , ret [ m2_key ] , ) = ( res [ 1 ] . T , res [ 2 ] . T , ) if transpose else res [ 1 : ] if spanning_cluster : _reducer ( 'percolation_probability' ) _reducer ( 'max_cluster_size' ) _reducer ( 'moments' , transpose = True ) ret [ 'number_of_runs' ] = row_a [ 'number_of_runs' ] + row_b [ 'number_of_runs' ] return ret
6509	def set_search_enviroment ( cls , * * kwargs ) : initializer = _load_class ( getattr ( settings , "SEARCH_INITIALIZER" , None ) , cls ) ( ) return initializer . initialize ( * * kwargs )
2234	def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) # print('ENQUEUE LIVE {!r} {!r}'.format(stream, line)) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : # print('ENQUEUE FINAL {!r} {!r}'.format(stream, line)) stream_queue . put ( line ) # print("STREAM IS DONE {!r}".format(stream)) stream_queue . put ( None ) # signal that the stream is finished # stream.close() stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True # thread dies with the program _thread . start ( ) return stream_queue
100	def compute_line_intersection_point ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 ) : def _make_line ( p1 , p2 ) : A = ( p1 [ 1 ] - p2 [ 1 ] ) B = ( p2 [ 0 ] - p1 [ 0 ] ) C = ( p1 [ 0 ] * p2 [ 1 ] - p2 [ 0 ] * p1 [ 1 ] ) return A , B , - C L1 = _make_line ( ( x1 , y1 ) , ( x2 , y2 ) ) L2 = _make_line ( ( x3 , y3 ) , ( x4 , y4 ) ) D = L1 [ 0 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 0 ] Dx = L1 [ 2 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 2 ] Dy = L1 [ 0 ] * L2 [ 2 ] - L1 [ 2 ] * L2 [ 0 ] if D != 0 : x = Dx / D y = Dy / D return x , y else : return False
8970	def step ( self , other_pub ) : if self . triggersStep ( other_pub ) : self . __wrapOtherPub ( other_pub ) self . __newRootKey ( "receiving" ) self . __newRatchetKey ( ) self . __newRootKey ( "sending" )
6848	def find_working_password ( self , usernames = None , host_strings = None ) : r = self . local_renderer if host_strings is None : host_strings = [ ] if not host_strings : host_strings . append ( self . genv . host_string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host_string in host_strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user_default_passwords [ username ] ) passwords . append ( self . genv . user_passwords [ username ] ) passwords . append ( self . env . default_password ) for password in passwords : with settings ( warn_only = True ) : r . env . host_string = host_string r . env . password = password r . env . user = username ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #print('ret.return_code:', ret.return_code) # print('ret000:[%s]' % ret) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. return host_string , username , password raise Exception ( 'No working login found.' )
8188	def betweenness_centrality ( self , normalized = True ) : bc = proximity . brandes_betweenness_centrality ( self , normalized ) for id , w in bc . iteritems ( ) : self [ id ] . _betweenness = w return bc
12206	def raise_for_status ( response ) : for err_name in web_exceptions . __all__ : err = getattr ( web_exceptions , err_name ) if err . status_code == response . status : payload = dict ( headers = response . headers , reason = response . reason , ) if issubclass ( err , web_exceptions . _HTTPMove ) : # pylint: disable=protected-access raise err ( response . headers [ 'Location' ] , * * payload ) raise err ( * * payload )
4713	def hooks_setup ( trun , parent , hnames = None ) : hooks = { "enter" : [ ] , "exit" : [ ] } if hnames is None : # Nothing to do, just return the struct return hooks for hname in hnames : # Fill out paths for med in HOOK_PATTERNS : for ptn in HOOK_PATTERNS [ med ] : fpath = os . sep . join ( [ trun [ "conf" ] [ "HOOKS" ] , ptn % hname ] ) if not os . path . exists ( fpath ) : continue hook = hook_setup ( parent , fpath ) if not hook : continue hooks [ med ] . append ( hook ) if not hooks [ "enter" ] + hooks [ "exit" ] : cij . err ( "rnr:hooks_setup:FAIL { hname: %r has no files }" % hname ) return None return hooks
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
1998	def visit ( self , node , use_fixed_point = False ) : cache = self . _cache visited = set ( ) stack = [ ] stack . append ( node ) while stack : node = stack . pop ( ) if node in cache : self . push ( cache [ node ] ) elif isinstance ( node , Operation ) : if node in visited : operands = [ self . pop ( ) for _ in range ( len ( node . operands ) ) ] value = self . _method ( node , * operands ) visited . remove ( node ) self . push ( value ) cache [ node ] = value else : visited . add ( node ) stack . append ( node ) stack . extend ( node . operands ) else : self . push ( self . _method ( node ) ) if use_fixed_point : old_value = None new_value = self . pop ( ) while old_value is not new_value : self . visit ( new_value ) old_value = new_value new_value = self . pop ( ) self . push ( new_value )
10286	def get_subgraph_peripheral_nodes ( graph : BELGraph , subgraph : Iterable [ BaseEntity ] , node_predicates : NodePredicates = None , edge_predicates : EdgePredicates = None , ) : node_filter = concatenate_node_predicates ( node_predicates = node_predicates ) edge_filter = and_edge_predicates ( edge_predicates = edge_predicates ) result = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for u , v , k , d in get_peripheral_successor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ v ] [ 'predecessor' ] [ u ] . append ( ( k , d ) ) for u , v , k , d in get_peripheral_predecessor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ u ] [ 'successor' ] [ v ] . append ( ( k , d ) ) return result
7911	def get ( self , key , local_default = None , required = False ) : # pylint: disable-msg=W0221 if key in self . _settings : return self . _settings [ key ] if local_default is not None : return local_default if key in self . _defs : setting_def = self . _defs [ key ] if setting_def . default is not None : return setting_def . default factory = setting_def . factory if factory is None : return None value = factory ( self ) if setting_def . cache is True : setting_def . default = value return value if required : raise KeyError ( key ) return local_default
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
6574	def formatter ( self , api_client , data , newval ) : if newval is None : return None user_param = data [ '_paramAdditionalUrls' ] urls = { } if isinstance ( newval , str ) : urls [ user_param [ 0 ] ] = newval else : for key , url in zip ( user_param , newval ) : urls [ key ] = url return urls
5713	def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
5028	def transmit_learner_data ( self , user ) : exporter = self . get_learner_data_exporter ( user ) transmitter = self . get_learner_data_transmitter ( ) transmitter . transmit ( exporter )
2256	def boolmask ( indices , maxval = None ) : if maxval is None : indices = list ( indices ) maxval = max ( indices ) + 1 mask = [ False ] * maxval for index in indices : mask [ index ] = True return mask
8413	def to_numeric ( self , td ) : if self . package == 'pandas' : return td . value / NANOSECONDS [ self . units ] else : return td . total_seconds ( ) / SECONDS [ self . units ]
2695	def parse_doc ( json_iter ) : global DEBUG for meta in json_iter : base_idx = 0 for graf_text in filter_quotes ( meta [ "text" ] , is_email = False ) : if DEBUG : print ( "graf_text:" , graf_text ) grafs , new_base_idx = parse_graf ( meta [ "id" ] , graf_text , base_idx ) base_idx = new_base_idx for graf in grafs : yield graf
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) # on windows os.path.relpath raises a ValueError if the two paths are on # different drives, in that case we just ignore the exception as the two # paths are anyway not relative relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) # copy extra files for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
9786	def resources ( ctx , gpu ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . build_job . resources ( user , project_name , _build , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
2594	def interactive ( f ) : # build new FunctionType, so it can have the right globals # interactive functions never have closures, that's kind of the point if isinstance ( f , FunctionType ) : mainmod = __import__ ( '__main__' ) f = FunctionType ( f . __code__ , mainmod . __dict__ , f . __name__ , f . __defaults__ , ) # associate with __main__ for uncanning f . __module__ = '__main__' return f
5787	def _advapi32_encrypt ( cipher , key , data , iv , padding ) : context_handle = None key_handle = None try : context_handle , key_handle = _advapi32_create_handles ( cipher , key , iv ) out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , buffer , out_len , buffer_len ) handle_error ( res ) output = bytes_from_buffer ( buffer , deref ( out_len ) ) # Remove padding when not required. CryptoAPI doesn't support this, so # we just manually remove it. if cipher == 'aes' and not padding : if output [ - 16 : ] != ( b'\x10' * 16 ) : raise ValueError ( 'Invalid padding generated by OS crypto library' ) output = output [ : - 16 ] return output finally : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle )
11978	def get_wildcard ( self ) : return _convert ( self . _ip , notation = NM_WILDCARD , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
1418	def create_execution_state ( self , topologyName , executionState ) : if not executionState or not executionState . IsInitialized ( ) : raise_ ( StateException ( "Execution State protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_execution_state_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) executionStateString = executionState . SerializeToString ( ) try : self . client . create ( path , value = executionStateString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating execution state" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating execution state" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating execution state" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : # Just re raise the exception. raise
4450	def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
10821	def query_by_user ( cls , user , * * kwargs ) : return cls . _filter ( cls . query . filter_by ( user_id = user . get_id ( ) ) , * * kwargs )
3998	def copy_from_local ( local_path , remote_name , remote_path , demote = True ) : if not os . path . exists ( local_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist' . format ( local_path ) ) temp_identifier = str ( uuid . uuid1 ( ) ) if os . path . isdir ( local_path ) : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_dir_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path ) else : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_file_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path )
6892	def serial_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None ) : # make sure to make the output directory if it doesn't exist if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] # read in the kdtree pickle with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _starfeatures_worker ( task ) return result
11618	def detect ( text ) : if sys . version_info < ( 3 , 0 ) : # Verify encoding try : text = text . decode ( 'utf-8' ) except UnicodeError : pass # Brahmic schemes are all within a specific range of code points. for L in text : code = ord ( L ) if code >= BRAHMIC_FIRST_CODE_POINT : for name , start_code in BLOCKS : if start_code <= code <= BRAHMIC_LAST_CODE_POINT : return name # Romanizations if Regex . IAST_OR_KOLKATA_ONLY . search ( text ) : if Regex . KOLKATA_ONLY . search ( text ) : return Scheme . Kolkata else : return Scheme . IAST if Regex . ITRANS_ONLY . search ( text ) : return Scheme . ITRANS if Regex . SLP1_ONLY . search ( text ) : return Scheme . SLP1 if Regex . VELTHUIS_ONLY . search ( text ) : return Scheme . Velthuis if Regex . ITRANS_OR_VELTHUIS_ONLY . search ( text ) : return Scheme . ITRANS return Scheme . HK
1655	def IsDerivedFunction ( clean_lines , linenum ) : # Scan back a few lines for start of current function for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : match = Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) if match : # Look for "override" after the matching closing parenthesis line , _ , closing_paren = CloseExpression ( clean_lines , i , len ( match . group ( 1 ) ) ) return ( closing_paren >= 0 and Search ( r'\boverride\b' , line [ closing_paren : ] ) ) return False
1872	def RDTSC ( cpu ) : val = cpu . icount cpu . RAX = val & 0xffffffff cpu . RDX = ( val >> 32 ) & 0xffffffff
5124	def show_type ( self , edge_type , * * kwargs ) : for v in self . g . nodes ( ) : e = ( v , v ) if self . g . is_edge ( e ) and self . g . ep ( e , 'edge_type' ) == edge_type : ei = self . g . edge_index [ e ] self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_highlight' ] ) self . g . set_vp ( v , 'vertex_color' , self . edge2queue [ ei ] . colors [ 'vertex_color' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) for e in self . g . edges ( ) : if self . g . ep ( e , 'edge_type' ) == edge_type : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , * * kwargs ) self . _update_all_colors ( )
5671	def plot_temporal_distance_cdf ( self ) : xvalues , cdf = self . profile_block_analyzer . _temporal_distance_cdf ( ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) xvalues = numpy . array ( xvalues ) / 60.0 ax . plot ( xvalues , cdf , "-k" ) ax . fill_between ( xvalues , cdf , color = "red" , alpha = 0.2 ) ax . set_ylabel ( "CDF(t)" ) ax . set_xlabel ( "Temporal distance t (min)" ) return fig
12130	def show ( self , exclude = [ ] ) : ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering if ( k in s ) and ( k not in exclude ) ] ) for s in self . specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) )
6976	def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr if kic_sdssgr < 0.8 : kepsdssr = ( keplermag - 0.2 * kic_sdssg ) / 0.8 else : kepsdssr = ( keplermag - 0.1 * kic_sdssg ) / 0.9 return kepsdssr
5162	def __intermediate_interface ( self , interface , uci_name ) : interface . update ( { '.type' : 'interface' , '.name' : uci_name , 'ifname' : interface . pop ( 'name' ) } ) if 'network' in interface : del interface [ 'network' ] if 'mac' in interface : # mac address of wireless interface must # be set in /etc/config/wireless, therfore # we can skip this in /etc/config/network if interface . get ( 'type' ) != 'wireless' : interface [ 'macaddr' ] = interface [ 'mac' ] del interface [ 'mac' ] if 'autostart' in interface : interface [ 'auto' ] = interface [ 'autostart' ] del interface [ 'autostart' ] if 'disabled' in interface : interface [ 'enabled' ] = not interface [ 'disabled' ] del interface [ 'disabled' ] if 'wireless' in interface : del interface [ 'wireless' ] if 'addresses' in interface : del interface [ 'addresses' ] return interface
7645	def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) # Make sure the discovered services are added to the list of known # services, and kick off characteristic discovery for each one. # NOTE: For some reason the services parameter is never set to a good # value, instead you must query peripheral.services() to enumerate the # discovered services. for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) # Kick off characteristic discovery for this service. Just discover # all characteristics for now. peripheral . discoverCharacteristics_forService_ ( None , service )
2084	def parse_args ( self , ctx , args ) : if not args and self . no_args_is_help and not ctx . resilient_parsing : click . echo ( ctx . get_help ( ) ) ctx . exit ( ) return super ( ActionSubcommand , self ) . parse_args ( ctx , args )
6130	def get ( self , * args , * * kwargs ) : try : req_func = self . session . get if self . session else requests . get req = req_func ( * args , * * kwargs ) req . raise_for_status ( ) self . failed_last = False return req except requests . exceptions . RequestException as e : self . log_error ( e ) for i in range ( 1 , self . num_retries ) : sleep_time = self . retry_rate * i self . log_function ( "Retrying in %s seconds" % sleep_time ) self . _sleep ( sleep_time ) try : req = requests . get ( * args , * * kwargs ) req . raise_for_status ( ) self . log_function ( "New request successful" ) return req except requests . exceptions . RequestException : self . log_function ( "New request failed" ) # Allows for the api to ignore one potentially bad request if not self . failed_last : self . failed_last = True raise ApiError ( e ) else : raise FatalApiError ( e )
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
6797	def get_media_timestamp ( self , last_timestamp = None ) : r = self . local_renderer _latest_timestamp = - 1e9999999999999999 for path in self . iter_static_paths ( ) : path = r . env . static_root + '/' + path self . vprint ( 'checking timestamp of path:' , path ) if not os . path . isfile ( path ) : continue #print('path:', path) _latest_timestamp = max ( _latest_timestamp , get_last_modified_timestamp ( path ) or _latest_timestamp ) if last_timestamp is not None and _latest_timestamp > last_timestamp : break self . vprint ( 'latest_timestamp:' , _latest_timestamp ) return _latest_timestamp
1137	def commonprefix ( m ) : if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1
3916	def _get_date_str ( timestamp , datetimefmt , show_date = False ) : fmt = '' if show_date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
10384	def remove_inconsistent_edges ( graph : BELGraph ) -> None : for u , v in get_inconsistent_edges ( graph ) : edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges )
867	def clear ( cls ) : # Clear the in-memory settings cache, forcing reload upon subsequent "get" # request. super ( Configuration , cls ) . clear ( ) # Reset in-memory custom configuration info. _CustomConfigurationFileWrapper . clear ( persistent = False )
3207	def delete ( self , batch_id ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _delete ( url = self . _build_path ( batch_id ) )
716	def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) jobID = None with open ( filePath , "r" ) as jobIdPickleFile : jobInfo = pickle . load ( jobIdPickleFile ) jobID = jobInfo [ "hyperSearchJobID" ] return jobID
7455	def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
6921	def _autocorr_func3 ( mags , lag , maglen , magmed , magstd ) : # from http://tinyurl.com/afz57c4 result = npcorrelate ( mags , mags , mode = 'full' ) result = result / npmax ( result ) return result [ int ( result . size / 2 ) : ]
10083	def edit ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) record_pid , record = self . fetch_published ( ) assert PIDStatus . REGISTERED == record_pid . status assert record [ '_deposit' ] == self [ '_deposit' ] self . model . json = self . _prepare_edit ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
3511	def sample ( model , n , method = "optgp" , thinning = 100 , processes = 1 , seed = None ) : if method == "optgp" : sampler = OptGPSampler ( model , processes , thinning = thinning , seed = seed ) elif method == "achr" : sampler = ACHRSampler ( model , thinning = thinning , seed = seed ) else : raise ValueError ( "method must be 'optgp' or 'achr'!" ) return pandas . DataFrame ( columns = [ rxn . id for rxn in model . reactions ] , data = sampler . sample ( n ) )
11072	def _to_primary_key ( self , value ) : if value is None : return None if isinstance ( value , self . base_class ) : if not value . _is_loaded : raise exceptions . DatabaseError ( 'Record must be loaded.' ) return value . _primary_key return self . base_class . _to_primary_key ( value )
3991	def _nginx_stream_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_proxy_string ( port_spec , bridge_ip ) ) server_string_spec += "\t }\n" return server_string_spec
13212	def build_jsonld ( self , url = None , code_url = None , ci_url = None , readme_url = None , license_id = None ) : jsonld = { '@context' : [ "https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/" "codemeta.jsonld" , "http://schema.org" ] , '@type' : [ 'Report' , 'SoftwareSourceCode' ] , 'language' : 'TeX' , 'reportNumber' : self . handle , 'name' : self . plain_title , 'description' : self . plain_abstract , 'author' : [ { '@type' : 'Person' , 'name' : author_name } for author_name in self . plain_authors ] , # This is a datetime.datetime; not a string. If writing to a file, # Need to convert this to a ISO 8601 string. 'dateModified' : self . revision_datetime } try : jsonld [ 'articleBody' ] = self . plain_content jsonld [ 'fileFormat' ] = 'text/plain' # MIME type of articleBody except RuntimeError : # raised by pypandoc when it can't convert the tex document self . _logger . exception ( 'Could not convert latex body to plain ' 'text for articleBody.' ) self . _logger . warning ( 'Falling back to tex source for articleBody' ) jsonld [ 'articleBody' ] = self . _tex jsonld [ 'fileFormat' ] = 'text/plain' # no mimetype for LaTeX? if url is not None : jsonld [ '@id' ] = url jsonld [ 'url' ] = url else : # Fallback to using the document handle as the ID. This isn't # entirely ideal from a linked data perspective. jsonld [ '@id' ] = self . handle if code_url is not None : jsonld [ 'codeRepository' ] = code_url if ci_url is not None : jsonld [ 'contIntegration' ] = ci_url if readme_url is not None : jsonld [ 'readme' ] = readme_url if license_id is not None : jsonld [ 'license_id' ] = None return jsonld
1212	def WorkerAgentGenerator ( agent_class ) : # Support special case where class is given as type-string (AgentsDictionary) or class-name-string. if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) # Last resort: Class name given as string? if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : """ Worker agent receiving a shared model to avoid creating multiple models. """ def __init__ ( self , model = None , * * kwargs ) : # Set our model externally. self . model = model # Be robust against `network` coming in from kwargs even though this agent doesn't have one if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) # Call super c'tor (which will call initialize_model and assign self.model to the return value). super ( WorkerAgent , self ) . __init__ ( * * kwargs ) def initialize_model ( self ) : # Return our model (already given and initialized). return self . model return WorkerAgent
8853	def on_open ( self ) : filename , filter = QtWidgets . QFileDialog . getOpenFileName ( self , 'Open' ) if filename : self . open_file ( filename ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True )
5103	def draw_graph ( self , line_kwargs = None , scatter_kwargs = None , * * kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "Matplotlib is required to draw the graph." ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_kwargs , scatter_kwargs = self . lines_scatter_args ( * * mpl_kwargs ) edge_collection = LineCollection ( * * line_kwargs ) ax . add_collection ( edge_collection ) ax . scatter ( * * scatter_kwargs ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) else : ax . set_axis_bgcolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) if 'fname' in kwargs : # savefig needs a positional argument for some reason new_kwargs = { k : v for k , v in kwargs . items ( ) if k in SAVEFIG_KWARGS } fig . savefig ( kwargs [ 'fname' ] , * * new_kwargs ) else : plt . ion ( ) plt . show ( )
8922	def localize_datetime ( dt , tz_name = 'UTC' ) : tz_aware_dt = dt if dt . tzinfo is None : utc = pytz . timezone ( 'UTC' ) aware = utc . localize ( dt ) timezone = pytz . timezone ( tz_name ) tz_aware_dt = aware . astimezone ( timezone ) else : logger . warn ( 'tzinfo already set' ) return tz_aware_dt
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
4986	def get_path_variables ( * * kwargs ) : enterprise_customer_uuid = kwargs . get ( 'enterprise_uuid' , '' ) course_run_id = kwargs . get ( 'course_id' , '' ) course_key = kwargs . get ( 'course_key' , '' ) program_uuid = kwargs . get ( 'program_uuid' , '' ) return enterprise_customer_uuid , course_run_id , course_key , program_uuid
1716	def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
12293	def annotate_metadata_action ( repo ) : package = repo . package print ( "Including history of actions" ) with cd ( repo . rootdir ) : filename = ".dgit/log.json" if os . path . exists ( filename ) : history = open ( filename ) . readlines ( ) actions = [ ] for a in history : try : a = json . loads ( a ) for x in [ 'code' ] : if x not in a or a [ x ] == None : a [ x ] = "..." actions . append ( a ) except : pass package [ 'actions' ] = actions
8575	def update_nic ( self , datacenter_id , server_id , nic_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
8122	def error ( message ) : global parser print ( _ ( "Error: " ) + message ) print ( ) parser . print_help ( ) sys . exit ( )
10250	def is_node_highlighted ( graph : BELGraph , node : BaseEntity ) -> bool : return NODE_HIGHLIGHT in graph . node [ node ]
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
13457	def upload_s3 ( file_path , bucket_name , file_key , force = False , acl = 'private' ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) if file_path . isdir ( ) : # Upload the contents of the dir path. paths = file_path . listdir ( ) paths_keys = list ( zip ( paths , [ '%s/%s' % ( file_key , p . name ) for p in paths ] ) ) else : # Upload just the given file path. paths_keys = [ ( file_path , file_key ) ] for p , k in paths_keys : headers = { } s3_key = bucket . get_key ( k ) if not s3_key : from boto . s3 . key import Key s3_key = Key ( bucket , k ) content_type = mimetypes . guess_type ( p ) [ 0 ] if content_type : headers [ 'Content-Type' ] = content_type file_size = p . stat ( ) . st_size file_data = p . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) # Check the hash. if s3_key . etag : s3_md5 = s3_key . etag . replace ( '"' , '' ) if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) continue elif not force : # Check if file on S3 is older than local file. s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( p . stat ( ) . st_mtime ) if local_datetime < s3_datetime : info ( "File %s hasn't been modified since last " "being uploaded" % ( file_key ) ) continue # File is newer, let's process and upload info ( "Uploading %s..." % ( file_key ) ) try : s3_key . set_contents_from_string ( file_data , headers , policy = acl , replace = True , md5 = ( file_md5 , file_md5_64 ) ) except Exception as e : error ( "Failed: %s" % e ) raise
2121	def disassociate_success_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'success' ) , parent , child )
7635	def import_lab ( namespace , filename , infer_duration = True , * * parse_options ) : # Create a new annotation object annotation = core . Annotation ( namespace ) parse_options . setdefault ( 'sep' , r'\s+' ) parse_options . setdefault ( 'engine' , 'python' ) parse_options . setdefault ( 'header' , None ) parse_options . setdefault ( 'index_col' , False ) # This is a hack to handle potentially ragged .lab data parse_options . setdefault ( 'names' , range ( 20 ) ) data = pd . read_csv ( filename , * * parse_options ) # Drop all-nan columns data = data . dropna ( how = 'all' , axis = 1 ) # Do we need to add a duration column? # This only applies to event annotations if len ( data . columns ) == 2 : # Insert a column of zeros after the timing data . insert ( 1 , 'duration' , 0 ) if infer_duration : data [ 'duration' ] [ : - 1 ] = data . loc [ : , 0 ] . diff ( ) [ 1 : ] . values else : # Convert from time to duration if infer_duration : data . loc [ : , 1 ] -= data [ 0 ] for row in data . itertuples ( ) : time , duration = row [ 1 : 3 ] value = [ x for x in row [ 3 : ] if x is not None ] [ - 1 ] annotation . append ( time = time , duration = duration , confidence = 1.0 , value = value ) return annotation
2556	def get ( self , tag = None , * * kwargs ) : # Stupid workaround since we can not use dom_tag in the method declaration if tag is None : tag = dom_tag attrs = [ ( dom_tag . clean_attribute ( attr ) , value ) for attr , value in kwargs . items ( ) ] results = [ ] for child in self . children : if ( isinstance ( tag , basestring ) and type ( child ) . __name__ == tag ) or ( not isinstance ( tag , basestring ) and isinstance ( child , tag ) ) : if all ( child . attributes . get ( attribute ) == value for attribute , value in attrs ) : # If the child is of correct type and has all attributes and values # in kwargs add as a result results . append ( child ) if isinstance ( child , dom_tag ) : # If the child is a dom_tag extend the search down through its children results . extend ( child . get ( tag , * * kwargs ) ) return results
8270	def _xml ( self ) : grouped = self . _weight_by_hue ( ) xml = "<colors query=\"" + self . name + "\" tags=\"" + ", " . join ( self . tags ) + "\">\n\n" for total_weight , normalized_weight , hue , ranges in grouped : if hue == self . blue : hue = "blue" clr = color ( hue ) xml += "\t<color name=\"" + clr . name + "\" weight=\"" + str ( normalized_weight ) + "\">\n " xml += "\t\t<rgb r=\"" + str ( clr . r ) + "\" g=\"" + str ( clr . g ) + "\" " xml += "b=\"" + str ( clr . b ) + "\" a=\"" + str ( clr . a ) + "\" />\n " for clr , rng , wgt in ranges : xml += "\t\t<shade name=\"" + str ( rng ) + "\" weight=\"" + str ( wgt / total_weight ) + "\" />\n " xml = xml . rstrip ( " " ) + "\t</color>\n\n" xml += "</colors>" return xml
11630	def __readNamelist ( cache , filename , unique_glyphs ) : if filename in cache : item = cache [ filename ] else : cps , header , noncodes = parseNamelist ( filename ) item = { "fileName" : filename , "ownCharset" : cps , "header" : header , "ownNoCharcode" : noncodes , "includes" : None # placeholder , "charset" : None # placeholder , "noCharcode" : None } cache [ filename ] = item if unique_glyphs or item [ "charset" ] is not None : return item # full-charset/includes are requested and not cached yet _loadNamelistIncludes ( item , unique_glyphs , cache ) return item
8678	def put ( self , name , value = None , modify = False , metadata = None , description = '' , encrypt = True , lock = False , key_type = 'secret' , add = False ) : def assert_key_is_unlocked ( existing_key ) : if existing_key and existing_key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be modified. ' 'Unlock the key and try again' . format ( name ) ) def assert_value_provided_for_new_key ( value , existing_key ) : if not value and not existing_key . get ( 'value' ) : raise GhostError ( 'You must provide a value for new keys' ) self . _assert_valid_stash ( ) self . _validate_key_schema ( value , key_type ) if value and encrypt and not isinstance ( value , dict ) : raise GhostError ( 'Value must be of type dict' ) # TODO: This should be refactored. `_handle_existing_key` deletes # the key rather implicitly. It shouldn't do that. # `existing_key` will be an empty dict if it doesn't exist key = self . _handle_existing_key ( name , modify or add ) assert_key_is_unlocked ( key ) assert_value_provided_for_new_key ( value , key ) new_key = dict ( name = name , lock = lock ) if value : # TODO: fix edge case in which encrypt is false and yet we might # try to add to an existing key. encrypt=false is only used when # `load`ing into a new stash, but someone might use it directly # from the API. if add : value = self . _update_existing_key ( key , value ) new_key [ 'value' ] = self . _encrypt ( value ) if encrypt else value else : new_key [ 'value' ] = key . get ( 'value' ) # TODO: Treat a case in which we try to update an existing key # but don't provide a value in which nothing will happen. new_key [ 'description' ] = description or key . get ( 'description' ) new_key [ 'created_at' ] = key . get ( 'created_at' ) or _get_current_time ( ) new_key [ 'modified_at' ] = _get_current_time ( ) new_key [ 'metadata' ] = metadata or key . get ( 'metadata' ) new_key [ 'uid' ] = key . get ( 'uid' ) or str ( uuid . uuid4 ( ) ) new_key [ 'type' ] = key . get ( 'type' ) or key_type key_id = self . _storage . put ( new_key ) audit ( storage = self . _storage . db_path , action = 'MODIFY' if ( modify or add ) else 'PUT' , message = json . dumps ( dict ( key_name = new_key [ 'name' ] , value = 'HIDDEN' , description = new_key [ 'description' ] , uid = new_key [ 'uid' ] , metadata = json . dumps ( new_key [ 'metadata' ] ) , lock = new_key [ 'lock' ] , type = new_key [ 'type' ] ) ) ) return key_id
7197	def plot ( self , spec = "rgb" , * * kwargs ) : if self . shape [ 0 ] == 1 or ( "bands" in kwargs and len ( kwargs [ "bands" ] ) == 1 ) : if "cmap" in kwargs : cmap = kwargs [ "cmap" ] del kwargs [ "cmap" ] else : cmap = "Greys_r" self . _plot ( tfm = self . _single_band , cmap = cmap , * * kwargs ) else : if spec == "rgb" and self . _has_token ( * * kwargs ) : self . _plot ( tfm = self . rgb , * * kwargs ) else : self . _plot ( tfm = getattr ( self , spec ) , * * kwargs )
3876	async def _get_or_fetch_conversation ( self , conv_id ) : conv = self . _conv_dict . get ( conv_id , None ) if conv is None : logger . info ( 'Fetching unknown conversation %s' , conv_id ) res = await self . _client . get_conversation ( hangouts_pb2 . GetConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_spec = hangouts_pb2 . ConversationSpec ( conversation_id = hangouts_pb2 . ConversationId ( id = conv_id ) ) , include_event = False ) ) conv_state = res . conversation_state event_cont_token = None if conv_state . HasField ( 'event_continuation_token' ) : event_cont_token = conv_state . event_continuation_token return self . _add_conversation ( conv_state . conversation , event_cont_token = event_cont_token ) else : return conv
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) # Try to set the random state from the last session to preserve # a single random stream when simulating timestamps multiple times if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
4925	def get_missing_params_message ( self , parameter_state ) : params = ', ' . join ( name for name , present in parameter_state if not present ) return self . MISSING_REQUIRED_PARAMS_MSG . format ( params )
1406	def validated_formatter ( self , url_format ) : # We try to create a string by substituting all known # parameters. If an unknown parameter is present, an error # will be thrown valid_parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy_formatted_url = url_format for key , value in valid_parameters . items ( ) : dummy_formatted_url = dummy_formatted_url . replace ( key , value ) # All $ signs must have been replaced if '$' in dummy_formatted_url : raise Exception ( "Invalid viz.url.format: %s" % ( url_format ) ) # No error is thrown, so the format is valid. return url_format
9094	def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url # Add this manager as an annotation, too self . _add_annotation_to_graph ( graph ) return namespace
3658	def _destroy_image_acquirer ( self , ia ) : id_ = None if ia . device : # ia . stop_image_acquisition ( ) # ia . _release_data_streams ( ) # id_ = ia . _device . id_ # if ia . device . node_map : # if ia . _chunk_adapter : ia . _chunk_adapter . detach_buffer ( ) ia . _chunk_adapter = None self . _logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id_ ) ) ia . device . node_map . disconnect ( ) self . _logger . info ( 'Disconnected the port from the NodeMap of {0}.' . format ( id_ ) ) # if ia . _device . is_open ( ) : ia . _device . close ( ) self . _logger . info ( 'Closed Device module, {0}.' . format ( id_ ) ) ia . _device = None # if id_ : self . _logger . info ( 'Destroyed the ImageAcquirer object which {0} ' 'had belonged to.' . format ( id_ ) ) else : self . _logger . info ( 'Destroyed an ImageAcquirer.' ) if self . _profiler : self . _profiler . print_diff ( ) self . _ias . remove ( ia )
411	def minibatches ( inputs = None , targets = None , batch_size = None , allow_dynamic_batch_size = False , shuffle = False ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( "The length of inputs and targets should be equal" ) if shuffle : indices = np . arange ( len ( inputs ) ) np . random . shuffle ( indices ) # for start_idx in range(0, len(inputs) - batch_size + 1, batch_size): # chulei: handling the case where the number of samples is not a multiple of batch_size, avoiding wasting samples for start_idx in range ( 0 , len ( inputs ) , batch_size ) : end_idx = start_idx + batch_size if end_idx > len ( inputs ) : if allow_dynamic_batch_size : end_idx = len ( inputs ) else : break if shuffle : excerpt = indices [ start_idx : end_idx ] else : excerpt = slice ( start_idx , end_idx ) if ( isinstance ( inputs , list ) or isinstance ( targets , list ) ) and ( shuffle == True ) : # zsdonghao: for list indexing when shuffle==True yield [ inputs [ i ] for i in excerpt ] , [ targets [ i ] for i in excerpt ] else : yield inputs [ excerpt ] , targets [ excerpt ]
5336	def __kibiter_version ( self ) : version = None es_url = self . conf [ 'es_enrichment' ] [ 'url' ] config_url = '.kibana/config/_search' url = urijoin ( es_url , config_url ) version = None try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) version = res . json ( ) [ 'hits' ] [ 'hits' ] [ 0 ] [ '_id' ] logger . debug ( "Kibiter version: %s" , version ) except requests . exceptions . HTTPError : logger . warning ( "Can not find Kibiter version" ) return version
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt # tl.logging.info(CNN.shape) # (5, 5, 3, 64) # exit() n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) # active mode fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col) # (5, 1, 32) 5 5 # exit() # plt.imshow( # np.reshape(CNN[count-1,:,:,:], (n_row, n_col)), # cmap='gray', interpolation="nearest") # theano if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) # distable tick plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
11975	def _add ( self , other ) : if isinstance ( other , self . __class__ ) : sum_ = self . _ip_dec + other . _ip_dec elif isinstance ( other , int ) : sum_ = self . _ip_dec + other else : other = self . __class__ ( other ) sum_ = self . _ip_dec + other . _ip_dec return sum_
4176	def window_gaussian ( N , alpha = 2.5 ) : t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) #t = linspace(-(N)/2., (N)/2., N) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
13361	def save ( self ) : env_data = [ dict ( name = env . name , root = env . path ) for env in self ] encode = yaml . safe_dump ( env_data , default_flow_style = False ) with open ( self . path , 'w' ) as f : f . write ( encode )
3764	def Parachor ( MW , rhol , rhog , sigma ) : rhol , rhog = rhol * 1000. , rhog * 1000. # Convert kg/m^3 to g/m^3 return sigma ** 0.25 * MW / ( rhol - rhog )
1333	def predictions_and_gradient ( self , image = None , label = None , strict = True , return_details = False ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 self . _total_gradient_calls += 1 predictions , gradient = self . __model . predictions_and_gradient ( image , label ) # noqa: E501 is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 assert gradient . shape == image . shape if return_details : return predictions , gradient , is_adversarial , is_best , distance else : return predictions , gradient , is_adversarial
8935	def provider ( workdir , commit = True , * * kwargs ) : return SCM_PROVIDER [ auto_detect ( workdir ) ] ( workdir , commit = commit , * * kwargs )
8777	def _chunks ( self , iterable , chunk_size ) : iterator = iter ( iterable ) chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) ) while chunk : yield chunk chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) )
4659	def as_quote ( self , quote ) : if quote == self [ "quote" ] [ "symbol" ] : return self . copy ( ) elif quote == self [ "base" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
5439	def _interval_to_seconds ( interval , valid_units = 'smhdw' ) : if not interval : return None try : last_char = interval [ - 1 ] if last_char == 's' and 's' in valid_units : return str ( float ( interval [ : - 1 ] ) ) + 's' elif last_char == 'm' and 'm' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 ) + 's' elif last_char == 'h' and 'h' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 ) + 's' elif last_char == 'd' and 'd' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 ) + 's' elif last_char == 'w' and 'w' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 * 7 ) + 's' else : raise ValueError ( 'Unsupported units in interval string %s: %s' % ( interval , last_char ) ) except ( ValueError , OverflowError ) as e : raise ValueError ( 'Unable to parse interval string %s: %s' % ( interval , e ) )
4833	def traverse_pagination ( response , endpoint , content_filter_query , query_params ) : results = response . get ( 'results' , [ ] ) page = 1 while response . get ( 'next' ) : page += 1 response = endpoint ( ) . post ( content_filter_query , * * dict ( query_params , page = page ) ) results += response . get ( 'results' , [ ] ) return results
4432	def unregister_hook ( self , func ) : if func in self . hooks : self . hooks . remove ( func )
5897	def get_meta_image_url ( request , image ) : rendition = image . get_rendition ( filter = 'original' ) return request . build_absolute_uri ( rendition . url )
11991	def decode_html_entities ( html ) : if not html : return html for entity , char in six . iteritems ( html_entity_map ) : html = html . replace ( entity , char ) return html
4622	def _new_masterpassword ( self , password ) : # make sure to not overwrite an existing key if self . config_key in self . config and self . config [ self . config_key ] : raise Exception ( "Storage already has a masterpassword!" ) self . decrypted_master = hexlify ( os . urandom ( 32 ) ) . decode ( "ascii" ) # Encrypt and save master self . password = password self . _save_encrypted_masterpassword ( ) return self . masterkey
3632	def baseId ( resource_id , return_version = False ) : version = 0 resource_id = resource_id + 0xC4000000 # 3288334336 # TODO: version is broken due ^^, needs refactoring while resource_id > 0x01000000 : # 16777216 version += 1 if version == 1 : resource_id -= 0x80000000 # 2147483648 # 0x50000000 # 1342177280 ? || 0x2000000 # 33554432 elif version == 2 : resource_id -= 0x03000000 # 50331648 else : resource_id -= 0x01000000 # 16777216 if return_version : return resource_id , version - 67 # just correct "magic number" return resource_id
427	def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : def get_ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash_ngram ( ngram ) : bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) return unigram_vocab_size + hash_ % n_buckets return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ]
8389	def write ( self , text , hashline = b"# {}" ) : if not text . endswith ( b"\n" ) : text += b"\n" actual_hash = hashlib . sha1 ( text ) . hexdigest ( ) with open ( self . filename , "wb" ) as f : f . write ( text ) f . write ( hashline . decode ( "utf8" ) . format ( actual_hash ) . encode ( "utf8" ) ) f . write ( b"\n" )
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : # If result[field] is not a string we can continue on if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
11714	def schedule ( self , variables = None , secure_variables = None , materials = None , return_new_instance = False , backoff_time = 1.0 ) : scheduling_args = dict ( variables = variables , secure_variables = secure_variables , material_fingerprint = materials , headers = { "Confirm" : True } , ) scheduling_args = dict ( ( k , v ) for k , v in scheduling_args . items ( ) if v is not None ) # TODO: Replace this with whatever is the official way as soon as gocd#990 is fixed. # https://github.com/gocd/gocd/issues/990 if return_new_instance : pipelines = self . history ( ) [ 'pipelines' ] if len ( pipelines ) == 0 : last_run = None else : last_run = pipelines [ 0 ] [ 'counter' ] response = self . _post ( '/schedule' , ok_status = 202 , * * scheduling_args ) if not response : return response max_tries = 10 while max_tries > 0 : current = self . instance ( ) if not last_run and current : return current elif last_run and current [ 'counter' ] > last_run : return current else : time . sleep ( backoff_time ) max_tries -= 1 # I can't come up with a scenario in testing where this would happen, but it seems # better than returning None. return response else : return self . _post ( '/schedule' , ok_status = 202 , * * scheduling_args )
6594	def run_multiple ( self , eventLoops ) : self . nruns += len ( eventLoops ) return self . communicationChannel . put_multiple ( eventLoops )
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] # tl.logging.info("new %f" % probs) probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
13163	def serialize_text ( out , text ) : padding = len ( out ) # we need to add padding to all lines # except the first one add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
8854	def on_save_as ( self ) : path = self . tabWidget . current_widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = QtWidgets . QFileDialog . getSaveFileName ( self , 'Save' , path ) if filename : self . tabWidget . save_current ( filename ) self . recent_files_manager . open_file ( filename ) self . menu_recents . update_actions ( ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True ) self . _update_status_bar ( self . tabWidget . current_widget ( ) )
6679	def getmtime ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) ) : return int ( func ( 'stat -c %%Y "%(path)s" ' % locals ( ) ) . strip ( ) )
9302	def regenerate_signing_key ( self , secret_key = None , region = None , service = None , date = None ) : if secret_key is None and ( self . signing_key is None or self . signing_key . secret_key is None ) : raise NoSecretKeyError secret_key = secret_key or self . signing_key . secret_key region = region or self . region service = service or self . service date = date or self . date if self . signing_key is None : store_secret_key = True else : store_secret_key = self . signing_key . store_secret_key self . signing_key = AWS4SigningKey ( secret_key , region , service , date , store_secret_key ) self . region = region self . service = service self . date = self . signing_key . date
11047	def _parse_field_value ( line ) : if line . startswith ( ':' ) : # Ignore the line return None , None if ':' not in line : # Treat the entire line as the field, use empty string as value return line , '' # Else field is before the ':' and value is after field , value = line . split ( ':' , 1 ) # If value starts with a space, remove it. value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
9649	def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : # Give preference to the environment variable here as it will not # derefrence sym links self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) # Try and work out the project name distribution = self . get_distribution ( ) if distribution : # Get name from setup.py self . project_name = distribution . get_name ( ) else : # ...failing that, use the current directory name self . project_name = self . project_dir . name # Descend into the 'src' directory to find the package # if necessary if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : # Lets try and work out the package_name from the project_name package_name = self . project_name . replace ( "-" , "_" ) # Now do some fuzzy matching def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) # If no matches, try removing the first part of the package name # (e.g. django-guardian becomes guardian) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name # Gets set to true even during dry run created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
3848	def _get_authorization_headers ( sapisid_cookie ) : # It doesn't seem to matter what the url and time are as long as they are # consistent. time_msec = int ( time . time ( ) * 1000 ) auth_string = '{} {} {}' . format ( time_msec , sapisid_cookie , ORIGIN_URL ) auth_hash = hashlib . sha1 ( auth_string . encode ( ) ) . hexdigest ( ) sapisidhash = 'SAPISIDHASH {}_{}' . format ( time_msec , auth_hash ) return { 'authorization' : sapisidhash , 'x-origin' : ORIGIN_URL , 'x-goog-authuser' : '0' , }
8885	def fit ( self , x , y = None ) : if self . _dtype is not None : iter2array ( x , dtype = self . _dtype ) else : iter2array ( x ) return self
5158	def _add_uninstall ( self , context ) : contents = self . _render_template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add uninstall.sh to list of included files self . _add_unique_file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
11639	def json_write_data ( json_data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json_data , fp , indent = 4 , sort_keys = True , ensure_ascii = False ) return True return False
2049	def create_contract ( self , price = 0 , address = None , caller = None , balance = 0 , init = None , gas = None ) : expected_address = self . create_account ( self . new_address ( sender = caller ) ) if address is None : address = expected_address elif caller is not None and address != expected_address : raise EthereumError ( f"Error: contract created from address {hex(caller)} with nonce {self.get_nonce(caller)} was expected to be at address {hex(expected_address)}, but create_contract was called with address={hex(address)}" ) self . start_transaction ( 'CREATE' , address , price , init , caller , balance , gas = gas ) self . _process_pending_transaction ( ) return address
6085	def unmasked_blurred_image_of_planes_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : unmasked_blurred_image_of_planes = [ ] for plane in planes : if plane . has_pixelization : unmasked_blurred_image_of_plane = None else : unmasked_blurred_image_of_plane = padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf = psf , unmasked_image_1d = plane . image_plane_image_1d ) unmasked_blurred_image_of_planes . append ( unmasked_blurred_image_of_plane ) return unmasked_blurred_image_of_planes
11291	def json ( request , * args , * * kwargs ) : # coerce to dictionary params = dict ( request . GET . items ( ) ) callback = params . pop ( 'callback' , None ) url = params . pop ( 'url' , None ) if not url : return HttpResponseBadRequest ( 'Required parameter missing: URL' ) try : provider = oembed . site . provider_for_url ( url ) if not provider . provides : raise OEmbedMissingEndpoint ( ) except OEmbedMissingEndpoint : raise Http404 ( 'No provider found for %s' % url ) query = dict ( [ ( smart_str ( k ) , smart_str ( v ) ) for k , v in params . items ( ) if v ] ) try : resource = oembed . site . embed ( url , * * query ) except OEmbedException , e : raise Http404 ( 'Error embedding %s: %s' % ( url , str ( e ) ) ) response = HttpResponse ( mimetype = 'application/json' ) json = resource . json if callback : response . write ( '%s(%s)' % ( defaultfilters . force_escape ( callback ) , json ) ) else : response . write ( json ) return response
2607	def update_memo ( self , task_id , task , r ) : if not self . memoize or not task [ 'memoize' ] : return if task [ 'hashsum' ] in self . memo_lookup_table : logger . info ( 'Updating appCache entry with latest %s:%s call' % ( task [ 'func_name' ] , task_id ) ) self . memo_lookup_table [ task [ 'hashsum' ] ] = r else : self . memo_lookup_table [ task [ 'hashsum' ] ] = r
2438	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True if validations . validate_review_comment ( comment ) : doc . reviews [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ReviewComment::Comment' ) else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
1564	def get_metrics_collector ( self ) : if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : raise RuntimeError ( "Metrics collector is not registered in this context" ) return self . metrics_collector
8151	def _addvar ( self , v ) : oldvar = self . _oldvars . get ( v . name ) if oldvar is not None : if isinstance ( oldvar , Variable ) : if oldvar . compliesTo ( v ) : v . value = oldvar . value else : # Set from commandline v . value = v . sanitize ( oldvar ) else : for listener in VarListener . listeners : listener . var_added ( v ) self . _vars [ v . name ] = v self . _namespace [ v . name ] = v . value self . _oldvars [ v . name ] = v return v
433	def draw_boxes_and_labels_to_image ( image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None ) : if len ( coords ) != len ( classes ) : raise AssertionError ( "number of coordinates and classes are equal" ) if len ( scores ) > 0 and len ( scores ) != len ( classes ) : raise AssertionError ( "number of scores and classes are equal" ) # don't change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) for i , _v in enumerate ( coords ) : if is_center : x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) else : x , y , x2 , y2 = coords [ i ] if is_rescale : # scale back to pixel unit if the coords are the portion of width and high x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) cv2 . rectangle ( image , ( int ( x ) , int ( y ) ) , ( int ( x2 ) , int ( y2 ) ) , # up-left and botton-right [ 0 , 255 , 0 ] , thick ) cv2 . putText ( image , classes_list [ classes [ i ] ] + ( ( " %.2f" % ( scores [ i ] ) ) if ( len ( scores ) != 0 ) else " " ) , ( int ( x ) , int ( y ) ) , # button left 0 , 1.5e-3 * imh , # bigger = larger font [ 0 , 0 , 256 ] , # self.meta['colors'][max_indx], int ( thick / 2 ) + 1 ) # bold if save_name is not None : # cv2.imwrite('_my.png', image) save_image ( image , save_name ) # if len(coords) == 0: # tl.logging.info("draw_boxes_and_labels_to_image: no bboxes exist, cannot draw !") return image
9399	def restart ( self ) : if self . _engine : self . _engine . repl . terminate ( ) executable = self . _executable if executable : os . environ [ 'OCTAVE_EXECUTABLE' ] = executable if 'OCTAVE_EXECUTABLE' not in os . environ and 'OCTAVE' in os . environ : os . environ [ 'OCTAVE_EXECUTABLE' ] = os . environ [ 'OCTAVE' ] self . _engine = OctaveEngine ( stdin_handler = self . _handle_stdin , logger = self . logger ) # Add local Octave scripts. self . _engine . eval ( 'addpath("%s");' % HERE . replace ( osp . sep , '/' ) )
10268	def main ( output ) : from hbp_knowledge import get_graph graph = get_graph ( ) text = to_html ( graph ) print ( text , file = output )
12739	def create_joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child_bone = self . bones [ child ] child_body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child_bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child_body ) )
11260	def match ( prev , pattern , * args , * * kw ) : to = 'to' in kw and kw . pop ( 'to' ) pattern_obj = re . compile ( pattern , * args , * * kw ) if to is dict : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groupdict ( ) elif to is tuple : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groups ( ) elif to is list : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield list ( match . groups ( ) ) else : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match
13454	def _usage ( prog_name = os . path . basename ( sys . argv [ 0 ] ) ) : spacer = ' ' * len ( 'usage: ' ) usage = prog_name + ' -b LIST [-S SEPARATOR] [file ...]\n' + spacer + prog_name + ' -c LIST [-S SEPERATOR] [file ...]\n' + spacer + prog_name + ' -f LIST [-d DELIM] [-e] [-S SEPERATOR] [-s] [file ...]' # Return usage message with trailing whitespace removed. return "usage: " + usage . rstrip ( )
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
11897	def _create_index_files ( root_dir , force_no_processing = False ) : # Initialise list of created file paths to build up as we make them created_files = [ ] # Walk the root dir downwards, creating index files as we go for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) # Sort the subdirectories by name dirs = sorted ( dirs ) # Get image files - all files in the directory matching IMAGE_FILE_REGEX image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] # Sort the image files by name image_files = sorted ( image_files ) # Create this directory's index file and add its name to the created # files list created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) # Return the list of created files return created_files
3300	def make_sub_element ( parent , tag , nsmap = None ) : if use_lxml : return etree . SubElement ( parent , tag , nsmap = nsmap ) return etree . SubElement ( parent , tag )
13053	def nmap_scan ( ) : # Create the search and config objects hs = HostSearch ( ) config = Config ( ) # Static options to be able to figure out what options to use depending on the input the user gives. nmap_types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } # Create an argument parser hs_parser = hs . argparser argparser = argparse . ArgumentParser ( parents = [ hs_parser ] , conflict_handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add_argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap_types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra_nmap_args = argparser . parse_known_args ( ) # Fix the tags for the search tags = nmap_types [ nmap_types . index ( arguments . type ) : ] tags = [ "!nmap_" + tag for tag in tags ] hosts = hs . get_hosts ( tags = tags ) hosts = [ host for host in hosts ] # Create the nmap arguments nmap_args = [ ] nmap_args . extend ( extra_nmap_args ) nmap_args . extend ( options [ arguments . type ] . split ( ' ' ) ) # Run nmap print_notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap_args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap_args , [ str ( h . address ) for h in hosts ] ) # Import the nmap result for host in hosts : host . add_tag ( "nmap_{}" . format ( arguments . type ) ) host . save ( ) print_notification ( "Nmap done, importing results" ) stats = import_nmap ( result , "nmap_{}" . format ( arguments . type ) , check_function = all_hosts , import_services = True ) stats [ 'scanned_hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap_scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print_notification ( "No hosts found" )
1274	def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
3304	def _run_paste ( app , config , mode ) : from paste import httpserver version = "WsgiDAV/{} {} Python {}" . format ( __version__ , httpserver . WSGIHandler . server_version , util . PYTHON_VERSION ) _logger . info ( "Running {}..." . format ( version ) ) # See http://pythonpaste.org/modules/httpserver.html for more options server = httpserver . serve ( app , host = config [ "host" ] , port = config [ "port" ] , server_version = version , # This option enables handling of keep-alive # and expect-100: protocol_version = "HTTP/1.1" , start_loop = False , ) if config [ "verbose" ] >= 5 : __handle_one_request = server . RequestHandlerClass . handle_one_request def handle_one_request ( self ) : __handle_one_request ( self ) if self . close_connection == 1 : _logger . debug ( "HTTP Connection : close" ) else : _logger . debug ( "HTTP Connection : continue" ) server . RequestHandlerClass . handle_one_request = handle_one_request # __handle = server.RequestHandlerClass.handle # def handle(self): # _logger.debug("open HTTP connection") # __handle(self) server . RequestHandlerClass . handle_one_request = handle_one_request host , port = server . server_address if host == "0.0.0.0" : _logger . info ( "Serving on 0.0.0.0:{} view at {}://127.0.0.1:{}" . format ( port , "http" , port ) ) else : _logger . info ( "Serving on {}://{}:{}" . format ( "http" , host , port ) ) try : server . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
10126	def flip ( self , angle , center = None ) : return self . rotate ( - angle , center = center ) . flip_y ( center = center ) . rotate ( angle , center = center )
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
5590	def tile ( self , zoom , row , col ) : tile = self . tile_pyramid . tile ( zoom , row , col ) return BufferedTile ( tile , pixelbuffer = self . pixelbuffer )
1093	def split ( pattern , string , maxsplit = 0 , flags = 0 ) : return _compile ( pattern , flags ) . split ( string , maxsplit )
9437	def strip_ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload
12314	def _run ( self , cmd ) : # This is here in case the .gitconfig is not accessible for # some reason. environ = os . environ . copy ( ) environ [ 'GIT_COMMITTER_NAME' ] = self . fullname environ [ 'GIT_COMMITTER_EMAIL' ] = self . email environ [ 'GIT_AUTHOR_NAME' ] = self . fullname environ [ 'GIT_AUTHOR_EMAIL' ] = self . email cmd = [ pipes . quote ( c ) for c in cmd ] cmd = " " . join ( [ '/usr/bin/git' ] + cmd ) cmd += "; exit 0" #print("Running cmd", cmd) try : output = subprocess . check_output ( cmd , stderr = subprocess . STDOUT , shell = True , env = environ ) except subprocess . CalledProcessError as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) # print("Output of command", output) return output
1201	def reset ( self ) : self . level . reset ( ) # optional: episode=-1, seed=None return self . level . observations ( ) [ self . state_attribute ]
3165	def get ( self , workflow_id , email_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = subscriber_hash return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' , subscriber_hash ) )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
1755	def read_register ( self , register ) : self . _publish ( 'will_read_register' , register ) value = self . _regfile . read ( register ) self . _publish ( 'did_read_register' , register , value ) return value
9531	def value_to_string ( self , obj ) : value = self . value_from_object ( obj ) return b64encode ( self . _dump ( value ) ) . decode ( 'ascii' )
841	def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : if cat is not None : assert idx is None idx = self . _categoryList . index ( cat ) if not self . useSparseMemory : pattern = self . _Memory [ idx ] if sparseBinaryForm : pattern = pattern . nonzero ( ) [ 0 ] else : ( nz , values ) = self . _Memory . rowNonZeros ( idx ) if not sparseBinaryForm : pattern = numpy . zeros ( self . _Memory . nCols ( ) ) numpy . put ( pattern , nz , 1 ) else : pattern = nz return pattern
11166	def unusedoptions ( self , sections ) : unused = set ( [ ] ) for section in _list ( sections ) : if not self . has_section ( section ) : continue options = self . options ( section ) raw_values = [ self . get ( section , option , raw = True ) for option in options ] for option in options : formatter = "%(" + option + ")s" for raw_value in raw_values : if formatter in raw_value : break else : unused . add ( option ) return list ( unused )
3771	def mixing_logarithmic ( fracs , props ) : if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
7122	def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash_func . update ( token ) return hash_func . hexdigest ( )
8826	def populate_subtasks ( self , context , sg , parent_job_id ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None ports = db_api . sg_gather_associated_ports ( context , db_sg ) if len ( ports ) == 0 : return { "ports" : 0 } for port in ports : job_body = dict ( action = "update port %s" % port [ 'id' ] , tenant_id = db_sg [ 'tenant_id' ] , resource_id = port [ 'id' ] , parent_id = parent_job_id ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_consumer = QuarkSGAsyncConsumerClient ( ) try : rpc_consumer . update_port ( context , port [ 'id' ] , job [ 'id' ] ) except om_exc . MessagingTimeout : # TODO(roaet): Not too sure what can be done here other than # updating the job as a failure? LOG . error ( "Failed to update port. Rabbit running?" ) return None
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
1667	def IsBlockInNameSpace ( nesting_state , is_forward_declaration ) : if is_forward_declaration : return len ( nesting_state . stack ) >= 1 and ( isinstance ( nesting_state . stack [ - 1 ] , _NamespaceInfo ) ) return ( len ( nesting_state . stack ) > 1 and nesting_state . stack [ - 1 ] . check_namespace_indentation and isinstance ( nesting_state . stack [ - 2 ] , _NamespaceInfo ) )
2581	def _load_checkpoints ( self , checkpointDirs ) : memo_lookup_table = { } for checkpoint_dir in checkpointDirs : logger . info ( "Loading checkpoints from {}" . format ( checkpoint_dir ) ) checkpoint_file = os . path . join ( checkpoint_dir , 'tasks.pkl' ) try : with open ( checkpoint_file , 'rb' ) as f : while True : try : data = pickle . load ( f ) # Copy and hash only the input attributes memo_fu = Future ( ) if data [ 'exception' ] : memo_fu . set_exception ( data [ 'exception' ] ) else : memo_fu . set_result ( data [ 'result' ] ) memo_lookup_table [ data [ 'hash' ] ] = memo_fu except EOFError : # Done with the checkpoint file break except FileNotFoundError : reason = "Checkpoint file was not found: {}" . format ( checkpoint_file ) logger . error ( reason ) raise BadCheckpoint ( reason ) except Exception : reason = "Failed to load checkpoint: {}" . format ( checkpoint_file ) logger . error ( reason ) raise BadCheckpoint ( reason ) logger . info ( "Completed loading checkpoint:{0} with {1} tasks" . format ( checkpoint_file , len ( memo_lookup_table . keys ( ) ) ) ) return memo_lookup_table
3935	def get ( self ) : logger . info ( 'Loading refresh_token from %s' , repr ( self . _filename ) ) try : with open ( self . _filename ) as f : return f . read ( ) except IOError as e : logger . info ( 'Failed to load refresh_token: %s' , e )
10282	def get_peripheral_predecessor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for v in subgraph : for u , _ , k in graph . in_edges ( v , keys = True ) : if u not in subgraph : yield u , v , k
3288	def _get_repo_info ( self , environ , rev , reload = False ) : caches = environ . setdefault ( "wsgidav.hg.cache" , { } ) if caches . get ( compat . to_native ( rev ) ) is not None : _logger . debug ( "_get_repo_info(%s): cache hit." % rev ) return caches [ compat . to_native ( rev ) ] start_time = time . time ( ) self . ui . pushbuffer ( ) commands . manifest ( self . ui , self . repo , rev ) res = self . ui . popbuffer ( ) files = [ ] dirinfos = { } filedict = { } for file in res . split ( "\n" ) : if file . strip ( ) == "" : continue file = file . replace ( "\\" , "/" ) # add all parent directories to 'dirinfos' parents = file . split ( "/" ) if len ( parents ) >= 1 : p1 = "" for i in range ( 0 , len ( parents ) - 1 ) : p2 = parents [ i ] dir = dirinfos . setdefault ( p1 , ( [ ] , [ ] ) ) if p2 not in dir [ 0 ] : dir [ 0 ] . append ( p2 ) if p1 == "" : p1 = p2 else : p1 = "%s/%s" % ( p1 , p2 ) dirinfos . setdefault ( p1 , ( [ ] , [ ] ) ) [ 1 ] . append ( parents [ - 1 ] ) filedict [ file ] = True files . sort ( ) cache = { "files" : files , "dirinfos" : dirinfos , "filedict" : filedict } caches [ compat . to_native ( rev ) ] = cache _logger . info ( "_getRepoInfo(%s) took %.3f" % ( rev , time . time ( ) - start_time ) ) return cache
11108	def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
9331	def cpu_count ( ) : num = os . getenv ( "OMP_NUM_THREADS" ) if num is None : num = os . getenv ( "PBS_NUM_PPN" ) try : return int ( num ) except : return multiprocessing . cpu_count ( )
6767	def address ( interface ) : with settings ( hide ( 'running' , 'stdout' ) ) : res = ( sudo ( "/sbin/ifconfig %(interface)s | grep 'inet '" % locals ( ) ) or '' ) . split ( '\n' ) [ - 1 ] . strip ( ) if 'addr' in res : return res . split ( ) [ 1 ] . split ( ':' ) [ 1 ] return res . split ( ) [ 1 ]
12797	def build_twisted_request ( self , method , url , extra_headers = { } , body_producer = None , full_url = False ) : uri = url if full_url else self . _url ( url ) raw_headers = self . get_headers ( ) if extra_headers : raw_headers . update ( extra_headers ) headers = http_headers . Headers ( ) for header in raw_headers : headers . addRawHeader ( header , raw_headers [ header ] ) agent = client . Agent ( reactor ) request = agent . request ( method , uri , headers , body_producer ) return ( reactor , request )
3019	def from_json ( cls , json_data ) : if not isinstance ( json_data , dict ) : json_data = json . loads ( _helpers . _from_bytes ( json_data ) ) private_key_pkcs8_pem = None pkcs12_val = json_data . get ( _PKCS12_KEY ) password = None if pkcs12_val is None : private_key_pkcs8_pem = json_data [ '_private_key_pkcs8_pem' ] signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) else : # NOTE: This assumes that private_key_pkcs8_pem is not also # in the serialized data. This would be very incorrect # state. pkcs12_val = base64 . b64decode ( pkcs12_val ) password = json_data [ '_private_key_password' ] signer = crypt . Signer . from_string ( pkcs12_val , password ) credentials = cls ( json_data [ '_service_account_email' ] , signer , scopes = json_data [ '_scopes' ] , private_key_id = json_data [ '_private_key_id' ] , client_id = json_data [ 'client_id' ] , user_agent = json_data [ '_user_agent' ] , * * json_data [ '_kwargs' ] ) if private_key_pkcs8_pem is not None : credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem if pkcs12_val is not None : credentials . _private_key_pkcs12 = pkcs12_val if password is not None : credentials . _private_key_password = password credentials . invalid = json_data [ 'invalid' ] credentials . access_token = json_data [ 'access_token' ] credentials . token_uri = json_data [ 'token_uri' ] credentials . revoke_uri = json_data [ 'revoke_uri' ] token_expiry = json_data . get ( 'token_expiry' , None ) if token_expiry is not None : credentials . token_expiry = datetime . datetime . strptime ( token_expiry , client . EXPIRY_FORMAT ) return credentials
2833	def stop ( self , pin ) : if pin not in self . pwm : raise ValueError ( 'Pin {0} is not configured as a PWM. Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]
9862	async def update_info ( self , * _ ) : query = gql ( """ { viewer { name homes { subscriptions { status } id } } } """ ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
3892	def run_example ( example_coroutine , * extra_args ) : args = _get_parser ( extra_args ) . parse_args ( ) logging . basicConfig ( level = logging . DEBUG if args . debug else logging . WARNING ) # Obtain hangups authentication cookies, prompting for credentials from # standard input if necessary. cookies = hangups . auth . get_auth_stdin ( args . token_path ) client = hangups . Client ( cookies ) loop = asyncio . get_event_loop ( ) task = asyncio . ensure_future ( _async_main ( example_coroutine , client , args ) , loop = loop ) try : loop . run_until_complete ( task ) except KeyboardInterrupt : task . cancel ( ) loop . run_until_complete ( task ) finally : loop . close ( )
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
2896	def is_completed ( self ) : mask = Task . NOT_FINISHED_MASK iter = Task . Iterator ( self . task_tree , mask ) try : next ( iter ) except StopIteration : # No waiting tasks found. return True return False
2892	def connect_outgoing ( self , taskspec , sequence_flow_id , sequence_flow_name , documentation ) : self . connect ( taskspec ) s = SequenceFlow ( sequence_flow_id , sequence_flow_name , documentation , taskspec ) self . outgoing_sequence_flows [ taskspec . name ] = s self . outgoing_sequence_flows_by_id [ sequence_flow_id ] = s
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) # now that we have all the files, concatenate them # a single file will be returned as normalized if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
861	def getMaxDelay ( inferences ) : maxDelay = 0 for inferenceElement , inference in inferences . iteritems ( ) : if isinstance ( inference , dict ) : for key in inference . iterkeys ( ) : maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement , key ) , maxDelay ) else : maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement ) , maxDelay ) return maxDelay
4665	def id ( self ) : # Store signatures temporarily since they are not part of # transaction id sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) # Generage Hash of the seriliazed version h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) # recover signatures self . data [ "signatures" ] = sigs # Return properly truncated tx hash return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
8865	def complete ( code , line , column , path , encoding , prefix ) : ret_val = [ ] try : script = jedi . Script ( code , line + 1 , column , path , encoding ) completions = script . completions ( ) print ( 'completions: %r' % completions ) except jedi . NotFoundError : completions = [ ] for completion in completions : ret_val . append ( { 'name' : completion . name , 'icon' : icon_from_typename ( completion . name , completion . type ) , 'tooltip' : completion . description } ) return ret_val
13103	def create_scan ( self , host_ips ) : now = datetime . datetime . now ( ) data = { "uuid" : self . get_template_uuid ( ) , "settings" : { "name" : "jackal-" + now . strftime ( "%Y-%m-%d %H:%M" ) , "text_targets" : host_ips } } response = requests . post ( self . url + 'scans' , data = json . dumps ( data ) , verify = False , headers = self . headers ) if response : result = json . loads ( response . text ) return result [ 'scan' ] [ 'id' ]
10039	def pick_coda_from_decimal ( decimal ) : decimal = Decimal ( decimal ) __ , digits , exp = decimal . as_tuple ( ) if exp < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] __ , digits , exp = decimal . normalize ( ) . as_tuple ( ) index = bisect_right ( EXP_INDICES , exp ) - 1 if index < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] else : return EXP_CODAS [ EXP_INDICES [ index ] ]
12284	def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
8899	def _dequeue_into_store ( transfersession ) : with connection . cursor ( ) as cursor : DBBackend . _dequeuing_delete_rmcb_records ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_buffered_records ( cursor , transfersession . id ) current_id = InstanceIDModel . get_current_instance_and_increment_counter ( ) DBBackend . _dequeuing_merge_conflict_buffer ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_merge_conflict_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_update_rmcs_last_saved_by ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_delete_mc_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_mc_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_buffer ( cursor , transfersession . id ) if getattr ( settings , 'MORANGO_DESERIALIZE_AFTER_DEQUEUING' , True ) : _deserialize_from_store ( transfersession . sync_session . profile )
11684	def _readxml ( self ) : block = re . sub ( r'<(/?)s>' , r'&lt;\1s&gt;' , self . _readblock ( ) ) try : xml = XML ( block ) except ParseError : xml = None return xml
11431	def _compare_fields ( field1 , field2 , strict = True ) : if strict : # Return a simple equal test on the field minus the position. return field1 [ : 4 ] == field2 [ : 4 ] else : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] : # Different indicators or controlfield value. return False else : # Compare subfields in a loose way. return set ( field1 [ 0 ] ) == set ( field2 [ 0 ] )
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) # See first of this XML is clean or OAI request if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : # We have an OAI request header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : # It was OAI deleted. Create special record self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
7280	def quit ( self ) : if self . _process is None : logger . debug ( 'Quit was called after self._process had already been released' ) return try : logger . debug ( 'Quitting OMXPlayer' ) process_group_id = os . getpgid ( self . _process . pid ) os . killpg ( process_group_id , signal . SIGTERM ) logger . debug ( 'SIGTERM Sent to pid: %s' % process_group_id ) self . _process_monitor . join ( ) except OSError : logger . error ( 'Could not find the process to kill' ) self . _process = None
9789	def projects ( ctx , page ) : user = get_username_or_local ( ctx . obj . get ( 'username' ) ) page = page or 1 try : response = PolyaxonClient ( ) . bookmark . projects ( username = user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get bookmarked projects for user `{}`.' . format ( user ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Bookmarked projects for user `{}`.' . format ( user ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No bookmarked projects found for user `{}`.' . format ( user ) ) objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
4646	def exists ( self ) : query = ( "SELECT name FROM sqlite_master " + "WHERE type='table' AND name=?" , ( self . __tablename__ , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } # Iterate over the returned keys from etcd update = { } for item in result . children : key = item . key value = item . value # Try to parse them as JSON strings, just in case it works try : value = pytool . json . from_json ( value ) except : pass # Make the key lower-case if we're not case-sensitive if not self . case_sensitive : key = key . lower ( ) # Strip off the prefix that we're using if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] # Store the key/value to update the config update [ key ] = value # Access cached settings directly to avoid recursion inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
10738	def grid_evaluation ( X , Y , f , vectorized = True ) : XX = np . reshape ( np . concatenate ( [ X [ ... , None ] , Y [ ... , None ] ] , axis = 2 ) , ( X . size , 2 ) , order = 'C' ) if vectorized : ZZ = f ( XX ) else : ZZ = np . array ( [ f ( x ) for x in XX ] ) return np . reshape ( ZZ , X . shape , order = 'C' )
7194	def histogram_stretch ( self , use_bands , * * kwargs ) : data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . _histogram_stretch ( data , * * kwargs )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
1414	def get_pplan ( self , topologyName , callback = None ) : isWatching = False # Temp dict used to return result # if callback is not provided. ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : """ Custom callback to get the topologies right now. """ ret [ "result" ] = data self . _get_pplan_with_watch ( topologyName , callback , isWatching ) # The topologies are now populated with the data. return ret [ "result" ]
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
6285	def toggle_pause ( self ) : self . controller . playing = not self . controller . playing self . music . toggle_pause ( )
9878	def _coincidences ( value_counts , value_domain , dtype = np . float64 ) : value_counts_matrices = value_counts . reshape ( value_counts . shape + ( 1 , ) ) pairable = np . maximum ( np . sum ( value_counts , axis = 1 ) , 2 ) diagonals = np . tile ( np . eye ( len ( value_domain ) ) , ( len ( value_counts ) , 1 , 1 ) ) * value_counts . reshape ( ( value_counts . shape [ 0 ] , 1 , value_counts . shape [ 1 ] ) ) unnormalized_coincidences = value_counts_matrices * value_counts_matrices . transpose ( ( 0 , 2 , 1 ) ) - diagonals return np . sum ( np . divide ( unnormalized_coincidences , ( pairable - 1 ) . reshape ( ( - 1 , 1 , 1 ) ) , dtype = dtype ) , axis = 0 )
13900	def db_to_specifier ( db_string ) : local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) # If this looks like a local specifier: if local_match : return 'local:' + local_match . groupdict ( ) [ 'database' ] # If this looks like a remote specifier: elif remote_match : # Just a fancy way of getting 3 variables in 2 lines... hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) local_url = settings . _ ( 'COUCHDB_SERVER' , 'http://127.0.0.1:5984/' ) localhost , localport = urlparse . urlparse ( local_url ) [ 1 ] . split ( ':' ) # If it's the local server, then return a local specifier. if ( localhost == hostname ) and ( localport == portnum ) : return 'local:' + database # Otherwise, prepare and return the remote specifier. return 'remote:%s:%s:%s' % ( hostname , portnum , database ) # Throw a wobbly. raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
11435	def _validate_record_field_positions_global ( record ) : all_fields = [ ] for tag , fields in record . items ( ) : previous_field_position_global = - 1 for field in fields : if field [ 4 ] < previous_field_position_global : return ( "Non ascending global field positions in tag '%s'." % tag ) previous_field_position_global = field [ 4 ] if field [ 4 ] in all_fields : return ( "Duplicate global field position '%d' in tag '%s'" % ( field [ 4 ] , tag ) )
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
2750	def get_all_sizes ( self ) : data = self . get_data ( "sizes/" ) sizes = list ( ) for jsoned in data [ 'sizes' ] : size = Size ( * * jsoned ) size . token = self . token sizes . append ( size ) return sizes
1122	def action ( inner_rule , loc = None ) : def decorator ( mapper ) : @ llrule ( loc , inner_rule . expected ) def outer_rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : return result if isinstance ( result , tuple ) : return mapper ( parser , * result ) else : return mapper ( parser , result ) return outer_rule return decorator
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
4128	def readwav ( filename ) : from scipy . io . wavfile import read as readwav samplerate , signal = readwav ( filename ) return signal , samplerate
10608	def _create_element_list ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
10217	def prerender ( graph : BELGraph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel_hgnc from bio2bel_hgnc . models import HumanGene graph : BELGraph = graph . copy ( ) enrich_protein_and_rna_origins ( graph ) collapse_all_variants ( graph ) genes : Set [ Gene ] = get_nodes_by_function ( graph , GENE ) hgnc_symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc_manager = bio2bel_hgnc . Manager ( ) human_genes = ( hgnc_manager . session . query ( HumanGene . symbol , HumanGene . location ) . filter ( HumanGene . symbol . in_ ( hgnc_symbols ) ) . all ( ) ) for human_gene in human_genes : result [ human_gene . symbol ] = { 'name' : human_gene . symbol , 'chr' : ( human_gene . location . split ( 'q' ) [ 0 ] if 'q' in human_gene . location else human_gene . location . split ( 'p' ) [ 0 ] ) , } df = get_df ( ) for _ , ( gene_id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc_symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
12374	def take_snapshot ( droplet , name ) : print "powering off" droplet . power_off ( ) droplet . wait ( ) # wait for pending actions to complete print "taking snapshot" droplet . take_snapshot ( name ) droplet . wait ( ) snapshots = droplet . snapshots ( ) print "Current snapshots" print snapshots
10750	def download ( self , bands , download_dir = None , metadata = False ) : super ( GoogleDownloader , self ) . validate_bands ( bands ) pattern = re . compile ( '^[^\s]+_(.+)\.tiff?' , re . I ) image_list = [ ] band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download_dir is None : download_dir = DOWNLOAD_DIR check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) filename = "%s%s" % ( self . sceneInfo . name , self . __remote_file_ext ) downloaded = self . fetch ( self . remote_file_url , download_dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder_path = join ( download_dir , self . sceneInfo . name ) logger . debug ( 'Starting data extraction in directory ' , folder_path ) tar . extractall ( folder_path ) remove ( downloaded [ 0 ] ) images_path = listdir ( folder_path ) for image_path in images_path : matched = pattern . match ( image_path ) file_path = join ( folder_path , image_path ) if matched and matched . group ( 1 ) in band_list : image_list . append ( [ file_path , getsize ( file_path ) ] ) elif matched : remove ( file_path ) except tarfile . ReadError as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image_list
9309	def get_sig_string ( req , cano_req , scope ) : amz_date = req . headers [ 'x-amz-date' ] hsh = hashlib . sha256 ( cano_req . encode ( ) ) sig_items = [ 'AWS4-HMAC-SHA256' , amz_date , scope , hsh . hexdigest ( ) ] sig_string = '\n' . join ( sig_items ) return sig_string
7351	def parse_netchop ( netchop_output ) : line_iterator = iter ( netchop_output . decode ( ) . split ( "\n" ) ) scores = [ ] for line in line_iterator : if "pos" in line and 'AA' in line and 'score' in line : scores . append ( [ ] ) if "----" not in next ( line_iterator ) : raise ValueError ( "Dashes expected" ) line = next ( line_iterator ) while '-------' not in line : score = float ( line . split ( ) [ 3 ] ) scores [ - 1 ] . append ( score ) line = next ( line_iterator ) return scores
11987	async def trigger ( self , event , data = None , socket_id = None ) : json_data = json . dumps ( data , cls = self . pusher . encoder ) query_string = self . signed_query ( event , json_data , socket_id ) signed_path = "%s?%s" % ( self . path , query_string ) pusher = self . pusher absolute_url = pusher . get_absolute_path ( signed_path ) response = await pusher . http . post ( absolute_url , data = json_data , headers = [ ( 'Content-Type' , 'application/json' ) ] ) response . raise_for_status ( ) return response . status_code == 202
1630	def CheckForHeaderGuard ( filename , clean_lines , error ) : # Don't check for header guards if there are error suppression # comments somewhere in this file. # # Because this is silencing a warning for a nonexistent line, we # only support the very specific NOLINT(build/header_guard) syntax, # and not the general NOLINT or NOLINT(*) syntax. raw_lines = clean_lines . lines_without_raw_strings for i in raw_lines : if Search ( r'//\s*NOLINT\(build/header_guard\)' , i ) : return # Allow pragma once instead of header guards for i in raw_lines : if Search ( r'^\s*#pragma\s+once' , i ) : return cppvar = GetHeaderGuardCPPVariable ( filename ) ifndef = '' ifndef_linenum = 0 define = '' endif = '' endif_linenum = 0 for linenum , line in enumerate ( raw_lines ) : linesplit = line . split ( ) if len ( linesplit ) >= 2 : # find the first occurrence of #ifndef and #define, save arg if not ifndef and linesplit [ 0 ] == '#ifndef' : # set ifndef to the header guard presented on the #ifndef line. ifndef = linesplit [ 1 ] ifndef_linenum = linenum if not define and linesplit [ 0 ] == '#define' : define = linesplit [ 1 ] # find the last occurrence of #endif, save entire line if line . startswith ( '#endif' ) : endif = line endif_linenum = linenum if not ifndef or not define or ifndef != define : error ( filename , 0 , 'build/header_guard' , 5 , 'No #ifndef header guard found, suggested CPP variable is: %s' % cppvar ) return # The guard should be PATH_FILE_H_, but we also allow PATH_FILE_H__ # for backward compatibility. if ifndef != cppvar : error_level = 0 if ifndef != cppvar + '_' : error_level = 5 ParseNolintSuppressions ( filename , raw_lines [ ifndef_linenum ] , ifndef_linenum , error ) error ( filename , ifndef_linenum , 'build/header_guard' , error_level , '#ifndef header guard has wrong style, please use: %s' % cppvar ) # Check for "//" comments on endif line. ParseNolintSuppressions ( filename , raw_lines [ endif_linenum ] , endif_linenum , error ) match = Match ( r'#endif\s*//\s*' + cppvar + r'(_)?\b' , endif ) if match : if match . group ( 1 ) == '_' : # Issue low severity warning for deprecated double trailing underscore error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif // %s"' % cppvar ) return # Didn't find the corresponding "//" comment. If this file does not # contain any "//" comments at all, it could be that the compiler # only wants "/**/" comments, look for those instead. no_single_line_comments = True for i in xrange ( 1 , len ( raw_lines ) - 1 ) : line = raw_lines [ i ] if Match ( r'^(?:(?:\'(?:\.|[^\'])*\')|(?:"(?:\.|[^"])*")|[^\'"])*//' , line ) : no_single_line_comments = False break if no_single_line_comments : match = Match ( r'#endif\s*/\*\s*' + cppvar + r'(_)?\s*\*/' , endif ) if match : if match . group ( 1 ) == '_' : # Low severity warning for double trailing underscore error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif /* %s */"' % cppvar ) return # Didn't find anything error ( filename , endif_linenum , 'build/header_guard' , 5 , '#endif line should be "#endif // %s"' % cppvar )
583	def _addRecordToKNN ( self , record ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
1460	def resolve_heron_suffix_issue ( abs_pex_path , class_path ) : # import top-level package named `heron` of a given pex file importer = zipimport . zipimporter ( abs_pex_path ) importer . load_module ( "heron" ) # remove 'heron' and the classname to_load_lst = class_path . split ( '.' ) [ 1 : - 1 ] loaded = [ 'heron' ] loaded_mod = None for to_load in to_load_lst : sub_importer = zipimport . zipimporter ( os . path . join ( abs_pex_path , '/' . join ( loaded ) ) ) loaded_mod = sub_importer . load_module ( to_load ) loaded . append ( to_load ) return loaded_mod
873	def getState ( self ) : varStates = dict ( ) for varName , var in self . permuteVars . iteritems ( ) : varStates [ varName ] = var . getState ( ) return dict ( id = self . particleId , genIdx = self . genIdx , swarmId = self . swarmId , varStates = varStates )
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
3849	async def fetch ( self , method , url , params = None , headers = None , data = None ) : logger . debug ( 'Sending request %s %s:\n%r' , method , url , data ) for retry_num in range ( MAX_RETRIES ) : try : async with self . fetch_raw ( method , url , params = params , headers = headers , data = data ) as res : async with async_timeout . timeout ( REQUEST_TIMEOUT ) : body = await res . read ( ) logger . debug ( 'Received response %d %s:\n%r' , res . status , res . reason , body ) except asyncio . TimeoutError : error_msg = 'Request timed out' except aiohttp . ServerDisconnectedError as err : error_msg = 'Server disconnected error: {}' . format ( err ) except ( aiohttp . ClientError , ValueError ) as err : error_msg = 'Request connection error: {}' . format ( err ) else : break logger . info ( 'Request attempt %d failed: %s' , retry_num , error_msg ) else : logger . info ( 'Request failed after %d attempts' , MAX_RETRIES ) raise exceptions . NetworkError ( error_msg ) if res . status != 200 : logger . info ( 'Request returned unexpected status: %d %s' , res . status , res . reason ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) return FetchResponse ( res . status , body )
6374	def pr_lmean ( self ) : precision = self . precision ( ) recall = self . recall ( ) if not precision or not recall : return 0.0 elif precision == recall : return precision return ( precision - recall ) / ( math . log ( precision ) - math . log ( recall ) )
10733	def to_bool ( option , value ) : if type ( value ) is str : if value . lower ( ) == 'true' : value = True elif value . lower ( ) == 'false' : value = False return ( option , value )
7235	def map ( self , features = None , query = None , styles = None , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 10 , center = None , image = None , image_bounds = None , cmap = 'viridis' , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , * * kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a key or set the MAPBOX_API_KEY environment variable. Use outside of GBDX Notebooks requires a MapBox API key, sign up for free at https://www.mapbox.com/pricing/" if features is None and query is not None : wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = None ) elif features is None and query is None and image is None : print ( 'Must provide either a list of features or a query or an image' ) return if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] geojson = { "type" : "FeatureCollection" , "features" : features } if center is None and features is not None : union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] elif center is None and image is not None : try : lon , lat = shape ( image ) . centroid . coords [ 0 ] except : lon , lat = box ( * image_bounds ) . centroid . coords [ 0 ] else : lat , lon = center map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorGeojsonLayer ( geojson , styles = styles , * * kwargs ) image_layer = self . _build_image_layer ( image , image_bounds , cmap ) template = BaseTemplate ( map_id , * * { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : 'dummy' } ) template . inject ( )
12632	def copy_groups_to_folder ( dicom_groups , folder_path , groupby_field_name ) : if dicom_groups is None or not dicom_groups : raise ValueError ( 'Expected a boyle.dicom.sets.DicomFileSet.' ) if not os . path . exists ( folder_path ) : os . makedirs ( folder_path , exist_ok = False ) for dcmg in dicom_groups : if groupby_field_name is not None and len ( groupby_field_name ) > 0 : dfile = DicomFile ( dcmg ) dir_name = '' for att in groupby_field_name : dir_name = os . path . join ( dir_name , dfile . get_attributes ( att ) ) dir_name = str ( dir_name ) else : dir_name = os . path . basename ( dcmg ) group_folder = os . path . join ( folder_path , dir_name ) os . makedirs ( group_folder , exist_ok = False ) log . debug ( 'Copying files to {}.' . format ( group_folder ) ) import shutil dcm_files = dicom_groups [ dcmg ] for srcf in dcm_files : destf = os . path . join ( group_folder , os . path . basename ( srcf ) ) while os . path . exists ( destf ) : destf += '+' shutil . copy2 ( srcf , destf )
10844	def sent ( self ) : sent_updates = [ ] url = PATHS [ 'GET_SENT' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : sent_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __sent = sent_updates return self . __sent
263	def plot_factor_contribution_to_perf ( perf_attrib_data , ax = None , title = 'Cumulative common returns attribution' , ) : if ax is None : ax = plt . gca ( ) factors_to_plot = perf_attrib_data . drop ( [ 'total_returns' , 'common_returns' ] , axis = 'columns' , errors = 'ignore' ) factors_cumulative = pd . DataFrame ( ) for factor in factors_to_plot : factors_cumulative [ factor ] = ep . cum_returns ( factors_to_plot [ factor ] ) for col in factors_cumulative : ax . plot ( factors_cumulative [ col ] ) ax . axhline ( 0 , color = 'k' ) configure_legend ( ax , change_colors = True ) ax . set_ylabel ( 'Cumulative returns by factor' ) ax . set_title ( title ) return ax
11292	def consume_json ( request ) : client = OEmbedConsumer ( ) urls = request . GET . getlist ( 'urls' ) width = request . GET . get ( 'width' ) height = request . GET . get ( 'height' ) template_dir = request . GET . get ( 'template_dir' ) output = { } ctx = RequestContext ( request ) for url in urls : try : provider = oembed . site . provider_for_url ( url ) except OEmbedMissingEndpoint : oembeds = None rendered = None else : oembeds = url rendered = client . parse_text ( url , width , height , context = ctx , template_dir = template_dir ) output [ url ] = { 'oembeds' : oembeds , 'rendered' : rendered , } return HttpResponse ( simplejson . dumps ( output ) , mimetype = 'application/json' )
10540	def delete_category ( category_id ) : try : res = _pybossa_req ( 'delete' , 'category' , category_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
6095	def voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid , regular_to_nearest_pix , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : regular_to_pix = np . zeros ( ( regular_grid . shape [ 0 ] ) ) for regular_index in range ( regular_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ regular_index ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( regular_grid [ regular_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( regular_grid [ regular_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_neighbor : closest_separation_from_pix_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : regular_to_pix [ regular_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return regular_to_pix
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) # initialize `TemporalMemoryMonitorMixin` attributes tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
4721	def trun_exit ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun:exit" ) rcode = 0 for hook in reversed ( trun [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::exit { rcode: %r }" % rcode , rcode ) return rcode
2388	def spell_correct ( string ) : # Create a temp file so that aspell could be used # By default, tempfile will delete this file when the file handle is closed. f = tempfile . NamedTemporaryFile ( mode = 'w' ) f . write ( string ) f . flush ( ) f_path = os . path . abspath ( f . name ) try : p = os . popen ( aspell_path + " -a < " + f_path + " --sug-mode=ultra" ) # Aspell returns a list of incorrect words with the above flags incorrect = p . readlines ( ) p . close ( ) except Exception : log . exception ( "aspell process failed; could not spell check" ) # Return original string if aspell fails return string , 0 , string finally : f . close ( ) incorrect_words = list ( ) correct_spelling = list ( ) for i in range ( 1 , len ( incorrect ) ) : if ( len ( incorrect [ i ] ) > 10 ) : #Reformat aspell output to make sense match = re . search ( ":" , incorrect [ i ] ) if hasattr ( match , "start" ) : begstring = incorrect [ i ] [ 2 : match . start ( ) ] begmatch = re . search ( " " , begstring ) begword = begstring [ 0 : begmatch . start ( ) ] sugstring = incorrect [ i ] [ match . start ( ) + 2 : ] sugmatch = re . search ( "," , sugstring ) if hasattr ( sugmatch , "start" ) : sug = sugstring [ 0 : sugmatch . start ( ) ] incorrect_words . append ( begword ) correct_spelling . append ( sug ) #Create markup based on spelling errors newstring = string markup_string = string already_subbed = [ ] for i in range ( 0 , len ( incorrect_words ) ) : sub_pat = r"\b" + incorrect_words [ i ] + r"\b" sub_comp = re . compile ( sub_pat ) newstring = re . sub ( sub_comp , correct_spelling [ i ] , newstring ) if incorrect_words [ i ] not in already_subbed : markup_string = re . sub ( sub_comp , '<bs>' + incorrect_words [ i ] + "</bs>" , markup_string ) already_subbed . append ( incorrect_words [ i ] ) return newstring , len ( incorrect_words ) , markup_string
6429	def stem ( self , word ) : lowered = word . lower ( ) if lowered [ - 3 : ] == 'ies' and lowered [ - 4 : - 3 ] not in { 'e' , 'a' } : return word [ : - 3 ] + ( 'Y' if word [ - 1 : ] . isupper ( ) else 'y' ) if lowered [ - 2 : ] == 'es' and lowered [ - 3 : - 2 ] not in { 'a' , 'e' , 'o' } : return word [ : - 1 ] if lowered [ - 1 : ] == 's' and lowered [ - 2 : - 1 ] not in { 'u' , 's' } : return word [ : - 1 ] return word
12513	def _crop_img_to ( image , slices , copy = True ) : img = check_img ( image ) data = img . get_data ( ) affine = img . get_affine ( ) cropped_data = data [ slices ] if copy : cropped_data = cropped_data . copy ( ) linear_part = affine [ : 3 , : 3 ] old_origin = affine [ : 3 , 3 ] new_origin_voxel = np . array ( [ s . start for s in slices ] ) new_origin = old_origin + linear_part . dot ( new_origin_voxel ) new_affine = np . eye ( 4 ) new_affine [ : 3 , : 3 ] = linear_part new_affine [ : 3 , 3 ] = new_origin new_img = nib . Nifti1Image ( cropped_data , new_affine ) return new_img
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : # relative path target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : # source_file is a folder print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
13483	def rsync_docs ( ) : assert options . paved . docs . rsync_location , "Please specify an rsync location in options.paved.docs.rsync_location." sh ( 'rsync -ravz %s/ %s/' % ( path ( options . paved . docs . path ) / options . paved . docs . build_rel , options . paved . docs . rsync_location ) )
8280	def _render_closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def _render ( cairo_ctx ) : ''' At the moment this is based on cairo. TODO: Need to work out how to move the cairo specific bits somewhere else. ''' # Go to initial point (CORNER or CENTER): transform = self . _call_transform_mode ( self . _transform ) if fillcolor is None and strokecolor is None : # Fixes _bug_FillStrokeNofillNostroke.bot return cairo_ctx . set_matrix ( transform ) # Run the path commands on the cairo context: self . _traverse ( cairo_ctx ) # Matrix affects stroke, so we need to reset it: cairo_ctx . set_matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : # Draw onto intermediate surface so that stroke # does not overlay fill cairo_ctx . push_group ( ) cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) e = cairo_ctx . stroke_extents ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_operator ( cairo . OPERATOR_SOURCE ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) cairo_ctx . pop_group_to_source ( ) cairo_ctx . paint ( ) else : # Fast path if no alpha in stroke cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) elif fillcolor is not None : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill ( ) elif strokecolor is not None : cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) return _render
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
13442	def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) #Copy base from cloud to local util . copy ( ccat , lcat ) #Apply changesets cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) # Write meta-data both to local and cloud mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) #Let's copy Smart Previews if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) #Finally, let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
3046	def _do_refresh_request ( self , http ) : body = self . _generate_refresh_request_body ( ) headers = self . _generate_refresh_request_headers ( ) logger . info ( 'Refreshing access_token' ) resp , content = transport . request ( http , self . token_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . token_response = d self . access_token = d [ 'access_token' ] self . refresh_token = d . get ( 'refresh_token' , self . refresh_token ) if 'expires_in' in d : delta = datetime . timedelta ( seconds = int ( d [ 'expires_in' ] ) ) self . token_expiry = delta + _UTCNOW ( ) else : self . token_expiry = None if 'id_token' in d : self . id_token = _extract_id_token ( d [ 'id_token' ] ) self . id_token_jwt = d [ 'id_token' ] else : self . id_token = None self . id_token_jwt = None # On temporary refresh errors, the user does not actually have to # re-authorize, so we unflag here. self . invalid = False if self . store : self . store . locked_put ( self ) else : # An {'error':...} response body means the token is expired or # revoked, so we flag the credentials as such. logger . info ( 'Failed to retrieve access token: %s' , content ) error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error' in d : error_msg = d [ 'error' ] if 'error_description' in d : error_msg += ': ' + d [ 'error_description' ] self . invalid = True if self . store is not None : self . store . locked_put ( self ) except ( TypeError , ValueError ) : pass raise HttpAccessTokenRefreshError ( error_msg , status = resp . status )
12883	def _run_supervisor ( self ) : import time still_supervising = lambda : ( multiprocessing . active_children ( ) or not self . log_queue . empty ( ) or not self . exception_queue . empty ( ) ) try : while still_supervising ( ) : # When a log message is received, make a logger with the same # name in this process and use it to re-log the message. It # will get handled in this process. try : record = self . log_queue . get_nowait ( ) logger = logging . getLogger ( record . name ) logger . handle ( record ) except queue . Empty : pass # When an exception is received, immediately re-raise it. try : exception = self . exception_queue . get_nowait ( ) except queue . Empty : pass else : raise exception # Sleep for a little bit, and make sure that the workers haven't # outlived their time limit. time . sleep ( 1 / self . frame_rate ) self . elapsed_time += 1 / self . frame_rate if self . time_limit and self . elapsed_time > self . time_limit : raise RuntimeError ( "timeout" ) # Make sure the workers don't outlive the supervisor, no matter how the # polling loop ended (e.g. normal execution or an exception). finally : for process in multiprocessing . active_children ( ) : process . terminate ( )
2930	def pre_parse_and_validate ( self , bpmn , filename ) : bpmn = self . _call_editor_hook ( 'pre_parse_and_validate' , bpmn , filename ) or bpmn return bpmn
11202	def strip_comments ( string , comment_symbols = frozenset ( ( '#' , '//' ) ) ) : lines = string . splitlines ( ) for k in range ( len ( lines ) ) : for symbol in comment_symbols : lines [ k ] = strip_comment_line_with_symbol ( lines [ k ] , start = symbol ) return '\n' . join ( lines )
7252	def order ( self , image_catalog_ids , batch_size = 100 , callback = None ) : def _order_single_batch ( url_ , ids , results_list ) : data = json . dumps ( ids ) if callback is None else json . dumps ( { "acquisitionIds" : ids , "callback" : callback } ) r = self . gbdx_connection . post ( url_ , data = data ) r . raise_for_status ( ) order_id = r . json ( ) . get ( "order_id" ) if order_id : results_list . append ( order_id ) self . logger . debug ( 'Place order' ) url = ( '%s/order' if callback is None else '%s/ordercb' ) % self . base_url batch_size = min ( 100 , batch_size ) if not isinstance ( image_catalog_ids , list ) : image_catalog_ids = [ image_catalog_ids ] sanitized_ids = list ( set ( ( id for id in ( _id . strip ( ) for _id in image_catalog_ids ) if id ) ) ) res = [ ] # Use itertool batch recipe acq_ids_by_batch = zip ( * ( [ iter ( sanitized_ids ) ] * batch_size ) ) for ids_batch in acq_ids_by_batch : _order_single_batch ( url , ids_batch , res ) # Order reminder remain_count = len ( sanitized_ids ) % batch_size if remain_count > 0 : _order_single_batch ( url , sanitized_ids [ - remain_count : ] , res ) if len ( res ) == 1 : return res [ 0 ] elif len ( res ) > 1 : return res
12646	def set_aad_metadata ( uri , resource , client ) : set_config_value ( 'authority_uri' , uri ) set_config_value ( 'aad_resource' , resource ) set_config_value ( 'aad_client' , client )
5100	def _dict2dict ( adj_dict ) : item = adj_dict . popitem ( ) adj_dict [ item [ 0 ] ] = item [ 1 ] if not isinstance ( item [ 1 ] , dict ) : new_dict = { } for key , value in adj_dict . items ( ) : new_dict [ key ] = { v : { } for v in value } adj_dict = new_dict return adj_dict
13715	def next ( self ) : queue = self . queue items = [ ] item = self . next_item ( ) if item is None : return items items . append ( item ) while len ( items ) < self . upload_size and not queue . empty ( ) : item = self . next_item ( ) if item : items . append ( item ) return items
5984	def image_psf_shape_tag_from_image_psf_shape ( image_psf_shape ) : if image_psf_shape is None : return '' else : y = str ( image_psf_shape [ 0 ] ) x = str ( image_psf_shape [ 1 ] ) return ( '_image_psf_' + y + 'x' + x )
12650	def is_fnmatch_regex ( string ) : is_regex = False regex_chars = [ '!' , '*' , '$' ] for c in regex_chars : if string . find ( c ) > - 1 : return True return is_regex
11842	def ModelBasedVacuumAgent ( ) : model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status ## Update the model here if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
5207	def format_output ( data : pd . DataFrame , source , col_maps = None ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) if source == 'bdp' : req_cols = [ 'ticker' , 'field' , 'value' ] else : req_cols = [ 'ticker' , 'field' , 'name' , 'value' , 'position' ] if any ( col not in data for col in req_cols ) : return pd . DataFrame ( ) if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) if source == 'bdp' : res = pd . DataFrame ( pd . concat ( [ pd . Series ( { * * { 'ticker' : t } , * * grp . set_index ( 'field' ) . value . to_dict ( ) } ) for t , grp in data . groupby ( 'ticker' ) ] , axis = 1 , sort = False ) ) . transpose ( ) . set_index ( 'ticker' ) else : res = pd . DataFrame ( pd . concat ( [ grp . loc [ : , [ 'name' , 'value' ] ] . set_index ( 'name' ) . transpose ( ) . reset_index ( drop = True ) . assign ( ticker = t ) for ( t , _ ) , grp in data . groupby ( [ 'ticker' , 'position' ] ) ] , sort = False ) ) . reset_index ( drop = True ) . set_index ( 'ticker' ) res . columns . name = None if col_maps is None : col_maps = dict ( ) return res . rename ( columns = lambda vv : col_maps . get ( vv , vv . lower ( ) . replace ( ' ' , '_' ) . replace ( '-' , '_' ) ) ) . apply ( pd . to_numeric , errors = 'ignore' , downcast = 'float' )
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
2589	def shutdown ( self , block = False ) : x = self . executor . shutdown ( wait = block ) logger . debug ( "Done with executor shutdown" ) return x
1756	def emulate_until ( self , target : int ) : self . _concrete = True self . _break_unicorn_at = target if self . emu : self . emu . _stop_at = target
9699	def event_choices ( events ) : if events is None : msg = "Please add some events in settings.WEBHOOK_EVENTS." raise ImproperlyConfigured ( msg ) try : choices = [ ( x , x ) for x in events ] except TypeError : """ Not a valid iterator, so we raise an exception """ msg = "settings.WEBHOOK_EVENTS must be an iterable object." raise ImproperlyConfigured ( msg ) return choices
10828	def delete ( cls , group , user ) : with db . session . begin_nested ( ) : cls . query . filter_by ( group = group , user_id = user . get_id ( ) ) . delete ( )
11986	async def _upload_file ( self , full_path ) : rel_path = os . path . relpath ( full_path , self . folder ) key = s3_key ( os . path . join ( self . key , rel_path ) ) ct = self . content_types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full_path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload_file ( self . bucket , file , key = key , ContentType = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full_path ) return size = self . all . pop ( full_path ) self . success [ key ] = size self . total_size += size percentage = 100 * ( 1 - len ( self . all ) / self . total_files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert_bytes ( size ) ) LOGGER . info ( message )
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
361	def exists_or_mkdir ( path , verbose = True ) : if not os . path . exists ( path ) : if verbose : logging . info ( "[*] creates %s ..." % path ) os . makedirs ( path ) return False else : if verbose : logging . info ( "[!] %s exists ..." % path ) return True
13683	def get ( self , url , params = { } ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . get ( self . host + url , params = params ) except RequestException as e : response = e . args return self . json_parse ( response . content )
7944	def _connected ( self ) : self . _auth_properties [ 'remote-ip' ] = self . _dst_addr [ 0 ] if self . _dst_service : self . _auth_properties [ 'service-domain' ] = self . _dst_name if self . _dst_hostname is not None : self . _auth_properties [ 'service-hostname' ] = self . _dst_hostname else : self . _auth_properties [ 'service-hostname' ] = self . _dst_addr [ 0 ] self . _auth_properties [ 'security-layer' ] = None self . event ( ConnectedEvent ( self . _dst_addr ) ) self . _set_state ( "connected" ) self . _stream . transport_connected ( )
6109	def xticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 1 ] ) , np . amax ( self . grid_stack . regular [ : , 1 ] ) , 4 )
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
1287	def process_docstring ( app , what , name , obj , options , lines ) : markdown = "\n" . join ( lines ) # ast = cm_parser.parse(markdown) # html = cm_renderer.render(ast) rest = m2r ( markdown ) rest . replace ( "\r\n" , "\n" ) del lines [ : ] lines . extend ( rest . split ( "\n" ) )
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
10318	def _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) : ret = dict ( ) runs = has_spanning_cluster . size # Bayesian posterior mean for Binomial proportion (uniform prior) k = has_spanning_cluster . sum ( dtype = np . float ) ret [ 'spanning_cluster' ] = ( ( k + 1 ) / ( runs + 2 ) ) # Bayesian credible interval for Binomial proportion (uniform # prior) ret [ 'spanning_cluster_ci' ] = scipy . stats . beta . ppf ( [ alpha / 2 , 1 - alpha / 2 ] , k + 1 , runs - k + 1 ) return ret
8860	def goto_assignments ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] # encoding = request_data['encoding'] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_assignments ( ) except jedi . NotFoundError : pass else : ret_val = [ ( d . module_path , d . line - 1 if d . line else None , d . column , d . full_name ) for d in definitions ] return ret_val
12878	def many_until1 ( these , term ) : first = [ these ( ) ] these_results , term_result = many_until ( these , term ) return ( first + these_results , term_result )
11236	def reusable ( func ) : sig = signature ( func ) origin = func while hasattr ( origin , '__wrapped__' ) : origin = origin . __wrapped__ return type ( origin . __name__ , ( ReusableGenerator , ) , dict ( [ ( '__doc__' , origin . __doc__ ) , ( '__module__' , origin . __module__ ) , ( '__signature__' , sig ) , ( '__wrapped__' , staticmethod ( func ) ) , ] + [ ( name , property ( compose ( itemgetter ( name ) , attrgetter ( '_bound_args.arguments' ) ) ) ) for name in sig . parameters ] + ( [ ( '__qualname__' , origin . __qualname__ ) , ] if sys . version_info > ( 3 , ) else [ ] ) ) )
6316	def reload_programs ( self ) : print ( "Reloading programs:" ) for name , program in self . _programs . items ( ) : if getattr ( program , 'program' , None ) : print ( " - {}" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )
8446	def switch ( template , version ) : temple . update . update ( new_template = template , new_version = version )
11386	def run ( self , raw_args ) : parser = self . parser args , kwargs = parser . parse_callback_args ( raw_args ) callback = kwargs . pop ( "main_callback" ) if parser . has_injected_quiet ( ) : levels = kwargs . pop ( "quiet_inject" , "" ) logging . inject_quiet ( levels ) try : ret_code = callback ( * args , * * kwargs ) ret_code = int ( ret_code ) if ret_code else 0 except ArgError as e : # https://hg.python.org/cpython/file/2.7/Lib/argparse.py#l2374 echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret_code = 2 return ret_code
40	def add ( self , * args , * * kwargs ) : idx = self . _next_idx super ( ) . add ( * args , * * kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
11943	def _get ( self , * args , * * kwargs ) : messages , all_retrieved = super ( StorageMixin , self ) . _get ( * args , * * kwargs ) if self . user . is_authenticated ( ) : inbox_messages = self . backend . inbox_list ( self . user ) else : inbox_messages = [ ] return messages + inbox_messages , all_retrieved
6066	def einstein_radius_rescaled ( self ) : return ( ( 3 - self . slope ) / ( 1 + self . axis_ratio ) ) * self . einstein_radius ** ( self . slope - 1 )
5134	def generate_pagerank_graph ( num_vertices = 250 , * * kwargs ) : g = minimal_random_graph ( num_vertices , * * kwargs ) r = np . zeros ( num_vertices ) for k , pr in nx . pagerank ( g ) . items ( ) : r [ k ] = pr g = set_types_rank ( g , rank = r , * * kwargs ) return g
8054	def handler ( self , conn , * args ) : # lines from cmd.Cmd self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\r\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) # end lines from cmd.Cmd if stop : self . shell = None conn . close ( ) return not stop
4741	def paths_from_env ( prefix = None , names = None ) : def expand_path ( path ) : """Expands variables in 'path' and turns it into absolute path""" return os . path . abspath ( os . path . expanduser ( os . path . expandvars ( path ) ) ) if prefix is None : prefix = "CIJ" if names is None : names = [ "ROOT" , "ENVS" , "TESTPLANS" , "TESTCASES" , "TESTSUITES" , "MODULES" , "HOOKS" , "TEMPLATES" ] conf = { v : os . environ . get ( "_" . join ( [ prefix , v ] ) ) for v in names } for env in ( e for e in conf . keys ( ) if e [ : len ( prefix ) ] in names and conf [ e ] ) : conf [ env ] = expand_path ( conf [ env ] ) if not os . path . exists ( conf [ env ] ) : err ( "%s_%s: %r, does not exist" % ( prefix , env , conf [ env ] ) ) return conf
244	def days_to_liquidate_positions ( positions , market_data , max_bar_consumption = 0.2 , capital_base = 1e6 , mean_volume_window = 5 ) : DV = market_data [ 'volume' ] * market_data [ 'price' ] roll_mean_dv = DV . rolling ( window = mean_volume_window , center = False ) . mean ( ) . shift ( ) roll_mean_dv = roll_mean_dv . replace ( 0 , np . nan ) positions_alloc = pos . get_percent_alloc ( positions ) positions_alloc = positions_alloc . drop ( 'cash' , axis = 1 ) days_to_liquidate = ( positions_alloc * capital_base ) / ( max_bar_consumption * roll_mean_dv ) return days_to_liquidate . iloc [ mean_volume_window : ]
4311	def _build_input_args ( input_filepath_list , input_format_list ) : if len ( input_format_list ) != len ( input_filepath_list ) : raise ValueError ( "input_format_list & input_filepath_list are not the same size" ) input_args = [ ] zipped = zip ( input_filepath_list , input_format_list ) for input_file , input_fmt in zipped : input_args . extend ( input_fmt ) input_args . append ( input_file ) return input_args
8494	def _error ( msg , * args ) : print ( msg % args , file = sys . stderr ) sys . exit ( 1 )
2933	def write_meta_data ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'MetaData' ) config . set ( 'MetaData' , 'entry_point_process' , self . wf_spec . name ) if self . editor : config . set ( 'MetaData' , 'editor' , self . editor ) for k , v in self . meta_data : config . set ( 'MetaData' , k , v ) if not self . PARSER_CLASS == BpmnParser : config . set ( 'MetaData' , 'parser_class_module' , inspect . getmodule ( self . PARSER_CLASS ) . __name__ ) config . set ( 'MetaData' , 'parser_class' , self . PARSER_CLASS . __name__ ) ini = StringIO ( ) config . write ( ini ) self . write_to_package_zip ( self . METADATA_FILE , ini . getvalue ( ) )
10637	def get_element_mfr_dictionary ( self ) : element_symbols = self . material . elements element_mfrs = self . get_element_mfrs ( ) result = dict ( ) for s , mfr in zip ( element_symbols , element_mfrs ) : result [ s ] = mfr return result
5625	def relative_path ( path = None , base_dir = None ) : if path_is_remote ( path ) or not os . path . isabs ( path ) : return path else : return os . path . relpath ( path , base_dir )
12129	def _build_specs ( self , specs , kwargs , fp_precision ) : if specs is None : overrides = param . ParamOverrides ( self , kwargs , allow_extra_keywords = True ) extra_kwargs = overrides . extra_keywords ( ) kwargs = dict ( [ ( k , v ) for ( k , v ) in kwargs . items ( ) if k not in extra_kwargs ] ) rounded_specs = list ( self . round_floats ( [ extra_kwargs ] , fp_precision ) ) if extra_kwargs == { } : return [ ] , kwargs , True else : return rounded_specs , kwargs , False return list ( self . round_floats ( specs , fp_precision ) ) , kwargs , True
5732	def parse_response ( gdb_mi_text ) : stream = StringStream ( gdb_mi_text , debug = _DEBUG ) if _GDB_MI_NOTIFY_RE . match ( gdb_mi_text ) : token , message , payload = _get_notify_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "notify" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_RESULT_RE . match ( gdb_mi_text ) : token , message , payload = _get_result_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "result" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) : return { "type" : "console" , "message" : None , "payload" : _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_LOG_RE . match ( gdb_mi_text ) : return { "type" : "log" , "message" : None , "payload" : _GDB_MI_LOG_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) : return { "type" : "target" , "message" : None , "payload" : _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif response_is_finished ( gdb_mi_text ) : return { "type" : "done" , "message" : None , "payload" : None } else : # This was not gdb mi output, so it must have just been printed by # the inferior program that's being debugged return { "type" : "output" , "message" : None , "payload" : gdb_mi_text }
1944	def _hook_syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg_read(self._to_unicorn_id('RIP')))} to perform syscall" ) self . sync_unicorn_to_manticore ( ) from . . native . cpu . abstractcpu import Syscall self . _to_raise = Syscall ( ) uc . emu_stop ( )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] # We want a color sequence of length 2n-2 # e.g. for n=5: a b c d | e d c b | a b c d ... m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n # p is a number in [0, n): scale it to be in [0, n-1) pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
13671	def strip_codes ( s : Any ) -> str : return codepat . sub ( '' , str ( s ) if ( s or ( s == 0 ) ) else '' )
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return # If we can read the credentials from a file, we don't need to know # what environment we are in. SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
1404	def load_configs ( self ) : self . statemgr_config . set_state_locations ( self . configs [ STATEMGRS_KEY ] ) if EXTRA_LINKS_KEY in self . configs : for extra_link in self . configs [ EXTRA_LINKS_KEY ] : self . extra_links . append ( self . validate_extra_link ( extra_link ) )
815	def Indicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) x [ pos ] = 1 return x
9436	def strip_ip ( packet ) : if not isinstance ( packet , IP ) : packet = IP ( packet ) payload = packet . payload return payload
10531	def find_project ( * * kwargs ) : try : res = _pybossa_req ( 'get' , 'project' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : return res except : # pragma: no cover raise
1128	def SeqN ( n , * inner_rules , * * kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
7785	def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _error_handler ( self . address , error_data ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
12237	def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad
7151	def get_checksum ( cls , phrase ) : phrase_split = phrase . split ( " " ) if len ( phrase_split ) < 12 : raise ValueError ( "Invalid mnemonic phrase" ) if len ( phrase_split ) > 13 : # Standard format phrase = phrase_split [ : 24 ] else : # MyMonero format phrase = phrase_split [ : 12 ] wstr = "" . join ( word [ : cls . unique_prefix_length ] for word in phrase ) wstr = bytearray ( wstr . encode ( 'utf-8' ) ) z = ( ( crc32 ( wstr ) & 0xffffffff ) ^ 0xffffffff ) >> 0 z2 = ( ( z ^ 0xffffffff ) >> 0 ) % len ( phrase ) return phrase_split [ z2 ]
12026	def abfIDfromFname ( fname ) : fname = os . path . abspath ( fname ) basename = os . path . basename ( fname ) return os . path . splitext ( basename ) [ 0 ]
4115	def lar2rc ( g ) : assert numpy . isrealobj ( g ) , 'Log area ratios not defined for complex reflection coefficients.' # Use the relation, tanh(x) = (1-exp(2x))/(1+exp(2x)) return - numpy . tanh ( - numpy . array ( g ) / 2 )
9748	async def choose_qtm_instance ( interface ) : instances = { } print ( "Available QTM instances:" ) async for i , qtm_instance in AsyncEnumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm_instance print ( "{} - {}" . format ( i , qtm_instance . info ) ) try : choice = int ( input ( "Connect to: " ) ) if choice not in instances : raise ValueError except ValueError : LOG . error ( "Invalid choice" ) return None return instances [ choice ] . host
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
4354	def _save_ack_callback ( self , msgid , callback ) : if msgid in self . ack_callbacks : return False self . ack_callbacks [ msgid ] = callback
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
234	def plot_sector_exposures_net ( net_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = sector_names [ i ] ) ax . set ( title = 'Net exposures to sectors' , ylabel = 'Proportion of net exposure \n in sectors' ) return ax
209	def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : # Create a 1 time token token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
13415	def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
11555	def enable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
11717	def create ( self , config ) : assert config [ "name" ] == self . name , "Given config is not for this template" data = self . _json_encode ( config ) headers = self . _default_headers ( ) return self . _request ( "" , ok_status = None , data = data , headers = headers )
1681	def AddFilters ( self , filters ) : for filt in filters . split ( ',' ) : clean_filt = filt . strip ( ) if clean_filt : self . filters . append ( clean_filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise ValueError ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )
182	def to_heatmap ( self , image_shape , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : from . heatmaps import HeatmapsOnImage return HeatmapsOnImage ( self . draw_heatmap_array ( image_shape , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : # there is only one image, use the PAN partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : # there are two images in this part, use the multi (or pansharpen) partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
12460	def parse_args ( args ) : from argparse import ArgumentParser description = ( 'Bootstrap Python projects and libraries with virtualenv ' 'and pip.' ) parser = ArgumentParser ( description = description ) parser . add_argument ( '--version' , action = 'version' , version = __version__ ) parser . add_argument ( '-c' , '--config' , default = DEFAULT_CONFIG , help = 'Path to config file. By default: {0}' . format ( DEFAULT_CONFIG ) ) parser . add_argument ( '-p' , '--pre-requirements' , default = [ ] , nargs = '+' , help = 'List of pre-requirements to check, separated by space.' ) parser . add_argument ( '-e' , '--env' , help = 'Virtual environment name. By default: {0}' . format ( CONFIG [ __script__ ] [ 'env' ] ) ) parser . add_argument ( '-r' , '--requirements' , help = 'Path to requirements file. By default: {0}' . format ( CONFIG [ __script__ ] [ 'requirements' ] ) ) parser . add_argument ( '-d' , '--install-dev-requirements' , action = 'store_true' , default = None , help = 'Install prefixed or suffixed "dev" requirements after ' 'installation of original requirements file or library completed ' 'without errors.' ) parser . add_argument ( '-C' , '--hook' , help = 'Execute this hook after bootstrap process.' ) parser . add_argument ( '--ignore-activated' , action = 'store_true' , default = None , help = 'Ignore pre-activated virtualenv, like on Travis CI.' ) parser . add_argument ( '--recreate' , action = 'store_true' , default = None , help = 'Recreate virtualenv on every run.' ) parser . add_argument ( '-q' , '--quiet' , action = 'store_true' , default = None , help = 'Minimize output, show only error messages.' ) return parser . parse_args ( args )
3202	def delete ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _delete ( url = self . _build_path ( campaign_id ) )
7865	def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_jid , self . success , self . failure )
9447	def hangup_all_calls ( self ) : path = '/' + self . api_version + '/HangupAllCalls/' method = 'POST' return self . request ( path , method )
5384	def _operation_status_message ( self ) : metadata = self . _op [ 'metadata' ] if not self . _op [ 'done' ] : if 'events' in metadata and metadata [ 'events' ] : # Get the last event last_event = metadata [ 'events' ] [ - 1 ] msg = last_event [ 'description' ] ds = last_event [ 'startTime' ] else : msg = 'Pending' ds = metadata [ 'createTime' ] else : ds = metadata [ 'endTime' ] if 'error' in self . _op : msg = self . _op [ 'error' ] [ 'message' ] else : msg = 'Success' return ( msg , google_base . parse_rfc3339_utc_string ( ds ) )
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
8317	def parse_balanced_image ( self , markup ) : opened = 0 closed = 0 for i in range ( len ( markup ) ) : if markup [ i ] == "[" : opened += 1 if markup [ i ] == "]" : closed += 1 if opened == closed : return markup [ : i + 1 ] return markup
12875	def not_followed_by ( parser ) : @ tri def not_followed_by_block ( ) : failed = object ( ) result = optional ( tri ( parser ) , failed ) if result != failed : fail ( [ "not " + _fun_to_str ( parser ) ] ) choice ( not_followed_by_block )
8225	def _key_pressed ( self , key , keycode ) : self . _namespace [ 'key' ] = key self . _namespace [ 'keycode' ] = keycode self . _namespace [ 'keydown' ] = True
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
3498	def find_carbon_sources ( model ) : try : model . slim_optimize ( error_value = None ) except OptimizationError : return [ ] reactions = model . reactions . get_by_any ( list ( model . medium ) ) reactions_fluxes = [ ( rxn , total_components_flux ( rxn . flux , reaction_elements ( rxn ) , consumption = True ) ) for rxn in reactions ] return [ rxn for rxn , c_flux in reactions_fluxes if c_flux > 0 ]
6193	def add ( self , num_particles , D ) : self . _plist += self . _generate ( num_particles , D , box = self . box , rs = self . rs )
1144	def init ( self ) : self . length = 0 self . input = [ ] # Initial 160 bit message digest (5 times 32 bit). self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0
11553	def disable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
6185	def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
11210	def _set_tzdata ( self , tzobj ) : # Copy the relevant attributes over as private attributes for attr in _tzfile . attrs : setattr ( self , '_' + attr , getattr ( tzobj , attr ) )
13446	def messages_from_response ( response ) : messages = [ ] if hasattr ( response , 'context' ) and response . context and 'messages' in response . context : messages = response . context [ 'messages' ] elif hasattr ( response , 'cookies' ) : # no "context" set-up or no messages item, check for message info in # the cookies morsel = response . cookies . get ( 'messages' ) if not morsel : return [ ] # use the decoder in the CookieStore to process and get a list of # messages from django . contrib . messages . storage . cookie import CookieStorage store = CookieStorage ( FakeRequest ( ) ) messages = store . _decode ( morsel . value ) else : return [ ] return [ ( m . message , m . level ) for m in messages ]
3169	def resume ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/resume' ) )
12758	def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
1002	def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : if self . lrnIterationIdx < 100 : alpha = 0.5 else : alpha = 0.1 self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + ( alpha * prevSeqLength ) )
6665	def verify_certificate_chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get_verbose , print_fail , print_success r = self . local_renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr_md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key_md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt_md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt_md5 == csr_md5 == key_md5 if self . verbose or not match : print ( 'crt:' , crt_md5 ) print ( 'csr:' , csr_md5 ) print ( 'key:' , key_md5 ) if match : print_success ( 'Files look good!' ) else : print_fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
4645	def delete ( self , key ) : query = ( "DELETE FROM {} WHERE {}=?" . format ( self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) connection . commit ( )
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
12337	def pip_r ( self , requirements , raise_on_error = True ) : cmd = "pip install -r %s" % requirements return self . wait ( cmd , raise_on_error = raise_on_error )
6071	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
10837	def zjitter ( jitter = 0.0 , radius = 5 ) : psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) # create a base image of one particle s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) sl = np . s_ [ s0 . pad : - s0 . pad , s0 . pad : - s0 . pad , s0 . pad : - s0 . pad ] # add up a bunch of trajectories finalimage = 0 * s0 . get_model_image ( ) [ sl ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( finalimage . shape [ 0 ] ) : offset = jitter * np . random . randn ( 3 ) * np . array ( [ 1 , 0 , 0 ] ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage [ i ] = s0 . get_model_image ( ) [ sl ] [ i ] position += s0 . obj . pos [ 0 ] position /= float ( finalimage . shape [ 0 ] ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) # measure the true inferred parameters return s , finalimage , position
4712	def script_run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log_fpath" ] , "a" ) as log_fd : log_fd . write ( "# script_fpath: %r\n" % script [ "fpath" ] ) log_fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ_ROOT=$(cij_root) && ' 'source $CIJ_ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ_TEST_RES_ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV_FPATH" ] , script [ "res_root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log_fd , stderr = STDOUT , cwd = script [ "res_root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
2878	def serialize_value ( self , parent_elem , value ) : if isinstance ( value , ( str , int ) ) or type ( value ) . __name__ == 'str' : parent_elem . text = str ( value ) elif value is None : parent_elem . text = None else : parent_elem . append ( value . serialize ( self ) )
9060	def beta ( self ) : from numpy_sugar . linalg import rsolve return rsolve ( self . _X [ "VT" ] , rsolve ( self . _X [ "tX" ] , self . mean ( ) ) )
190	def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )
6369	def recall ( self ) : if self . _tp + self . _fn == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fn )
5584	def prepare_path ( self , tile ) : makedirs ( os . path . dirname ( self . get_path ( tile ) ) )
7312	def is_local_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return ip . is_loopback except ValueError as e : return None
11308	def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
6158	def FIR_fix_header ( fname_out , h ) : M = len ( h ) hq = int16 ( rint ( h * 2 ** 15 ) ) N = 8 # Coefficients per line f = open ( fname_out , 'wt' ) f . write ( '//define a FIR coefficient Array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef M_FIR\n' ) f . write ( '#define M_FIR %d\n' % M ) f . write ( '#endif\n' ) f . write ( '/************************************************************************/\n' ) f . write ( '/* FIR Filter Coefficients */\n' ) f . write ( 'int16_t h_FIR[M_FIR] = {' ) kk = 0 for k in range ( M ) : # k_mod = k % M if ( kk < N - 1 ) and ( k < M - 1 ) : f . write ( '%5d,' % hq [ k ] ) kk += 1 elif ( kk == N - 1 ) & ( k < M - 1 ) : f . write ( '%5d,\n' % hq [ k ] ) if k < M : f . write ( ' ' ) kk = 0 else : f . write ( '%5d' % hq [ k ] ) f . write ( '};\n' ) f . write ( '/************************************************************************/\n' ) f . close ( )
9360	def _to_lower_alpha_only ( s ) : s = re . sub ( r'\n' , ' ' , s . lower ( ) ) return re . sub ( r'[^a-z\s]' , '' , s )
12283	def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
5432	def build_logging_param ( logging_uri , util_class = OutputFileParamUtil ) : if not logging_uri : return job_model . LoggingParam ( None , None ) recursive = not logging_uri . endswith ( '.log' ) oututil = util_class ( '' ) _ , uri , provider = oututil . parse_uri ( logging_uri , recursive ) if '*' in uri . basename : raise ValueError ( 'Wildcards not allowed in logging URI: %s' % uri ) return job_model . LoggingParam ( uri , provider )
7139	def spend_key ( self ) : key = self . _backend . spend_key ( ) if key == numbers . EMPTY_KEY : return None return key
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) # allow overriding htmode if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) # disables n return 'NONE'
6997	def runcp_worker ( task ) : pfpickle , outdir , lcbasedir , kwargs = task try : return runcp ( pfpickle , outdir , lcbasedir , * * kwargs ) except Exception as e : LOGEXCEPTION ( ' could not make checkplots for %s: %s' % ( pfpickle , e ) ) return None
1466	def setDefault ( self , constant , start , end ) : starttime = start / 60 * 60 if starttime < start : starttime += 60 endtime = end / 60 * 60 while starttime <= endtime : # STREAMCOMP-1559 # Second check is a work around, because the response from tmaster # contains value 0, if it is queries for the current timestamp, # since the bucket is created in the tmaster, but is not filled # by the metrics. if starttime not in self . timeline or self . timeline [ starttime ] == 0 : self . timeline [ starttime ] = constant starttime += 60
4558	def make_segments ( strip , length ) : if len ( strip ) % length : raise ValueError ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except ValueError : return s
7054	def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys . path . append ( os . path . dirname ( module ) ) importedok = importlib . import_module ( os . path . basename ( module . replace ( '.py' , '' ) ) ) else : importedok = importlib . import_module ( module ) except Exception as e : LOGEXCEPTION ( 'could not import the module: %s for LC format: %s. ' 'check the file path or fully qualified module name?' % ( module , formatkey ) ) importedok = False return importedok
3564	def read_value ( self , timeout_sec = TIMEOUT_SEC ) : # Kick off a query to read the value of the characteristic, then wait # for the result to return asyncronously. self . _value_read . clear ( ) self . _device . _peripheral . readValueForCharacteristic_ ( self . _characteristic ) if not self . _value_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting to read characteristic value!' ) return self . _characteristic . value ( )
6044	def padded_grid_from_shape_psf_shape_and_pixel_scale ( cls , shape , psf_shape , pixel_scale ) : padded_shape = ( shape [ 0 ] + psf_shape [ 0 ] - 1 , shape [ 1 ] + psf_shape [ 1 ] - 1 ) padded_regular_grid = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( padded_shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) padded_mask = msk . Mask . unmasked_for_shape_and_pixel_scale ( shape = padded_shape , pixel_scale = pixel_scale ) return PaddedRegularGrid ( arr = padded_regular_grid , mask = padded_mask , image_shape = shape )
5087	def has_implicit_access_to_enrollment_api ( user , obj ) : # pylint: disable=unused-argument request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE , obj )
5316	def setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : if colormode : self . colormode = colormode if colorpalette : if extend_colors : self . update_palette ( colorpalette ) else : self . colorpalette = colorpalette
7734	def nfkc ( data ) : if isinstance ( data , list ) : data = u"" . join ( data ) return unicodedata . normalize ( "NFKC" , data )
13333	def path_resolver ( resolver , path ) : path = unipath ( path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
11021	def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
6296	def instance ( self , program : moderngl . Program ) -> moderngl . VertexArray : vao = self . vaos . get ( program . glo ) if vao : return vao program_attributes = [ name for name , attr in program . _members . items ( ) if isinstance ( attr , moderngl . Attribute ) ] # Make sure all attributes are covered for attrib_name in program_attributes : # Ignore built in attributes for now if attrib_name . startswith ( 'gl_' ) : continue # Do we have a buffer mapping to this attribute? if not sum ( buffer . has_attribute ( attrib_name ) for buffer in self . buffers ) : raise VAOError ( "VAO {} doesn't have attribute {} for program {}" . format ( self . name , attrib_name , program . name ) ) vao_content = [ ] # Pick out the attributes we can actually map for buffer in self . buffers : content = buffer . content ( program_attributes ) if content : vao_content . append ( content ) # Any attribute left is not accounted for if program_attributes : for attrib_name in program_attributes : if attrib_name . startswith ( 'gl_' ) : continue raise VAOError ( "Did not find a buffer mapping for {}" . format ( [ n for n in program_attributes ] ) ) # Create the vao if self . _index_buffer : vao = context . ctx ( ) . vertex_array ( program , vao_content , self . _index_buffer , self . _index_element_size ) else : vao = context . ctx ( ) . vertex_array ( program , vao_content ) self . vaos [ program . glo ] = vao return vao
2327	def orient_undirected_graph ( self , data , graph ) : # Building setup w/ arguments. self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) whitelist = DataFrame ( list ( nx . edges ( graph ) ) , columns = [ "from" , "to" ] ) blacklist = DataFrame ( list ( nx . edges ( nx . DiGraph ( DataFrame ( - nx . adj_matrix ( graph , weight = None ) . to_dense ( ) + 1 , columns = list ( graph . nodes ( ) ) , index = list ( graph . nodes ( ) ) ) ) ) ) , columns = [ "from" , "to" ] ) results = self . _run_bnlearn ( data , whitelist = whitelist , blacklist = blacklist , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
8412	def numeric_to_timedelta ( self , numerics ) : if self . package == 'pandas' : return [ self . type ( int ( x * self . factor ) , units = 'ns' ) for x in numerics ] else : return [ self . type ( seconds = x * self . factor ) for x in numerics ]
4431	async def ensure_voice ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_connected : if not ctx . author . voice or not ctx . author . voice . channel : await ctx . send ( 'You aren\'t connected to any voice channel.' ) raise commands . CommandInvokeError ( 'Author not connected to voice channel.' ) permissions = ctx . author . voice . channel . permissions_for ( ctx . me ) if not permissions . connect or not permissions . speak : await ctx . send ( 'Missing permissions `CONNECT` and/or `SPEAK`.' ) raise commands . CommandInvokeError ( 'Bot has no permissions CONNECT and/or SPEAK' ) player . store ( 'channel' , ctx . channel . id ) await player . connect ( ctx . author . voice . channel . id ) else : if player . connected_channel . id != ctx . author . voice . channel . id : return await ctx . send ( 'Join my voice channel!' )
219	def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f"Package {package!r} could not be found." assert ( spec . origin is not None ) , "Directory 'statics' in package {package!r} could not be found." directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) assert os . path . isdir ( directory ) , "Directory 'statics' in package {package!r} could not be found." directories . append ( directory ) return directories
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
98	def quokka_polygons ( size = None , extract = None ) : # TODO get rid of this deferred import from imgaug . augmentables . polys import Polygon , PolygonsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) polygons = [ ] for poly_json in json_dict [ "polygons" ] : polygons . append ( Polygon ( [ ( point [ "x" ] - left , point [ "y" ] - top ) for point in poly_json [ "keypoints" ] ] ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) psoi = PolygonsOnImage ( polygons , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) psoi = psoi . on ( shape_resized ) return psoi
9681	def config2 ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x3D ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 9 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "AMSamplingInterval" ] = self . _16bit_unsigned ( config [ 0 ] , config [ 1 ] ) data [ "AMIdleIntervalCount" ] = self . _16bit_unsigned ( config [ 2 ] , config [ 3 ] ) data [ 'AMFanOnIdle' ] = config [ 4 ] data [ 'AMLaserOnIdle' ] = config [ 5 ] data [ 'AMMaxDataArraysInFile' ] = self . _16bit_unsigned ( config [ 6 ] , config [ 7 ] ) data [ 'AMOnlySavePMData' ] = config [ 8 ] sleep ( 0.1 ) return data
8492	def main ( ) : parser = argparse . ArgumentParser ( description = "Helper for working with " "pyconfigs" ) target_group = parser . add_mutually_exclusive_group ( ) target_group . add_argument ( '-f' , '--filename' , help = "parse an individual file or directory" , metavar = 'F' ) target_group . add_argument ( '-m' , '--module' , help = "parse a package or module, recursively looking inside it" , metavar = 'M' ) parser . add_argument ( '-v' , '--view-call' , help = "show the actual pyconfig call made (default: show namespace)" , action = 'store_true' ) parser . add_argument ( '-l' , '--load-configs' , help = "query the currently set value for each key found" , action = 'store_true' ) key_group = parser . add_mutually_exclusive_group ( ) key_group . add_argument ( '-a' , '--all' , help = "show keys which don't have defaults set" , action = 'store_true' ) key_group . add_argument ( '-k' , '--only-keys' , help = "show a list of discovered keys without values" , action = 'store_true' ) parser . add_argument ( '-n' , '--natural-sort' , help = "sort by filename and line (default: alphabetical by key)" , action = 'store_true' ) parser . add_argument ( '-s' , '--source' , help = "show source annotations (implies --natural-sort)" , action = 'store_true' ) parser . add_argument ( '-c' , '--color' , help = "toggle output colors (default: %s)" % bool ( pygments ) , action = 'store_const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse_args ( ) if args . color and not pygments : _error ( "Pygments is required for color output.\n" " pip install pygments" ) if args . module : _handle_module ( args ) if args . filename : _handle_file ( args )
11309	def map_to_dictionary ( self , url , obj , * * kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider_url , provider_name = self . provider_from_url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider_name' : provider_name , 'provider_url' : provider_url , 'type' : self . resource_type } # a hook self . preprocess ( obj , mapping , * * kwargs ) # resize image if we have a photo, otherwise use the given maximums if self . resource_type == 'photo' and self . get_image ( obj ) : self . resize_photo ( obj , mapping , maxwidth , maxheight ) elif self . resource_type in ( 'video' , 'rich' , 'photo' ) : width , height = size_to_nearest ( maxwidth , maxheight , self . _meta . valid_sizes , self . _meta . force_fit ) mapping . update ( width = width , height = height ) # create a thumbnail if self . get_image ( obj ) : self . thumbnail ( obj , mapping ) # map attributes to the mapping dictionary. if the attribute is # a callable, it must have an argument signature of # (self, obj) for attr in ( 'title' , 'author_name' , 'author_url' , 'html' ) : self . map_attr ( mapping , attr , obj ) # fix any urls if 'url' in mapping : mapping [ 'url' ] = relative_to_full ( mapping [ 'url' ] , url ) if 'thumbnail_url' in mapping : mapping [ 'thumbnail_url' ] = relative_to_full ( mapping [ 'thumbnail_url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render_html ( obj , context = Context ( mapping ) ) # a hook self . postprocess ( obj , mapping , * * kwargs ) return mapping
3908	def put ( self , coro ) : # Avoid logging when a coroutine is queued or executed to avoid log # spam from coroutines that are started on every keypress. assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
11791	def mrv ( assignment , csp ) : return argmin_random_tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num_legal_values ( csp , var , assignment ) )
5104	def lines_scatter_args ( self , line_kwargs = None , scatter_kwargs = None , pos = None ) : if pos is not None : self . set_pos ( pos ) elif self . pos is None : self . set_pos ( ) edge_pos = [ 0 for e in self . edges ( ) ] for e in self . edges ( ) : ei = self . edge_index [ e ] edge_pos [ ei ] = ( self . pos [ e [ 0 ] ] , self . pos [ e [ 1 ] ] ) line_collecton_kwargs = { 'segments' : edge_pos , 'colors' : self . edge_color , 'linewidths' : ( 1 , ) , 'antialiaseds' : ( 1 , ) , 'linestyle' : 'solid' , 'transOffset' : None , 'cmap' : plt . cm . ocean_r , 'pickradius' : 5 , 'zorder' : 0 , 'facecolors' : None , 'norm' : None , 'offsets' : None , 'offset_position' : 'screen' , 'hatch' : None , } scatter_kwargs_ = { 'x' : self . pos [ : , 0 ] , 'y' : self . pos [ : , 1 ] , 's' : 50 , 'c' : self . vertex_fill_color , 'alpha' : None , 'norm' : None , 'vmin' : None , 'vmax' : None , 'marker' : 'o' , 'zorder' : 2 , 'cmap' : plt . cm . ocean_r , 'linewidths' : 1 , 'edgecolors' : self . vertex_color , 'facecolors' : None , 'antialiaseds' : None , 'offset_position' : 'screen' , 'hatch' : None , } line_kwargs = { } if line_kwargs is None else line_kwargs scatter_kwargs = { } if scatter_kwargs is None else scatter_kwargs for key , value in line_kwargs . items ( ) : if key in line_collecton_kwargs : line_collecton_kwargs [ key ] = value for key , value in scatter_kwargs . items ( ) : if key in scatter_kwargs_ : scatter_kwargs_ [ key ] = value return line_collecton_kwargs , scatter_kwargs_
4709	def power_btn ( self , interval = 200 ) : if self . __power_btn_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_BTN" ) return 1 return self . __press ( self . __power_btn_port , interval = interval )
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , * * kwargs ) : path = self . BezierPath ( * * kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
5404	def _get_delocalization_env ( self , outputs , user_project ) : # Add variables for paths that need to be delocalized, for example: # OUTPUT_COUNT: 1 # OUTPUT_0: MY_OUTPUT_FILE # OUTPUT_RECURSIVE_0: 0 # OUTPUT_SRC_0: gs://mybucket/mypath/myfile # OUTPUT_DST_0: /mnt/data/outputs/mybucket/mypath/myfile non_empty_outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT_COUNT' : str ( len ( non_empty_outputs ) ) } for idx , var in enumerate ( non_empty_outputs ) : env [ 'OUTPUT_{}' . format ( idx ) ] = var . name env [ 'OUTPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT_SRC_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) # For wildcard paths, the destination must be a directory if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
11141	def load_repository ( self , path , verbose = True , ntrials = 3 ) : assert isinstance ( ntrials , int ) , "ntrials must be integer" assert ntrials > 0 , "ntrials must be >0" repo = None for _trial in range ( ntrials ) : try : self . __load_repository ( path = path , verbose = True ) except Exception as err1 : try : from . OldRepository import Repository REP = Repository ( path ) except Exception as err2 : #traceback.print_exc() error = "Unable to load repository using neiher new style (%s) nor old style (%s)" % ( err1 , err2 ) if self . DEBUG_PRINT_FAILED_TRIALS : print ( "Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute" % ( _trial , inspect . stack ( ) [ 1 ] [ 3 ] , str ( error ) ) ) else : error = None repo = REP break else : error = None repo = self break # check and return assert error is None , error return repo
6672	def is_dir ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isdir ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
6261	def swap_buffers ( self ) : self . frames += 1 glfw . swap_buffers ( self . window ) self . poll_events ( )
8203	def set_size ( self , size ) : if self . size is None : self . size = size return size else : return self . size
9351	def number ( type = None , length = None , prefixes = None ) : # select credit card type if type and type in CARDS : card = type else : card = random . choice ( list ( CARDS . keys ( ) ) ) # select a credit card number's prefix if not prefixes : prefixes = CARDS [ card ] [ 'prefixes' ] prefix = random . choice ( prefixes ) # select length of the credit card number, if it's not set if not length : length = CARDS [ card ] [ 'length' ] # generate all digits but the last one result = str ( prefix ) for d in range ( length - len ( str ( prefix ) ) ) : result += str ( basic . number ( ) ) last_digit = check_digit ( int ( result ) ) return int ( result [ : - 1 ] + str ( last_digit ) )
1352	def make_success_response ( self , result ) : response = self . make_response ( constants . RESPONSE_STATUS_SUCCESS ) response [ constants . RESPONSE_KEY_RESULT ] = result return response
5547	def _get_zoom ( zoom , input_raster , pyramid_type ) : if not zoom : minzoom = 1 maxzoom = get_best_zoom_level ( input_raster , pyramid_type ) elif len ( zoom ) == 1 : minzoom = zoom [ 0 ] maxzoom = zoom [ 0 ] elif len ( zoom ) == 2 : if zoom [ 0 ] < zoom [ 1 ] : minzoom = zoom [ 0 ] maxzoom = zoom [ 1 ] else : minzoom = zoom [ 1 ] maxzoom = zoom [ 0 ] return minzoom , maxzoom
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
4503	def SPI ( ledtype = None , num = 0 , * * kwargs ) : from . . . project . types . ledtype import make if ledtype is None : raise ValueError ( 'Must provide ledtype value!' ) ledtype = make ( ledtype ) if num == 0 : raise ValueError ( 'Must provide num value >0!' ) if ledtype not in SPI_DRIVERS . keys ( ) : raise ValueError ( '{} is not a valid LED type.' . format ( ledtype ) ) return SPI_DRIVERS [ ledtype ] ( num , * * kwargs )
436	def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : if shape is None : shape = [ 28 , 28 ] import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) fig = plt . figure ( fig_idx ) # show all feature images n_units = W . shape [ 1 ] num_r = int ( np . sqrt ( n_units ) ) # 每行显示的个数 若25个hidden unit -> 每行显示5个 num_c = int ( np . ceil ( n_units / num_r ) ) count = int ( 1 ) for _row in range ( 1 , num_r + 1 ) : for _col in range ( 1 , num_c + 1 ) : if count > n_units : break fig . add_subplot ( num_r , num_c , count ) # ------------------------------------------------------------ # plt.imshow(np.reshape(W[:,count-1],(28,28)), cmap='gray') # ------------------------------------------------------------ feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) # feature[feature<0.0001] = 0 # value threshold # if count == 1 or count == 2: # print(np.mean(feature)) # if np.std(feature) < 0.03: # condition threshold # feature = np.zeros_like(feature) # if np.mean(feature) < -0.015: # condition threshold # feature = np.zeros_like(feature) plt . imshow ( np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = "nearest" ) # , vmin=np.min(feature), vmax=np.max(feature)) # plt.title(name) # ------------------------------------------------------------ # plt.imshow(np.reshape(W[:,count-1] ,(np.sqrt(size),np.sqrt(size))), cmap='gray', interpolation="nearest") plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) # distable tick plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
8019	async def websocket_send ( self , message , stream_name ) : text = message . get ( "text" ) # todo what to do on binary! json = await self . decode_json ( text ) data = { "stream" : stream_name , "payload" : json } await self . send_json ( data )
3789	def property_derivative_T ( self , T , P , zs , ws , order = 1 ) : sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_T ( T , P , zs , ws , method , order ) except : pass return None
594	def _cacheSequenceInfoType ( self ) : hasReset = self . resetFieldName is not None hasSequenceId = self . sequenceIdFieldName is not None if hasReset and not hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_RESET_ONLY self . _prevSequenceId = 0 elif not hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_SEQUENCEID_ONLY self . _prevSequenceId = None elif hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_BOTH else : self . _sequenceInfoType = self . SEQUENCEINFO_NONE
6027	def voronoi_from_pixel_centers ( pixel_centers ) : return scipy . spatial . Voronoi ( np . asarray ( [ pixel_centers [ : , 1 ] , pixel_centers [ : , 0 ] ] ) . T , qhull_options = 'Qbb Qc Qx Qm' )
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
8919	def _get_service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed_service_types : self . params [ "service" ] = value else : raise OWSInvalidParameterValue ( "Service %s is not supported" % value , value = "service" ) else : raise OWSMissingParameterValue ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
5985	def inversion_psf_shape_tag_from_inversion_psf_shape ( inversion_psf_shape ) : if inversion_psf_shape is None : return '' else : y = str ( inversion_psf_shape [ 0 ] ) x = str ( inversion_psf_shape [ 1 ] ) return ( '_inv_psf_' + y + 'x' + x )
9852	def _export_dx ( self , filename , type = None , typequote = '"' , * * kwargs ) : root , ext = os . path . splitext ( filename ) filename = root + '.dx' comments = [ 'OpenDX density file written by gridDataFormats.Grid.export()' , 'File format: http://opendx.sdsc.edu/docs/html/pages/usrgu068.htm#HDREDF' , 'Data are embedded in the header and tied to the grid positions.' , 'Data is written in C array order: In grid[x,y,z] the axis z is fastest' , 'varying, then y, then finally x, i.e. z is the innermost loop.' ] # write metadata in comments section if self . metadata : comments . append ( 'Meta data stored with the python Grid object:' ) for k in self . metadata : comments . append ( ' ' + str ( k ) + ' = ' + str ( self . metadata [ k ] ) ) comments . append ( '(Note: the VMD dx-reader chokes on comments below this line)' ) components = dict ( positions = OpenDX . gridpositions ( 1 , self . grid . shape , self . origin , self . delta ) , connections = OpenDX . gridconnections ( 2 , self . grid . shape ) , data = OpenDX . array ( 3 , self . grid , type = type , typequote = typequote ) , ) dx = OpenDX . field ( 'density' , components = components , comments = comments ) dx . write ( filename )
13690	def remove_peer ( self , peer ) : if type ( peer ) == list : for x in peer : check_url ( x ) for i in self . PEERS : if x in i : self . PEERS . remove ( i ) elif type ( peer ) == str : check_url ( peer ) for i in self . PEERS : if peer == i : self . PEERS . remove ( i ) else : raise ValueError ( 'peer paramater did not pass url validation' )
4911	def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
5827	def dataset_search ( self , dataset_returning_query ) : self . _validate_search_query ( dataset_returning_query ) return self . _execute_search_query ( dataset_returning_query , DatasetSearchResult )
872	def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )
7400	def up ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__lt = self . order ) . order_by ( '-order' ) )
6208	def save_photon_hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge_da ( ) data = self . _make_photon_hdf5 ( identity = identity ) phc . hdf5 . save_photon_hdf5 ( data , h5_fname = str ( filepath ) , overwrite = overwrite )
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
12329	def compute_sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
4492	def fetch ( args ) : storage , remote_path = split_storage ( args . remote ) local_path = args . local if local_path is None : _ , local_path = os . path . split ( remote_path ) local_path_exists = os . path . exists ( local_path ) if local_path_exists and not args . force and not args . update : sys . exit ( "Local file %s already exists, not overwriting." % local_path ) directory , _ = os . path . split ( local_path ) if directory : makedirs ( directory , exist_ok = True ) osf = _setup_osf ( args ) project = osf . project ( args . project ) store = project . storage ( storage ) for file_ in store . files : if norm_remote_path ( file_ . path ) == remote_path : if local_path_exists and not args . force and args . update : if file_ . hashes . get ( 'md5' ) == checksum ( local_path ) : print ( "Local file %s already matches remote." % local_path ) break with open ( local_path , 'wb' ) as fp : file_ . write_to ( fp ) # only fetching one file so we are done break
7110	def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq in zip ( X , y ) : trainer . append ( xseq , yseq ) trainer . set_params ( self . params ) if self . filename : filename = self . filename else : filename = 'model.tmp' trainer . train ( filename ) tagger = pycrfsuite . Tagger ( ) tagger . open ( filename ) self . estimator = tagger
12581	def setup_logging ( log_config_file = op . join ( op . dirname ( __file__ ) , 'logger.yml' ) , log_default_level = LOG_LEVEL , env_key = MODULE_NAME . upper ( ) + '_LOG_CFG' ) : path = log_config_file value = os . getenv ( env_key , None ) if value : path = value if op . exists ( path ) : log_cfg = yaml . load ( read ( path ) . format ( MODULE_NAME ) ) logging . config . dictConfig ( log_cfg ) #print('Started logging using config file {0}.'.format(path)) else : logging . basicConfig ( level = log_default_level ) #print('Started default logging. Could not find config file ' # 'in {0}.'.format(path)) log = logging . getLogger ( __name__ ) log . debug ( 'Start logging.' )
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
13186	def download_observations ( observer_code ) : page_number = 1 observations = [ ] while True : logger . info ( 'Downloading page %d...' , page_number ) response = requests . get ( WEBOBS_RESULTS_URL , params = { 'obscode' : observer_code , 'num_results' : 200 , 'obs_types' : 'all' , 'page' : page_number , } ) logger . debug ( response . request . url ) parser = WebObsResultsParser ( response . text ) observations . extend ( parser . get_observations ( ) ) # kinda silly, but there's no need for lxml machinery here if '>Next</a>' not in response . text : break page_number += 1 return observations
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
3473	def build_reaction_string ( self , use_metabolite_names = False ) : def format ( number ) : return "" if number == 1 else str ( number ) . rstrip ( "." ) + " " id_type = 'id' if use_metabolite_names : id_type = 'name' reactant_bits = [ ] product_bits = [ ] for met in sorted ( self . _metabolites , key = attrgetter ( "id" ) ) : coefficient = self . _metabolites [ met ] name = str ( getattr ( met , id_type ) ) if coefficient >= 0 : product_bits . append ( format ( coefficient ) + name ) else : reactant_bits . append ( format ( abs ( coefficient ) ) + name ) reaction_string = ' + ' . join ( reactant_bits ) if not self . reversibility : if self . lower_bound < 0 and self . upper_bound <= 0 : reaction_string += ' <-- ' else : reaction_string += ' --> ' else : reaction_string += ' <=> ' reaction_string += ' + ' . join ( product_bits ) return reaction_string
11962	def _dot_to_dec ( ip , check = True ) : if check and not is_dot ( ip ) : raise ValueError ( '_dot_to_dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
3897	def generate_message_doc ( message_descriptor , locations , path , name_prefix = '' ) : # message_type is 4 prefixed_name = name_prefix + message_descriptor . name print ( make_subsection ( prefixed_name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for field_index , field in enumerate ( message_descriptor . field ) : field_location = locations [ path + ( 2 , field_index ) ] if field . type not in [ 11 , 14 ] : type_str = TYPE_TO_STR [ field . type ] else : type_str = make_link ( field . type_name . lstrip ( '.' ) ) row_tuples . append ( ( make_code ( field . name ) , field . number , type_str , LABEL_TO_STR [ field . label ] , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Field' , 'Number' , 'Type' , 'Label' , 'Description' ) , row_tuples ) # Generate nested messages nested_types = enumerate ( message_descriptor . nested_type ) for index , nested_message_desc in nested_types : generate_message_doc ( nested_message_desc , locations , path + ( 3 , index ) , name_prefix = prefixed_name + '.' ) # Generate nested enums for index , nested_enum_desc in enumerate ( message_descriptor . enum_type ) : generate_enum_doc ( nested_enum_desc , locations , path + ( 4 , index ) , name_prefix = prefixed_name + '.' )
791	def jobSetCompleted ( self , jobID , completionReason , completionMsg , useConnectionID = True ) : # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' completion_reason=%%s, ' ' completion_msg=%%s, ' ' end_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of jobID=%s to " "completed, but this job could not be found or " "belongs to some other CJM" % ( jobID ) )
6368	def precision_gain ( self ) : if self . population ( ) == 0 : return float ( 'NaN' ) random_precision = self . cond_pos_pop ( ) / self . population ( ) return self . precision ( ) / random_precision
8320	def parse_categories ( self , markup ) : categories = [ ] m = re . findall ( self . re [ "category" ] , markup ) for category in m : category = category . split ( "|" ) page = category [ 0 ] . strip ( ) display = u"" if len ( category ) > 1 : display = category [ 1 ] . strip ( ) #if not categories.has_key(page): # categories[page] = WikipediaLink(page, u"", display) if not page in categories : categories . append ( page ) return categories
1922	def resolve ( self , symbol ) : with open ( self . binary_path , 'rb' ) as f : elffile = ELFFile ( f ) # iterate over sections and identify symbol table section for section in elffile . iter_sections ( ) : if not isinstance ( section , SymbolTableSection ) : continue # get list of symbols by name symbols = section . get_symbol_by_name ( symbol ) if not symbols : continue # return first indexed memory address for the symbol, return symbols [ 0 ] . entry [ 'st_value' ] raise ValueError ( f"The {self.binary_path} ELFfile does not contain symbol {symbol}" )
11849	def percept ( self , agent ) : return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
12201	def from_yamlfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . from_yamlstring ( fp . read ( ) , selector_handler = selector_handler , strict = strict , debug = debug )
3002	def start ( self ) : if self . extra_args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . _toggle_value : nbextensions . install_nbextension_python ( _pkg_name , overwrite = True , symlink = False , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) else : nbextensions . uninstall_nbextension_python ( _pkg_name , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) self . toggle_nbextension_python ( _pkg_name ) self . toggle_server_extension_python ( _pkg_name )
11395	def load_class ( path ) : package , klass = path . rsplit ( '.' , 1 ) module = import_module ( package ) return getattr ( module , klass )
13147	def shrink_indexes_in_place ( self , triples ) : _ent_roots = self . UnionFind ( self . _ent_id ) _rel_roots = self . UnionFind ( self . _rel_id ) for t in triples : _ent_roots . add ( t . head ) _ent_roots . add ( t . tail ) _rel_roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = _ent_roots . find ( t . head ) r = _rel_roots . find ( t . relation ) t = _ent_roots . find ( t . tail ) triples [ i ] = kgedata . TripleIndex ( h , r , t ) ents = bidict ( ) available_ent_idx = 0 for previous_idx , ent_exist in enumerate ( _ent_roots . roots ( ) ) : if not ent_exist : self . _ents . inverse . pop ( previous_idx ) else : ents [ self . _ents . inverse [ previous_idx ] ] = available_ent_idx available_ent_idx += 1 rels = bidict ( ) available_rel_idx = 0 for previous_idx , rel_exist in enumerate ( _rel_roots . roots ( ) ) : if not rel_exist : self . _rels . inverse . pop ( previous_idx ) else : rels [ self . _rels . inverse [ previous_idx ] ] = available_rel_idx available_rel_idx += 1 self . _ents = ents self . _rels = rels self . _ent_id = available_ent_idx self . _rel_id = available_rel_idx
3317	def digest_auth_user ( self , realm , user_name , environ ) : user = self . _get_realm_entry ( realm , user_name ) if user is None : return False password = user . get ( "password" ) environ [ "wsgidav.auth.roles" ] = user . get ( "roles" , [ ] ) return self . _compute_http_digest_a1 ( realm , user_name , password )
10117	def extend ( self , items , replace = True ) : if isinstance ( items , dict ) or isinstance ( items , SortableDict ) : items = list ( items . items ( ) ) for ( key , value ) in items : self . append ( key , value , replace = replace )
1072	def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist )
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID # already a parent if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent # found the actual parent return None
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
5598	def execute ( mp , resampling = "nearest" , scale_method = None , scales_minmax = None ) : with mp . open ( "raster" , resampling = resampling ) as raster_file : # exit if input tile is empty if raster_file . is_empty ( ) : return "empty" # actually read data and iterate through bands scaled = ( ) mask = ( ) raster_data = raster_file . read ( ) if raster_data . ndim == 2 : raster_data = ma . expand_dims ( raster_data , axis = 0 ) if not scale_method : scales_minmax = [ ( i , i ) for i in range ( len ( raster_data ) ) ] for band , ( scale_min , scale_max ) in zip ( raster_data , scales_minmax ) : if scale_method in [ "dtype_scale" , "minmax_scale" ] : scaled += ( _stretch_array ( band , scale_min , scale_max ) , ) elif scale_method == "crop" : scaled += ( np . clip ( band , scale_min , scale_max ) , ) else : scaled += ( band , ) mask += ( band . mask , ) return ma . masked_array ( np . stack ( scaled ) , np . stack ( mask ) )
8930	def workdir_is_clean ( self , quiet = False ) : # Update the index self . run ( 'git update-index -q --ignore-submodules --refresh' , * * RUN_KWARGS ) unchanged = True # Disallow unstaged changes in the working tree try : self . run ( 'git diff-files --quiet --ignore-submodules --' , report_error = False , * * RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'You have unstaged changes!' ) self . run ( 'git diff-files --name-status -r --ignore-submodules -- >&2' , * * RUN_KWARGS ) # Disallow uncommitted changes in the index try : self . run ( 'git diff-index --cached --quiet HEAD --ignore-submodules --' , report_error = False , * * RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'Your index contains uncommitted changes!' ) self . run ( 'git diff-index --cached --name-status -r --ignore-submodules HEAD -- >&2' , * * RUN_KWARGS ) return unchanged
7475	def dask_chroms ( data , samples ) : ## example concatenating with dask h5s = [ os . path . join ( data . dirs . across , s . name + ".tmp.h5" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from_array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) ## max chrom (should we check for variable hits? if so, things can get wonk) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] ## max pos maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] ## min pos mask = stack == 0 stack [ mask ] = 9223372036854775807 ## max int64 value minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to_hdf5 ( data . clust_database , "/chroms" ) ## close the h5 handles _ = [ i . close ( ) for i in handles ]
1462	def new_source ( self , source ) : source_streamlet = None if callable ( source ) : source_streamlet = SupplierStreamlet ( source ) elif isinstance ( source , Generator ) : source_streamlet = GeneratorStreamlet ( source ) else : raise RuntimeError ( "Builder's new source has to be either a Generator or a function" ) self . _sources . append ( source_streamlet ) return source_streamlet
7609	def get_all_locations ( self , timeout : int = None ) : url = self . api . LOCATIONS return self . _get_model ( url , timeout = timeout )
2213	def delete ( path , verbose = False ) : if not os . path . exists ( path ) : # if the file does exists and is not a broken link if os . path . islink ( path ) : if verbose : # nocover print ( 'Deleting broken link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : # nocover # Only on windows will a file be a directory and not exist if verbose : print ( 'Deleting broken directory link="{}"' . format ( path ) ) os . rmdir ( path ) elif os . path . isfile ( path ) : # nocover # This is a windows only case if verbose : print ( 'Deleting broken file link="{}"' . format ( path ) ) os . unlink ( path ) else : if verbose : # nocover print ( 'Not deleting non-existant path="{}"' . format ( path ) ) else : if os . path . islink ( path ) : if verbose : # nocover print ( 'Deleting symbolic link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isfile ( path ) : if verbose : # nocover print ( 'Deleting file="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : if verbose : # nocover print ( 'Deleting directory="{}"' . format ( path ) ) if sys . platform . startswith ( 'win32' ) : # nocover # Workaround bug that prevents shutil from working if # the directory contains junctions from ubelt import _win32_links _win32_links . _win32_rmtree ( path , verbose = verbose ) else : import shutil shutil . rmtree ( path )
729	def prettyPrintPattern ( self , bits , verbosity = 1 ) : numberMap = self . numberMapForBits ( bits ) text = "" numberList = [ ] numberItems = sorted ( numberMap . iteritems ( ) , key = lambda ( number , bits ) : len ( bits ) , reverse = True ) for number , bits in numberItems : if verbosity > 2 : strBits = [ str ( n ) for n in bits ] numberText = "{0} (bits: {1})" . format ( number , "," . join ( strBits ) ) elif verbosity > 1 : numberText = "{0} ({1} bits)" . format ( number , len ( bits ) ) else : numberText = str ( number ) numberList . append ( numberText ) text += "[{0}]" . format ( ", " . join ( numberList ) ) return text
5466	def get_action_environment ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'environment' )
3152	def delete ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
9143	def _iterate_managers ( connection , skip ) : for idx , name , manager_cls in _iterate_manage_classes ( skip ) : if name in skip : continue try : manager = manager_cls ( connection = connection ) except TypeError as e : click . secho ( f'Could not instantiate {name}: {e}' , fg = 'red' ) else : yield idx , name , manager
7902	def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . set_stream ( stream )
1286	def build_metagraph_list ( self ) : ops = [ ] self . ignore_unknown_dtypes = True for key in sorted ( self . meta_params ) : value = self . convert_data_to_string ( self . meta_params [ key ] ) if len ( value ) == 0 : continue if isinstance ( value , str ) : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) else : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) return ops
7337	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : sequence_dict = check_sequence_dictionary ( sequence_dict ) peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) # take each mutated sequence in the dataframe # and general MHC binding scores for all k-mer substrings binding_predictions = [ ] expected_peptides = set ( [ ] ) normalized_alleles = [ ] for key , amino_acid_sequence in sequence_dict . items ( ) : for l in peptide_lengths : for i in range ( len ( amino_acid_sequence ) - l + 1 ) : expected_peptides . add ( amino_acid_sequence [ i : i + l ] ) self . _check_peptide_inputs ( expected_peptides ) for allele in self . alleles : # IEDB MHCII predictor expects DRA1 to be omitted. allele = normalize_allele_name ( allele , omit_dra1 = True ) normalized_alleles . append ( allele ) request = self . _get_iedb_request_params ( amino_acid_sequence , allele ) logger . info ( "Calling IEDB (%s) with request %s" , self . url , request ) response_df = _query_iedb ( request , self . url ) for _ , row in response_df . iterrows ( ) : binding_predictions . append ( BindingPrediction ( source_sequence_name = key , offset = row [ 'start' ] - 1 , allele = row [ 'allele' ] , peptide = row [ 'peptide' ] , affinity = row [ 'ic50' ] , percentile_rank = row [ 'rank' ] , prediction_method_name = "iedb-" + self . prediction_method ) ) self . _check_results ( binding_predictions , alleles = normalized_alleles , peptides = expected_peptides ) return BindingPredictionCollection ( binding_predictions )
918	def warning ( self , msg , * args , * * kwargs ) : self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
12046	def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
9052	def covariance ( self ) : from numpy_sugar . linalg import ddot , sum2diag Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] return sum2diag ( dot ( ddot ( Q0 , self . v0 * S0 ) , Q0 . T ) , self . v1 )
12792	def post ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None , listener = None ) : return self . _fetch ( "POST" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , listener = listener , full_return = True )
9222	def reverse_guard ( lst ) : rev = { '<' : '>=' , '>' : '=<' , '>=' : '<' , '=<' : '>' } return [ rev [ l ] if l in rev else l for l in lst ]
13723	def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
6881	def _parse_csv_header_lcc_csv_v1 ( headerlines ) : # the first three lines indicate the format name, comment char, separator commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] # next, find the indices of the various LC sections metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator
6576	def populate_fields ( api_client , instance , data ) : for key , value in instance . __class__ . _fields . items ( ) : default = getattr ( value , "default" , None ) newval = data . get ( value . field , default ) if isinstance ( value , SyntheticField ) : newval = value . formatter ( api_client , data , newval ) setattr ( instance , key , newval ) continue model_class = getattr ( value , "model" , None ) if newval and model_class : if isinstance ( newval , list ) : newval = model_class . from_json_list ( api_client , newval ) else : newval = model_class . from_json ( api_client , newval ) if newval and value . formatter : newval = value . formatter ( api_client , newval ) setattr ( instance , key , newval )
757	def _allow_new_attributes ( f ) : def decorated ( self , * args , * * kw ) : """The decorated function that replaces __init__() or __setstate__() """ # Run the original function if not hasattr ( self , '_canAddAttributes' ) : self . __dict__ [ '_canAddAttributes' ] = 1 else : self . _canAddAttributes += 1 assert self . _canAddAttributes >= 1 # Save add attribute counter count = self . _canAddAttributes f ( self , * args , * * kw ) # Restore _CanAddAttributes if deleted from dict (can happen in __setstte__) if hasattr ( self , '_canAddAttributes' ) : self . _canAddAttributes -= 1 else : self . _canAddAttributes = count - 1 assert self . _canAddAttributes >= 0 if self . _canAddAttributes == 0 : del self . _canAddAttributes decorated . __doc__ = f . __doc__ decorated . __name__ = f . __name__ return decorated
1758	def _raw_read ( self , where : int , size = 1 ) -> bytes : map = self . memory . map_containing ( where ) start = map . _get_offset ( where ) mapType = type ( map ) if mapType is FileMap : end = map . _get_offset ( where + size ) if end > map . _mapped_size : logger . warning ( f"Missing {end - map._mapped_size} bytes at the end of {map._filename}" ) raw_data = map . _data [ map . _get_offset ( where ) : min ( end , map . _mapped_size ) ] if len ( raw_data ) < end : raw_data += b'\x00' * ( end - len ( raw_data ) ) data = b'' for offset in sorted ( map . _overlay . keys ( ) ) : data += raw_data [ len ( data ) : offset ] data += map . _overlay [ offset ] data += raw_data [ len ( data ) : ] elif mapType is AnonMap : data = bytes ( map . _data [ start : start + size ] ) else : data = b'' . join ( self . memory [ where : where + size ] ) assert len ( data ) == size , 'Raw read resulted in wrong data read which should never happen' return data
2299	def predict_features ( self , df_features , df_target , nh = 20 , idx = 0 , dropout = 0. , activation_function = th . nn . ReLU , lr = 0.01 , l1 = 0.1 , batch_size = - 1 , train_epochs = 1000 , test_epochs = 1000 , device = None , verbose = None , nb_runs = 3 ) : device , verbose = SETTINGS . get_default ( ( 'device' , device ) , ( 'verbose' , verbose ) ) x = th . FloatTensor ( scale ( df_features . values ) ) . to ( device ) y = th . FloatTensor ( scale ( df_target . values ) ) . to ( device ) out = [ ] for i in range ( nb_runs ) : model = FSGNN_model ( [ x . size ( ) [ 1 ] + 1 , nh , 1 ] , dropout = dropout , activation_function = activation_function ) . to ( device ) out . append ( model . train ( x , y , lr = 0.01 , l1 = 0.1 , batch_size = - 1 , train_epochs = train_epochs , test_epochs = test_epochs , device = device , verbose = verbose ) ) return list ( np . mean ( np . array ( out ) , axis = 0 ) )
1050	def format_exception ( etype , value , tb , limit = None ) : if tb : list = [ 'Traceback (most recent call last):\n' ] list = list + format_tb ( tb , limit ) else : list = [ ] list = list + format_exception_only ( etype , value ) return list
10567	def _check_filters ( song , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : include = True if include_filters : if all_includes : if not all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False else : if not any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False if exclude_filters : if all_excludes : if all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False else : if any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False return include
6005	def generate_poisson_noise ( image , exposure_time_map , seed = - 1 ) : setup_random_seed ( seed ) image_counts = np . multiply ( image , exposure_time_map ) return image - np . divide ( np . random . poisson ( image_counts , image . shape ) , exposure_time_map )
1325	def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : # pixel influence on target class alphas = a . gradient ( image , target ) * mask # pixel influence on sum of residual classes # (don't evaluate if fast == True) if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) # compute saliency map # (take into account both pos. & neg. perturbations) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) # find optimal pixel & direction of perturbation idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
11472	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : # create a dictionary of user urls -> rendered responses replacements = { } user_urls = set ( re . findall ( URL_RE , text ) ) for user_url in user_urls : try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) except OEmbedException : if urlize_all_links : replacements [ user_url ] = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) replacements [ user_url ] = replacement . strip ( ) # go through the text recording URLs that can be replaced # taking note of their start & end indexes user_urls = re . finditer ( URL_RE , text ) matches = [ ] for match in user_urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) # replace the URLs in order, offsetting the indices each go for indx , ( start , end , user_url ) in enumerate ( matches ) : replacement = replacements [ user_url ] difference = len ( replacement ) - len ( user_url ) # insert the replacement between two slices of text surrounding the # original url text = text [ : start ] + replacement + text [ end : ] # iterate through the rest of the matches offsetting their indices # based on the difference between replacement/original for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark_safe ( text )
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
10145	def from_schema ( self , schema_node ) : params = [ ] for param_schema in schema_node . children : location = param_schema . name if location is 'body' : name = param_schema . __class__ . __name__ if name == 'body' : name = schema_node . __class__ . __name__ + 'Body' param = self . parameter_converter ( location , param_schema ) param [ 'name' ] = name if self . ref : param = self . _ref ( param ) params . append ( param ) elif location in ( ( 'path' , 'header' , 'headers' , 'querystring' , 'GET' ) ) : for node_schema in param_schema . children : param = self . parameter_converter ( location , node_schema ) if self . ref : param = self . _ref ( param ) params . append ( param ) return params
8209	def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
5069	def ungettext_min_max ( singular , plural , range_text , min_val , max_val ) : if min_val is None and max_val is None : return None if min_val == max_val or min_val is None or max_val is None : # pylint: disable=translation-of-non-string return ungettext ( singular , plural , min_val or max_val ) . format ( min_val or max_val ) return range_text . format ( min_val , max_val )
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
5737	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-shared' . format ( PUBSUB_OBJECT_PREFIX , self . name ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating shared subscription {}" . format ( subscription_name ) ) try : self . subscriber_client . create_subscription ( subscription_path , topic = topic_path ) except google . cloud . exceptions . Conflict : # Another worker created the subscription before us, ignore. pass return subscription_path
8532	def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to ( msg_b . args ) : return False , 'argument signature of methods do not match' return True , None
5165	def __intermediate_proto ( self , interface , address ) : # proto defaults to static address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : # allow override on interface level return interface . pop ( 'proto' )
23	def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( "data" ) as f : return pickle . load ( f ) else : with open ( path , "rb" ) as f : return pickle . load ( f )
5503	def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . BadArgumentUsage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config_file ) if remove : try : conf . cfg . remove_option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write_config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has_section ( key [ 0 ] ) : conf . cfg . add_section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write_config ( )
7173	def calc_intent ( self , query ) : matches = self . calc_intents ( query ) if len ( matches ) == 0 : return MatchData ( '' , '' ) best_match = max ( matches , key = lambda x : x . conf ) best_matches = ( match for match in matches if match . conf == best_match . conf ) return min ( best_matches , key = lambda x : sum ( map ( len , x . matches . values ( ) ) ) )
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
4194	def plot_window ( self ) : from pylab import plot , xlim , grid , title , ylabel , axis x = linspace ( 0 , 1 , self . N ) xlim ( 0 , 1 ) plot ( x , self . data ) grid ( True ) title ( '%s Window (%s points)' % ( self . name . capitalize ( ) , self . N ) ) ylabel ( 'Amplitude' ) axis ( [ 0 , 1 , 0 , 1.1 ] )
10199	def hash_id ( iso_timestamp , msg ) : return '{0}-{1}' . format ( iso_timestamp , hashlib . sha1 ( msg . get ( 'unique_id' ) . encode ( 'utf-8' ) + str ( msg . get ( 'visitor_id' ) ) . encode ( 'utf-8' ) ) . hexdigest ( ) )
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } # register original timestamp and filebody to dict for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) # file not modify -> continue if not modified : continue # file modifies -> create the modification object new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) # overwrite new timestamp and filebody timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody # append file modification object to manager manager . add_object ( obj ) # return new modification object yield obj time . sleep ( sleep )
8711	def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
8349	def isSelfClosingTag ( self , name ) : return self . SELF_CLOSING_TAGS . has_key ( name ) or self . instanceSelfClosingTags . has_key ( name )
10467	def getAnyAppWithWindow ( cls ) : # Refresh the runningApplications list apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) if hasattr ( ref , 'windows' ) and len ( ref . windows ( ) ) > 0 : return ref raise ValueError ( 'No GUI application found.' )
5560	def bounds ( self ) : if self . _raw [ "bounds" ] is None : return self . process_pyramid . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "bounds" ] ) )
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : # Register the BP blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) # Finally, attach the blueprints to its parent processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
5495	def make_aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
2068	def get_cars_data ( ) : df = pd . read_csv ( 'source_data/cars/car.data.txt' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = [ { 'col' : 'buying' , 'mapping' : [ ( 'vhigh' , 0 ) , ( 'high' , 1 ) , ( 'med' , 2 ) , ( 'low' , 3 ) ] } , { 'col' : 'maint' , 'mapping' : [ ( 'vhigh' , 0 ) , ( 'high' , 1 ) , ( 'med' , 2 ) , ( 'low' , 3 ) ] } , { 'col' : 'doors' , 'mapping' : [ ( '2' , 0 ) , ( '3' , 1 ) , ( '4' , 2 ) , ( '5more' , 3 ) ] } , { 'col' : 'persons' , 'mapping' : [ ( '2' , 0 ) , ( '4' , 1 ) , ( 'more' , 2 ) ] } , { 'col' : 'lug_boot' , 'mapping' : [ ( 'small' , 0 ) , ( 'med' , 1 ) , ( 'big' , 2 ) ] } , { 'col' : 'safety' , 'mapping' : [ ( 'high' , 0 ) , ( 'med' , 1 ) , ( 'low' , 2 ) ] } , ] return X , y , mapping
13756	def copy_file ( src , dest ) : dir_path = os . path . dirname ( dest ) if not os . path . exists ( dir_path ) : os . makedirs ( dir_path ) shutil . copy2 ( src , dest )
5554	def _raw_at_zoom ( config , zooms ) : params_per_zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in _RESERVED_PARAMETERS : out_element = _element_at_zoom ( name , element , zoom ) if out_element is not None : params [ name ] = out_element params_per_zoom [ zoom ] = params return params_per_zoom
10698	def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
3893	def _get_parser ( extra_args ) : parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter , ) dirs = appdirs . AppDirs ( 'hangups' , 'hangups' ) default_token_path = os . path . join ( dirs . user_cache_dir , 'refresh_token.txt' ) parser . add_argument ( '--token-path' , default = default_token_path , help = 'path used to store OAuth refresh token' ) parser . add_argument ( '-d' , '--debug' , action = 'store_true' , help = 'log detailed debugging messages' ) for extra_arg in extra_args : parser . add_argument ( extra_arg , required = True ) return parser
8923	def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
6563	def load_cnf ( fp ) : fp = iter ( fp ) # handle lists/tuples/etc csp = ConstraintSatisfactionProblem ( dimod . BINARY ) # first look for the problem num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break # now parse the clauses, picking up where we left off looking for the header clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] # line ends with a trailing 0 # -1 is the notation for NOT(1) variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
7818	def _update_handlers ( self ) : handler_map = defaultdict ( list ) for i , obj in enumerate ( self . handlers ) : for dummy , handler in inspect . getmembers ( obj , callable ) : if not hasattr ( handler , "_pyxmpp_event_handled" ) : continue # pylint: disable-msg=W0212 event_class = handler . _pyxmpp_event_handled handler_map [ event_class ] . append ( ( i , handler ) ) self . _handler_map = handler_map
5325	def measure_memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj_id = id ( obj ) if obj_id in seen : return 0 # Important mark as seen *before* entering recursion to gracefully handle # self-referential objects seen . add ( obj_id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure_memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure_memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , '__dict__' ) : size += cls . measure_memory ( obj . __dict__ , seen ) elif hasattr ( obj , '__iter__' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure_memory ( i , seen ) for i in obj ] ) return size
13265	def get_plugin_instance ( plugin_class , * args , * * kwargs ) : assert issubclass ( plugin_class , BasePlugin ) , type ( plugin_class ) global _yaz_plugin_instance_cache qualname = plugin_class . __qualname__ if not qualname in _yaz_plugin_instance_cache : plugin_class = get_plugin_list ( ) [ qualname ] _yaz_plugin_instance_cache [ qualname ] = plugin = plugin_class ( * args , * * kwargs ) # find any yaz.dependency decorators, and call them when necessary funcs = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . ismethod ( func ) and hasattr ( func , "yaz_dependency_config" ) ] for func in funcs : signature = inspect . signature ( func ) assert all ( parameter . kind is parameter . POSITIONAL_OR_KEYWORD and issubclass ( parameter . annotation , BasePlugin ) for parameter in signature . parameters . values ( ) ) , "All parameters for {} must type hint to a BasePlugin" . format ( func ) func ( * [ get_plugin_instance ( parameter . annotation ) for parameter in signature . parameters . values ( ) ] ) return _yaz_plugin_instance_cache [ qualname ]
10223	def get_regulatory_pairs ( graph : BELGraph ) -> Set [ NodePair ] : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_DECREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( ( u , v ) ) return results
10932	def get_termination_stats ( self , get_cos = True ) : delta_vals = self . _last_vals - self . param_vals delta_err = self . _last_error - self . error frac_err = delta_err / self . error to_return = { 'delta_vals' : delta_vals , 'delta_err' : delta_err , 'num_iter' : 1 * self . _num_iter , 'frac_err' : frac_err , 'error' : self . error , 'exp_err' : self . _exp_err } if get_cos : model_cosine = self . calc_model_cosine ( ) to_return . update ( { 'model_cosine' : model_cosine } ) return to_return
4767	def is_not_same_as ( self , other ) : if self . val is other : self . _err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
5017	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
12357	def connect ( self , interactive = False ) : from poseidon . ssh import SSHClient rs = SSHClient ( self . ip_address , interactive = interactive ) return rs
8458	def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
13193	def geom_to_xml_element ( geom ) : if geom . srs . srid != 4326 : raise NotImplementedError ( "Only WGS 84 lat/long geometries (SRID 4326) are supported." ) # GeoJSON output is far more standard than GML, so go through that return geojson_to_gml ( json . loads ( geom . geojson ) )
3000	def splitsDF ( symbol , timeframe = 'ytd' , token = '' , version = '' ) : s = splits ( symbol , timeframe , token , version ) df = _splitsToDF ( s ) return df
1508	def add_additional_args ( parsers ) : for parser in parsers : cli_args . add_verbose ( parser ) cli_args . add_config ( parser ) parser . add_argument ( '--heron-dir' , default = config . get_heron_dir ( ) , help = 'Path to Heron home directory' )
92	def _quokka_normalize_extract ( extract ) : # TODO get rid of this deferred import from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == "square" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( "Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage " + "for parameter 'extract', got %s." % ( type ( extract ) , ) ) return bb
9347	def argsort ( data , out = None , chunksize = None , baseargsort = None , argmerge = None , np = None ) : if baseargsort is None : baseargsort = lambda x : x . argsort ( ) if argmerge is None : argmerge = default_argmerge if chunksize is None : chunksize = 1024 * 1024 * 16 if out is None : arg1 = numpy . empty ( len ( data ) , dtype = 'intp' ) out = arg1 else : assert out . dtype == numpy . dtype ( 'intp' ) assert len ( out ) == len ( data ) arg1 = out if np is None : np = sharedmem . cpu_count ( ) if np <= 1 or len ( data ) < chunksize : out [ : ] = baseargsort ( data ) return out CHK = [ slice ( i , i + chunksize ) for i in range ( 0 , len ( data ) , chunksize ) ] DUMMY = slice ( len ( data ) , len ( data ) ) if len ( CHK ) % 2 : CHK . append ( DUMMY ) with sharedmem . TPool ( ) as pool : def work ( i ) : C = CHK [ i ] start , stop , step = C . indices ( len ( data ) ) arg1 [ C ] = baseargsort ( data [ C ] ) arg1 [ C ] += start pool . map ( work , range ( len ( CHK ) ) ) arg2 = numpy . empty_like ( arg1 ) flip = 0 while len ( CHK ) > 1 : with sharedmem . TPool ( ) as pool : def work ( i ) : C1 = CHK [ i ] C2 = CHK [ i + 1 ] start1 , stop1 , step1 = C1 . indices ( len ( data ) ) start2 , stop2 , step2 = C2 . indices ( len ( data ) ) # print 'argmerge', start1, stop1, start2, stop2 assert start2 == stop1 argmerge ( data , arg1 [ C1 ] , arg1 [ C2 ] , arg2 [ start1 : stop2 ] ) return slice ( start1 , stop2 ) CHK = pool . map ( work , range ( 0 , len ( CHK ) , 2 ) ) arg1 , arg2 = arg2 , arg1 flip = flip + 1 if len ( CHK ) == 1 : break if len ( CHK ) % 2 : CHK . append ( DUMMY ) if flip % 2 != 0 : # only even flips out ends up pointing to arg2 and needs to be # copied out [ : ] = arg1 return out
1465	def get_command_handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli_help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : # due to broken pipe problems pass only first 10 KiB data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
5656	def validate_day_start_ut ( conn ) : G = GTFS ( conn ) cur = conn . execute ( 'SELECT date, day_start_ut FROM days' ) for date , day_start_ut in cur : #print date, day_start_ut assert day_start_ut == G . get_day_start_ut ( date )
8044	def leapfrog ( self , kind , value = None ) : while self . current is not None : if self . current . kind == kind and ( value is None or self . current . value == value ) : self . consume ( kind ) return self . stream . move ( )
4289	def generate_media_pages ( gallery ) : writer = PageWriter ( gallery . settings , index_title = gallery . title ) for album in gallery . albums . values ( ) : medias = album . medias next_medias = medias [ 1 : ] + [ None ] previous_medias = [ None ] + medias [ : - 1 ] # The media group allows us to easily get next and previous links media_groups = zip ( medias , next_medias , previous_medias ) for media_group in media_groups : writer . write ( album , media_group )
1505	def template_statemgr_yaml ( cl_args , zookeepers ) : statemgr_config_file_template = "%s/standalone/templates/statemgr.template.yaml" % cl_args [ "config_path" ] statemgr_config_file_actual = "%s/standalone/statemgr.yaml" % cl_args [ "config_path" ] template_file ( statemgr_config_file_template , statemgr_config_file_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '"%s"' % zk if ":" in zk else '"%s:2181"' % zk for zk in zookeepers ] ) } )
5295	def get_start_date ( self , obj ) : obj_date = getattr ( obj , self . get_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : # It's a date rather than datetime, so we use it as is pass return obj_date
10618	def get_compound_amounts ( self ) : result = self . _compound_masses * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
5346	def compose_mailing_lists ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'mailing_lists' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] urls = [ url [ 'url' ] . replace ( 'mailto:' , '' ) for url in data [ p ] [ 'mailing_lists' ] if url [ 'url' ] not in projects [ p ] [ 'mailing_lists' ] ] projects [ p ] [ 'mailing_lists' ] += urls for p in [ project for project in data if len ( data [ project ] [ 'dev_list' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] mailing_list = data [ p ] [ 'dev_list' ] [ 'url' ] . replace ( 'mailto:' , '' ) projects [ p ] [ 'mailing_lists' ] . append ( mailing_list ) return projects
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) # List file to backup files = self . file_list ( ) # then download each of then self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
5075	def is_course_run_enrollable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) end = parse_datetime_handle_invalid ( course_run . get ( 'end' ) ) enrollment_start = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_start' ) ) enrollment_end = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_end' ) ) return ( not end or end > now ) and ( not enrollment_start or enrollment_start < now ) and ( not enrollment_end or enrollment_end > now )
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) # Handle the null route. if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : # We've matched all of our components, there might be more # segments for something else to process. break else : return NO_MATCH elif them is None : # We've run out of path segments to match, so this route can't be # the matching one. return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
8889	def _self_referential_fk ( klass_model ) : for f in klass_model . _meta . concrete_fields : if f . related_model : if issubclass ( klass_model , f . related_model ) : return f . attname return None
13773	def init_logs ( path = None , target = None , logger_name = 'root' , level = logging . DEBUG , maxBytes = 1 * 1024 * 1024 , backupCount = 5 , application_name = 'default' , server_hostname = None , fields = None ) : log_file = os . path . abspath ( os . path . join ( path , target ) ) logger = logging . getLogger ( logger_name ) logger . setLevel ( level ) handler = logging . handlers . RotatingFileHandler ( log_file , maxBytes = maxBytes , backupCount = backupCount ) handler . setLevel ( level ) handler . setFormatter ( JsonFormatter ( application_name = application_name , server_hostname = server_hostname , fields = fields ) ) logger . addHandler ( handler )
1640	def CheckCommaSpacing ( filename , clean_lines , linenum , error ) : raw = clean_lines . lines_without_raw_strings line = clean_lines . elided [ linenum ] # You should always have a space after a comma (either as fn arg or operator) # # This does not apply when the non-space character following the # comma is another comma, since the only time when that happens is # for empty macro arguments. # # We run this check in two passes: first pass on elided lines to # verify that lines contain missing whitespaces, second pass on raw # lines to confirm that those missing whitespaces are not due to # elided comments. if ( Search ( r',[^,\s]' , ReplaceAll ( r'\boperator\s*,\s*\(' , 'F(' , line ) ) and Search ( r',[^,\s]' , raw [ linenum ] ) ) : error ( filename , linenum , 'whitespace/comma' , 3 , 'Missing space after ,' ) # You should always have a space after a semicolon # except for few corner cases # TODO(unknown): clarify if 'if (1) { return 1;}' is requires one more # space after ; if Search ( r';[^\s};\\)/]' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 3 , 'Missing space after ;' )
11641	def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
2287	def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , * * kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , * * kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , * * kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
8097	def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
5891	def smart_str ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : if strings_only and isinstance ( string , ( type ( None ) , int ) ) : return string # if isinstance(s, Promise): # return unicode(s).encode(encoding, errors) if isinstance ( string , str ) : try : return string . encode ( encoding , errors ) except UnicodeEncodeError : return string . encode ( 'utf-8' , errors ) elif not isinstance ( string , bytes ) : try : return str ( string ) . encode ( encoding , errors ) except UnicodeEncodeError : if isinstance ( string , Exception ) : # An Exception subclass containing non-ASCII data that doesn't # know how to print itself properly. We shouldn't raise a # further exception. return ' ' . join ( [ smart_str ( arg , encoding , strings_only , errors ) for arg in string ] ) return str ( string ) . encode ( encoding , errors ) else : return string
7487	def concat_multiple_inputs ( data , sample ) : ## if more than one tuple in fastq list if len ( sample . files . fastqs ) > 1 : ## create a cat command to append them all (doesn't matter if they ## are gzipped, cat still works). Grab index 0 of tuples for R1s. cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . fastqs ] isgzip = ".gz" if not sample . files . fastqs [ 0 ] [ 0 ] . endswith ( ".gz" ) : isgzip = "" ## write to new concat handle conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concat.fq{}" . format ( isgzip ) ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: {}, {}" . format ( cmd1 , res1 ) ) ## Only set conc2 if R2 actually exists conc2 = 0 if "pair" in data . paramsdict [ "datatype" ] : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . fastqs ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concat.fq{}" . format ( isgzip ) ) with open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "Error concatenating fastq files. Make sure all " + "these files exist: {}\nError message: {}" . format ( cmd2 , proc2 . returncode ) ) ## store new file handles sample . files . concat = [ ( conc1 , conc2 ) ] return sample . files . concat
7581	def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : ## stat lines if "Estimated Ln Prob of Data" in line : self . est_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean_lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) ## matrix lines nonline = psearch . search ( line ) popline = dsearch . search ( line ) #if ") : " in line: if nonline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
1854	def SHLD ( cpu , dest , src , count ) : OperandSize = dest . size tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) MASK = ( ( 1 << OperandSize ) - 1 ) t0 = ( arg0 << tempCount ) t1 = arg1 >> ( OperandSize - tempCount ) res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) res = res & MASK dest . write ( res ) if isinstance ( tempCount , int ) and tempCount == 0 : pass else : SIGN_MASK = 1 << ( OperandSize - 1 ) lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount )
13466	def set_moments ( self , sx , sxp , sxxp ) : self . _sx = sx self . _sxp = sxp self . _sxxp = sxxp emit = _np . sqrt ( sx ** 2 * sxp ** 2 - sxxp ** 2 ) self . _store_emit ( emit = emit )
6593	def receive_one ( self ) : if not self . runid_pkgidx_map : return None while True : if not self . runid_to_return : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_next_finished_pkgidx_result_pair ( ) if ret is not None : break if self . runid_pkgidx_map : time . sleep ( self . sleep ) return ret
11268	def join ( prev , sep , * args , * * kw ) : yield sep . join ( prev , * args , * * kw )
5849	def list_files ( self , dataset_id , glob = "." , is_dir = False ) : data = { "list" : { "glob" : glob , "isDir" : is_dir } } return self . _get_success_json ( self . _post_json ( routes . list_files ( dataset_id ) , data , failure_message = "Failed to list files for dataset {}" . format ( dataset_id ) ) ) [ 'files' ]
10403	def microcanonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'n' , 'uint32' ) , ( 'edge' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'has_spanning_cluster' , 'bool' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'uint32' ) , ( 'moments' , '(5,)uint64' ) , ] ) return _ndarray_dtype ( fields )
899	def prettyPrintSequence ( self , sequence , verbosity = 1 ) : text = "" for i in xrange ( len ( sequence ) ) : pattern = sequence [ i ] if pattern == None : text += "<reset>" if i < len ( sequence ) - 1 : text += "\n" else : text += self . patternMachine . prettyPrintPattern ( pattern , verbosity = verbosity ) return text
3172	def get ( self , store_id , customer_id , * * queryparams ) : self . store_id = store_id self . customer_id = customer_id return self . _mc_client . _get ( url = self . _build_path ( store_id , 'customers' , customer_id ) , * * queryparams )
3200	def create ( self , data ) : if 'recipients' not in data : raise KeyError ( 'The campaign must have recipients' ) if 'list_id' not in data [ 'recipients' ] : raise KeyError ( 'The campaign recipients must have a list_id' ) if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) if 'type' not in data : raise KeyError ( 'The campaign must have a type' ) if not data [ 'type' ] in [ 'regular' , 'plaintext' , 'rss' , 'variate' , 'abspilt' ] : raise ValueError ( 'The campaign type must be one of "regular", "plaintext", "rss", or "variate"' ) if data [ 'type' ] == 'variate' : if 'variate_settings' not in data : raise KeyError ( 'The variate campaign must have variate_settings' ) if 'winner_criteria' not in data [ 'variate_settings' ] : raise KeyError ( 'The campaign variate_settings must have a winner_criteria' ) if data [ 'variate_settings' ] [ 'winner_criteria' ] not in [ 'opens' , 'clicks' , 'total_revenue' , 'manual' ] : raise ValueError ( 'The campaign variate_settings ' 'winner_criteria must be one of "opens", "clicks", "total_revenue", or "manual"' ) if data [ 'type' ] == 'rss' : if 'rss_opts' not in data : raise KeyError ( 'The rss campaign must have rss_opts' ) if 'feed_url' not in data [ 'rss_opts' ] : raise KeyError ( 'The campaign rss_opts must have a feed_url' ) if not data [ 'rss_opts' ] [ 'frequency' ] in [ 'daily' , 'weekly' , 'monthly' ] : raise ValueError ( 'The rss_opts frequency must be one of "daily", "weekly", or "monthly"' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . campaign_id = response [ 'id' ] else : self . campaign_id = None return response
11172	def posarghelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : docs = [ ] makelabel = lambda posarg : ' ' * indent + posarg . displayname + ': ' helpindent = _autoindent ( [ makelabel ( p ) for p in self . positional_args ] , indent , maxindent ) for posarg in self . positional_args : label = makelabel ( posarg ) text = posarg . formatname + '. ' + posarg . docs wrapped = self . _wrap_labelled ( label , text , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
1679	def CheckNextIncludeOrder ( self , header_type ) : error_message = ( 'Found %s after %s' % ( self . _TYPE_NAMES [ header_type ] , self . _SECTION_NAMES [ self . _section ] ) ) last_section = self . _section if header_type == _C_SYS_HEADER : if self . _section <= self . _C_SECTION : self . _section = self . _C_SECTION else : self . _last_header = '' return error_message elif header_type == _CPP_SYS_HEADER : if self . _section <= self . _CPP_SECTION : self . _section = self . _CPP_SECTION else : self . _last_header = '' return error_message elif header_type == _LIKELY_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION elif header_type == _POSSIBLE_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : # This will always be the fallback because we're not sure # enough that the header is associated with this file. self . _section = self . _OTHER_H_SECTION else : assert header_type == _OTHER_HEADER self . _section = self . _OTHER_H_SECTION if last_section != self . _section : self . _last_header = '' return ''
8307	def get_command_responses ( self ) : if not self . response_queue . empty ( ) : yield None while not self . response_queue . empty ( ) : line = self . response_queue . get ( ) if line is not None : yield line
7248	def get_stdout ( self , workflow_id , task_id ) : url = '%(wf_url)s/%(wf_id)s/tasks/%(task_id)s/stdout' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id , 'task_id' : task_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . text
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : # This is Python2 if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : # noqa, pylint: disable=undefined-variable return s . encode ( encoding ) else : # And this is Python3 if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
58	def extend ( self , all_sides = 0 , top = 0 , right = 0 , bottom = 0 , left = 0 ) : return BoundingBox ( x1 = self . x1 - all_sides - left , x2 = self . x2 + all_sides + right , y1 = self . y1 - all_sides - top , y2 = self . y2 + all_sides + bottom )
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( * * self . schema )
12061	def processArgs ( ) : if len ( sys . argv ) < 2 : print ( "\n\nERROR:" ) print ( "this script requires arguments!" ) print ( 'try "python command.py info"' ) return if sys . argv [ 1 ] == 'info' : print ( "import paths:\n " , "\n " . join ( sys . path ) ) print ( ) print ( "python version:" , sys . version ) print ( "SWHLab path:" , __file__ ) print ( "SWHLab version:" , swhlab . __version__ ) return if sys . argv [ 1 ] == 'glanceFolder' : abfFolder = swhlab . common . gui_getFolder ( ) if not abfFolder or not os . path . isdir ( abfFolder ) : print ( "bad path" ) return fnames = sorted ( glob . glob ( abfFolder + "/*.abf" ) ) outFolder = tempfile . gettempdir ( ) + "/swhlab/" if os . path . exists ( outFolder ) : shutil . rmtree ( outFolder ) os . mkdir ( outFolder ) outFile = outFolder + "/index.html" out = '<html><body>' out += '<h2>%s</h2>' % abfFolder for i , fname in enumerate ( fnames ) : print ( "\n\n### PROCESSING %d of %d" % ( i , len ( fnames ) ) ) saveAs = os . path . join ( os . path . dirname ( outFolder ) , os . path . basename ( fname ) ) + ".png" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href="%s"><img src="%s"></a><br>' % ( saveAs , saveAs ) swhlab . analysis . glance . processAbf ( fname , saveAs ) out += '</body></html>' with open ( outFile , 'w' ) as f : f . write ( out ) webbrowser . open_new_tab ( outFile ) return print ( "\n\nERROR:\nI'm not sure how to process these arguments!" ) print ( sys . argv )
1007	def _learnPhase1 ( self , activeColumns , readOnly = False ) : # Save previous active state and start out on a clean slate self . lrnActiveState [ 't' ] . fill ( 0 ) # For each column, turn on the predicted cell. There will always be at most # one predicted cell per column numUnpredictedColumns = 0 for c in activeColumns : predictingCells = numpy . where ( self . lrnPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictedCells = len ( predictingCells ) assert numPredictedCells <= 1 # If we have a predicted cell, turn it on. The segment's posActivation # count will have already been incremented by processSegmentUpdates if numPredictedCells == 1 : i = predictingCells [ 0 ] self . lrnActiveState [ 't' ] [ c , i ] = 1 continue numUnpredictedColumns += 1 if readOnly : continue # If no predicted cell, pick the closest matching one to reinforce, or # if none exists, create a new segment on a cell in that column i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't-1' ] , self . minThreshold ) if s is not None and s . isSequenceSegment ( ) : if self . verbosity >= 4 : print "Learn branch 0, found segment match. Learning on col=" , c self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , s , self . lrnActiveState [ 't-1' ] , newSynapses = True ) s . totalActivations += 1 # This will update the permanences, posActivationsCount, and the # lastActiveIteration (age). trimSegment = self . _adaptSegment ( segUpdate ) if trimSegment : self . _trimSegmentsInCell ( c , i , [ s ] , minPermanence = 0.00001 , minNumSyns = 0 ) # If no close match exists, create a new one else : # Choose a cell in this column to add a new segment to i = self . _getCellForNewSegment ( c ) if ( self . verbosity >= 4 ) : print "Learn branch 1, no match. Learning on col=" , c , print ", newCellIdxInCol=" , i self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , None , self . lrnActiveState [ 't-1' ] , newSynapses = True ) segUpdate . sequenceSegment = True # Make it a sequence segment self . _adaptSegment ( segUpdate ) # No need to check whether perm reached 0 # Determine if we are out of sequence or not and reset our PAM counter # if we are in sequence numBottomUpColumns = len ( activeColumns ) if numUnpredictedColumns < numBottomUpColumns / 2 : return True # in sequence else : return False
5329	def get_raw ( config , backend_section , arthur ) : if arthur : task = TaskRawDataArthurCollection ( config , backend_section = backend_section ) else : task = TaskRawDataCollection ( config , backend_section = backend_section ) TaskProjects ( config ) . execute ( ) try : task . execute ( ) logging . info ( "Loading raw data finished!" ) except Exception as e : logging . error ( str ( e ) ) sys . exit ( - 1 )
13648	def get_fuel_prices_for_station ( self , station : int ) -> List [ Price ] : response = requests . get ( '{}/prices/station/{}' . format ( API_URL_BASE , station ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return [ Price . deserialize ( data ) for data in data [ 'prices' ] ]
12288	def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : # adapted from https://gist.github.com/mgerring/3645889 def export_as_csv ( modeladmin , request , queryset ) : # pylint: disable=unused-argument """ Export model fields to CSV. """ opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) # this data is truly categorical, with no known concept of ordering mapping = None return X , y , mapping
13130	def parse_single_computer ( entry ) : computer = Computer ( dns_hostname = get_field ( entry , 'dNSHostName' ) , description = get_field ( entry , 'description' ) , os = get_field ( entry , 'operatingSystem' ) , group_id = get_field ( entry , 'primaryGroupID' ) ) try : ip = str ( ipaddress . ip_address ( get_field ( entry , 'IPv4' ) ) ) except ValueError : ip = '' if ip : computer . ip = ip elif computer . dns_hostname : computer . ip = resolve_ip ( computer . dns_hostname ) return computer
7081	def send_email ( sender , subject , content , email_recipient_list , email_address_list , email_user = None , email_pass = None , email_server = None ) : if not email_user : email_user = EMAIL_USER if not email_pass : email_pass = EMAIL_PASSWORD if not email_server : email_server = EMAIL_SERVER if not email_server and email_user and email_pass : raise ValueError ( "no email server address and " "credentials available, can't continue" ) msg_text = EMAIL_TEMPLATE . format ( sender = sender , hostname = socket . gethostname ( ) , activity_time = '%sZ' % datetime . utcnow ( ) . isoformat ( ) , activity_report = content ) email_sender = '%s <%s>' % ( sender , EMAIL_USER ) # put together the recipient and email lists email_recipients = [ ( '%s <%s>' % ( x , y ) ) for ( x , y ) in zip ( email_recipient_list , email_address_list ) ] # put together the rest of the message email_msg = MIMEText ( msg_text ) email_msg [ 'From' ] = email_sender email_msg [ 'To' ] = ', ' . join ( email_recipients ) email_msg [ 'Message-Id' ] = make_msgid ( ) email_msg [ 'Subject' ] = '[%s on %s] %s' % ( sender , socket . gethostname ( ) , subject ) email_msg [ 'Date' ] = formatdate ( time . time ( ) ) # start the email process try : server = smtplib . SMTP ( EMAIL_SERVER , 587 ) server_ehlo_response = server . ehlo ( ) if server . has_extn ( 'STARTTLS' ) : try : tls_start_response = server . starttls ( ) tls_ehlo_response = server . ehlo ( ) login_response = server . login ( EMAIL_USER , EMAIL_PASSWORD ) send_response = ( server . sendmail ( email_sender , email_address_list , email_msg . as_string ( ) ) ) except Exception as e : print ( 'script email sending failed with error: %s' % e ) send_response = None if send_response is not None : print ( 'script email sent successfully' ) quit_response = server . quit ( ) return True else : quit_response = server . quit ( ) return False else : print ( 'email server does not support STARTTLS,' ' bailing out...' ) quit_response = server . quit ( ) return False except Exception as e : print ( 'sending email failed with error: %s' % e ) returnval = False quit_response = server . quit ( ) return returnval
1743	def save_image ( tensor , filename , nrow = 8 , padding = 2 , pad_value = 0 ) : from PIL import Image grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value ) im = Image . fromarray ( pre_pillow_float_img_process ( grid ) ) im . save ( filename )
794	def getActiveJobCountForClientInfo ( self , clientInfo ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount
1544	def get_logical_plan ( cluster , env , topology , role ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_logical_plan ( cluster , env , topology , role ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
2243	def argval ( key , default = util_const . NoParam , argv = None ) : if argv is None : # nocover argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key n_max = len ( argv ) - 1 for argx , item in enumerate ( argv ) : for key_ in keys : if item == key_ : if argx < n_max : value = argv [ argx + 1 ] return value elif item . startswith ( key_ + '=' ) : value = '=' . join ( item . split ( '=' ) [ 1 : ] ) return value value = default return value
13386	def upstream_url ( self , uri ) : return self . application . options . upstream + self . request . uri
12935	def _parse_allele_data ( self ) : # Get allele frequencies if they exist. pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , * * cln_data ) # A few ClinVar variants are only reported as a combination with # other variants, and no single-variant effect is proposed. Skip these. if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
4066	def item_fields ( self ) : # Check for a valid cached version if self . templates . get ( "item_fields" ) and not self . _updated ( "/itemFields" , self . templates [ "item_fields" ] , "item_fields" ) : return self . templates [ "item_fields" ] [ "tmplt" ] query_string = "/itemFields" # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , "item_fields" )
11385	def body ( self ) : if not hasattr ( self , '_body' ) : self . _body = inspect . getsource ( self . module ) return self . _body
9517	def trim_Ns ( self ) : # get index of first base that is not an N i = 0 while i < len ( self ) and self . seq [ i ] in 'nN' : i += 1 # strip off start of sequence and quality self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] # strip the ends self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
13220	def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( * * row ) ) return settings
11086	def whoami ( self , msg , args ) : output = [ "Hello %s" % msg . user ] if hasattr ( self . _bot . dispatcher , 'auth_manager' ) and msg . user . is_admin is True : output . append ( "You are a *bot admin*." ) output . append ( "Bot version: %s-%s" % ( self . _bot . version , self . _bot . commit ) ) return '\n' . join ( output )
8212	def draw_freehand ( self ) : if _ctx . _ns [ "mousedown" ] : x , y = mouse ( ) if self . show_grid : x , y = self . grid . snap ( x , y ) if self . freehand_move == True : cmd = MOVETO self . freehand_move = False else : cmd = LINETO # Add a new LINETO to the path, # except when starting to draw, # then a MOVETO is added to the path. pt = PathElement ( ) if cmd != MOVETO : pt . freehand = True # Used when mixed with curve drawing. else : pt . freehand = False pt . cmd = cmd pt . x = x pt . y = y pt . ctrl1 = Point ( x , y ) pt . ctrl2 = Point ( x , y ) self . _points . append ( pt ) # Draw the current location of the cursor. r = 4 _ctx . nofill ( ) _ctx . stroke ( self . handle_color ) _ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) _ctx . fontsize ( 9 ) _ctx . fill ( self . handle_color ) _ctx . text ( " (" + str ( int ( pt . x ) ) + ", " + str ( int ( pt . y ) ) + ")" , pt . x + r , pt . y ) self . _dirty = True else : # Export the updated drawing, # remember to do a MOVETO on the next interaction. self . freehand_move = True if self . _dirty : self . _points [ - 1 ] . freehand = False self . export_svg ( ) self . _dirty = False
7888	def error ( self , stanza ) : err = stanza . get_error ( ) self . __logger . debug ( "Error from: %r Condition: %r" % ( stanza . get_from ( ) , err . get_condition ) )
10871	def f_theta ( cos_theta , zint , z , n2n1 = 0.95 , sph6_ab = None , * * kwargs ) : wvfront = ( np . outer ( np . ones_like ( z ) * zint , cos_theta ) - np . outer ( zint + z , csqrt ( n2n1 ** 2 - 1 + cos_theta ** 2 ) ) ) if ( sph6_ab is not None ) and ( not np . isnan ( sph6_ab ) ) : sec2_theta = 1.0 / ( cos_theta * cos_theta ) wvfront += sph6_ab * ( sec2_theta - 1 ) * ( sec2_theta - 2 ) * cos_theta #Ensuring evanescent waves are always suppressed: if wvfront . dtype == np . dtype ( 'complex128' ) : wvfront . imag = - np . abs ( wvfront . imag ) return wvfront
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : # use the default timezone (settings.TIME_ZONE) for localhost tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
11689	def get_metadata ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content ) . getchildren ( ) [ 0 ]
2557	def clean_attribute ( attribute ) : # Shorthand attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) # Workaround for Python's reserved words if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] # Workaround for dash if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) # Workaround for colon if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
8218	def do_fullscreen ( self , widget ) : self . fullscreen ( ) self . is_fullscreen = True # next lines seem to be needed for window switching really to # fullscreen mode before reading it's size values while Gtk . events_pending ( ) : Gtk . main_iteration ( ) # we pass informations on full-screen size to bot self . bot . _screen_width = Gdk . Screen . width ( ) self . bot . _screen_height = Gdk . Screen . height ( ) self . bot . _screen_ratio = self . bot . _screen_width / self . bot . _screen_height
6094	def mapping_matrix_from_sub_to_pix ( sub_to_pix , pixels , regular_pixels , sub_to_regular , sub_grid_fraction ) : mapping_matrix = np . zeros ( ( regular_pixels , pixels ) ) for sub_index in range ( sub_to_regular . shape [ 0 ] ) : mapping_matrix [ sub_to_regular [ sub_index ] , sub_to_pix [ sub_index ] ] += sub_grid_fraction return mapping_matrix
1330	def predictions ( self , image , strict = True , return_details = False ) : in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 predictions = self . __model . predictions ( image ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 if return_details : return predictions , is_adversarial , is_best , distance else : return predictions , is_adversarial
189	def copy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = lss , shape = shape )
7389	def node_theta ( self , node ) : group = self . find_node_group_membership ( node ) return self . group_theta ( group )
8846	def update_terminal_colors ( self ) : self . color_scheme = self . create_color_scheme ( background = self . syntax_highlighter . color_scheme . background , foreground = self . syntax_highlighter . color_scheme . formats [ 'normal' ] . foreground ( ) . color ( ) )
6670	def task ( * args , * * kwargs ) : precursors = kwargs . pop ( 'precursors' , None ) post_callback = kwargs . pop ( 'post_callback' , False ) if args and callable ( args [ 0 ] ) : # direct decoration, @task return _task ( * args ) # callable decoration, @task(precursors=['satchel']) def wrapper ( meth ) : if precursors : meth . deploy_before = list ( precursors ) if post_callback : #from burlap.common import post_callbacks #post_callbacks.append(meth) meth . is_post_callback = True return _task ( meth ) return wrapper
11525	def create_big_thumbnail ( self , token , bitstream_id , item_id , width = 575 ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'bitstreamId' ] = bitstream_id parameters [ 'itemId' ] = item_id parameters [ 'width' ] = width response = self . request ( 'midas.thumbnailcreator.create.big.thumbnail' , parameters ) return response
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( * * metric ) else : self . c . put_metric_data ( * * metrics )
9997	def del_cells ( self , name ) : if name in self . cells : cells = self . cells [ name ] self . cells . del_item ( name ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) elif name in self . dynamic_spaces : cells = self . dynamic_spaces . pop ( name ) self . dynamic_spaces . set_update ( ) else : raise KeyError ( "Cells '%s' does not exist" % name ) NullImpl ( cells )
5860	def default ( self , obj ) : if obj is None : return [ ] elif isinstance ( obj , list ) : return [ i . as_dictionary ( ) for i in obj ] elif isinstance ( obj , dict ) : return self . _keys_to_camel_case ( obj ) else : return obj . as_dictionary ( )
749	def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] # Limit the number of predictions to include. likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
8845	def _at_block_start ( tc , line ) : if tc . atBlockStart ( ) : return True column = tc . columnNumber ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
2316	def create_graph_from_data ( self , data , * * kwargs ) : # Building setup w/ arguments. self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_pc ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : # noinspection PyTypeChecker galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
8911	def ows_security_tween_factory ( handler , registry ) : security = owssecurity_factory ( registry ) def ows_security_tween ( request ) : try : security . check_request ( request ) return handler ( request ) except OWSException as err : logger . exception ( "security check failed." ) return err except Exception as err : logger . exception ( "unknown error" ) return OWSNoApplicableCode ( "{}" . format ( err ) ) return ows_security_tween
11455	def get_config_item ( cls , key , kb_name , allow_substring = True ) : config_dict = cls . kbs . get ( kb_name , None ) if config_dict : if key in config_dict : return config_dict [ key ] elif allow_substring : res = [ v for k , v in config_dict . items ( ) if key in k ] if res : return res [ 0 ] return key
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
6389	def _sb_short_word ( self , term , r1_prefixes = None ) : if self . _sb_r1 ( term , r1_prefixes ) == len ( term ) and self . _sb_ends_in_short_syllable ( term ) : return True return False
12121	def get_data_around ( self , timePoints , thisSweep = False , padding = 0.02 , msDeriv = 0 ) : if not np . array ( timePoints ) . shape : timePoints = [ float ( timePoints ) ] data = None for timePoint in timePoints : if thisSweep : sweep = self . currentSweep else : sweep = int ( timePoint / self . sweepInterval ) timePoint = timePoint - sweep * self . sweepInterval self . setSweep ( sweep ) if msDeriv : dx = int ( msDeriv * self . rate / 1000 ) #points per ms newData = ( self . dataY [ dx : ] - self . dataY [ : - dx ] ) * self . rate / 1000 / dx else : newData = self . dataY padPoints = int ( padding * self . rate ) pad = np . empty ( padPoints ) * np . nan Ic = timePoint * self . rate #center point (I) newData = np . concatenate ( ( pad , pad , newData , pad , pad ) ) Ic += padPoints * 2 newData = newData [ Ic - padPoints : Ic + padPoints ] newData = newData [ : int ( padPoints * 2 ) ] #TODO: omg so much trouble with this! if data is None : data = [ newData ] else : data = np . vstack ( ( data , newData ) ) #TODO: omg so much trouble with this! return data
465	def set_gpu_fraction ( gpu_fraction = 0.3 ) : tl . logging . info ( "[TL]: GPU MEM Fraction %f" % gpu_fraction ) gpu_options = tf . GPUOptions ( per_process_gpu_memory_fraction = gpu_fraction ) sess = tf . Session ( config = tf . ConfigProto ( gpu_options = gpu_options ) ) return sess
13721	def query ( self , wql ) : try : self . __wql = [ 'wmic' , '-U' , self . args . domain + '\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( "wql: {}" . format ( self . __wql ) ) self . __output = subprocess . check_output ( self . __wql ) self . logger . debug ( "output: {}" . format ( self . __output ) ) self . logger . debug ( "wmi connect succeed." ) self . __wmi_output = self . __output . splitlines ( ) [ 1 : ] self . logger . debug ( "wmi_output: {}" . format ( self . __wmi_output ) ) self . __csv_header = csv . DictReader ( self . __wmi_output , delimiter = '|' ) self . logger . debug ( "csv_header: {}" . format ( self . __csv_header ) ) return list ( self . __csv_header ) except subprocess . CalledProcessError as e : self . unknown ( "Connect by wmi and run wql error: %s" % e )
6118	def circular_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
7671	def save ( self , path_or_file , strict = True , fmt = 'auto' ) : self . validate ( strict = strict ) with _open ( path_or_file , mode = 'w' , fmt = fmt ) as fdesc : json . dump ( self . __json__ , fdesc , indent = 2 )
12232	def register_prefs ( * args , * * kwargs ) : swap_settings_module = bool ( kwargs . get ( 'swap_settings_module' , True ) ) if __PATCHED_LOCALS_SENTINEL not in get_frame_locals ( 2 ) : raise SitePrefsException ( 'Please call `patch_locals()` right before the `register_prefs()`.' ) bind_proxy ( args , * * kwargs ) unpatch_locals ( ) swap_settings_module and proxy_settings_module ( )
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
7958	def handle_read ( self ) : with self . lock : logger . debug ( "handle_read()" ) if self . _eof or self . _socket is None : return if self . _state == "tls-handshake" : while True : logger . debug ( "tls handshake read..." ) self . _continue_tls_handshake ( ) logger . debug ( " state: {0}" . format ( self . _tls_state ) ) if self . _tls_state != "want_read" : break elif self . _tls_state == "connected" : while self . _socket and not self . _eof : logger . debug ( "tls socket read..." ) try : data = self . _socket . read ( 4096 ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : break elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data ) else : while self . _socket and not self . _eof : logger . debug ( "raw socket read..." ) try : data = self . _socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data )
11178	def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
5929	def getLogLevel ( self , section , option ) : return logging . getLevelName ( self . get ( section , option ) . upper ( ) )
309	def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ "green" , "orange" , "orangered" , "darkred" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) # Plot returns line graph label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
13704	def iter_add_text ( self , lines , prepend = None , append = None ) : if ( prepend is None ) and ( append is None ) : yield from lines else : # Build up a format string, with optional {prepend}/{append} fmtpcs = [ '{prepend}' ] if prepend else [ ] fmtpcs . append ( '{line}' ) if append : fmtpcs . append ( '{append}' ) fmtstr = '' . join ( fmtpcs ) yield from ( fmtstr . format ( prepend = prepend , line = line , append = append ) for line in lines )
6675	def umask ( self , use_sudo = False ) : func = use_sudo and run_as_root or self . run return func ( 'umask' )
1492	def register_timer_task_in_sec ( self , task , second ) : # Python time is in float second_in_float = float ( second ) expiration = time . time ( ) + second_in_float heappush ( self . timer_tasks , ( expiration , task ) )
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. #g/mol to kg/mol dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
4203	def LEVINSON ( r , order = None , allow_singularity = False ) : #from numpy import isrealobj T0 = numpy . real ( r [ 0 ] ) T = r [ 1 : ] M = len ( T ) if order is None : M = len ( T ) else : assert order <= M , 'order must be less than size of the input data' M = order realdata = numpy . isrealobj ( r ) if realdata is True : A = numpy . zeros ( M , dtype = float ) ref = numpy . zeros ( M , dtype = float ) else : A = numpy . zeros ( M , dtype = complex ) ref = numpy . zeros ( M , dtype = complex ) P = T0 for k in range ( 0 , M ) : save = T [ k ] if k == 0 : temp = - save / P else : #save += sum([A[j]*T[k-j-1] for j in range(0,k)]) for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] temp = - save / P if realdata : P = P * ( 1. - temp ** 2. ) else : P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 and allow_singularity == False : raise ValueError ( "singular matrix" ) A [ k ] = temp ref [ k ] = temp # save reflection coeff at each step if k == 0 : continue khalf = ( k + 1 ) // 2 if realdata is True : for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] if j != kj : A [ kj ] += temp * save else : for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) return A , P , ref
793	def jobGetModelIDs ( self , jobID ) : rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( job_id = jobID ) , [ 'model_id' ] ) return [ r [ 0 ] for r in rows ]
10147	def _ref ( self , param , base_name = None ) : name = base_name or param . get ( 'title' , '' ) or param . get ( 'name' , '' ) pointer = self . json_pointer + name self . parameter_registry [ name ] = param return { '$ref' : pointer }
13730	def value_to_bool ( config_val , evar ) : if not config_val : return False if config_val . strip ( ) . lower ( ) == 'true' : return True else : return False
6971	def _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coeffs [ 0 ] * fsv * fsv + coeffs [ 1 ] * fsv + coeffs [ 2 ] * fdv * fdv + coeffs [ 3 ] * fdv + coeffs [ 4 ] * fkv * fkv + coeffs [ 5 ] * fkv + coeffs [ 6 ] + coeffs [ 7 ] * fsv * fdv + coeffs [ 8 ] * fsv * fkv + coeffs [ 9 ] * fdv * fkv + coeffs [ 10 ] * np . sin ( 2 * pi_value * xcc ) + coeffs [ 11 ] * np . cos ( 2 * pi_value * xcc ) + coeffs [ 12 ] * np . sin ( 2 * pi_value * ycc ) + coeffs [ 13 ] * np . cos ( 2 * pi_value * ycc ) + coeffs [ 14 ] * np . sin ( 4 * pi_value * xcc ) + coeffs [ 15 ] * np . cos ( 4 * pi_value * xcc ) + coeffs [ 16 ] * np . sin ( 4 * pi_value * ycc ) + coeffs [ 17 ] * np . cos ( 4 * pi_value * ycc ) + coeffs [ 18 ] * bgv + coeffs [ 19 ] * bge + coeffs [ 20 ] * iha + coeffs [ 21 ] * izd )
3863	def _get_event_request_header ( self ) : otr_status = ( hangouts_pb2 . OFF_THE_RECORD_STATUS_OFF_THE_RECORD if self . is_off_the_record else hangouts_pb2 . OFF_THE_RECORD_STATUS_ON_THE_RECORD ) return hangouts_pb2 . EventRequestHeader ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , client_generated_id = self . _client . get_client_generated_id ( ) , expected_otr = otr_status , delivery_medium = self . _get_default_delivery_medium ( ) , )
1303	def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )
6440	def dist_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . dist ( src , tar , qval , alphabet )
10438	def startprocessmonitor ( self , process_name , interval = 2 ) : if process_name in self . _process_stats : # Stop previously running instance # At any point, only one process name can be tracked # If an instance already exist, then stop it self . _process_stats [ process_name ] . stop ( ) # Create an instance of process stat self . _process_stats [ process_name ] = ProcessStats ( process_name , interval ) # start monitoring the process self . _process_stats [ process_name ] . start ( ) return 1
13453	def spawn ( func , * args , * * kwargs ) : return gevent . spawn ( wrap_uncaught_greenlet_exceptions ( func ) , * args , * * kwargs )
9002	def _register_instruction_in_defs ( self , instruction ) : type_ = instruction . type color_ = instruction . color instruction_to_svg_dict = self . _instruction_to_svg . instruction_to_svg_dict instruction_id = "{}:{}" . format ( type_ , color_ ) defs_id = instruction_id + ":defs" if instruction_id not in self . _instruction_type_color_to_symbol : svg_dict = instruction_to_svg_dict ( instruction ) self . _compute_scale ( instruction_id , svg_dict ) symbol = self . _make_definition ( svg_dict , instruction_id ) self . _instruction_type_color_to_symbol [ defs_id ] = symbol [ DEFINITION_HOLDER ] . pop ( "defs" , { } ) self . _instruction_type_color_to_symbol [ instruction_id ] = symbol return instruction_id
9945	def copy_file ( self , path , prefixed_path , source_storage ) : # Skip this file if it was already copied earlier if prefixed_path in self . copied_files : return self . log ( "Skipping '%s' (already copied earlier)" % path ) # Delete the target file if needed or break if not self . delete_file ( path , prefixed_path , source_storage ) : return # The full path of the source file source_path = source_storage . path ( path ) # Finally start copying if self . dry_run : self . log ( "Pretending to copy '%s'" % source_path , level = 1 ) else : self . log ( "Copying '%s'" % source_path , level = 1 ) with source_storage . open ( path ) as source_file : self . storage . save ( prefixed_path , source_file ) self . copied_files . append ( prefixed_path )
2205	def userhome ( username = None ) : if username is None : # get home directory for the current user if 'HOME' in os . environ : userhome_dpath = os . environ [ 'HOME' ] else : # nocover if sys . platform . startswith ( 'win32' ) : # win32 fallback when HOME is not defined if 'USERPROFILE' in os . environ : userhome_dpath = os . environ [ 'USERPROFILE' ] elif 'HOMEPATH' in os . environ : drive = os . environ . get ( 'HOMEDRIVE' , '' ) userhome_dpath = join ( drive , os . environ [ 'HOMEPATH' ] ) else : raise OSError ( "Cannot determine the user's home directory" ) else : # posix fallback when HOME is not defined import pwd userhome_dpath = pwd . getpwuid ( os . getuid ( ) ) . pw_dir else : # A specific user directory was requested if sys . platform . startswith ( 'win32' ) : # nocover # get the directory name for the current user c_users = dirname ( userhome ( ) ) userhome_dpath = join ( c_users , username ) if not exists ( userhome_dpath ) : raise KeyError ( 'Unknown user: {}' . format ( username ) ) else : import pwd try : pwent = pwd . getpwnam ( username ) except KeyError : # nocover raise KeyError ( 'Unknown user: {}' . format ( username ) ) userhome_dpath = pwent . pw_dir return userhome_dpath
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
10150	def generate ( self , title = None , version = None , base_path = None , info = None , swagger = None , * * kwargs ) : title = title or self . api_title version = version or self . api_version info = info or self . swagger . get ( 'info' , { } ) swagger = swagger or self . swagger base_path = base_path or self . base_path swagger = swagger . copy ( ) info . update ( title = title , version = version ) swagger . update ( swagger = '2.0' , info = info , basePath = base_path ) paths , tags = self . _build_paths ( ) # Update the provided tags with the extracted ones preserving order if tags : swagger . setdefault ( 'tags' , [ ] ) tag_names = { t [ 'name' ] for t in swagger [ 'tags' ] } for tag in tags : if tag [ 'name' ] not in tag_names : swagger [ 'tags' ] . append ( tag ) # Create/Update swagger sections with extracted values where not provided if paths : swagger . setdefault ( 'paths' , { } ) merge_dicts ( swagger [ 'paths' ] , paths ) definitions = self . definitions . definition_registry if definitions : swagger . setdefault ( 'definitions' , { } ) merge_dicts ( swagger [ 'definitions' ] , definitions ) parameters = self . parameters . parameter_registry if parameters : swagger . setdefault ( 'parameters' , { } ) merge_dicts ( swagger [ 'parameters' ] , parameters ) responses = self . responses . response_registry if responses : swagger . setdefault ( 'responses' , { } ) merge_dicts ( swagger [ 'responses' ] , responses ) return swagger
10156	def merge_dicts ( base , changes ) : for k , v in changes . items ( ) : if isinstance ( v , dict ) : merge_dicts ( base . setdefault ( k , { } ) , v ) else : base . setdefault ( k , v )
5421	def _get_job_metadata ( provider , user_id , job_name , script , task_ids , user_project , unique_job_id ) : create_time = dsub_util . replace_timezone ( datetime . datetime . now ( ) , tzlocal ( ) ) user_id = user_id or dsub_util . get_os_user ( ) job_metadata = provider . prepare_job_metadata ( script . name , job_name , user_id , create_time ) if unique_job_id : job_metadata [ 'job-id' ] = uuid . uuid4 ( ) . hex job_metadata [ 'create-time' ] = create_time job_metadata [ 'script' ] = script job_metadata [ 'user-project' ] = user_project if task_ids : job_metadata [ 'task-ids' ] = dsub_util . compact_interval_string ( list ( task_ids ) ) return job_metadata
3915	def _update ( self ) : typing_users = [ self . _conversation . get_user ( user_id ) for user_id , status in self . _typing_statuses . items ( ) if status == hangups . TYPING_TYPE_STARTED ] displayed_names = [ user . first_name for user in typing_users if not user . is_self ] if displayed_names : typing_message = '{} {} typing...' . format ( ', ' . join ( sorted ( displayed_names ) ) , 'is' if len ( displayed_names ) == 1 else 'are' ) else : typing_message = '' if not self . _is_connected : self . _widget . set_text ( "RECONNECTING..." ) elif self . _message is not None : self . _widget . set_text ( self . _message ) else : self . _widget . set_text ( typing_message )
3139	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The promo rule must have an id' ) if 'description' not in data : raise KeyError ( 'This promo rule must have a description' ) if 'amount' not in data : raise KeyError ( 'This promo rule must have an amount' ) if 'target' not in data : raise KeyError ( 'This promo rule must apply to a target (example per_item, total, or shipping' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'promo-rules' ) , data = data ) if response is not None : return response
3774	def select_valid_methods ( self , T ) : # Consider either only the user's methods or all methods # Tabular data will be in both when inserted if self . forced : considered_methods = list ( self . user_methods ) else : considered_methods = list ( self . all_methods ) # User methods (incl. tabular data); add back later, after ranking the rest if self . user_methods : [ considered_methods . remove ( i ) for i in self . user_methods ] # Index the rest of the methods by ranked_methods, and add them to a list, sorted_methods preferences = sorted ( [ self . ranked_methods . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods [ i ] for i in preferences ] # Add back the user's methods to the top, in order. if self . user_methods : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods ) ] sorted_valid_methods = [ ] for method in sorted_methods : if self . test_method_validity ( T , method ) : sorted_valid_methods . append ( method ) return sorted_valid_methods
10324	def microcanonical_averages_arrays ( microcanonical_averages ) : ret = dict ( ) for n , microcanonical_average in enumerate ( microcanonical_averages ) : assert n == microcanonical_average [ 'n' ] if n == 0 : num_edges = microcanonical_average [ 'M' ] num_sites = microcanonical_average [ 'N' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_average ) ret [ 'max_cluster_size' ] = np . empty ( num_edges + 1 ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( num_edges + 1 ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , num_edges + 1 ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , num_edges + 1 , 2 ) ) ret [ 'max_cluster_size' ] [ n ] = microcanonical_average [ 'max_cluster_size' ] ret [ 'max_cluster_size_ci' ] [ n ] = ( microcanonical_average [ 'max_cluster_size_ci' ] ) if spanning_cluster : ret [ 'spanning_cluster' ] [ n ] = ( microcanonical_average [ 'spanning_cluster' ] ) ret [ 'spanning_cluster_ci' ] [ n ] = ( microcanonical_average [ 'spanning_cluster_ci' ] ) ret [ 'moments' ] [ : , n ] = microcanonical_average [ 'moments' ] ret [ 'moments_ci' ] [ : , n ] = microcanonical_average [ 'moments_ci' ] # normalize by number of sites for key in ret : if 'spanning_cluster' in key : continue ret [ key ] /= num_sites ret [ 'M' ] = num_edges ret [ 'N' ] = num_sites return ret
10353	def write_boilerplate ( name : str , version : Optional [ str ] = None , description : Optional [ str ] = None , authors : Optional [ str ] = None , contact : Optional [ str ] = None , copyright : Optional [ str ] = None , licenses : Optional [ str ] = None , disclaimer : Optional [ str ] = None , namespace_url : Optional [ Mapping [ str , str ] ] = None , namespace_patterns : Optional [ Mapping [ str , str ] ] = None , annotation_url : Optional [ Mapping [ str , str ] ] = None , annotation_patterns : Optional [ Mapping [ str , str ] ] = None , annotation_list : Optional [ Mapping [ str , Set [ str ] ] ] = None , pmids : Optional [ Iterable [ Union [ str , int ] ] ] = None , entrez_ids : Optional [ Iterable [ Union [ str , int ] ] ] = None , file : Optional [ TextIO ] = None , ) -> None : lines = make_knowledge_header ( name = name , version = version or '1.0.0' , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , namespace_url = namespace_url , namespace_patterns = namespace_patterns , annotation_url = annotation_url , annotation_patterns = annotation_patterns , annotation_list = annotation_list , ) for line in lines : print ( line , file = file ) if pmids is not None : for line in make_pubmed_abstract_group ( pmids ) : print ( line , file = file ) if entrez_ids is not None : for line in make_pubmed_gene_group ( entrez_ids ) : print ( line , file = file )
10884	def slicer ( self ) : return tuple ( np . s_ [ l : r ] for l , r in zip ( * self . bounds ) )
367	def rotation ( x , rg = 20 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if is_random : theta = np . pi / 180 * np . random . uniform ( - rg , rg ) else : theta = np . pi / 180 * rg rotation_matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) , 0 ] , [ np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
6852	def configure ( self , reboot = 1 ) : r = self . local_renderer for ip , hostname in self . iter_hostnames ( ) : self . vprint ( 'ip/hostname:' , ip , hostname ) r . genv . host_string = ip r . env . hostname = hostname with settings ( warn_only = True ) : r . sudo ( 'echo "{hostname}" > /etc/hostname' ) r . sudo ( 'echo "127.0.0.1 {hostname}" | cat - /etc/hosts > /tmp/out && mv /tmp/out /etc/hosts' ) r . sudo ( r . env . set_hostname_command ) if r . env . auto_reboot and int ( reboot ) : r . reboot ( )
11834	def connect ( self , A , B , distance = 1 ) : self . connect1 ( A , B , distance ) if not self . directed : self . connect1 ( B , A , distance )
10155	def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) # Parameters shouldn't have a title schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
987	def mmPrettyPrintSequenceCellRepresentations ( self , sortby = "Column" ) : self . _mmComputeTransitionTraces ( ) table = PrettyTable ( [ "Pattern" , "Column" , "predicted=>active cells" ] ) for sequenceLabel , predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . iteritems ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) for column , cells in cellsForColumn . iteritems ( ) : table . add_row ( [ sequenceLabel , column , list ( cells ) ] ) return table . get_string ( sortby = sortby ) . encode ( "utf-8" )
1995	def _named_stream ( self , name , binary = False ) : with self . _store . save_stream ( self . _named_key ( name ) , binary = binary ) as s : yield s
7316	def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field_items = field . split ( '.' ) field_name = getattr ( model , field_items [ 0 ] , None ) class_name = field_name . property . mapper . class_ new_model = getattr ( class_name , field_items [ 1 ] ) return field_name . has ( OPERATORS [ operator ] ( new_model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
438	def read_and_decode ( filename , is_train = None ) : filename_queue = tf . train . string_input_producer ( [ filename ] ) reader = tf . TFRecordReader ( ) _ , serialized_example = reader . read ( filename_queue ) features = tf . parse_single_example ( serialized_example , features = { 'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , } ) # You can do more image distortion here for training data img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) # img = tf.cast(img, tf.float32) #* (1. / 255) - 0.5 if is_train == True : # 1. Randomly crop a [height, width] section of the image. img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) # 2. Randomly flip the image horizontally. img = tf . image . random_flip_left_right ( img ) # 3. Randomly change brightness. img = tf . image . random_brightness ( img , max_delta = 63 ) # 4. Randomly change contrast. img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) # 5. Subtract off the mean and divide by the variance of the pixels. img = tf . image . per_image_standardization ( img ) elif is_train == False : # 1. Crop the central [height, width] of the image. img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) # 2. Subtract off the mean and divide by the variance of the pixels. img = tf . image . per_image_standardization ( img ) elif is_train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label
9833	def initialize ( self ) : return self . DXclasses [ self . type ] ( self . id , * * self . args )
9961	def show_tree ( model = None ) : if model is None : model = mx . cur_model ( ) view = get_modeltree ( model ) app = QApplication . instance ( ) if not app : raise RuntimeError ( "QApplication does not exist." ) view . show ( ) app . exec_ ( )
2414	def write_creation_info ( creation_info , out ) : out . write ( '# Creation Info\n\n' ) # Write sorted creators for creator in sorted ( creation_info . creators ) : write_value ( 'Creator' , creator , out ) # write created write_value ( 'Created' , creation_info . created_iso_format , out ) # possible comment if creation_info . has_comment : write_text_value ( 'CreatorComment' , creation_info . comment , out )
11133	def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
13550	def _get_resource ( self , url , data_key = None ) : headers = { "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . getURL ( url , headers ) if response . status != 200 : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
10559	def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
8847	def mouseMoveEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mouseMoveEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) assert isinstance ( cursor , QtGui . QTextCursor ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if QtWidgets . QApplication . overrideCursor ( ) is None : QtWidgets . QApplication . setOverrideCursor ( QtGui . QCursor ( QtCore . Qt . PointingHandCursor ) ) else : if QtWidgets . QApplication . overrideCursor ( ) is not None : QtWidgets . QApplication . restoreOverrideCursor ( )
6834	def vagrant ( self , name = '' ) : r = self . local_renderer config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) r . genv . update ( extra_args )
9311	def amz_cano_querystring ( qs ) : safe_qs_amz_chars = '&=+' safe_qs_unresvd = '-_.~' # If Python 2, switch to working entirely in str # as quote() has problems with Unicode if PY2 : qs = qs . encode ( 'utf-8' ) safe_qs_amz_chars = safe_qs_amz_chars . encode ( ) safe_qs_unresvd = safe_qs_unresvd . encode ( ) qs = unquote ( qs ) space = b' ' if PY2 else ' ' qs = qs . split ( space ) [ 0 ] qs = quote ( qs , safe = safe_qs_amz_chars ) qs_items = { } for name , vals in parse_qs ( qs , keep_blank_values = True ) . items ( ) : name = quote ( name , safe = safe_qs_unresvd ) vals = [ quote ( val , safe = safe_qs_unresvd ) for val in vals ] qs_items [ name ] = vals qs_strings = [ ] for name , vals in qs_items . items ( ) : for val in vals : qs_strings . append ( '=' . join ( [ name , val ] ) ) qs = '&' . join ( sorted ( qs_strings ) ) if PY2 : qs = unicode ( qs ) return qs
1405	def validate_extra_link ( self , extra_link ) : if EXTRA_LINK_NAME_KEY not in extra_link or EXTRA_LINK_FORMATTER_KEY not in extra_link : raise Exception ( "Invalid extra.links format. " + "Extra link must include a 'name' and 'formatter' field" ) self . validated_formatter ( extra_link [ EXTRA_LINK_FORMATTER_KEY ] ) return extra_link
1000	def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
6985	def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : if outdir and not os . path . exists ( outdir ) : os . mkdir ( outdir ) if maxobjects is not None : lclist = lclist [ : maxobjects ] tasks = [ ( x , binsizesec , { 'outdir' : outdir , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'minbinelems' : minbinelems } ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( timebinlc_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
12183	def method_exists ( cls , method ) : methods = cls . API_METHODS for key in method . split ( '.' ) : methods = methods . get ( key ) if methods is None : break if isinstance ( methods , str ) : logger . debug ( '%r: %r' , method , methods ) return True return False
2135	def associate_notification_template ( self , workflow , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , workflow , notification_template )
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
218	def parse_docstring ( self , func_or_method : typing . Callable ) -> dict : docstring = func_or_method . __doc__ if not docstring : return { } # We support having regular docstrings before the schema # definition. Here we return just the schema part from # the docstring. docstring = docstring . split ( "---" ) [ - 1 ] parsed = yaml . safe_load ( docstring ) if not isinstance ( parsed , dict ) : # A regular docstring (not yaml formatted) can return # a simple string here, which wouldn't follow the schema. return { } return parsed
4336	def overdrive ( self , gain_db = 20.0 , colour = 20.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'db_level must be a number.' ) if not is_number ( colour ) : raise ValueError ( 'colour must be a number.' ) effect_args = [ 'overdrive' , '{:f}' . format ( gain_db ) , '{:f}' . format ( colour ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'overdrive' ) return self
6950	def jhk_to_sdssi ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSI_JHK , SDSSI_JH , SDSSI_JK , SDSSI_HK , SDSSI_J , SDSSI_H , SDSSI_K )
2775	def add_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = POST , params = { "droplet_ids" : droplet_ids } )
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
11004	def psffunc ( self , x , y , z , * * kwargs ) : #do_pinhole?? FIXME if self . polychromatic : func = psfcalc . calculate_polychrome_pinhole_psf else : func = psfcalc . calculate_pinhole_psf x0 , y0 = [ psfcalc . vec_to_halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap_and_calc_psf ( x0 , y0 , z , func , * * kwargs ) return vls / vls . sum ( )
2776	def remove_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = DELETE , params = { "droplet_ids" : droplet_ids } )
2210	def parse_requirements ( fname = 'requirements.txt' ) : from os . path import dirname , join , exists import re require_fpath = join ( dirname ( __file__ ) , fname ) def parse_line ( line ) : """ Parse information from a line in a requirements text file """ info = { } if line . startswith ( '-e ' ) : info [ 'package' ] = line . split ( '#egg=' ) [ 1 ] else : # Remove versioning from the package pat = '(' + '|' . join ( [ '>=' , '==' , '>' ] ) + ')' parts = re . split ( pat , line , maxsplit = 1 ) parts = [ p . strip ( ) for p in parts ] info [ 'package' ] = parts [ 0 ] if len ( parts ) > 1 : op , rest = parts [ 1 : ] if ';' in rest : # Handle platform specific dependencies # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies version , platform_deps = map ( str . strip , rest . split ( ';' ) ) info [ 'platform_deps' ] = platform_deps else : version = rest # NOQA info [ 'version' ] = ( op , version ) return info # This breaks on pip install, so check that it exists. if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as f : packages = [ ] for line in f . readlines ( ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : info = parse_line ( line ) package = info [ 'package' ] if not sys . version . startswith ( '3.4' ) : # apparently package_deps are broken in 3.4 platform_deps = info . get ( 'platform_deps' ) if platform_deps is not None : package += ';' + platform_deps packages . append ( package ) return packages return [ ]
2194	def encoding ( self ) : if self . redirect is not None : return self . redirect . encoding else : return super ( TeeStringIO , self ) . encoding
12421	def dump ( obj , fp , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : if startindex < 0 : raise ValueError ( 'startindex must be non-negative, but was {}' . format ( startindex ) ) try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return if isinstance ( firstkey , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator for key , value in six . iteritems ( obj ) : if isinstance ( value , ( list , tuple , set ) ) : for index , item in enumerate ( value , start = startindex ) : fp . write ( key ) fp . write ( index_separator ) fp . write ( converter ( str ( index ) ) ) fp . write ( separator ) fp . write ( item ) fp . write ( newline ) else : fp . write ( key ) fp . write ( separator ) fp . write ( value ) fp . write ( newline )
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
11437	def _fields_sort_by_indicators ( fields ) : field_dict = { } field_positions_global = [ ] for field in fields : field_dict . setdefault ( field [ 1 : 3 ] , [ ] ) . append ( field ) field_positions_global . append ( field [ 4 ] ) indicators = field_dict . keys ( ) indicators . sort ( ) field_list = [ ] for indicator in indicators : for field in field_dict [ indicator ] : field_list . append ( field [ : 4 ] + ( field_positions_global . pop ( 0 ) , ) ) return field_list
9477	def parse_dom ( dom ) : root = dom . getElementsByTagName ( "graphml" ) [ 0 ] graph = root . getElementsByTagName ( "graph" ) [ 0 ] name = graph . getAttribute ( 'id' ) g = Graph ( name ) # # Get attributes # attributes = [] # for attr in root.getElementsByTagName("key"): # attributes.append(attr) # Get nodes for node in graph . getElementsByTagName ( "node" ) : n = g . add_node ( id = node . getAttribute ( 'id' ) ) for attr in node . getElementsByTagName ( "data" ) : if attr . firstChild : n [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : n [ attr . getAttribute ( "key" ) ] = "" # Get edges for edge in graph . getElementsByTagName ( "edge" ) : source = edge . getAttribute ( 'source' ) dest = edge . getAttribute ( 'target' ) # source/target attributes refer to IDs: http://graphml.graphdrawing.org/xmlns/1.1/graphml-structure.xsd e = g . add_edge_by_id ( source , dest ) for attr in edge . getElementsByTagName ( "data" ) : if attr . firstChild : e [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : e [ attr . getAttribute ( "key" ) ] = "" return g
4192	def compute_response ( self , * * kargs ) : from numpy . fft import fft , fftshift norm = kargs . get ( 'norm' , self . norm ) # do some padding. Default is max(2048, data.len*2) NFFT = kargs . get ( 'NFFT' , 2048 ) if NFFT < len ( self . data ) : NFFT = self . data . size * 2 # compute the fft modulus A = fft ( self . data , NFFT ) mag = abs ( fftshift ( A ) ) # do we want to normalise the data if norm is True : mag = mag / max ( mag ) response = 20. * stools . log10 ( mag ) # factor 20 we are looking at the response # not the powe #response = clip(response,mindB,100) self . __response = response
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
10552	def _forbidden_attributes ( obj ) : for key in list ( obj . data . keys ( ) ) : if key in list ( obj . reserved_keys . keys ( ) ) : obj . data . pop ( key ) return obj
6954	def make_combined_periodogram ( pflist , outfile , addmethods = False ) : import matplotlib . pyplot as plt for pf in pflist : if pf [ 'method' ] == 'pdm' : plt . plot ( pf [ 'periods' ] , np . max ( pf [ 'lspvals' ] ) / pf [ 'lspvals' ] - 1.0 , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) else : plt . plot ( pf [ 'periods' ] , pf [ 'lspvals' ] / np . max ( pf [ 'lspvals' ] ) , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) plt . xlabel ( 'period [days]' ) plt . ylabel ( 'normalized periodogram power' ) plt . xscale ( 'log' ) plt . legend ( ) plt . tight_layout ( ) plt . savefig ( outfile ) plt . close ( 'all' ) return outfile
8475	def _getClassInstance ( path , args = None ) : if not path . endswith ( ".py" ) : return None if args is None : args = { } classname = AtomShieldsScanner . _getClassName ( path ) basename = os . path . basename ( path ) . replace ( ".py" , "" ) sys . path . append ( os . path . dirname ( path ) ) try : mod = __import__ ( basename , globals ( ) , locals ( ) , [ classname ] , - 1 ) class_ = getattr ( mod , classname ) instance = class_ ( * * args ) except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) return None finally : sys . path . remove ( os . path . dirname ( path ) ) return instance
2761	def get_certificate ( self , id ) : return Certificate . get_object ( api_token = self . token , cert_id = id )
4654	def constructTx ( self ) : ops = list ( ) for op in self . ops : if isinstance ( op , ProposalBuilder ) : # This operation is a proposal an needs to be deal with # differently proposal = op . get_raw ( ) if proposal : ops . append ( proposal ) elif isinstance ( op , self . operation_class ) : ops . extend ( [ op ] ) else : # otherwise, we simply wrap ops into Operations ops . extend ( [ self . operation_class ( op ) ] ) # We now wrap everything into an actual transaction ops = self . add_required_fees ( ops , asset_id = self . fee_asset_id ) expiration = formatTimeFromNow ( self . expiration or self . blockchain . expiration or 30 # defaults to 30 seconds ) ref_block_num , ref_block_prefix = self . get_block_params ( ) self . tx = self . signed_transaction_class ( ref_block_num = ref_block_num , ref_block_prefix = ref_block_prefix , expiration = expiration , operations = ops , ) dict . update ( self , self . tx . json ( ) ) self . _unset_require_reconstruction ( )
6153	def fir_remez_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandstop_order ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fsamp = fs ) # Bump up the order by N_bump to bring down the final d_pass & d_stop # Initially make sure the number of taps is even so N_bump needs to be odd if np . mod ( n , 2 ) != 0 : n += 1 N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 , maxiter = 25 , grid_density = 16 ) print ( 'N_bump must be odd to maintain odd filter length' ) print ( 'Remez filter taps = %d.' % N_taps ) return b
9840	def __array ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'type' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: type was "%s", not a string.' % tok . text ) self . currentobject [ 'type' ] = tok . value ( ) elif tok . equals ( 'rank' ) : tok = self . __consume ( ) try : self . currentobject [ 'rank' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: rank was "%s", not an integer.' % tok . text ) elif tok . equals ( 'items' ) : tok = self . __consume ( ) try : self . currentobject [ 'size' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: items was "%s", not an integer.' % tok . text ) elif tok . equals ( 'data' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: data was "%s", not a string.' % tok . text ) if tok . text != 'follows' : raise NotImplementedError ( 'array: Only the "data follows header" format is supported.' ) if not self . currentobject [ 'size' ] : raise DXParseError ( "array: missing number of items" ) # This is the slow part. Once we get here, we are just # reading in a long list of numbers. Conversion to floats # will be done later when the numpy array is created. # Don't assume anything about whitespace or the number of elements per row self . currentobject [ 'array' ] = [ ] while len ( self . currentobject [ 'array' ] ) < self . currentobject [ 'size' ] : self . currentobject [ 'array' ] . extend ( self . dxfile . readline ( ) . strip ( ) . split ( ) ) # If you assume that there are three elements per row # (except the last) the following version works and is a little faster. # for i in range(int(numpy.ceil(self.currentobject['size']/3))): # self.currentobject['array'].append(self.dxfile.readline()) # self.currentobject['array'] = ' '.join(self.currentobject['array']).split() elif tok . equals ( 'attribute' ) : # not used at the moment attribute = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'string' ) : raise DXParseError ( 'array: "string" expected.' ) value = self . __consume ( ) . value ( ) else : raise DXParseError ( 'array: ' + str ( tok ) + ' not recognized.' )
6307	def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
5256	def disassemble_all ( bytecode , pc = 0 , fork = DEFAULT_FORK ) : if isinstance ( bytecode , bytes ) : bytecode = bytearray ( bytecode ) if isinstance ( bytecode , str ) : bytecode = bytearray ( bytecode . encode ( 'latin-1' ) ) bytecode = iter ( bytecode ) while True : instr = disassemble_one ( bytecode , pc = pc , fork = fork ) if not instr : return pc += instr . size yield instr
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
11819	def go ( self , state , direction ) : state1 = vector_add ( state , direction ) return if_ ( state1 in self . states , state1 , state )
12620	def have_same_affine ( one_img , another_img , only_check_3d = False ) : img1 = check_img ( one_img ) img2 = check_img ( another_img ) ndim1 = len ( img1 . shape ) ndim2 = len ( img2 . shape ) if ndim1 < 3 : raise ValueError ( 'Image {} has only {} dimensions, at least 3 dimensions is expected.' . format ( repr_imgs ( img1 ) , ndim1 ) ) if ndim2 < 3 : raise ValueError ( 'Image {} has only {} dimensions, at least 3 dimensions is expected.' . format ( repr_imgs ( img2 ) , ndim1 ) ) affine1 = img1 . get_affine ( ) affine2 = img2 . get_affine ( ) if only_check_3d : affine1 = affine1 [ : 3 , : 3 ] affine2 = affine2 [ : 3 , : 3 ] try : return np . allclose ( affine1 , affine2 ) except ValueError : return False except : raise
6244	def draw ( self , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : if self . mesh_program : self . mesh_program . draw ( self , projection_matrix = projection_matrix , view_matrix = view_matrix , camera_matrix = camera_matrix , time = time )
2070	def basen_to_integer ( self , X , cols , base ) : out_cols = X . columns . values . tolist ( ) for col in cols : col_list = [ col0 for col0 in out_cols if str ( col0 ) . startswith ( str ( col ) ) ] insert_at = out_cols . index ( col_list [ 0 ] ) if base == 1 : value_array = np . array ( [ int ( col0 . split ( '_' ) [ - 1 ] ) for col0 in col_list ] ) else : len0 = len ( col_list ) value_array = np . array ( [ base ** ( len0 - 1 - i ) for i in range ( len0 ) ] ) X . insert ( insert_at , col , np . dot ( X [ col_list ] . values , value_array . T ) ) X . drop ( col_list , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
3958	def resolve ( cls , all_known_repos , name ) : match = None for repo in all_known_repos : if repo . remote_path == name : # user passed in a full name return repo if name == repo . short_name : if match is None : match = repo else : raise RuntimeError ( 'Short repo name {} is ambiguous. It matches both {} and {}' . format ( name , match . remote_path , repo . remote_path ) ) if match is None : raise RuntimeError ( 'Short repo name {} does not match any known repos' . format ( name ) ) return match
4839	def get_course_details ( self , course_id ) : return self . _load_data ( self . COURSES_ENDPOINT , resource_id = course_id , many = False )
7559	def set_mkl_thread_limit ( cores ) : if "linux" in sys . platform : mkl_rt = ctypes . CDLL ( 'libmkl_rt.so' ) else : mkl_rt = ctypes . CDLL ( 'libmkl_rt.dylib' ) oldlimit = mkl_rt . mkl_get_max_threads ( ) mkl_rt . mkl_set_num_threads ( ctypes . byref ( ctypes . c_int ( cores ) ) ) return oldlimit
7650	def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : with _open ( path_or_file , mode = 'r' , fmt = fmt ) as fdesc : jam = JAMS ( * * json . load ( fdesc ) ) if validate : jam . validate ( strict = strict ) return jam
13366	def apply ( f , obj , * args , * * kwargs ) : return vectorize ( f ) ( obj , * args , * * kwargs )
11781	def compare ( algorithms = [ PluralityLearner , NaiveBayesLearner , NearestNeighborLearner , DecisionTreeLearner ] , datasets = [ iris , orings , zoo , restaurant , SyntheticRestaurant ( 20 ) , Majority ( 7 , 100 ) , Parity ( 7 , 100 ) , Xor ( 100 ) ] , k = 10 , trials = 1 ) : print_table ( [ [ a . __name__ . replace ( 'Learner' , '' ) ] + [ cross_validation ( a , d , k , trials ) for d in datasets ] for a in algorithms ] , header = [ '' ] + [ d . name [ 0 : 7 ] for d in datasets ] , numfmt = '%.2f' )
9800	def bookmark ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : PolyaxonClient ( ) . experiment_group . bookmark ( user , project_name , _group ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments group is bookmarked." )
1987	def save_state ( self , state , key ) : with self . save_stream ( key , binary = True ) as f : self . _serializer . serialize ( state , f )
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
11625	def generate ( grammar = None , num = 1 , output = sys . stdout , max_recursion = 10 , seed = None ) : if seed is not None : gramfuzz . rand . seed ( seed ) fuzzer = gramfuzz . GramFuzzer ( ) fuzzer . load_grammar ( grammar ) cat_group = os . path . basename ( grammar ) . replace ( ".py" , "" ) results = fuzzer . gen ( cat_group = cat_group , num = num , max_recursion = max_recursion ) for res in results : output . write ( res )
1994	def save_state ( self , state , state_id = None ) : assert isinstance ( state , StateBase ) if state_id is None : state_id = self . _get_id ( ) else : self . rm_state ( state_id ) self . _store . save_state ( state , f'{self._prefix}{state_id:08x}{self._suffix}' ) return state_id
7572	def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : ## try refreshing taker, else quit try : taker = itertools . takewhile ( lambda x : x [ 0 ] != "//\n" , pairdealer ) oneclust = [ "" . join ( taker . next ( ) ) ] except StopIteration : #LOGGER.debug('last chunk %s', chunk) return 1 , chunk ## load one cluster while 1 : try : oneclust . append ( "" . join ( taker . next ( ) ) ) except StopIteration : break chunk . append ( "" . join ( oneclust ) ) ccnt += 1 return 0 , chunk
8284	def _segment_lengths ( self , relative = False , n = 20 ) : # From nodebox_gl lengths = [ ] first = True for el in self . _get_elements ( ) : if first is True : close_x , close_y = el . x , el . y first = False elif el . cmd == MOVETO : close_x , close_y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . _linelength ( x0 , y0 , close_x , close_y ) ) elif el . cmd == LINETO : lengths . append ( self . _linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y # (el.c1x, el.c1y, el.c2x, el.c2y, el.x, el.y) lengths . append ( self . _curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : # Relative segment lengths' sum is 1.0. return map ( lambda l : l / length , lengths ) except ZeroDivisionError : # If the length is zero, just return zero for all segments return [ 0.0 ] * len ( lengths ) else : return lengths
13001	def hr_diagram_from_data ( data , x_range , y_range ) : _ , color_mapper = hr_diagram_color_helper ( [ ] ) data_dict = { 'x' : list ( data [ 'temperature' ] ) , 'y' : list ( data [ 'luminosity' ] ) , 'color' : list ( data [ 'color' ] ) } source = ColumnDataSource ( data = data_dict ) pf = figure ( y_axis_type = 'log' , x_range = x_range , y_range = y_range ) _diagram ( source = source , plot_figure = pf , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) show_with_bokeh_server ( pf )
223	def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } # Get server name and port - required in WSGI, not in ASGI server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] # Get client IP address if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] # Go through headers and make them into environ entries for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) # HTTPbis say only ASCII chars are allowed in headers, but we latin1 just in case value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
2633	def scale_in ( self , blocks ) : status = dict ( zip ( self . engines , self . provider . status ( self . engines ) ) ) # This works for blocks=0 to_kill = [ engine for engine in status if status [ engine ] == "RUNNING" ] [ : blocks ] if self . provider : r = self . provider . cancel ( to_kill ) else : logger . error ( "No execution provider available" ) r = None return r
8201	def settings ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
12551	def dump_raw_data ( filename , data ) : if data . ndim == 3 : # Begin 3D fix data = data . reshape ( [ data . shape [ 0 ] , data . shape [ 1 ] * data . shape [ 2 ] ] ) # End 3D fix a = array . array ( 'f' ) for o in data : a . fromlist ( list ( o . flatten ( ) ) ) # if is_little_endian(): # a.byteswap() with open ( filename , 'wb' ) as rawf : a . tofile ( rawf )
6136	def add_model_string ( self , model_str , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'inlined_input' ) ret_data = self . file_create ( File . from_string ( model_str , position , file_id ) ) return ret_data
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
5174	def events ( self , * * kwargs ) : return self . __api . events ( query = EqualsOperator ( "report" , self . hash_ ) , * * kwargs )
9246	def compound_changelog ( self ) : self . fetch_and_filter_tags ( ) tags_sorted = self . sort_tags_by_date ( self . filtered_tags ) self . filtered_tags = tags_sorted self . fetch_and_filter_issues_and_pr ( ) log = str ( self . options . frontmatter ) if self . options . frontmatter else u"" log += u"{0}\n\n" . format ( self . options . header ) if self . options . unreleased_only : log += self . generate_unreleased_section ( ) else : log += self . generate_log_for_all_tags ( ) try : with open ( self . options . base ) as fh : log += fh . read ( ) except ( TypeError , IOError ) : pass return log
8281	def _linepoint ( self , t , x0 , y0 , x1 , y1 ) : # Originally from nodebox-gl out_x = x0 + t * ( x1 - x0 ) out_y = y0 + t * ( y1 - y0 ) return ( out_x , out_y )
7588	def nexmake ( mdict , nlocus , dirs , mcmc_burnin , mcmc_ngen , mcmc_sample_freq ) : ## create matrix as a string max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = "{:<" + str ( max_name_len + 1 ) + "} {}\n" matrix = "" for i in mdict . items ( ) : matrix += namestring . format ( i [ 0 ] , i [ 1 ] ) ## write nexus block handle = os . path . join ( dirs , "{}.nex" . format ( nlocus ) ) with open ( handle , 'w' ) as outnex : outnex . write ( NEXBLOCK . format ( * * { "ntax" : len ( mdict ) , "nchar" : len ( mdict . values ( ) [ 0 ] ) , "matrix" : matrix , "ngen" : mcmc_ngen , "sfreq" : mcmc_sample_freq , "burnin" : mcmc_burnin , } ) )
3033	def credentials_from_clientsecrets_and_code ( filename , scope , code , message = None , redirect_uri = 'postmessage' , http = None , cache = None , device_uri = None ) : flow = flow_from_clientsecrets ( filename , scope , message = message , cache = cache , redirect_uri = redirect_uri , device_uri = device_uri ) credentials = flow . step2_exchange ( code , http = http ) return credentials
13824	def ToJsonString ( self ) : if self . seconds < 0 or self . nanos < 0 : result = '-' seconds = - self . seconds + int ( ( 0 - self . nanos ) // 1e9 ) nanos = ( 0 - self . nanos ) % 1e9 else : result = '' seconds = self . seconds + int ( self . nanos // 1e9 ) nanos = self . nanos % 1e9 result += '%d' % seconds if ( nanos % 1e9 ) == 0 : # If there are 0 fractional digits, the fractional # point '.' should be omitted when serializing. return result + 's' if ( nanos % 1e6 ) == 0 : # Serialize 3 fractional digits. return result + '.%03ds' % ( nanos / 1e6 ) if ( nanos % 1e3 ) == 0 : # Serialize 6 fractional digits. return result + '.%06ds' % ( nanos / 1e3 ) # Serialize 9 fractional digits. return result + '.%09ds' % nanos
10160	def ci ( ctx ) : opts = [ '' ] # 'tox' makes no sense in Travis if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
4067	def update_item ( self , payload , last_modified = None ) : to_send = self . check_items ( [ payload ] ) [ 0 ] if last_modified is None : modified = payload [ "version" ] else : modified = last_modified ident = payload [ "key" ] headers = { "If-Unmodified-Since-Version" : str ( modified ) } headers . update ( self . default_headers ( ) ) req = requests . patch ( url = self . endpoint + "/{t}/{u}/items/{id}" . format ( t = self . library_type , u = self . library_id , id = ident ) , headers = headers , data = json . dumps ( to_send ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
48	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : if copy : image = np . copy ( image ) if image . ndim == 2 : assert ia . is_single_number ( color ) , ( "Got a 2D image. Expected then 'color' to be a single number, " "but got %s." % ( str ( color ) , ) ) elif image . ndim == 3 and ia . is_single_number ( color ) : color = [ color ] * image . shape [ - 1 ] input_dtype = image . dtype alpha_color = color if alpha < 0.01 : # keypoint invisible, nothing to do return image elif alpha > 0.99 : alpha = 1 else : image = image . astype ( np . float32 , copy = False ) alpha_color = alpha * np . array ( color ) height , width = image . shape [ 0 : 2 ] y , x = self . y_int , self . x_int x1 = max ( x - size // 2 , 0 ) x2 = min ( x + 1 + size // 2 , width ) y1 = max ( y - size // 2 , 0 ) y2 = min ( y + 1 + size // 2 , height ) x1_clipped , x2_clipped = np . clip ( [ x1 , x2 ] , 0 , width ) y1_clipped , y2_clipped = np . clip ( [ y1 , y2 ] , 0 , height ) x1_clipped_ooi = ( x1_clipped < 0 or x1_clipped >= width ) x2_clipped_ooi = ( x2_clipped < 0 or x2_clipped >= width + 1 ) y1_clipped_ooi = ( y1_clipped < 0 or y1_clipped >= height ) y2_clipped_ooi = ( y2_clipped < 0 or y2_clipped >= height + 1 ) x_ooi = ( x1_clipped_ooi and x2_clipped_ooi ) y_ooi = ( y1_clipped_ooi and y2_clipped_ooi ) x_zero_size = ( x2_clipped - x1_clipped ) < 1 # min size is 1px y_zero_size = ( y2_clipped - y1_clipped ) < 1 if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size : if alpha == 1 : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = color else : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = ( ( 1 - alpha ) * image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] + alpha_color ) else : if raise_if_out_of_image : raise Exception ( "Cannot draw keypoint x=%.8f, y=%.8f on image with " "shape %s." % ( y , x , image . shape ) ) if image . dtype . name != input_dtype . name : if input_dtype . name == "uint8" : image = np . clip ( image , 0 , 255 , out = image ) image = image . astype ( input_dtype , copy = False ) return image
1807	def SETBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) , 1 , 0 ) )
12652	def generate_config ( output_directory ) : if not op . isdir ( output_directory ) : os . makedirs ( output_directory ) config_file = op . join ( output_directory , "config.ini" ) open_file = open ( config_file , "w" ) open_file . write ( "[BOOL]\nManualNIfTIConv=0\n" ) open_file . close ( ) return config_file
6721	def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
8196	def click ( self , node ) : if not self . has_node ( node . id ) : return if node == self . root : return self . _dx , self . _dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
8420	def same_log10_order_of_magnitude ( x , delta = 0.1 ) : dmin = np . log10 ( np . min ( x ) * ( 1 - delta ) ) dmax = np . log10 ( np . max ( x ) * ( 1 + delta ) ) return np . floor ( dmin ) == np . floor ( dmax )
3698	def Hsub ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : # pragma: no cover def list_methods ( ) : methods = [ ] # if Hfus(T=T, P=P, MW=MW, CASRN=CASRN) and Hvap(T=T, P=P, MW=MW, CASRN=CASRN): # methods.append('Hfus + Hvap') if CASRN in GharagheiziHsub_data . index : methods . append ( 'Ghazerati Appendix, at 298K' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section # if Method == 'Hfus + Hvap': # p1 = Hfus(T=T, P=P, MW=MW, CASRN=CASRN) # p2 = Hvap(T=T, P=P, MW=MW, CASRN=CASRN) # if p1 and p2: # _Hsub = p1 + p2 # else: # _Hsub = None if Method == 'Ghazerati Appendix, at 298K' : _Hsub = float ( GharagheiziHsub_data . at [ CASRN , 'Hsub' ] ) elif Method == 'None' or not _Hsub or not MW : return None else : raise Exception ( 'Failure in in function' ) _Hsub = property_molar_to_mass ( _Hsub , MW ) return _Hsub
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : # pylint: disable-msg=W0212 logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
723	def getTerminationCallbacks ( self , terminationFunc ) : activities = [ None ] * len ( ModelTerminator . _MILESTONES ) for index , ( iteration , _ ) in enumerate ( ModelTerminator . _MILESTONES ) : cb = functools . partial ( terminationFunc , index = index ) activities [ index ] = PeriodicActivityRequest ( repeating = False , period = iteration , cb = cb )
4759	def env ( ) : ssh = cij . env_to_dict ( PREFIX , REQUIRED ) if "KEY" in ssh : ssh [ "KEY" ] = cij . util . expand_path ( ssh [ "KEY" ] ) if cij . ENV . get ( "SSH_PORT" ) is None : cij . ENV [ "SSH_PORT" ] = "22" cij . warn ( "cij.ssh.env: SSH_PORT was not set, assigned: %r" % ( cij . ENV . get ( "SSH_PORT" ) ) ) if cij . ENV . get ( "SSH_CMD_TIME" ) is None : cij . ENV [ "SSH_CMD_TIME" ] = "1" cij . warn ( "cij.ssh.env: SSH_CMD_TIME was not set, assigned: %r" % ( cij . ENV . get ( "SSH_CMD_TIME" ) ) ) return 0
4894	def _collect_grades_data ( self , enterprise_enrollment , course_details ) : if self . grades_api is None : self . grades_api = GradesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : grades_data = self . grades_api . get_course_grade ( course_id , username ) except HttpNotFoundError as error : # Grade not found, so we have nothing to report. if hasattr ( error , 'content' ) : response_content = json . loads ( error . content ) if response_content . get ( 'error_code' , '' ) == 'user_not_enrolled' : # This means the user has an enterprise enrollment record but is not enrolled in the course yet LOGGER . info ( "User [%s] not enrolled in course [%s], enterprise enrollment [%d]" , username , course_id , enterprise_enrollment . pk ) return None , None , None LOGGER . error ( "No grades data found for [%d]: [%s], [%s]" , enterprise_enrollment . pk , course_id , username ) return None , None , None # Prepare to process the course end date and pass/fail grade course_end_date = course_details . get ( 'end' ) if course_end_date is not None : course_end_date = parse_datetime ( course_end_date ) now = timezone . now ( ) is_passing = grades_data . get ( 'passed' ) # We can consider a course complete if: # * the course's end date has passed if course_end_date is not None and course_end_date < now : completed_date = course_end_date grade = self . grade_passing if is_passing else self . grade_failing # * Or, the learner has a passing grade (as of now) elif is_passing : completed_date = now grade = self . grade_passing # Otherwise, the course is still in progress else : completed_date = None grade = self . grade_incomplete return completed_date , grade , is_passing
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
11827	def boggle_neighbors ( n2 , cache = { } ) : if cache . get ( n2 ) : return cache . get ( n2 ) n = exact_sqrt ( n2 ) neighbors = [ None ] * n2 for i in range ( n2 ) : neighbors [ i ] = [ ] on_top = i < n on_bottom = i >= n2 - n on_left = i % n == 0 on_right = ( i + 1 ) % n == 0 if not on_top : neighbors [ i ] . append ( i - n ) if not on_left : neighbors [ i ] . append ( i - n - 1 ) if not on_right : neighbors [ i ] . append ( i - n + 1 ) if not on_bottom : neighbors [ i ] . append ( i + n ) if not on_left : neighbors [ i ] . append ( i + n - 1 ) if not on_right : neighbors [ i ] . append ( i + n + 1 ) if not on_left : neighbors [ i ] . append ( i - 1 ) if not on_right : neighbors [ i ] . append ( i + 1 ) cache [ n2 ] = neighbors return neighbors
9320	def _validate_collection ( self ) : if not self . _id : msg = "No 'id' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _title : msg = "No 'title' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_read is None : msg = "No 'can_read' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_write is None : msg = "No 'can_write' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _id not in self . url : msg = "The collection '{}' does not match the url for queries '{}'" raise ValidationError ( msg . format ( self . _id , self . url ) )
12803	def get_room ( self , id ) : if id not in self . _rooms : self . _rooms [ id ] = Room ( self , id ) return self . _rooms [ id ]
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : # The user is already enrolled in the program, so redirect to the program's dashboard. return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) # Check to see if access to any of the course runs in the program are restricted for this user. course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
8623	def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException ( 'Error retrieving user id: %s' % response . text , response . text )
7967	def feed ( self , data ) : with self . lock : if self . in_use : raise StreamParseError ( "StreamReader.feed() is not reentrant!" ) self . in_use = True try : if not self . _started : # workaround for lxml bug when fed with a big chunk at once if len ( data ) > 1 : self . parser . feed ( data [ : 1 ] ) data = data [ 1 : ] self . _started = True if data : self . parser . feed ( data ) else : self . parser . close ( ) except ElementTree . ParseError , err : self . handler . stream_parse_error ( unicode ( err ) ) finally : self . in_use = False
12800	def is_text ( self ) : return self . type in [ self . _TYPE_PASTE , self . _TYPE_TEXT , self . _TYPE_TWEET ]
7685	def clicks ( annotation , sr = 22050 , length = None , * * kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , * * kwargs )
2968	def _sm_relieve_pain ( self , * args , * * kwargs ) : _logger . info ( "Ending the degradation for blockade %s" % self . _blockade_name ) self . _do_reset_all ( ) # set a timer for the next pain event millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
12056	def ftp_folder_match ( ftp , localFolder , deleteStuff = True ) : for fname in glob . glob ( localFolder + "/*.*" ) : ftp_upload ( ftp , fname ) return
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
2075	def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) # Some models, like logistic regression, like normalized features otherwise they underperform and/or take a long time to converge. # To be rigorous, we should have trained the normalization on each fold individually via pipelines. # See grid_search_example to learn how to do it. X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
3606	def get_async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_get_request , args = ( endpoint , params , headers ) , callback = callback )
4987	def get_course_run_id ( user , enterprise_customer , course_key ) : try : course = CourseCatalogApiServiceClient ( enterprise_customer . site ) . get_course_details ( course_key ) except ImproperlyConfigured : raise Http404 users_all_enrolled_courses = EnrollmentApiClient ( ) . get_enrolled_courses ( user . username ) users_active_course_runs = get_active_course_runs ( course , users_all_enrolled_courses ) if users_all_enrolled_courses else [ ] course_run = get_current_course_run ( course , users_active_course_runs ) if course_run : course_run_id = course_run [ 'key' ] return course_run_id else : raise Http404
10816	def is_member ( self , user , with_pending = False ) : m = Membership . get ( self , user ) if m is not None : if with_pending : return True elif m . state == MembershipState . ACTIVE : return True return False
13616	def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url )
4694	def regex_find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
7779	def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( "\n" ) started = 0 current = None for l in lines : if not l : continue if l [ - 1 ] == "\r" : l = l [ : - 1 ] if not l : continue if l [ 0 ] in " \t" : if current is None : continue current += l [ 1 : ] continue if not started and current and current . upper ( ) . strip ( ) == "BEGIN:VCARD" : started = 1 elif started and current . upper ( ) . strip ( ) == "END:VCARD" : current = None break elif current and started : self . _process_rfc2425_record ( current ) current = l if started and current : self . _process_rfc2425_record ( current )
11468	def rm ( self , filename ) : try : self . _ftp . delete ( filename ) except error_perm : # target is either a directory # either it does not exist try : current_folder = self . _ftp . pwd ( ) self . cd ( filename ) except error_perm : print ( '550 Delete operation failed %s ' 'does not exist!' % ( filename , ) ) else : self . cd ( current_folder ) print ( '550 Delete operation failed %s ' 'is a folder. Use rmdir function ' 'to delete it.' % ( filename , ) )
13191	def json_struct_to_xml ( json_obj , root , custom_namespace = None ) : if isinstance ( root , ( str , unicode ) ) : if root . startswith ( '!' ) : root = etree . Element ( '{%s}%s' % ( NS_PROTECTED , root [ 1 : ] ) ) elif root . startswith ( '+' ) : if not custom_namespace : raise Exception ( "JSON fields starts with +, but no custom namespace provided" ) root = etree . Element ( '{%s}%s' % ( custom_namespace , root [ 1 : ] ) ) else : root = etree . Element ( root ) if root . tag in ( 'attachments' , 'grouped_events' , 'media_files' ) : for link in json_obj : root . append ( json_link_to_xml ( link ) ) elif isinstance ( json_obj , ( str , unicode ) ) : root . text = json_obj elif isinstance ( json_obj , ( int , float ) ) : root . text = unicode ( json_obj ) elif isinstance ( json_obj , dict ) : if frozenset ( json_obj . keys ( ) ) == frozenset ( ( 'type' , 'coordinates' ) ) : root . append ( geojson_to_gml ( json_obj ) ) else : for key , val in json_obj . items ( ) : if key == 'url' or key . endswith ( '_url' ) : el = json_link_to_xml ( val , json_link_key_to_xml_rel ( key ) ) else : el = json_struct_to_xml ( val , key , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif isinstance ( json_obj , list ) : tag_name = root . tag if tag_name . endswith ( 'ies' ) : tag_name = tag_name [ : - 3 ] + 'y' elif tag_name . endswith ( 's' ) : tag_name = tag_name [ : - 1 ] for val in json_obj : el = json_struct_to_xml ( val , tag_name , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif json_obj is None : return None else : raise NotImplementedError return root
4261	def _restore_cache ( gallery ) : cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) try : if os . path . exists ( cachePath ) : with open ( cachePath , "rb" ) as cacheFile : gallery . exifCache = pickle . load ( cacheFile ) logger . debug ( "Loaded cache with %d entries" , len ( gallery . exifCache ) ) else : gallery . exifCache = { } except Exception as e : logger . warn ( "Could not load cache: %s" , e ) gallery . exifCache = { }
10782	def _feature_guess ( im , rad , minmass = None , use_tp = False , trim_edge = False ) : if minmass is None : # we use 1% of the feature size mass as a cutoff; # it's easier to remove than to add minmass = rad ** 3 * 4 / 3. * np . pi * 0.01 # 0.03 is a magic number; works well if use_tp : diameter = np . ceil ( 2 * rad ) diameter += 1 - ( diameter % 2 ) df = peri . trackpy . locate ( im , int ( diameter ) , minmass = minmass ) npart = np . array ( df [ 'mass' ] ) . size guess = np . zeros ( [ npart , 3 ] ) guess [ : , 0 ] = df [ 'z' ] guess [ : , 1 ] = df [ 'y' ] guess [ : , 2 ] = df [ 'x' ] mass = df [ 'mass' ] else : guess , mass = initializers . local_max_featuring ( im , radius = rad , minmass = minmass , trim_edge = trim_edge ) npart = guess . shape [ 0 ] # I want to return these sorted by mass: inds = np . argsort ( mass ) [ : : - 1 ] # biggest mass first return guess [ inds ] . copy ( ) , npart
10409	def finalized_canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ( 'p' , 'float64' ) , ( 'alpha' , 'float64' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_std' , 'float64' ) , ( 'percolation_probability_ci' , '(2,)float64' ) , ] ) fields . extend ( [ ( 'percolation_strength_mean' , 'float64' ) , ( 'percolation_strength_std' , 'float64' ) , ( 'percolation_strength_ci' , '(2,)float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_std' , '(5,)float64' ) , ( 'moments_ci' , '(5,2)float64' ) , ] ) return _ndarray_dtype ( fields )
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : """ we internally distinguish between tasks executed by backend and tasks executed with no specific backend. """ backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] # stopper won't be set unless wait_for_threads is True stopper = threading . Event ( ) # launching threads for tasks by backend if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : # Start new Threads and add them to the threads list to complete t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) # launch thread for global tasks if len ( global_tasks ) > 0 : # FIXME timer is applied to all global_tasks, does it make sense? # All tasks are executed in the same thread sequentially gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) # Give enough time create and run all threads stopper . set ( ) # All threads must stop in the next iteration # Wait for all threads to complete for t in threads : t . join ( ) # Checking for exceptions in threads to log them self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
4297	def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) # positionals._option_string_actions for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
6420	def readfile ( fn ) : with open ( path . join ( HERE , fn ) , 'r' , encoding = 'utf-8' ) as f : return f . read ( )
4054	def collections_sub ( self , collection , * * kwargs ) : query_string = "/{t}/{u}/collections/{c}/collections" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _build_query ( query_string )
4339	def pitch ( self , n_semitones , quick = False ) : if not is_number ( n_semitones ) : raise ValueError ( "n_semitones must be a positive number" ) if n_semitones < - 12 or n_semitones > 12 : logger . warning ( "Using an extreme pitch shift. " "Quality of results will be poor" ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'pitch' ] if quick : effect_args . append ( '-q' ) effect_args . append ( '{:f}' . format ( n_semitones * 100. ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'pitch' ) return self
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
10166	def get_md_device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\W+' , line ) # Raid status # Active or 'started'. An inactive array is usually faulty. # Stopped arrays aren't visible here. ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : # Raid type (ex: RAID5) ret [ 'type' ] = splitted [ 2 ] # Array's components ret [ 'components' ] = self . get_components ( line , with_type = True ) else : # Raid type (ex: RAID5) ret [ 'type' ] = None # Array's components ret [ 'components' ] = self . get_components ( line , with_type = False ) return ret
9287	def consumer ( self , callback , blocking = True , immortal = False , raw = False ) : if not self . _connected : raise ConnectionError ( "not connected to a server" ) line = b'' while True : try : for line in self . _socket_readlines ( blocking ) : if line [ 0 : 1 ] != b'#' : if raw : callback ( line ) else : callback ( self . _parse ( line ) ) else : self . logger . debug ( "Server: %s" , line . decode ( 'utf8' ) ) except ParseError as exp : self . logger . log ( 11 , "%s\n Packet: %s" , exp . message , exp . packet ) except UnknownFormat as exp : self . logger . log ( 9 , "%s\n Packet: %s" , exp . message , exp . packet ) except LoginError as exp : self . logger . error ( "%s: %s" , exp . __class__ . __name__ , exp . message ) except ( KeyboardInterrupt , SystemExit ) : raise except ( ConnectionDrop , ConnectionError ) : self . close ( ) if not immortal : raise else : self . connect ( blocking = blocking ) continue except GenericError : pass except StopIteration : break except : self . logger . error ( "APRS Packet: %s" , line ) raise if not blocking : break
7918	def _validate_ip_address ( family , address ) : try : info = socket . getaddrinfo ( address , 0 , family , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , address ) ) raise ValueError ( "Bad IP address" ) if not info : logger . debug ( "getaddrinfo result empty" ) raise ValueError ( "Bad IP address" ) addr = info [ 0 ] [ 4 ] logger . debug ( " got address: {0!r}" . format ( addr ) ) try : return socket . getnameinfo ( addr , socket . NI_NUMERICHOST ) [ 0 ] except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , addr ) ) raise ValueError ( "Bad IP address" )
311	def sortino_ratio ( returns , required_return = 0 , period = DAILY ) : return ep . sortino_ratio ( returns , required_return = required_return )
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
5126	def start_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = True
7479	def build_clustbits ( data , ipyclient , force ) : ## If you run this step then we clear all tmp .fa and .indel.h5 files if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) os . mkdir ( data . tmpdir ) ## parallel client lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " building clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) uhandle = os . path . join ( data . dirs . across , data . name + ".utemp" ) usort = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) async1 = "" ## skip usorting if not force and already exists if not os . path . exists ( usort ) or force : ## send sort job to engines. Sorted seeds allows us to work through ## the utemp file one locus at a time instead of reading all into mem. LOGGER . info ( "building reads file -- loading utemp file into mem" ) async1 = lbview . apply ( sort_seeds , * ( uhandle , usort ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async1 . ready ( ) : break else : time . sleep ( 0.1 ) ## send count seeds job to engines. async2 = lbview . apply ( count_seeds , usort ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 1 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async2 . ready ( ) : break else : time . sleep ( 0.1 ) ## wait for both to finish while printing progress timer nseeds = async2 . result ( ) ## send the clust bit building job to work and track progress async3 = lbview . apply ( sub_build_clustbits , * ( data , usort , nseeds ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 2 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async3 . ready ( ) : break else : time . sleep ( 0.1 ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 3 , printstr . format ( elapsed ) , spacer = data . _spacer ) print ( "" ) ## check for errors for job in [ async1 , async2 , async3 ] : try : if not job . successful ( ) : raise IPyradWarningExit ( job . result ( ) ) except AttributeError : ## If we skip usorting then async1 == "" so the call to ## successful() raises, but we can ignore it. pass
8038	def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass # Following assumes any variable messages take the format # of 'Fixed text "variable text".' only: # e.g. 'Unknown directive type "req".' # ---> 'Unknown directive type' # e.g. 'Unknown interpreted text role "need".' # ---> 'Unknown interpreted text role' if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
3653	def represent_pixel_location ( self ) : if self . data is None : return None # return self . _data . reshape ( self . height + self . y_padding , int ( self . width * self . _num_components_per_pixel + self . x_padding ) )
5543	def clip ( self , array , geometries , inverted = False , clip_buffer = 0 ) : return commons_clip . clip_array_with_vector ( array , self . tile . affine , geometries , inverted = inverted , clip_buffer = clip_buffer * self . tile . pixel_x_size )
4573	def hsv2rgb_spectrum ( hsv ) : h , s , v = hsv return hsv2rgb_raw ( ( ( h * 192 ) >> 8 , s , v ) )
9915	def update ( self , instance , validated_data ) : is_primary = validated_data . pop ( "is_primary" , False ) instance = super ( EmailSerializer , self ) . update ( instance , validated_data ) if is_primary : instance . set_primary ( ) return instance
1200	def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
12693	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) and is_disjoint ( self . __grouping . grouping_colnames , args , __DISJOINT_SETS_ERROR__ ) : return self . __do_aggregate ( clazz , new_col , * args )
8878	def find_libname ( self , name ) : names = [ "{}.lib" , "lib{}.lib" , "{}lib.lib" ] names = [ n . format ( name ) for n in names ] dirs = self . get_library_dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = "Could not find the {} library." . format ( name ) raise ValueError ( msg )
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue # Strip previous schema if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) # wow, such security token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
3951	def _read_comparator ( self , mux , gain , data_rate , mode , high_threshold , low_threshold , active_low , traditional , latching , num_readings ) : assert num_readings == 1 or num_readings == 2 or num_readings == 4 , 'Num readings must be 1, 2, or 4!' # Set high and low threshold register values. self . _device . writeList ( ADS1x15_POINTER_HIGH_THRESHOLD , [ ( high_threshold >> 8 ) & 0xFF , high_threshold & 0xFF ] ) self . _device . writeList ( ADS1x15_POINTER_LOW_THRESHOLD , [ ( low_threshold >> 8 ) & 0xFF , low_threshold & 0xFF ] ) # Now build up the appropriate config register value. config = ADS1x15_CONFIG_OS_SINGLE # Go out of power-down mode for conversion. # Specify mux value. config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET # Validate the passed in gain and then set it in the config. if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] # Set the mode (continuous or single shot). config |= mode # Get the default data rate if none is specified (default differs between # ADS1015 and ADS1115). if data_rate is None : data_rate = self . _data_rate_default ( ) # Set the data rate (this is controlled by the subclass as it differs # between ADS1015 and ADS1115). config |= self . _data_rate_config ( data_rate ) # Enable window mode if required. if not traditional : config |= ADS1x15_CONFIG_COMP_WINDOW # Enable active high mode if required. if not active_low : config |= ADS1x15_CONFIG_COMP_ACTIVE_HIGH # Enable latching mode if required. if latching : config |= ADS1x15_CONFIG_COMP_LATCHING # Set number of comparator hits before alerting. config |= ADS1x15_CONFIG_COMP_QUE [ num_readings ] # Send the config value to start the ADC conversion. # Explicitly break the 16-bit value down to a big endian pair of bytes. self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) # Wait for the ADC sample to finish based on the sample rate plus a # small offset to be sure (0.1 millisecond). time . sleep ( 1.0 / data_rate + 0.0001 ) # Retrieve the result. result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
6127	def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
4117	def poly2lsf ( a ) : #Line spectral frequencies are not defined for complex polynomials. # Normalize the polynomial a = numpy . array ( a ) if a [ 0 ] != 1 : a /= a [ 0 ] if max ( numpy . abs ( numpy . roots ( a ) ) ) >= 1.0 : error ( 'The polynomial must have all roots inside of the unit circle.' ) # Form the sum and differnce filters p = len ( a ) - 1 # The leading one in the polynomial is not used a1 = numpy . concatenate ( ( a , numpy . array ( [ 0 ] ) ) ) a2 = a1 [ - 1 : : - 1 ] P1 = a1 - a2 # Difference filter Q1 = a1 + a2 # Sum Filter # If order is even, remove the known root at z = 1 for P1 and z = -1 for Q1 # If odd, remove both the roots from P1 if p % 2 : # Odd order P , r = deconvolve ( P1 , [ 1 , 0 , - 1 ] ) Q = Q1 else : # Even order P , r = deconvolve ( P1 , [ 1 , - 1 ] ) Q , r = deconvolve ( Q1 , [ 1 , 1 ] ) rP = numpy . roots ( P ) rQ = numpy . roots ( Q ) aP = numpy . angle ( rP [ 1 : : 2 ] ) aQ = numpy . angle ( rQ [ 1 : : 2 ] ) lsf = sorted ( numpy . concatenate ( ( - aP , - aQ ) ) ) return lsf
10917	def get_residuals_update_tile ( st , padded_tile ) : inner_tile = st . ishape . intersection ( [ st . ishape , padded_tile ] ) return inner_tile . translate ( - st . pad )
12591	def get_reliabledictionary_schema ( client , application_name , service_name , dictionary_name , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) result = json . dumps ( dictionary . get_information ( ) , indent = 4 ) if ( output_file == None ) : output_file = "{}-{}-{}-schema-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( 'Printed schema information to: ' + output_file ) print ( result )
2826	def convert_tanh ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting tanh ...' ) if names == 'short' : tf_name = 'TANH' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) tanh = keras . layers . Activation ( 'tanh' , name = tf_name ) layers [ scope_name ] = tanh ( layers [ inputs [ 0 ] ] )
8982	def _set_pixel ( self , x , y , color ) : if not self . is_in_bounds ( x , y ) : return rgb = self . _convert_rrggbb_to_image_color ( color ) x -= self . _min_x y -= self . _min_y self . _image . putpixel ( ( x , y ) , rgb )
11314	def update_hidden_notes ( self ) : if not self . tag_as_cern : notes = record_get_field_instances ( self . record , tag = "595" ) for field in notes : for dummy , value in field [ 0 ] : if value == "CDS" : self . tag_as_cern = True record_delete_fields ( self . record , tag = "595" )
11	def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] if self . compute_Q : logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] logs += [ ( 'episode' , self . n_episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs
6495	def log_indexing_error ( cls , indexing_errors ) : indexing_errors_log = [ ] for indexing_error in indexing_errors : indexing_errors_log . append ( str ( indexing_error ) ) raise exceptions . ElasticsearchException ( ', ' . join ( indexing_errors_log ) )
5209	def info_qry ( tickers , flds ) -> str : full_list = '\n' . join ( [ f'tickers: {tickers[:8]}' ] + [ f' {tickers[n:(n + 8)]}' for n in range ( 8 , len ( tickers ) , 8 ) ] ) return f'{full_list}\nfields: {flds}'
12835	def on_exit_stage ( self ) : # 1. Let the forum react to the end of the game. Local forums don't # react to this, but remote forums take the opportunity to stop # trying to extract tokens from messages. self . forum . on_finish_game ( ) # 2. Let the actors react to the end of the game. for actor in self . actors : actor . on_finish_game ( ) # 3. Let the world react to the end of the game. with self . world . _unlock_temporarily ( ) : self . world . on_finish_game ( )
7803	def display_name ( self ) : if self . subject_name : return u", " . join ( [ u", " . join ( [ u"{0}={1}" . format ( k , v ) for k , v in dn_tuple ] ) for dn_tuple in self . subject_name ] ) for name_type in ( "XmppAddr" , "DNS" , "SRV" ) : names = self . alt_names . get ( name_type ) if names : return names [ 0 ] return u"<unknown>"
10404	def canonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'float64' ) , ( 'moments' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
7033	def get_new_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) # url for getting an API key url = '%s/api/key' % lcc_server # get the API key resp = urlopen ( url ) if resp . code == 200 : respdict = json . loads ( resp . read ( ) ) else : LOGERROR ( 'could not fetch the API key from LCC-Server at: %s' % lcc_server ) LOGERROR ( 'the HTTP status code was: %s' % resp . status_code ) return None # # now that we have an API key dict, get the API key out of it and write it # to the APIKEYFILE # apikey = respdict [ 'result' ] [ 'apikey' ] expires = respdict [ 'result' ] [ 'expires' ] # write this to the apikey file if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) # chmod it to the correct value os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
5367	def compact_interval_string ( value_list ) : if not value_list : return '' value_list . sort ( ) # Start by simply building up a list of separate contiguous intervals interval_list = [ ] curr = [ ] for val in value_list : if curr and ( val > curr [ - 1 ] + 1 ) : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) curr = [ val ] else : curr . append ( val ) if curr : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) # For each interval collapse it down to "first, last" or just "first" if # if first == last. return ',' . join ( [ '{}-{}' . format ( pair [ 0 ] , pair [ 1 ] ) if pair [ 0 ] != pair [ 1 ] else str ( pair [ 0 ] ) for pair in interval_list ] )
7945	def _continue_connect ( self ) : try : self . _socket . connect ( self . _dst_addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] == errno . EISCONN : pass elif err . args [ 0 ] in BLOCKING_ERRORS : return None elif self . _dst_addrs : self . _set_state ( "connect" ) return None elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return None else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) raise self . _connected ( )
1812	def SETNB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF == False , 1 , 0 ) )
1677	def ResetSection ( self , directive ) : # The name of the current section. self . _section = self . _INITIAL_SECTION # The path of last found header. self . _last_header = '' # Update list of includes. Note that we never pop from the # include list. if directive in ( 'if' , 'ifdef' , 'ifndef' ) : self . include_list . append ( [ ] ) elif directive in ( 'else' , 'elif' ) : self . include_list [ - 1 ] = [ ]
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
2966	def _sm_to_pain ( self , * args , * * kwargs ) : _logger . info ( "Starting chaos for blockade %s" % self . _blockade_name ) self . _do_blockade_event ( ) # start the timer to end the pain millisec = random . randint ( self . _run_min_time , self . _run_max_time ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
11257	def values ( prev , * keys , * * kw ) : d = next ( prev ) if isinstance ( d , dict ) : yield [ d [ k ] for k in keys if k in d ] for d in prev : yield [ d [ k ] for k in keys if k in d ] else : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ] for d in prev : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ]
3096	def callback_handler ( self ) : decorator = self class OAuth2Handler ( webapp . RequestHandler ) : """Handler for the redirect_uri of the OAuth 2.0 dance.""" @ login_required def get ( self ) : error = self . request . get ( 'error' ) if error : errormsg = self . request . get ( 'error_description' , error ) self . response . out . write ( 'The authorization request failed: {0}' . format ( _safe_html ( errormsg ) ) ) else : user = users . get_current_user ( ) decorator . _create_flow ( self ) credentials = decorator . flow . step2_exchange ( self . request . params ) decorator . _storage_class ( decorator . _credentials_class , None , decorator . _credentials_property_name , user = user ) . put ( credentials ) redirect_uri = _parse_state_value ( str ( self . request . get ( 'state' ) ) , user ) if redirect_uri is None : self . response . out . write ( 'The authorization request failed' ) return if ( decorator . _token_response_param and credentials . token_response ) : resp_json = json . dumps ( credentials . token_response ) redirect_uri = _helpers . _add_query_parameter ( redirect_uri , decorator . _token_response_param , resp_json ) self . redirect ( redirect_uri ) return OAuth2Handler
2177	def authorized ( self ) : if self . _client . client . signature_method == SIGNATURE_RSA : # RSA only uses resource_owner_key return bool ( self . _client . client . resource_owner_key ) else : # other methods of authentication use all three pieces return ( bool ( self . _client . client . client_secret ) and bool ( self . _client . client . resource_owner_key ) and bool ( self . _client . client . resource_owner_secret ) )
2176	def request ( self , method , url , data = None , headers = None , withhold_token = False , client_id = None , client_secret = None , * * kwargs ) : if not is_secure_transport ( url ) : raise InsecureTransportError ( ) if self . token and not withhold_token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance_hook [ "protected_request" ] ) , ) for hook in self . compliance_hook [ "protected_request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) # Attempt to retrieve and save new access token if expired except TokenExpiredError : if self . auto_refresh_url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto_refresh_url , ) # We mustn't pass auth twice. auth = kwargs . pop ( "auth" , None ) if client_id and client_secret and ( auth is None ) : log . debug ( 'Encoding client_id "%s" with client_secret as Basic auth credentials.' , client_id , ) auth = requests . auth . HTTPBasicAuth ( client_id , client_secret ) token = self . refresh_token ( self . auto_refresh_url , auth = auth , * * kwargs ) if self . token_updater : log . debug ( "Updating token to %s using %s." , token , self . token_updater ) self . token_updater ( token ) url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) else : raise TokenUpdated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( OAuth2Session , self ) . request ( method , url , headers = headers , data = data , * * kwargs )
4143	def _numpy_solver ( A , B ) : x = numpy . linalg . solve ( A , B ) return x
12365	def get ( self , id ) : info = super ( Images , self ) . get ( id ) return ImageActions ( self . api , parent = self , * * info )
2692	def split_grafs ( lines ) : graf = [ ] for line in lines : line = line . strip ( ) if len ( line ) < 1 : if len ( graf ) > 0 : yield "\n" . join ( graf ) graf = [ ] else : graf . append ( line ) if len ( graf ) > 0 : yield "\n" . join ( graf )
6021	def from_fits_renormalized ( cls , file_path , hdu , pixel_scale ) : psf = PSF . from_fits_with_scale ( file_path , hdu , pixel_scale ) psf [ : , : ] = np . divide ( psf , np . sum ( psf ) ) return psf
8807	def _make_job_dict ( job ) : body = { "id" : job . get ( 'id' ) , "action" : job . get ( 'action' ) , "completed" : job . get ( 'completed' ) , "tenant_id" : job . get ( 'tenant_id' ) , "created_at" : job . get ( 'created_at' ) , "transaction_id" : job . get ( 'transaction_id' ) , "parent_id" : job . get ( 'parent_id' , None ) } if not body [ 'transaction_id' ] : body [ 'transaction_id' ] = job . get ( 'id' ) completed = 0 for sub in job . subtransactions : if sub . get ( 'completed' ) : completed += 1 pct = 100 if job . get ( 'completed' ) else 0 if len ( job . subtransactions ) > 0 : pct = float ( completed ) / len ( job . subtransactions ) * 100.0 body [ 'transaction_percent' ] = int ( pct ) body [ 'completed_subtransactions' ] = completed body [ 'subtransactions' ] = len ( job . subtransactions ) return body
10809	def update ( self , name = None , description = None , privacy_policy = None , subscription_policy = None , is_managed = None ) : with db . session . begin_nested ( ) : if name is not None : self . name = name if description is not None : self . description = description if ( privacy_policy is not None and PrivacyPolicy . validate ( privacy_policy ) ) : self . privacy_policy = privacy_policy if ( subscription_policy is not None and SubscriptionPolicy . validate ( subscription_policy ) ) : self . subscription_policy = subscription_policy if is_managed is not None : self . is_managed = is_managed db . session . merge ( self ) return self
11742	def _compute_follow ( self ) : self . _follow [ self . start_symbol ] . add ( END_OF_INPUT ) while True : changed = False for nonterminal , productions in self . nonterminals . items ( ) : for production in productions : for i , symbol in enumerate ( production . rhs ) : if symbol not in self . nonterminals : continue first = self . first ( production . rhs [ i + 1 : ] ) new_follow = first - set ( [ EPSILON ] ) if EPSILON in first or i == ( len ( production . rhs ) - 1 ) : new_follow |= self . _follow [ nonterminal ] if new_follow - self . _follow [ symbol ] : self . _follow [ symbol ] |= new_follow changed = True if not changed : break
4914	def course_enrollments ( self , request , pk ) : enterprise_customer = self . get_object ( ) serializer = serializers . EnterpriseCustomerCourseEnrollmentsSerializer ( data = request . data , many = True , context = { 'enterprise_customer' : enterprise_customer , 'request_user' : request . user , } ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP_200_OK ) return Response ( serializer . errors , status = HTTP_400_BAD_REQUEST )
7467	def summarize_results ( self , individual_results = False ) : ## return results depending on algorithm ## algorithm 00 if ( not self . params . infer_delimit ) & ( not self . params . infer_sptree ) : if individual_results : ## return a list of parsed CSV results return [ _parse_00 ( i ) for i in self . files . outfiles ] else : ## concatenate each CSV and then get stats w/ describe return pd . concat ( [ pd . read_csv ( i , sep = '\t' , index_col = 0 ) for i in self . files . mcmcfiles ] ) . describe ( ) . T ## algorithm 01 if self . params . infer_delimit & ( not self . params . infer_sptree ) : return _parse_01 ( self . files . outfiles , individual = individual_results ) ## others else : return "summary function not yet ready for this type of result"
7329	async def request ( self , method , url , future , headers = None , session = None , encoding = None , * * kwargs ) : await self . setup # prepare request arguments, particularly the headers req_kwargs = await self . headers . prepare_request ( method = method , url = url , headers = headers , proxy = self . proxy , * * kwargs ) if encoding is None : encoding = self . encoding session = session if ( session is not None ) else self . _session logger . debug ( "making request with parameters: %s" % req_kwargs ) async with session . request ( * * req_kwargs ) as response : if response . status < 400 : data = await data_processing . read ( response , self . _loads , encoding = encoding ) future . set_result ( data_processing . PeonyResponse ( data = data , headers = response . headers , url = response . url , request = req_kwargs ) ) else : # throw exception if status is not 2xx await exceptions . throw ( response , loads = self . _loads , encoding = encoding , url = url )
8611	def get_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) ) return response
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : # FIXME: settings have statements, variables have rows WTF? :-( for statement in table . rows : if statement [ 0 ] != "" : yield statement
2794	def get_object ( cls , api_token , image_id_or_slug ) : if cls . _is_string ( image_id_or_slug ) : image = cls ( token = api_token , slug = image_id_or_slug ) image . load ( use_slug = True ) else : image = cls ( token = api_token , id = image_id_or_slug ) image . load ( ) return image
978	def _countOverlapIndices ( self , i , j ) : if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : iRep = self . bucketMap [ i ] jRep = self . bucketMap [ j ] return self . _countOverlap ( iRep , jRep ) else : raise ValueError ( "Either i or j don't exist" )
10284	def count_targets ( edge_iter : EdgeIterator ) -> Counter : return Counter ( v for _ , v , _ in edge_iter )
4479	def file_empty ( fp ) : # for python 2 we need to use a homemade peek() if six . PY2 : contents = fp . read ( ) fp . seek ( 0 ) return not bool ( contents ) else : return not fp . peek ( )
41	def update_priorities ( self , idxes , priorities ) : assert len ( idxes ) == len ( priorities ) for idx , priority in zip ( idxes , priorities ) : assert priority > 0 assert 0 <= idx < len ( self . _storage ) self . _it_sum [ idx ] = priority ** self . _alpha self . _it_min [ idx ] = priority ** self . _alpha self . _max_priority = max ( self . _max_priority , priority )
4627	def encrypt ( self , wif ) : if not self . unlocked ( ) : raise WalletLocked return format ( bip38 . encrypt ( str ( wif ) , self . masterkey ) , "encwif" )
837	def getDistances ( self , inputPattern ) : dist = self . _getDistances ( inputPattern ) return ( dist , self . _categoryList )
13088	def write_config ( self , initialize_indices = False ) : if not os . path . exists ( self . config_dir ) : os . mkdir ( self . config_dir ) with open ( self . config_file , 'w' ) as configfile : self . config . write ( configfile ) if initialize_indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create_connection create_connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
319	def calc_distribution_stats ( x ) : return pd . Series ( { 'mean' : np . mean ( x ) , 'median' : np . median ( x ) , 'std' : np . std ( x ) , '5%' : np . percentile ( x , 5 ) , '25%' : np . percentile ( x , 25 ) , '75%' : np . percentile ( x , 75 ) , '95%' : np . percentile ( x , 95 ) , 'IQR' : np . subtract . reduce ( np . percentile ( x , [ 75 , 25 ] ) ) , } )
5480	def cancel ( batch_fn , cancel_fn , ops ) : # Canceling many operations one-by-one can be slow. # The Pipelines API doesn't directly support a list of operations to cancel, # but the requests can be performed in batch. canceled_ops = [ ] error_messages = [ ] max_batch = 256 total_ops = len ( ops ) for first_op in range ( 0 , total_ops , max_batch ) : batch_canceled , batch_messages = _cancel_batch ( batch_fn , cancel_fn , ops [ first_op : first_op + max_batch ] ) canceled_ops . extend ( batch_canceled ) error_messages . extend ( batch_messages ) return canceled_ops , error_messages
10965	def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field_reduce_func ( fields )
5021	def get_enterprise_customer_from_catalog_id ( catalog_id ) : try : return str ( EnterpriseCustomerCatalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except EnterpriseCustomerCatalog . DoesNotExist : return None
9266	def sort_tags_by_date ( self , tags ) : if self . options . verbose : print ( "Sorting tags..." ) tags . sort ( key = lambda x : self . get_time_of_tag ( x ) ) tags . reverse ( ) return tags
12641	def get_config_value ( name , fallback = None ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . get ( 'servicefabric' , name , fallback )
5005	def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : # pylint: disable=invalid-name sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
7080	def tic_objectsearch ( objectid , idcol_to_use = "ID" , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'columns' : '*' , 'filters' : [ { "paramName" : idcol_to_use , "values" : [ str ( objectid ) ] } ] } service = 'Mast.Catalogs.Filtered.Tic' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
6766	def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is_file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]
12462	def prepare_args ( config , bootstrap ) : config = copy . deepcopy ( config ) environ = dict ( copy . deepcopy ( os . environ ) ) data = { 'env' : bootstrap [ 'env' ] , 'pip' : pip_cmd ( bootstrap [ 'env' ] , '' , return_path = True ) , 'requirements' : bootstrap [ 'requirements' ] } environ . update ( data ) if isinstance ( config , string_types ) : return config . format ( * * environ ) for key , value in iteritems ( config ) : if not isinstance ( value , string_types ) : continue config [ key ] = value . format ( * * environ ) return config_to_args ( config )
5123	def show_active ( self , * * kwargs ) : g = self . g for v in g . nodes ( ) : self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) is_active = False my_iter = g . in_edges ( v ) if g . is_directed ( ) else g . out_edges ( v ) for e in my_iter : ei = g . edge_index [ e ] if self . edge2queue [ ei ] . _active : is_active = True break if is_active : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_active' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) for e in g . edges ( ) : ei = g . edge_index [ e ] if self . edge2queue [ ei ] . _active : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , * * kwargs ) self . _update_all_colors ( )
11152	def sha256file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha256 , nbytes = nbytes , chunk_size = chunk_size )
2528	def get_annotation_date ( self , r_term ) : annotation_date_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationDate' ] , None ) ) ) if len ( annotation_date_list ) != 1 : self . error = True msg = 'Annotation must have exactly one annotation date.' self . logger . log ( msg ) return return six . text_type ( annotation_date_list [ 0 ] [ 2 ] )
3314	def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : # TODO: review this # XP and Vista MiniRedir submit PUT with Content-Length 0, # before LOCK and the real PUT. So we have to accept this. _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) # elif content_length < 0: # # TODO: review this # # If CONTENT_LENGTH is invalid, we may try to workaround this # # by reading until the end of the stream. This may block however! # # The iterator produced small chunks of varying size, but not # # sure, if we always get everything before it times out. # _logger.warning("PUT with invalid Content-Length (%s). " # "Trying to read all (this may timeout)..." # .format(environ.get("CONTENT_LENGTH"))) # nb = 0 # try: # for s in environ["wsgi.input"]: # environ["wsgidav.some_input_read"] = 1 # _logger.debug("PUT: read from wsgi.input.__iter__, len=%s" % len(s)) # yield s # nb += len (s) # except socket.timeout: # _logger.warning("PUT: input timed out after writing %s bytes" % nb) # hasErrors = True else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) # This happens with litmus expect-100 test: if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
8654	def get_messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset # GET /api/messages/0.1/messages response = make_get_request ( session , 'messages' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
10285	def get_subgraph_edges ( graph : BELGraph , annotation : str , value : str , source_filter = None , target_filter = None , ) : if source_filter is None : source_filter = keep_node_permissive if target_filter is None : target_filter = keep_node_permissive for u , v , k , data in graph . edges ( keys = True , data = True ) : if not edge_has_annotation ( data , annotation ) : continue if data [ ANNOTATIONS ] [ annotation ] == value and source_filter ( graph , u ) and target_filter ( graph , v ) : yield u , v , k , data
498	def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
5160	def render ( self ) : # get jinja2 template template_name = '{0}.jinja2' . format ( self . get_name ( ) ) template = self . template_env . get_template ( template_name ) # render template and cleanup context = getattr ( self . backend , 'intermediate_data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )
7892	def join ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if self . joined : raise RuntimeError ( "Room is already joined" ) p = MucPresence ( to_jid = self . room_jid ) p . make_join_request ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) self . manager . stream . send ( p )
2861	def _transaction_end ( self ) : # Ask to return response bytes immediately. self . _command . append ( '\x87' ) # Send the entire command to the MPSSE. self . _ft232h . _write ( '' . join ( self . _command ) ) # Read response bytes and return them. return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
2540	def set_pkg_summary ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_summary_set : self . package_summary_set = True doc . package . summary = text else : raise CardinalityError ( 'Package::Summary' )
13173	def parents ( self , name = None ) : p = self . parent while p is not None : if name is None or p . tagname == name : yield p p = p . parent
2453	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True match = self . VERIF_CODE_REGEX . match ( code ) if match : doc . package . verif_code = match . group ( self . VERIF_CODE_CODE_GRP ) if match . group ( self . VERIF_CODE_EXC_FILES_GRP ) is not None : doc . package . verif_exc_files = match . group ( self . VERIF_CODE_EXC_FILES_GRP ) . split ( ',' ) return True else : raise SPDXValueError ( 'Package::VerificationCode' ) else : raise CardinalityError ( 'Package::VerificationCode' )
11232	def run_excel_to_html ( ) : # Capture commandline arguments. prog='' argument must # match the command name in setup.py entry_points parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
5995	def plot_mask ( mask , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if mask is not None : plt . gca ( ) edge_pixels = mask . masked_grid_index_to_pixel [ mask . edge_pixels ] + 0.5 if zoom_offset_pixels is not None : edge_pixels -= zoom_offset_pixels edge_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = edge_pixels ) edge_units = convert_grid_units ( array = mask , grid_arcsec = edge_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = edge_units [ : , 0 ] , x = edge_units [ : , 1 ] , s = pointsize , c = 'k' )
13847	def splitext_files_only ( filepath ) : return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
10433	def selectlastrow ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) cell = object_handle . AXRows [ - 1 ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : # Selected pass return 1
1947	def sync_unicorn_to_manticore ( self ) : self . write_backs_disabled = True for reg in self . registers : val = self . _emu . reg_read ( self . _to_unicorn_id ( reg ) ) self . _cpu . write_register ( reg , val ) if len ( self . _mem_delta ) > 0 : logger . debug ( f"Syncing {len(self._mem_delta)} writes back into Manticore" ) for location in self . _mem_delta : value , size = self . _mem_delta [ location ] self . _cpu . write_int ( location , value , size * 8 ) self . write_backs_disabled = False self . _mem_delta = { }
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
13365	def gather ( obj ) : if hasattr ( obj , '__distob_gather__' ) : return obj . __distob_gather__ ( ) elif ( isinstance ( obj , collections . Sequence ) and not isinstance ( obj , string_types ) ) : return [ gather ( subobj ) for subobj in obj ] else : return obj
13902	def ensure_specifier_exists ( db_spec ) : local_match = LOCAL_RE . match ( db_spec ) remote_match = REMOTE_RE . match ( db_spec ) plain_match = PLAIN_RE . match ( db_spec ) if local_match : db_name = local_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True elif remote_match : hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) server = shortcuts . get_server ( server_url = ( 'http://%s:%s' % ( hostname , portnum ) ) ) if database not in server : server . create ( database ) return True elif plain_match : db_name = plain_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True return False
2574	def handle_app_update ( self , task_id , future , memo_cbk = False ) : if not self . tasks [ task_id ] [ 'app_fu' ] . done ( ) : logger . error ( "Internal consistency error: app_fu is not done for task {}" . format ( task_id ) ) if not self . tasks [ task_id ] [ 'app_fu' ] == future : logger . error ( "Internal consistency error: callback future is not the app_fu in task structure, for task {}" . format ( task_id ) ) if not memo_cbk : # Update the memoizer with the new result if this is not a # result from a memo lookup and the task has reached a terminal state. self . memoizer . update_memo ( task_id , self . tasks [ task_id ] , future ) if self . checkpoint_mode == 'task_exit' : self . checkpoint ( tasks = [ task_id ] ) # Submit _*_stage_out tasks for output data futures that correspond with remote files if ( self . tasks [ task_id ] [ 'app_fu' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None and self . tasks [ task_id ] [ 'executor' ] != 'data_manager' and self . tasks [ task_id ] [ 'func_name' ] != '_ftp_stage_in' and self . tasks [ task_id ] [ 'func_name' ] != '_http_stage_in' ) : for dfu in self . tasks [ task_id ] [ 'app_fu' ] . outputs : f = dfu . file_obj if isinstance ( f , File ) and f . is_remote ( ) : self . data_manager . stage_out ( f , self . tasks [ task_id ] [ 'executor' ] ) return
6636	def publish ( self , registry = None ) : if ( registry is None ) or ( registry == registry_access . Registry_Base_URL ) : if 'private' in self . description and self . description [ 'private' ] : return "this %s is private and cannot be published" % ( self . description_filename . split ( '.' ) [ 0 ] ) upload_archive = os . path . join ( self . path , 'upload.tar.gz' ) fsutils . rmF ( upload_archive ) fd = os . open ( upload_archive , os . O_CREAT | os . O_EXCL | os . O_RDWR | getattr ( os , "O_BINARY" , 0 ) ) with os . fdopen ( fd , 'rb+' ) as tar_file : tar_file . truncate ( ) self . generateTarball ( tar_file ) logger . debug ( 'generated tar file of length %s' , tar_file . tell ( ) ) tar_file . seek ( 0 ) # calculate the hash of the file before we upload it: shasum = hashlib . sha256 ( ) while True : chunk = tar_file . read ( 1000 ) if not chunk : break shasum . update ( chunk ) logger . debug ( 'generated tar file has hash %s' , shasum . hexdigest ( ) ) tar_file . seek ( 0 ) with self . findAndOpenReadme ( ) as readme_file_wrapper : if not readme_file_wrapper : logger . warning ( "no readme.md file detected" ) with open ( self . getDescriptionFile ( ) , 'r' ) as description_file : return registry_access . publish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , description_file , tar_file , readme_file_wrapper . file , readme_file_wrapper . extension ( ) . lower ( ) , registry = registry )
1935	def get_source_for ( self , asm_offset , runtime = True ) : srcmap = self . get_srcmap ( runtime ) try : beg , size , _ , _ = srcmap [ asm_offset ] except KeyError : #asm_offset pointing outside the known bytecode return '' output = '' nl = self . source_code [ : beg ] . count ( '\n' ) + 1 snippet = self . source_code [ beg : beg + size ] for l in snippet . split ( '\n' ) : output += ' %s %s\n' % ( nl , l ) nl += 1 return output
1498	def process_incoming_tuples ( self ) : # back-pressure if self . output_helper . is_out_queue_available ( ) : self . _read_tuples_and_execute ( ) self . output_helper . send_out_tuples ( ) else : # update outqueue full count self . bolt_metrics . update_out_queue_full_count ( )
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
5983	def output_subplot_array ( output_path , output_filename , output_format ) : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : raise exc . PlottingException ( 'You cannot output a subplots with format .fits' )
809	def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : # If this is the first sample, then allocate a numpy array # of the appropriate size in which to store all samples. if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] # Add the sample vector and category lable self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] # Add the partition ID if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]
8515	def format_timedelta ( td_object ) : def get_total_seconds ( td ) : # timedelta.total_seconds not in py2.6 return ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 1e6 ) / 1e6 seconds = int ( get_total_seconds ( td_object ) ) periods = [ ( 'year' , 60 * 60 * 24 * 365 ) , ( 'month' , 60 * 60 * 24 * 30 ) , ( 'day' , 60 * 60 * 24 ) , ( 'hour' , 60 * 60 ) , ( 'minute' , 60 ) , ( 'second' , 1 ) ] strings = [ ] for period_name , period_seconds in periods : if seconds > period_seconds : period_value , seconds = divmod ( seconds , period_seconds ) if period_value == 1 : strings . append ( "%s %s" % ( period_value , period_name ) ) else : strings . append ( "%s %ss" % ( period_value , period_name ) ) return ", " . join ( strings )
5214	def earning ( ticker , by = 'Geo' , typ = 'Revenue' , ccy = None , level = None , * * kwargs ) -> pd . DataFrame : ovrd = 'G' if by [ 0 ] . upper ( ) == 'G' else 'P' new_kw = dict ( raw = True , Product_Geo_Override = ovrd ) header = bds ( tickers = ticker , flds = 'PG_Bulk_Header' , * * new_kw , * * kwargs ) if ccy : kwargs [ 'Eqy_Fund_Crncy' ] = ccy if level : kwargs [ 'PG_Hierarchy_Level' ] = level data = bds ( tickers = ticker , flds = f'PG_{typ}' , * * new_kw , * * kwargs ) return assist . format_earning ( data = data , header = header )
6491	def _process_exclude_dictionary ( exclude_dictionary ) : # not_properties will hold the generated term queries. not_properties = [ ] for exclude_property in exclude_dictionary : exclude_values = exclude_dictionary [ exclude_property ] if not isinstance ( exclude_values , list ) : exclude_values = [ exclude_values ] not_properties . extend ( [ { "term" : { exclude_property : exclude_value } } for exclude_value in exclude_values ] ) # Returning a query segment with an empty list freaks out ElasticSearch, # so just return an empty segment. if not not_properties : return { } return { "not" : { "filter" : { "or" : not_properties } } }
7366	def run_multiple_commands_redirect_stdout ( multiple_args_dict , print_commands = True , process_limit = - 1 , polling_freq = 0.5 , * * kwargs ) : assert len ( multiple_args_dict ) > 0 assert all ( len ( args ) > 0 for args in multiple_args_dict . values ( ) ) assert all ( hasattr ( f , 'name' ) for f in multiple_args_dict . keys ( ) ) if process_limit < 0 : logger . debug ( "Using %d processes" % cpu_count ( ) ) process_limit = cpu_count ( ) start_time = time . time ( ) processes = Queue ( maxsize = process_limit ) def add_to_queue ( process ) : process . start ( ) if print_commands : handler = logging . FileHandler ( process . redirect_stdout_file . name ) handler . setLevel ( logging . DEBUG ) logger . addHandler ( handler ) logger . debug ( " " . join ( process . args ) ) logger . removeHandler ( handler ) processes . put ( process ) for f , args in multiple_args_dict . items ( ) : p = AsyncProcess ( args , redirect_stdout_file = f , * * kwargs ) if not processes . full ( ) : add_to_queue ( p ) else : while processes . full ( ) : # Are there any done processes? to_remove = [ ] for possibly_done in processes . queue : if possibly_done . poll ( ) is not None : possibly_done . wait ( ) to_remove . append ( possibly_done ) # Remove them from the queue and stop checking if to_remove : for process_to_remove in to_remove : processes . queue . remove ( process_to_remove ) break # Check again in a second if there weren't time . sleep ( polling_freq ) add_to_queue ( p ) # Wait for all the rest of the processes while not processes . empty ( ) : processes . get ( ) . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "Ran %d commands in %0.4f seconds" , len ( multiple_args_dict ) , elapsed_time )
1934	def get_constructor_arguments ( self ) -> str : item = self . _constructor_abi_item return '()' if item is None else self . tuple_signature_for_components ( item [ 'inputs' ] )
10184	def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
13716	def next_item ( self ) : queue = self . queue try : item = queue . get ( block = True , timeout = 5 ) return item except Exception : return None
7739	def check_bidi ( data ) : has_l = False has_ral = False for char in data : if stringprep . in_table_d1 ( char ) : has_ral = True elif stringprep . in_table_d2 ( char ) : has_l = True if has_l and has_ral : raise StringprepError ( "Both RandALCat and LCat characters present" ) if has_ral and ( not stringprep . in_table_d1 ( data [ 0 ] ) or not stringprep . in_table_d1 ( data [ - 1 ] ) ) : raise StringprepError ( "The first and the last character must" " be RandALCat" ) return data
30	def initialize ( ) : new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) ALREADY_INITIALIZED . update ( new_variables )
4644	def get ( self , key , default = None ) : if key in self : return self . __getitem__ ( key ) else : return default
307	def show_profit_attribution ( round_trips ) : total_pnl = round_trips [ 'pnl' ] . sum ( ) pnl_attribution = round_trips . groupby ( 'symbol' ) [ 'pnl' ] . sum ( ) / total_pnl pnl_attribution . name = '' pnl_attribution . index = pnl_attribution . index . map ( utils . format_asset ) utils . print_table ( pnl_attribution . sort_values ( inplace = False , ascending = False , ) , name = 'Profitability (PnL / PnL total) per name' , float_format = '{:.2%}' . format , )
4474	def __serial_transform ( self , jam , steps ) : # This uses the round-robin itertools recipe if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
12051	def getParent2 ( abfFname , groups ) : if ".abf" in abfFname : abfFname = os . path . basename ( abfFname ) . replace ( ".abf" , "" ) for parentID in groups . keys ( ) : if abfFname in groups [ parentID ] : return parentID return abfFname
7687	def multi_segment ( annotation , sr = 22050 , length = None , * * kwargs ) : # Pentatonic scale, because why not PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
13614	def apply_orientation ( im ) : try : kOrientationEXIFTag = 0x0112 if hasattr ( im , '_getexif' ) : # only present in JPEGs e = im . _getexif ( ) # returns None if no EXIF data if e is not None : #log.info('EXIF data found: %r', e) orientation = e [ kOrientationEXIFTag ] f = orientation_funcs [ orientation ] return f ( im ) except : # We'd be here with an invalid orientation value or some random error? pass # log.exception("Error applying EXIF Orientation tag") return im
12025	def remove ( self , line_data , root_type = None ) : roots = [ ld for ld in self . ancestors ( line_data ) if ( root_type and ld [ 'line_type' ] == root_type ) or ( not root_type and not ld [ 'parents' ] ) ] or [ line_data ] for root in roots : root [ 'line_status' ] = 'removed' root_descendants = self . descendants ( root ) for root_descendant in root_descendants : root_descendant [ 'line_status' ] = 'removed' root_ancestors = self . ancestors ( root ) # BFS, so we will process closer ancestors first for root_ancestor in root_ancestors : if len ( [ ld for ld in root_ancestor [ 'children' ] if ld [ 'line_status' ] != 'removed' ] ) == 0 : # if all children of a root_ancestor is removed # remove this root_ancestor root_ancestor [ 'line_status' ] = 'removed'
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : # Look for closing parenthesis nearby. We need one to confirm where # the declarator ends and where the virt-specifier starts to avoid # false positives. line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return # Check that at most one of "override" or "final" is present, not both if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
1150	def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None - warnings get lost return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass # the file (probably stderr) is invalid - this warning gets lost.
3296	def ref_url_to_path ( self , ref_url ) : return "/" + compat . unquote ( util . lstripstr ( ref_url , self . share_path ) ) . lstrip ( "/" )
3939	def get_chunks ( self , new_data_bytes ) : self . _buf += new_data_bytes while True : buf_decoded = _best_effort_decode ( self . _buf ) buf_utf16 = buf_decoded . encode ( 'utf-16' ) [ 2 : ] length_str_match = LEN_REGEX . match ( buf_decoded ) if length_str_match is None : break else : length_str = length_str_match . group ( 1 ) # Both lengths are in number of bytes in UTF-16 encoding. # The length of the submission: length = int ( length_str ) * 2 # The length of the submission length and newline: length_length = len ( ( length_str + '\n' ) . encode ( 'utf-16' ) [ 2 : ] ) if len ( buf_utf16 ) - length_length < length : break submission = buf_utf16 [ length_length : length_length + length ] yield submission . decode ( 'utf-16' ) # Drop the length and the submission itself from the beginning # of the buffer. drop_length = ( len ( ( length_str + '\n' ) . encode ( ) ) + len ( submission . decode ( 'utf-16' ) . encode ( ) ) ) self . _buf = self . _buf [ drop_length : ]
3655	def stop_image_acquisition ( self ) : if self . is_acquiring_images : # self . _is_acquiring_images = False # if self . thread_image_acquisition . is_running : # TODO self . thread_image_acquisition . stop ( ) with MutexLocker ( self . thread_image_acquisition ) : # self . device . node_map . AcquisitionStop . execute ( ) try : # Unlock TLParamsLocked in order to allow full device # configuration: self . device . node_map . TLParamsLocked . value = 0 except LogicalErrorException : # SFNC < 2.0 pass for data_stream in self . _data_streams : # Stop image acquisition. try : data_stream . stop_acquisition ( ACQ_STOP_FLAGS_LIST . ACQ_STOP_FLAGS_KILL ) except ( ResourceInUseException , TimeoutException ) as e : self . _logger . error ( e , exc_info = True ) # Flash the queue for image acquisition process. data_stream . flush_buffer_queue ( ACQ_QUEUE_TYPE_LIST . ACQ_QUEUE_ALL_DISCARD ) for event_manager in self . _event_new_buffer_managers : event_manager . flush_event_queue ( ) if self . _create_ds_at_connection : self . _release_buffers ( ) else : self . _release_data_streams ( ) # self . _has_acquired_1st_image = False # self . _chunk_adapter . detach_buffer ( ) # self . _logger . info ( '{0} stopped image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
679	def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
6826	def clone ( self , remote_url , path = None , use_sudo = False , user = None ) : cmd = 'git clone --quiet %s' % remote_url if path is not None : cmd = cmd + ' %s' % path if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
11228	def before ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self last = None if inc : for i in gen : if i > dt : break last = i else : for i in gen : if i >= dt : break last = i return last
9918	def validate_key ( self , key ) : try : confirmation = models . EmailConfirmation . objects . select_related ( "email__user" ) . get ( key = key ) except models . EmailConfirmation . DoesNotExist : raise serializers . ValidationError ( _ ( "The provided verification key is invalid." ) ) if confirmation . is_expired : raise serializers . ValidationError ( _ ( "That verification code has expired." ) ) # Cache confirmation instance self . _confirmation = confirmation return key
6656	def calibrateEB ( variances , sigma2 ) : if ( sigma2 <= 0 or min ( variances ) == max ( variances ) ) : return ( np . maximum ( variances , 0 ) ) sigma = np . sqrt ( sigma2 ) eb_prior = gfit ( variances , sigma ) # Set up a partial execution of the function part = functools . partial ( gbayes , g_est = eb_prior , sigma = sigma ) if len ( variances ) >= 200 : # Interpolate to speed up computations: calib_x = np . percentile ( variances , np . arange ( 0 , 102 , 2 ) ) calib_y = list ( map ( part , calib_x ) ) calib_all = np . interp ( variances , calib_x , calib_y ) else : calib_all = list ( map ( part , variances ) ) return np . asarray ( calib_all )
5893	def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url # Response content type needs to be text/html here or else # IE will try to download the file. return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
11571	def set_bit_map ( self , shape , color ) : for row in range ( 0 , 8 ) : data = shape [ row ] # shift data into buffer bit_mask = 0x80 for column in range ( 0 , 8 ) : if data & bit_mask : self . set_pixel ( row , column , color , True ) bit_mask >>= 1 self . output_entire_buffer ( )
570	def _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) : msg = StringIO . StringIO ( ) print >> msg , "Exception occurred while running model %s: %r (%s)" % ( modelID , e , type ( e ) ) traceback . print_exc ( None , msg ) completionReason = jobsDAO . CMPL_REASON_ERROR completionMsg = msg . getvalue ( ) logger . error ( completionMsg ) # Write results to the model database for the error case. Ignore # InvalidConnectionException, as this is usually caused by orphaned models # # TODO: do we really want to set numRecords to 0? Last updated value might # be useful for debugging if type ( e ) is not InvalidConnectionException : jobsDAO . modelUpdateResults ( modelID , results = None , numRecords = 0 ) # TODO: Make sure this wasn't the best model in job. If so, set the best # appropriately # If this was an exception that should mark the job as failed, do that # now. if type ( e ) == JobFailException : workerCmpReason = jobsDAO . jobGetFields ( jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : jobsDAO . jobSetFields ( jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = ": " . join ( str ( i ) for i in e . args ) ) , useConnectionID = False , ignoreUnchanged = True ) return ( completionReason , completionMsg )
4341	def repeat ( self , count = 1 ) : if not isinstance ( count , int ) or count < 1 : raise ValueError ( "count must be a postive integer." ) effect_args = [ 'repeat' , '{}' . format ( count ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'repeat' )
8279	def _append_element ( self , render_func , pe ) : self . _render_funcs . append ( render_func ) self . _elements . append ( pe )
396	def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : if action_list is None : n_action = len ( probs ) action_list = np . arange ( n_action ) else : if len ( action_list ) != len ( probs ) : raise Exception ( "number of actions should equal to number of probabilities." ) return np . random . choice ( action_list , p = probs )
7960	def handle_err ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise PyXMPPIOError ( "Unhandled error on socket" )
8339	def _findAll ( self , name , attrs , text , limit , generator , * * kwargs ) : if isinstance ( name , SoupStrainer ) : strainer = name else : # Build a SoupStrainer strainer = SoupStrainer ( name , attrs , text , * * kwargs ) results = ResultSet ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except StopIteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
13449	def authed_post ( self , url , data , response_code = 200 , follow = False , headers = { } ) : if not self . authed : self . authorize ( ) response = self . client . post ( url , data , follow = follow , * * headers ) self . assertEqual ( response_code , response . status_code ) return response
413	def find_top_model ( self , sess , sort = None , model_name = 'model' , * * kwargs ) : # print(kwargs) # {} kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) s = time . time ( ) d = self . db . Model . find_one ( filter = kwargs , sort = sort ) _temp_file_name = '_find_one_model_ztemp_file' if d is not None : params_id = d [ 'params_id' ] graphs = d [ 'architecture' ] _datetime = d [ 'time' ] exists_or_mkdir ( _temp_file_name , False ) with open ( os . path . join ( _temp_file_name , 'graph.pkl' ) , 'wb' ) as file : pickle . dump ( graphs , file , protocol = pickle . HIGHEST_PROTOCOL ) else : print ( "[Database] FAIL! Cannot find model: {}" . format ( kwargs ) ) return False try : params = self . _deserialization ( self . model_fs . get ( params_id ) . read ( ) ) np . savez ( os . path . join ( _temp_file_name , 'params.npz' ) , params = params ) network = load_graph_and_params ( name = _temp_file_name , sess = sess ) del_folder ( _temp_file_name ) pc = self . db . Model . find ( kwargs ) print ( "[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s" . format ( kwargs , sort , _datetime , round ( time . time ( ) - s , 2 ) ) ) # put all informations of model into the TL layer for key in d : network . __dict__ . update ( { "_%s" % key : d [ key ] } ) # check whether more parameters match the requirement params_id_list = pc . distinct ( 'params_id' ) n_params = len ( params_id_list ) if n_params != 1 : print ( " Note that there are {} models match the kwargs" . format ( n_params ) ) return network except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
10331	def group_nodes_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ BaseEntity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
3953	def get_last_result ( self ) : # Retrieve the conversion register value, convert to a signed int, and # return it. result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
13036	def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) # pass the total entry number for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
10620	def amount ( self ) : return sum ( self . get_compound_amount ( c ) for c in self . material . compounds )
3603	def _build_endpoint_url ( self , url , name = None ) : if not url . endswith ( self . URL_SEPERATOR ) : url = url + self . URL_SEPERATOR if name is None : name = '' return '%s%s%s' % ( urlparse . urljoin ( self . dsn , url ) , name , self . NAME_EXTENSION )
10274	def prune_mechanism_by_data ( graph , key : Optional [ str ] = None ) -> None : remove_unweighted_leaves ( graph , key = key ) remove_unweighted_sources ( graph , key = key )
4136	def _plots_are_current ( src_file , image_file ) : first_image_file = image_file . format ( 1 ) has_image = os . path . exists ( first_image_file ) src_file_changed = check_md5sum_change ( src_file ) return has_image and not src_file_changed
8261	def reverse ( self ) : colors = ColorList . copy ( self ) _list . reverse ( colors ) return colors
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
8383	def update ( self ) : if self . delay > 0 : # It takes a while for the popup to appear. self . delay -= 1 return if self . fi == 0 : # Only one text in queue, displayed infinitely. if len ( self . q ) == 1 : self . fn = float ( "inf" ) # Else, display time depends on text length. else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : # Rotate to the next text in queue. self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
1825	def LEAVE ( cpu ) : cpu . STACK = cpu . FRAME cpu . FRAME = cpu . pop ( cpu . address_bit_size )
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
4242	def _seek_country ( self , ipnum ) : try : offset = 0 seek_depth = 127 if len ( str ( ipnum ) ) > 10 else 31 for depth in range ( seek_depth , - 1 , - 1 ) : if self . _flags & const . MEMORY_CACHE : startIndex = 2 * self . _recordLength * offset endIndex = startIndex + ( 2 * self . _recordLength ) buf = self . _memory [ startIndex : endIndex ] else : startIndex = 2 * self . _recordLength * offset readLength = 2 * self . _recordLength try : self . _lock . acquire ( ) self . _fp . seek ( startIndex , os . SEEK_SET ) buf = self . _fp . read ( readLength ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) x = [ 0 , 0 ] for i in range ( 2 ) : for j in range ( self . _recordLength ) : byte = buf [ self . _recordLength * i + j ] x [ i ] += ord ( byte ) << ( j * 8 ) if ipnum & ( 1 << depth ) : if x [ 1 ] >= self . _databaseSegments : self . _netmask = seek_depth - depth + 1 return x [ 1 ] offset = x [ 1 ] else : if x [ 0 ] >= self . _databaseSegments : self . _netmask = seek_depth - depth + 1 return x [ 0 ] offset = x [ 0 ] except ( IndexError , UnicodeDecodeError ) : pass raise GeoIPError ( 'Corrupt database' )
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
7262	def get_most_recent_images ( self , results , types = [ ] , sensors = [ ] , N = 1 ) : if not len ( results ) : return None # filter on type if types : results = [ r for r in results if r [ 'type' ] in types ] # filter on sensor if sensors : results = [ r for r in results if r [ 'properties' ] . get ( 'sensorPlatformName' ) in sensors ] # sort by date: #sorted(results, key=results.__getitem__('properties').get('timestamp')) newlist = sorted ( results , key = lambda k : k [ 'properties' ] . get ( 'timestamp' ) , reverse = True ) return newlist [ : N ]
2731	def create ( self ) : # URL https://api.digitalocean.com/v2/domains data = { "name" : self . name , "ip_address" : self . ip_address , } domain = self . get_data ( "domains" , type = POST , params = data ) return domain
10421	def count_annotation_values ( graph : BELGraph , annotation : str ) -> Counter : return Counter ( iter_annotation_values ( graph , annotation ) )
1619	def CleanseRawStrings ( raw_lines ) : delimiter = None lines_without_raw_strings = [ ] for line in raw_lines : if delimiter : # Inside a raw string, look for the end end = line . find ( delimiter ) if end >= 0 : # Found the end of the string, match leading space for this # line and resume copying the original lines, and also insert # a "" on the last line. leading_space = Match ( r'^(\s*)\S' , line ) line = leading_space . group ( 1 ) + '""' + line [ end + len ( delimiter ) : ] delimiter = None else : # Haven't found the end yet, append a blank line. line = '""' # Look for beginning of a raw string, and replace them with # empty strings. This is done in a loop to handle multiple raw # strings on the same line. while delimiter is None : # Look for beginning of a raw string. # See 2.14.15 [lex.string] for syntax. # # Once we have matched a raw string, we check the prefix of the # line to make sure that the line is not part of a single line # comment. It's done this way because we remove raw strings # before removing comments as opposed to removing comments # before removing raw strings. This is because there are some # cpplint checks that requires the comments to be preserved, but # we don't want to check comments that are inside raw strings. matched = Match ( r'^(.*?)\b(?:R|u8R|uR|UR|LR)"([^\s\\()]*)\((.*)$' , line ) if ( matched and not Match ( r'^([^\'"]|\'(\\.|[^\'])*\'|"(\\.|[^"])*")*//' , matched . group ( 1 ) ) ) : delimiter = ')' + matched . group ( 2 ) + '"' end = matched . group ( 3 ) . find ( delimiter ) if end >= 0 : # Raw string ended on same line line = ( matched . group ( 1 ) + '""' + matched . group ( 3 ) [ end + len ( delimiter ) : ] ) delimiter = None else : # Start of a multi-line raw string line = matched . group ( 1 ) + '""' else : break lines_without_raw_strings . append ( line ) # TODO(unknown): if delimiter is not None here, we might want to # emit a warning for unterminated string. return lines_without_raw_strings
4550	def fill_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : fill_rect ( setter , x + r , y , w - 2 * r , h , color , aa ) _fill_circle_helper ( setter , x + w - r - 1 , y + r , r , 1 , h - 2 * r - 1 , color , aa ) _fill_circle_helper ( setter , x + r , y + r , r , 2 , h - 2 * r - 1 , color , aa )
4477	def norm_remote_path ( path ) : path = os . path . normpath ( path ) if path . startswith ( os . path . sep ) : return path [ 1 : ] else : return path
2850	def _mpsse_enable ( self ) : # Reset MPSSE by sending mask = 0 and mode = 0 self . _check ( ftdi . set_bitmode , 0 , 0 ) # Enable MPSSE by sending mask = 0 and mode = 2 self . _check ( ftdi . set_bitmode , 0 , 2 )
6325	def encode ( self , text ) : text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) minval = Fraction ( 0 ) maxval = Fraction ( 1 ) for char in text + '\x00' : prob_range = self . _probs [ char ] delta = maxval - minval maxval = minval + prob_range [ 1 ] * delta minval = minval + prob_range [ 0 ] * delta # I tried without the /2 just to check. Doesn't work. # Keep scaling up until the error range is >= 1. That # gives me the minimum number of bits needed to resolve # down to the end-of-data character. delta = ( maxval - minval ) / 2 nbits = long ( 0 ) while delta < 1 : nbits += 1 delta *= 2 # The below condition shouldn't ever be false if nbits == 0 : # pragma: no cover return 0 , 0 # using -1 instead of /2 avg = ( maxval + minval ) * 2 ** ( nbits - 1 ) # Could return a rational instead ... # the division truncation is deliberate return avg . numerator // avg . denominator , nbits
3732	def mixture_from_any ( ID ) : if type ( ID ) == list : if len ( ID ) == 1 : ID = ID [ 0 ] else : raise Exception ( 'If the input is a list, the list must contain only one item.' ) ID = ID . lower ( ) . strip ( ) ID2 = ID . replace ( ' ' , '' ) ID3 = ID . replace ( '-' , '' ) for i in [ ID , ID2 , ID3 ] : if i in _MixtureDictLookup : return _MixtureDictLookup [ i ] raise Exception ( 'Mixture name not recognized' )
8278	def objs ( self ) : for obj in self . objects . itervalues ( ) : if obj . sessionid in self . sessions : yield obj
10902	def lbl ( axis , label , size = 22 ) : at = AnchoredText ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set_boxstyle ( "round,pad=0.,rounding_size=0.0" ) #bb = axis.get_yaxis_transform() #at = AnchoredText(label, # loc=3, prop=dict(size=18), frameon=True, # bbox_to_anchor=(-0.5,1),#(-.255, 0.90), # bbox_transform=bb,#axis.transAxes # ) axis . add_artist ( at )
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
3636	def club ( self , sort = 'desc' , ctype = 'player' , defId = '' , start = 0 , count = None , page_size = itemsPerPage [ 'club' ] , level = None , category = None , assetId = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None ) : method = 'GET' url = 'club' if count : # backward compatibility, will be removed in future page_size = count params = { 'sort' : sort , 'type' : ctype , 'defId' : defId , 'start' : start , 'count' : page_size } if level : params [ 'level' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params ) # pinEvent if start == 0 : if ctype == 'player' : pgid = 'Club - Players - List View' elif ctype == 'staff' : pgid = 'Club - Staff - List View' elif ctype in ( 'item' , 'kit' , 'ball' , 'badge' , 'stadium' ) : pgid = 'Club - Club Items - List View' # else: # TODO: THIS IS probably WRONG, detect all ctypes # pgid = 'Club - Club Items - List View' events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) , self . pin . event ( 'page_view' , pgid ) ] if rc [ 'itemData' ] : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( { 'itemData' : i } ) for i in rc [ 'itemData' ] ]
7215	def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ]
7607	def search_tournaments ( self , name : str , * * params : keys ) : url = self . api . TOURNAMENT params [ 'name' ] = name return self . _get_model ( url , PartialTournament , * * params )
4634	def derive_private_key ( self , sequence ) : encoded = "%s %d" % ( str ( self ) , sequence ) a = bytes ( encoded , "ascii" ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . pubkey . prefix )
9776	def outputs ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : PolyaxonClient ( ) . job . download_outputs ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download outputs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
8353	def parse_declaration ( self , i ) : j = None if self . rawdata [ i : i + 9 ] == '<![CDATA[' : k = self . rawdata . find ( ']]>' , i ) if k == - 1 : k = len ( self . rawdata ) data = self . rawdata [ i + 9 : k ] j = k + 3 self . _toStringSubclass ( data , CData ) else : try : j = SGMLParser . parse_declaration ( self , i ) except SGMLParseError : toHandle = self . rawdata [ i : ] self . handle_data ( toHandle ) j = i + len ( toHandle ) return j
3361	def shadow_price ( self ) : try : check_solver_status ( self . _model . solver . status ) return self . _model . constraints [ self . id ] . dual except AttributeError : raise RuntimeError ( "metabolite '{}' is not part of a model" . format ( self . id ) ) # Due to below all-catch, which sucks, need to reraise these. except ( RuntimeError , OptimizationError ) as err : raise_with_traceback ( err ) # Would love to catch CplexSolverError and GurobiError here. except Exception as err : raise_from ( OptimizationError ( "Likely no solution exists. Original solver message: {}." "" . format ( str ( err ) ) ) , err )
261	def show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : risk_exposures , perf_attrib_data = perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , ) perf_attrib_stats , risk_exposure_stats = create_perf_attrib_stats ( perf_attrib_data , risk_exposures ) percentage_formatter = '{:.2%}' . format float_formatter = '{:.2f}' . format summary_stats = perf_attrib_stats . loc [ [ 'Annualized Specific Return' , 'Annualized Common Return' , 'Annualized Total Return' , 'Specific Sharpe Ratio' ] ] # Format return rows in summary stats table as percentages. for col_name in ( 'Annualized Specific Return' , 'Annualized Common Return' , 'Annualized Total Return' , ) : summary_stats [ col_name ] = percentage_formatter ( summary_stats [ col_name ] ) # Display sharpe to two decimal places. summary_stats [ 'Specific Sharpe Ratio' ] = float_formatter ( summary_stats [ 'Specific Sharpe Ratio' ] ) print_table ( summary_stats , name = 'Summary Statistics' ) print_table ( risk_exposure_stats , name = 'Exposures Summary' , # In exposures table, format exposure column to 2 decimal places, and # return columns as percentages. formatters = { 'Average Risk Factor Exposure' : float_formatter , 'Annualized Return' : percentage_formatter , 'Cumulative Return' : percentage_formatter , } , )
11659	def inverse_transform ( self , X ) : X = check_array ( X , copy = self . copy ) X -= self . min_ X /= self . scale_ return X
937	def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( "(%s) Creating local checkpoint in %r..." , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) # Clean up old saved state, if any if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) # Create a new directory for saving state self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( "(%s) Pickling Model instance..." , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( "(%s) Finished pickling Model instance" , self ) # Tell the model to save extra data, if any, that's too big for pickling self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( "(%s) Finished creating local checkpoint" , self ) return
11288	def get_request_subfields ( root ) : request = root . find ( 'request' ) responsedate = root . find ( 'responseDate' ) subs = [ ( "9" , request . text ) , ( "h" , responsedate . text ) , ( "m" , request . attrib [ "metadataPrefix" ] ) ] return subs
4766	def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
7548	def cluster_info ( ipyclient , spacer = "" ) : ## get engine data, skips busy engines. hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) ## report it hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
1513	def wait_for_master_to_start ( single_master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) if r . status_code == 200 : break except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( * * jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
3761	def draw_2d ( self , Hs = False ) : # pragma: no cover try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
8528	def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
13390	def format_pathname ( pathname , max_length ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( pathname ) > max_length : pathname = "...{}" . format ( pathname [ - ( max_length - 3 ) : ] ) return pathname
6532	def get_local_config ( project_path , use_cache = True ) : pyproject_path = os . path . join ( project_path , 'pyproject.toml' ) if os . path . exists ( pyproject_path ) : with open ( pyproject_path , 'r' ) as config_file : config = pytoml . load ( config_file ) config = config . get ( 'tool' , { } ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
10779	def _remove_closest_particle ( self , p ) : #1. find closest pos: dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) #2. delete self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
11932	def find_block ( context , * names ) : block_set = context . render_context [ BLOCK_CONTEXT_KEY ] for name in names : block = block_set . get_block ( name ) if block is not None : return block raise template . TemplateSyntaxError ( 'No widget found for: %r' % ( names , ) )
4401	def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . ElementTree ( file = xml ) records = self . _parse_deputies ( tree . getroot ( ) ) df = pd . DataFrame ( records , columns = ( 'congressperson_id' , 'budget_id' , 'condition' , 'congressperson_document' , 'civil_name' , 'congressperson_name' , 'picture_url' , 'gender' , 'state' , 'party' , 'phone_number' , 'email' ) ) return self . _translate ( df )
1155	def pop ( self ) : it = iter ( self ) try : value = next ( it ) except StopIteration : raise KeyError self . discard ( value ) return value
1216	def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
44	def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
6219	def interleaves ( self , info ) : return info . byte_offset == self . component_type . size * self . components
12730	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis1 ( ) ) , np . array ( self . ode_obj . getAxis2 ( ) ) ]
13465	def __register_library ( self , module_name : str , attr : str , fallback : str = None ) : # Import the module Named in the string try : module = importlib . import_module ( module_name ) # If module is not found it checks if an alternative is is listed # If it is then it substitutes it, just so that the code can run except ImportError : if fallback is not None : module = importlib . import_module ( fallback ) self . __logger . warn ( module_name + " not available: Replaced with " + fallback ) else : self . __logger . warn ( module_name + " not available: No Replacement Specified" ) # Cram the module into the __sketch in the form of module -> "attr" # AKA the same as `import module as attr` if not attr in dir ( self . __sketch ) : setattr ( self . __sketch , attr , module ) else : self . __logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
13185	def get_default_tag ( app ) : view_func = get_view_function ( app , request . path , request . method ) if view_func : return view_func . __name__
8143	def rotate ( self , angle ) : #When a layer rotates, its corners will fall outside #of its defined width and height. #Thus, its bounding box needs to be expanded. #Calculate the diagonal width, and angle from the layer center. #This way we can use the layers's corners #to calculate the bounding box. from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) #The rotation box's background color #is the mean pixel value of the rotating image. #This is the best option to avoid borders around #the rotated image. bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box #Since rotate changes the bounding box size, #update the layers' width, height, and position, #so it rotates from the center. self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : # cleanup for item in batch : self . queue . task_done ( ) return success
607	def findRequirements ( ) : requirementsPath = os . path . join ( REPO_DIR , "requirements.txt" ) requirements = parse_file ( requirementsPath ) if nupicBindingsPrereleaseInstalled ( ) : # User has a pre-release version of nupic.bindings installed, which is only # possible if the user installed and built nupic.bindings from source and # it is up to the user to decide when to update nupic.bindings. We'll # quietly remove the entry in requirements.txt so as to not conflate the # two. requirements = [ req for req in requirements if "nupic.bindings" not in req ] return requirements
9263	def get_filtered_pull_requests ( self , pull_requests ) : pull_requests = self . filter_by_labels ( pull_requests , "pull requests" ) pull_requests = self . filter_merged_pull_requests ( pull_requests ) if self . options . verbose > 1 : print ( "\tremaining pull requests: {}" . format ( len ( pull_requests ) ) ) return pull_requests
11602	def save_formset ( self , request , form , formset , change ) : instances = formset . save ( commit = False ) for instance in instances : if isinstance ( instance , Photo ) : instance . author = request . user instance . save ( )
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem # Add indicator variables and new constraints to_add = [ ] for i in internal : rxn = model . reactions [ i ] # indicator variable a_i indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) # -M*(1 - a_i) <= v_i <= M*a_i on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) # -(max_bound + 1) * a_i + 1 <= G_i <= -(max_bound + 1) * a_i + 1000 delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) # Add nullspace constraints for G_i for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
4148	def centerdc_gen ( self ) : for a in range ( 0 , self . N ) : yield ( a - self . N / 2 ) * self . df
7561	def get_sampled ( data , totn , node ) : ## convert tip names to ints names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } ## skip some nodes if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## get counts on down edges if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = set ( cdict [ i ] for i in down_r . get_leaf_names ( ) ) lendl = set ( cdict [ i ] for i in down_l . get_leaf_names ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up_r . get_leaf_names ( ) ) ## everyone else lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ "quartets" ] . shape [ 0 ] while 1 : ## break condition if idx >= end : break ## counts matches qrts = io5 [ "quartets" ] [ idx : idx + data . _chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 ## increase span idx += data . _chunksize return sampled
11256	def flatten ( prev , depth = sys . maxsize ) : def inner_flatten ( iterable , curr_level , max_levels ) : for i in iterable : if hasattr ( i , '__iter__' ) and curr_level < max_levels : for j in inner_flatten ( i , curr_level + 1 , max_levels ) : yield j else : yield i for d in prev : if hasattr ( d , '__iter__' ) and depth > 0 : for inner_d in inner_flatten ( d , 1 , depth ) : yield inner_d else : yield d
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : # raises exception if not an int self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
4035	def cleanwrap ( func ) : def enc ( self , * args , * * kwargs ) : """ Send each item to _cleanup() """ return ( func ( self , item , * * kwargs ) for item in args ) return enc
3831	async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
5044	def notify_program_learners ( cls , enterprise_customer , program_details , users ) : program_name = program_details . get ( 'title' ) program_branding = program_details . get ( 'type' ) program_uuid = program_details . get ( 'uuid' ) lms_root_url = get_configuration_value_for_site ( enterprise_customer . site , 'LMS_ROOT_URL' , settings . LMS_ROOT_URL ) program_path = urlquote ( '/dashboard/programs/{program_uuid}/?tpa_hint={tpa_hint}' . format ( program_uuid = program_uuid , tpa_hint = enterprise_customer . identity_provider , ) ) destination_url = '{site}/{login_or_register}?next={program_path}' . format ( site = lms_root_url , login_or_register = '{login_or_register}' , program_path = program_path ) program_type = 'program' program_start = get_earliest_start_date_from_program ( program_details ) with mail . get_connection ( ) as email_conn : for user in users : login_or_register = 'register' if isinstance ( user , PendingEnterpriseCustomerUser ) else 'login' destination_url = destination_url . format ( login_or_register = login_or_register ) send_email_notification_message ( user = user , enrolled_in = { 'name' : program_name , 'url' : destination_url , 'type' : program_type , 'start' : program_start , 'branding' : program_branding , } , enterprise_customer = enterprise_customer , email_connection = email_conn )
11679	def connect ( self ) : try : logger . info ( u'Connecting %s:%d' % ( self . host , self . port ) ) self . sock . connect ( ( self . host , self . port ) ) except socket . error : raise ConnectionError ( ) self . state = CONNECTED
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
7693	def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "authzid" ) if not authzid : return True try : jid = JID ( authzid ) except ValueError : return False if "username" not in properties : result = False elif jid . local != properties [ "username" ] : result = False elif jid . domain != stream . me . domain : result = False elif jid . resource : result = False else : result = True return result
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
8396	def trans_new ( name , transform , inverse , breaks = None , minor_breaks = None , _format = None , domain = ( - np . inf , np . inf ) , doc = '' , * * kwargs ) : def _get ( func ) : if isinstance ( func , ( classmethod , staticmethod , MethodType ) ) : return func else : return staticmethod ( func ) klass_name = '{}_trans' . format ( name ) d = { 'transform' : _get ( transform ) , 'inverse' : _get ( inverse ) , 'domain' : domain , '__doc__' : doc , * * kwargs } if breaks : d [ 'breaks_' ] = _get ( breaks ) if minor_breaks : d [ 'minor_breaks' ] = _get ( minor_breaks ) if _format : d [ 'format' ] = _get ( _format ) return type ( klass_name , ( trans , ) , d )
8434	def apply ( cls , x , palette , na_value = None , trans = None ) : if trans is not None : x = trans . transform ( x ) limits = cls . train ( x ) return cls . map ( x , palette , limits , na_value )
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
12398	def gen_methods ( self , * args , * * kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , * * kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method # Fall back to built-in types, then types, then collections. typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) # Try the generic handler. yield from self . gen_generic ( )
563	def invariant ( self ) : # Verify the description and singleNodeOnly attributes assert isinstance ( self . description , str ) assert isinstance ( self . singleNodeOnly , bool ) # Make sure that all items dicts are really dicts assert isinstance ( self . inputs , dict ) assert isinstance ( self . outputs , dict ) assert isinstance ( self . parameters , dict ) assert isinstance ( self . commands , dict ) # Verify all item dicts hasDefaultInput = False for k , v in self . inputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , InputSpec ) v . invariant ( ) if v . isDefaultInput : assert not hasDefaultInput hasDefaultInput = True hasDefaultOutput = False for k , v in self . outputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , OutputSpec ) v . invariant ( ) if v . isDefaultOutput : assert not hasDefaultOutput hasDefaultOutput = True for k , v in self . parameters . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , ParameterSpec ) v . invariant ( ) for k , v in self . commands . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , CommandSpec ) v . invariant ( )
5629	def _get_digest ( self ) : return hmac . new ( self . _secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . _secret else None
7863	def is_certificate_valid ( stream , cert ) : try : logger . debug ( "tls_is_certificate_valid(cert = {0!r})" . format ( cert ) ) if not cert : logger . warning ( "No TLS certificate information received." ) return False if not cert . validated : logger . warning ( "TLS certificate not validated." ) return False srv_type = stream . transport . _dst_service # pylint: disable=W0212 if cert . verify_server ( stream . peer , srv_type ) : logger . debug ( " tls: certificate valid for {0!r}" . format ( stream . peer ) ) return True else : logger . debug ( " tls: certificate not valid for {0!r}" . format ( stream . peer ) ) return False except : logger . exception ( "Exception caught while checking a certificate" ) raise
8069	def randomChildElement ( self , node ) : choices = [ e for e in node . childNodes if e . nodeType == e . ELEMENT_NODE ] chosen = random . choice ( choices ) if _debug : sys . stderr . write ( '%s available choices: %s\n' % ( len ( choices ) , [ e . toxml ( ) for e in choices ] ) ) sys . stderr . write ( 'Chosen: %s\n' % chosen . toxml ( ) ) return chosen
9405	def _get_user_class ( self , name ) : self . _user_classes . setdefault ( name , _make_user_class ( self , name ) ) return self . _user_classes [ name ]
13020	def _execute ( self , query , commit = False , working_columns = None ) : log . debug ( "RawlBase._execute()" ) result = [ ] if working_columns is None : working_columns = self . columns with RawlConnection ( self . dsn ) as conn : query_id = random . randrange ( 9999 ) curs = conn . cursor ( ) try : log . debug ( "Executing(%s): %s" % ( query_id , query . as_string ( curs ) ) ) except : log . exception ( "LOGGING EXCEPTION LOL" ) curs . execute ( query ) log . debug ( "Executed" ) if commit == True : log . debug ( "COMMIT(%s)" % query_id ) conn . commit ( ) log . debug ( "curs.rowcount: %s" % curs . rowcount ) if curs . rowcount > 0 : #result = curs.fetchall() # Process the results into a dict and stuff it in a RawlResult # object. Then append that object to result result_rows = curs . fetchall ( ) for row in result_rows : i = 0 row_dict = { } for col in working_columns : try : #log.debug("row_dict[%s] = row[%s] which is %s" % (col, i, row[i])) # For aliased columns, we need to get rid of the dot col = col . replace ( '.' , '_' ) row_dict [ col ] = row [ i ] except IndexError : pass i += 1 log . debug ( "Appending dict to result: %s" % row_dict ) rr = RawlResult ( working_columns , row_dict ) result . append ( rr ) curs . close ( ) return result
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
4973	def clean_channel_worker_username ( self ) : channel_worker_username = self . cleaned_data [ 'channel_worker_username' ] . strip ( ) try : User . objects . get ( username = channel_worker_username ) except User . DoesNotExist : raise ValidationError ( ValidationMessages . INVALID_CHANNEL_WORKER . format ( channel_worker_username = channel_worker_username ) ) return channel_worker_username
13820	def _ConvertListValueMessage ( value , message ) : if not isinstance ( value , list ) : raise ParseError ( 'ListValue must be in [] which is {0}.' . format ( value ) ) message . ClearField ( 'values' ) for item in value : _ConvertValueMessage ( item , message . values . add ( ) )
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
11332	def err ( format_msg , * args , * * kwargs ) : exc_info = kwargs . pop ( "exc_info" , False ) stderr . warning ( str ( format_msg ) . format ( * args , * * kwargs ) , exc_info = exc_info )
7365	def run_command ( args , * * kwargs ) : assert len ( args ) > 0 start_time = time . time ( ) process = AsyncProcess ( args , * * kwargs ) process . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "%s took %0.4f seconds" , args [ 0 ] , elapsed_time )
2549	def include ( f ) : fl = open ( f , 'r' ) data = fl . read ( ) fl . close ( ) return raw ( data )
3936	def set ( self , refresh_token ) : logger . info ( 'Saving refresh_token to %s' , repr ( self . _filename ) ) try : with open ( self . _filename , 'w' ) as f : f . write ( refresh_token ) except IOError as e : logger . warning ( 'Failed to save refresh_token: %s' , e )
3156	def delete ( self , list_id , segment_id ) : return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
5883	def post_cleanup ( self ) : parse_tags = [ 'p' ] if self . config . parse_lists : parse_tags . extend ( [ 'ul' , 'ol' ] ) if self . config . parse_headers : parse_tags . extend ( [ 'h1' , 'h2' , 'h3' , 'h4' , 'h5' , 'h6' ] ) target_node = self . article . top_node node = self . add_siblings ( target_node ) for elm in self . parser . getChildren ( node ) : e_tag = self . parser . getTag ( elm ) if e_tag not in parse_tags : if ( self . is_highlink_density ( elm ) or self . is_table_and_no_para_exist ( elm ) or not self . is_nodescore_threshold_met ( node , elm ) ) : self . parser . remove ( elm ) return node
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( * * filters ) . delete ( )
2779	def get_data ( self , url , headers = dict ( ) , params = dict ( ) , render_json = True ) : url = urljoin ( self . end_point , url ) response = requests . get ( url , headers = headers , params = params , timeout = self . get_timeout ( ) ) if render_json : return response . json ( ) return response . content
6780	def manifest_filename ( self ) : r = self . local_renderer tp_fn = r . format ( r . env . data_dir + '/manifest.yaml' ) return tp_fn
9344	def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
12910	def intersection ( self , other , recursive = True ) : if not isinstance ( other , composite ) : raise AssertionError ( 'Cannot intersect composite and {} types' . format ( type ( other ) ) ) if self . meta_type != other . meta_type : return composite ( { } ) if self . meta_type == 'list' : keep = [ ] for item in self . _list : if item in other . _list : if recursive and isinstance ( item , composite ) : keep . extend ( item . intersection ( other . index ( item ) , recursive = True ) ) else : keep . append ( item ) return composite ( keep ) elif self . meta_type == 'dict' : keep = { } for key in self . _dict : item = self . _dict [ key ] if key in other . _dict : if recursive and isinstance ( item , composite ) and isinstance ( other . get ( key ) , composite ) : keep [ key ] = item . intersection ( other . get ( key ) , recursive = True ) elif item == other [ key ] : keep [ key ] = item return composite ( keep ) return
7674	def slice ( self , start_time , end_time , strict = False ) : # Make sure duration is set in file metadata if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) # Make sure start and end times are within the file start/end times if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) # Create a new jams jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) # trim annotations jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) # adjust dutation jam_sliced . file_metadata . duration = end_time - start_time # Document jam-level trim in top level sandbox if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
6045	def padded_blurred_image_2d_from_padded_image_1d_and_psf ( self , padded_image_1d , psf ) : padded_model_image_1d = self . convolve_array_1d_with_psf ( padded_array_1d = padded_image_1d , psf = psf ) return self . scaled_array_2d_from_array_1d ( array_1d = padded_model_image_1d )
10151	def _build_paths ( self ) : paths = { } tags = [ ] for service in self . services : path , path_obj = self . _extract_path_from_service ( service ) service_tags = getattr ( service , 'tags' , [ ] ) self . _check_tags ( service_tags ) tags = self . _get_tags ( tags , service_tags ) for method , view , args in service . definitions : if method . lower ( ) in map ( str . lower , self . ignore_methods ) : continue op = self . _extract_operation_from_view ( view , args ) if any ( ctype in op . get ( 'consumes' , [ ] ) for ctype in self . ignore_ctypes ) : continue # XXX: Swagger doesn't support different schemas for for a same method # with different ctypes as cornice. If this happens, you may ignore one # content-type from the documentation otherwise we raise an Exception # Related to https://github.com/OAI/OpenAPI-Specification/issues/146 previous_definition = path_obj . get ( method . lower ( ) ) if previous_definition : raise CorniceSwaggerException ( ( "Swagger doesn't support multiple " "views for a same method. You may " "ignore one." ) ) # If tag not defined and a default tag is provided if 'tags' not in op and self . default_tags : if callable ( self . default_tags ) : op [ 'tags' ] = self . default_tags ( service , method ) else : op [ 'tags' ] = self . default_tags op_tags = op . get ( 'tags' , [ ] ) self . _check_tags ( op_tags ) # Add service tags if service_tags : new_tags = service_tags + op_tags op [ 'tags' ] = list ( OrderedDict . fromkeys ( new_tags ) ) # Add method tags to root tags tags = self . _get_tags ( tags , op_tags ) # If operation id is not defined and a default generator is provided if 'operationId' not in op and self . default_op_ids : if not callable ( self . default_op_ids ) : raise CorniceSwaggerException ( 'default_op_id should be a callable.' ) op [ 'operationId' ] = self . default_op_ids ( service , method ) # If security options not defined and default is provided if 'security' not in op and self . default_security : if callable ( self . default_security ) : op [ 'security' ] = self . default_security ( service , method ) else : op [ 'security' ] = self . default_security if not isinstance ( op . get ( 'security' , [ ] ) , list ) : raise CorniceSwaggerException ( 'security should be a list or callable' ) path_obj [ method . lower ( ) ] = op paths [ path ] = path_obj return paths , tags
75	def DirectedEdgeDetect ( alpha = 0 , direction = ( 0.0 , 1.0 ) , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) deg = int ( direction_sample * 360 ) % 360 rad = np . deg2rad ( deg ) x = np . cos ( rad - 0.5 * np . pi ) y = np . sin ( rad - 0.5 * np . pi ) direction_vector = np . array ( [ x , y ] ) matrix_effect = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) for x in [ - 1 , 0 , 1 ] : for y in [ - 1 , 0 , 1 ] : if ( x , y ) != ( 0 , 0 ) : cell_vector = np . array ( [ x , y ] ) distance_deg = np . rad2deg ( ia . angle_between_vectors ( cell_vector , direction_vector ) ) distance = distance_deg / 180 similarity = ( 1 - distance ) ** 4 matrix_effect [ y + 1 , x + 1 ] = similarity matrix_effect = matrix_effect / np . sum ( matrix_effect ) matrix_effect = matrix_effect * ( - 1 ) matrix_effect [ 1 , 1 ] = 1 matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
11960	def is_bits_nm ( nm ) : try : bits = int ( str ( nm ) ) except ValueError : return False if bits > 32 or bits < 0 : return False return True
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
5311	def translate_colorname_to_ansi_code ( colorname , offset , colormode , colorpalette ) : try : red , green , blue = colorpalette [ colorname ] except KeyError : raise ColorfulError ( 'the color "{0}" is unknown. Use a color in your color palette (by default: X11 rgb.txt)' . format ( # noqa colorname ) ) else : return translate_rgb_to_ansi_code ( red , green , blue , offset , colormode )
1609	def make_tick_tuple ( ) : return HeronTuple ( id = TupleHelper . TICK_TUPLE_ID , component = TupleHelper . TICK_SOURCE_COMPONENT , stream = TupleHelper . TICK_TUPLE_ID , task = None , values = None , creation_time = time . time ( ) , roots = None )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] # a list of dictionaries for all the urls we can match endpoint = reverse ( 'oembed_json' ) # the public endpoint for our oembeds providers = oembed . site . get_providers ( ) for provider in providers : # first make sure this provider class is exposed at the public endpoint if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : # django providers define their regex_list by using urlreversing url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) # this regex replacement is set to be non-greedy, which results # in things like /news/*/*/*/*/ -- this is more explicit if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
1218	def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before save" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , # Do we need this? )
9377	def calculate_stats ( data_list , stats_to_calculate = [ 'mean' , 'std' ] , percentiles_to_calculate = [ ] ) : stats_to_numpy_method_map = { 'mean' : numpy . mean , 'avg' : numpy . mean , 'std' : numpy . std , 'standard_deviation' : numpy . std , 'median' : numpy . median , 'min' : numpy . amin , 'max' : numpy . amax } calculated_stats = { } calculated_percentiles = { } if len ( data_list ) == 0 : return calculated_stats , calculated_percentiles for stat in stats_to_calculate : if stat in stats_to_numpy_method_map . keys ( ) : calculated_stats [ stat ] = stats_to_numpy_method_map [ stat ] ( data_list ) else : logger . error ( "Unsupported stat : " + str ( stat ) ) for percentile in percentiles_to_calculate : if isinstance ( percentile , float ) or isinstance ( percentile , int ) : calculated_percentiles [ percentile ] = numpy . percentile ( data_list , percentile ) else : logger . error ( "Unsupported percentile requested (should be int or float): " + str ( percentile ) ) return calculated_stats , calculated_percentiles
3737	def molecular_diameter ( Tc = None , Pc = None , Vc = None , Zc = None , omega = None , Vm = None , Vb = None , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and Pc and omega : methods . append ( TEEGOTOSTEWARD4 ) if Tc and Pc : methods . append ( SILVALIUMACEDO ) methods . append ( BSLC2 ) methods . append ( TEEGOTOSTEWARD3 ) if Vc and Zc : methods . append ( STIELTHODOSMD ) if Vc : methods . append ( FLYNN ) methods . append ( BSLC1 ) if Vb : methods . append ( BSLB ) if Vm : methods . append ( BSLM ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : sigma = sigma_Flynn ( Vc ) elif Method == BSLC1 : sigma = sigma_Bird_Stewart_Lightfoot_critical_1 ( Vc ) elif Method == BSLC2 : sigma = sigma_Bird_Stewart_Lightfoot_critical_2 ( Tc , Pc ) elif Method == TEEGOTOSTEWARD3 : sigma = sigma_Tee_Gotoh_Steward_1 ( Tc , Pc ) elif Method == SILVALIUMACEDO : sigma = sigma_Silva_Liu_Macedo ( Tc , Pc ) elif Method == BSLB : sigma = sigma_Bird_Stewart_Lightfoot_boiling ( Vb ) elif Method == BSLM : sigma = sigma_Bird_Stewart_Lightfoot_melting ( Vm ) elif Method == STIELTHODOSMD : sigma = sigma_Stiel_Thodos ( Vc , Zc ) elif Method == TEEGOTOSTEWARD4 : sigma = sigma_Tee_Gotoh_Steward_2 ( Tc , Pc , omega ) elif Method == MAGALHAES : sigma = float ( MagalhaesLJ_data . at [ CASRN , "sigma" ] ) elif Method == NONE : sigma = None else : raise Exception ( 'Failure in in function' ) return sigma
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
13217	def connection_dsn ( self , name = None ) : return ' ' . join ( "%s=%s" % ( param , value ) for param , value in self . _connect_options ( name ) )
945	def _checkpointLabelFromCheckpointDir ( checkpointDir ) : assert checkpointDir . endswith ( g_defaultCheckpointExtension ) lastSegment = os . path . split ( checkpointDir ) [ 1 ] checkpointLabel = lastSegment [ 0 : - len ( g_defaultCheckpointExtension ) ] return checkpointLabel
1875	def PSUBB ( cpu , dest , src ) : result = [ ] value_a = dest . read ( ) value_b = src . read ( ) for i in reversed ( range ( 0 , dest . size , 8 ) ) : a = Operators . EXTRACT ( value_a , i , 8 ) b = Operators . EXTRACT ( value_b , i , 8 ) result . append ( ( a - b ) & 0xff ) dest . write ( Operators . CONCAT ( 8 * len ( result ) , * result ) )
5306	def rgb_to_ansi256 ( r , g , b ) : if r == g and g == b : if r < 8 : return 16 if r > 248 : return 231 return round ( ( ( r - 8 ) / 247.0 ) * 24 ) + 232 ansi_r = 36 * round ( r / 255.0 * 5.0 ) ansi_g = 6 * round ( g / 255.0 * 5.0 ) ansi_b = round ( b / 255.0 * 5.0 ) ansi = 16 + ansi_r + ansi_g + ansi_b return ansi
2328	def orient_directed_graph ( self , data , graph ) : warnings . warn ( "The algorithm is ran on the skeleton of the given graph." ) return self . orient_undirected_graph ( data , nx . Graph ( graph ) )
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
7115	def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False , build = False ) : sources = [ # Machine-specific ( configs_dirs , 'hostname' ) , ( configs_dirs , 'hostname-local' ) , ( configs_dirs , 'hostname-build' ) , # Global ( configs_dirs , 'common' ) , # Environment + Cluster ( configs_dirs , 'common-%s' % environment ) , ( configs_dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs_dirs , 'common-local' ) , ( configs_dirs , 'common-build' ) , # Machine-specific overrides ( configs_dirs , 'common-overrides' ) , # Application-specific ( [ app_dir ] , '%s-default' % app ) , ( [ app_dir ] , '%s-%s' % ( app , environment ) ) , ( [ app_dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs_dirs , app ) , ( configs_dirs , '%s-%s' % ( app , environment ) ) , ( configs_dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app_dir ] , '%s-local' % app ) , ( [ app_dir ] , '%s-build' % app ) , ( configs_dirs , '%s-local' % app ) , ( configs_dirs , '%s-build' % app ) , # Machine-specific application override ( configs_dirs , '%s-overrides' % app ) , ] # Filter out build sources if not requested if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] # Filter out local sources if not build and not local if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available_sources ( sources )
2810	def convert_transpose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transpose ...' ) if params [ 'perm' ] [ 0 ] != 0 : if inputs [ 0 ] in layers : print ( '!!! Cannot permute batch dimension. Result may be wrong !!!' ) layers [ scope_name ] = layers [ inputs [ 0 ] ] else : print ( 'Skip weight matrix transpose, result may be wrong.' ) else : if names : tf_name = 'PERM' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) permute = keras . layers . Permute ( params [ 'perm' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = permute ( layers [ inputs [ 0 ] ] )
96	def quokka_keypoints ( size = None , extract = None ) : # TODO get rid of this deferred import from imgaug . augmentables . kps import Keypoint , KeypointsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) keypoints = [ ] for kp_dict in json_dict [ "keypoints" ] : keypoints . append ( Keypoint ( x = kp_dict [ "x" ] - left , y = kp_dict [ "y" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) kpsoi = KeypointsOnImage ( keypoints , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) kpsoi = kpsoi . on ( shape_resized ) return kpsoi
13236	def to_timezone ( self , dt ) : if timezone . is_aware ( dt ) : return dt . astimezone ( self . timezone ) else : return timezone . make_aware ( dt , self . timezone )
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) # check the number of lines in the input xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None # turn the input into a param dict params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done # we won't wait for the LC ZIP to complete if email_when_done = True if email_when_done : download_data = False # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # hit the server api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) # check the status of the search status = searchresult [ 0 ] # now we'll check if we want to download the data if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
1547	def configure ( level = logging . INFO , logfile = None ) : # Remove all the existing StreamHandlers to avoid duplicate for handler in Log . handlers : if isinstance ( handler , logging . StreamHandler ) : Log . handlers . remove ( handler ) Log . setLevel ( level ) # if logfile is specified, FileHandler is used if logfile is not None : log_format = "[%(asctime)s] [%(levelname)s]: %(message)s" formatter = logging . Formatter ( fmt = log_format , datefmt = date_format ) file_handler = logging . FileHandler ( logfile ) file_handler . setFormatter ( formatter ) Log . addHandler ( file_handler ) # otherwise, use StreamHandler to output to stream (stdout, stderr...) else : log_format = "[%(asctime)s] %(log_color)s[%(levelname)s]%(reset)s: %(message)s" # pylint: disable=redefined-variable-type formatter = colorlog . ColoredFormatter ( fmt = log_format , datefmt = date_format ) stream_handler = logging . StreamHandler ( ) stream_handler . setFormatter ( formatter ) Log . addHandler ( stream_handler )
9993	def get_dynspace ( self , args , kwargs = None ) : node = get_node ( self , * convert_args ( args , kwargs ) ) key = node [ KEY ] if key in self . param_spaces : return self . param_spaces [ key ] else : last_self = self . system . self self . system . self = self try : space_args = self . eval_formula ( node ) finally : self . system . self = last_self if space_args is None : space_args = { "bases" : [ self ] } # Default else : if "bases" in space_args : bases = get_impls ( space_args [ "bases" ] ) if isinstance ( bases , StaticSpaceImpl ) : space_args [ "bases" ] = [ bases ] elif bases is None : space_args [ "bases" ] = [ self ] # Default else : space_args [ "bases" ] = bases else : space_args [ "bases" ] = [ self ] space_args [ "arguments" ] = node_get_args ( node ) space = self . _new_dynspace ( * * space_args ) self . param_spaces [ key ] = space space . inherit ( clear_value = False ) return space
7983	def registration_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise RegistrationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
4458	def sort_by ( self , field , asc = True ) : self . _sortby = SortbyField ( field , asc ) return self
314	def rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) : if factor_returns . ndim > 1 : # Apply column-wise return factor_returns . apply ( partial ( rolling_beta , returns ) , rolling_window = rolling_window ) else : out = pd . Series ( index = returns . index ) for beg , end in zip ( returns . index [ 0 : - rolling_window ] , returns . index [ rolling_window : ] ) : out . loc [ end ] = ep . beta ( returns . loc [ beg : end ] , factor_returns . loc [ beg : end ] ) return out
10969	def read_environment ( ) : out = { } for k , v in iteritems ( os . environ ) : if transform ( k ) in default_conf : out [ transform ( k ) ] = v return out
4577	def get_server ( self , key , * * kwds ) : kwds = dict ( self . kwds , * * kwds ) server = self . servers . get ( key ) if server : # Make sure it's the right server. server . check_keywords ( self . constructor , kwds ) else : # Make a new server server = _CachedServer ( self . constructor , key , kwds ) self . servers [ key ] = server return server
6726	def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) # Clear host key on localhost. known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
2720	def wait ( self , update_every_seconds = 1 ) : while self . status == u'in-progress' : sleep ( update_every_seconds ) self . load ( ) return self . status == u'completed'
2092	def last_job_data ( self , pk = None , * * kwargs ) : ujt = self . get ( pk , include_debug_header = True , * * kwargs ) # Determine the appropriate inventory source update. if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
8377	def get_attribute ( element , attribute , default = 0 ) : a = element . getAttribute ( attribute ) if a == "" : return default return a
4256	def get_compressed_filename ( self , filename ) : if not os . path . splitext ( filename ) [ 1 ] [ 1 : ] in self . suffixes_to_compress : return False file_stats = None compressed_stats = None compressed_filename = '{}.{}' . format ( filename , self . suffix ) try : file_stats = os . stat ( filename ) compressed_stats = os . stat ( compressed_filename ) except OSError : # FileNotFoundError is for Python3 only pass if file_stats and compressed_stats : return ( compressed_filename if file_stats . st_mtime > compressed_stats . st_mtime else False ) else : return compressed_filename
9799	def stop ( ctx , yes , pending ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not yes and not click . confirm ( "Are sure you want to stop experiments " "in group `{}`" . format ( _group ) ) : click . echo ( 'Existing without stopping experiments in group.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment_group . stop ( user , project_name , _group , pending = pending ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiments in group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments in group are being stopped." )
7066	def delete_spot_fleet_cluster ( spot_fleet_reqid , client = None , ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . cancel_spot_fleet_requests ( SpotFleetRequestIds = [ spot_fleet_reqid ] , TerminateInstances = True ) return resp
2181	def parse_authorization_response ( self , url ) : log . debug ( "Parsing token from query part of url %s" , url ) token = dict ( urldecode ( urlparse ( url ) . query ) ) log . debug ( "Updating internal client token attribute." ) self . _populate_attributes ( token ) self . token = token return token
5356	def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
988	def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ "inputFilePath" ] scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] dateEncoderArgs = recordParams [ "dateEncoderArgs" ] scalarEncoder = ScalarEncoder ( * * scalarEncoderArgs ) dateEncoder = DateEncoder ( * * dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( { "verbosity" : verbosity } ) ) sensor = network . regions [ "sensor" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) # Create the spatial pooler region spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( spatialParams ) ) # Link the SP region to the sensor input network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "resetOut" , destInput = "resetIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) # Add the TPRegion on top of the SPRegion network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , json . dumps ( temporalParams ) ) network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "topDownIn" ) spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] # Make sure learning is enabled spatialPoolerRegion . setParameter ( "learningMode" , True ) # We want temporal anomalies so disable anomalyMode in the SP. This mode is # used for computing anomalies in a non-temporal model. spatialPoolerRegion . setParameter ( "anomalyMode" , False ) temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] # Enable topDownMode to get the predicted columns output temporalPoolerRegion . setParameter ( "topDownMode" , True ) # Make sure learning is enabled (this is the default) temporalPoolerRegion . setParameter ( "learningMode" , True ) # Enable inference mode so we get predictions temporalPoolerRegion . setParameter ( "inferenceMode" , True ) # Enable anomalyMode to compute the anomaly score. temporalPoolerRegion . setParameter ( "anomalyMode" , True ) return network
11915	def render ( self , template , * * data ) : # make a copy and update the copy dct = self . global_data . copy ( ) dct . update ( data ) try : html = self . env . get_template ( template ) . render ( * * dct ) except TemplateNotFound : raise JinjaTemplateNotFound return html
13464	def add_memory ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) form = MemoryForm ( request . POST or None , request . FILES or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo_list = request . FILES . getlist ( 'photos' ) photo_count = len ( photo_list ) for upload_file in photo_list : process_upload ( upload_file , instance , form , event , request ) if photo_count > 1 : msg += "{} images were added and should appear soon." . format ( photo_count ) else : msg += "{} image was added and should appear soon." . format ( photo_count ) messages . success ( request , msg ) return HttpResponseRedirect ( '../' ) return render ( request , 'happenings/add_memories.html' , { 'form' : form , 'event' : event } )
9315	def _ensure_datetime_to_string ( maybe_dttm ) : if isinstance ( maybe_dttm , datetime . datetime ) : maybe_dttm = _format_datetime ( maybe_dttm ) return maybe_dttm
11454	def from_source ( cls , source ) : bibrecs = BibRecordPackage ( source ) bibrecs . parse ( ) for bibrec in bibrecs . get_records ( ) : yield cls ( bibrec )
12825	def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
13201	def format_short_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . short_title is None : return None output_text = convert_lsstdoc_tex ( self . short_title , 'html5' , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
1963	def sys_rt_sigprocmask ( self , cpu , how , newset , oldset ) : return self . sys_sigprocmask ( cpu , how , newset , oldset )
11536	def map_pin ( self , abstract_pin_id , physical_pin_id ) : if physical_pin_id : self . _pin_mapping [ abstract_pin_id ] = physical_pin_id else : self . _pin_mapping . pop ( abstract_pin_id , None )
13141	def build_index_and_mapping ( triples ) : ents = bidict ( ) rels = bidict ( ) ent_id = 0 rel_id = 0 collected = [ ] for t in triples : for e in ( t . head , t . tail ) : if e not in ents : ents [ e ] = ent_id ent_id += 1 if t . relation not in rels : rels [ t . relation ] = rel_id rel_id += 1 collected . append ( kgedata . TripleIndex ( ents [ t . head ] , rels [ t . relation ] , ents [ t . tail ] ) ) return collected , ents , rels
6117	def circular ( cls , shape , pixel_scale , radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
8211	def insert_point ( self , x , y ) : try : bezier = _ctx . ximport ( "bezier" ) except : from nodebox . graphics import bezier # Do a number of checks distributed along the path. # Keep the one closest to the actual mouse location. # Ten checks works fast but leads to imprecision in sharp corners # and curves closely located next to each other. # I prefer the slower but more stable approach. n = 100 closest = None dx0 = float ( "inf" ) dy0 = float ( "inf" ) for i in range ( n ) : t = float ( i ) / n pt = self . path . point ( t ) dx = abs ( pt . x - x ) dy = abs ( pt . y - y ) if dx + dy <= dx0 + dy0 : dx0 = dx dy0 = dy closest = t # Next, scan the area around the approximation. # If the closest point is located at 0.2 on the path, # we need to scan between 0.1 and 0.3 for a better # approximation. If 1.5 was the best guess, scan # 1.40, 1.41 ... 1.59 and so on. # Each decimal precision takes 20 iterations. decimals = [ 3 , 4 ] for d in decimals : d = 1.0 / pow ( 10 , d ) for i in range ( 20 ) : t = closest - d + float ( i ) * d * 0.1 if t < 0.0 : t = 1.0 + t if t > 1.0 : t = t - 1.0 pt = self . path . point ( t ) dx = abs ( pt . x - x ) dy = abs ( pt . y - y ) if dx <= dx0 and dy <= dy0 : dx0 = dx dy0 = dy closest_precise = t closest = closest_precise # Update the points list with the inserted point. p = bezier . insert_point ( self . path , closest_precise ) i , t , pt = bezier . _locate ( self . path , closest_precise ) i += 1 pt = PathElement ( ) pt . cmd = p [ i ] . cmd pt . x = p [ i ] . x pt . y = p [ i ] . y pt . ctrl1 = Point ( p [ i ] . ctrl1 . x , p [ i ] . ctrl1 . y ) pt . ctrl2 = Point ( p [ i ] . ctrl2 . x , p [ i ] . ctrl2 . y ) pt . freehand = False self . _points . insert ( i , pt ) self . _points [ i - 1 ] . ctrl1 = Point ( p [ i - 1 ] . ctrl1 . x , p [ i - 1 ] . ctrl1 . y ) self . _points [ i + 1 ] . ctrl1 = Point ( p [ i + 1 ] . ctrl1 . x , p [ i + 1 ] . ctrl1 . y ) self . _points [ i + 1 ] . ctrl2 = Point ( p [ i + 1 ] . ctrl2 . x , p [ i + 1 ] . ctrl2 . y )
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , * * conn ) : # Python 2 and 3 support: try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , * * conn )
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
13627	def Timestamp ( value , _divisor = 1. , tz = UTC , encoding = None ) : value = Float ( value , encoding ) if value is not None : value = value / _divisor return datetime . fromtimestamp ( value , tz ) return None
8832	def if_ ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
7049	def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR = { 0.3 : MASSESRADII_0_3GYR , 1.0 : MASSESRADII_1_0GYR , 4.5 : MASSESRADII_4_5GYR } if age not in MR : print ( 'given age not in Fortney 2007, returning...' ) return massradius = MR [ age ] if ( planetdist in massradius ) and ( coremass in massradius [ planetdist ] ) : print ( 'getting % Gyr M-R for planet dist %s AU, ' 'core mass %s Mearth...' % ( age , planetdist , coremass ) ) massradrelation = massradius [ planetdist ] [ coremass ] outdict = { 'mass' : array ( massradrelation [ mass ] ) , 'radius' : array ( massradrelation [ radius ] ) } return outdict
4918	def course_detail ( self , request , pk , course_key ) : # pylint: disable=invalid-name,unused-argument enterprise_customer_catalog = self . get_object ( ) course = enterprise_customer_catalog . get_course ( course_key ) if not course : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseDetailSerializer ( course , context = context ) return Response ( serializer . data )
6608	def wait ( self ) : sleep = 5 while True : if self . clusterprocids_outstanding : self . poll ( ) if not self . clusterprocids_outstanding : break time . sleep ( sleep ) return self . clusterprocids_finished
2962	def expand_partitions ( containers , partitions ) : # filter out holy containers that don't belong # to any partition at all all_names = frozenset ( c . name for c in containers if not c . holy ) holy_names = frozenset ( c . name for c in containers if c . holy ) neutral_names = frozenset ( c . name for c in containers if c . neutral ) partitions = [ frozenset ( p ) for p in partitions ] unknown = set ( ) holy = set ( ) union = set ( ) for partition in partitions : unknown . update ( partition - all_names - holy_names ) holy . update ( partition - all_names ) union . update ( partition ) if unknown : raise BlockadeError ( 'Partitions contain unknown containers: %s' % list ( unknown ) ) if holy : raise BlockadeError ( 'Partitions contain holy containers: %s' % list ( holy ) ) # put any leftover containers in an implicit partition leftover = all_names . difference ( union ) if leftover : partitions . append ( leftover ) # we create an 'implicit' partition for the neutral containers # in case they are not part of the leftover anyways if not neutral_names . issubset ( leftover ) : partitions . append ( neutral_names ) return partitions
6339	def lcsseq ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) # row 0 and column 0 are initialized to 0 already for i , src_char in enumerate ( src ) : for j , tar_char in enumerate ( tar ) : if src_char == tar_char : lengths [ i + 1 , j + 1 ] = lengths [ i , j ] + 1 else : lengths [ i + 1 , j + 1 ] = max ( lengths [ i + 1 , j ] , lengths [ i , j + 1 ] ) # read the substring out from the matrix result = '' i , j = len ( src ) , len ( tar ) while i != 0 and j != 0 : if lengths [ i , j ] == lengths [ i - 1 , j ] : i -= 1 elif lengths [ i , j ] == lengths [ i , j - 1 ] : j -= 1 else : result = src [ i - 1 ] + result i -= 1 j -= 1 return result
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) # Update origData with the new data if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) # Update origData with the new data obj . _origData [ thisField ] = newValue
11824	def genetic_search ( problem , fitness_fn , ngen = 1000 , pmut = 0.1 , n = 20 ) : s = problem . initial_state states = [ problem . result ( s , a ) for a in problem . actions ( s ) ] random . shuffle ( states ) return genetic_algorithm ( states [ : n ] , problem . value , ngen , pmut )
9274	def filter_excluded_tags ( self , all_tags ) : filtered_tags = copy . deepcopy ( all_tags ) if self . options . exclude_tags : filtered_tags = self . apply_exclude_tags ( filtered_tags ) if self . options . exclude_tags_regex : filtered_tags = self . apply_exclude_tags_regex ( filtered_tags ) return filtered_tags
10889	def kvectors ( self , norm = False , form = 'broadcast' , real = False , shift = False ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . fft . fftfreq ( self . shape [ i ] ) / norm [ i ] for i in range ( self . dim ) ) if shift : v = list ( np . fft . fftshift ( t ) for t in v ) if real : v [ - 1 ] = v [ - 1 ] [ : ( self . shape [ - 1 ] + 1 ) // 2 ] return self . _format_vector ( v , form = form )
751	def _addAnomalyClassifierRegion ( self , network , params , spEnable , tmEnable ) : allParams = copy . deepcopy ( params ) knnParams = dict ( k = 1 , distanceMethod = 'rawOverlap' , distanceNorm = 1 , doBinarization = 1 , replaceDuplicates = 0 , maxStoredPatterns = 1000 ) allParams . update ( knnParams ) # Set defaults if not set if allParams [ 'trainRecords' ] is None : allParams [ 'trainRecords' ] = DEFAULT_ANOMALY_TRAINRECORDS if allParams [ 'cacheSize' ] is None : allParams [ 'cacheSize' ] = DEFAULT_ANOMALY_CACHESIZE # Remove current instance if already created (used for deserializing) if self . _netInfo is not None and self . _netInfo . net is not None and self . _getAnomalyClassifier ( ) is not None : self . _netInfo . net . removeRegion ( 'AnomalyClassifier' ) network . addRegion ( "AnomalyClassifier" , "py.KNNAnomalyClassifierRegion" , json . dumps ( allParams ) ) # Attach link to SP if spEnable : network . link ( "SP" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "bottomUpOut" , destInput = "spBottomUpOut" ) else : network . link ( "sensor" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "spBottomUpOut" ) # Attach link to TM if tmEnable : network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "tpTopDownOut" ) network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "lrnActiveStateT" , destInput = "tpLrnActiveStateT" ) else : raise RuntimeError ( "TemporalAnomaly models require a TM region." )
12791	def delete ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None ) : return self . _fetch ( "DELETE" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , full_return = True )
10509	def stoplog ( self ) : if self . _file_logger : self . logger . removeHandler ( _file_logger ) self . _file_logger = None return 1
8803	def notify ( context , event_type , ipaddress , send_usage = False , * args , * * kwargs ) : if ( event_type == IP_ADD and not CONF . QUARK . notify_ip_add ) or ( event_type == IP_DEL and not CONF . QUARK . notify_ip_delete ) or ( event_type == IP_ASSOC and not CONF . QUARK . notify_flip_associate ) or ( event_type == IP_DISASSOC and not CONF . QUARK . notify_flip_disassociate ) or ( event_type == IP_EXISTS and not CONF . QUARK . notify_ip_exists ) : LOG . debug ( 'IP_BILL: notification {} is disabled by config' . format ( event_type ) ) return # Do not send notifications when we are undoing due to an error if 'rollback' in kwargs and kwargs [ 'rollback' ] : LOG . debug ( 'IP_BILL: not sending notification because we are in undo' ) return # ip.add needs the allocated_at time. # All other events need the current time. ts = ipaddress . allocated_at if event_type == IP_ADD else _now ( ) payload = build_payload ( ipaddress , event_type , event_time = ts ) # Send the notification with the payload do_notify ( context , event_type , payload ) # When we deallocate an IP or associate a FLIP we must send # a usage message to billing. # In other words when we supply end_time we must send USAGE to billing # immediately. # Our billing period is 24 hrs. If the address was allocated after midnight # send the start_time as as. If the address was allocated yesterday, then # send midnight as the start_time. # Note: if allocated_at is empty we assume today's midnight. if send_usage : if ipaddress . allocated_at is not None and ipaddress . allocated_at >= _midnight_today ( ) : start_time = ipaddress . allocated_at else : start_time = _midnight_today ( ) payload = build_payload ( ipaddress , IP_EXISTS , start_time = start_time , end_time = ts ) do_notify ( context , IP_EXISTS , payload )
8067	def loadGrammar ( self , grammar , searchpaths = None ) : self . grammar = self . _load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . getElementsByTagName ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
12564	def _partition_data ( datavol , roivol , roivalue , maskvol = None , zeroe = True ) : if maskvol is not None : # get all masked time series within this roi r indices = ( roivol == roivalue ) * ( maskvol > 0 ) else : # get all time series within this roi r indices = roivol == roivalue if datavol . ndim == 4 : ts = datavol [ indices , : ] else : ts = datavol [ indices ] # remove zeroed time series if zeroe : if datavol . ndim == 4 : ts = ts [ ts . sum ( axis = 1 ) != 0 , : ] return ts
4672	def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
9913	def _create ( cls , model_class , * args , * * kwargs ) : manager = cls . _get_manager ( model_class ) return manager . create_user ( * args , * * kwargs )
6467	def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
5932	def scale_impropers ( mol , impropers , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_impropers = [ ] for im in mol . impropers : atypes = ( im . atom1 . get_atomtype ( ) , im . atom2 . get_atomtype ( ) , im . atom3 . get_atomtype ( ) , im . atom4 . get_atomtype ( ) ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] # special-case: this is a [ dihedral ] override in molecule block, continue and don't match if im . gromacs [ 'param' ] != [ ] : for p in im . gromacs [ 'param' ] : p [ 'kpsi' ] *= scale new_impropers . append ( im ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , im . gromacs [ 'func' ] ) if ( key in impropers ) : for i , imt in enumerate ( impropers [ key ] ) : imA = copy . deepcopy ( im ) param = copy . deepcopy ( imt . gromacs [ 'param' ] ) # Only check the first dihedral in a list if not impropers [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kpsi' ] *= scale imA . gromacs [ 'param' ] = param if i == 0 : imA . comment = "; banned lines {0} found={1}\n ; parameters for types {2}-{3}-{4}-{5}-9 at LINE({6})\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if imt . line in banned_lines else 0 , imt . atype1 , imt . atype2 , imt . atype3 , imt . atype4 , imt . line ) new_impropers . append ( imA ) break #assert(len(mol.impropers) == new_impropers) mol . impropers = new_impropers return mol
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
10366	def complex_has_member ( graph : BELGraph , complex_node : ComplexAbundance , member_node : BaseEntity ) -> bool : return any ( # TODO can't you look in the members of the complex object (if it's enumerated) v == member_node for _ , v , data in graph . out_edges ( complex_node , data = True ) if data [ RELATION ] == HAS_COMPONENT )
3617	def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
12298	def discover_all_plugins ( self ) : for v in pkg_resources . iter_entry_points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
1301	def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )
565	def updateResultsForJob ( self , forceUpdate = True ) : updateInterval = time . time ( ) - self . _lastUpdateAttemptTime if updateInterval < self . _MIN_UPDATE_INTERVAL and not forceUpdate : return self . logger . info ( "Attempting model selection for jobID=%d: time=%f" " lastUpdate=%f" % ( self . _jobID , time . time ( ) , self . _lastUpdateAttemptTime ) ) timestampUpdated = self . _cjDB . jobUpdateSelectionSweep ( self . _jobID , self . _MIN_UPDATE_INTERVAL ) if not timestampUpdated : self . logger . info ( "Unable to update selection sweep timestamp: jobID=%d" " updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) if not forceUpdate : return self . _lastUpdateAttemptTime = time . time ( ) self . logger . info ( "Succesfully updated selection sweep timestamp jobid=%d updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) minUpdateRecords = self . _MIN_UPDATE_THRESHOLD jobResults = self . _getJobResults ( ) if forceUpdate or jobResults is None : minUpdateRecords = 0 candidateIDs , bestMetric = self . _cjDB . modelsGetCandidates ( self . _jobID , minUpdateRecords ) self . logger . info ( "Candidate models=%s, metric=%s, jobID=%s" % ( candidateIDs , bestMetric , self . _jobID ) ) if len ( candidateIDs ) == 0 : return self . _jobUpdateCandidate ( candidateIDs [ 0 ] , bestMetric , results = jobResults )
13467	def set_Courant_Snyder ( self , beta , alpha , emit = None , emit_n = None ) : self . _store_emit ( emit = emit , emit_n = emit_n ) self . _sx = _np . sqrt ( beta * self . emit ) self . _sxp = _np . sqrt ( ( 1 + alpha ** 2 ) / beta * self . emit ) self . _sxxp = - alpha * self . emit
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
3081	def xsrf_secret_key ( ) : secret = memcache . get ( XSRF_MEMCACHE_ID , namespace = OAUTH2CLIENT_NAMESPACE ) if not secret : # Load the one and only instance of SiteXsrfSecretKey. model = SiteXsrfSecretKey . get_or_insert ( key_name = 'site' ) if not model . secret : model . secret = _generate_new_xsrf_secret_key ( ) model . put ( ) secret = model . secret memcache . add ( XSRF_MEMCACHE_ID , secret , namespace = OAUTH2CLIENT_NAMESPACE ) return str ( secret )
11519	def perform_upload ( self , upload_token , filename , * * kwargs ) : parameters = dict ( ) parameters [ 'uploadtoken' ] = upload_token parameters [ 'filename' ] = filename try : create_additional_revision = kwargs [ 'create_additional_revision' ] except KeyError : create_additional_revision = False if not create_additional_revision : parameters [ 'revision' ] = 'head' optional_keys = [ 'mode' , 'folderid' , 'item_id' , 'itemid' , 'revision' ] for key in optional_keys : if key in kwargs : if key == 'item_id' : parameters [ 'itemid' ] = kwargs [ key ] continue if key == 'folder_id' : parameters [ 'folderid' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] # We may want a different name than path file_payload = open ( kwargs . get ( 'filepath' , filename ) , 'rb' ) # Arcane getting of the file size using fstat. More details can be # found in the python library docs parameters [ 'length' ] = os . fstat ( file_payload . fileno ( ) ) . st_size response = self . request ( 'midas.upload.perform' , parameters , file_payload ) return response
8513	def fit_and_score_estimator ( estimator , parameters , cv , X , y = None , scoring = None , iid = True , n_jobs = 1 , verbose = 1 , pre_dispatch = '2*n_jobs' ) : scorer = check_scoring ( estimator , scoring = scoring ) n_samples = num_samples ( X ) X , y = check_arrays ( X , y , allow_lists = True , sparse_format = 'csr' , allow_nans = True ) if y is not None : if len ( y ) != n_samples : raise ValueError ( 'Target variable (y) has a different number ' 'of samples (%i) than data (X: %i samples)' % ( len ( y ) , n_samples ) ) cv = check_cv ( cv = cv , y = y , classifier = is_classifier ( estimator ) ) out = Parallel ( n_jobs = n_jobs , verbose = verbose , pre_dispatch = pre_dispatch ) ( delayed ( _fit_and_score ) ( clone ( estimator ) , X , y , scorer , train , test , verbose , parameters , fit_params = None ) for train , test in cv . split ( X , y ) ) assert len ( out ) == cv . n_splits train_scores , test_scores = [ ] , [ ] n_train_samples , n_test_samples = [ ] , [ ] for test_score , n_test , train_score , n_train , _ in out : train_scores . append ( train_score ) test_scores . append ( test_score ) n_test_samples . append ( n_test ) n_train_samples . append ( n_train ) train_scores , test_scores = map ( list , check_arrays ( train_scores , test_scores , warn_nans = True , replace_nans = True ) ) if iid : if verbose > 0 and is_msmbuilder_estimator ( estimator ) : print ( '[CV] Using MSMBuilder API n_samples averaging' ) print ( '[CV] n_train_samples: %s' % str ( n_train_samples ) ) print ( '[CV] n_test_samples: %s' % str ( n_test_samples ) ) mean_test_score = np . average ( test_scores , weights = n_test_samples ) mean_train_score = np . average ( train_scores , weights = n_train_samples ) else : mean_test_score = np . average ( test_scores ) mean_train_score = np . average ( train_scores ) grid_scores = { 'mean_test_score' : mean_test_score , 'test_scores' : test_scores , 'mean_train_score' : mean_train_score , 'train_scores' : train_scores , 'n_test_samples' : n_test_samples , 'n_train_samples' : n_train_samples } return grid_scores
8429	def cmap_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) def _cmap_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _cmap_pal
9611	def _request ( self , method , url , body ) : if method != 'POST' and method != 'PUT' : body = None s = Session ( ) LOGGER . debug ( 'Method: {0}, Url: {1}, Body: {2}.' . format ( method , url , body ) ) req = Request ( method , url , json = body ) prepped = s . prepare_request ( req ) res = s . send ( prepped , timeout = self . _timeout or None ) res . raise_for_status ( ) # TODO try catch return res . json ( )
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , * * filters )
2598	def can_sequence ( obj ) : if istype ( obj , sequence_types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
13605	def url_correct ( self , point , auth = None , export = None ) : newUrl = self . __url + point + '.json' if auth or export : newUrl += "?" if auth : newUrl += ( "auth=" + auth ) if export : if not newUrl . endswith ( '?' ) : newUrl += "&" newUrl += "format=export" return newUrl
3347	def add_members ( self , new_members ) : if isinstance ( new_members , string_types ) or hasattr ( new_members , "id" ) : warn ( "need to pass in a list" ) new_members = [ new_members ] self . _members . update ( new_members )
1066	def getheader ( self , name , default = None ) : return self . dict . get ( name . lower ( ) , default )
4298	def _convert_config_to_stdin ( config , parser ) : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) args = [ ] for key , val in config . items ( SECTION ) : keyp = '--{0}' . format ( key ) action = parser . _option_string_actions [ keyp ] if action . const : try : if config . getboolean ( SECTION , key ) : args . append ( keyp ) except ValueError : args . extend ( [ keyp , val ] ) # Pass it as is to get the error from ArgumentParser. elif any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : # Some keys with empty values shouldn't be passed into args to use their defaults # from ArgumentParser. if val != '' : args . extend ( [ keyp , val ] ) else : args . extend ( [ keyp , val ] ) return args
12291	def annotate_metadata_data ( repo , task , patterns = [ "*" ] , size = 0 ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] matching_files = repo . find_matching_files ( patterns ) package = repo . package rootdir = repo . rootdir files = package [ 'resources' ] for f in files : relativepath = f [ 'relativepath' ] if relativepath in matching_files : path = os . path . join ( rootdir , relativepath ) if task == 'preview' : print ( "Adding preview for " , relativepath ) f [ 'content' ] = open ( path ) . read ( ) [ : size ] elif task == 'schema' : for r in representations : if r . can_process ( path ) : print ( "Adding schema for " , path ) f [ 'schema' ] = r . get_schema ( path ) break
8441	def _parse_link_header ( headers ) : links = { } if 'link' in headers : link_headers = headers [ 'link' ] . split ( ', ' ) for link_header in link_headers : ( url , rel ) = link_header . split ( '; ' ) url = url [ 1 : - 1 ] rel = rel [ 5 : - 1 ] links [ rel ] = url return links
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
8652	def create_thread ( session , member_ids , context_type , context , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } thread_data = { 'members[]' : member_ids , 'context_type' : context_type , 'context' : context , 'message' : message , } # POST /api/messages/0.1/threads/ response = make_post_request ( session , 'threads' , headers , form_data = thread_data ) json_data = response . json ( ) if response . status_code == 200 : return Thread ( json_data [ 'result' ] ) else : raise ThreadNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7073	def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
7543	def chunk_clusters ( data , sample ) : ## counter for split job submission num = 0 ## set optim size for chunks in N clusters. The first few chunks take longer ## because they contain larger clusters, so we create 4X as many chunks as ## processors so that they are split more evenly. optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) ## break up the file into smaller tmp files for each engine ## chunking by cluster is a bit trickier than chunking by N lines chunkslist = [ ] ## open to clusters with gzip . open ( sample . files . clusters , 'rb' ) as clusters : ## create iterator to sample 2 lines at a time pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## Use iterator to sample til end of cluster done = 0 while not done : ## grab optim clusters and write to file. done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
6122	def zoom_region ( self ) : # Have to convert mask to bool for invert function to work. where = np . array ( np . where ( np . invert ( self . astype ( 'bool' ) ) ) ) y0 , x0 = np . amin ( where , axis = 1 ) y1 , x1 = np . amax ( where , axis = 1 ) return [ y0 , y1 + 1 , x0 , x1 + 1 ]
11125	def rename_file ( self , relativePath , name , newName , replace = False , verbose = True ) : # normalize path relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage # check directory in repository assert name in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' is not found in repository relative path '%s'" % ( name , relativePath ) # get real path realPath = os . path . join ( self . __path , relativePath , name ) assert os . path . isfile ( realPath ) , "file '%s' is not found in system" % realPath # assert directory new name doesn't exist in repository assert newName not in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' already exists in repository relative path '%s'" % ( newName , relativePath ) # check new directory in system newRealPath = os . path . join ( self . __path , relativePath , newName ) if os . path . isfile ( newRealPath ) : if replace : os . remove ( newRealPath ) if verbose : warnings . warn ( "file '%s' already exists found in system, it is now replaced by '%s' because 'replace' flag is True." % ( newRealPath , realPath ) ) else : raise Exception ( "file '%s' already exists in system but not registered in repository." % newRealPath ) # rename file os . rename ( realPath , newRealPath ) dict . __setitem__ ( dict . __getitem__ ( dirInfoDict , "files" ) , newName , dict . __getitem__ ( dirInfoDict , "files" ) . pop ( name ) ) # save repository self . save ( )
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
8229	def ximport ( self , libName ) : # from Nodebox lib = __import__ ( libName ) self . _namespace [ libName ] = lib lib . _ctx = self return lib
12126	def spec_formatter ( cls , spec ) : return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
4682	def getKeyType ( self , account , pub ) : for authority in [ "owner" , "active" ] : for key in account [ authority ] [ "key_auths" ] : if str ( pub ) == key [ 0 ] : return authority if str ( pub ) == account [ "options" ] [ "memo_key" ] : return "memo" return None
3011	def locked_get ( self ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( * * filters ) entity = query . first ( ) if entity : credential = getattr ( entity , self . property_name ) if credential and hasattr ( credential , 'set_store' ) : credential . set_store ( self ) return credential else : return None
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : # check if this file is readable/writeable by user only fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) # get today's datetime now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : # this hideous incantation is required for lesser Pythons expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
8322	def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except ValueError : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , "utf_8" , "replace" ) except : return "" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( "true" , "1" , "yes" ) : return True else : return False
2932	def package_for_editor_signavio ( self , spec , filename ) : signavio_file = filename [ : - len ( '.bpmn20.xml' ) ] + '.signavio.xml' if os . path . exists ( signavio_file ) : self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( signavio_file ) , signavio_file ) f = open ( signavio_file , 'r' ) try : signavio_tree = ET . parse ( f ) finally : f . close ( ) svg_node = one ( signavio_tree . findall ( './/svg-representation' ) ) self . write_to_package_zip ( "%s.svg" % spec . name , svg_node . text )
11267	def walk ( prev , inital_path , * args , * * kw ) : for dir_path , dir_names , filenames in os . walk ( inital_path ) : for filename in filenames : yield os . path . join ( dir_path , filename )
7940	def _got_srv ( self , addrs ) : with self . lock : if not addrs : self . _dst_service = None if self . _dst_port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] else : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Could not resolve SRV for service {0!r}" " on host {1!r} and fallback port number not given" . format ( self . _dst_service , self . _dst_name ) ) elif addrs == [ ( "." , 0 ) ] : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Service {0!r} not available on host {1!r}" . format ( self . _dst_service , self . _dst_name ) ) else : self . _dst_nameports = addrs self . _set_state ( "resolve-hostname" )
12703	def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
11075	def set ( self , user ) : self . log . info ( "Loading user information for %s/%s" , user . id , user . username ) self . load_user_info ( user ) self . log . info ( "Loading user rights for %s/%s" , user . id , user . username ) self . load_user_rights ( user ) self . log . info ( "Added user: %s/%s" , user . id , user . username ) self . _add_user_to_cache ( user ) return user
6379	def dist_manhattan ( src , tar , qval = 2 , alphabet = None ) : return Manhattan ( ) . dist ( src , tar , qval , alphabet )
11649	def transform ( self , X ) : n = self . train_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( n ) ) if self . copy : X = X . copy ( ) if self . shift_ != 0 and X is self . train_ or ( X . shape == self . train_ . shape and np . allclose ( X , self . train_ ) ) : X [ xrange ( n ) , xrange ( n ) ] += self . shift_ return X
11462	def update_subject_categories ( self , primary , secondary , kb ) : category_fields = record_get_field_instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record_delete_fields ( self . record , "650" ) for field in category_fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new_value = self . get_config_item ( value , kb ) if new_value != value : new_subs = [ ( '2' , secondary ) , ( 'a' , new_value ) ] else : new_subs = [ ( '2' , primary ) , ( 'a' , value ) ] record_add_field ( self . record , "650" , ind1 = "1" , ind2 = "7" , subfields = new_subs ) break
13624	def Integer ( value , base = 10 , encoding = None ) : try : return int ( Text ( value , encoding ) , base ) except ( TypeError , ValueError ) : return None
578	def dictDiffAndReport ( da , db ) : differences = dictDiff ( da , db ) if not differences : return differences if differences [ 'inAButNotInB' ] : print ">>> inAButNotInB: %s" % differences [ 'inAButNotInB' ] if differences [ 'inBButNotInA' ] : print ">>> inBButNotInA: %s" % differences [ 'inBButNotInA' ] for key in differences [ 'differentValues' ] : print ">>> da[%s] != db[%s]" % ( key , key ) print "da[%s] = %r" % ( key , da [ key ] ) print "db[%s] = %r" % ( key , db [ key ] ) return differences
9905	def ping ( self ) : self . __validate_ping_param ( ) ping_proc = subprocrunner . SubprocessRunner ( self . __get_ping_command ( ) ) ping_proc . run ( ) return PingResult ( ping_proc . stdout , ping_proc . stderr , ping_proc . returncode )
2199	def platform_config_dir ( ) : if LINUX : # nocover dpath_ = os . environ . get ( 'XDG_CONFIG_HOME' , '~/.config' ) elif DARWIN : # nocover dpath_ = '~/Library/Application Support' elif WIN32 : # nocover dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : # nocover raise NotImplementedError ( 'Unknown Platform %r' % ( sys . platform , ) ) dpath = normpath ( expanduser ( dpath_ ) ) return dpath
8621	def get_self ( session , user_details = None ) : # Set compact to true if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'self' , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise SelfNotRetrievedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3377	def check_solver_status ( status , raise_error = False ) : if status == OPTIMAL : return elif ( status in has_primals ) and not raise_error : warn ( "solver status is '{}'" . format ( status ) , UserWarning ) elif status is None : raise OptimizationError ( "model was not optimized yet or solver context switched" ) else : raise OptimizationError ( "solver status is '{}'" . format ( status ) )
13343	def _broadcast_shape ( * args ) : #TODO: currently incorrect result if a Sequence is provided as an input shapes = [ a . shape if hasattr ( type ( a ) , '__array_interface__' ) else ( ) for a in args ] ndim = max ( len ( sh ) for sh in shapes ) # new common ndim after broadcasting for i , sh in enumerate ( shapes ) : if len ( sh ) < ndim : shapes [ i ] = ( 1 , ) * ( ndim - len ( sh ) ) + sh return tuple ( max ( sh [ ax ] for sh in shapes ) for ax in range ( ndim ) )
8399	def transform ( x ) : try : x = date2num ( x ) except AttributeError : # numpy datetime64 # This is not ideal because the operations do not # preserve the np.datetime64 type. May be need # a datetime64_trans x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
4509	def set_device_id ( self , dev , id ) : if id < 0 or id > 255 : raise ValueError ( "ID must be an unsigned byte!" ) com , code , ok = io . send_packet ( CMDTYPE . SETID , 1 , dev , self . baudrate , 5 , id ) if not ok : raise_error ( code )
1668	def ShouldCheckNamespaceIndentation ( nesting_state , is_namespace_indent_item , raw_lines_no_comments , linenum ) : is_forward_declaration = IsForwardClassDeclaration ( raw_lines_no_comments , linenum ) if not ( is_namespace_indent_item or is_forward_declaration ) : return False # If we are in a macro, we do not want to check the namespace indentation. if IsMacroDefinition ( raw_lines_no_comments , linenum ) : return False return IsBlockInNameSpace ( nesting_state , is_forward_declaration )
7891	def set_stream ( self , stream ) : _unused = stream if self . joined and self . handler : self . handler . user_left ( self . me , None ) self . joined = False
3746	def calculate_P ( self , T , P , method ) : if method == LUCAS : mu = self . T_dependent_property ( T ) Psat = self . Psat ( T ) if hasattr ( self . Psat , '__call__' ) else self . Psat mu = Lucas ( T , P , self . Tc , self . Pc , self . omega , Psat , mu ) elif method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
5047	def get ( self , request , customer_uuid ) : context = self . _build_context ( request , customer_uuid ) manage_learners_form = ManageLearnersForm ( user = request . user , enterprise_customer = context [ self . ContextParameters . ENTERPRISE_CUSTOMER ] ) context . update ( { self . ContextParameters . MANAGE_LEARNERS_FORM : manage_learners_form } ) return render ( request , self . template , context )
621	def parseStringList ( s ) : assert isinstance ( s , basestring ) return [ int ( i ) for i in s . split ( ) ]
12432	def create ( self ) : # create virtualenv self . create_virtualenv ( ) # create project self . create_project ( ) # generate uwsgi script self . create_uwsgi_script ( ) # generate nginx config self . create_nginx_config ( ) # generate management scripts self . create_manage_scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
4481	def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
10142	def upload_files ( selected_file , selected_host , only_link , file_name ) : try : answer = requests . post ( url = selected_host [ 0 ] + "upload.php" , files = { 'files[]' : selected_file } ) file_name_1 = re . findall ( r'"url": *"((h.+\/){0,1}(.+?))"[,\}]' , answer . text . replace ( "\\" , "" ) ) [ 0 ] [ 2 ] if only_link : return [ selected_host [ 1 ] + file_name_1 , "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) ] else : return "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) except requests . exceptions . ConnectionError : print ( file_name + ' couldn\'t be uploaded to ' + selected_host [ 0 ] )
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
11957	def is_oct ( ip ) : try : dec = int ( str ( ip ) , 8 ) except ( TypeError , ValueError ) : return False if dec > 0o37777777777 or dec < 0 : return False return True
12449	def _add_method ( self , effect , verb , resource , conditions ) : if verb != '*' and not hasattr ( HttpVerb , verb ) : raise NameError ( 'Invalid HTTP verb ' + verb + '. Allowed verbs in HttpVerb class' ) resource_pattern = re . compile ( self . path_regex ) if not resource_pattern . match ( resource ) : raise NameError ( 'Invalid resource path: ' + resource + '. Path should match ' + self . path_regex ) if resource [ : 1 ] == '/' : resource = resource [ 1 : ] resource_arn = ( 'arn:aws:execute-api:' + self . region + ':' + self . aws_account_id + ':' + self . rest_api_id + '/' + self . stage + '/' + verb + '/' + resource ) if effect . lower ( ) == 'allow' : self . allowMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } ) elif effect . lower ( ) == 'deny' : self . denyMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } )
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
12352	def rebuild ( self , image , wait = True ) : return self . _action ( 'rebuild' , image = image , wait = wait )
5483	def setup_service ( api_name , api_version , credentials = None ) : if not credentials : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return apiclient . discovery . build ( api_name , api_version , credentials = credentials )
12024	def adopt ( self , old_parent , new_parent ) : try : # assume line_data(dict) old_id = old_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : # assume line_index(int) old_id = self . lines [ old_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : # assume feature_id(str) old_id = old_parent old_feature = self . features [ old_id ] old_indexes = [ ld [ 'line_index' ] for ld in old_feature ] try : # assume line_data(dict) new_id = new_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : # assume line_index(int) new_id = self . lines [ new_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : # assume feature_id(str) new_id = new_parent new_feature = self . features [ new_id ] new_indexes = [ ld [ 'line_index' ] for ld in new_feature ] # build a list of children to be moved # add the child to the new parent's children list if its not already there # update the child's parent list and parent attribute # finally remove the old parent's children list children = old_feature [ 0 ] [ 'children' ] new_parent_children_set = set ( [ ld [ 'line_index' ] for ld in new_feature [ 0 ] [ 'children' ] ] ) for child in children : if child [ 'line_index' ] not in new_parent_children_set : new_parent_children_set . add ( child [ 'line_index' ] ) for new_ld in new_feature : new_ld [ 'children' ] . append ( child ) child [ 'parents' ] . append ( new_feature ) child [ 'attributes' ] [ 'Parent' ] . append ( new_id ) # remove multiple, list.remove() only removes 1 child [ 'parents' ] = [ f for f in child [ 'parents' ] if f [ 0 ] [ 'attributes' ] [ 'ID' ] != old_id ] child [ 'attributes' ] [ 'Parent' ] = [ d for d in child [ 'attributes' ] [ 'Parent' ] if d != old_id ] for old_ld in old_feature : old_ld [ 'children' ] = [ ] return children
642	def readConfigFile ( cls , filename , path = None ) : properties = cls . _readConfigFile ( filename , path ) # Create properties dict if necessary if cls . _properties is None : cls . _properties = dict ( ) for name in properties : if 'value' in properties [ name ] : cls . _properties [ name ] = properties [ name ] [ 'value' ]
4502	def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
7295	def create_document_dictionary ( self , document , document_key = None , owner_document = None ) : doc_dict = self . create_doc_dict ( document , document_key , owner_document ) for doc_key , doc_field in doc_dict . items ( ) : # Base fields should not be evaluated if doc_key . startswith ( "_" ) : continue if isinstance ( doc_field , ListField ) : doc_dict [ doc_key ] = self . create_list_dict ( document , doc_field , doc_key ) elif isinstance ( doc_field , EmbeddedDocumentField ) : doc_dict [ doc_key ] = self . create_document_dictionary ( doc_dict [ doc_key ] . document_type_obj , doc_key ) else : doc_dict [ doc_key ] = { "_document" : document , "_key" : doc_key , "_document_field" : doc_field , "_widget" : get_widget ( doc_dict [ doc_key ] , getattr ( doc_field , 'disabled' , False ) ) } return doc_dict
13695	def debug ( * args , * * kwargs ) : if not ( DEBUG and args ) : return None # Include parent class name when given. parent = kwargs . get ( 'parent' , None ) with suppress ( KeyError ) : kwargs . pop ( 'parent' ) # Go back more than once when given. backlevel = kwargs . get ( 'back' , 1 ) with suppress ( KeyError ) : kwargs . pop ( 'back' ) frame = inspect . currentframe ( ) # Go back a number of frames (usually 1). while backlevel > 0 : frame = frame . f_back backlevel -= 1 fname = os . path . split ( frame . f_code . co_filename ) [ - 1 ] lineno = frame . f_lineno if parent : func = '{}.{}' . format ( parent . __class__ . __name__ , frame . f_code . co_name ) else : func = frame . f_code . co_name lineinfo = '{}:{} {}: ' . format ( C ( fname , 'yellow' ) , C ( str ( lineno ) . ljust ( 4 ) , 'blue' ) , C ( ) . join ( C ( func , 'magenta' ) , '()' ) . ljust ( 20 ) ) # Patch args to stay compatible with print(). pargs = list ( C ( a , 'green' ) . str ( ) for a in args ) pargs [ 0 ] = '' . join ( ( lineinfo , pargs [ 0 ] ) ) print_err ( * pargs , * * kwargs )
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : # pylint: disable=missing-docstring e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
2524	def get_review_date ( self , r_term ) : reviewed_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewDate' ] , None ) ) ) if len ( reviewed_list ) != 1 : self . error = True msg = 'Review must have exactlyone review date' self . logger . log ( msg ) return return six . text_type ( reviewed_list [ 0 ] [ 2 ] )
12124	def to_table ( args , vdims = [ ] ) : if not Table : return "HoloViews Table not available" kdims = [ dim for dim in args . constant_keys + args . varying_keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
9815	def stop ( ctx , commit , yes ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not yes and not click . confirm ( "Are sure you want to stop notebook " "for project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without stopping notebook.' ) sys . exit ( 1 ) if commit is None : commit = True try : PolyaxonClient ( ) . project . stop_notebook ( user , project_name , commit ) Printer . print_success ( 'Notebook is being deleted' ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9655	def run_commands ( commands , settings ) : sprint = settings [ "sprint" ] quiet = settings [ "quiet" ] error = settings [ "error" ] enhanced_errors = True the_shell = None if settings [ "no_enhanced_errors" ] : enhanced_errors = False if "shell" in settings : the_shell = settings [ "shell" ] windows_p = sys . platform == "win32" STDOUT = None STDERR = None if quiet : STDOUT = PIPE STDERR = PIPE commands = commands . rstrip ( ) sprint ( "About to run commands '{}'" . format ( commands ) , level = "verbose" ) if not quiet : sprint ( commands ) if the_shell : tmp = shlex . split ( the_shell ) the_shell = tmp [ 0 ] tmp = tmp [ 1 : ] if enhanced_errors and not windows_p : tmp . append ( "-e" ) tmp . append ( commands ) commands = tmp else : if enhanced_errors and not windows_p : commands = [ "-e" , commands ] p = Popen ( commands , shell = True , stdout = STDOUT , stderr = STDERR , executable = the_shell ) out , err = p . communicate ( ) if p . returncode : if quiet : error ( err . decode ( locale . getpreferredencoding ( ) ) ) error ( "Command failed to run" ) sys . exit ( 1 )
1366	def validateInterval ( self , startTime , endTime ) : start = int ( startTime ) end = int ( endTime ) if start > end : raise Exception ( "starttime is greater than endtime." )
832	def drawFile ( dataset , matrix , patterns , cells , w , fnum ) : score = 0 count = 0 assert len ( patterns ) == len ( cells ) for p in xrange ( len ( patterns ) - 1 ) : matrix [ p + 1 : , p ] = [ len ( set ( patterns [ p ] ) . intersection ( set ( q ) ) ) * 100 / w for q in patterns [ p + 1 : ] ] matrix [ p , p + 1 : ] = [ len ( set ( cells [ p ] ) . intersection ( set ( r ) ) ) * 5 / 2 for r in cells [ p + 1 : ] ] score += sum ( abs ( np . array ( matrix [ p + 1 : , p ] ) - np . array ( matrix [ p , p + 1 : ] ) ) ) count += len ( matrix [ p + 1 : , p ] ) print 'Score' , score / count fig = pyl . figure ( figsize = ( 10 , 10 ) , num = fnum ) pyl . matshow ( matrix , fignum = fnum ) pyl . colorbar ( ) pyl . title ( 'Coincidence Space' , verticalalignment = 'top' , fontsize = 12 ) pyl . xlabel ( 'The Mirror Image Visualization for ' + dataset , fontsize = 17 ) pyl . ylabel ( 'Encoding space' , fontsize = 12 )
12062	def stats_first ( abf ) : msg = "" for sweep in range ( abf . sweeps ) : for AP in abf . APs [ sweep ] : for key in sorted ( AP . keys ( ) ) : if key [ - 1 ] is "I" or key [ - 2 : ] in [ "I1" , "I2" ] : continue msg += "%s = %s\n" % ( key , AP [ key ] ) return msg
11909	def to_pattern_matrix ( D ) : result = np . zeros_like ( D ) # This is a cleverer way of doing # # for (u, v) in zip(*(D.nonzero())): # result[u, v] = 1 # result [ D . nonzero ( ) ] = 1 return result
2045	def set_storage_data ( self , storage_address , offset , value ) : self . _world_state [ storage_address ] [ 'storage' ] [ offset ] = value
11147	def is_repository_file ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) if relativePath == '' : return False , False , False , False relaDir , name = os . path . split ( relativePath ) fileOnDisk = os . path . isfile ( os . path . join ( self . __path , relativePath ) ) infoOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % name ) ) classOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileClass % name ) ) cDir = self . __repo [ 'walk_repo' ] if len ( relaDir ) : for dirname in relaDir . split ( os . sep ) : dList = [ d for d in cDir if isinstance ( d , dict ) ] if not len ( dList ) : cDir = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : cDir = None break cDir = cDict [ 0 ] [ dirname ] if cDir is None : return False , fileOnDisk , infoOnDisk , classOnDisk #if name not in cDir: if str ( name ) not in [ str ( i ) for i in cDir ] : return False , fileOnDisk , infoOnDisk , classOnDisk # this is a repository registered file. check whether all is on disk return True , fileOnDisk , infoOnDisk , classOnDisk
761	def modifyBits ( inputVal , maxChanges ) : changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] if changes == 0 : return inputVal inputWidth = len ( inputVal ) whatToChange = np . random . random_integers ( 0 , 41 , changes ) runningIndex = - 1 numModsDone = 0 for i in xrange ( inputWidth ) : if numModsDone >= changes : break if inputVal [ i ] == 1 : runningIndex += 1 if runningIndex in whatToChange : if i != 0 and inputVal [ i - 1 ] == 0 : inputVal [ i - 1 ] = 1 inputVal [ i ] = 0 return inputVal
9085	def update_backend ( use_pypi = False , index = 'dev' , build = True , user = None , version = None ) : get_vars ( ) if value_asbool ( build ) : upload_backend ( index = index , user = user ) with fab . cd ( '{apphome}' . format ( * * AV ) ) : if value_asbool ( use_pypi ) : command = 'bin/pip install --upgrade briefkasten' else : command = 'bin/pip install --upgrade --pre -i {ploy_default_publish_devpi}/briefkasten/{index}/+simple/ briefkasten' . format ( index = index , user = user , * * AV ) if version : command = '%s==%s' % ( command , version ) fab . sudo ( command ) briefkasten_ctl ( 'restart' )
7343	def clone_with_updates ( self , * * kwargs ) : fields_dict = self . to_dict ( ) fields_dict . update ( kwargs ) return BindingPrediction ( * * fields_dict )
11937	def create_message ( self , level , msg_text , extra_tags = '' , date = None , url = None ) : if not date : now = timezone . now ( ) else : now = date r = now . isoformat ( ) if now . microsecond : r = r [ : 23 ] + r [ 26 : ] if r . endswith ( '+00:00' ) : r = r [ : - 6 ] + 'Z' fingerprint = r + msg_text msg_id = hashlib . sha256 ( fingerprint . encode ( 'ascii' , 'ignore' ) ) . hexdigest ( ) return Message ( id = msg_id , message = msg_text , level = level , tags = extra_tags , date = r , url = url )
6452	def dist_abs ( self , src , tar ) : if tar == src : return 0 elif not src : return len ( tar ) elif not tar : return len ( src ) src_bag = Counter ( src ) tar_bag = Counter ( tar ) return max ( sum ( ( src_bag - tar_bag ) . values ( ) ) , sum ( ( tar_bag - src_bag ) . values ( ) ) , )
10203	def register_queries ( ) : return [ dict ( query_name = 'bucket-file-download-histogram' , query_class = ESDateHistogramQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) , required_filters = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) ) ) , dict ( query_name = 'bucket-file-download-total' , query_class = ESTermsQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( # bucket_id='bucket_id', ) , required_filters = dict ( bucket_id = 'bucket_id' , ) , aggregated_fields = [ 'file_key' ] ) ) , ]
2645	def App ( apptype , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . python import PythonApp from parsl . app . bash import BashApp logger . warning ( "The 'App' decorator will be deprecated in Parsl 0.8. Please use 'python_app' or 'bash_app' instead." ) if apptype == 'python' : app_class = PythonApp elif apptype == 'bash' : app_class = BashApp else : raise InvalidAppTypeError ( "Invalid apptype requested {}; must be 'python' or 'bash'" . format ( apptype ) ) def wrapper ( f ) : return app_class ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
3830	async def rename_conversation ( self , rename_conversation_request ) : response = hangouts_pb2 . RenameConversationResponse ( ) await self . _pb_request ( 'conversations/renameconversation' , rename_conversation_request , response ) return response
3058	def _write_credentials_file ( credentials_file , credentials ) : data = { 'file_version' : 2 , 'credentials' : { } } for key , credential in iteritems ( credentials ) : credential_json = credential . to_json ( ) encoded_credential = _helpers . _from_bytes ( base64 . b64encode ( _helpers . _to_bytes ( credential_json ) ) ) data [ 'credentials' ] [ key ] = encoded_credential credentials_file . seek ( 0 ) json . dump ( data , credentials_file ) credentials_file . truncate ( )
6668	def populate_fabfile ( ) : stack = inspect . stack ( ) fab_frame = None for frame_obj , script_fn , line , _ , _ , _ in stack : if 'fabfile.py' in script_fn : fab_frame = frame_obj break if not fab_frame : return try : locals_ = fab_frame . f_locals for module_name , module in sub_modules . items ( ) : locals_ [ module_name ] = module for role_name , role_func in role_commands . items ( ) : assert role_name not in sub_modules , ( 'The role %s conflicts with a built-in submodule. ' 'Please choose a different name.' ) % ( role_name ) locals_ [ role_name ] = role_func locals_ [ 'common' ] = common # Put all debug commands into the global namespace. # for _debug_name in debug.debug.get_tasks(): # print('_debug_name:', _debug_name) locals_ [ 'shell' ] = shell #debug.debug.shell # Put all virtual satchels in the global namespace so Fabric can find them. for _module_alias in common . post_import_modules : exec ( "import %s" % _module_alias ) # pylint: disable=exec-used locals_ [ _module_alias ] = locals ( ) [ _module_alias ] finally : del stack
8258	def sort_by_distance ( self , reversed = False ) : if len ( self ) == 0 : return ColorList ( ) # Find the darkest color in the list. root = self [ 0 ] for clr in self [ 1 : ] : if clr . brightness < root . brightness : root = clr # Remove the darkest color from the stack, # put it in the sorted list as starting element. stack = [ clr for clr in self ] stack . remove ( root ) sorted = [ root ] # Now find the color in the stack closest to that color. # Take this color from the stack and add it to the sorted list. # Now find the color closest to that color, etc. while len ( stack ) > 1 : closest , distance = stack [ 0 ] , stack [ 0 ] . distance ( sorted [ - 1 ] ) for clr in stack [ 1 : ] : d = clr . distance ( sorted [ - 1 ] ) if d < distance : closest , distance = clr , d stack . remove ( closest ) sorted . append ( closest ) sorted . append ( stack [ 0 ] ) if reversed : _list . reverse ( sorted ) return ColorList ( sorted )
11150	def get_text_fingerprint ( text , hash_meth , encoding = "utf-8" ) : # pragma: no cover m = hash_meth ( ) m . update ( text . encode ( encoding ) ) return m . hexdigest ( )
4887	def get_course_final_price ( self , mode , currency = '$' , enterprise_catalog_uuid = None ) : try : price_details = self . client . baskets . calculate . get ( sku = [ mode [ 'sku' ] ] , username = self . user . username , catalog = enterprise_catalog_uuid , ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to get price details for sku %s due to: %s' , mode [ 'sku' ] , str ( exc ) ) price_details = { } price = price_details . get ( 'total_incl_tax' , mode [ 'min_price' ] ) if price != mode [ 'min_price' ] : return format_price ( price , currency ) return mode [ 'original_price' ]
4787	def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
8074	def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , * * kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , * * kwargs )
4215	def name ( cls ) : parent , sep , mod_name = cls . __module__ . rpartition ( '.' ) mod_name = mod_name . replace ( '_' , ' ' ) return ' ' . join ( [ mod_name , cls . __name__ ] )
10498	def tripleClickMouse ( self , coord ) : # Note above re: double-clicks applies to triple-clicks modFlags = 0 for i in range ( 2 ) : self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 3 ) self . _postQueuedEvents ( )
7130	def create_log_config ( verbose , quiet ) : if verbose and quiet : raise ValueError ( "Supplying both --quiet and --verbose makes no sense." ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger_cfg = { "handlers" : [ "click_handler" ] , "level" : level } return { "version" : 1 , "formatters" : { "click_formatter" : { "format" : "%(message)s" } } , "handlers" : { "click_handler" : { "level" : level , "class" : "doc2dash.__main__.ClickEchoHandler" , "formatter" : "click_formatter" , } } , "loggers" : { "doc2dash" : logger_cfg , "__main__" : logger_cfg } , }
11807	def encode ( plaintext , code ) : from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
12719	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
3402	def is_boundary_type ( reaction , boundary_type , external_compartment ) : # Check if the reaction has an annotation. Annotations dominate everything. sbo_term = reaction . annotation . get ( "sbo" , "" ) if isinstance ( sbo_term , list ) : sbo_term = sbo_term [ 0 ] sbo_term = sbo_term . upper ( ) if sbo_term == sbo_terms [ boundary_type ] : return True if sbo_term in [ sbo_terms [ k ] for k in sbo_terms if k != boundary_type ] : return False # Check if the reaction is in the correct compartment (exterior or inside) correct_compartment = external_compartment in reaction . compartments if boundary_type != "exchange" : correct_compartment = not correct_compartment # Check if the reaction has the correct reversibility rev_type = True if boundary_type == "demand" : rev_type = not reaction . reversibility elif boundary_type == "sink" : rev_type = reaction . reversibility return ( reaction . boundary and not any ( ex in reaction . id for ex in excludes [ boundary_type ] ) and correct_compartment and rev_type )
13910	def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
7641	def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output. Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
7809	def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : # PyPy doesn't have .getpeercert data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
1187	def unexpo ( intpart , fraction , expo ) : if expo > 0 : # Move the point left f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : # Move the point right i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
10500	def waitForCreation ( self , timeout = 10 , notification = 'AXCreated' ) : callback = AXCallbacks . returnElemCallback retelem = None args = ( retelem , ) return self . waitFor ( timeout , notification , callback = callback , args = args )
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : # pylint: disable=len-as-condition return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
1643	def CheckSectionSpacing ( filename , clean_lines , class_info , linenum , error ) : # Skip checks if the class is small, where small means 25 lines or less. # 25 lines seems like a good cutoff since that's the usual height of # terminals, and any class that can't fit in one screen can't really # be considered "small". # # Also skip checks if we are on the first line. This accounts for # classes that look like # class Foo { public: ... }; # # If we didn't find the end of the class, last_line would be zero, # and the check will be skipped by the first condition. if ( class_info . last_line - class_info . starting_linenum <= 24 or linenum <= class_info . starting_linenum ) : return matched = Match ( r'\s*(public|protected|private):' , clean_lines . lines [ linenum ] ) if matched : # Issue warning if the line before public/protected/private was # not a blank line, but don't do this if the previous line contains # "class" or "struct". This can happen two ways: # - We are at the beginning of the class. # - We are forward-declaring an inner class that is semantically # private, but needed to be public for implementation reasons. # Also ignores cases where the previous line ends with a backslash as can be # common when defining classes in C macros. prev_line = clean_lines . lines [ linenum - 1 ] if ( not IsBlankLine ( prev_line ) and not Search ( r'\b(class|struct)\b' , prev_line ) and not Search ( r'\\$' , prev_line ) ) : # Try a bit harder to find the beginning of the class. This is to # account for multi-line base-specifier lists, e.g.: # class Derived # : public Base { end_class_head = class_info . starting_linenum for i in range ( class_info . starting_linenum , linenum ) : if Search ( r'\{\s*$' , clean_lines . lines [ i ] ) : end_class_head = i break if end_class_head < linenum - 1 : error ( filename , linenum , 'whitespace/blank_line' , 3 , '"%s:" should be preceded by a blank line' % matched . group ( 1 ) )
12562	def get_unique_nonzeros ( arr ) : rois = np . unique ( arr ) rois = rois [ np . nonzero ( rois ) ] rois . sort ( ) return rois
6777	def iter_dict_differences ( a , b ) : common_keys = set ( a ) . union ( b ) for k in common_keys : a_value = a . get ( k ) b_value = b . get ( k ) if a_value != b_value : yield k , ( a_value , b_value )
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) # [{'LocalDateTime': '20160824161431.977000+480'}]' self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) # '20160824161431' self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type: # datetime.datetime return self . current_time_format
6350	def _pnums_with_leading_space ( self , phonetic ) : alt_start = phonetic . find ( '(' ) if alt_start == - 1 : return ' ' + self . _phonetic_number ( phonetic ) prefix = phonetic [ : alt_start ] alt_start += 1 # get past the ( alt_end = phonetic . find ( ')' , alt_start ) alt_string = phonetic [ alt_start : alt_end ] alt_end += 1 # get past the ) suffix = phonetic [ alt_end : ] alt_array = alt_string . split ( '|' ) result = '' for alt in alt_array : result += self . _pnums_with_leading_space ( prefix + alt + suffix ) return result
7704	def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if replace : self . remove_item ( item . jid ) else : raise ValueError ( "JID already in the roster" ) index = len ( self . _items ) self . _items . append ( item ) self . _jids [ item . jid ] = index
1530	def monitor ( self ) : def trigger_watches_based_on_files ( watchers , path , directory , ProtoClass ) : """ For all the topologies in the watchers, check if the data in directory has changed. Trigger the callback if it has. """ for topology , callbacks in watchers . items ( ) : file_path = os . path . join ( path , topology ) data = "" if os . path . exists ( file_path ) : with open ( os . path . join ( path , topology ) ) as f : data = f . read ( ) if topology not in directory or data != directory [ topology ] : proto_object = ProtoClass ( ) proto_object . ParseFromString ( data ) for callback in callbacks : callback ( proto_object ) directory [ topology ] = data while not self . monitoring_thread_stop_signal : topologies_path = self . get_topologies_path ( ) topologies = [ ] if os . path . isdir ( topologies_path ) : topologies = list ( filter ( lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , os . listdir ( topologies_path ) ) ) if set ( topologies ) != set ( self . topologies_directory ) : for callback in self . topologies_watchers : callback ( topologies ) self . topologies_directory = topologies trigger_watches_based_on_files ( self . topology_watchers , topologies_path , self . topologies_directory , Topology ) # Get the directory name for execution state execution_state_path = os . path . dirname ( self . get_execution_state_path ( "" ) ) trigger_watches_based_on_files ( self . execution_state_watchers , execution_state_path , self . execution_state_directory , ExecutionState ) # Get the directory name for packing_plan packing_plan_path = os . path . dirname ( self . get_packing_plan_path ( "" ) ) trigger_watches_based_on_files ( self . packing_plan_watchers , packing_plan_path , self . packing_plan_directory , PackingPlan ) # Get the directory name for pplan pplan_path = os . path . dirname ( self . get_pplan_path ( "" ) ) trigger_watches_based_on_files ( self . pplan_watchers , pplan_path , self . pplan_directory , PhysicalPlan ) # Get the directory name for tmaster tmaster_path = os . path . dirname ( self . get_tmaster_path ( "" ) ) trigger_watches_based_on_files ( self . tmaster_watchers , tmaster_path , self . tmaster_directory , TMasterLocation ) # Get the directory name for scheduler location scheduler_location_path = os . path . dirname ( self . get_scheduler_location_path ( "" ) ) trigger_watches_based_on_files ( self . scheduler_location_watchers , scheduler_location_path , self . scheduler_location_directory , SchedulerLocation ) # Sleep for some time self . event . wait ( timeout = 5 )
12582	def get_3D_from_4D ( filename , vol_idx = 0 ) : def remove_4th_element_from_hdr_string ( hdr , fieldname ) : if fieldname in hdr : hdr [ fieldname ] = ' ' . join ( hdr [ fieldname ] . split ( ) [ : 3 ] ) vol , hdr = load_raw_data_with_mhd ( filename ) if vol . ndim != 4 : raise ValueError ( 'Volume in {} does not have 4 dimensions.' . format ( op . join ( op . dirname ( filename ) , hdr [ 'ElementDataFile' ] ) ) ) if not 0 <= vol_idx < vol . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, not {}.' . format ( filename , vol . shape [ 3 ] , vol_idx ) ) new_vol = vol [ : , : , : , vol_idx ] . copy ( ) hdr [ 'NDims' ] = 3 remove_4th_element_from_hdr_string ( hdr , 'ElementSpacing' ) remove_4th_element_from_hdr_string ( hdr , 'DimSize' ) return new_vol , hdr
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
8299	def decodeOSC ( data ) : table = { "i" : readInt , "f" : readFloat , "s" : readString , "b" : readBlob } decoded = [ ] address , rest = readString ( data ) typetags = "" if address == "#bundle" : time , rest = readLong ( rest ) # decoded.append(address) # decoded.append(time) while len ( rest ) > 0 : length , rest = readInt ( rest ) decoded . append ( decodeOSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = readString ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
11373	def return_letters_from_string ( text ) : out = "" for letter in text : if letter . isalpha ( ) : out += letter return out
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) # Zero crossing lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
6499	def search ( self , query_string = None , field_dictionary = None , filter_dictionary = None , exclude_dictionary = None , facet_terms = None , exclude_ids = None , use_field_match = False , * * kwargs ) : # pylint: disable=too-many-arguments, too-many-locals, too-many-branches, arguments-differ log . debug ( "searching index with %s" , query_string ) elastic_queries = [ ] elastic_filters = [ ] # We have a query string, search all fields for matching text within the "content" node if query_string : if six . PY2 : query_string = query_string . encode ( 'utf-8' ) . translate ( None , RESERVED_CHARACTERS ) else : query_string = query_string . translate ( query_string . maketrans ( '' , '' , RESERVED_CHARACTERS ) ) elastic_queries . append ( { "query_string" : { "fields" : [ "content.*" ] , "query" : query_string } } ) if field_dictionary : if use_field_match : elastic_queries . extend ( _process_field_queries ( field_dictionary ) ) else : elastic_filters . extend ( _process_field_filters ( field_dictionary ) ) if filter_dictionary : elastic_filters . extend ( _process_filters ( filter_dictionary ) ) # Support deprecated argument of exclude_ids if exclude_ids : if not exclude_dictionary : exclude_dictionary = { } if "_id" not in exclude_dictionary : exclude_dictionary [ "_id" ] = [ ] exclude_dictionary [ "_id" ] . extend ( exclude_ids ) if exclude_dictionary : elastic_filters . append ( _process_exclude_dictionary ( exclude_dictionary ) ) query_segment = { "match_all" : { } } if elastic_queries : query_segment = { "bool" : { "must" : elastic_queries } } query = query_segment if elastic_filters : filter_segment = { "bool" : { "must" : elastic_filters } } query = { "filtered" : { "query" : query_segment , "filter" : filter_segment , } } body = { "query" : query } if facet_terms : facet_query = _process_facet_terms ( facet_terms ) if facet_query : body [ "facets" ] = facet_query try : es_response = self . _es . search ( index = self . index_name , body = body , * * kwargs ) except exceptions . ElasticsearchException as ex : message = six . text_type ( ex ) if 'QueryParsingException' in message : log . exception ( "Malformed search query: %s" , message ) raise QueryParseError ( 'Malformed search query.' ) else : # log information and re-raise log . exception ( "error while searching index - %s" , str ( message ) ) raise return _translate_hits ( es_response )
12211	def invalidate_cache ( user , size = None ) : sizes = set ( AUTO_GENERATE_AVATAR_SIZES ) if size is not None : sizes . add ( size ) for prefix in cached_funcs : for size in sizes : cache . delete ( get_cache_key ( user , size , prefix ) )
2803	def convert_slice ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting slice ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert slice by multiple dimensions' ) if params [ 'axes' ] [ 0 ] not in [ 0 , 1 , 2 , 3 ] : raise AssertionError ( 'Slice by dimension more than 3 or less than 0 is not supported' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) , start = int ( params [ 'starts' ] [ 0 ] ) , end = int ( params [ 'ends' ] [ 0 ] ) ) : if axis == 0 : return x [ start : end ] elif axis == 1 : return x [ : , start : end ] elif axis == 2 : return x [ : , : , start : end ] elif axis == 3 : return x [ : , : , : , start : end ] lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
9265	def fetch_and_filter_tags ( self ) : self . all_tags = self . fetcher . get_all_tags ( ) self . filtered_tags = self . get_filtered_tags ( self . all_tags ) self . fetch_tags_dates ( )
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack # Iterate over the bytecode. for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect # Re-check the stack. _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
4331	def loudness ( self , gain_db = - 10.0 , reference_level = 65.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'gain_db must be a number.' ) if not is_number ( reference_level ) : raise ValueError ( 'reference_level must be a number' ) if reference_level > 75 or reference_level < 50 : raise ValueError ( 'reference_level must be between 50 and 75' ) effect_args = [ 'loudness' , '{:f}' . format ( gain_db ) , '{:f}' . format ( reference_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'loudness' ) return self
13439	def lock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : return False else : with open ( lockfile , "w" ) : pass return True
914	def lscsum ( lx , epsilon = None ) : lx = numpy . asarray ( lx ) base = lx . max ( ) # If the input is the log of 0's, catch this condition before we generate # an exception, and return the log(0) if numpy . isinf ( base ) : return base # If the user specified an epsilon and we are below it, return epsilon if ( epsilon is not None ) and ( base < epsilon ) : return epsilon x = numpy . exp ( lx - base ) ssum = x . sum ( ) result = numpy . log ( ssum ) + base # try: # conventional = numpy.log(numpy.exp(lx).sum()) # if not similar(result, conventional): # if numpy.isinf(conventional).any() and not numpy.isinf(result).any(): # # print "Scaled log sum avoided underflow or overflow." # pass # else: # import sys # print >>sys.stderr, "Warning: scaled log sum did not match." # print >>sys.stderr, "Scaled log result:" # print >>sys.stderr, result # print >>sys.stderr, "Conventional result:" # print >>sys.stderr, conventional # except FloatingPointError, e: # # print "Scaled log sum avoided underflow or overflow." # pass return result
5148	def write ( self , name , path = './' ) : byte_object = self . generate ( ) file_name = '{0}.tar.gz' . format ( name ) if not path . endswith ( '/' ) : path += '/' f = open ( '{0}{1}' . format ( path , file_name ) , 'wb' ) f . write ( byte_object . getvalue ( ) ) f . close ( )
504	def _categoryToLabelList ( self , category ) : if category is None : return [ ] labelList = [ ] labelNum = 0 while category > 0 : if category % 2 == 1 : labelList . append ( self . saved_categories [ labelNum ] ) labelNum += 1 category = category >> 1 return labelList
8576	def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response = self . _perform_request ( '/requests/%s' % request_id ) return response
1979	def wait ( self , readfds , writefds , timeout ) : logger . info ( "WAIT:" ) logger . info ( "\tProcess %d is going to wait for [ %r %r %r ]" , self . _current , readfds , writefds , timeout ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout else : self . timers [ self . _current ] = None procid = self . _current # self.sched() next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . info ( "\tTransfer control from process %d to %d" , procid , self . _current ) logger . info ( "\tREMOVING %r from %r. Current: %r" , procid , self . running , self . _current ) self . running . remove ( procid ) if self . _current not in self . running : logger . info ( "\tCurrent not running. Checking for timers..." ) self . _current = None if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . check_timers ( )
599	def computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) : nActiveColumns = len ( activeColumns ) if nActiveColumns > 0 : # Test whether each element of a 1-D array is also present in a second # array. Sum to get the total # of columns that are active and were # predicted. score = numpy . in1d ( activeColumns , prevPredictedColumns ) . sum ( ) # Get the percent of active columns that were NOT predicted, that is # our anomaly score. score = ( nActiveColumns - score ) / float ( nActiveColumns ) else : # There are no active columns. score = 0.0 return score
478	def word_to_id ( self , word ) : if word in self . _vocab : return self . _vocab [ word ] else : return self . _unk_id
11457	def match ( self , query = None , * * kwargs ) : from invenio . search_engine import perform_request_search if not query : # We use default setup recid = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = "035:%s" % ( recid , ) , of = "id" ) else : if "recid" not in kwargs : kwargs [ "recid" ] = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = query % kwargs , of = "id" )
10737	def path_from_keywords ( keywords , into = 'path' ) : subdirs = [ ] def prepare_string ( s ) : s = str ( s ) s = re . sub ( '[][{},*"' + f"'{os.sep}]" , '_' , s ) #replace characters that make bash life difficult by underscore if into == 'file' : s = s . replace ( '_' , ' ' ) #Remove underscore because they will be used as separator if ' ' in s : s = s . title ( ) s = s . replace ( ' ' , '' ) return s if isinstance ( keywords , set ) : keywords_list = sorted ( keywords ) for property in keywords_list : subdirs . append ( prepare_string ( property ) ) else : keywords_list = sorted ( keywords . items ( ) ) for property , value in keywords_list : # @reservedassignment if Bool . valid ( value ) : subdirs . append ( ( '' if value else ( 'not_' if into == 'path' else 'not' ) ) + prepare_string ( property ) ) #elif String.valid(value): # subdirs.append(prepare_string(value)) elif ( Float | Integer ) . valid ( value ) : subdirs . append ( '{}{}' . format ( prepare_string ( property ) , prepare_string ( value ) ) ) else : subdirs . append ( '{}{}{}' . format ( prepare_string ( property ) , '_' if into == 'path' else '' , prepare_string ( value ) ) ) if into == 'path' : out = os . path . join ( * subdirs ) else : out = '_' . join ( subdirs ) return out
11351	def merge_from_list ( self , list_args ) : def xs ( name , parser_args , list_args ) : """build the generator of matching list_args""" for args , kwargs in list_args : if len ( set ( args ) & parser_args ) > 0 : yield args , kwargs else : if 'dest' in kwargs : if kwargs [ 'dest' ] == name : yield args , kwargs for args , kwargs in xs ( self . name , self . parser_args , list_args ) : self . merge_args ( args ) self . merge_kwargs ( kwargs )
3009	def has_credentials ( self ) : credentials = _credentials_from_request ( self . request ) return ( credentials and not credentials . invalid and credentials . has_scopes ( self . _get_scopes ( ) ) )
480	def id_to_word ( self , word_id ) : if word_id >= len ( self . reverse_vocab ) : return self . reverse_vocab [ self . unk_id ] else : return self . reverse_vocab [ word_id ]
11881	def scanProcessForMapping ( pid , searchPortion , isExactMatch = False , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e with open ( '/proc/%d/maps' % ( pid , ) , 'r' ) as f : contents = f . read ( ) lines = contents . split ( '\n' ) matchedMappings = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , searchIn : bool ( searchFor == searchIn ) else : isMatch = lambda searchFor , searchIn : bool ( searchFor . lower ( ) == searchIn . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , searchIn : bool ( searchFor in searchIn ) else : isMatch = lambda searchFor , searchIn : bool ( searchFor . lower ( ) in searchIn . lower ( ) ) for line in lines : portion = ' ' . join ( line . split ( ' ' ) [ 5 : ] ) . lstrip ( ) if isMatch ( searchPortion , portion ) : matchedMappings . append ( '\t' + line ) if len ( matchedMappings ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'matchedMappings' : matchedMappings , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
5248	def custom_req ( session , request ) : # flush event queue in case previous call errored out while ( session . tryNextEvent ( ) ) : pass print ( "Sending Request:\n %s" % request ) session . sendRequest ( request ) messages = [ ] # Process received events while ( True ) : # We provide timeout to give the chance for Ctrl+C handling: ev = session . nextEvent ( 500 ) for msg in ev : print ( "Message Received:\n %s" % msg ) messages . append ( msg ) if ev . eventType ( ) == blpapi . Event . RESPONSE : # Response completely received, so we could exit break return messages
13798	def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : # All input data are lines of JSON like the following: # ["<cmd_name>" "<cmd_arg1>" "<cmd_arg2>" ...] # So I handle this by dispatching to various methods. cmd = json . loads ( line ) except Exception , exc : # Sometimes errors come up. Once again, I can't predict # anything, but can at least tell CouchDB about the error. self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : # Automagically get the command handler. handler = getattr ( self , 'handle_' + cmd [ 0 ] , None ) if not handler : # We are ready to not find commands. It probably won't # happen, but fortune favours the prepared. self . wfile . write ( repr ( CommandNotFound ( cmd [ 0 ] ) ) + NEWLINE ) continue return_value = handler ( * cmd [ 1 : ] ) if not return_value : continue # We write the output back to CouchDB. self . wfile . write ( one_lineify ( json . dumps ( return_value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue
9293	def db_value ( self , value ) : value = self . transform_value ( value ) return self . hhash . encrypt ( value , salt_size = self . salt_size , rounds = self . rounds )
417	def find_datasets ( self , dataset_name = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) pc = self . db . Dataset . find ( kwargs ) if pc is not None : dataset_id_list = pc . distinct ( 'dataset_id' ) dataset_list = [ ] for dataset_id in dataset_id_list : # you may have multiple Buckets files tmp = self . dataset_fs . get ( dataset_id ) . read ( ) dataset_list . append ( self . _deserialization ( tmp ) ) else : print ( "[Database] FAIL! Cannot find any dataset: {}" . format ( kwargs ) ) return False print ( "[Database] Find {} datasets SUCCESS, took: {}s" . format ( len ( dataset_list ) , round ( time . time ( ) - s , 2 ) ) ) return dataset_list
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
4569	def dump ( data , file = sys . stdout , use_yaml = None , * * kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , * * kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , * * kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
11783	def add_example ( self , example ) : self . check_example ( example ) self . examples . append ( example )
56	def deepcopy ( self , keypoints = None , shape = None ) : # for some reason deepcopy is way slower here than manual copy if keypoints is None : keypoints = [ kp . deepcopy ( ) for kp in self . keypoints ] if shape is None : shape = tuple ( self . shape ) return KeypointsOnImage ( keypoints , shape )
3465	def flux ( self ) : try : check_solver_status ( self . _model . solver . status ) return self . forward_variable . primal - self . reverse_variable . primal except AttributeError : raise RuntimeError ( "reaction '{}' is not part of a model" . format ( self . id ) ) # Due to below all-catch, which sucks, need to reraise these. except ( RuntimeError , OptimizationError ) as err : raise_with_traceback ( err ) # Would love to catch CplexSolverError and GurobiError here. except Exception as err : raise_from ( OptimizationError ( "Likely no solution exists. Original solver message: {}." "" . format ( str ( err ) ) ) , err )
12468	def save_traceback ( err ) : # Store logs to ~/.bootstrapper directory dirname = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) ) ) ) # But ensure that directory exists if not os . path . isdir ( dirname ) : os . mkdir ( dirname ) # Now we ready to put traceback to log file filename = os . path . join ( dirname , '{0}.log' . format ( __script__ ) ) with open ( filename , 'a+' ) as handler : traceback . print_exc ( file = handler ) # And show colorized message message = ( 'User aborted workflow' if isinstance ( err , KeyboardInterrupt ) else 'Unexpected error catched' ) print_error ( message ) print_error ( 'Full log stored to {0}' . format ( filename ) , False ) return True
13123	def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
275	def sample_colormap ( cmap_name , n_samples ) : colors = [ ] colormap = cm . cmap_d [ cmap_name ] for i in np . linspace ( 0 , 1 , n_samples ) : colors . append ( colormap ( i ) ) return colors
6007	def load_image ( image_path , image_hdu , pixel_scale ) : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = image_path , hdu = image_hdu , pixel_scale = pixel_scale )
2588	def get_data_manager ( cls ) : from parsl . dataflow . dflow import DataFlowKernelLoader dfk = DataFlowKernelLoader . dfk ( ) return dfk . executors [ 'data_manager' ]
13856	def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
2739	def get_object ( cls , api_token , firewall_id ) : firewall = cls ( token = api_token , id = firewall_id ) firewall . load ( ) return firewall
1151	def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except NameError : unicodetype = ( ) try : message = str ( message ) except UnicodeEncodeError : pass s = "%s: %s: %s\n" % ( lineno , category . __name__ , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += " %s\n" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except UnicodeDecodeError : pass s = "%s:%s" % ( filename , s ) return s
8487	def init ( self , hosts = None , cacert = None , client_cert = None , client_key = None ) : # Try to get the etcd module try : import etcd self . module = etcd except ImportError : pass if not self . module : return self . _parse_jetconfig ( ) # Check env for overriding configuration or pyconfig setting hosts = env ( 'PYCONFIG_ETCD_HOSTS' , hosts ) protocol = env ( 'PYCONFIG_ETCD_PROTOCOL' , None ) cacert = env ( 'PYCONFIG_ETCD_CACERT' , cacert ) client_cert = env ( 'PYCONFIG_ETCD_CERT' , client_cert ) client_key = env ( 'PYCONFIG_ETCD_KEY' , client_key ) # Parse auth string if there is one username = None password = None auth = env ( 'PYCONFIG_ETCD_AUTH' , None ) if auth : auth = auth . split ( ':' ) auth . append ( '' ) username = auth [ 0 ] password = auth [ 1 ] # Create new etcd instance hosts = self . _parse_hosts ( hosts ) if hosts is None : return kw = { } # Need this when passing a list of hosts to python-etcd, which we # always do, even if it's a list of one kw [ 'allow_reconnect' ] = True # Grab optional protocol argument if protocol : kw [ 'protocol' ] = protocol # Add auth to constructor if we got it if username : kw [ 'username' ] = username if password : kw [ 'password' ] = password # Assign the SSL args if we have 'em if cacert : kw [ 'ca_cert' ] = os . path . abspath ( cacert ) if client_cert and client_key : kw [ 'cert' ] = ( ( os . path . abspath ( client_cert ) , os . path . abspath ( client_key ) ) ) elif client_cert : kw [ 'cert' ] = os . path . abspath ( client_cert ) if cacert or client_cert or client_key : kw [ 'protocol' ] = 'https' self . client = self . module . Client ( hosts , * * kw )
13749	def many_to_one ( clsname , * * kw ) : @ declared_attr def m2o ( cls ) : cls . _references ( ( cls . __name__ , clsname ) ) return relationship ( clsname , * * kw ) return m2o
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
9775	def logs ( ctx , past , follow , hide_time ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if past : try : response = PolyaxonClient ( ) . job . logs ( user , project_name , _job , stream = False ) get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . job . logs ( user , project_name , _job , message_handler = get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
12105	def _qsub_block ( self , output_dir , error_dir , tid_specs ) : processes = [ ] job_names = [ ] for ( tid , spec ) in tid_specs : job_name = "%s_%s_tid_%d" % ( self . batch_name , self . job_timestamp , tid ) job_names . append ( job_name ) cmd_args = self . command ( self . command . _formatter ( spec ) , tid , self . _launchinfo ) popen_args = self . _qsub_args ( [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) ] , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) processes . append ( p ) self . message ( "Invoked qsub for %d commands" % len ( processes ) ) if ( self . reduction_fn is not None ) or self . dynamic : self . _qsub_collate_and_launch ( output_dir , error_dir , job_names )
13757	def get_path_extension ( path ) : file_path , file_ext = os . path . splitext ( path ) return file_ext . lstrip ( '.' )
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : # conditionally turn off autoescaping for .txt extensions in format if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
7461	def save_json ( data ) : ## data as dict #### skip _ipcluster because it's made new #### skip _headers because it's loaded new #### statsfiles save only keys #### samples save only keys datadict = OrderedDict ( [ ( "_version" , data . __dict__ [ "_version" ] ) , ( "_checkpoint" , data . __dict__ [ "_checkpoint" ] ) , ( "name" , data . __dict__ [ "name" ] ) , ( "dirs" , data . __dict__ [ "dirs" ] ) , ( "paramsdict" , data . __dict__ [ "paramsdict" ] ) , ( "samples" , data . __dict__ [ "samples" ] . keys ( ) ) , ( "populations" , data . __dict__ [ "populations" ] ) , ( "database" , data . __dict__ [ "database" ] ) , ( "clust_database" , data . __dict__ [ "clust_database" ] ) , ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "barcodes" , data . __dict__ [ "barcodes" ] ) , ( "stats_files" , data . __dict__ [ "stats_files" ] ) , ( "_hackersonly" , data . __dict__ [ "_hackersonly" ] ) , ] ) ## sample dict sampledict = OrderedDict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . _to_fulldict ( ) ## json format it using cumstom Encoder class fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
12558	def drain_rois ( img ) : img_data = get_img_data ( img ) out = np . zeros ( img_data . shape , dtype = img_data . dtype ) krn_dim = [ 3 ] * img_data . ndim kernel = np . ones ( krn_dim , dtype = int ) vals = np . unique ( img_data ) vals = vals [ vals != 0 ] for i in vals : roi = img_data == i hits = scn . binary_hit_or_miss ( roi , kernel ) roi [ hits ] = 0 out [ roi > 0 ] = i return out
5281	def make_tpot_pmml_config ( config , user_classpath = [ ] ) : tpot_keys = set ( config . keys ( ) ) classes = _supported_classes ( user_classpath ) pmml_keys = ( set ( classes ) ) . union ( set ( [ _strip_module ( class_ ) for class_ in classes ] ) ) return { key : config [ key ] for key in ( tpot_keys ) . intersection ( pmml_keys ) }
2289	def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , * * kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
6381	def dist_jaro_winkler ( src , tar , qval = 1 , mode = 'winkler' , long_strings = False , boost_threshold = 0.7 , scaling_factor = 0.1 , ) : return JaroWinkler ( ) . dist ( src , tar , qval , mode , long_strings , boost_threshold , scaling_factor )
10561	def _mutagen_fields_to_single_value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
13109	def r_annotations ( self ) : target = request . args . get ( "target" , None ) wildcard = request . args . get ( "wildcard" , "." , type = str ) include = request . args . get ( "include" ) exclude = request . args . get ( "exclude" ) limit = request . args . get ( "limit" , None , type = int ) start = request . args . get ( "start" , 1 , type = int ) expand = request . args . get ( "expand" , False , type = bool ) if target : try : urn = MyCapytain . common . reference . URN ( target ) except ValueError : return "invalid urn" , 400 count , annotations = self . __queryinterface__ . getAnnotations ( urn , wildcard = wildcard , include = include , exclude = exclude , limit = limit , start = start , expand = expand ) else : # Note that this implementation is not done for too much annotations # because we do not implement pagination here count , annotations = self . __queryinterface__ . getAnnotations ( None , limit = limit , start = start , expand = expand ) mapped = [ ] response = { "@context" : type ( self ) . JSONLD_CONTEXT , "id" : url_for ( ".r_annotations" , start = start , limit = limit ) , "type" : "AnnotationCollection" , "startIndex" : start , "items" : [ ] , "total" : count } for a in annotations : mapped . append ( { "id" : url_for ( ".r_annotation" , sha = a . sha ) , "body" : url_for ( ".r_annotation_body" , sha = a . sha ) , "type" : "Annotation" , "target" : a . target . to_json ( ) , "dc:type" : a . type_uri , "owl:sameAs" : [ a . uri ] , "nemo:slug" : a . slug } ) response [ "items" ] = mapped response = jsonify ( response ) return response
10629	def HHV ( self , HHV ) : self . _HHV = HHV # MJ/kg coal if self . isCoal : self . _DH298 = self . _calculate_DH298_coal ( )
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , * * kw ) : return assertion . test ( subject , expected , * args , * * kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators # Register operator Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
11848	def things_near ( self , location , radius = None ) : if radius is None : radius = self . perceptible_distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
11723	def init_app ( self , app , * * kwargs ) : # Init the configuration self . init_config ( app ) # Enable Rate limiter self . limiter = Limiter ( app , key_func = get_ipaddr ) # Enable secure HTTP headers if app . config [ 'APP_ENABLE_SECURE_HEADERS' ] : self . talisman = Talisman ( app , * * app . config . get ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) ) # Enable PING view if app . config [ 'APP_HEALTH_BLUEPRINT_ENABLED' ] : blueprint = Blueprint ( 'invenio_app_ping' , __name__ ) @ blueprint . route ( '/ping' ) def ping ( ) : """Load balancer ping view.""" return 'OK' ping . talisman_view_options = { 'force_https' : False } app . register_blueprint ( blueprint ) requestid_header = app . config . get ( 'APP_REQUESTID_HEADER' ) if requestid_header : @ app . before_request def set_request_id ( ) : """Extracts a request id from an HTTP header.""" request_id = request . headers . get ( requestid_header ) if request_id : # Capped at 200 to protect against malicious clients # sending very large headers. g . request_id = request_id [ : 200 ] # If installed register the Flask-DebugToolbar extension try : from flask_debugtoolbar import DebugToolbarExtension app . extensions [ 'flask-debugtoolbar' ] = DebugToolbarExtension ( app ) except ImportError : app . logger . debug ( 'Flask-DebugToolbar extension not installed.' ) # Register self app . extensions [ 'invenio-app' ] = self
8612	def list_volumes ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/volumes?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
7782	def update_state ( self ) : self . _lock . acquire ( ) try : now = datetime . utcnow ( ) if self . state == 'new' : self . state = 'fresh' if self . state == 'fresh' : if now > self . freshness_time : self . state = 'old' if self . state == 'old' : if now > self . expire_time : self . state = 'stale' if self . state == 'stale' : if now > self . purge_time : self . state = 'purged' self . state_value = _state_values [ self . state ] return self . state finally : self . _lock . release ( )
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
12903	def _parse_genotype ( self , vcf_fields ) : format_col = vcf_fields [ 8 ] . split ( ':' ) genome_data = vcf_fields [ 9 ] . split ( ':' ) try : gt_idx = format_col . index ( 'GT' ) except ValueError : return [ ] return [ int ( x ) for x in re . split ( r'[\|/]' , genome_data [ gt_idx ] ) if x != '.' ]
3386	def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
13213	def rename ( self , from_name , to_name ) : log . info ( 'renaming database from %s to %s' % ( from_name , to_name ) ) self . _run_stmt ( 'alter database %s rename to %s' % ( from_name , to_name ) )
13015	def match ( self , uri ) : absolute_uri = self . __absolute__ ( uri ) return absolute_uri . startswith ( self . __path__ ) and op . exists ( absolute_uri )
2150	def delete ( self , pk = None , fail_on_missing = False , * * kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . delete ( pk = pk , fail_on_missing = fail_on_missing , * * kwargs )
2813	def convert_unsqueeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting unsqueeze ...' ) if names == 'short' : tf_name = 'UNSQ' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
5591	def tiles_from_bounds ( self , bounds , zoom ) : for tile in self . tiles_from_bbox ( box ( * bounds ) , zoom ) : yield self . tile ( * tile . id )
1988	def load_state ( self , key , delete = True ) : with self . load_stream ( key , binary = True ) as f : state = self . _serializer . deserialize ( f ) if delete : self . rm ( key ) return state
2033	def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 32 ) self . _store ( address , value , 32 )
7695	def timeout_handler ( interval , recurring = None ) : def decorator ( func ) : """The decorator""" func . _pyxmpp_timeout = interval func . _pyxmpp_recurring = recurring return func return decorator
2543	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True self . file ( doc ) . license_comment = text return True else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
5880	def get_siblings_content ( self , current_sibling , baselinescore_siblings_para ) : if current_sibling . tag == 'p' and self . parser . getText ( current_sibling ) : tmp = current_sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential_paragraphs = self . parser . getElementsByTag ( current_sibling , tag = 'p' ) if potential_paragraphs is None : return None paragraphs = list ( ) for first_paragraph in potential_paragraphs : text = self . parser . getText ( first_paragraph ) if text : # no len(text) > 0 word_stats = self . stopwords_class ( language = self . get_language ( ) ) . get_stopword_count ( text ) paragraph_score = word_stats . get_stopword_count ( ) sibling_baseline_score = float ( .30 ) high_link_density = self . is_highlink_density ( first_paragraph ) score = float ( baselinescore_siblings_para * sibling_baseline_score ) if score < paragraph_score and not high_link_density : para = self . parser . createElement ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
5559	def _unflatten_tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) # we are at the end of a branch if len ( path ) == 1 : tree [ key ] = value # there are more branches else : # create new dict if not path [ 0 ] in tree : tree [ path [ 0 ] ] = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) # add keys to existing dict else : branch = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
10903	def examine_unexplained_noise ( state , bins = 1000 , xlim = ( - 10 , 10 ) ) : r = state . residuals q = np . fft . fftn ( r ) #Get the expected values of `sigma`: calc_sig = lambda x : np . sqrt ( np . dot ( x , x ) / x . size ) rh , xr = np . histogram ( r . ravel ( ) / calc_sig ( r . ravel ( ) ) , bins = bins , density = True ) bigq = np . append ( q . real . ravel ( ) , q . imag . ravel ( ) ) qh , xq = np . histogram ( bigq / calc_sig ( q . real . ravel ( ) ) , bins = bins , density = True ) xr = 0.5 * ( xr [ 1 : ] + xr [ : - 1 ] ) xq = 0.5 * ( xq [ 1 : ] + xq [ : - 1 ] ) gauss = lambda t : np . exp ( - t * t * 0.5 ) / np . sqrt ( 2 * np . pi ) plt . figure ( figsize = [ 16 , 8 ] ) axes = [ ] for a , ( x , r , lbl ) in enumerate ( [ [ xr , rh , 'Real' ] , [ xq , qh , 'Fourier' ] ] ) : ax = plt . subplot ( 1 , 2 , a + 1 ) ax . semilogy ( x , r , label = 'Data' ) ax . plot ( x , gauss ( x ) , label = 'Gauss Fit' , scalex = False , scaley = False ) ax . set_xlabel ( 'Residuals value $r/\sigma$' ) ax . set_ylabel ( 'Probability $P(r/\sigma)$' ) ax . legend ( loc = 'upper right' ) ax . set_title ( '{}-Space' . format ( lbl ) ) ax . set_xlim ( xlim ) axes . append ( ax ) return axes
2120	def associate_success_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , * * kwargs )
6835	def vagrant_settings ( self , name = '' , * args , * * kwargs ) : config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) kwargs . update ( extra_args ) return self . settings ( * args , * * kwargs )
13241	def daily_periods ( self , range_start = datetime . date . min , range_end = datetime . date . max , exclude_dates = tuple ( ) ) : tz = self . timezone period = self . period weekdays = self . weekdays current_date = max ( range_start , self . start_date ) end_date = range_end if self . end_date : end_date = min ( end_date , self . end_date ) while current_date <= end_date : if current_date . weekday ( ) in weekdays and current_date not in exclude_dates : yield Period ( tz . localize ( datetime . datetime . combine ( current_date , period . start ) ) , tz . localize ( datetime . datetime . combine ( current_date , period . end ) ) ) current_date += datetime . timedelta ( days = 1 )
1008	def _learnPhase2 ( self , readOnly = False ) : # Clear out predicted state to start with self . lrnPredictedState [ 't' ] . fill ( 0 ) # Compute new predicted state. When computing predictions for # phase 2, we predict at most one cell per column (the one with the best # matching segment). for c in xrange ( self . numberOfCols ) : # Is there a cell predicted to turn on in this column? i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue # Turn on the predicted state for the best matching cell and queue # the pertinent segment up for an update, which will get processed if # the cell receives bottom up in the future. self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue # Queue up this segment for updating segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 # increment totalActivations self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : # creates a new pooling segment if no best matching segment found # sum(all synapses) >= minThreshold, "weak" activation predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )
9999	def cells_to_series ( cells , args ) : paramlen = len ( cells . formula . parameters ) is_multidx = paramlen > 1 if len ( cells . data ) == 0 : data = { } indexes = None elif paramlen == 0 : # Const Cells data = list ( cells . data . values ( ) ) indexes = [ np . nan ] else : if len ( args ) > 0 : defaults = tuple ( param . default for param in cells . formula . signature . parameters . values ( ) ) updated_args = [ ] for arg in args : if len ( arg ) > paramlen : arg = arg [ : paramlen ] elif len ( arg ) < paramlen : arg += defaults [ len ( arg ) : ] updated_args . append ( arg ) items = [ ( arg , cells . data [ arg ] ) for arg in updated_args if arg in cells . data ] else : items = [ ( key , value ) for key , value in cells . data . items ( ) ] if not is_multidx : # Peel 1-element tuple items = [ ( key [ 0 ] , value ) for key , value in items ] if len ( items ) == 0 : indexes , data = None , { } else : indexes , data = zip ( * items ) if is_multidx : indexes = pd . MultiIndex . from_tuples ( indexes ) result = pd . Series ( data = data , name = cells . name , index = indexes ) if indexes is not None and any ( i is not np . nan for i in indexes ) : result . index . names = list ( cells . formula . parameters ) return result
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
2888	def disconnect ( self , callback ) : if self . weak_subscribers is not None : with self . lock : index = self . _weakly_connected_index ( callback ) if index is not None : self . weak_subscribers . pop ( index ) [ 0 ] if self . hard_subscribers is not None : try : index = self . _hard_callbacks ( ) . index ( callback ) except ValueError : pass else : self . hard_subscribers . pop ( index )
13027	def detect_os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system_os = '' for line in out : if line . startswith ( 'Target OS:' ) : system_os = line . replace ( 'Target OS: ' , '' ) break return system_os
8419	def is_close_to_int ( x ) : if not np . isfinite ( x ) : return False return abs ( x - nearest_int ( x ) ) < 1e-10
1230	def tf_optimization ( self , states , internals , actions , terminal , reward , next_states = None , next_internals = None ) : arguments = self . optimizer_arguments ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) return self . optimizer . minimize ( * * arguments )
1458	def _get_deps_list ( abs_path_to_pex ) : pex = zipfile . ZipFile ( abs_path_to_pex , mode = 'r' ) deps = list ( set ( [ re . match ( egg_regex , i ) . group ( 1 ) for i in pex . namelist ( ) if re . match ( egg_regex , i ) is not None ] ) ) return deps
11614	def export_posterior_probability ( self , filename , title = "Posterior Probability" ) : self . probability . save ( h5file = filename , title = title )
7903	def set_handlers ( self , priority = 10 ) : self . stream . set_message_handler ( "groupchat" , self . __groupchat_message , None , priority ) self . stream . set_message_handler ( "error" , self . __error_message , None , priority ) self . stream . set_presence_handler ( "available" , self . __presence_available , None , priority ) self . stream . set_presence_handler ( "unavailable" , self . __presence_unavailable , None , priority ) self . stream . set_presence_handler ( "error" , self . __presence_error , None , priority )
2270	def _win32_symlink2 ( path , link , allow_fallback = True , verbose = 0 ) : if _win32_can_symlink ( ) : return _win32_symlink ( path , link , verbose ) else : return _win32_junction ( path , link , verbose )
1771	def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_dir , args = ( self . workspace , ) ) t . start ( )
649	def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , "can't generate non-overlapping coincidences" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
11550	def setup ( self , configuration = "ModbusSerialClient(method='rtu',port='/dev/cu.usbmodem14101',baudrate=9600)" ) : from pymodbus3 . client . sync import ModbusSerialClient , ModbusUdpClient , ModbusTcpClient self . _client = eval ( configuration ) self . _client . connect ( )
10858	def get_update_tile ( self , params , values ) : doglobal , particles = self . _update_type ( params ) if doglobal : return self . shape . copy ( ) # 1) store the current parameters of interest values0 = self . get_values ( params ) # 2) calculate the current tileset tiles0 = [ self . _tile ( n ) for n in particles ] # 3) update to newer parameters and calculate tileset self . set_values ( params , values ) tiles1 = [ self . _tile ( n ) for n in particles ] # 4) revert parameters & return union of all tiles self . set_values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
12717	def angles ( self ) : return [ self . ode_obj . getAngle ( i ) for i in range ( self . ADOF ) ]
1153	def _hash ( self ) : MAX = sys . maxint MASK = 2 * MAX + 1 n = len ( self ) h = 1927868237 * ( n + 1 ) h &= MASK for x in self : hx = hash ( x ) h ^= ( hx ^ ( hx << 16 ) ^ 89869747 ) * 3644798167 h &= MASK h = h * 69069 + 907133923 h &= MASK if h > MAX : h -= MASK + 1 if h == - 1 : h = 590923713 return h
9600	def wait_for ( self , timeout = 10000 , interval = 1000 , asserter = lambda x : x ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for ( driver ) : asserter ( driver ) return driver return _wait_for ( self )
10658	def amount_fractions ( masses ) : n = amounts ( masses ) n_total = sum ( n . values ( ) ) return { compound : n [ compound ] / n_total for compound in n . keys ( ) }
11898	def _get_image_from_file ( dir_path , image_file ) : # Save ourselves the effort if PIL is not present, and return None now if not PIL_ENABLED : return None # Put together full path path = os . path . join ( dir_path , image_file ) # Try to read the image img = None try : img = Image . open ( path ) except IOError as exptn : print ( 'Error loading image file %s: %s' % ( path , exptn ) ) # Return image or None return img
8771	def _add_default_tz_bindings ( self , context , switch , network_id ) : default_tz = CONF . NVP . default_tz # If there is no default tz specified it's pointless to try # and add any additional default tz bindings. if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_add_default_tz_bindings()." ) return # This should never be called without a neutron network uuid, # we require it to bind some segment allocations. if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_add_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . add ( context , switch , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
5039	def is_user_enrolled ( cls , user , course_id , course_mode ) : enrollment_client = EnrollmentApiClient ( ) try : enrollments = enrollment_client . get_course_enrollment ( user . username , course_id ) if enrollments and course_mode == enrollments . get ( 'mode' ) : return True except HttpClientError as exc : logging . error ( 'Error while checking enrollment status of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) except KeyError as exc : logging . warning ( 'Error while parsing enrollment data of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) return False
1781	def AAM ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AH = Operators . UDIV ( cpu . AL , imm ) cpu . AL = Operators . UREM ( cpu . AL , imm ) # Defined flags: ...sz.p. cpu . _calculate_logic_flags ( 8 , cpu . AL )
2783	def save ( self ) : data = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } return self . get_data ( "domains/%s/records/%s" % ( self . domain , self . id ) , type = PUT , params = data )
5968	def energy_minimize ( dirname = 'em' , mdp = config . templates [ 'em.mdp' ] , struct = 'solvate/ionized.gro' , top = 'top/system.top' , output = 'em.pdb' , deffnm = "em" , mdrunner = None , mdrun_args = None , * * kwargs ) : structure = realpath ( struct ) topology = realpath ( top ) mdp_template = config . get_template ( mdp ) deffnm = deffnm . strip ( ) mdrun_args = { } if mdrun_args is None else mdrun_args # write the processed topology to the default output kwargs . setdefault ( 'pp' , 'processed.top' ) # filter some kwargs that might come through when feeding output # from previous stages such as solvate(); necessary because *all* # **kwargs must be *either* substitutions in the mdp file *or* valid # command line parameters for ``grompp``. kwargs . pop ( 'ndx' , None ) # mainselection is not used but only passed through; right now we # set it to the default that is being used in all argument lists # but that is not pretty. TODO. mainselection = kwargs . pop ( 'mainselection' , '"Protein"' ) # only interesting when passed from solvate() qtot = kwargs . pop ( 'qtot' , 0 ) # mdp is now the *output* MDP that will be generated from mdp_template mdp = deffnm + '.mdp' tpr = deffnm + '.tpr' logger . info ( "[{dirname!s}] Energy minimization of struct={struct!r}, top={top!r}, mdp={mdp!r} ..." . format ( * * vars ( ) ) ) cbook . add_mdp_includes ( topology , kwargs ) if qtot != 0 : # At the moment this is purely user-reported and really only here because # it might get fed into the function when using the keyword-expansion pipeline # usage paradigm. wmsg = "Total charge was reported as qtot = {qtot:g} <> 0; probably a problem." . format ( * * vars ( ) ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = BadParameterWarning ) with in_dir ( dirname ) : unprocessed = cbook . edit_mdp ( mdp_template , new_mdp = mdp , * * kwargs ) check_mdpargs ( unprocessed ) gromacs . grompp ( f = mdp , o = tpr , c = structure , r = structure , p = topology , * * unprocessed ) mdrun_args . update ( v = True , stepout = 10 , deffnm = deffnm , c = output ) if mdrunner is None : mdrun = run . get_double_or_single_prec_mdrun ( ) mdrun ( * * mdrun_args ) else : if type ( mdrunner ) is type : # class # user wants full control and provides simulation.MDrunner **class** # NO CHECKING --- in principle user can supply any callback they like mdrun = mdrunner ( * * mdrun_args ) mdrun . run ( ) else : # anything with a run() method that takes mdrun arguments... try : mdrunner . run ( mdrunargs = mdrun_args ) except AttributeError : logger . error ( "mdrunner: Provide a gromacs.run.MDrunner class or instance or a callback with a run() method" ) raise TypeError ( "mdrunner: Provide a gromacs.run.MDrunner class or instance or a callback with a run() method" ) # em.gro --> gives 'Bad box in file em.gro' warning --- why?? # --> use em.pdb instead. if not os . path . exists ( output ) : errmsg = "Energy minimized system NOT produced." logger . error ( errmsg ) raise GromacsError ( errmsg ) final_struct = realpath ( output ) logger . info ( "[{dirname!s}] energy minimized structure {final_struct!r}" . format ( * * vars ( ) ) ) return { 'struct' : final_struct , 'top' : topology , 'mainselection' : mainselection , }
3154	def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
2282	def launch_R_script ( template , arguments , output_function = None , verbose = True , debug = False ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_R_script_' + id + '/' ) try : scriptpath = '/tmp/cdt_R_script_' + id + '/instance_{}' . format ( os . path . basename ( template ) ) copy ( template , scriptpath ) with fileinput . FileInput ( scriptpath , inplace = True ) as file : for line in file : mline = line for elt in arguments : mline = mline . replace ( elt , arguments [ elt ] ) print ( mline , end = '' ) if output_function is None : output = subprocess . call ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) else : if verbose : process = subprocess . Popen ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True ) else : process = subprocess . Popen ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) process . wait ( ) output = output_function ( ) # Cleaning up except Exception as e : if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) raise e except KeyboardInterrupt : if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) raise KeyboardInterrupt if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) return output
3272	def _init ( self ) : # TODO: recalc self.path from <self._file_path>, to fix correct file system case # On windows this would lead to correct URLs self . provider . _count_get_resource_inst_init += 1 tableName , primKey = self . provider . _split_path ( self . path ) display_type = "Unknown" displayTypeComment = "" contentType = "text/html" # _logger.debug("getInfoDict(%s), nc=%s" % (path, self.connectCount)) if tableName is None : display_type = "Database" elif primKey is None : # "database" and table name display_type = "Database Table" else : contentType = "text/csv" if primKey == "_ENTIRE_CONTENTS" : display_type = "Database Table Contents" displayTypeComment = "CSV Representation of Table Contents" else : display_type = "Database Record" displayTypeComment = "Attributes available as properties" # Avoid calling is_collection, since it would call isExisting -> _init_connection is_collection = primKey is None self . _cache = { "content_length" : None , "contentType" : contentType , "created" : time . time ( ) , "display_name" : self . name , "etag" : hashlib . md5 ( ) . update ( self . path ) . hexdigest ( ) , # "etag": md5.new(self.path).hexdigest(), "modified" : None , "support_ranges" : False , "display_info" : { "type" : display_type , "typeComment" : displayTypeComment } , } # Some resource-only infos: if not is_collection : self . _cache [ "modified" ] = time . time ( ) _logger . debug ( "---> _init, nc=%s" % self . provider . _count_initConnection )
6294	def transform ( self , program : moderngl . Program , buffer : moderngl . Buffer , mode = None , vertices = - 1 , first = 0 , instances = 1 ) : vao = self . instance ( program ) if mode is None : mode = self . mode vao . transform ( buffer , mode = mode , vertices = vertices , first = first , instances = instances )
1779	def AAA ( cpu ) : cpu . AF = Operators . OR ( cpu . AL & 0x0F > 9 , cpu . AF ) cpu . CF = cpu . AF cpu . AH = Operators . ITEBV ( 8 , cpu . AF , cpu . AH + 1 , cpu . AH ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) """ if (cpu.AL & 0x0F > 9) or cpu.AF == 1: cpu.AL = cpu.AL + 6 cpu.AH = cpu.AH + 1 cpu.AF = True cpu.CF = True else: cpu.AF = False cpu.CF = False """ cpu . AL = cpu . AL & 0x0f
7725	def __from_xmlnode ( self , xmlnode ) : actor = None reason = None n = xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != MUC_USER_NS : continue if n . name == "actor" : actor = n . getContent ( ) if n . name == "reason" : reason = n . getContent ( ) n = n . next self . __init ( from_utf8 ( xmlnode . prop ( "affiliation" ) ) , from_utf8 ( xmlnode . prop ( "role" ) ) , from_utf8 ( xmlnode . prop ( "jid" ) ) , from_utf8 ( xmlnode . prop ( "nick" ) ) , from_utf8 ( actor ) , from_utf8 ( reason ) , )
2823	def convert_lrelu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting lrelu ...' ) if names == 'short' : tf_name = 'lRELU' + random_string ( 3 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) leakyrelu = keras . layers . LeakyReLU ( alpha = params [ 'alpha' ] , name = tf_name ) layers [ scope_name ] = leakyrelu ( layers [ inputs [ 0 ] ] )
3608	def put_async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_put_request , args = ( endpoint , data , params , headers ) , callback = callback )
9714	async def connect ( host , port = 22223 , version = "1.19" , on_event = None , on_disconnect = None , timeout = 5 , loop = None , ) -> QRTConnection : loop = loop or asyncio . get_event_loop ( ) try : _ , protocol = await loop . create_connection ( lambda : QTMProtocol ( loop = loop , on_event = on_event , on_disconnect = on_disconnect ) , host , port , ) except ( ConnectionRefusedError , TimeoutError , OSError ) as exception : LOG . error ( exception ) return None try : await protocol . set_version ( version ) except QRTCommandException as exception : LOG . error ( Exception ) return None except TypeError as exception : # TODO: fix test requiring this (test_connect_set_version) LOG . error ( exception ) return None return QRTConnection ( protocol , timeout = timeout )
7499	def resolve_ambigs ( tmpseq ) : ## iterate over the bases 'RSKWYM': [82, 83, 75, 87, 89, 77] for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : ## get all site in this ambig idx , idy = np . where ( tmpseq == ambig ) ## get the two resolutions of the ambig res1 , res2 = AMBIGS [ ambig . view ( "S1" ) ] ## randomly sample half those sites halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) ## replace ambig bases with their resolutions for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq
12579	def mask_and_flatten ( self ) : self . _check_for_mask ( ) return self . get_data ( smoothed = True , masked = True , safe_copy = False ) [ self . get_mask_indices ( ) ] , self . get_mask_indices ( ) , self . mask . shape
2163	def list ( self , group = None , host_filter = None , * * kwargs ) : if group : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'groups__in' , group ) , ) if host_filter : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'host_filter' , host_filter ) , ) return super ( Resource , self ) . list ( * * kwargs )
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
7057	def s3_get_file ( bucket , filename , local_file , altexts = None , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . download_file ( bucket , filename , local_file ) return local_file except Exception as e : if altexts is not None : for alt_extension in altexts : split_ext = os . path . splitext ( filename ) check_file = split_ext [ 0 ] + alt_extension try : client . download_file ( bucket , check_file , local_file . replace ( split_ext [ - 1 ] , alt_extension ) ) return local_file . replace ( split_ext [ - 1 ] , alt_extension ) except Exception as e : pass else : LOGEXCEPTION ( 'could not download s3://%s/%s' % ( bucket , filename ) ) if raiseonfail : raise return None
3126	def get ( self , template_id , * * queryparams ) : self . template_id = template_id return self . _mc_client . _get ( url = self . _build_path ( template_id ) , * * queryparams )
3956	def update_nginx_from_config ( nginx_config ) : logging . info ( 'Updating nginx with new Dusty config' ) temp_dir = tempfile . mkdtemp ( ) os . mkdir ( os . path . join ( temp_dir , 'html' ) ) _write_nginx_config ( constants . NGINX_BASE_CONFIG , os . path . join ( temp_dir , constants . NGINX_PRIMARY_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'http' ] , os . path . join ( temp_dir , constants . NGINX_HTTP_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'stream' ] , os . path . join ( temp_dir , constants . NGINX_STREAM_CONFIG_NAME ) ) _write_nginx_config ( constants . NGINX_502_PAGE_HTML , os . path . join ( temp_dir , 'html' , constants . NGINX_502_PAGE_NAME ) ) sync_local_path_to_vm ( temp_dir , constants . NGINX_CONFIG_DIR_IN_VM )
13806	def validate_params ( required , optional , params ) : missing_fields = [ x for x in required if x not in params ] if missing_fields : field_strings = ", " . join ( missing_fields ) raise Exception ( "Missing fields: %s" % field_strings ) disallowed_fields = [ x for x in params if x not in optional and x not in required ] if disallowed_fields : field_strings = ", " . join ( disallowed_fields ) raise Exception ( "Disallowed fields: %s" % field_strings )
2564	def start ( self ) : self . comm . Barrier ( ) logger . debug ( "Manager synced with workers" ) self . _kill_event = threading . Event ( ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) start = None result_counter = 0 task_recv_counter = 0 task_sent_counter = 0 logger . info ( "Loop start" ) while not self . _kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) # In this block we attempt to probe MPI for a set amount of time, # and if we have exhausted all available MPI events, we move on # to the next block. The timer and counter trigger balance # fairness and responsiveness. timer = time . time ( ) + 0.05 counter = min ( 10 , comm . size ) while time . time ( ) < timer : info = MPI . Status ( ) if counter > 10 : logger . debug ( "Hit max mpi events per round" ) break if not self . comm . Iprobe ( status = info ) : logger . debug ( "Timer expired, processed {} mpi events" . format ( counter ) ) break else : tag = info . Get_tag ( ) logger . info ( "Message with tag {} received" . format ( tag ) ) counter += 1 if tag == RESULT_TAG : result = self . recv_result_from_workers ( ) self . pending_result_queue . put ( result ) result_counter += 1 elif tag == TASK_REQUEST_TAG : worker_rank = self . recv_task_request_from_workers ( ) self . ready_worker_queue . put ( worker_rank ) else : logger . error ( "Unknown tag {} - ignoring this message and continuing" . format ( tag ) ) available_worker_cnt = self . ready_worker_queue . qsize ( ) available_task_cnt = self . pending_task_queue . qsize ( ) logger . debug ( "[MAIN] Ready workers: {} Ready tasks: {}" . format ( available_worker_cnt , available_task_cnt ) ) this_round = min ( available_worker_cnt , available_task_cnt ) for i in range ( this_round ) : worker_rank = self . ready_worker_queue . get ( ) task = self . pending_task_queue . get ( ) comm . send ( task , dest = worker_rank , tag = worker_rank ) task_sent_counter += 1 logger . debug ( "Assigning worker:{} task:{}" . format ( worker_rank , task [ 'task_id' ] ) ) if not start : start = time . time ( ) logger . debug ( "Tasks recvd:{} Tasks dispatched:{} Results recvd:{}" . format ( task_recv_counter , task_sent_counter , result_counter ) ) # print("[{}] Received: {}".format(self.identity, msg)) # time.sleep(random.randint(4,10)/10) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "mpi_worker_pool ran for {} seconds" . format ( delta ) )
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
5016	def transmit ( self , payload , * * kwargs ) : IntegratedChannelLearnerDataTransmissionAudit = apps . get_model ( # pylint: disable=invalid-name app_label = kwargs . get ( 'app_label' , 'integrated_channel' ) , model_name = kwargs . get ( 'model_name' , 'LearnerDataTransmissionAudit' ) , ) # Since we have started sending courses to integrated channels instead of course runs, # we need to attempt to send transmissions with course keys and course run ids in order to # ensure that we account for whether courses or course runs exist in the integrated channel. # The exporters have been changed to return multiple transmission records to attempt, # one by course key and one by course run id. # If the transmission with the course key succeeds, the next one will get skipped. # If it fails, the one with the course run id will be attempted and (presumably) succeed. for learner_data in payload . export ( ) : serialized_payload = learner_data . serialize ( enterprise_configuration = self . enterprise_configuration ) LOGGER . debug ( 'Attempting to transmit serialized payload: %s' , serialized_payload ) enterprise_enrollment_id = learner_data . enterprise_course_enrollment_id if learner_data . completed_timestamp is None : # The user has not completed the course, so we shouldn't send a completion status call LOGGER . info ( 'Skipping in-progress enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue previous_transmissions = IntegratedChannelLearnerDataTransmissionAudit . objects . filter ( enterprise_course_enrollment_id = enterprise_enrollment_id , error_message = '' ) if previous_transmissions . exists ( ) : # We've already sent a completion status call for this enrollment LOGGER . info ( 'Skipping previously sent enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue try : code , body = self . client . create_course_completion ( getattr ( learner_data , kwargs . get ( 'remote_user_id' ) ) , serialized_payload ) LOGGER . info ( 'Successfully sent completion status call for enterprise enrollment {}' . format ( enterprise_enrollment_id , ) ) except RequestException as request_exception : code = 500 body = str ( request_exception ) self . handle_transmission_error ( learner_data , request_exception ) learner_data . status = str ( code ) learner_data . error_message = body if code >= 400 else '' learner_data . save ( )
9369	def legal_ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
11112	def load_repository ( self , path ) : # try to open if path . strip ( ) in ( '' , '.' ) : path = os . getcwd ( ) repoPath = os . path . realpath ( os . path . expanduser ( path ) ) if not self . is_repository ( repoPath ) : raise Exception ( "no repository found in '%s'" % str ( repoPath ) ) # get pyrepinfo path repoInfoPath = os . path . join ( repoPath , ".pyrepinfo" ) try : fd = open ( repoInfoPath , 'rb' ) except Exception as e : raise Exception ( "unable to open repository file(%s)" % e ) # before doing anything try to lock repository # can't decorate with @acquire_lock because this will point to old repository # path or to current working directory which might not be the path anyways L = Locker ( filePath = None , lockPass = str ( uuid . uuid1 ( ) ) , lockPath = os . path . join ( repoPath , ".pyreplock" ) ) acquired , code = L . acquire_lock ( ) # check if acquired. if not acquired : warnings . warn ( "code %s. Unable to aquire the lock when calling 'load_repository'. You may try again!" % ( code , ) ) return try : # unpickle file try : repo = pickle . load ( fd ) except Exception as e : fd . close ( ) raise Exception ( "unable to pickle load repository (%s)" % e ) finally : fd . close ( ) # check if it's a PyrepInfo instance if not isinstance ( repo , Repository ) : raise Exception ( ".pyrepinfo in '%s' is not a repository instance." % s ) else : # update info path self . __reset_repository ( ) self . __update_repository ( repo ) self . __path = repoPath # set timestamp self . __state = self . _get_or_create_state ( ) except Exception as e : L . release_lock ( ) raise Exception ( e ) finally : L . release_lock ( ) # set loaded repo locker path to L because repository have been moved to another directory self . __locker = L # return return self
9888	def _call_multi_fortran_z_attr ( self , names , data_types , num_elems , entry_nums , attr_nums , var_names , input_type_code , func , data_offset = None ) : # isolate input type code variables idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : # maximimum array dimension max_num = num_elems [ idx ] . max ( ) sub_num_elems = num_elems [ idx ] sub_names = np . array ( names ) [ idx ] sub_var_names = np . array ( var_names ) [ idx ] # zVariable numbers, 'entry' number sub_entry_nums = entry_nums [ idx ] # attribute number sub_attr_nums = attr_nums [ idx ] status , data = func ( self . fname , sub_attr_nums , sub_entry_nums , len ( sub_attr_nums ) , max_num , len ( self . fname ) ) if ( status == 0 ) . all ( ) : if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset self . _process_return_multi_z_attr ( data , sub_names , sub_var_names , sub_num_elems ) else : # raise ValueError('CDF Error code :', status) idx , = np . where ( status != 0 ) # raise first error raise IOError ( fortran_cdf . statusreporter ( status [ idx ] [ 0 ] ) )
11489	def _download_folder_recursive ( folder_id , path = '.' ) : session . token = verify_credentials ( ) cur_folder = session . communicator . folder_get ( session . token , folder_id ) # Replace any '/' in the folder name. folder_path = os . path . join ( path , cur_folder [ 'name' ] . replace ( '/' , '_' ) ) print ( 'Creating folder at {0}' . format ( folder_path ) ) try : os . mkdir ( folder_path ) except OSError as e : if e . errno == errno . EEXIST and session . allow_existing_download_paths : pass else : raise cur_children = session . communicator . folder_children ( session . token , folder_id ) for item in cur_children [ 'items' ] : _download_item ( item [ 'item_id' ] , folder_path , item = item ) for folder in cur_children [ 'folders' ] : _download_folder_recursive ( folder [ 'folder_id' ] , folder_path ) for callback in session . folder_download_callbacks : callback ( session . communicator , session . token , cur_folder , folder_path )
2478	def reset ( self ) : # FIXME: this state does not make sense self . reset_creation_info ( ) self . reset_document ( ) self . reset_package ( ) self . reset_file_stat ( ) self . reset_reviews ( ) self . reset_annotations ( ) self . reset_extr_lics ( )
10987	def _pick_state_im_name ( state_name , im_name , use_full_path = False ) : initial_dir = os . getcwd ( ) if ( state_name is None ) or ( im_name is None ) : wid = tk . Tk ( ) wid . withdraw ( ) if state_name is None : state_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select pre-featured state' ) os . chdir ( os . path . dirname ( state_name ) ) if im_name is None : im_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select new image' ) if ( not use_full_path ) and ( os . path . dirname ( im_name ) != '' ) : im_path = os . path . dirname ( im_name ) os . chdir ( im_path ) im_name = os . path . basename ( im_name ) else : os . chdir ( initial_dir ) return state_name , im_name
7897	def get_room_jid ( self , nick = None ) : if nick is None : return self . room_jid return JID ( self . room_jid . node , self . room_jid . domain , nick )
3551	def list_descriptors ( self ) : paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , 'Descriptors' ) return map ( BluezGattDescriptor , get_provider ( ) . _get_objects_by_path ( paths ) )
12577	def _mask_data ( self , data ) : self . _check_for_mask ( ) msk_data = self . mask . get_data ( ) if self . ndim == 3 : return data [ msk_data ] , np . where ( msk_data ) elif self . ndim == 4 : return _apply_mask_to_4d_data ( data , self . mask ) else : raise ValueError ( 'Cannot mask {} with {} dimensions using mask {}.' . format ( self , self . ndim , self . mask ) )
5620	def get_best_zoom_level ( input_file , tile_pyramid_type ) : tile_pyramid = BufferedTilePyramid ( tile_pyramid_type ) with rasterio . open ( input_file , "r" ) as src : xmin , ymin , xmax , ymax = reproject_geometry ( segmentize_geometry ( box ( src . bounds . left , src . bounds . bottom , src . bounds . right , src . bounds . top ) , get_segmentize_value ( input_file , tile_pyramid ) ) , src_crs = src . crs , dst_crs = tile_pyramid . crs ) . bounds x_dif = xmax - xmin y_dif = ymax - ymin size = float ( src . width + src . height ) avg_resolution = ( ( x_dif / float ( src . width ) ) * ( float ( src . width ) / size ) + ( y_dif / float ( src . height ) ) * ( float ( src . height ) / size ) ) for zoom in range ( 0 , 40 ) : if tile_pyramid . pixel_x_size ( zoom ) <= avg_resolution : return zoom - 1
5003	def handle ( self , * args , * * options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : # Assign admin role to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : # Assign operator role to staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : # Assign enterprise learner role to enterprise customer users. self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : # Assign enterprise enrollment api admin to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : # Assign enterprise catalog admin role to users with having credentials in catalog. self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
11637	def get_data ( filename ) : name , ext = get_file_extension ( filename ) func = json_get_data if ext == '.json' else yaml_get_data return func ( filename )
4158	def ma ( X , Q , M ) : if Q <= 0 or Q >= M : raise ValueError ( 'Q(MA) must be in ]0,lag[' ) #C Fit a high-order AR to the data a , rho , _c = yulewalker . aryule ( X , M , 'biased' ) #! Eq. (10.5) #add an element unity to the AR parameter array a = np . insert ( a , 0 , 1 ) #C Find MA parameters from autocorrelations by Yule-Walker method ma_params , _p , _c = yulewalker . aryule ( a , Q , 'biased' ) #! Eq. (10.7) return ma_params , rho
1254	def setup_saver ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] for c in self . get_savable_components ( ) : c . register_saver_ops ( ) # TensorFlow saver object # TODO potentially make other options configurable via saver spec. self . saver = tf . train . Saver ( var_list = global_variables , # should be given? reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True # filename=None )
8362	def encode ( self , o ) : # This is for extremely simple cases and benchmarks. if isinstance ( o , basestring ) : if isinstance ( o , str ) : _encoding = self . encoding if ( _encoding is not None and not ( _encoding == 'utf-8' ) ) : o = o . decode ( _encoding ) if self . ensure_ascii : return encode_basestring_ascii ( o ) else : return encode_basestring ( o ) # This doesn't pass the iterator directly to ''.join() because the # exceptions aren't as detailed. The list call should be roughly # equivalent to the PySequence_Fast that ''.join() would do. chunks = list ( self . iterencode ( o ) ) return '' . join ( chunks )
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
4560	def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
6638	def getScript ( self , scriptname ) : script = self . description . get ( 'scripts' , { } ) . get ( scriptname , None ) if script is not None : if isinstance ( script , str ) or isinstance ( script , type ( u'unicode string' ) ) : import shlex script = shlex . split ( script ) # if the command is a python script, run it with the python # interpreter being used to run yotta, also fetch the absolute path # to the script relative to this module (so that the script can be # distributed with the module, no matter what current working # directory it will be executed in): if len ( script ) and script [ 0 ] . lower ( ) . endswith ( '.py' ) : if not os . path . isabs ( script [ 0 ] ) : absscript = os . path . abspath ( os . path . join ( self . path , script [ 0 ] ) ) logger . debug ( 'rewriting script %s to be absolute path %s' , script [ 0 ] , absscript ) script [ 0 ] = absscript import sys script = [ sys . executable ] + script return script
1197	def nested ( * managers ) : warn ( "With-statements now directly support multiple context managers" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : # Don't rely on sys.exc_info() still containing # the right information. Another exception may # have been raised and caught by an exit method raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]
4416	async def play ( self , track_index : int = 0 , ignore_shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . _lavalink . dispatch_event ( QueueEndEvent ( self ) ) else : if self . shuffle and not ignore_shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track_index , len ( self . queue ) - 1 ) ) self . current = track await self . _lavalink . ws . send ( op = 'play' , guildId = self . guild_id , track = track . track ) await self . _lavalink . dispatch_event ( TrackStartEvent ( self , track ) )
3858	def _on_watermark_notification ( self , notif ) : # Update the conversation: if self . get_user ( notif . user_id ) . is_self : logger . info ( 'latest_read_timestamp for {} updated to {}' . format ( self . id_ , notif . read_timestamp ) ) self_conversation_state = ( self . _conversation . self_conversation_state ) self_conversation_state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( notif . read_timestamp ) ) # Update the participants' watermarks: previous_timestamp = self . _watermarks . get ( notif . user_id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read_timestamp > previous_timestamp : logger . info ( ( 'latest_read_timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id_ , notif . user_id . chat_id , notif . read_timestamp ) ) self . _watermarks [ notif . user_id ] = notif . read_timestamp
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : # (False, "Domain lookup failed, defaulting to {0}".format(UDP_IP)) pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) # UDP sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
3435	def slim_optimize ( self , error_value = float ( 'nan' ) , message = None ) : self . solver . optimize ( ) if self . solver . status == optlang . interface . OPTIMAL : return self . solver . objective . value elif error_value is not None : return error_value else : assert_optimal ( self , message )
10256	def get_causal_central_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_central ( graph , node ) }
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
6664	def list_expiration_dates ( self , base = 'roles/all/ssl' ) : max_fn_len = 0 max_date_len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration_date = self . get_expiration_date ( fqfn ) max_fn_len = max ( max_fn_len , len ( fn ) ) max_date_len = max ( max_date_len , len ( str ( expiration_date ) ) ) data . append ( ( fn , expiration_date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max_fn_len ) , 'Expiration Date' . ljust ( max_date_len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max_fn_len ) , str ( dt ) . ljust ( max_date_len ) , expired ) )
2552	def attr ( * args , * * kwargs ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) else : raise ValueError ( 'not in a tag context' )
7617	def coerce_annotation ( ann , namespace ) : ann = convert ( ann , namespace ) ann . validate ( strict = True ) return ann
2793	def create ( self ) : params = { "name" : self . name , "type" : self . type , "dns_names" : self . dns_names , "private_key" : self . private_key , "leaf_certificate" : self . leaf_certificate , "certificate_chain" : self . certificate_chain } data = self . get_data ( "certificates/" , type = POST , params = params ) if data : self . id = data [ 'certificate' ] [ 'id' ] self . not_after = data [ 'certificate' ] [ 'not_after' ] self . sha1_fingerprint = data [ 'certificate' ] [ 'sha1_fingerprint' ] self . created_at = data [ 'certificate' ] [ 'created_at' ] self . type = data [ 'certificate' ] [ 'type' ] self . dns_names = data [ 'certificate' ] [ 'dns_names' ] self . state = data [ 'certificate' ] [ 'state' ] return self
9202	def count_cycles ( series , ndigits = None , left = False , right = False ) : counts = defaultdict ( float ) round_ = _get_round_function ( ndigits ) for low , high , mult in extract_cycles ( series , left = left , right = right ) : delta = round_ ( abs ( high - low ) ) counts [ delta ] += mult return sorted ( counts . items ( ) )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) # type: libsbml.Parameter parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
6335	def sim ( self , src , tar , * args , * * kwargs ) : return 1.0 - self . dist ( src , tar , * args , * * kwargs )
11812	def present ( self , results ) : for ( score , d ) in results : doc = self . documents [ d ] print ( "%5.2f|%25s | %s" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )
9587	def isarray ( array , test , dim = 2 ) : if dim > 1 : return all ( isarray ( array [ i ] , test , dim - 1 ) for i in range ( len ( array ) ) ) return all ( test ( i ) for i in array )
4325	def dcshift ( self , shift = 0.0 ) : if not is_number ( shift ) or shift < - 2 or shift > 2 : raise ValueError ( 'shift must be a number between -2 and 2.' ) effect_args = [ 'dcshift' , '{:f}' . format ( shift ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'dcshift' ) return self
3969	def _conditional_links ( assembled_specs , app_name ) : link_to_apps = [ ] potential_links = assembled_specs [ 'apps' ] [ app_name ] [ 'conditional_links' ] for potential_link in potential_links [ 'apps' ] : if potential_link in assembled_specs [ 'apps' ] : link_to_apps . append ( potential_link ) for potential_link in potential_links [ 'services' ] : if potential_link in assembled_specs [ 'services' ] : link_to_apps . append ( potential_link ) return link_to_apps
2656	def makedirs ( self , path , mode = 511 , exist_ok = False ) : if exist_ok is False and self . isdir ( path ) : raise OSError ( 'Target directory {} already exists' . format ( path ) ) self . execute_wait ( 'mkdir -p {}' . format ( path ) ) self . sftp_client . chmod ( path , mode )
4988	def eligible_for_direct_audit_enrollment ( self , request , enterprise_customer , resource_id , course_key = None ) : course_identifier = course_key if course_key else resource_id # Return it in one big statement to utilize short-circuiting behavior. Avoid the API call if possible. return request . GET . get ( 'audit' ) and request . path == self . COURSE_ENROLLMENT_VIEW_URL . format ( enterprise_customer . uuid , course_identifier ) and enterprise_customer . catalog_contains_course ( resource_id ) and EnrollmentApiClient ( ) . has_course_mode ( resource_id , 'audit' )
2391	def regenerate_good_tokens ( string ) : toks = nltk . word_tokenize ( string ) pos_string = nltk . pos_tag ( toks ) pos_seq = [ tag [ 1 ] for tag in pos_string ] pos_ngrams = ngrams ( pos_seq , 2 , 4 ) sel_pos_ngrams = f7 ( pos_ngrams ) return sel_pos_ngrams
8402	def rescale_max ( x , to = ( 0 , 1 ) , _from = None ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) out = x / _from [ 1 ] * to [ 1 ] if not array_like : out = out [ 0 ] return out
1059	def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) # Return the wrapper so this can be used as a decorator via partial() return wrapper
9323	def refresh_information ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( * * response ) self . _loaded_information = True
11548	def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 #if false, factor of 1, if true, factor of 2 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : #found no info from idx dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
11722	def app_class ( ) : try : pkg_resources . get_distribution ( 'invenio-files-rest' ) from invenio_files_rest . app import Flask as FlaskBase except pkg_resources . DistributionNotFound : from flask import Flask as FlaskBase # Add Host header validation via APP_ALLOWED_HOSTS configuration variable. class Request ( TrustedHostsMixin , FlaskBase . request_class ) : pass class Flask ( FlaskBase ) : request_class = Request return Flask
10035	def add_arguments ( parser ) : parser . add_argument ( '-e' , '--environment' , help = 'Environment name' , required = False , nargs = '+' ) parser . add_argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the app to be deleted' , action = 'store_true' )
3921	def get_menu_widget ( self , close_callback ) : return ConversationMenu ( self . _coroutine_queue , self . _conversation , close_callback , self . _keys )
9984	def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
3818	async def _base_request ( self , url , content_type , response_type , data ) : headers = { 'content-type' : content_type , # This header is required for Protocol Buffer responses. It causes # them to be base64 encoded: 'X-Goog-Encode-Response-If-Executable' : 'base64' , } params = { # "alternative representation type" (desired response format). 'alt' : response_type , # API key (required to avoid 403 Forbidden "Daily Limit for # Unauthenticated Use Exceeded. Continued use requires signup"). 'key' : API_KEY , } res = await self . _session . fetch ( 'post' , url , headers = headers , params = params , data = data , ) return res
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : # See if this path exists in the data dir data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename # If not, we might have already received a full path if os . path . isfile ( filename ) : return filename # If we didn't find anything, raise an error from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
5920	def fit ( self , xy = False , * * kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc force = kwargs . pop ( 'force' , self . force ) if xy : fitmode = 'rotxy+transxy' kwargs . pop ( 'fit' , None ) infix_default = '_fitxy' else : fitmode = kwargs . pop ( 'fit' , 'rot+trans' ) # user can use 'progressive', too infix_default = '_fit' dt = kwargs . get ( 'dt' ) if dt : infix_default += '_dt{0:d}ps' . format ( int ( dt ) ) # dt in ps kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , infix_default , 'xtc' ) ) ) fitgroup = kwargs . pop ( 'fitgroup' , 'backbone' ) kwargs . setdefault ( 'input' , [ fitgroup , "system" ] ) if kwargs . get ( 'center' , False ) : logger . warn ( "Transformer.fit(): center=%(center)r used: centering should not be combined with fitting." , kwargs ) if len ( kwargs [ 'inputs' ] ) != 3 : logger . error ( "If you insist on centering you must provide three groups in the 'input' kwarg: (center, fit, output)" ) raise ValuError ( "Insufficient index groups for centering,fitting,output" ) logger . info ( "Fitting trajectory %r to with xy=%r..." , kwargs [ 'f' ] , xy ) logger . info ( "Fitting on index group %(fitgroup)r" , vars ( ) ) with utilities . in_dir ( self . dirname ) : if self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : logger . warn ( "File %r exists; force regenerating it with force=True." , kwargs [ 'o' ] ) else : gromacs . trjconv ( fit = fitmode , * * kwargs ) logger . info ( "Fitted trajectory (fitmode=%s): %r." , fitmode , kwargs [ 'o' ] ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
10979	def approve ( group_id , user_id ) : membership = Membership . query . get_or_404 ( ( user_id , group_id ) ) group = membership . group if group . can_edit ( current_user ) : try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( '%(user)s accepted to %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( 'You cannot approve memberships for the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
7079	def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : service = 'Mast.Tic.Crossmatch' xmatch_input = { 'fields' : [ { 'name' : 'ra' , 'type' : 'float' } , { 'name' : 'dec' , 'type' : 'float' } ] } xmatch_input [ 'data' ] = [ { 'ra' : x , 'dec' : y } for ( x , y ) in zip ( ra , decl ) ] params = { 'raColumn' : 'ra' , 'decColumn' : 'dec' , 'radius' : radius_arcsec / 3600.0 } return mast_query ( service , params , data = xmatch_input , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
10956	def get ( self , name ) : for c in self . comps : if c . category == name : return c return None
5105	def poisson_random_measure ( t , rate , rate_max ) : scale = 1.0 / rate_max t = t + exponential ( scale ) while rate_max * uniform ( ) > rate ( t ) : t = t + exponential ( scale ) return t
1133	def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]
7264	def validate ( method ) : # Name error template name_error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed_opts : raise ValueError ( name_error . format ( name ) ) return method ( self , name , * args ) return validator
8066	def get_source ( self , doc ) : start_iter = doc . get_start_iter ( ) end_iter = doc . get_end_iter ( ) source = doc . get_text ( start_iter , end_iter , False ) return source
4149	def onesided_gen ( self ) : if self . N % 2 == 0 : for n in range ( 0 , self . N // 2 + 1 ) : yield n * self . df else : for n in range ( 0 , ( self . N + 1 ) // 2 ) : yield n * self . df
5435	def tasks_file_to_task_descriptors ( tasks , retries , input_file_param_util , output_file_param_util ) : task_descriptors = [ ] path = tasks [ 'path' ] task_min = tasks . get ( 'min' ) task_max = tasks . get ( 'max' ) # Load the file and set up a Reader that tokenizes the fields param_file = dsub_util . load_file ( path ) reader = csv . reader ( param_file , delimiter = '\t' ) # Read the first line and extract the parameters header = six . advance_iterator ( reader ) job_params = parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) # Build a list of records from the parsed input file for row in reader : # Tasks are numbered starting at 1 and since the first line of the TSV # file is a header, the first task appears on line 2. task_id = reader . line_num - 1 if task_min and task_id < task_min : continue if task_max and task_id > task_max : continue if len ( row ) != len ( job_params ) : dsub_util . print_error ( 'Unexpected number of fields %s vs %s: line %s' % ( len ( row ) , len ( job_params ) , reader . line_num ) ) # Each row can contain "envs", "inputs", "outputs" envs = set ( ) inputs = set ( ) outputs = set ( ) labels = set ( ) for i in range ( 0 , len ( job_params ) ) : param = job_params [ i ] name = param . name if isinstance ( param , job_model . EnvParam ) : envs . add ( job_model . EnvParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . LabelParam ) : labels . add ( job_model . LabelParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . InputFileParam ) : inputs . add ( input_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) elif isinstance ( param , job_model . OutputFileParam ) : outputs . add ( output_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) task_descriptors . append ( job_model . TaskDescriptor ( { 'task-id' : task_id , 'task-attempt' : 1 if retries else None } , { 'labels' : labels , 'envs' : envs , 'inputs' : inputs , 'outputs' : outputs } , job_model . Resources ( ) ) ) # Ensure that there are jobs to execute (and not just a header) if not task_descriptors : raise ValueError ( 'No tasks added from %s' % path ) return task_descriptors
5059	def send_email_notification_message ( user , enrolled_in , enterprise_customer , email_connection = None ) : if hasattr ( user , 'first_name' ) and hasattr ( user , 'username' ) : # PendingEnterpriseCustomerUsers don't have usernames or real names. We should # template slightly differently to make sure weird stuff doesn't happen. user_name = user . first_name if not user_name : user_name = user . username else : user_name = None # Users have an `email` attribute; PendingEnterpriseCustomerUsers have `user_email`. if hasattr ( user , 'email' ) : user_email = user . email elif hasattr ( user , 'user_email' ) : user_email = user . user_email else : raise TypeError ( _ ( '`user` must have one of either `email` or `user_email`.' ) ) msg_context = { 'user_name' : user_name , 'enrolled_in' : enrolled_in , 'organization_name' : enterprise_customer . name , } try : enterprise_template_config = enterprise_customer . enterprise_enrollment_template except ( ObjectDoesNotExist , AttributeError ) : enterprise_template_config = None plain_msg , html_msg = build_notification_message ( msg_context , enterprise_template_config ) subject_line = get_notification_subject_line ( enrolled_in [ 'name' ] , enterprise_template_config ) from_email_address = get_configuration_value_for_site ( enterprise_customer . site , 'DEFAULT_FROM_EMAIL' , default = settings . DEFAULT_FROM_EMAIL ) return mail . send_mail ( subject_line , plain_msg , from_email_address , [ user_email ] , html_message = html_msg , connection = email_connection )
4989	def redirect ( self , request , * args , * * kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) resource_id = course_key or course_run_id or program_uuid # Replace enterprise UUID and resource ID with '{}', to easily match with a path in RouterView.VIEWS. Example: # /enterprise/fake-uuid/course/course-v1:cool+course+2017/enroll/ -> /enterprise/{}/course/{}/enroll/ path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) # Remove course_key from kwargs if it exists because delegate views are not expecting it. kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , * * kwargs )
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
7780	def _process_rfc2425_record ( self , data ) : label , value = data . split ( ":" , 1 ) value = value . replace ( "\\n" , "\n" ) . replace ( "\\N" , "\n" ) psplit = label . lower ( ) . split ( ";" ) name = psplit [ 0 ] params = psplit [ 1 : ] if u"." in name : name = name . split ( "." , 1 ) [ 1 ] name = name . upper ( ) if name in ( u"X-DESC" , u"X-JABBERID" ) : name = name [ 2 : ] if not self . components . has_key ( name ) : return if params : params = dict ( [ p . split ( "=" , 1 ) for p in params ] ) cl , tp = self . components [ name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( name ) : raise ValueError ( "Duplicate %s" % ( name , ) ) try : self . content [ name ] = cl ( name , value , params ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( name ) : self . content [ name ] = [ ] try : self . content [ name ] . append ( cl ( name , value , params ) ) except Empty : pass else : return
238	def create_full_tear_sheet ( returns , positions = None , transactions = None , market_data = None , benchmark_rets = None , slippage = None , live_start_date = None , sector_mappings = None , bayesian = False , round_trips = False , estimate_intraday = 'infer' , hide_positions = False , cone_std = ( 1.0 , 1.5 , 2.0 ) , bootstrap = False , unadjusted_returns = None , style_factor_panel = None , sectors = None , caps = None , shares_held = None , volumes = None , percentile = None , turnover_denom = 'AGB' , set_context = True , factor_returns = None , factor_loadings = None , pos_in_dollars = True , header_rows = None , factor_partitions = FACTOR_PARTITIONS ) : if ( unadjusted_returns is None ) and ( slippage is not None ) and ( transactions is not None ) : unadjusted_returns = returns . copy ( ) returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , slippage ) positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) create_returns_tear_sheet ( returns , positions = positions , transactions = transactions , live_start_date = live_start_date , cone_std = cone_std , benchmark_rets = benchmark_rets , bootstrap = bootstrap , turnover_denom = turnover_denom , header_rows = header_rows , set_context = set_context ) create_interesting_times_tear_sheet ( returns , benchmark_rets = benchmark_rets , set_context = set_context ) if positions is not None : create_position_tear_sheet ( returns , positions , hide_positions = hide_positions , set_context = set_context , sector_mappings = sector_mappings , estimate_intraday = False ) if transactions is not None : create_txn_tear_sheet ( returns , positions , transactions , unadjusted_returns = unadjusted_returns , estimate_intraday = False , set_context = set_context ) if round_trips : create_round_trip_tear_sheet ( returns = returns , positions = positions , transactions = transactions , sector_mappings = sector_mappings , estimate_intraday = False ) if market_data is not None : create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , last_n_days = 125 , estimate_intraday = False ) if style_factor_panel is not None : create_risk_tear_sheet ( positions , style_factor_panel , sectors , caps , shares_held , volumes , percentile ) if factor_returns is not None and factor_loadings is not None : create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , factor_partitions = factor_partitions ) if bayesian : create_bayesian_tear_sheet ( returns , live_start_date = live_start_date , benchmark_rets = benchmark_rets , set_context = set_context )
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
12087	def html_singleAll ( self , template = "basic" ) : for fname in smartSort ( self . cells ) : if template == "fixed" : self . html_single_fixed ( fname ) else : self . html_single_basic ( fname )
334	def compute_bayes_cone ( preds , starting_value = 1. ) : def scoreatpercentile ( cum_preds , p ) : return [ stats . scoreatpercentile ( c , p ) for c in cum_preds . T ] cum_preds = np . cumprod ( preds + 1 , 1 ) * starting_value perc = { p : scoreatpercentile ( cum_preds , p ) for p in ( 5 , 25 , 75 , 95 ) } return perc
8638	def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
10079	def _publish_new ( self , id_ = None ) : minter = current_pidstore . minters [ current_app . config [ 'DEPOSIT_PID_MINTER' ] ] id_ = id_ or uuid . uuid4 ( ) record_pid = minter ( id_ , self ) self [ '_deposit' ] [ 'pid' ] = { 'type' : record_pid . pid_type , 'value' : record_pid . pid_value , 'revision_id' : 0 , } data = dict ( self . dumps ( ) ) data [ '$schema' ] = self . record_schema with self . _process_files ( id_ , data ) : record = self . published_record_class . create ( data , id_ = id_ ) return record
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] # if you don't provide a version number, only the latest # will be included in the response body if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
9385	def convert_to_G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : # No unit value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
4621	def _decrypt_masterpassword ( self ) : aes = AESCipher ( self . password ) checksum , encrypted_master = self . config [ self . config_key ] . split ( "$" ) try : decrypted_master = aes . decrypt ( encrypted_master ) except Exception : self . _raise_wrongmasterpassexception ( ) if checksum != self . _derive_checksum ( decrypted_master ) : self . _raise_wrongmasterpassexception ( ) self . decrypted_master = decrypted_master
9687	def read_gsc_sfr ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 8 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "GSC" ] = self . _calculate_float ( config [ 0 : 4 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 4 : ] ) return data
11938	def add_message_for ( users , level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) m = backend . create_message ( level , message_text , extra_tags , date , url ) backend . archive_store ( users , m ) backend . inbox_store ( users , m )
8341	def _invert ( h ) : i = { } for k , v in h . items ( ) : i [ v ] = k return i
4800	def is_directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . _err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self
9858	def create_url ( self , path , params = { } , opts = { } ) : if opts : warnings . warn ( '`opts` has been deprecated. Use `params` instead.' , DeprecationWarning , stacklevel = 2 ) params = params or opts if self . _shard_strategy == SHARD_STRATEGY_CRC : crc = zlib . crc32 ( path . encode ( 'utf-8' ) ) & 0xffffffff index = crc % len ( self . _domains ) # Deterministically choose domain domain = self . _domains [ index ] elif self . _shard_strategy == SHARD_STRATEGY_CYCLE : domain = self . _domains [ self . _shard_next_index ] self . _shard_next_index = ( self . _shard_next_index + 1 ) % len ( self . _domains ) else : domain = self . _domains [ 0 ] scheme = "https" if self . _use_https else "http" url_obj = UrlHelper ( domain , path , scheme , sign_key = self . _sign_key , include_library_param = self . _include_library_param , params = params ) return str ( url_obj )
10663	def elements ( compounds ) : elementlist = [ parse_compound ( compound ) . count ( ) . keys ( ) for compound in compounds ] return set ( ) . union ( * elementlist )
2493	def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
8393	def parse_pylint_output ( pylint_output ) : for line in pylint_output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT_PARSEABLE_REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed_dict = parsed . groupdict ( ) parsed_dict [ 'linenum' ] = int ( parsed_dict [ 'linenum' ] ) yield PylintError ( * * parsed_dict )
8614	def create_volume ( self , datacenter_id , volume ) : data = ( json . dumps ( self . _create_volume_dict ( volume ) ) ) response = self . _perform_request ( url = '/datacenters/%s/volumes' % datacenter_id , method = 'POST' , data = data ) return response
7050	def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate_type , sigclip ) = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # get the LC into a dict lcdict = readerfunc ( lcfile ) # this should handle lists/tuples being returned by readerfunc # we assume that the first element is the actual lcdict # FIXME: figure out how to not need this assumption if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] outdict = { } # dereference the columns and get them from the lcdict if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) # normalize here if not using special normalization if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs # # now we'll do: 1. sigclip, 2. reform to timebase, 3. renorm to zero # # 1. sigclip as requested stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip ) # 2. now, we'll renorm to the timebase mags_interpolator = spi . interp1d ( stimes , smags , kind = interpolate_type , fill_value = 'extrapolate' ) errs_interpolator = spi . interp1d ( stimes , serrs , kind = interpolate_type , fill_value = 'extrapolate' ) interpolated_mags = mags_interpolator ( timebase ) interpolated_errs = errs_interpolator ( timebase ) # 3. renorm to zero magmedian = np . median ( interpolated_mags ) renormed_mags = interpolated_mags - magmedian # update the dict outdict = { 'mags' : renormed_mags , 'errs' : interpolated_errs , 'origmags' : interpolated_mags } # # done with this magcol # return outdict except Exception as e : LOGEXCEPTION ( 'reform LC task failed: %s' % repr ( task ) ) return None
9873	def CherryPyWSGIServer ( bind_addr , wsgi_app , numthreads = 10 , server_name = None , max = - 1 , request_queue_size = 5 , timeout = 10 , shutdown_timeout = 5 ) : max_threads = max if max_threads < 0 : max_threads = 0 return Rocket ( bind_addr , 'wsgi' , { 'wsgi_app' : wsgi_app } , min_threads = numthreads , max_threads = max_threads , queue_size = request_queue_size , timeout = timeout )
4183	def window_blackman_harris ( N ) : a0 = 0.35875 a1 = 0.48829 a2 = 0.14128 a3 = 0.01168 return _coeff4 ( N , a0 , a1 , a2 , a3 )
9540	def number_range_exclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) <= min or type ( v ) >= max : raise ValueError ( v ) return checker
4676	def getOwnerKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "owner" ] [ "key_auths" ] : key = self . getPrivateKeyForPublicKey ( authority [ 0 ] ) if key : return key raise KeyNotFound
9904	def post_process ( self , group , event , is_new , is_sample , * * kwargs ) : if not self . is_configured ( group . project ) : return host = self . get_option ( 'server_host' , group . project ) port = int ( self . get_option ( 'server_port' , group . project ) ) prefix = self . get_option ( 'prefix' , group . project ) hostname = self . get_option ( 'hostname' , group . project ) or socket . gethostname ( ) resolve_age = group . project . get_option ( 'sentry:resolve_age' , None ) now = int ( time . time ( ) ) template = '%s.%%s[%s]' % ( prefix , group . project . slug ) level = group . get_level_display ( ) label = template % level groups = group . project . group_set . filter ( status = STATUS_UNRESOLVED ) if resolve_age : oldest = timezone . now ( ) - timedelta ( hours = int ( resolve_age ) ) groups = groups . filter ( last_seen__gt = oldest ) num_errors = groups . filter ( level = group . level ) . count ( ) metric = Metric ( hostname , label , num_errors , now ) log . info ( 'will send %s=%s to zabbix' , label , num_errors ) send_to_zabbix ( [ metric ] , host , port )
4089	def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
10255	def get_causal_source_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_source ( graph , node ) }
8444	def update ( check , enter_parameters , version ) : if check : if temple . update . up_to_date ( version = version ) : print ( 'Temple package is up to date' ) else : msg = ( 'This temple package is out of date with the latest template.' ' Update your package by running "temple update" and commiting changes.' ) raise temple . exceptions . NotUpToDateWithTemplateError ( msg ) else : temple . update . update ( new_version = version , enter_parameters = enter_parameters )
6199	def simulate_diffusion ( self , save_pos = False , total_emission = True , radial = False , rs = None , seed = 1 , path = './' , wrap_func = wrap_periodic , chunksize = 2 ** 19 , chunkslice = 'times' , verbose = True ) : if rs is None : rs = np . random . RandomState ( seed = seed ) self . open_store_traj ( chunksize = chunksize , chunkslice = chunkslice , radial = radial , path = path ) # Save current random state for reproducibility self . traj_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) em_store = self . emission_tot if total_emission else self . emission print ( '- Start trajectories simulation - %s' % ctime ( ) , flush = True ) if verbose : print ( '[PID %d] Diffusion time:' % os . getpid ( ) , end = '' ) i_chunk = 0 t_chunk_size = self . emission . chunkshape [ 1 ] chunk_duration = t_chunk_size * self . t_step par_start_pos = self . particles . positions prev_time = 0 for time_size in iter_chunksize ( self . n_samples , t_chunk_size ) : if verbose : curr_time = int ( chunk_duration * ( i_chunk + 1 ) ) if curr_time > prev_time : print ( ' %ds' % curr_time , end = '' , flush = True ) prev_time = curr_time POS , em = self . _sim_trajectories ( time_size , par_start_pos , rs , total_emission = total_emission , save_pos = save_pos , radial = radial , wrap_func = wrap_func ) ## Append em to the permanent storage # if total_emission, data is just a linear array # otherwise is a 2-D array (self.num_particles, c_size) em_store . append ( em ) if save_pos : self . position . append ( np . vstack ( POS ) . astype ( 'float32' ) ) i_chunk += 1 self . store . h5file . flush ( ) # Save current random state self . traj_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . store . h5file . flush ( ) print ( '\n- End trajectories simulation - %s' % ctime ( ) , flush = True )
5127	def stop_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = False
7019	def merge_hatpi_textlc_apertures ( lclist ) : lcaps = { } framekeys = [ ] for lc in lclist : lcd = read_hatpi_textlc ( lc ) # figure what aperture this is and put it into the lcdict. if two LCs # with the same aperture (i.e. TF1 and TF1) are provided, the later one # in the lclist will overwrite the previous one, for col in lcd [ 'columns' ] : if col . startswith ( 'itf' ) : lcaps [ col ] = lcd thisframekeys = lcd [ 'frk' ] . tolist ( ) framekeys . extend ( thisframekeys ) # uniqify the framekeys framekeys = sorted ( list ( set ( framekeys ) ) )
5423	def _wait_after ( provider , job_ids , poll_interval , stop_on_failure ) : # Each time through the loop, the job_set is re-set to the jobs remaining to # check. Jobs are removed from the list when they complete. # # We exit the loop when: # * No jobs remain are running, OR # * stop_on_failure is TRUE AND at least one job returned an error # remove NO_JOB job_ids_to_check = { j for j in job_ids if j != dsub_util . NO_JOB } error_messages = [ ] while job_ids_to_check and ( not error_messages or not stop_on_failure ) : print ( 'Waiting for: %s.' % ( ', ' . join ( job_ids_to_check ) ) ) # Poll until any remaining jobs have completed jobs_left = _wait_for_any_job ( provider , job_ids_to_check , poll_interval ) # Calculate which jobs just completed jobs_completed = job_ids_to_check . difference ( jobs_left ) # Get all tasks for the newly completed jobs tasks_completed = provider . lookup_job_tasks ( { '*' } , job_ids = jobs_completed ) # We don't want to overwhelm the user with output when there are many # tasks per job. So we get a single "dominant" task for each of the # completed jobs (one that is representative of the job's fate). dominant_job_tasks = _dominant_task_for_jobs ( tasks_completed ) if len ( dominant_job_tasks ) != len ( jobs_completed ) : # print info about the jobs we couldn't find # (should only occur for "--after" where the job ID is a typo). jobs_found = dsub_util . tasks_to_job_ids ( dominant_job_tasks ) jobs_not_found = jobs_completed . difference ( jobs_found ) for j in jobs_not_found : error = '%s: not found' % j print_error ( ' %s' % error ) error_messages += [ error ] # Print the dominant task for the completed jobs for t in dominant_job_tasks : job_id = t . get_field ( 'job-id' ) status = t . get_field ( 'task-status' ) print ( ' %s: %s' % ( str ( job_id ) , str ( status ) ) ) if status in [ 'FAILURE' , 'CANCELED' ] : error_messages += [ provider . get_tasks_completion_messages ( [ t ] ) ] job_ids_to_check = jobs_left return error_messages
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
6376	def dist_abs ( self , src , tar , max_offset = 5 ) : if not src : return len ( tar ) if not tar : return len ( src ) src_len = len ( src ) tar_len = len ( tar ) src_cur = 0 tar_cur = 0 lcss = 0 local_cs = 0 while ( src_cur < src_len ) and ( tar_cur < tar_len ) : if src [ src_cur ] == tar [ tar_cur ] : local_cs += 1 else : lcss += local_cs local_cs = 0 if src_cur != tar_cur : src_cur = tar_cur = max ( src_cur , tar_cur ) for i in range ( max_offset ) : if not ( ( src_cur + i < src_len ) or ( tar_cur + i < tar_len ) ) : break if ( src_cur + i < src_len ) and ( src [ src_cur + i ] == tar [ tar_cur ] ) : src_cur += i local_cs += 1 break if ( tar_cur + i < tar_len ) and ( src [ src_cur ] == tar [ tar_cur + i ] ) : tar_cur += i local_cs += 1 break src_cur += 1 tar_cur += 1 lcss += local_cs return round ( max ( src_len , tar_len ) - lcss )
9258	def find_issues_to_add ( all_issues , tag_name ) : filtered = [ ] for issue in all_issues : if issue . get ( "milestone" ) : if issue [ "milestone" ] [ "title" ] == tag_name : iss = copy . deepcopy ( issue ) filtered . append ( iss ) return filtered
1232	def import_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
3814	def _get_upload_session_status ( res ) : response = json . loads ( res . body . decode ( ) ) if 'sessionStatus' not in response : try : info = ( response [ 'errorMessage' ] [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) reason = '{} : {}' . format ( info [ 'status' ] , info [ 'message' ] ) except KeyError : reason = 'unknown reason' raise exceptions . NetworkError ( 'image upload failed: {}' . format ( reason ) ) return response [ 'sessionStatus' ]
9859	def set_parameter ( self , key , value ) : if value is None or isinstance ( value , ( int , float , bool ) ) : value = str ( value ) if key . endswith ( '64' ) : value = urlsafe_b64encode ( value . encode ( 'utf-8' ) ) value = value . replace ( b ( '=' ) , b ( '' ) ) self . _parameters [ key ] = value
744	def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , * * kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( "Method required a TemporalAnomaly model." ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( "Model does not support this command. Model must" "be an active anomalyDetector model." ) return func ( self , * args , * * kwargs ) return _decorator
9353	def job_title ( ) : result = random . choice ( get_dictionary ( 'job_titles' ) ) . strip ( ) result = result . replace ( '#{N}' , job_title_suffix ( ) ) return result
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
7931	def send_message ( source_jid , password , target_jid , body , subject = None , message_type = "chat" , message_thread = None , settings = None ) : # pylint: disable=R0913,R0912 if sys . version_info . major < 3 : # pylint: disable-msg=W0404 from locale import getpreferredencoding encoding = getpreferredencoding ( ) if isinstance ( source_jid , str ) : source_jid = source_jid . decode ( encoding ) if isinstance ( password , str ) : password = password . decode ( encoding ) if isinstance ( target_jid , str ) : target_jid = target_jid . decode ( encoding ) if isinstance ( body , str ) : body = body . decode ( encoding ) if isinstance ( message_type , str ) : message_type = message_type . decode ( encoding ) if isinstance ( message_thread , str ) : message_thread = message_thread . decode ( encoding ) if not isinstance ( source_jid , JID ) : source_jid = JID ( source_jid ) if not isinstance ( target_jid , JID ) : target_jid = JID ( target_jid ) msg = Message ( to_jid = target_jid , body = body , subject = subject , stanza_type = message_type ) def action ( client ) : """Send a mesage `msg` via a client.""" client . stream . send ( msg ) if settings is None : settings = XMPPSettings ( { "starttls" : True , "tls_verify_peer" : False } ) if password is not None : settings [ "password" ] = password handler = FireAndForget ( source_jid , action , settings ) try : handler . run ( ) except KeyboardInterrupt : handler . disconnect ( ) raise
1665	def CheckRedundantVirtual ( filename , clean_lines , linenum , error ) : # Look for "virtual" on current line. line = clean_lines . elided [ linenum ] virtual = Match ( r'^(.*)(\bvirtual\b)(.*)$' , line ) if not virtual : return # Ignore "virtual" keywords that are near access-specifiers. These # are only used in class base-specifier and do not apply to member # functions. if ( Search ( r'\b(public|protected|private)\s+$' , virtual . group ( 1 ) ) or Match ( r'^\s+(public|protected|private)\b' , virtual . group ( 3 ) ) ) : return # Ignore the "virtual" keyword from virtual base classes. Usually # there is a column on the same line in these cases (virtual base # classes are rare in google3 because multiple inheritance is rare). if Match ( r'^.*[^:]:[^:].*$' , line ) : return # Look for the next opening parenthesis. This is the start of the # parameter list (possibly on the next line shortly after virtual). # TODO(unknown): doesn't work if there are virtual functions with # decltype() or other things that use parentheses, but csearch suggests # that this is rare. end_col = - 1 end_line = - 1 start_col = len ( virtual . group ( 2 ) ) for start_line in xrange ( linenum , min ( linenum + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ start_line ] [ start_col : ] parameter_list = Match ( r'^([^(]*)\(' , line ) if parameter_list : # Match parentheses to find the end of the parameter list ( _ , end_line , end_col ) = CloseExpression ( clean_lines , start_line , start_col + len ( parameter_list . group ( 1 ) ) ) break start_col = 0 if end_col < 0 : return # Couldn't find end of parameter list, give up # Look for "override" or "final" after the parameter list # (possibly on the next few lines). for i in xrange ( end_line , min ( end_line + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ i ] [ end_col : ] match = Search ( r'\b(override|final)\b' , line ) if match : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"virtual" is redundant since function is ' 'already declared as "%s"' % match . group ( 1 ) ) ) # Set end_col to check whole lines after we are done with the # first line. end_col = 0 if Search ( r'[^\w]\s*$' , line ) : break
12375	def allowed_operations ( self ) : if self . slug is not None : return self . meta . detail_allowed_operations return self . meta . list_allowed_operations
10838	def interactions ( self ) : interactions = [ ] url = PATHS [ 'GET_INTERACTIONS' ] % self . id response = self . api . get ( url = url ) for interaction in response [ 'interactions' ] : interactions . append ( ResponseObject ( interaction ) ) self . __interactions = interactions return self . __interactions
12736	def are_connected ( self , body_a , body_b ) : return bool ( ode . areConnected ( self . get_body ( body_a ) . ode_body , self . get_body ( body_b ) . ode_body ) )
1990	def rm ( self , key ) : path = os . path . join ( self . uri , key ) os . remove ( path )
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } # name_migration_map -> object_migration_map # Based on the name mapping in name_migration_map build an object to # object mapping to be used in the replacing of variables # inv: object_migration_map's keys should ALWAYS be external/foreign # expressions, and its values should ALWAYS be internal/local expressions object_migration_map = { } #List of foreign vars used in expression foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : # If a variable with the same name was previously migrated if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : # foreign_var was not found in the local declared variables nor # any variable with the same name was previously migrated # let's make a new unique internal name for it migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) # Create and declare a new variable of given type if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : # Note that we are discarding the ArrayProxy encapsulation new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) # Update the var to var mapping object_migration_map [ foreign_var ] = new_var # Update the name to name mapping name_migration_map [ foreign_var . name ] = new_var . name # Actually replace each appearance of migrated variables by the new ones migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
1765	def pop_int ( self , force = False ) : value = self . read_int ( self . STACK , force = force ) self . STACK += self . address_bit_size // 8 return value
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
3799	def Bahadori_liquid ( T , M ) : A = [ - 6.48326E-2 , 2.715015E-3 , - 1.08580E-5 , 9.853917E-9 ] B = [ 1.565612E-2 , - 1.55833E-4 , 5.051114E-7 , - 4.68030E-10 ] C = [ - 1.80304E-4 , 1.758693E-6 , - 5.55224E-9 , 5.201365E-12 ] D = [ 5.880443E-7 , - 5.65898E-9 , 1.764384E-11 , - 1.65944E-14 ] X , Y = M , T a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
2044	def get_storage_data ( self , storage_address , offset ) : value = self . _world_state [ storage_address ] [ 'storage' ] . get ( offset , 0 ) return simplify ( value )
18	def learn ( network , env , seed = None , nsteps = 5 , total_timesteps = int ( 80e6 ) , vf_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 0.5 , lr = 7e-4 , lrschedule = 'linear' , epsilon = 1e-5 , alpha = 0.99 , gamma = 0.99 , log_interval = 100 , load_path = None , * * network_kwargs ) : set_global_seeds ( seed ) # Get the nb of env nenvs = env . num_envs policy = build_policy ( env , network , * * network_kwargs ) # Instantiate the model object (that creates step_model and train_model) model = Model ( policy = policy , env = env , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm , lr = lr , alpha = alpha , epsilon = epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule ) if load_path is not None : model . load ( load_path ) # Instantiate the runner object runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) epinfobuf = deque ( maxlen = 100 ) # Calculate the batch_size nbatch = nenvs * nsteps # Start total timer tstart = time . time ( ) for update in range ( 1 , total_timesteps // nbatch + 1 ) : # Get mini batch of experiences obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) epinfobuf . extend ( epinfos ) policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) nseconds = time . time ( ) - tstart # Calculate the fps (frame per second) fps = int ( ( update * nbatch ) / nseconds ) if update % log_interval == 0 or update == 1 : # Calculates if value function is a good predicator of the returns (ev > 1) # or if it's just worse than predicting nothing (ev =< 0) ev = explained_variance ( values , rewards ) logger . record_tabular ( "nupdates" , update ) logger . record_tabular ( "total_timesteps" , update * nbatch ) logger . record_tabular ( "fps" , fps ) logger . record_tabular ( "policy_entropy" , float ( policy_entropy ) ) logger . record_tabular ( "value_loss" , float ( value_loss ) ) logger . record_tabular ( "explained_variance" , float ( ev ) ) logger . record_tabular ( "eprewmean" , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) logger . record_tabular ( "eplenmean" , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) logger . dump_tabular ( ) return model
8117	def circle_line_intersection ( cx , cy , radius , x1 , y1 , x2 , y2 , infinite = False ) : # Based on: http://www.vb-helper.com/howto_net_line_circle_intersections.html dx = x2 - x1 dy = y2 - y1 A = dx * dx + dy * dy B = 2 * ( dx * ( x1 - cx ) + dy * ( y1 - cy ) ) C = pow ( x1 - cx , 2 ) + pow ( y1 - cy , 2 ) - radius * radius det = B * B - 4 * A * C if A <= 0.0000001 or det < 0 : return [ ] elif det == 0 : # One point of intersection. t = - B / ( 2 * A ) return [ ( x1 + t * dx , y1 + t * dy ) ] else : # Two points of intersection. # A point of intersection lies on the line segment if 0 <= t <= 1, # and on an extension of the segment otherwise. points = [ ] det2 = sqrt ( det ) t1 = ( - B + det2 ) / ( 2 * A ) t2 = ( - B - det2 ) / ( 2 * A ) if infinite or 0 <= t1 <= 1 : points . append ( ( x1 + t1 * dx , y1 + t1 * dy ) ) if infinite or 0 <= t2 <= 1 : points . append ( ( x1 + t2 * dx , y1 + t2 * dy ) ) return points
11148	def create_package ( self , path = None , name = None , mode = None ) : # check mode assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : #mode = 'w:bz2' mode = 'w:' # get root if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path # get name if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext # create tar file tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) # walk directory and create empty directories for dpath in sorted ( list ( self . walk_directories_path ( recursive = True ) ) ) : t = tarfile . TarInfo ( dpath ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) tarHandler . add ( os . path . join ( self . __path , dpath , self . __dirInfo ) , arcname = self . __dirInfo ) # walk files and add to tar for fpath in self . walk_files_path ( recursive = True ) : relaPath , fname = os . path . split ( fpath ) tarHandler . add ( os . path . join ( self . __path , fpath ) , arcname = fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) , arcname = self . __fileInfo % fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileClass % fname ) , arcname = self . __fileClass % fname ) # save repository .pyrepinfo tarHandler . add ( os . path . join ( self . __path , self . __repoFile ) , arcname = ".pyrepinfo" ) # close tar file tarHandler . close ( )
11899	def _get_src_from_image ( img , fallback_image_file ) : # If the image is None, then we can't process, so we should return the # path to the file itself if img is None : return fallback_image_file # Target format should be the same as the original image format, unless it's # a TIF/TIFF, which can't be displayed by most browsers; we convert these # to jpeg target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' # If we have an actual Image, great - put together the base64 image string try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
11737	def route ( bp , * args , * * kwargs ) : kwargs [ 'strict_slashes' ] = kwargs . pop ( 'strict_slashes' , False ) body = _validate_schema ( kwargs . pop ( '_body' , None ) ) query = _validate_schema ( kwargs . pop ( '_query' , None ) ) output = _validate_schema ( kwargs . pop ( 'marshal_with' , None ) ) validate = kwargs . pop ( 'validate' , True ) def decorator ( f ) : @ bp . route ( * args , * * kwargs ) @ wraps ( f ) def wrapper ( * inner_args , * * inner_kwargs ) : """If a schema (_body and/or _query) was supplied to the route decorator, the deserialized :class`marshmallow.Schema` object is injected into the decorated function's kwargs.""" try : if query is not None : query . strict = validate url = furl ( request . url ) inner_kwargs [ '_query' ] = query . load ( data = url . args ) if body is not None : body . strict = validate json_data = request . get_json ( ) if json_data is None : # Set json_data to empty dict if body is empty, so it gets picked up by the validator json_data = { } inner_kwargs [ '_body' ] = body . load ( data = json_data ) except ValidationError as err : return jsonify ( err . messages ) , 422 if output : data = output . dump ( f ( * inner_args , * * inner_kwargs ) ) return jsonify ( data [ 0 ] ) return f ( * inner_args , * * inner_kwargs ) return f return decorator
7206	def generate_workflow_description ( self ) : if not self . tasks : raise WorkflowError ( 'Workflow contains no tasks, and cannot be executed.' ) self . definition = self . workflow_skeleton ( ) if self . batch_values : self . definition [ "batch_values" ] = self . batch_values all_input_port_values = [ t . inputs . __getattribute__ ( input_port_name ) . value for t in self . tasks for input_port_name in t . inputs . _portnames ] for task in self . tasks : # only include multiplex output ports in this task if other tasks refer to them in their inputs. # 1. find the multplex output port_names in this task # 2. see if they are referred to in any other tasks inputs # 3. If not, exclude them from the workflow_def output_multiplex_ports_to_exclude = [ ] multiplex_output_port_names = [ portname for portname in task . outputs . _portnames if task . outputs . __getattribute__ ( portname ) . is_multiplex ] for p in multiplex_output_port_names : output_port_reference = 'source:' + task . name + ':' + p if output_port_reference not in all_input_port_values : output_multiplex_ports_to_exclude . append ( p ) task_def = task . generate_task_workflow_json ( output_multiplex_ports_to_exclude = output_multiplex_ports_to_exclude ) self . definition [ 'tasks' ] . append ( task_def ) if self . callback : self . definition [ 'callback' ] = self . callback return self . definition
1326	def from_keras ( cls , model , bounds , input_shape = None , channel_axis = 3 , preprocessing = ( 0 , 1 ) ) : import tensorflow as tf if input_shape is None : try : input_shape = model . input_shape [ 1 : ] except AttributeError : raise ValueError ( 'Please specify input_shape manually or ' 'provide a model with an input_shape attribute' ) with tf . keras . backend . get_session ( ) . as_default ( ) : inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) logits = model ( inputs ) return cls ( inputs , logits , bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing )
10043	def create_blueprint ( endpoints ) : from invenio_records_ui . views import create_url_rule blueprint = Blueprint ( 'invenio_deposit_ui' , __name__ , static_folder = '../static' , template_folder = '../templates' , url_prefix = '' , ) @ blueprint . errorhandler ( PIDDeletedError ) def tombstone_errorhandler ( error ) : """Render tombstone page.""" return render_template ( current_app . config [ 'DEPOSIT_UI_TOMBSTONE_TEMPLATE' ] , pid = error . pid , record = error . record or { } , ) , 410 for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) options . pop ( 'jsonschema' , None ) options . pop ( 'schemaform' , None ) blueprint . add_url_rule ( * * create_url_rule ( endpoint , * * options ) ) @ blueprint . route ( '/deposit' ) @ login_required def index ( ) : """List user deposits.""" return render_template ( current_app . config [ 'DEPOSIT_UI_INDEX_TEMPLATE' ] ) @ blueprint . route ( '/deposit/new' ) @ login_required def new ( ) : """Create new deposit.""" deposit_type = request . values . get ( 'type' ) return render_template ( current_app . config [ 'DEPOSIT_UI_NEW_TEMPLATE' ] , record = { '_deposit' : { 'id' : None } } , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , ) return blueprint
6228	def init ( window = None , project = None , timeline = None ) : from demosys . effects . registry import Effect from demosys . scene import camera window . timeline = timeline # Inject attributes into the base Effect class setattr ( Effect , '_window' , window ) setattr ( Effect , '_ctx' , window . ctx ) setattr ( Effect , '_project' , project ) # Set up the default system camera window . sys_camera = camera . SystemCamera ( aspect = window . aspect_ratio , fov = 60.0 , near = 1 , far = 1000 ) setattr ( Effect , '_sys_camera' , window . sys_camera ) print ( "Loading started at" , time . time ( ) ) project . load ( ) # Initialize timer timer_cls = import_string ( settings . TIMER ) window . timer = timer_cls ( ) window . timer . start ( )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 # for numerical reasons we subtract the max logit # (mathematically it doesn't matter!) # otherwise exp(logits) might become too large or too small logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
12423	def load ( fp , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : converter = None output = cls ( ) arraykeys = set ( ) for line in fp : if converter is None : if isinstance ( line , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator key , value = line . strip ( ) . split ( separator , 1 ) keyparts = key . split ( index_separator ) try : index = int ( keyparts [ - 1 ] ) endwithint = True except ValueError : endwithint = False # We do everything in-place to ensure that we maintain order when using # an OrderedDict. if len ( keyparts ) > 1 and endwithint : # If this is an array key basekey = key . rsplit ( index_separator , 1 ) [ 0 ] if basekey not in arraykeys : arraykeys . add ( basekey ) if basekey in output : # If key already exists as non-array, fix it if not isinstance ( output [ basekey ] , dict ) : output [ basekey ] = { - 1 : output [ basekey ] } else : output [ basekey ] = { } output [ basekey ] [ index ] = value else : if key in output and isinstance ( output [ key ] , dict ) : output [ key ] [ - 1 ] = value else : output [ key ] = value # Convert array keys for key in arraykeys : output [ key ] = list_cls ( pair [ 1 ] for pair in sorted ( six . iteritems ( output [ key ] ) ) ) return output
10234	def _get_catalysts_in_reaction ( reaction : Reaction ) -> Set [ BaseAbundance ] : return { reactant for reactant in reaction . reactants if reactant in reaction . products }
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
3583	def _print_tree ( self ) : # This is based on the bluez sample code get-managed-objects.py. objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( " %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( " %s = %s" % ( key , properties [ key ] ) )
184	def coords_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , LineString ) : pass elif isinstance ( other , tuple ) : other = LineString ( [ other ] ) else : other = LineString ( other ) if len ( self . coords ) == 0 and len ( other . coords ) == 0 : return True elif 0 in [ len ( self . coords ) , len ( other . coords ) ] : # only one of the two line strings has no coords return False self_subd = self . subdivide ( points_per_edge ) other_subd = other . subdivide ( points_per_edge ) dist_self2other = self_subd . compute_pointwise_distances ( other_subd ) dist_other2self = other_subd . compute_pointwise_distances ( self_subd ) dist = max ( np . max ( dist_self2other ) , np . max ( dist_other2self ) ) return dist < max_distance
3987	def _move_temp_binary_to_path ( tmp_binary_path ) : # pylint: disable=E1101 binary_path = _get_binary_location ( ) if not binary_path . endswith ( constants . DUSTY_BINARY_NAME ) : raise RuntimeError ( 'Refusing to overwrite binary {}' . format ( binary_path ) ) st = os . stat ( binary_path ) permissions = st . st_mode owner = st . st_uid group = st . st_gid shutil . move ( tmp_binary_path , binary_path ) os . chown ( binary_path , owner , group ) os . chmod ( binary_path , permissions ) return binary_path
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
1438	def update_received_packet ( self , received_pkt_size_bytes ) : self . update_count ( self . RECEIVED_PKT_COUNT ) self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes )
9009	def next_instruction_in_row ( self ) : index = self . index_in_row + 1 if index >= len ( self . row_instructions ) : return None return self . row_instructions [ index ]
13500	def tile ( ) : figs = plt . get_fignums ( ) # Keep track of x, y, size for figures x = 0 y = 0 # maxy = 0 toppad = 21 size = np . array ( [ 0 , 0 ] ) if ( len ( figs ) != 0 ) : fig = plt . figure ( figs [ 0 ] ) screen = fig . canvas . window . get_screen ( ) screenx = screen . get_monitor_geometry ( screen . get_primary_monitor ( ) ) screenx = screenx [ 2 ] fig = plt . figure ( figs [ 0 ] ) fig . canvas . manager . window . move ( x , y ) maxy = np . array ( fig . canvas . manager . window . get_position ( ) ) [ 1 ] size = np . array ( fig . canvas . manager . window . get_size ( ) ) y = maxy x += size [ 0 ] + 1 for fig in figs [ 1 : ] : fig = plt . figure ( fig ) size = np . array ( fig . canvas . manager . window . get_size ( ) ) if ( x + size [ 0 ] > screenx ) : x = 0 y = maxy maxy = y + size [ 1 ] + toppad else : maxy = max ( maxy , y + size [ 1 ] + toppad ) fig . canvas . manager . window . move ( x , y ) x += size [ 0 ] + 1
9238	def user_and_project_from_git ( self , options , arg0 = None , arg1 = None ) : user , project = self . user_project_from_option ( options , arg0 , arg1 ) if user and project : return user , project try : remote = subprocess . check_output ( [ 'git' , 'config' , '--get' , 'remote.{0}.url' . format ( options . git_remote ) ] ) except subprocess . CalledProcessError : return None , None except WindowsError : print ( "git binary not found." ) exit ( 1 ) else : return self . user_project_from_remote ( remote )
12475	def remove_all ( filelist , folder = '' ) : if not folder : for f in filelist : os . remove ( f ) else : for f in filelist : os . remove ( op . join ( folder , f ) )
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : # read the matrix header and array vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) # move on to next field fd . seek ( next_pos ) # pack and return the array if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
4111	def rc2poly ( kr , r0 = None ) : # Initialize the recursion from . levinson import levup p = len ( kr ) #% p is the order of the prediction polynomial. a = numpy . array ( [ 1 , kr [ 0 ] ] ) #% a is a true polynomial. e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) # Continue the recursion for k=2,3,...,p, where p is the order of the # prediction polynomial. for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
7583	def run ( self , ipyclient = None , quiet = False , force = False , block = False , ) : ## stop before trying in raxml if force : for key , oldfile in self . trees : if os . path . exists ( oldfile ) : os . remove ( oldfile ) if os . path . exists ( self . trees . info ) : print ( "Error: set a new name for this job or use Force flag.\nFile exists: {}" . format ( self . trees . info ) ) return ## TODO: add a progress bar tracker here. It could even read it from ## the info file that is being written. ## submit it if not ipyclient : proc = _call_raxml ( self . _command_list ) self . stdout = proc [ 0 ] self . stderr = proc [ 1 ] else : ## find all hosts and submit job to the host with most available engines lbview = ipyclient . load_balanced_view ( ) self . async = lbview . apply ( _call_raxml , self . _command_list ) ## initiate random seed if not quiet : if not ipyclient : ## look for errors if "Overall execution time" not in self . stdout : print ( "Error in raxml run\n" + self . stdout ) else : print ( "job {} finished successfully" . format ( self . params . n ) ) else : print ( "job {} submitted to cluster" . format ( self . params . n ) )
2341	def forward ( self , x ) : self . noise . normal_ ( ) return self . layers ( th . cat ( [ x , self . noise ] , 1 ) )
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
3847	def parse_watermark_notification ( p ) : return WatermarkNotification ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , read_timestamp = from_timestamp ( p . latest_read_timestamp ) , )
2193	def isatty ( self ) : # nocover return ( self . redirect is not None and hasattr ( self . redirect , 'isatty' ) and self . redirect . isatty ( ) )
3988	def parallel_task_queue ( pool_size = multiprocessing . cpu_count ( ) ) : task_queue = TaskQueue ( pool_size ) yield task_queue task_queue . execute ( )
9139	def label ( labels = [ ] , language = 'any' , sortLabel = False ) : if not labels : return None if not language : language = 'und' labels = [ dict_to_label ( l ) for l in labels ] l = False if sortLabel : l = find_best_label_for_type ( labels , language , 'sortLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'prefLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'altLabel' ) if l : return l else : return label ( labels , 'any' , sortLabel ) if language != 'any' else None
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 # start index of diffusion-based populations for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
10537	def find_category ( * * kwargs ) : try : res = _pybossa_req ( 'get' , 'category' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Category ( category ) for category in res ] else : return res except : # pragma: no cover raise
11532	def setup ( self , address , rack = 0 , slot = 1 , port = 102 ) : rack = int ( rack ) slot = int ( slot ) port = int ( port ) address = str ( address ) self . _client = snap7 . client . Client ( ) self . _client . connect ( address , rack , slot , port )
5940	def _build_arg_list ( self , * * kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : # XXX: check flag against allowed values flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] # python-illegal keywords are '_'-quoted if not flag . startswith ( '-' ) : flag = '-' + flag # now flag is guaranteed to start with '-' if value is True : arglist . append ( flag ) # simple command line flag elif value is False : if flag . startswith ( '-no' ) : # negate a negated flag ('noX=False' --> X=True --> -X ... but who uses that?) arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) # gromacs switches booleans by prefixing 'no' elif value is None : pass # ignore flag = None else : try : arglist . extend ( [ flag ] + value ) # option with value list except TypeError : arglist . extend ( [ flag , value ] ) # option with single value return list ( map ( str , arglist ) )
2305	def plot_curves ( i_batch , adv_loss , gen_loss , l1_reg , cols ) : from matplotlib import pyplot as plt if i_batch == 0 : try : ax . clear ( ) ax . plot ( range ( len ( adv_plt ) ) , adv_plt , "r-" , linewidth = 1.5 , markersize = 4 , label = "Discriminator" ) ax . plot ( range ( len ( adv_plt ) ) , gen_plt , "g-" , linewidth = 1.5 , markersize = 4 , label = "Generators" ) ax . plot ( range ( len ( adv_plt ) ) , l1_plt , "b-" , linewidth = 1.5 , markersize = 4 , label = "L1-Regularization" ) plt . legend ( ) adv_plt . append ( adv_loss . cpu ( ) . data [ 0 ] ) gen_plt . append ( gen_loss . cpu ( ) . data [ 0 ] / cols ) l1_plt . append ( l1_reg . cpu ( ) . data [ 0 ] ) plt . pause ( 0.0001 ) except NameError : plt . ion ( ) fig , ax = plt . figure ( ) plt . xlabel ( "Epoch" ) plt . ylabel ( "Losses" ) plt . pause ( 0.0001 ) adv_plt = [ adv_loss . cpu ( ) . data [ 0 ] ] gen_plt = [ gen_loss . cpu ( ) . data [ 0 ] / cols ] l1_plt = [ l1_reg . cpu ( ) . data [ 0 ] ] else : adv_plt . append ( adv_loss . cpu ( ) . data [ 0 ] ) gen_plt . append ( gen_loss . cpu ( ) . data [ 0 ] / cols ) l1_plt . append ( l1_reg . cpu ( ) . data [ 0 ] )
1399	def extract_tmaster ( self , topology ) : tmasterLocation = { "name" : None , "id" : None , "host" : None , "controller_port" : None , "master_port" : None , "stats_port" : None , } if topology . tmaster : tmasterLocation [ "name" ] = topology . tmaster . topology_name tmasterLocation [ "id" ] = topology . tmaster . topology_id tmasterLocation [ "host" ] = topology . tmaster . host tmasterLocation [ "controller_port" ] = topology . tmaster . controller_port tmasterLocation [ "master_port" ] = topology . tmaster . master_port tmasterLocation [ "stats_port" ] = topology . tmaster . stats_port return tmasterLocation
10096	def create_new_locale ( self , template_id , locale , version_name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version_name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . _api_request ( self . TEMPLATES_LOCALES_ENDPOINT % template_id , self . HTTP_POST , payload = payload , timeout = timeout )
9492	def compile_bytecode ( code : list ) -> bytes : bc = b"" for i , op in enumerate ( code ) : try : # Get the bytecode. if isinstance ( op , _PyteOp ) or isinstance ( op , _PyteAugmentedComparator ) : bc_op = op . to_bytes ( bc ) elif isinstance ( op , int ) : bc_op = op . to_bytes ( 1 , byteorder = "little" ) elif isinstance ( op , bytes ) : bc_op = op else : raise CompileError ( "Could not compile code of type {}" . format ( type ( op ) ) ) bc += bc_op except Exception as e : print ( "Fatal compiliation error on operator {i} ({op})." . format ( i = i , op = op ) ) raise e return bc
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 # J/x -> kWh/x result = result / compound . molar_mass # x/mol -> x/kg result = result * mass # x/kg -> x return result
6938	def parallel_update_objectinfo_cpdir ( cpdir , cpglob = 'checkplot-*.pkl*' , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : cplist = sorted ( glob . glob ( os . path . join ( cpdir , cpglob ) ) ) return parallel_update_objectinfo_cplist ( cplist , liststartindex = liststartindex , maxobjects = maxobjects , nworkers = nworkers , fast_mode = fast_mode , findercmap = findercmap , finderconvolve = finderconvolve , deredden_object = deredden_object , custom_bandpasses = custom_bandpasses , gaia_submit_timeout = gaia_submit_timeout , gaia_submit_tries = gaia_submit_tries , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , complete_query_later = complete_query_later , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , maxnumneighbors = maxnumneighbors , plotdpi = plotdpi , findercachedir = findercachedir , verbose = verbose )
5314	def style_string ( string , ansi_style , colormode , nested = False ) : ansi_start_code , ansi_end_code = ansi_style # replace nest placeholders with the current begin style if PY2 : if isinstance ( string , str ) : string = string . decode ( DEFAULT_ENCODING ) string = UNICODE ( string ) . replace ( ansi . NEST_PLACEHOLDER , ansi_start_code ) return '{start_code}{string}{end_code}{nest_ph}' . format ( start_code = ansi_start_code , string = string , end_code = ansi_end_code , nest_ph = ansi . NEST_PLACEHOLDER if nested else '' )
1785	def CMPXCHG ( cpu , dest , src ) : size = dest . size reg_name = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] accumulator = cpu . read_register ( reg_name ) sval = src . read ( ) dval = dest . read ( ) cpu . write_register ( reg_name , dval ) dest . write ( Operators . ITEBV ( size , accumulator == dval , sval , dval ) ) # Affected Flags o..szapc cpu . _calculate_CMP_flags ( size , accumulator - dval , accumulator , dval )
6408	def lmean ( nums ) : if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
4756	def postprocess ( trun ) : plog = [ ] plog . append ( ( "trun" , process_trun ( trun ) ) ) for tsuite in trun [ "testsuites" ] : plog . append ( ( "tsuite" , process_tsuite ( tsuite ) ) ) for tcase in tsuite [ "testcases" ] : plog . append ( ( "tcase" , process_tcase ( tcase ) ) ) for task , success in plog : if not success : cij . err ( "rprtr::postprocess: FAILED for %r" % task ) return sum ( ( success for task , success in plog ) )
9653	def write_shas_to_shastore ( sha_dict ) : if sys . version_info [ 0 ] < 3 : fn_open = open else : fn_open = io . open with fn_open ( ".shastore" , "w" ) as fh : fh . write ( "---\n" ) fh . write ( 'sake version: {}\n' . format ( constants . VERSION ) ) if sha_dict : fh . write ( yaml . dump ( sha_dict ) ) fh . write ( "..." )
828	def getFieldDescription ( self , fieldName ) : # Find which field it's in description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) # Return the offset and width return ( offset , description [ i + 1 ] [ 1 ] - offset )
5848	def get_preferred_credentials ( api_key , site , cred_file = DEFAULT_CITRINATION_CREDENTIALS_FILE ) : profile_api_key , profile_site = get_credentials_from_file ( cred_file ) if api_key is None : api_key = os . environ . get ( citr_env_vars . CITRINATION_API_KEY ) if api_key is None or len ( api_key ) == 0 : api_key = profile_api_key if site is None : site = os . environ . get ( citr_env_vars . CITRINATION_SITE ) if site is None or len ( site ) == 0 : site = profile_site if site is None : site = "https://citrination.com" return api_key , site
5347	def compose_github ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'github_repos' ] ) > 0 ] : if 'github' not in projects [ p ] : projects [ p ] [ 'github' ] = [ ] urls = [ url [ 'url' ] for url in data [ p ] [ 'github_repos' ] if url [ 'url' ] not in projects [ p ] [ 'github' ] ] projects [ p ] [ 'github' ] += urls return projects
7644	def can_convert ( annotation , target_namespace ) : # If we're already in the target namespace, do nothing if annotation . namespace == target_namespace : return True if target_namespace in __CONVERSION__ : # Look for a way to map this namespace to the target for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return True return False
5688	def get_transit_events ( self , start_time_ut = None , end_time_ut = None , route_type = None ) : table_name = self . _get_day_trips_table_name ( ) event_query = "SELECT stop_I, seq, trip_I, route_I, routes.route_id AS route_id, routes.type AS route_type, " "shape_id, day_start_ut+dep_time_ds AS dep_time_ut, day_start_ut+arr_time_ds AS arr_time_ut " "FROM " + table_name + " " "JOIN trips USING(trip_I) " "JOIN routes USING(route_I) " "JOIN stop_times USING(trip_I)" where_clauses = [ ] if end_time_ut : where_clauses . append ( table_name + ".start_time_ut< {end_time_ut}" . format ( end_time_ut = end_time_ut ) ) where_clauses . append ( "dep_time_ut <={end_time_ut}" . format ( end_time_ut = end_time_ut ) ) if start_time_ut : where_clauses . append ( table_name + ".end_time_ut > {start_time_ut}" . format ( start_time_ut = start_time_ut ) ) where_clauses . append ( "arr_time_ut >={start_time_ut}" . format ( start_time_ut = start_time_ut ) ) if route_type is not None : assert route_type in ALL_ROUTE_TYPES where_clauses . append ( "routes.type={route_type}" . format ( route_type = route_type ) ) if len ( where_clauses ) > 0 : event_query += " WHERE " for i , where_clause in enumerate ( where_clauses ) : if i is not 0 : event_query += " AND " event_query += where_clause # ordering is required for later stages event_query += " ORDER BY trip_I, day_start_ut+dep_time_ds;" events_result = pd . read_sql_query ( event_query , self . conn ) # 'filter' results so that only real "events" are taken into account from_indices = numpy . nonzero ( ( events_result [ 'trip_I' ] [ : - 1 ] . values == events_result [ 'trip_I' ] [ 1 : ] . values ) * ( events_result [ 'seq' ] [ : - 1 ] . values < events_result [ 'seq' ] [ 1 : ] . values ) ) [ 0 ] to_indices = from_indices + 1 # these should have same trip_ids assert ( events_result [ 'trip_I' ] [ from_indices ] . values == events_result [ 'trip_I' ] [ to_indices ] . values ) . all ( ) trip_Is = events_result [ 'trip_I' ] [ from_indices ] from_stops = events_result [ 'stop_I' ] [ from_indices ] to_stops = events_result [ 'stop_I' ] [ to_indices ] shape_ids = events_result [ 'shape_id' ] [ from_indices ] dep_times = events_result [ 'dep_time_ut' ] [ from_indices ] arr_times = events_result [ 'arr_time_ut' ] [ to_indices ] route_types = events_result [ 'route_type' ] [ from_indices ] route_ids = events_result [ 'route_id' ] [ from_indices ] route_Is = events_result [ 'route_I' ] [ from_indices ] durations = arr_times . values - dep_times . values assert ( durations >= 0 ) . all ( ) from_seqs = events_result [ 'seq' ] [ from_indices ] to_seqs = events_result [ 'seq' ] [ to_indices ] data_tuples = zip ( from_stops , to_stops , dep_times , arr_times , shape_ids , route_types , route_ids , trip_Is , durations , from_seqs , to_seqs , route_Is ) columns = [ "from_stop_I" , "to_stop_I" , "dep_time_ut" , "arr_time_ut" , "shape_id" , "route_type" , "route_id" , "trip_I" , "duration" , "from_seq" , "to_seq" , "route_I" ] df = pd . DataFrame . from_records ( data_tuples , columns = columns ) return df
13520	def configure ( self , url = None , token = None , test = False ) : if url is None : url = Config . get_value ( "url" ) if token is None : token = Config . get_value ( "token" ) self . server_url = url self . auth_header = { "Authorization" : "Basic {0}" . format ( token ) } self . configured = True if test : self . test_connection ( ) Config . set ( "url" , url ) Config . set ( "token" , token )
1	def nature_cnn ( unscaled_images , * * conv_kwargs ) : scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h3 = conv_to_fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) )
9788	def bookmark ( ctx , username ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
9169	def parse_archive_uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident_hash = path [ - 1 ] ident_hash = unquote ( ident_hash ) return ident_hash
9507	def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
11547	def url ( self ) : if len ( self . drivers ) > 0 : return self . drivers [ 0 ] . url else : return self . _url
1560	def register_metric ( self , name , metric , time_bucket_in_sec ) : collector = self . get_metrics_collector ( ) collector . register_metric ( name , metric , time_bucket_in_sec )
3627	def normalize_cols ( table ) : longest_row_len = max ( [ len ( row ) for row in table ] ) for row in table : while len ( row ) < longest_row_len : row . append ( '' ) return table
1616	def Search ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . search ( s )
12660	def copy ( configfile = '' , destpath = '' , overwrite = False , sub_node = '' ) : log . info ( 'Running {0} {1} {2}' . format ( os . path . basename ( __file__ ) , whoami ( ) , locals ( ) ) ) assert ( os . path . isfile ( configfile ) ) if os . path . exists ( destpath ) : if os . listdir ( destpath ) : raise FolderAlreadyExists ( 'Folder {0} already exists. Please clean ' 'it or change destpath.' . format ( destpath ) ) else : log . info ( 'Creating folder {0}' . format ( destpath ) ) path ( destpath ) . makedirs_p ( ) from boyle . files . file_tree_map import FileTreeMap file_map = FileTreeMap ( ) try : file_map . from_config_file ( configfile ) except Exception as e : raise FileTreeMapError ( str ( e ) ) if sub_node : sub_map = file_map . get_node ( sub_node ) if not sub_map : raise FileTreeMapError ( 'Could not find sub node ' '{0}' . format ( sub_node ) ) file_map . _filetree = { } file_map . _filetree [ sub_node ] = sub_map try : file_map . copy_to ( destpath , overwrite = overwrite ) except Exception as e : raise FileTreeMapError ( str ( e ) )
9078	def make_df_getter ( data_url : str , data_path : str , * * kwargs ) -> Callable [ [ Optional [ str ] , bool , bool ] , pd . DataFrame ] : download_function = make_downloader ( data_url , data_path ) def get_df ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> pd . DataFrame : """Get the data as a pandas DataFrame. :param url: The URL (or file path) to download. :param cache: If true, the data is downloaded to the file system, else it is loaded from the internet :param force_download: If true, overwrites a previously cached file """ if url is None and cache : url = download_function ( force_download = force_download ) return pd . read_csv ( url or data_url , * * kwargs ) return get_df
10557	def login ( self , oauth_filename = "oauth" , uploader_id = None ) : cls_name = type ( self ) . __name__ oauth_cred = os . path . join ( os . path . dirname ( OAUTH_FILEPATH ) , oauth_filename + '.cred' ) try : if not self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) : try : self . api . perform_oauth ( storage_filepath = oauth_cred ) except OSError : logger . exception ( "\nUnable to login with specified oauth code." ) self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) except ( OSError , ValueError ) : logger . exception ( "{} authentication failed." . format ( cls_name ) ) return False if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
12166	def remove_listener ( self , event , listener ) : with contextlib . suppress ( ValueError ) : self . _listeners [ event ] . remove ( listener ) return True with contextlib . suppress ( ValueError ) : self . _once [ event ] . remove ( listener ) return True return False
12028	def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
7042	def stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = False ) : ndet = len ( fmags ) if ndet > 9 : # get the median and ndet medmag = npmedian ( fmags ) # get the stetson index elements delta_prefactor = ( ndet / ( ndet - 1 ) ) sigma_i = delta_prefactor * ( fmags - medmag ) / ferrs # Nicole's clever trick to advance indices by 1 and do x_i*x_(i+1) sigma_j = nproll ( sigma_i , 1 ) if weightbytimediff : difft = npdiff ( ftimes ) deltat = npmedian ( difft ) weights_i = npexp ( - difft / deltat ) products = ( weights_i * sigma_i [ 1 : ] * sigma_j [ 1 : ] ) else : # ignore first elem since it's actually x_0*x_n products = ( sigma_i * sigma_j ) [ 1 : ] stetsonj = ( npsum ( npsign ( products ) * npsqrt ( npabs ( products ) ) ) ) / ndet return stetsonj else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate stetson J index' ) return npnan
4765	def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
1899	def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : # if True check if constraints are feasible self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
7797	def _register_server_authenticator ( klass , name ) : # pylint: disable-msg=W0212 SERVER_MECHANISMS_D [ name ] = klass items = sorted ( SERVER_MECHANISMS_D . items ( ) , key = _key_func , reverse = True ) SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items ] SECURE_SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items if v . _pyxmpp_sasl_secure ]
4773	def contains_only ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . _err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
4669	def decrypt ( encrypted_privkey , passphrase ) : d = unhexlify ( base58decode ( encrypted_privkey ) ) d = d [ 2 : ] # remove trailing 0x01 and 0x42 flagbyte = d [ 0 : 1 ] # get flag byte d = d [ 1 : ] # get payload assert flagbyte == b"\xc0" , "Flagbyte has to be 0xc0" salt = d [ 0 : 4 ] d = d [ 4 : - 4 ] if SCRYPT_MODULE == "scrypt" : # pragma: no cover key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : # pragma: no cover key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) # pragma: no cover derivedhalf1 = key [ 0 : 32 ] derivedhalf2 = key [ 32 : 64 ] encryptedhalf1 = d [ 0 : 16 ] encryptedhalf2 = d [ 16 : 32 ] aes = AES . new ( derivedhalf2 , AES . MODE_ECB ) decryptedhalf2 = aes . decrypt ( encryptedhalf2 ) decryptedhalf1 = aes . decrypt ( encryptedhalf1 ) privraw = decryptedhalf1 + decryptedhalf2 privraw = "%064x" % ( int ( hexlify ( privraw ) , 16 ) ^ int ( hexlify ( derivedhalf1 ) , 16 ) ) wif = Base58 ( privraw ) """ Verify Salt """ privkey = PrivateKey ( format ( wif , "wif" ) ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) saltverify = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if saltverify != salt : # pragma: no cover raise SaltException ( "checksum verification failed! Password may be incorrect." ) return wif
3291	def get_href ( self ) : # Nautilus chokes, if href encodes '(' as '%28' # So we don't encode 'extra' and 'safe' characters (see rfc2068 3.2.1) safe = "/" + "!*'()," + "$-_|." return compat . quote ( self . provider . mount_path + self . provider . share_path + self . get_preferred_path ( ) , safe = safe , )
2280	def create_graph_from_data ( self , data , * * kwargs ) : # Building setup w/ arguments. self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_ccdr ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
2872	def all_info_files ( self ) : try : for info_file in list_files_in_dir ( self . info_dir ) : if not os . path . basename ( info_file ) . endswith ( '.trashinfo' ) : self . on_non_trashinfo_found ( ) else : yield info_file except OSError : # when directory does not exist pass
5638	def _temporal_distance_cdf ( self ) : distance_split_points = set ( ) for block in self . _profile_blocks : if block . distance_start != float ( 'inf' ) : distance_split_points . add ( block . distance_end ) distance_split_points . add ( block . distance_start ) distance_split_points_ordered = numpy . array ( sorted ( list ( distance_split_points ) ) ) temporal_distance_split_widths = distance_split_points_ordered [ 1 : ] - distance_split_points_ordered [ : - 1 ] trip_counts = numpy . zeros ( len ( temporal_distance_split_widths ) ) delta_peaks = defaultdict ( lambda : 0 ) for block in self . _profile_blocks : if block . distance_start == block . distance_end : delta_peaks [ block . distance_end ] += block . width ( ) else : start_index = numpy . searchsorted ( distance_split_points_ordered , block . distance_end ) end_index = numpy . searchsorted ( distance_split_points_ordered , block . distance_start ) trip_counts [ start_index : end_index ] += 1 unnormalized_cdf = numpy . array ( [ 0 ] + list ( numpy . cumsum ( temporal_distance_split_widths * trip_counts ) ) ) if not ( numpy . isclose ( [ unnormalized_cdf [ - 1 ] ] , [ self . _end_time - self . _start_time - sum ( delta_peaks . values ( ) ) ] , atol = 1E-4 ) . all ( ) ) : print ( unnormalized_cdf [ - 1 ] , self . _end_time - self . _start_time - sum ( delta_peaks . values ( ) ) ) raise RuntimeError ( "Something went wrong with cdf computation!" ) if len ( delta_peaks ) > 0 : for peak in delta_peaks . keys ( ) : if peak == float ( 'inf' ) : continue index = numpy . nonzero ( distance_split_points_ordered == peak ) [ 0 ] [ 0 ] unnormalized_cdf = numpy . insert ( unnormalized_cdf , index , unnormalized_cdf [ index ] ) distance_split_points_ordered = numpy . insert ( distance_split_points_ordered , index , distance_split_points_ordered [ index ] ) # walk_waiting_time_fraction = walk_total_time / (self.end_time_dep - self.start_time_dep) unnormalized_cdf [ ( index + 1 ) : ] = unnormalized_cdf [ ( index + 1 ) : ] + delta_peaks [ peak ] norm_cdf = unnormalized_cdf / ( unnormalized_cdf [ - 1 ] + delta_peaks [ float ( 'inf' ) ] ) return distance_split_points_ordered , norm_cdf
6949	def jhk_to_sdssr ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSR_JHK , SDSSR_JH , SDSSR_JK , SDSSR_HK , SDSSR_J , SDSSR_H , SDSSR_K )
6733	def str_to_list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string_types ) : raise NotImplementedError ( 'Unknown type: %s' % type ( s ) ) return [ _ . strip ( ) . lower ( ) for _ in ( s or '' ) . split ( ',' ) if _ . strip ( ) ]
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
6391	def encode ( self , word , max_length = 8 ) : # Lowercase input & filter unknown characters word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '÷' # Perform initial eudex coding of each character values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] # Right-shift by one to determine if second instance should be skipped shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) # Add padding after first character & trim beyond max_length values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) # Combine individual character values into eudex hash hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
1412	def _get_topologies_with_watch ( self , callback , isWatching ) : path = self . get_topologies_path ( ) if isWatching : LOG . info ( "Adding children watch for path: " + path ) # pylint: disable=unused-variable @ self . client . ChildrenWatch ( path ) def watch_topologies ( topologies ) : """ callback to watch topologies """ callback ( topologies ) # Returning False will result in no future watches # being triggered. If isWatching is True, then # the future watches will be triggered. return isWatching
13144	def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
11621	def set_script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise IllegalInput ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : # n = -1 is the default script .. self . curr_script = n self . delta = n * DELTA return
2840	def write_gpio ( self , gpio = None ) : if gpio is not None : self . gpio = gpio self . _device . writeList ( self . GPIO , self . gpio )
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
1076	def _ymd2ord ( year , month , day ) : assert 1 <= month <= 12 , 'month must be in 1..12' dim = _days_in_month ( year , month ) assert 1 <= day <= dim , ( 'day must be in 1..%d' % dim ) return ( _days_before_year ( year ) + _days_before_month ( year , month ) + day )
9165	def expandvars_dict ( settings ) : return dict ( ( key , os . path . expandvars ( value ) ) for key , value in settings . iteritems ( ) )
7425	def bedtools_merge ( data , sample ) : LOGGER . info ( "Entering bedtools_merge: %s" , sample . name ) mappedreads = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) ## command to call `bedtools bamtobed`, and pipe output to stdout ## Usage: bedtools bamtobed [OPTIONS] -i <bam> ## Usage: bedtools merge [OPTIONS] -i <bam> cmd1 = [ ipyrad . bins . bedtools , "bamtobed" , "-i" , mappedreads ] cmd2 = [ ipyrad . bins . bedtools , "merge" , "-i" , "-" ] ## If PE the -d flag to tell bedtools how far apart to allow mate pairs. ## If SE the -d flag is negative, specifying that SE reads need to ## overlap by at least a specific number of bp. This prevents the ## stairstep syndrome when a + and - read are both extending from ## the same cutsite. Passing a negative number to `merge -d` gets this done. if 'pair' in data . paramsdict [ "datatype" ] : check_insert_size ( data , sample ) #cmd2.insert(2, str(data._hackersonly["max_inner_mate_distance"])) cmd2 . insert ( 2 , str ( data . _hackersonly [ "max_inner_mate_distance" ] ) ) cmd2 . insert ( 2 , "-d" ) else : cmd2 . insert ( 2 , str ( - 1 * data . _hackersonly [ "min_SE_refmap_overlap" ] ) ) cmd2 . insert ( 2 , "-d" ) ## pipe output from bamtobed into merge LOGGER . info ( "stdv: bedtools merge cmds: %s %s" , cmd1 , cmd2 ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) result = proc2 . communicate ( ) [ 0 ] proc1 . stdout . close ( ) ## check for errors and do cleanup if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , result ) ## Write the bedfile out, because it's useful sometimes. if os . path . exists ( ipyrad . __debugflag__ ) : with open ( os . path . join ( data . dirs . refmapping , sample . name + ".bed" ) , 'w' ) as outfile : outfile . write ( result ) ## Report the number of regions we're returning nregions = len ( result . strip ( ) . split ( "\n" ) ) LOGGER . info ( "bedtools_merge: Got # regions: %s" , nregions ) return result
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : # f is the name of a static class method on the datatype. f = getattr ( datatype , f , None ) return f and f ( desc ) # Automatically load strings that look like JSON or Yaml filenames. desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' # This is because it's the "drivers" directory, whereas # the others are animation, control, layout, project # without the s. TODO: rename drivers/ to driver/ in v4 cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
3333	def dynamic_instantiate_middleware ( name , args , expand = None ) : def _expand ( v ) : """Replace some string templates with defined values.""" if expand and compat . is_basestring ( v ) and v . lower ( ) in expand : return expand [ v ] return v try : the_class = dynamic_import_class ( name ) inst = None if type ( args ) in ( tuple , list ) : args = tuple ( map ( _expand , args ) ) inst = the_class ( * args ) else : assert type ( args ) is dict args = { k : _expand ( v ) for k , v in args . items ( ) } inst = the_class ( * * args ) _logger . debug ( "Instantiate {}({}) => {}" . format ( name , args , inst ) ) except Exception : _logger . exception ( "ERROR: Instantiate {}({}) => {}" . format ( name , args , inst ) ) return inst
12678	def unescape ( escaped , escape_char = ESCAPE_CHAR ) : if isinstance ( escaped , bytes ) : # always work on text escaped = escaped . decode ( 'utf8' ) escape_pat = re . compile ( re . escape ( escape_char ) . encode ( 'utf8' ) + b'([a-z0-9]{2})' , re . IGNORECASE ) buf = escape_pat . subn ( _unescape_char , escaped . encode ( 'utf8' ) ) [ 0 ] return buf . decode ( 'utf8' )
10766	def get_poll ( self , arg , * , request_policy = None ) : if isinstance ( arg , str ) : # Maybe we received an url to parse match = self . _url_re . match ( arg ) if match : arg = match . group ( 'id' ) return self . _http_client . get ( '{}/{}' . format ( self . _POLLS , arg ) , request_policy = request_policy , cls = strawpoll . Poll )
2889	def parse_node ( self ) : try : self . task = self . create_task ( ) self . task . documentation = self . parser . _parse_documentation ( self . node , xpath = self . xpath , task_parser = self ) boundary_event_nodes = self . process_xpath ( './/bpmn:boundaryEvent[@attachedToRef="%s"]' % self . get_id ( ) ) if boundary_event_nodes : parent_task = _BoundaryEventParent ( self . spec , '%s.BoundaryEventParent' % self . get_id ( ) , self . task , lane = self . task . lane ) self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = parent_task parent_task . connect_outgoing ( self . task , '%s.FromBoundaryEventParent' % self . get_id ( ) , None , None ) for boundary_event in boundary_event_nodes : b = self . process_parser . parse_node ( boundary_event ) parent_task . connect_outgoing ( b , '%s.FromBoundaryEventParent' % boundary_event . get ( 'id' ) , None , None ) else : self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process_xpath ( './/bpmn:sequenceFlow[@sourceRef="%s"]' % self . get_id ( ) ) if len ( outgoing ) > 1 and not self . handles_multiple_outgoing ( ) : raise ValidationException ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process_parser . filename ) for sequence_flow in outgoing : target_ref = sequence_flow . get ( 'targetRef' ) target_node = one ( self . process_xpath ( './/*[@id="%s"]' % target_ref ) ) c = self . process_parser . parse_node ( target_node ) children . append ( ( c , target_node , sequence_flow ) ) if children : default_outgoing = self . node . get ( 'default' ) if not default_outgoing : ( c , target_node , sequence_flow ) = children [ 0 ] default_outgoing = sequence_flow . get ( 'id' ) for ( c , target_node , sequence_flow ) in children : self . connect_outgoing ( c , target_node , sequence_flow , sequence_flow . get ( 'id' ) == default_outgoing ) return parent_task if boundary_event_nodes else self . task except ValidationException : raise except Exception as ex : exc_info = sys . exc_info ( ) tb = "" . join ( traceback . format_exception ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise ValidationException ( "%r" % ( ex ) , node = self . node , filename = self . process_parser . filename )
8553	def delete_ipblock ( self , ipblock_id ) : response = self . _perform_request ( url = '/ipblocks/' + ipblock_id , method = 'DELETE' ) return response
12874	def satisfies ( guard ) : i = peek ( ) if ( i is EndOfFile ) or ( not guard ( i ) ) : fail ( [ "<satisfies predicate " + _fun_to_str ( guard ) + ">" ] ) next ( ) return i
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : # chrome <=55 cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : # chrome >=56 cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
1531	def get_pplan ( self , topologyName , callback = None ) : if callback : self . pplan_watchers [ topologyName ] . append ( callback ) else : pplan_path = self . get_pplan_path ( topologyName ) with open ( pplan_path ) as f : data = f . read ( ) pplan = PhysicalPlan ( ) pplan . ParseFromString ( data ) return pplan
2797	def transfer ( self , new_region_slug ) : return self . get_data ( "images/%s/actions/" % self . id , type = POST , params = { "type" : "transfer" , "region" : new_region_slug } )
12020	def fasta_dict_to_file ( fasta_dict , fasta_file , line_char_limit = None ) : fasta_fp = fasta_file if isinstance ( fasta_file , str ) : fasta_fp = open ( fasta_file , 'wb' ) for key in fasta_dict : seq = fasta_dict [ key ] [ 'seq' ] if line_char_limit : seq = '\n' . join ( [ seq [ i : i + line_char_limit ] for i in range ( 0 , len ( seq ) , line_char_limit ) ] ) fasta_fp . write ( u'{0:s}\n{1:s}\n' . format ( fasta_dict [ key ] [ 'header' ] , seq ) )
5831	def update ( self , id , configuration , name , description ) : data = { "configuration" : configuration , "name" : name , "description" : description } failure_message = "Dataview creation failed" self . _patch_json ( 'v1/data_views/' + id , data , failure_message = failure_message )
2276	def _win32_dir ( path , star = '' ) : from ubelt import util_cmd import re wrapper = 'cmd /S /C "{}"' # the /S will preserve all inner quotes command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util_cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) # parse the output of dir to get some info # Remove header and footer lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type_or_size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) # if type is a junction then name will also contain the linked loc if name == '.' or name == '..' : continue if type_or_size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : # colons cannot be in path names, so use that to find where # the name ends pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type_or_size , name , pointed else : yield type_or_size , name , None
8643	def post_track ( session , user_id , project_id , latitude , longitude ) : tracking_data = { 'user_id' : user_id , 'project_id' : project_id , 'track_point' : { 'latitude' : latitude , 'longitude' : longitude } } # POST /api/projects/0.1/tracks/ response = make_post_request ( session , 'tracks' , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
2389	def f7 ( seq ) : seen = set ( ) seen_add = seen . add return [ x for x in seq if x not in seen and not seen_add ( x ) ]
11107	def walk_files_relative_path ( self , relativePath = "" ) : def walk_files ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) files = dict . __getitem__ ( directory , 'files' ) for f in sorted ( files ) : yield os . path . join ( relativePath , f ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = directories . __getitem__ ( k ) for e in walk_files ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_files ( dir , relativePath = '' )
3115	def _get_flow_for_token ( csrf_token , request ) : flow_pickle = request . session . get ( _FLOW_KEY . format ( csrf_token ) , None ) return None if flow_pickle is None else jsonpickle . decode ( flow_pickle )
10182	def _aggregations_process ( aggregation_types = None , start_date = None , end_date = None , update_bookmark = False , eager = False ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) if eager : aggregate_events . apply ( ( aggregation_types , ) , dict ( start_date = start_date , end_date = end_date , update_bookmark = update_bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate_events . delay ( aggregation_types , start_date = start_date , end_date = end_date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )
460	def evaluation ( y_test = None , y_predict = None , n_classes = None ) : c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) acc = accuracy_score ( y_test , y_predict ) tl . logging . info ( 'confusion matrix: \n%s' % c_mat ) tl . logging . info ( 'f1-score : %s' % f1 ) tl . logging . info ( 'f1-score(macro) : %f' % f1_macro ) # same output with > f1_score(y_true, y_pred, average='macro') tl . logging . info ( 'accuracy-score : %f' % acc ) return c_mat , f1 , acc , f1_macro
2991	def symbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( symbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
2229	def hash_file ( fpath , blocksize = 65536 , stride = 1 , hasher = NoParam , hashlen = NoParam , base = NoParam ) : base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) with open ( fpath , 'rb' ) as file : buf = file . read ( blocksize ) if stride > 1 : # skip blocks when stride is greater than 1 while len ( buf ) > 0 : hasher . update ( buf ) file . seek ( blocksize * ( stride - 1 ) , 1 ) buf = file . read ( blocksize ) else : # otherwise hash the entire file while len ( buf ) > 0 : hasher . update ( buf ) buf = file . read ( blocksize ) # Get the hashed representation text = _digest_hasher ( hasher , hashlen , base ) return text
7952	def wait_for_writability ( self ) : with self . lock : while True : if self . _state in ( "closing" , "closed" , "aborted" ) : return False if self . _socket and bool ( self . _write_queue ) : return True self . _write_queue_cond . wait ( ) return False
12780	def get_users ( self , sort = True ) : self . _load ( ) if sort : self . users . sort ( key = operator . itemgetter ( "name" ) ) return self . users
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
3445	def add_linear_obj ( model ) : coefs = { } for rxn in find_boundary_types ( model , "exchange" ) : export = len ( rxn . reactants ) == 1 if export : coefs [ rxn . reverse_variable ] = 1 else : coefs [ rxn . forward_variable ] = 1 model . objective . set_linear_coefficients ( coefs ) model . objective . direction = "min"
6165	def sqrt_rc_imp ( Ns , alpha , M = 6 ) : # Design the filter n = np . arange ( - M * Ns , M * Ns + 1 ) b = np . zeros ( len ( n ) ) Ns *= 1.0 a = alpha for i in range ( len ( n ) ) : if abs ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) <= np . finfo ( np . float ) . eps / 2 : b [ i ] = 1 / 2. * ( ( 1 + a ) * np . sin ( ( 1 + a ) * np . pi / ( 4. * a ) ) - ( 1 - a ) * np . cos ( ( 1 - a ) * np . pi / ( 4. * a ) ) + ( 4 * a ) / np . pi * np . sin ( ( 1 - a ) * np . pi / ( 4. * a ) ) ) else : b [ i ] = 4 * a / ( np . pi * ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) ) b [ i ] = b [ i ] * ( np . cos ( ( 1 + a ) * np . pi * n [ i ] / Ns ) + np . sinc ( ( 1 - a ) * n [ i ] / Ns ) * ( 1 - a ) * np . pi / ( 4. * a ) ) return b
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
13052	def nmap ( nmap_args , ips ) : config = Config ( ) arguments = [ 'nmap' , '-Pn' ] arguments . extend ( ips ) arguments . extend ( nmap_args ) output_file = '' now = datetime . datetime . now ( ) if not '-oA' in nmap_args : output_name = 'nmap_jackal_{}' . format ( now . strftime ( "%Y-%m-%d %H:%M" ) ) path_name = os . path . join ( config . get ( 'nmap' , 'directory' ) , output_name ) print_notification ( "Writing output of nmap to {}" . format ( path_name ) ) if not os . path . exists ( config . get ( 'nmap' , 'directory' ) ) : os . makedirs ( config . get ( 'nmap' , 'directory' ) ) output_file = path_name + '.xml' arguments . extend ( [ '-oA' , path_name ] ) else : output_file = nmap_args [ nmap_args . index ( '-oA' ) + 1 ] + '.xml' print_notification ( "Starting nmap" ) subprocess . call ( arguments ) with open ( output_file , 'r' ) as f : return f . read ( )
11356	def escape_for_xml ( data , tags_to_keep = None ) : data = re . sub ( "&" , "&amp;" , data ) if tags_to_keep : data = re . sub ( r"(<)(?![\/]?({0})\b)" . format ( "|" . join ( tags_to_keep ) ) , '&lt;' , data ) else : data = re . sub ( "<" , "&lt;" , data ) return data
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
2960	def __base_state ( self , containers ) : return dict ( blockade_id = self . _blockade_id , containers = containers , version = self . _state_version )
8936	def fail ( message , exitcode = 1 ) : sys . stderr . write ( 'ERROR: {}\n' . format ( message ) ) sys . stderr . flush ( ) sys . exit ( exitcode )
8637	def award_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'award' } # POST /api/projects/0.1/bids/{bid_id}/?action=award endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAwardedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) # Copy "dj_db_*" into "db_*". for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) # Optionally set any root logins needed for administrative commands. self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
11005	def _req ( self , url , method = 'GET' , * * kw ) : send = requests . post if method == 'POST' else requests . get try : r = send ( url , headers = self . _token_header ( ) , timeout = self . settings [ 'timeout' ] , * * kw ) except requests . exceptions . Timeout : raise ApiError ( 'Request timed out (%s seconds)' % self . settings [ 'timeout' ] ) try : json = r . json ( ) except ValueError : raise ApiError ( 'Received not JSON response from API' ) if json . get ( 'status' ) != 'ok' : raise ApiError ( 'API error: received unexpected json from API: %s' % json ) return json
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) # Defined Flags: szp cpu . _calculate_logic_flags ( dest . size , res )
10397	def remove_random_edge_until_has_leaves ( self ) -> None : while True : leaves = set ( self . iter_leaves ( ) ) if leaves : return self . remove_random_edge ( )
12811	def lineReceived ( self , line ) : while self . _in_header : if line : self . _headers . append ( line ) else : http , status , message = self . _headers [ 0 ] . split ( " " , 2 ) status = int ( status ) if status == 200 : self . factory . get_stream ( ) . connected ( ) else : self . factory . continueTrying = 0 self . transport . loseConnection ( ) self . factory . get_stream ( ) . disconnected ( RuntimeError ( status , message ) ) return self . _in_header = False break else : try : self . _len_expected = int ( line , 16 ) self . setRawMode ( ) except : pass
10046	def extract_actions_from_class ( record_class ) : for name in dir ( record_class ) : method = getattr ( record_class , name , None ) if method and getattr ( method , '__deposit_action__' , False ) : yield method . __name__
7209	def cancel ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot cancel.' ) if self . batch_values : self . workflow . batch_workflow_cancel ( self . id ) else : self . workflow . cancel ( self . id )
1836	def JG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
2063	def is_declared ( self , expression_var ) : if not isinstance ( expression_var , Variable ) : raise ValueError ( f'Expression must be a Variable (not a {type(expression_var)})' ) return any ( expression_var is x for x in self . get_declared_variables ( ) )
3030	def _extract_id_token ( id_token ) : if type ( id_token ) == bytes : segments = id_token . split ( b'.' ) else : segments = id_token . split ( u'.' ) if len ( segments ) != 3 : raise VerifyJwtTokenError ( 'Wrong number of segments in token: {0}' . format ( id_token ) ) return json . loads ( _helpers . _from_bytes ( _helpers . _urlsafe_b64decode ( segments [ 1 ] ) ) )
6614	def receive_one ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive_one ( )
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
7370	def permission_check ( data , command_permissions , command = None , permissions = None ) : if permissions : pass elif command : if hasattr ( command , 'permissions' ) : permissions = command . permissions else : return True # true if no permission is required else : msg = "{name} must be called with command or permissions argument" raise RuntimeError ( msg . format ( name = "_permission_check" ) ) return any ( data [ 'sender' ] [ 'id' ] in command_permissions [ permission ] for permission in permissions if permission in command_permissions )
9779	def whoami ( ) : try : user = PolyaxonClient ( ) . auth . get_user ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\nUsername: {username}, Email: {email}\n" . format ( * * user . to_dict ( ) ) )
10111	def iterrows ( lines_or_file , namedtuples = False , dicts = False , encoding = 'utf-8' , * * kw ) : if namedtuples and dicts : raise ValueError ( 'either namedtuples or dicts can be chosen as output format' ) elif namedtuples : _reader = NamedTupleReader elif dicts : _reader = UnicodeDictReader else : _reader = UnicodeReader with _reader ( lines_or_file , encoding = encoding , * * fix_kw ( kw ) ) as r : for item in r : yield item
11853	def add_edge ( self , edge ) : start , end , lhs , found , expects = edge if edge not in self . chart [ end ] : self . chart [ end ] . append ( edge ) if self . trace : print '%10s: added %s' % ( caller ( 2 ) , edge ) if not expects : self . extender ( edge ) else : self . predictor ( edge )
9317	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( * * response )
8563	def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE' ) return response
10868	def j2 ( x ) : to_return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to_return [ x == 0 ] = 0 return to_return
11752	def compute_precedence ( terminals , productions , precedence_levels ) : precedence = collections . OrderedDict ( ) for terminal in terminals : precedence [ terminal ] = DEFAULT_PREC level_precs = range ( len ( precedence_levels ) , 0 , - 1 ) for i , level in zip ( level_precs , precedence_levels ) : assoc = level [ 0 ] for symbol in level [ 1 : ] : precedence [ symbol ] = ( assoc , i ) for production , prec_symbol in productions : if prec_symbol is None : prod_terminals = [ symbol for symbol in production . rhs if symbol in terminals ] or [ None ] precedence [ production ] = precedence . get ( prod_terminals [ - 1 ] , DEFAULT_PREC ) else : precedence [ production ] = precedence . get ( prec_symbol , DEFAULT_PREC ) return precedence
3645	def tradepileDelete ( self , trade_id ) : # item_id instead of trade_id? method = 'DELETE' url = 'trade/%s' % trade_id self . __request__ ( method , url ) # returns nothing # TODO: validate status code return True
5318	def format ( self , string , * args , * * kwargs ) : return string . format ( c = self , * args , * * kwargs )
4938	def logo_path ( instance , filename ) : extension = os . path . splitext ( filename ) [ 1 ] . lower ( ) instance_id = str ( instance . id ) fullname = os . path . join ( "enterprise/branding/" , instance_id , instance_id + "_logo" + extension ) if default_storage . exists ( fullname ) : default_storage . delete ( fullname ) return fullname
13545	def get_user ( self , user_id ) : url = "/2/users/%s" % user_id return self . user_from_json ( self . _get_resource ( url ) [ "user" ] )
8673	def purge_stash ( force , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) # Maybe we should verify that the list is empty # afterwards? click . echo ( 'Purge complete!' ) except GhostError as ex : sys . exit ( ex )
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
13642	def check_confirmations_or_resend ( self , use_open_peers = False , * * kw ) : if self . confirmations ( ) == 0 : self . send ( use_open_peers , * * kw )
6096	def voronoi_sub_to_pix_from_grids_and_geometry ( sub_grid , regular_to_nearest_pix , sub_to_regular , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : sub_to_pix = np . zeros ( ( sub_grid . shape [ 0 ] ) ) for sub_index in range ( sub_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ sub_to_regular [ sub_index ] ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( sub_grid [ sub_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( sub_grid [ sub_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_to_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( sub_grid [ sub_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( sub_grid [ sub_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_to_neighbor : closest_separation_from_pix_to_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_to_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : sub_to_pix [ sub_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return sub_to_pix
2788	def resize ( self , size_gigabytes , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "resize" , "size_gigabytes" : size_gigabytes , "region" : region } )
8234	def complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) # A contrasting color: much darker or lighter than the original. c = clr . copy ( ) if clr . brightness > 0.4 : c . brightness = 0.1 + c . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) # A soft supporting color: lighter and less saturated. c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.3 colors . append ( c ) # A contrasting complement: very dark or very light. clr = clr . complement c = clr . copy ( ) if clr . brightness > 0.3 : c . brightness = 0.1 + clr . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) # The complement and a light supporting variant. colors . append ( clr ) c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.25 colors . append ( c ) return colors
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
52	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) else : keypoints = [ kp . project ( self . shape , shape ) for kp in self . keypoints ] return self . deepcopy ( keypoints , shape )
269	def detect_intraday ( positions , transactions , threshold = 0.25 ) : daily_txn = transactions . copy ( ) daily_txn . index = daily_txn . index . date txn_count = daily_txn . groupby ( level = 0 ) . symbol . nunique ( ) . sum ( ) daily_pos = positions . drop ( 'cash' , axis = 1 ) . replace ( 0 , np . nan ) return daily_pos . count ( axis = 1 ) . sum ( ) / txn_count < threshold
6348	def _apply_final_rules ( self , phonetic , final_rules , language_arg , strip ) : # optimization to save time if not final_rules : return phonetic # expand the result phonetic = self . _expand_alternates ( phonetic ) phonetic_array = phonetic . split ( '|' ) for k in range ( len ( phonetic_array ) ) : phonetic = phonetic_array [ k ] phonetic2 = '' phoneticx = self . _normalize_lang_attrs ( phonetic , True ) i = 0 while i < len ( phonetic ) : found = False if phonetic [ i ] == '[' : # skip over language attribute attrib_start = i i += 1 while True : if phonetic [ i ] == ']' : i += 1 phonetic2 += phonetic [ attrib_start : i ] break i += 1 continue for rule in final_rules : pattern = rule [ _PATTERN_POS ] pattern_length = len ( pattern ) lcontext = rule [ _LCONTEXT_POS ] rcontext = rule [ _RCONTEXT_POS ] right = '^' + rcontext left = lcontext + '$' # check to see if next sequence in phonetic matches the # string in the rule if ( pattern_length > len ( phoneticx ) - i ) or phoneticx [ i : i + pattern_length ] != pattern : continue # check that right context is satisfied if rcontext != '' : if not search ( right , phoneticx [ i + pattern_length : ] ) : continue # check that left context is satisfied if lcontext != '' : if not search ( left , phoneticx [ : i ] ) : continue # check for incompatible attributes candidate = self . _apply_rule_if_compat ( phonetic2 , rule [ _PHONETIC_POS ] , language_arg ) # The below condition shouldn't ever be false if candidate is not None : # pragma: no branch phonetic2 = candidate found = True break if not found : # character in name for which there is no substitution in # the table phonetic2 += phonetic [ i ] pattern_length = 1 i += pattern_length phonetic_array [ k ] = self . _expand_alternates ( phonetic2 ) phonetic = '|' . join ( phonetic_array ) if strip : phonetic = self . _normalize_lang_attrs ( phonetic , True ) if '|' in phonetic : phonetic = '(' + self . _remove_dupes ( phonetic ) + ')' return phonetic
3228	def gce_list_aggregated ( service = None , key_name = 'name' , * * kwargs ) : resp_list = [ ] req = service . aggregatedList ( * * kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key_name in item : resp_list . extend ( item [ key_name ] ) req = service . aggregatedList_next ( previous_request = req , previous_response = resp ) return resp_list
7804	def verify_server ( self , server_name , srv_type = 'xmpp-client' ) : server_jid = JID ( server_name ) if "XmppAddr" not in self . alt_names and "DNS" not in self . alt_names and "SRV" not in self . alt_names : return self . verify_jid_against_common_name ( server_jid ) names = [ name for name in self . alt_names . get ( "DNS" , [ ] ) if not name . startswith ( u"*." ) ] names += self . alt_names . get ( "XmppAddr" , [ ] ) for name in names : logger . debug ( "checking {0!r} against {1!r}" . format ( server_jid , name ) ) try : jid = JID ( name ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_jid : logger . debug ( "Match!" ) return True if srv_type and self . verify_jid_against_srv_name ( server_jid , srv_type ) : return True wildcards = [ name [ 2 : ] for name in self . alt_names . get ( "DNS" , [ ] ) if name . startswith ( "*." ) ] if not wildcards or not "." in server_jid . domain : return False logger . debug ( "checking {0!r} against wildcard domains: {1!r}" . format ( server_jid , wildcards ) ) server_domain = JID ( domain = server_jid . domain . split ( "." , 1 ) [ 1 ] ) for domain in wildcards : logger . debug ( "checking {0!r} against {1!r}" . format ( server_domain , domain ) ) try : jid = JID ( domain ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_domain : logger . debug ( "Match!" ) return True return False
4169	def zpk2tf ( z , p , k ) : import scipy . signal b , a = scipy . signal . zpk2tf ( z , p , k ) return b , a
2699	def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
9121	def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) # set the message dropbox . message = data . get ( 'message' ) # recognize submission from watchdog if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) # a non-js client might have uploaded an attachment via the form's fileupload field: if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) # now we can call the process method dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
1920	def decree ( cls , path , concrete_start = '' , * * kwargs ) : try : return cls ( _make_decree ( path , concrete_start ) , * * kwargs ) except KeyError : # FIXME(mark) magic parsing for DECREE should raise better error raise Exception ( f'Invalid binary: {path}' )
10013	def parse_env_config ( config , env_name ) : all_env = get ( config , 'app.all_environments' , { } ) env = get ( config , 'app.environments.' + str ( env_name ) , { } ) return merge_dict ( all_env , env )
6263	def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
13674	def add_directory ( self , * args , * * kwargs ) : exc = kwargs . get ( 'exclusions' , None ) for path in args : self . files . append ( DirectoryPath ( path , self , exclusions = exc ) )
12409	def package_info ( cls , package ) : if package not in cls . package_info_cache : package_json_url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . getLogger ( 'requests' ) . setLevel ( logging . WARN ) response = requests . get ( package_json_url ) response . raise_for_status ( ) cls . package_info_cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package_json_url , e ) cls . package_info_cache [ package ] = None return cls . package_info_cache [ package ]
2715	def create ( self , * * kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get_data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
12264	def _send_file_internal ( self , * args , * * kwargs ) : super ( Key , self ) . _send_file_internal ( * args , * * kwargs ) mimicdb . backend . sadd ( tpl . bucket % self . bucket . name , self . name ) mimicdb . backend . hmset ( tpl . key % ( self . bucket . name , self . name ) , dict ( size = self . size , md5 = self . md5 ) )
5555	def _element_at_zoom ( name , element , zoom ) : # If element is a dictionary, analyze subitems. if isinstance ( element , dict ) : if "format" in element : # we have an input or output driver here return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element # If there is only one subelement, collapse unless it is # input. In such case, return a dictionary. if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) # If subelement is empty, return None if len ( out_elements ) == 0 : return None return out_elements # If element is a zoom level statement, filter element. elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) # If element is a string but not a zoom level statement, return # element. else : return element # Return all other types as they are. else : return element
8799	def update_group_states_for_vifs ( self , vifs , ack ) : vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in vifs ] self . set_fields ( vif_keys , SECURITY_GROUP_ACK , ack )
11638	def write_data ( data , filename ) : name , ext = get_file_extension ( filename ) func = json_write_data if ext == '.json' else yaml_write_data return func ( data , filename )
4258	def url_from_path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )
724	def getDataRowCount ( self ) : inputRowCountAfterAggregation = 0 while True : record = self . getNextRecord ( ) if record is None : return inputRowCountAfterAggregation inputRowCountAfterAggregation += 1 if inputRowCountAfterAggregation > 10000 : raise RuntimeError ( 'No end of datastream found.' )
9471	def _xml ( self , root ) : element = root . createElement ( self . name ) # Add attributes keys = self . attrs . keys ( ) keys . sort ( ) for a in keys : element . setAttribute ( a , self . attrs [ a ] ) if self . body : text = root . createTextNode ( self . body ) element . appendChild ( text ) for c in self . elements : element . appendChild ( c . _xml ( root ) ) return element
9137	def clear_cache ( module_name : str , keep_database : bool = True ) -> None : data_dir = get_data_dir ( module_name ) if not os . path . exists ( data_dir ) : return for name in os . listdir ( data_dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep_database : continue path = os . path . join ( data_dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data_dir )
5461	def get_job_and_task_param ( job_params , task_params , field ) : return job_params . get ( field , set ( ) ) | task_params . get ( field , set ( ) )
8139	def desaturate ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "L" ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
11239	def bug_info ( exc_type , exc_value , exc_trace ) : if hasattr ( sys , 'ps1' ) or not sys . stderr . isatty ( ) : # We are in interactive mode or don't have a tty-like device, so we call the default hook sys . __excepthook__ ( exc_type , exc_value , exc_trace ) else : # Need to import non-built-ins here, so if dependencies haven't been installed, both tracebacks will print # (e.g. the ImportError and the Exception that got you here) import ipdb # We are NOT in interactive mode, print the exception traceback . print_exception ( exc_type , exc_value , exc_trace ) print # Start the debugger in post-mortem mode. ipdb . post_mortem ( exc_trace )
12681	def copy_attributes ( source , destination , ignore_patterns = [ ] ) : for attr in _wildcard_filter ( dir ( source ) , * ignore_patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
6503	def find_matches ( strings , words , length_hoped ) : lower_words = [ w . lower ( ) for w in words ] def has_match ( string ) : """ Do any of the words match within the string """ lower_string = string . lower ( ) for test_word in lower_words : if test_word in lower_string : return True return False shortened_strings = [ textwrap . wrap ( s ) for s in strings ] short_string_list = list ( chain . from_iterable ( shortened_strings ) ) matches = [ ms for ms in short_string_list if has_match ( ms ) ] cumulative_len = 0 break_at = None for idx , match in enumerate ( matches ) : cumulative_len += len ( match ) if cumulative_len >= length_hoped : break_at = idx break return matches [ 0 : break_at ]
2471	def set_file_atrificat_of_project ( self , doc , symbol , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_artifact ( symbol , value ) else : raise OrderError ( 'File::Artificat' )
3705	def Townsend_Hales ( T , Tc , Vc , omega ) : Tr = T / Tc return Vc / ( 1 + 0.85 * ( 1 - Tr ) + ( 1.692 + 0.986 * omega ) * ( 1 - Tr ) ** ( 1 / 3. ) )
12532	def _store_dicom_paths ( self , folders ) : if isinstance ( folders , str ) : folders = [ folders ] for folder in folders : if not os . path . exists ( folder ) : raise FolderNotFound ( folder ) self . items . extend ( list ( find_all_dicom_files ( folder ) ) )
12332	def get_diffs ( history ) : # First get all possible representations mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] #print(prev['subject'], "==>", curr['subject']) #print(curr['changes']) for c in curr [ 'changes' ] : path = c [ 'path' ] # Skip the metadata file if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue # Find a handler for this kind of file... handler = None for r in representations : if r . can_process ( path ) : handler = r break if handler is None : continue # print(path, "being handled by", handler) v1_hex = prev [ 'commit' ] v2_hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1_hex , v2_hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue # Check to make sure that path1 = os . path . join ( temp1 , v1_hex , path ) path2 = os . path . join ( temp1 , v2_hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : # print("One of the two output files is missing") shutil . rmtree ( temp1 ) continue #print(path1, path2) # Now call the handler diff = handler . get_diff ( path1 , path2 ) # print("Inserting diff", diff) c [ 'diff' ] = diff except Exception as e : #traceback.print_exc() #print("Cleaning up - Exception ", temp1) shutil . rmtree ( temp1 )
7921	def __prepare_domain ( data ) : # pylint: disable=R0912 if not data : raise JIDError ( "Domain must be given" ) data = unicode ( data ) if not data : raise JIDError ( "Domain must be given" ) if u'[' in data : if data [ 0 ] == u'[' and data [ - 1 ] == u']' : try : addr = _validate_ip_address ( socket . AF_INET6 , data [ 1 : - 1 ] ) return "[{0}]" . format ( addr ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) raise JIDError ( u"Invalid IPv6 literal in JID domainpart" ) else : raise JIDError ( u"Invalid use of '[' or ']' in JID domainpart" ) elif data [ 0 ] . isdigit ( ) and data [ - 1 ] . isdigit ( ) : try : addr = _validate_ip_address ( socket . AF_INET , data ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) data = UNICODE_DOT_RE . sub ( u"." , data ) data = data . rstrip ( u"." ) labels = data . split ( u"." ) try : labels = [ idna . nameprep ( label ) for label in labels ] except UnicodeError : raise JIDError ( u"Domain name invalid" ) for label in labels : if not STD3_LABEL_RE . match ( label ) : raise JIDError ( u"Domain name invalid" ) try : idna . ToASCII ( label ) except UnicodeError : raise JIDError ( u"Domain name invalid" ) domain = u"." . join ( labels ) if len ( domain . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Domain name too long" ) return domain
9847	def _load_cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
5268	def _check_input ( self , input ) : if isinstance ( input , str ) : return 'st' elif isinstance ( input , list ) : if all ( isinstance ( item , str ) for item in input ) : return 'gst' raise ValueError ( "String argument should be of type String or" " a list of strings" )
2152	def get ( self , pk = None , * * kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . get ( pk = pk , * * kwargs )
11173	def strsettings ( self , indent = 0 , maxindent = 25 , width = 0 ) : out = [ ] makelabel = lambda name : ' ' * indent + name + ': ' settingsindent = _autoindent ( [ makelabel ( s ) for s in self . options ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] label = makelabel ( name ) settingshelp = "%s(%s): %s" % ( option . formatname , option . strvalue , option . location ) wrapped = self . _wrap_labelled ( label , settingshelp , settingsindent , width ) out . extend ( wrapped ) return '\n' . join ( out )
8381	def hover ( self , node ) : if self . popup == False : return if self . popup == True or self . popup . node != node : if self . popup_text . has_key ( node . id ) : texts = self . popup_text [ node . id ] else : texts = None self . popup = popup ( self . _ctx , node , texts ) self . popup . draw ( )
3401	def find_external_compartment ( model ) : if model . boundary : counts = pd . Series ( tuple ( r . compartments ) [ 0 ] for r in model . boundary ) most = counts . value_counts ( ) most = most . index [ most == most . max ( ) ] . to_series ( ) else : most = None like_external = compartment_shortlist [ "e" ] + [ "e" ] matches = pd . Series ( [ co in like_external for co in model . compartments ] , index = model . compartments ) if matches . sum ( ) == 1 : compartment = matches . index [ matches ] [ 0 ] LOGGER . info ( "Compartment `%s` sounds like an external compartment. " "Using this one without counting boundary reactions" % compartment ) return compartment elif most is not None and matches . sum ( ) > 1 and matches [ most ] . sum ( ) == 1 : compartment = most [ matches [ most ] ] [ 0 ] LOGGER . warning ( "There are several compartments that look like an " "external compartment but `%s` has the most boundary " "reactions, so using that as the external " "compartment." % compartment ) return compartment elif matches . sum ( ) > 1 : raise RuntimeError ( "There are several compartments (%s) that look " "like external compartments but we can't tell " "which one to use. Consider renaming your " "compartments please." ) if most is not None : return most [ 0 ] LOGGER . warning ( "Could not identify an external compartment by name and" " choosing one with the most boundary reactions. That " "might be complete nonsense or change suddenly. " "Consider renaming your compartments using " "`Model.compartments` to fix this." ) # No info in the model, so give up raise RuntimeError ( "The heuristic for discovering an external compartment " "relies on names and boundary reactions. Yet, there " "are neither compartments with recognized names nor " "boundary reactions in the model." )
2955	def update ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = False )
13512	def is_colour ( value ) : global PREDEFINED , HEX_MATCH , RGB_MATCH , RGBA_MATCH , HSL_MATCH , HSLA_MATCH value = value . strip ( ) # hex match if HEX_MATCH . match ( value ) or RGB_MATCH . match ( value ) or RGBA_MATCH . match ( value ) or HSL_MATCH . match ( value ) or HSLA_MATCH . match ( value ) or value in PREDEFINED : return True return False
10270	def get_unweighted_upstream_leaves ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT return filter_nodes ( graph , [ node_is_upstream_leaf , data_missing_key_builder ( key ) ] )
13331	def add ( name , path , branch , type ) : if not name and not path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv module add my_module ./path/to/my_module\n' ' cpenv module add my_module git@github.com:user/my_module.git' ' cpenv module add my_module git@github.com:user/my_module.git --branch=master --type=shared' ) click . echo ( examples ) return if not name : click . echo ( 'Missing required argument: name' ) return if not path : click . echo ( 'Missing required argument: path' ) env = cpenv . get_active_env ( ) if type == 'local' : if not env : click . echo ( '\nActivate an environment to add a local module.\n' ) return if click . confirm ( '\nAdd {} to active env {}?' . format ( name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : env . add_module ( name , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) return module_paths = cpenv . get_module_paths ( ) click . echo ( '\nAvailable module paths:\n' ) for i , mod_path in enumerate ( module_paths ) : click . echo ( ' {}. {}' . format ( i , mod_path ) ) choice = click . prompt ( 'Where do you want to add your module?' , type = int , default = 0 ) module_root = module_paths [ choice ] module_path = utils . unipath ( module_root , name ) click . echo ( 'Creating module {}...' . format ( module_path ) , nl = False ) try : cpenv . create_module ( module_path , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) )
11274	def check_pidfile ( pidfile , debug ) : # Check PID exists and see if the PID is running if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) # try and read the PID file. If no luck, remove it try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass # PID is not active, remove the PID file os . unlink ( pidfile ) # Create a PID file, to ensure this is script is only run once (at a time) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
7177	def retype_file ( src , pyi_dir , targets , * , quiet = False , hg = False ) : with tokenize . open ( src ) as src_buffer : src_encoding = src_buffer . encoding src_node = lib2to3_parse ( src_buffer . read ( ) ) try : with open ( ( pyi_dir / src . name ) . with_suffix ( '.pyi' ) ) as pyi_file : pyi_txt = pyi_file . read ( ) except FileNotFoundError : if not quiet : print ( f'warning: .pyi file for source {src} not found in {pyi_dir}' , file = sys . stderr , ) else : pyi_ast = ast3 . parse ( pyi_txt ) assert isinstance ( pyi_ast , ast3 . Module ) reapply_all ( pyi_ast . body , src_node ) fix_remaining_type_comments ( src_node ) targets . mkdir ( parents = True , exist_ok = True ) with open ( targets / src . name , 'w' , encoding = src_encoding ) as target_file : target_file . write ( lib2to3_unparse ( src_node , hg = hg ) ) return targets / src . name
12433	def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[-_\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
1147	def deepcopy ( x , memo = None , _nil = [ ] ) : if memo is None : memo = { } d = id ( x ) y = memo . get ( d , _nil ) if y is not _nil : return y cls = type ( x ) copier = _deepcopy_dispatch . get ( cls ) if copier : y = copier ( x , memo ) else : try : issc = issubclass ( cls , type ) except TypeError : # cls is not a class (old Boost; see SF #502085) issc = 0 if issc : y = _deepcopy_atomic ( x , memo ) else : copier = getattr ( x , "__deepcopy__" , None ) if copier : y = copier ( memo ) else : reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(deep)copyable object of type %s" % cls ) y = _reconstruct ( x , rv , 1 , memo ) memo [ d ] = y _keep_alive ( x , memo ) # Make sure x lives at least as long as d return y
4016	def get_app_volume_mounts ( app_name , assembled_specs , test = False ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] volumes = [ get_command_files_volume_mount ( app_name , test = test ) ] volumes . append ( get_asset_volume_mount ( app_name ) ) repo_mount = _get_app_repo_volume_mount ( app_spec ) if repo_mount : volumes . append ( repo_mount ) volumes += _get_app_libs_volume_mounts ( app_name , assembled_specs ) return volumes
2186	def load ( self , cfgstr = None ) : from six . moves import cPickle as pickle cfgstr = self . _rectify_cfgstr ( cfgstr ) dpath = self . dpath fname = self . fname verbose = self . verbose if not self . enabled : if verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) raise IOError ( 3 , 'Cache Loading Is Disabled' ) fpath = self . get_fpath ( cfgstr = cfgstr ) if not exists ( fpath ) : if verbose > 2 : self . log ( '[cacher] ... cache does not exist: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) raise IOError ( 2 , 'No such file or directory: %r' % ( fpath , ) ) else : if verbose > 3 : self . log ( '[cacher] ... cache exists: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) try : with open ( fpath , 'rb' ) as file_ : data = pickle . load ( file_ ) except Exception as ex : if verbose > 0 : self . log ( 'CORRUPTED? fpath = %s' % ( fpath , ) ) if verbose > 1 : self . log ( '[cacher] ... CORRUPTED? dpath={} cfgstr={}' . format ( basename ( dpath ) , cfgstr ) ) if isinstance ( ex , ( EOFError , IOError , ImportError ) ) : raise IOError ( str ( ex ) ) else : if verbose > 1 : self . log ( '[cacher] ... unknown reason for exception' ) raise else : if self . verbose > 2 : self . log ( '[cacher] ... {} cache hit' . format ( self . fname ) ) elif verbose > 1 : self . log ( '[cacher] ... cache hit' ) return data
6475	def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
3931	def _auth_with_refresh_token ( session , refresh_token ) : # Make a token request. token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
1601	def chain ( cmd_list ) : command = ' | ' . join ( map ( lambda x : ' ' . join ( x ) , cmd_list ) ) chained_proc = functools . reduce ( pipe , [ None ] + cmd_list ) stdout_builder = proc . async_stdout_builder ( chained_proc ) chained_proc . wait ( ) return { 'command' : command , 'stdout' : stdout_builder . result ( ) }
5949	def filename ( self , filename = None , ext = None , set_default = False , use_my_ext = False ) : if filename is None : if not hasattr ( self , '_filename' ) : self . _filename = None # add attribute to class if self . _filename : filename = self . _filename else : raise ValueError ( "A file name is required because no default file name was defined." ) my_ext = None else : filename , my_ext = os . path . splitext ( filename ) if set_default : # replaces existing default file name self . _filename = filename if my_ext and use_my_ext : ext = my_ext if ext is not None : if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] # strip a dot to avoid annoying mistakes if ext != "" : filename = filename + os . extsep + ext return filename
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
3416	def _get_id_compartment ( id ) : bracket_search = _bracket_re . findall ( id ) if len ( bracket_search ) == 1 : return bracket_search [ 0 ] [ 1 ] underscore_search = _underscore_re . findall ( id ) if len ( underscore_search ) == 1 : return underscore_search [ 0 ] [ 1 ] return None
7357	def _check_peptide_lengths ( self , peptide_lengths = None ) : if not peptide_lengths : peptide_lengths = self . default_peptide_lengths if not peptide_lengths : raise ValueError ( ( "Must either provide 'peptide_lengths' argument " "or set 'default_peptide_lengths" ) ) if isinstance ( peptide_lengths , int ) : peptide_lengths = [ peptide_lengths ] require_iterable_of ( peptide_lengths , int ) for peptide_length in peptide_lengths : if ( self . min_peptide_length is not None and peptide_length < self . min_peptide_length ) : raise ValueError ( "Invalid peptide length %d, shorter than min %d" % ( peptide_length , self . min_peptide_length ) ) elif ( self . max_peptide_length is not None and peptide_length > self . max_peptide_length ) : raise ValueError ( "Invalid peptide length %d, longer than max %d" % ( peptide_length , self . max_peptide_length ) ) return peptide_lengths
479	def word_to_id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk_id
5363	def stdout ( self ) : if self . _streaming : stdout = [ ] while not self . __stdout . empty ( ) : try : line = self . __stdout . get_nowait ( ) stdout . append ( line ) except : pass else : stdout = self . __stdout return stdout
11060	def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test_mode : self . plugins . save_state ( )
9639	def emit ( self , record ) : try : if self . max_messages : p = self . redis_client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max_messages , - 1 ) p . execute ( ) else : self . redis_client . rpush ( self . key , self . format ( record ) ) except redis . RedisError : pass
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
12654	def convert_dcm2nii ( input_dir , output_dir , filename ) : # a few checks before doing the job if not op . exists ( input_dir ) : raise IOError ( 'Expected an existing folder in {}.' . format ( input_dir ) ) if not op . exists ( output_dir ) : raise IOError ( 'Expected an existing output folder in {}.' . format ( output_dir ) ) # create a temporary folder for dcm2nii export tmpdir = tempfile . TemporaryDirectory ( prefix = 'dcm2nii_' ) # call dcm2nii arguments = '-o "{}" -i y' . format ( tmpdir . name ) try : call_out = call_dcm2nii ( input_dir , arguments ) except : raise else : log . info ( 'Converted "{}" to nifti.' . format ( input_dir ) ) # get the filenames of the files that dcm2nii produced filenames = glob ( op . join ( tmpdir . name , '*.nii*' ) ) # cleanup `filenames`, using only the post-processed (reoriented, cropped, etc.) images by dcm2nii cleaned_filenames = remove_dcm2nii_underprocessed ( filenames ) # copy files to the output_dir filepaths = [ ] for srcpath in cleaned_filenames : dstpath = op . join ( output_dir , filename ) realpath = copy_w_plus ( srcpath , dstpath ) filepaths . append ( realpath ) # copy any other file produced by dcm2nii that is not a NifTI file, e.g., *.bvals, *.bvecs, etc. basename = op . basename ( remove_ext ( srcpath ) ) aux_files = set ( glob ( op . join ( tmpdir . name , '{}.*' . format ( basename ) ) ) ) - set ( glob ( op . join ( tmpdir . name , '{}.nii*' . format ( basename ) ) ) ) for aux_file in aux_files : aux_dstpath = copy_w_ext ( aux_file , output_dir , remove_ext ( op . basename ( realpath ) ) ) filepaths . append ( aux_dstpath ) return filepaths
12256	def lbfgs ( x , rho , f_df , maxiter = 20 ) : def f_df_augmented ( theta ) : f , df = f_df ( theta ) obj = f + ( rho / 2. ) * np . linalg . norm ( theta - x ) ** 2 grad = df + rho * ( theta - x ) return obj , grad res = scipy_minimize ( f_df_augmented , x , jac = True , method = 'L-BFGS-B' , options = { 'maxiter' : maxiter , 'disp' : False } ) return res . x
1799	def CMOVO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , src . read ( ) , dest . read ( ) ) )
12673	def aggregate ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . aggregate ( args [ 1 ] , args [ 2 ] , * args [ 3 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . AGGREGATE , * args )
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
6740	def get_packager ( ) : # TODO: remove once fabric stops using contextlib.nested. # https://github.com/fabric/fabric/issues/1364 import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager #TODO:cache result by current env.host_string so we can handle multiple hosts with different OSes with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
12571	def get ( self , key ) : node = self . get_node ( key ) if node is None : raise KeyError ( 'No object named %s in the file' % key ) if hasattr ( node , 'attrs' ) : if 'pandas_type' in node . attrs : return self . _read_group ( node ) return self . _read_array ( node )
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
1555	def _add_out_streams ( self , spbl ) : if self . outputs is None : return # sanitize outputs and get a map <stream_id -> out fields> output_map = self . _sanitize_outputs ( ) for stream_id , out_fields in output_map . items ( ) : out_stream = spbl . outputs . add ( ) out_stream . stream . CopyFrom ( self . _get_stream_id ( self . name , stream_id ) ) out_stream . schema . CopyFrom ( self . _get_stream_schema ( out_fields ) )
8308	def ensure_pycairo_context ( self , ctx ) : if self . cairocffi and isinstance ( ctx , self . cairocffi . Context ) : from shoebot . util . cairocffi . cairocffi_to_pycairo import _UNSAFE_cairocffi_context_to_pycairo return _UNSAFE_cairocffi_context_to_pycairo ( ctx ) else : return ctx
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
57	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )
12871	def chain ( * args ) : def chain_block ( * args , * * kwargs ) : v = args [ 0 ] ( * args , * * kwargs ) for p in args [ 1 : ] : v = p ( v ) return v return chain_block
5441	def rewrite_uris ( self , raw_uri , file_provider ) : if file_provider == job_model . P_GCS : normalized , docker_path = _gcs_uri_rewriter ( raw_uri ) elif file_provider == job_model . P_LOCAL : normalized , docker_path = _local_uri_rewriter ( raw_uri ) else : raise ValueError ( 'File provider not supported: %r' % file_provider ) return normalized , os . path . join ( self . _relative_path , docker_path )
9245	def encapsulate_string ( raw_string ) : raw_string . replace ( '\\' , '\\\\' ) enc_string = re . sub ( "([<>*_()\[\]#])" , r"\\\1" , raw_string ) return enc_string
3273	def as_DAVError ( e ) : if isinstance ( e , DAVError ) : return e elif isinstance ( e , Exception ) : # traceback.print_exc() return DAVError ( HTTP_INTERNAL_ERROR , src_exception = e ) else : return DAVError ( HTTP_INTERNAL_ERROR , "{}" . format ( e ) )
10307	def calculate_global_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : universe = set ( itt . chain . from_iterable ( dict_of_sets . values ( ) ) ) universe_size = len ( universe ) result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = 1.0 - len ( dict_of_sets [ x ] | dict_of_sets [ y ] ) / universe_size for x in dict_of_sets : result [ x ] [ x ] = 1.0 - len ( x ) / universe_size return dict ( result )
12191	async def _get_socket_url ( self ) : data = await self . api . execute_method ( self . RTM_START_ENDPOINT , simple_latest = True , no_unreads = True , ) return data [ 'url' ]
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
476	def data_to_token_ids ( data_path , target_path , vocabulary_path , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if not gfile . Exists ( target_path ) : tl . logging . info ( "Tokenizing data in %s" % data_path ) vocab , _ = initialize_vocabulary ( vocabulary_path ) with gfile . GFile ( data_path , mode = "rb" ) as data_file : with gfile . GFile ( target_path , mode = "w" ) as tokens_file : counter = 0 for line in data_file : counter += 1 if counter % 100000 == 0 : tl . logging . info ( " tokenizing line %d" % counter ) token_ids = sentence_to_token_ids ( line , vocab , tokenizer , normalize_digits , UNK_ID = UNK_ID , _DIGIT_RE = _DIGIT_RE ) tokens_file . write ( " " . join ( [ str ( tok ) for tok in token_ids ] ) + "\n" ) else : tl . logging . info ( "Target path %s exists" % target_path )
13530	def add_child ( self , * * kwargs ) : data_class = self . graph . data_content_type . model_class ( ) node = Node . objects . create ( graph = self . graph ) data_class . objects . create ( node = node , * * kwargs ) node . parents . add ( self ) self . children . add ( node ) return node
1782	def AAS ( cpu ) : if ( cpu . AL & 0x0F > 9 ) or cpu . AF == 1 : cpu . AX = cpu . AX - 6 cpu . AH = cpu . AH - 1 cpu . AF = True cpu . CF = True else : cpu . AF = False cpu . CF = False cpu . AL = cpu . AL & 0x0f
4251	def country_name_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . country_name_by_addr ( addr )
8120	def intersection ( self , b ) : if not self . intersects ( b ) : return None mx , my = max ( self . x , b . x ) , max ( self . y , b . y ) return Bounds ( mx , my , min ( self . x + self . width , b . x + b . width ) - mx , min ( self . y + self . height , b . y + b . height ) - my )
12313	def instantiate ( repo , name = None , filename = None ) : default_transformers = repo . options . get ( 'transformer' , { } ) # If a name is specified, then lookup the options from dgit.json # if specfied. Otherwise it is initialized to an empty list of # files. transformers = { } if name is not None : # Handle the case generator is specified.. if name in default_transformers : transformers = { name : default_transformers [ name ] } else : transformers = { name : { 'files' : [ ] , } } else : transformers = default_transformers #========================================= # Map the filename patterns to list of files #========================================= # Instantiate the files from the patterns specified input_matching_files = None if filename is not None : input_matching_files = repo . find_matching_files ( [ filename ] ) for t in transformers : for k in transformers [ t ] : if "files" not in k : continue if k == "files" and input_matching_files is not None : # Use the files specified on the command line.. transformers [ t ] [ k ] = input_matching_files else : # Try to match the specification if transformers [ t ] [ k ] is None or len ( transformers [ t ] [ k ] ) == 0 : transformers [ t ] [ k ] = [ ] else : matching_files = repo . find_matching_files ( transformers [ t ] [ k ] ) transformers [ t ] [ k ] = matching_files return transformers
9813	def url ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . get_project ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . has_notebook : click . echo ( get_notebook_url ( user , project_name ) ) else : Printer . print_warning ( 'This project `{}` does not have a running notebook.' . format ( project_name ) ) click . echo ( 'You can start a notebook with this command: polyaxon notebook start --help' )
8952	def get_project_root ( ) : try : tasks_py = sys . modules [ 'tasks' ] except KeyError : return None else : return os . path . abspath ( os . path . dirname ( tasks_py . __file__ ) )
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : # pragma: no cover parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
6069	def intensity_at_radius ( self , radius ) : return self . intensity * np . exp ( - self . sersic_constant * ( ( ( radius / self . effective_radius ) ** ( 1. / self . sersic_index ) ) - 1 ) )
960	def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : # Create a path by joining the filename with our local json schema root jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , "jsonschema" , opfJsonSchemaFilename ) # Validate jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return
836	def _removeRows ( self , rowsToRemove ) : # Form a numpy array of row indices to be removed removalArray = numpy . array ( rowsToRemove ) # Remove categories self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) # Remove the partition ID, if any for these rows and rebuild the id map. for row in reversed ( rowsToRemove ) : # Go backwards # Remove these patterns from partitionList self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) # Remove actual patterns if self . useSparseMemory : # Delete backwards for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) # Sanity checks numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
10676	def Cp ( compound_string , T , mass = 1.0 ) : formula , phase = _split_compound_string_ ( compound_string ) TK = T + 273.15 compound = compounds [ formula ] result = compound . Cp ( phase , TK ) return _finalise_result_ ( compound , result , mass )
6977	def kepler_lcdict_to_pkl ( lcdict , outfile = None ) : if not outfile : outfile = '%s-keplc.pkl' % lcdict [ 'objectid' ] . replace ( ' ' , '-' ) # we're using pickle.HIGHEST_PROTOCOL here, this will make Py3 pickles # unreadable for Python 2.7 with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return os . path . abspath ( outfile )
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
10417	def variants_of ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return _get_filtered_variants_of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS_VARIANT and pybel . struct . has_protein_modification ( v ) ) }
1073	def getdelimited ( self , beginchar , endchars , allowcomments = 1 ) : if self . field [ self . pos ] != beginchar : return '' slist = [ '' ] quote = 0 self . pos += 1 while self . pos < len ( self . field ) : if quote == 1 : slist . append ( self . field [ self . pos ] ) quote = 0 elif self . field [ self . pos ] in endchars : self . pos += 1 break elif allowcomments and self . field [ self . pos ] == '(' : slist . append ( self . getcomment ( ) ) continue # have already advanced pos from getcomment elif self . field [ self . pos ] == '\\' : quote = 1 else : slist . append ( self . field [ self . pos ] ) self . pos += 1 return '' . join ( slist )
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) # Add the bin bounds to the dictionary of data [bytes 0-29] for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
9403	def _isobject ( self , name , exist ) : if exist in [ 2 , 5 ] : return False cmd = 'isobject(%s)' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) return resp == 'ans = 1'
7640	def convert_jams ( jams_file , output_prefix , csv = False , comment_char = '#' , namespaces = None ) : if namespaces is None : raise ValueError ( 'No namespaces provided. Try ".*" for all namespaces.' ) jam = jams . load ( jams_file ) # Get all the annotations # Filter down to the unique ones # For each annotation # generate the comment string # generate the output filename # dump to csv # Make a counter object for each namespace type counter = collections . Counter ( ) annotations = [ ] for query in namespaces : annotations . extend ( jam . search ( namespace = query ) ) if csv : suffix = 'csv' sep = ',' else : suffix = 'lab' sep = '\t' for ann in annotations : index = counter [ ann . namespace ] counter [ ann . namespace ] += 1 filename = os . path . extsep . join ( [ get_output_name ( output_prefix , ann . namespace , index ) , suffix ] ) comment = get_comments ( jam , ann ) # Dump to disk lab_dump ( ann , comment , filename , sep , comment_char )
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
12189	async def from_api_token ( cls , token = None , api_cls = SlackBotApi ) : api = api_cls . from_env ( ) if token is None else api_cls ( api_token = token ) data = await api . execute_method ( cls . API_AUTH_ENDPOINT ) return cls ( data [ 'user_id' ] , data [ 'user' ] , api )
11788	def smooth_for ( self , o ) : if o not in self . dictionary : self . dictionary [ o ] = self . default self . n_obs += self . default self . sampler = None
1650	def _DropCommonSuffixes ( filename ) : for suffix in itertools . chain ( ( '%s.%s' % ( test_suffix . lstrip ( '_' ) , ext ) for test_suffix , ext in itertools . product ( _test_suffixes , GetNonHeaderExtensions ( ) ) ) , ( '%s.%s' % ( suffix , ext ) for suffix , ext in itertools . product ( [ 'inl' , 'imp' , 'internal' ] , GetHeaderExtensions ( ) ) ) ) : if ( filename . endswith ( suffix ) and len ( filename ) > len ( suffix ) and filename [ - len ( suffix ) - 1 ] in ( '-' , '_' ) ) : return filename [ : - len ( suffix ) - 1 ] return os . path . splitext ( filename ) [ 0 ]
8990	def first_consumed_mesh ( self ) : for instruction in self . instructions : if instruction . consumes_meshes ( ) : return instruction . first_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
12378	def get ( self , request , response ) : # Ensure we're allowed to read the resource. self . assert_operations ( 'read' ) # Delegate to `read` to retrieve the items. items = self . read ( ) # if self.slug is not None and not items: # # Requested a specific resource but nothing is returned. # # Attempt to resolve by changing what we understand as # # a slug to a path. # self.path = self.path + self.slug if self.path else self.slug # self.slug = None # # Attempt to retreive the resource again. # items = self.read() # Ensure that if we have a slug and still no items that a 404 # is rasied appropriately. if not items : raise http . exceptions . NotFound ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string_types ) ) and items : # Paginate over the collection. items = pagination . paginate ( self . request , self . response , items ) # Build the response object. self . make_response ( items )
2627	def show_summary ( self ) : self . get_instance_state ( ) status_string = "EC2 Summary:\n\tVPC IDs: {}\n\tSubnet IDs: \ {}\n\tSecurity Group ID: {}\n\tRunning Instance IDs: {}\n" . format ( self . vpc_id , self . sn_ids , self . sg_id , self . instances ) status_string += "\tInstance States:\n\t\t" self . get_instance_state ( ) for state in self . instance_states . keys ( ) : status_string += "Instance ID: {} State: {}\n\t\t" . format ( state , self . instance_states [ state ] ) status_string += "\n" logger . info ( status_string ) return status_string
13034	def write_triples ( filename , triples , delimiter = DEFAULT_DELIMITER , triple_order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple_order ) f . write ( line + "\n" )
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) # GIF duration is only measured to a hundredth of a second duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
5487	def send_payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service_name , 'params' : params , 'id' : text_type ( uuid . uuid4 ( ) ) } ) data_binary = data . encode ( 'utf-8' ) url_request = Request ( self . service_url , data_binary , headers = self . headers ) return urlopen ( url_request ) . read ( )
6907	def equatorial_to_galactic ( ra , decl , equinox = 'J2000' ) : # convert the ra/decl to gl, gb radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree , equinox = equinox ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return gl , gb
4323	def contrast ( self , amount = 75 ) : if not is_number ( amount ) or amount < 0 or amount > 100 : raise ValueError ( 'amount must be a number between 0 and 100.' ) effect_args = [ 'contrast' , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'contrast' ) return self
6547	def wait_for_field ( self ) : self . exec_command ( "Wait({0}, InputField)" . format ( self . timeout ) . encode ( "ascii" ) ) if self . status . keyboard != b"U" : raise KeyboardStateError ( "keyboard not unlocked, state was: {0}" . format ( self . status . keyboard . decode ( "ascii" ) ) )
657	def averageOnTime ( vectors , numSamples = None ) : # Special case given a 1 dimensional vector: it represents a single column if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) # How many samples will we look at? if numSamples is None : numSamples = numElements countOn = range ( numElements ) else : countOn = numpy . random . randint ( 0 , numElements , numSamples ) # Compute the on-times and accumulate the frequency counts of each on-time # encountered sumOfLengths = 0.0 onTimeFreqCounts = None n = 0 for i in countOn : ( onTime , segments , durations ) = _listOfOnTimesInVec ( vectors [ : , i ] ) if onTime != 0.0 : sumOfLengths += onTime n += segments onTimeFreqCounts = _accumulateFrequencyCounts ( durations , onTimeFreqCounts ) # Return the average on time of each element that was on. if n > 0 : return ( sumOfLengths / n , onTimeFreqCounts ) else : return ( 0.0 , onTimeFreqCounts )
9562	def _apply_skips ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for skip in self . _skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . __name__ , skip . __doc__ ) if context is not None : p [ 'context' ] = context yield p
3736	def Stockmayer ( Tm = None , Tb = None , Tc = None , Zc = None , omega = None , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and omega : methods . append ( TEEGOTOSTEWARD2 ) if Tc : methods . append ( FLYNN ) methods . append ( BSLC ) methods . append ( TEEGOTOSTEWARD1 ) if Tb : methods . append ( BSLB ) if Tm : methods . append ( BSLM ) if Tc and Zc : methods . append ( STIELTHODOS ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : epsilon = epsilon_Flynn ( Tc ) elif Method == BSLC : epsilon = epsilon_Bird_Stewart_Lightfoot_critical ( Tc ) elif Method == BSLB : epsilon = epsilon_Bird_Stewart_Lightfoot_boiling ( Tb ) elif Method == BSLM : epsilon = epsilon_Bird_Stewart_Lightfoot_melting ( Tm ) elif Method == STIELTHODOS : epsilon = epsilon_Stiel_Thodos ( Tc , Zc ) elif Method == TEEGOTOSTEWARD1 : epsilon = epsilon_Tee_Gotoh_Steward_1 ( Tc ) elif Method == TEEGOTOSTEWARD2 : epsilon = epsilon_Tee_Gotoh_Steward_2 ( Tc , omega ) elif Method == MAGALHAES : epsilon = float ( MagalhaesLJ_data . at [ CASRN , "epsilon" ] ) elif Method == NONE : epsilon = None else : raise Exception ( 'Failure in in function' ) return epsilon
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) # Read the entire buffer into a python bytes object. # read_pixel_bytes: New in version 2.32. pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) # type: bytes width , height = g [ 2 ] , g [ 3 ] # Probably for SSE alignment reasons, the pixbuf has extra data in each line. # The args after "raw" help handle this; see # http://effbot.org/imagingbook/decoder.htm#the-raw-decoder return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
12605	def _to_string ( data ) : sdata = data . copy ( ) for k , v in data . items ( ) : if isinstance ( v , datetime ) : sdata [ k ] = timestamp_to_date_str ( v ) elif not isinstance ( v , ( string_types , float , int ) ) : sdata [ k ] = str ( v ) return sdata
2998	def marketYesterdayDF ( token = '' , version = '' ) : x = marketYesterday ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . DataFrame ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
1265	def sanity_check_actions ( actions_spec ) : # Leave incoming spec-dict intact. actions = copy . deepcopy ( actions_spec ) # Unique action shortform. is_unique = ( 'type' in actions ) if is_unique : actions = dict ( action = actions ) # Normalize actions. for name , action in actions . items ( ) : # Set default type to int if 'type' not in action : action [ 'type' ] = 'int' # Check required values if action [ 'type' ] == 'int' : if 'num_actions' not in action : raise TensorForceError ( "Action requires value 'num_actions' set!" ) elif action [ 'type' ] == 'float' : if ( 'min_value' in action ) != ( 'max_value' in action ) : raise TensorForceError ( "Action requires both values 'min_value' and 'max_value' set!" ) # Set default shape to empty tuple (single-int, discrete action space) if 'shape' not in action : action [ 'shape' ] = ( ) # Convert int to unary tuple if isinstance ( action [ 'shape' ] , int ) : action [ 'shape' ] = ( action [ 'shape' ] , ) return actions , is_unique
8275	def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
10690	def render ( self , format = ReportFormat . printout ) : table = self . _generate_table_ ( ) if format == ReportFormat . printout : print ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . latex : self . _render_latex_ ( table ) elif format == ReportFormat . txt : self . _render_txt_ ( table ) elif format == ReportFormat . csv : self . _render_csv_ ( table ) elif format == ReportFormat . string : return str ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . matplotlib : self . _render_matplotlib_ ( ) elif format == ReportFormat . png : if self . output_path is None : self . _render_matplotlib_ ( ) else : self . _render_matplotlib_ ( True )
6323	def ac_encode ( text , probs ) : coder = Arithmetic ( ) coder . set_probs ( probs ) return coder . encode ( text )
159	def Grayscale ( alpha = 0 , from_colorspace = "RGB" , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ChangeColorspace ( to_colorspace = ChangeColorspace . GRAY , alpha = alpha , from_colorspace = from_colorspace , name = name , deterministic = deterministic , random_state = random_state )
12598	def read_xl ( xl_path : str ) : xl_path , choice = _check_xl_path ( xl_path ) reader = XL_READERS [ choice ] return reader ( xl_path )
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
6344	def idf ( self , term , transform = None ) : docs_with_term = 0 docs = self . docs_of_words ( ) for doc in docs : doc_set = set ( doc ) if transform : transformed_doc = [ ] for word in doc_set : transformed_doc . append ( transform ( word ) ) doc_set = set ( transformed_doc ) if term in doc_set : docs_with_term += 1 if docs_with_term == 0 : return float ( 'inf' ) return log10 ( len ( docs ) / docs_with_term )
6922	def autocorr_magseries ( times , mags , errs , maxlags = 1000 , func = _autocorr_func3 , fillgaps = 0.0 , filterwindow = 11 , forcetimebin = None , sigclip = 3.0 , magsarefluxes = False , verbose = True ) : # get the gap-filled timeseries interpolated = fill_magseries_gaps ( times , mags , errs , fillgaps = fillgaps , forcetimebin = forcetimebin , sigclip = sigclip , magsarefluxes = magsarefluxes , filterwindow = filterwindow , verbose = verbose ) if not interpolated : print ( 'failed to interpolate light curve to minimum cadence!' ) return None itimes , imags = interpolated [ 'itimes' ] , interpolated [ 'imags' ] , # calculate the lags up to maxlags if maxlags : lags = nparange ( 0 , maxlags ) else : lags = nparange ( itimes . size ) series_stdev = 1.483 * npmedian ( npabs ( imags ) ) if func != _autocorr_func3 : # get the autocorrelation as a function of the lag of the mag series autocorr = nparray ( [ func ( imags , x , imags . size , 0.0 , series_stdev ) for x in lags ] ) # this doesn't need a lags array else : autocorr = _autocorr_func3 ( imags , lags [ 0 ] , imags . size , 0.0 , series_stdev ) # return only the maximum number of lags if maxlags is not None : autocorr = autocorr [ : maxlags ] interpolated . update ( { 'minitime' : itimes . min ( ) , 'lags' : lags , 'acf' : autocorr } ) return interpolated
6601	def put_package ( self , package ) : self . last_package_index += 1 package_index = self . last_package_index package_fullpath = self . package_fullpath ( package_index ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/task_00009.p.gz' with gzip . open ( package_fullpath , 'wb' ) as f : pickle . dump ( package , f , protocol = pickle . HIGHEST_PROTOCOL ) f . close ( ) result_fullpath = self . result_fullpath ( package_index ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009/result.p.gz' result_dir = os . path . dirname ( result_fullpath ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009' alphatwirl . mkdir_p ( result_dir ) return package_index
10870	def calc_pts_lag ( npts = 20 ) : scl = { 15 : 0.072144 , 20 : 0.051532 , 25 : 0.043266 } [ npts ] pts0 , wts0 = np . polynomial . laguerre . laggauss ( npts ) pts = np . sinh ( pts0 * scl ) wts = scl * wts0 * np . cosh ( pts0 * scl ) * np . exp ( pts0 ) return pts , wts
13116	def create_connection ( conf ) : host_config = { } host_config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use_ssl' ) ) : host_config [ 'use_ssl' ] = True if conf . get ( 'jackal' , 'ca_certs' ) : host_config [ 'ca_certs' ] = conf . get ( 'jackal' , 'ca_certs' ) if int ( conf . get ( 'jackal' , 'client_certs' ) ) : host_config [ 'client_cert' ] = conf . get ( 'jackal' , 'client_cert' ) host_config [ 'client_key' ] = conf . get ( 'jackal' , 'client_key' ) # Disable hostname checking for now. host_config [ 'ssl_assert_hostname' ] = False connections . create_connection ( * * host_config )
139	def to_bounding_box ( self ) : # TODO get rid of this deferred import from imgaug . augmentables . bbs import BoundingBox xx = self . xx yy = self . yy return BoundingBox ( x1 = min ( xx ) , x2 = max ( xx ) , y1 = min ( yy ) , y2 = max ( yy ) , label = self . label )
8414	def round_any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
6710	def shell ( self , gui = 0 , command = '' , dryrun = None , shell_interactive_cmd_str = None ) : from burlap . common import get_hosts_for_site if dryrun is not None : self . dryrun = dryrun r = self . local_renderer if r . genv . SITE != r . genv . default_site : shell_hosts = get_hosts_for_site ( ) if shell_hosts : r . genv . host_string = shell_hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default_site if int ( gui ) : r . env . shell_default_options . append ( '-X' ) if 'host_string' not in self . genv or not self . genv . host_string : if 'available_sites' in self . genv and r . env . SITE not in r . genv . available_sites : raise Exception ( 'No host_string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host_string set.' ) if '@' in r . genv . host_string : r . env . shell_host_string = r . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' if command : r . env . shell_interactive_cmd_str = command else : r . env . shell_interactive_cmd_str = r . format ( shell_interactive_cmd_str or r . env . shell_interactive_cmd ) r . env . shell_default_options_str = ' ' . join ( r . env . shell_default_options ) if self . is_local : self . vprint ( 'Using direct local.' ) cmd = '{shell_interactive_cmd_str}' elif r . genv . key_filename : self . vprint ( 'Using key filename.' ) # If host_string contains the port, then strip it off and pass separately. port = r . env . shell_host_string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell_host_string = r . env . shell_host_string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell_default_options_str} -i {key_filename} {shell_host_string} "{shell_interactive_cmd_str}"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' else : # No explicit password or key file needed? self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' r . local ( cmd )
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : # replace with string that is BEFORE match res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : # replace with string that is AFTER match res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) # we will not do any replacements if we dont have this npar or dig is 0 if not num or num > len ( npar ) : res += '$' + dig else : # None - undefined has to be replaced with '' res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
4487	def update ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) url = self . _upload_url # peek at the file to check if it is an ampty file which needs special # handling in requests. If we pass a file like object to data that # turns out to be of length zero then no file is created on the OSF if fp . peek ( 1 ) : response = self . _put ( url , data = fp ) else : response = self . _put ( url , data = b'' ) if response . status_code != 200 : msg = ( 'Could not update {} (status ' 'code: {}).' . format ( self . path , response . status_code ) ) raise RuntimeError ( msg )
8118	def invert ( self ) : m = self . matrix d = m [ 0 ] * m [ 4 ] - m [ 1 ] * m [ 3 ] self . matrix = [ m [ 4 ] / d , - m [ 1 ] / d , 0 , - m [ 3 ] / d , m [ 0 ] / d , 0 , ( m [ 3 ] * m [ 7 ] - m [ 4 ] * m [ 6 ] ) / d , - ( m [ 0 ] * m [ 7 ] - m [ 1 ] * m [ 6 ] ) / d , 1 ]
9571	def discovery_view ( self , message ) : for handler in self . registered_handlers : if handler . check ( message ) : return handler . view return None
2459	def set_pkg_summary ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_summary_set : self . package_summary_set = True if validations . validate_pkg_summary ( text ) : doc . package . summary = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Summary' ) else : raise CardinalityError ( 'Package::Summary' )
6846	def is_present ( self , host = None ) : r = self . local_renderer r . env . host = host or self . genv . host_string ret = r . _local ( "getent hosts {host} | awk '{{ print $1 }}'" , capture = True ) or '' if self . verbose : print ( 'ret:' , ret ) ret = ret . strip ( ) if self . verbose : print ( 'Host %s %s present.' % ( r . env . host , 'IS' if bool ( ret ) else 'IS NOT' ) ) ip = ret ret = bool ( ret ) if not ret : return False r . env . ip = ip with settings ( warn_only = True ) : ret = r . _local ( 'ping -c 1 {ip}' , capture = True ) or '' packet_loss = re . findall ( r'([0-9]+)% packet loss' , ret ) # print('packet_loss:',packet_loss) ip_accessible = packet_loss and int ( packet_loss [ 0 ] ) < 100 if self . verbose : print ( 'IP %s accessible: %s' % ( ip , ip_accessible ) ) return bool ( ip_accessible )
2638	def submit ( self , command , blocksize , tasks_per_node , job_name = "parsl.auto" ) : wrapped_cmd = self . launcher ( command , tasks_per_node , 1 ) instance , name = self . create_instance ( command = wrapped_cmd ) self . provisioned_blocks += 1 self . resources [ name ] = { "job_id" : name , "status" : translate_table [ instance [ 'status' ] ] } return name
3535	def clickmap ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickmapNode ( )
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
9105	def dropbox_factory ( request ) : try : return request . registry . settings [ 'dropbox_container' ] . get_dropbox ( request . matchdict [ 'drop_id' ] ) except KeyError : raise HTTPNotFound ( 'no such dropbox' )
11209	def datetime_exists ( dt , tz = None ) : if tz is None : if dt . tzinfo is None : raise ValueError ( 'Datetime is naive and no time zone provided.' ) tz = dt . tzinfo dt = dt . replace ( tzinfo = None ) # This is essentially a test of whether or not the datetime can survive # a round trip to UTC. dt_rt = dt . replace ( tzinfo = tz ) . astimezone ( tzutc ( ) ) . astimezone ( tz ) dt_rt = dt_rt . replace ( tzinfo = None ) return dt == dt_rt
10778	def update_field ( self , poses = None ) : m = np . clip ( self . particle_field , 0 , 1 ) part_color = np . zeros ( self . _image . shape ) for a in range ( 4 ) : part_color [ : , : , : , a ] = self . part_col [ a ] self . field = np . zeros ( self . _image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part_color [ : , : , : , a ] + ( 1 - m ) * self . _image [ : , : , : , a ]
4039	def _cache ( self , response , key ) : # cache template and retrieval time for subsequent calls thetime = datetime . datetime . utcnow ( ) . replace ( tzinfo = pytz . timezone ( "GMT" ) ) self . templates [ key ] = { "tmplt" : response . json ( ) , "updated" : thetime } return copy . deepcopy ( response . json ( ) )
482	def createAndStartSwarm ( client , clientInfo = "" , clientKey = "" , params = "" , minimumWorkers = None , maximumWorkers = None , alreadyRunning = False ) : if minimumWorkers is None : minimumWorkers = Configuration . getInt ( "nupic.hypersearch.minWorkersPerSwarm" ) if maximumWorkers is None : maximumWorkers = Configuration . getInt ( "nupic.hypersearch.maxWorkersPerSwarm" ) return ClientJobsDAO . get ( ) . jobInsert ( client = client , cmdLine = "$HYPERSEARCH" , clientInfo = clientInfo , clientKey = clientKey , alreadyRunning = alreadyRunning , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = ClientJobsDAO . JOB_TYPE_HS )
11196	def compress ( obj , level = 6 , return_type = "bytes" ) : if isinstance ( obj , binary_type ) : b = zlib . compress ( obj , level ) elif isinstance ( obj , string_types ) : b = zlib . compress ( obj . encode ( "utf-8" ) , level ) else : b = zlib . compress ( pickle . dumps ( obj , protocol = 2 ) , level ) if return_type == "bytes" : return b elif return_type == "str" : return base64 . b64encode ( b ) . decode ( "utf-8" ) else : raise ValueError ( "'return_type' has to be one of 'bytes', 'str'!" )
10344	def load_differential_gene_expression ( path : str , gene_symbol_column : str = 'Gene.symbol' , logfc_column : str = 'logFC' , aggregator : Optional [ Callable [ [ List [ float ] ] , float ] ] = None , ) -> Mapping [ str , float ] : if aggregator is None : aggregator = np . median # Load the data frame df = pd . read_csv ( path ) # Check the columns exist in the data frame assert gene_symbol_column in df . columns assert logfc_column in df . columns # throw away columns that don't have gene symbols - these represent control sequences df = df . loc [ df [ gene_symbol_column ] . notnull ( ) , [ gene_symbol_column , logfc_column ] ] values = defaultdict ( list ) for _ , gene_symbol , log_fold_change in df . itertuples ( ) : values [ gene_symbol ] . append ( log_fold_change ) return { gene_symbol : aggregator ( log_fold_changes ) for gene_symbol , log_fold_changes in values . items ( ) }
5225	def flatten ( iterable , maps = None , unique = False ) -> list : if iterable is None : return [ ] if maps is None : maps = dict ( ) if isinstance ( iterable , ( str , int , float ) ) : return [ maps . get ( iterable , iterable ) ] else : x = [ maps . get ( item , item ) for item in _to_gen_ ( iterable ) ] return list ( set ( x ) ) if unique else x
7576	def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : ## create call string outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] ## call the shell function proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) ## cleanup oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
6302	def add_package ( self , name ) : name , cls_name = parse_package_string ( name ) if name in self . package_map : return package = EffectPackage ( name ) package . load ( ) self . packages . append ( package ) self . package_map [ package . name ] = package # Load effect package dependencies self . polulate ( package . effect_packages )
2659	def initialize_scaling ( self ) : debug_opts = "--debug" if self . worker_debug else "" max_workers = "" if self . max_workers == float ( 'inf' ) else "--max_workers={}" . format ( self . max_workers ) worker_logdir = "{}/{}" . format ( self . run_dir , self . label ) if self . worker_logdir_root is not None : worker_logdir = "{}/{}" . format ( self . worker_logdir_root , self . label ) l_cmd = self . launch_cmd . format ( debug = debug_opts , prefetch_capacity = self . prefetch_capacity , task_url = self . worker_task_url , result_url = self . worker_result_url , cores_per_worker = self . cores_per_worker , max_workers = max_workers , nodes_per_block = self . provider . nodes_per_block , heartbeat_period = self . heartbeat_period , heartbeat_threshold = self . heartbeat_threshold , poll_period = self . poll_period , logdir = worker_logdir ) self . launch_cmd = l_cmd logger . debug ( "Launch command: {}" . format ( self . launch_cmd ) ) self . _scaling_enabled = self . provider . scaling_enabled logger . debug ( "Starting HighThroughputExecutor with provider:\n%s" , self . provider ) if hasattr ( self . provider , 'init_blocks' ) : try : self . scale_out ( blocks = self . provider . init_blocks ) except Exception as e : logger . error ( "Scaling out failed: {}" . format ( e ) ) raise e
3711	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeLiquids ] return Amgat ( zs , Vms ) elif method == COSTALD_MIXTURE : return COSTALD_mixture ( zs , T , self . Tcs , self . Vcs , self . omegas ) elif method == COSTALD_MIXTURE_FIT : return COSTALD_mixture ( zs , T , self . Tcs , self . COSTALD_Vchars , self . COSTALD_omegas ) elif method == RACKETT : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Zcs ) elif method == RACKETT_PARAMETERS : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Z_RAs ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) rho = Laliberte_density ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return rho_to_Vm ( rho , MW ) else : raise Exception ( 'Method not valid' )
5270	def _get_word_start_index ( self , idx ) : i = 0 for _idx in self . word_starts [ 1 : ] : if idx < _idx : return i else : i += 1 return i
654	def _accumulateFrequencyCounts ( values , freqCounts = None ) : # How big does our freqCounts vector need to be? values = numpy . array ( values ) numEntries = values . max ( ) + 1 if freqCounts is not None : numEntries = max ( numEntries , freqCounts . size ) # Where do we accumulate the results? if freqCounts is not None : if freqCounts . size != numEntries : newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) newCounts [ 0 : freqCounts . size ] = freqCounts else : newCounts = freqCounts else : newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) # Accumulate the new values for v in values : newCounts [ v ] += 1 return newCounts
2311	def b_fit_score ( self , x , y ) : x = np . reshape ( minmax_scale ( x ) , ( - 1 , 1 ) ) y = np . reshape ( minmax_scale ( y ) , ( - 1 , 1 ) ) poly = PolynomialFeatures ( degree = self . degree ) poly_x = poly . fit_transform ( x ) poly_x [ : , 1 ] = 0 poly_x [ : , 2 ] = 0 regressor = LinearRegression ( ) regressor . fit ( poly_x , y ) y_predict = regressor . predict ( poly_x ) error = mean_squared_error ( y_predict , y ) return error
2185	def tryload ( self , cfgstr = None , on_error = 'raise' ) : cfgstr = self . _rectify_cfgstr ( cfgstr ) if self . enabled : try : if self . verbose > 1 : self . log ( '[cacher] tryload fname={}' . format ( self . fname ) ) return self . load ( cfgstr ) except IOError : if self . verbose > 0 : self . log ( '[cacher] ... {} cache miss' . format ( self . fname ) ) except Exception : if self . verbose > 0 : self . log ( '[cacher] ... failed to load' ) if on_error == 'raise' : raise elif on_error == 'clear' : self . clear ( cfgstr ) return None else : raise KeyError ( 'Unknown method on_error={}' . format ( on_error ) ) else : if self . verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) return None
3413	def _update_optional ( cobra_object , new_dict , optional_attribute_dict , ordered_keys ) : for key in ordered_keys : default = optional_attribute_dict [ key ] value = getattr ( cobra_object , key ) if value is None or value == default : continue new_dict [ key ] = _fix_type ( value )
7619	def chord ( ref , est , * * kwargs ) : namespace = 'chord' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_interval , ref_value = ref . to_interval_values ( ) est_interval , est_value = est . to_interval_values ( ) return mir_eval . chord . evaluate ( ref_interval , ref_value , est_interval , est_value , * * kwargs )
5537	def get_raw_output ( self , tile , _baselevel_readonly = False ) : if not isinstance ( tile , ( BufferedTile , tuple ) ) : raise TypeError ( "'tile' must be a tuple or BufferedTile" ) if isinstance ( tile , tuple ) : tile = self . config . output_pyramid . tile ( * tile ) if _baselevel_readonly : tile = self . config . baselevels [ "tile_pyramid" ] . tile ( * tile . id ) # Return empty data if zoom level is outside of process zoom levels. if tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( tile ) # TODO implement reprojection if tile . crs != self . config . process_pyramid . crs : raise NotImplementedError ( "reprojection between processes not yet implemented" ) if self . config . mode == "memory" : # Determine affected process Tile and check whether it is already # cached. process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] return self . _extract ( in_tile = process_tile , in_data = self . _execute_using_cache ( process_tile ) , out_tile = tile ) # TODO: cases where tile intersects with multiple process tiles process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] # get output_tiles that intersect with current tile if tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( tile . bounds , tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( tile ) if self . config . mode == "readonly" or _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . config . output . empty ( tile ) elif self . config . mode == "continue" and not _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . _process_and_overwrite_output ( tile , process_tile ) elif self . config . mode == "overwrite" and not _baselevel_readonly : return self . _process_and_overwrite_output ( tile , process_tile )
10167	def get_md_status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : # The final 2 entries on this line: [n/m] [UUUU_] # [n/m] means that ideally the array would have n devices however, currently, m devices are in use. # Obviously when m >= n then things are good. ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] # [UUUU_] represents the status of each device, either U for up or _ for down. ret [ 'config' ] = splitted [ - 2 ] return ret
5870	def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
12512	def niftilist_to_array ( img_filelist , outdtype = None ) : try : first_img = img_filelist [ 0 ] vol = get_img_data ( first_img ) except IndexError as ie : raise Exception ( 'Error getting the first item of img_filelis: {}' . format ( repr_imgs ( img_filelist [ 0 ] ) ) ) from ie if not outdtype : outdtype = vol . dtype outmat = np . zeros ( ( len ( img_filelist ) , np . prod ( vol . shape ) ) , dtype = outdtype ) try : for i , img_file in enumerate ( img_filelist ) : vol = get_img_data ( img_file ) outmat [ i , : ] = vol . flatten ( ) except Exception as exc : raise Exception ( 'Error on reading file {0}.' . format ( img_file ) ) from exc return outmat , vol . shape
800	def modelsGetFieldsForJob ( self , jobID , fields , ignoreKilled = False ) : assert len ( fields ) >= 1 , 'fields is empty' # Form the sequence of field name strings that will go into the # request dbFields = [ self . _models . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( dbFields ) query = 'SELECT model_id, %s FROM %s ' ' WHERE job_id=%%s ' % ( dbFieldsStr , self . modelsTableName ) sqlParams = [ jobID ] if ignoreKilled : query += ' AND (completion_reason IS NULL OR completion_reason != %s)' sqlParams . append ( self . CMPL_REASON_KILLED ) # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows is None : # fetchall is defined to return a (possibly-empty) sequence of # sequences; however, we occasionally see None returned and don't know # why... self . _logger . error ( "Unexpected None result from cursor.fetchall; " "query=%r; Traceback=%r" , query , traceback . format_exc ( ) ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
5660	def print_coords ( rows , prefix = '' ) : lat = [ row [ 'lat' ] for row in rows ] lon = [ row [ 'lon' ] for row in rows ] print ( 'COORDS' + '-' * 5 ) print ( "%slat, %slon = %r, %r" % ( prefix , prefix , lat , lon ) ) print ( '-' * 5 )
8497	def _output ( calls , args ) : # Sort the keys appropriately if args . natural_sort or args . source : calls = sorted ( calls , key = lambda c : ( c . filename , c . lineno ) ) else : calls = sorted ( calls , key = lambda c : c . key ) out = [ ] # Handle displaying only the list of keys if args . only_keys : keys = set ( ) for call in calls : if call . key in keys : continue out . append ( _format_call ( call , args ) ) keys . add ( call . key ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' ) # We're done here return # Build a list of keys which have default values available, so that we can # toggle between displaying only those keys with defaults and all keys keys = set ( ) for call in calls : if call . default : keys . add ( call . key ) for call in calls : if not args . all and not call . default and call . key in keys : continue out . append ( _format_call ( call , args ) ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' )
8629	def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) # Hire Me job, required project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } # POST /api/projects/0.1/projects/ response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
7711	def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
3295	def set_share_path ( self , share_path ) : # if isinstance(share_path, unicode): # share_path = share_path.encode("utf8") assert share_path == "" or share_path . startswith ( "/" ) if share_path == "/" : share_path = "" # This allows to code 'absPath = share_path + path' assert share_path in ( "" , "/" ) or not share_path . endswith ( "/" ) self . share_path = share_path
11020	def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article_filename = find_last_article ( config [ 'CONTENT_DIR' ] ) if not article_filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article_filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find_images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\nAdd these images to the latest article' ) : abort ( config ) url_prefix = os . path . join ( '{filename}' , IMAGES_PATH ) images_dir = os . path . join ( config [ 'CONTENT_DIR' ] , IMAGES_PATH ) os . makedirs ( images_dir , exist_ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image_basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url_prefix , image_basename ) ) image_filename = os . path . join ( images_dir , image_basename ) print ( filename , image_filename ) import_image ( filename , image_filename ) content = '\n' for url in urls : url = url . replace ( '\\' , '/' ) content += '\n![image description]({})\n' . format ( url ) header ( 'Adding to article: {}' . format ( article_filename ) ) with click . open_file ( article_filename , 'a' ) as f : f . write ( content ) click . launch ( article_filename )
13420	def validate ( cls , definition ) : schema_path = os . path . join ( os . path . dirname ( __file__ ) , '../../schema/mapper_definition_schema.json' ) with open ( schema_path , 'r' ) as jsonfp : schema = json . load ( jsonfp ) # Validation of JSON schema jsonschema . validate ( definition , schema ) # Validation of JSON properties relations assert definition [ 'main_key' ] in definition [ 'supported_keys' ] , '\'main_key\' must be contained in \'supported_keys\'' assert set ( definition . get ( 'list_valued_keys' , [ ] ) ) <= set ( definition [ 'supported_keys' ] ) , '\'list_valued_keys\' must be a subset of \'supported_keys\'' assert set ( definition . get ( 'disjoint' , [ ] ) ) <= set ( definition . get ( 'list_valued_keys' , [ ] ) ) , '\'disjoint\' must be a subset of \'list_valued_keys\'' assert set ( definition . get ( 'key_synonyms' , { } ) . values ( ) ) <= set ( definition [ 'supported_keys' ] ) , '\'The values of the \'key_synonyms\' mapping must be in \'supported_keys\''
8502	def as_live ( self ) : key = self . get_key ( ) default = pyconfig . get ( key ) if default : default = repr ( default ) else : default = self . _default ( ) or NotSet ( ) return "%s = %s" % ( key , default )
2600	def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
11509	def delete_item ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.delete' , parameters ) return response
6556	def projection ( self , variables ) : # resolve iterables or mutability problems by casting the variables to a set variables = set ( variables ) if not variables . issubset ( self . variables ) : raise ValueError ( "Cannot project to variables not in the constraint." ) idxs = [ i for i , v in enumerate ( self . variables ) if v in variables ] configurations = frozenset ( tuple ( config [ i ] for i in idxs ) for config in self . configurations ) variables = tuple ( self . variables [ i ] for i in idxs ) return self . from_configurations ( configurations , variables , self . vartype )
13840	def ConsumeBool ( self ) : try : result = ParseBool ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
2308	def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
11745	def closure ( self , rules ) : closure = set ( ) todo = set ( rules ) while todo : rule = todo . pop ( ) closure . add ( rule ) # If the dot is at the end, there's no need to process it. if rule . at_end : continue symbol = rule . rhs [ rule . pos ] for production in self . nonterminals [ symbol ] : for first in self . first ( rule . rest ) : if EPSILON in production . rhs : # Move immediately to the end if the production # goes to epsilon new_rule = DottedRule ( production , 1 , first ) else : new_rule = DottedRule ( production , 0 , first ) if new_rule not in closure : todo . add ( new_rule ) return frozenset ( closure )
11176	def parse ( self , argv ) : if len ( argv ) < self . nargs : raise BadNumberOfArguments ( self . nargs , len ( argv ) ) if self . nargs == 1 : return self . parse_argument ( argv . pop ( 0 ) ) return [ self . parse_argument ( argv . pop ( 0 ) ) for tmp in range ( self . nargs ) ]
13645	def parse ( parser , argv = None , settings_key = 'settings' , no_args_func = None ) : argv = argv or sys . argv commands = command_list ( ) if type ( argv ) not in [ list , tuple ] : raise TypeError ( "argv only can be list or tuple" ) # match sub-parser if len ( argv ) >= 2 and argv [ 1 ] in commands : sub_parsers = parser . add_subparsers ( ) class_name = argv [ 1 ] . capitalize ( ) + 'Component' from cliez . conf import ( COMPONENT_ROOT , LOGGING_CONFIG , EPILOG , GENERAL_ARGUMENTS ) sys . path . insert ( 0 , os . path . dirname ( COMPONENT_ROOT ) ) mod = importlib . import_module ( '{}.components.{}' . format ( os . path . basename ( COMPONENT_ROOT ) , argv [ 1 ] ) ) # dynamic load component klass = getattr ( mod , class_name ) sub_parser = append_arguments ( klass , sub_parsers , EPILOG , GENERAL_ARGUMENTS ) options = parser . parse_args ( argv [ 1 : ] ) settings = Settings . bind ( getattr ( options , settings_key ) ) if settings_key and hasattr ( options , settings_key ) else None obj = klass ( parser , sub_parser , options , settings ) # init logger logger_level = logging . CRITICAL if hasattr ( options , 'verbose' ) : if options . verbose == 1 : logger_level = logging . ERROR elif options . verbose == 2 : logger_level = logging . WARNING elif options . verbose == 3 : logger_level = logging . INFO obj . logger . setLevel ( logging . INFO ) pass if hasattr ( options , 'debug' ) and options . debug : logger_level = logging . DEBUG # http lib use a strange way to logging try : import http . client as http_client http_client . HTTPConnection . debuglevel = 1 except Exception : # do nothing pass pass loggers = LOGGING_CONFIG [ 'loggers' ] for k , v in loggers . items ( ) : v . setdefault ( 'level' , logger_level ) if logger_level in [ logging . INFO , logging . DEBUG ] : v [ 'handlers' ] = [ 'stdout' ] pass logging_config . dictConfig ( LOGGING_CONFIG ) # this may not necessary # obj.logger.setLevel(logger_level) obj . run ( options ) # return object to make unit test easy return obj # print all sub commands when user set. if not parser . description and len ( commands ) : sub_parsers = parser . add_subparsers ( ) [ sub_parsers . add_parser ( v ) for v in commands ] pass pass options = parser . parse_args ( argv [ 1 : ] ) if no_args_func and callable ( no_args_func ) : return no_args_func ( options ) else : parser . _print_message ( "nothing to do...\n" ) pass
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
8949	def warning ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;7;33;40mWARNING: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
6252	def create_normal_matrix ( self , modelview ) : normal_m = Matrix33 . from_matrix44 ( modelview ) normal_m = normal_m . inverse normal_m = normal_m . transpose ( ) return normal_m
5156	def _get_install_context ( self ) : config = self . config # layer2 VPN list l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) # bridge list bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) # crontabs present? cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break # return context return dict ( hostname = config [ 'general' ] [ 'hostname' ] , # hostname is required l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , # radios might be empty cron = cron )
2664	def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
2868	def get_platform_gpio ( * * keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPiGPIOAdapter ( RPi . GPIO , * * keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . GPIO return AdafruitBBIOAdapter ( Adafruit_BBIO . GPIO , * * keywords ) elif plat == Platform . MINNOWBOARD : import mraa return AdafruitMinnowAdapter ( mraa , * * keywords ) elif plat == Platform . JETSON_NANO : import Jetson . GPIO return RPiGPIOAdapter ( Jetson . GPIO , * * keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
10082	def _prepare_edit ( self , record ) : data = record . dumps ( ) # Keep current record revision for merging. data [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] = record . revision_id data [ '_deposit' ] [ 'status' ] = 'draft' data [ '$schema' ] = self . build_deposit_schema ( record ) return data
864	def makeDirectoryFromAbsolutePath ( absDirPath ) : assert os . path . isabs ( absDirPath ) try : os . makedirs ( absDirPath ) except OSError , e : if e . errno != os . errno . EEXIST : raise return absDirPath
2238	def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : # see also 'SHLIB_EXT' multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : # handle PEP 3149 -- ABI version tagged .so files # ABI = application binary interface tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) # not sure why this one is valid but it is tags = [ t for t in tags if t ] return tags
13131	def parse_domain_computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = HostSearch ( ) count = 0 entry_count = 0 print_notification ( "Parsing {} entries" . format ( len ( data ) ) ) for system in data : entry_count += 1 parsed = parse_single_computer ( system ) if parsed . ip : try : host = hs . id_to_object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns_hostname ) if parsed . os : host . os = parsed . os host . domain_controller = parsed . dc host . add_tag ( 'domaindump' ) host . save ( ) count += 1 except ValueError : pass sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}] {} resolved" . format ( entry_count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
7424	def check_insert_size ( data , sample ) : ## pipe stats output to grep cmd1 = [ ipyrad . bins . samtools , "stats" , sample . files . mapped_reads ] cmd2 = [ "grep" , "SN" ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) ## get piped result res = proc2 . communicate ( ) [ 0 ] ## raise exception on failure and do cleanup if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , res ) ## starting vals avg_insert = 0 stdv_insert = 0 avg_len = 0 ## iterate over results for line in res . split ( "\n" ) : if "insert size average" in line : avg_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) elif "insert size standard deviation" in line : ## hack to fix sim data when stdv is 0.0. Shouldn't ## impact real data bcz stdv gets rounded up below stdv_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) + 0.1 elif "average length" in line : avg_len = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) LOGGER . debug ( "avg {} stdv {} avg_len {}" . format ( avg_insert , stdv_insert , avg_len ) ) ## If all values return successfully set the max inner mate distance. ## This is tricky. avg_insert is the average length of R1+R2+inner mate ## distance. avg_len is the average length of a read. If there are lots ## of reads that overlap then avg_insert will be close to but bigger than ## avg_len. We are looking for the right value for `bedtools merge -d` ## which wants to know the max distance between reads. if all ( [ avg_insert , stdv_insert , avg_len ] ) : ## If 2 * the average length of a read is less than the average ## insert size then most reads DO NOT overlap if stdv_insert < 5 : stdv_insert = 5. if ( 2 * avg_len ) < avg_insert : hack = avg_insert + ( 3 * np . math . ceil ( stdv_insert ) ) - ( 2 * avg_len ) ## If it is > than the average insert size then most reads DO ## overlap, so we have to calculate inner mate distance a little ## differently. else : hack = ( avg_insert - avg_len ) + ( 3 * np . math . ceil ( stdv_insert ) ) ## set the hackerdict value LOGGER . info ( "stdv: hacked insert size is %s" , hack ) data . _hackersonly [ "max_inner_mate_distance" ] = int ( np . math . ceil ( hack ) ) else : ## If something fsck then set a relatively conservative distance data . _hackersonly [ "max_inner_mate_distance" ] = 300 LOGGER . debug ( "inner mate distance for {} - {}" . format ( sample . name , data . _hackersonly [ "max_inner_mate_distance" ] ) )
10056	def put ( self , pid , record , key ) : try : data = json . loads ( request . data . decode ( 'utf-8' ) ) new_key = data [ 'filename' ] except KeyError : raise WrongFile ( ) new_key_secure = secure_filename ( new_key ) if not new_key_secure or new_key != new_key_secure : raise WrongFile ( ) try : obj = record . files . rename ( str ( key ) , new_key_secure ) except KeyError : abort ( 404 ) record . commit ( ) db . session . commit ( ) return self . make_response ( obj = obj , pid = pid , record = record )
462	def exit_tensorflow ( sess = None , port = 6006 ) : text = "[TL] Close tensorboard and nvidia-process if available" text2 = "[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on " if sess is not None : sess . close ( ) if _platform == "linux" or _platform == "linux2" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) # kill tensorboard 6006 os . system ( "nvidia-smi | grep python |awk '{print $3}'|xargs kill" ) # kill all nvidia-smi python process _exit ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( "lsof -i tcp:" + str ( port ) + " | grep -v PID | awk '{print $2}' | xargs kill" , shell = True ) # kill tensorboard elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( text2 + _platform )
195	def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : # TODO allow (1, None) and set to identity matrix if k == 1 k_param = iap . handle_discrete_param ( k , "k" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , "angle" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : # avoid cyclic import between blur and geometric from . import geometric as iaa_geometric # force discrete for k_sample via int() in case of stochastic parameter k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
11205	def name_from_string ( self , tzname_str ) : if not tzname_str . startswith ( '@' ) : return tzname_str name_splt = tzname_str . split ( ',-' ) try : offset = int ( name_splt [ 1 ] ) except : raise ValueError ( "Malformed timezone string." ) return self . load_name ( offset )
12568	def create_dataset ( self , ds_name , data , attrs = None , dtype = None ) : if ds_name in self . _datasets : ds = self . _datasets [ ds_name ] if ds . dtype != data . dtype : warnings . warn ( 'Dataset and data dtype are different!' ) else : if dtype is None : dtype = data . dtype ds = self . _group . create_dataset ( ds_name , data . shape , dtype = dtype ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) ds . read_direct ( data ) self . _datasets [ ds_name ] = ds return ds
10992	def _calc_ilm_order ( imshape ) : zorder = int ( imshape [ 0 ] / 6.25 ) + 1 l_npts = int ( imshape [ 1 ] / 42.5 ) + 1 npts = ( ) for a in range ( l_npts ) : if a < 5 : npts += ( int ( imshape [ 2 ] * [ 59 , 39 , 29 , 19 , 14 ] [ a ] / 512. ) + 1 , ) else : npts += ( int ( imshape [ 2 ] * 11 / 512. ) + 1 , ) return npts , zorder
6982	def _get_legendre_deg_ctd ( npts ) : from scipy . interpolate import interp1d degs = nparray ( [ 4 , 5 , 6 , 10 , 15 ] ) pts = nparray ( [ 1e2 , 3e2 , 5e2 , 1e3 , 3e3 ] ) fn = interp1d ( pts , degs , kind = 'linear' , bounds_error = False , fill_value = ( min ( degs ) , max ( degs ) ) ) legendredeg = int ( npfloor ( fn ( npts ) ) ) return legendredeg
12690	def queue ( users , label , extra_context = None , sender = None ) : if extra_context is None : extra_context = { } if isinstance ( users , QuerySet ) : users = [ row [ "pk" ] for row in users . values ( "pk" ) ] else : users = [ user . pk for user in users ] notices = [ ] for user in users : notices . append ( ( user , label , extra_context , sender ) ) NoticeQueueBatch ( pickled_data = base64 . b64encode ( pickle . dumps ( notices ) ) ) . save ( )
6573	def formatter ( self , api_client , data , newval ) : url_map = data . get ( "audioUrlMap" ) audio_url = data . get ( "audioUrl" ) # Only an audio URL, not a quality map. This happens for most of the # mobile client tokens and some of the others now. In this case # substitute the empirically determined default values in the format # used by the rest of the function so downstream consumers continue to # work. if audio_url and not url_map : url_map = { BaseAPIClient . HIGH_AUDIO_QUALITY : { "audioUrl" : audio_url , "bitrate" : 64 , "encoding" : "aacplus" , } } elif not url_map : # No audio url available (e.g. ad tokens) return None valid_audio_formats = [ BaseAPIClient . HIGH_AUDIO_QUALITY , BaseAPIClient . MED_AUDIO_QUALITY , BaseAPIClient . LOW_AUDIO_QUALITY ] # Only iterate over sublist, starting at preferred audio quality, or # from the beginning of the list if nothing is found. Ensures that the # bitrate used will always be the same or lower quality than was # specified to prevent audio from skipping for slow connections. preferred_quality = api_client . default_audio_quality if preferred_quality in valid_audio_formats : i = valid_audio_formats . index ( preferred_quality ) valid_audio_formats = valid_audio_formats [ i : ] for quality in valid_audio_formats : audio_url = url_map . get ( quality ) if audio_url : return audio_url [ self . field ] return audio_url [ self . field ] if audio_url else None
13249	def get_url_from_entry ( entry ) : if 'url' in entry . fields : return entry . fields [ 'url' ] elif entry . type . lower ( ) == 'docushare' : return 'https://ls.st/' + entry . fields [ 'handle' ] elif 'adsurl' in entry . fields : return entry . fields [ 'adsurl' ] elif 'doi' in entry . fields : return 'https://doi.org/' + entry . fields [ 'doi' ] else : raise NoEntryUrlError ( )
6562	def iter_complete_graphs ( start , stop , factory = None ) : _ , nodes = start nodes = list ( nodes ) # we'll be appending if factory is None : factory = count ( ) while len ( nodes ) < stop : # we need to construct a new graph each time, this is actually faster than copy and add # the new edges in any case G = nx . complete_graph ( nodes ) yield G v = next ( factory ) while v in G : v = next ( factory ) nodes . append ( v )
12964	def all ( self , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultiple ( matchedKeys , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
11352	def _fill_text ( self , text , width , indent ) : lines = [ ] for line in text . splitlines ( False ) : if line : # https://docs.python.org/2/library/textwrap.html lines . extend ( textwrap . wrap ( line . strip ( ) , width , initial_indent = indent , subsequent_indent = indent ) ) else : lines . append ( line ) text = "\n" . join ( lines ) return text
1775	def CPUID ( cpu ) : # FIXME Choose conservative values and consider returning some default when eax not here conf = { 0x0 : ( 0x0000000d , 0x756e6547 , 0x6c65746e , 0x49656e69 ) , 0x1 : ( 0x000306c3 , 0x05100800 , 0x7ffafbff , 0xbfebfbff ) , 0x2 : ( 0x76035a01 , 0x00f0b5ff , 0x00000000 , 0x00c10000 ) , 0x4 : { 0x0 : ( 0x1c004121 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 0x1 : ( 0x1c004122 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 0x2 : ( 0x1c004143 , 0x01c0003f , 0x000001ff , 0x00000000 ) , 0x3 : ( 0x1c03c163 , 0x03c0003f , 0x00000fff , 0x00000006 ) } , 0x7 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0x8 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0xb : { 0x0 : ( 0x00000001 , 0x00000002 , 0x00000100 , 0x00000005 ) , 0x1 : ( 0x00000004 , 0x00000004 , 0x00000201 , 0x00000003 ) } , 0xd : { 0x0 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0x1 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) } , } if cpu . EAX not in conf : logger . warning ( 'CPUID with EAX=%x not implemented @ %x' , cpu . EAX , cpu . PC ) cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 return if isinstance ( conf [ cpu . EAX ] , tuple ) : cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] return if cpu . ECX not in conf [ cpu . EAX ] : logger . warning ( 'CPUID with EAX=%x ECX=%x not implemented' , cpu . EAX , cpu . ECX ) cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 return cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] [ cpu . ECX ]
5736	def cleanup ( self ) : if self . subscription : logger . info ( "Deleting worker subscription..." ) self . subscriber_client . delete_subscription ( self . subscription )
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( * * jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( * * jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( * * jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( * * rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
534	def _getRegions ( self ) : def makeRegion ( name , r ) : """Wrap a engine region with a nupic.engine_internal.Region Also passes the containing nupic.engine_internal.Network network in _network. This function is passed a value wrapper to the CollectionWrapper """ r = Region ( r , self ) #r._network = self return r regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) return regions
3699	def Tliquidus ( Tms = None , ws = None , xs = None , CASRNs = None , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if none_and_length_check ( [ Tms ] ) : methods . append ( 'Maximum' ) methods . append ( 'Simple' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == 'Maximum' : _Tliq = max ( Tms ) elif Method == 'Simple' : _Tliq = mixing_simple ( xs , Tms ) elif Method == 'None' : return None else : raise Exception ( 'Failure in in function' ) return _Tliq
4545	def draw_circle ( setter , x0 , y0 , r , color = None ) : f = 1 - r ddF_x = 1 ddF_y = - 2 * r x = 0 y = r setter ( x0 , y0 + r , color ) setter ( x0 , y0 - r , color ) setter ( x0 + r , y0 , color ) setter ( x0 - r , y0 , color ) while x < y : if f >= 0 : y -= 1 ddF_y += 2 f += ddF_y x += 1 ddF_x += 2 f += ddF_x setter ( x0 + x , y0 + y , color ) setter ( x0 - x , y0 + y , color ) setter ( x0 + x , y0 - y , color ) setter ( x0 - x , y0 - y , color ) setter ( x0 + y , y0 + x , color ) setter ( x0 - y , y0 + x , color ) setter ( x0 + y , y0 - x , color ) setter ( x0 - y , y0 - x , color )
6272	def swap_buffers ( self ) : self . frames += 1 if self . headless_frames and self . frames >= self . headless_frames : self . close ( )
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
6918	def _get_acf_peakheights ( lags , acf , npeaks = 20 , searchinterval = 1 ) : maxinds = argrelmax ( acf , order = searchinterval ) [ 0 ] maxacfs = acf [ maxinds ] maxlags = lags [ maxinds ] mininds = argrelmin ( acf , order = searchinterval ) [ 0 ] minacfs = acf [ mininds ] minlags = lags [ mininds ] relpeakheights = npzeros ( npeaks ) relpeaklags = npzeros ( npeaks , dtype = npint64 ) peakindices = npzeros ( npeaks , dtype = npint64 ) for peakind , mxi in enumerate ( maxinds [ : npeaks ] ) : # check if there are no mins to the left # throw away this peak because it's probably spurious # (FIXME: is this OK?) if npall ( mxi < mininds ) : continue leftminind = mininds [ mininds < mxi ] [ - 1 ] # the last index to the left rightminind = mininds [ mininds > mxi ] [ 0 ] # the first index to the right relpeakheights [ peakind ] = ( acf [ mxi ] - ( acf [ leftminind ] + acf [ rightminind ] ) / 2.0 ) relpeaklags [ peakind ] = lags [ mxi ] peakindices [ peakind ] = peakind # figure out the bestperiod if possible if relpeakheights [ 0 ] > relpeakheights [ 1 ] : bestlag = relpeaklags [ 0 ] bestpeakheight = relpeakheights [ 0 ] bestpeakindex = peakindices [ 0 ] else : bestlag = relpeaklags [ 1 ] bestpeakheight = relpeakheights [ 1 ] bestpeakindex = peakindices [ 1 ] return { 'maxinds' : maxinds , 'maxacfs' : maxacfs , 'maxlags' : maxlags , 'mininds' : mininds , 'minacfs' : minacfs , 'minlags' : minlags , 'relpeakheights' : relpeakheights , 'relpeaklags' : relpeaklags , 'peakindices' : peakindices , 'bestlag' : bestlag , 'bestpeakheight' : bestpeakheight , 'bestpeakindex' : bestpeakindex }
7724	def __init ( self , affiliation , role , jid = None , nick = None , actor = None , reason = None ) : if not affiliation : affiliation = None elif affiliation not in affiliations : raise ValueError ( "Bad affiliation" ) self . affiliation = affiliation if not role : role = None elif role not in roles : raise ValueError ( "Bad role" ) self . role = role if jid : self . jid = JID ( jid ) else : self . jid = None if actor : self . actor = JID ( actor ) else : self . actor = None self . nick = nick self . reason = reason
1238	def put ( self , item , priority = None ) : if not self . _isfull ( ) : self . _memory . append ( None ) position = self . _next_position_then_increment ( ) old_priority = 0 if self . _memory [ position ] is None else ( self . _memory [ position ] . priority or 0 ) row = _SumRow ( item , priority ) self . _memory [ position ] = row self . _update_internal_nodes ( position , ( row . priority or 0 ) - old_priority )
5701	def _feed_calendar_span ( gtfs , stats ) : n_feeds = _n_gtfs_sources ( gtfs ) [ 0 ] max_start = None min_end = None if n_feeds > 1 : for i in range ( n_feeds ) : feed_key = "feed_" + str ( i ) + "_" start_key = feed_key + "calendar_start" end_key = feed_key + "calendar_end" calendar_span = gtfs . conn . cursor ( ) . execute ( 'SELECT min(date), max(date) FROM trips, days ' 'WHERE trips.trip_I = days.trip_I AND trip_id LIKE ?;' , ( feed_key + '%' , ) ) . fetchone ( ) stats [ start_key ] = calendar_span [ 0 ] stats [ end_key ] = calendar_span [ 1 ] if calendar_span [ 0 ] is not None and calendar_span [ 1 ] is not None : if not max_start and not min_end : max_start = calendar_span [ 0 ] min_end = calendar_span [ 1 ] else : if gtfs . get_day_start_ut ( calendar_span [ 0 ] ) > gtfs . get_day_start_ut ( max_start ) : max_start = calendar_span [ 0 ] if gtfs . get_day_start_ut ( calendar_span [ 1 ] ) < gtfs . get_day_start_ut ( min_end ) : min_end = calendar_span [ 1 ] stats [ "latest_feed_start_date" ] = max_start stats [ "earliest_feed_end_date" ] = min_end else : stats [ "latest_feed_start_date" ] = stats [ "start_date" ] stats [ "earliest_feed_end_date" ] = stats [ "end_date" ] return stats
13731	def validate_is_not_none ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
3996	def _cleanup_path ( path ) : try : yield finally : if os . path . exists ( path ) : if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path )
4032	def _randone ( d , limit = 20 , grouprefs = None ) : if grouprefs is None : grouprefs = { } ret = '' for i in d : if i [ 0 ] == sre_parse . IN : ret += choice ( _in ( i [ 1 ] ) ) elif i [ 0 ] == sre_parse . LITERAL : ret += unichr ( i [ 1 ] ) elif i [ 0 ] == sre_parse . CATEGORY : ret += choice ( CATEGORIES . get ( i [ 1 ] , [ '' ] ) ) elif i [ 0 ] == sre_parse . ANY : ret += choice ( CATEGORIES [ 'category_any' ] ) elif i [ 0 ] == sre_parse . MAX_REPEAT or i [ 0 ] == sre_parse . MIN_REPEAT : if i [ 1 ] [ 1 ] + 1 - i [ 1 ] [ 0 ] >= limit : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 0 ] + limit - 1 else : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 1 ] for _ in range ( randint ( min , max ) ) : ret += _randone ( list ( i [ 1 ] [ 2 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . BRANCH : ret += _randone ( choice ( i [ 1 ] [ 1 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . SUBPATTERN or i [ 0 ] == sre_parse . ASSERT : subexpr = i [ 1 ] [ 1 ] if IS_PY36_OR_GREATER and i [ 0 ] == sre_parse . SUBPATTERN : subexpr = i [ 1 ] [ 3 ] subp = _randone ( subexpr , limit , grouprefs ) if i [ 1 ] [ 0 ] : grouprefs [ i [ 1 ] [ 0 ] ] = subp ret += subp elif i [ 0 ] == sre_parse . AT : continue elif i [ 0 ] == sre_parse . NOT_LITERAL : c = list ( CATEGORIES [ 'category_any' ] ) if unichr ( i [ 1 ] ) in c : c . remove ( unichr ( i [ 1 ] ) ) ret += choice ( c ) elif i [ 0 ] == sre_parse . GROUPREF : ret += grouprefs [ i [ 1 ] ] elif i [ 0 ] == sre_parse . ASSERT_NOT : pass else : print ( '[!] cannot handle expression "%s"' % str ( i ) ) return ret
13548	def update ( dst , src ) : stack = [ ( dst , src ) ] def isdict ( o ) : return hasattr ( o , 'keys' ) while stack : current_dst , current_src = stack . pop ( ) for key in current_src : if key not in current_dst : current_dst [ key ] = current_src [ key ] else : if isdict ( current_src [ key ] ) and isdict ( current_dst [ key ] ) : stack . append ( ( current_dst [ key ] , current_src [ key ] ) ) else : current_dst [ key ] = current_src [ key ] return dst
9212	def get_channel_image ( self , channel , img_size = 300 , skip_cache = False ) : from bs4 import BeautifulSoup from wikipedia . exceptions import PageError import re import wikipedia wikipedia . set_lang ( 'fr' ) if not channel : _LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return # Check if the image is in cache if channel in self . _cache_channel_img and not skip_cache : img = self . _cache_channel_img [ channel ] _LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel_info = self . get_channel_info ( channel ) query = channel_info [ 'wiki_page' ] if not query : _LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return _LOGGER . debug ( 'Query: %s' , query ) # If there is a max image size defined use it. if 'max_img_size' in channel_info : if img_size > channel_info [ 'max_img_size' ] : _LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel_info [ 'max_img_size' ] ) img_size = channel_info [ 'max_img_size' ] try : page = wikipedia . page ( query ) _LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = BeautifulSoup ( page . html ( ) , 'html.parser' ) images = soup . find_all ( 'img' ) img_src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img_src = re . sub ( r'\d+px' , '{}px' . format ( img_size ) , i [ 'src' ] ) img = 'https:{}' . format ( img_src ) if img_src else None # Cache result self . _cache_channel_img [ channel ] = img return img except PageError : _LOGGER . error ( 'Could not fetch channel image for %s' , channel )
5213	def intraday ( ticker , dt , session = '' , * * kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( * * kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( * * kw ) ) return cur_data
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
3905	def add_conversation_tab ( self , conv_id , switch = False ) : conv_widget = self . get_conv_widget ( conv_id ) self . _tabbed_window . set_tab ( conv_widget , switch = switch , title = conv_widget . title )
9769	def delete ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not click . confirm ( "Are sure you want to delete job `{}`" . format ( _job ) ) : click . echo ( 'Existing without deleting job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . job . delete_job ( user , project_name , _job ) # Purge caching JobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Job `{}` was delete successfully" . format ( _job ) )
11104	def acquire_lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with self . locker as r : # get the result acquired , code , _ = r if acquired : try : r = func ( self , * args , * * kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . __name__ ) ) e = None r = None # raise error after exiting with statement and releasing the lock! if e is not None : traceback . print_stack ( ) raise Exception ( e ) return r return wrapper
4036	def ss_wrap ( func ) : def wrapper ( self , * args , * * kwargs ) : if not self . savedsearch : self . savedsearch = SavedSearch ( self ) return func ( self , * args , * * kwargs ) return wrapper
1958	def _open ( self , f ) : if None in self . files : fd = self . files . index ( None ) self . files [ fd ] = f else : fd = len ( self . files ) self . files . append ( f ) return fd
13090	def main ( branch ) : try : # Ensure that we're in a git repository. This command is silent unless # you're not actually in a git repository, in which case, you receive a # "Not a git repository" error message. output = subprocess . check_output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . CalledProcessError : # Bail if we're not in a git repository. return # This behavior ensures a better user experience for those that aren't # intimately familiar with git. ensure_remote_branch_is_tracked ( branch ) # Switch to the specified branch and update it. subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) # Pulling is always safe here, because we never commit to this branch. subprocess . check_call ( [ 'git' , 'pull' , '--quiet' ] ) # Checkout the top commit in the branch, effectively going "untracked." subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) # Clean up the repository of Python cruft. Because we've just switched # branches and compiled Python files should not be version controlled, # there are likely leftover compiled Python files sitting on disk which may # confuse some tools, such as sqlalchemy-migrate. subprocess . check_call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) # For the sake of user experience, give some familiar output. print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
10091	def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add_directive ( 'autopyramid' , RouteDirective )
8269	def contains ( self , clr ) : if not isinstance ( clr , Color ) : return False if not isinstance ( clr , _list ) : clr = [ clr ] for clr in clr : if clr . is_grey and not self . grayscale : return ( self . black . contains ( clr ) or self . white . contains ( clr ) ) for r , v in [ ( self . h , clr . h ) , ( self . s , clr . s ) , ( self . b , clr . brightness ) , ( self . a , clr . a ) ] : if isinstance ( r , _list ) : pass elif isinstance ( r , tuple ) : r = [ r ] else : r = [ ( r , r ) ] for min , max in r : if not ( min <= v <= max ) : return False return True
2230	def register ( self , hash_types ) : # ensure iterable if not isinstance ( hash_types , ( list , tuple ) ) : hash_types = [ hash_types ] def _decor_closure ( hash_func ) : for hash_type in hash_types : key = ( hash_type . __module__ , hash_type . __name__ ) self . keyed_extensions [ key ] = ( hash_type , hash_func ) return hash_func return _decor_closure
13094	def start_processes ( self ) : self . relay = subprocess . Popen ( [ 'ntlmrelayx.py' , '-6' , '-tf' , self . targets_file , '-w' , '-l' , self . directory , '-of' , self . output_file ] , cwd = self . directory ) self . responder = subprocess . Popen ( [ 'responder' , '-I' , self . interface_name ] )
10966	def set_shape ( self , shape , inner ) : for c in self . comps : c . set_shape ( shape , inner )
5641	def remove_dangling_shapes ( db_conn ) : db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL = "SELECT trips.trip_I, shape_id, min(shape_break) as min_shape_break, max(shape_break) as max_shape_break FROM trips, stop_times WHERE trips.trip_I=stop_times.trip_I GROUP BY trips.trip_I" trip_min_max_shape_seqs = pandas . read_sql ( SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL , db_conn ) rows = [ ] for row in trip_min_max_shape_seqs . itertuples ( ) : shape_id , min_shape_break , max_shape_break = row . shape_id , row . min_shape_break , row . max_shape_break if min_shape_break is None or max_shape_break is None : min_shape_break = float ( '-inf' ) max_shape_break = float ( '-inf' ) rows . append ( ( shape_id , min_shape_break , max_shape_break ) ) DELETE_SQL_BASE = "DELETE FROM shapes WHERE shape_id=? AND (seq<? OR seq>?)" db_conn . executemany ( DELETE_SQL_BASE , rows ) remove_dangling_shapes_references ( db_conn )
13780	def FindEnumTypeByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) if full_name not in self . _enum_descriptors : self . FindFileContainingSymbol ( full_name ) return self . _enum_descriptors [ full_name ]
12158	def abfSort ( IDs ) : IDs = list ( IDs ) monO = [ ] monN = [ ] monD = [ ] good = [ ] for ID in IDs : if ID is None : continue if 'o' in ID : monO . append ( ID ) elif 'n' in ID : monN . append ( ID ) elif 'd' in ID : monD . append ( ID ) else : good . append ( ID ) return sorted ( good ) + sorted ( monO ) + sorted ( monN ) + sorted ( monD )
10862	def param_particle_rad ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , 'a' ) for i in ind ]
11013	def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except SubprocessError : context . exit ( 1 )
12923	def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
6427	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) src_comp = bz2 . compress ( src , self . _level ) [ 10 : ] tar_comp = bz2 . compress ( tar , self . _level ) [ 10 : ] concat_comp = bz2 . compress ( src + tar , self . _level ) [ 10 : ] concat_comp2 = bz2 . compress ( tar + src , self . _level ) [ 10 : ] return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
7587	def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . tests , list ) : ld = [ [ ( key , i [ key ] ) for key in keys ] for i in self . tests ] dd = [ dict ( i ) for i in ld ] df = pd . DataFrame ( dd ) return df else : return pd . DataFrame ( pd . Series ( self . tests ) ) . T else : return None
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
2445	def reset_package ( self ) : # FIXME: this state does not make sense self . package_set = False self . package_vers_set = False self . package_file_name_set = False self . package_supplier_set = False self . package_originator_set = False self . package_down_location_set = False self . package_home_set = False self . package_verif_set = False self . package_chk_sum_set = False self . package_source_info_set = False self . package_conc_lics_set = False self . package_license_declared_set = False self . package_license_comment_set = False self . package_cr_text_set = False self . package_summary_set = False self . package_desc_set = False
845	def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm # Sparse memory if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == "rawOverlap" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfInput" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == "pctOverlapOfProto" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfLarger" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "norm" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( "Unimplemented distance method %s" % self . distanceMethod ) # Dense memory else : if self . distanceMethod == "norm" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( "Not implemented yet for dense storage...." ) return dist
11597	def _rc_dbsize ( self ) : result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
10878	def calculate_polychrome_linescan_psf ( x , y , z , normalize = False , kfki = 0.889 , sigkf = 0.1 , zint = 100. , nkpts = 3 , dist_type = 'gaussian' , wrap = True , * * kwargs ) : kfkipts , wts = get_polydisp_pts_wts ( kfki , sigkf , dist_type = dist_type , nkpts = nkpts ) #0. Set up vecs if wrap : xpts = vec_to_halfvec ( x ) ypts = vec_to_halfvec ( y ) x3 , y3 , z3 = np . meshgrid ( xpts , ypts , z , indexing = 'ij' ) else : x3 , y3 , z3 = np . meshgrid ( x , y , z , indexing = 'ij' ) rho3 = np . sqrt ( x3 * x3 + y3 * y3 ) #1. Hilm if wrap : y2 , z2 = np . meshgrid ( ypts , z , indexing = 'ij' ) hilm0 = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , * * kwargs ) if ypts [ 0 ] == 0 : hilm = np . append ( hilm0 [ - 1 : 0 : - 1 ] , hilm0 , axis = 0 ) else : hilm = np . append ( hilm0 [ : : - 1 ] , hilm0 , axis = 0 ) else : y2 , z2 = np . meshgrid ( y , z , indexing = 'ij' ) hilm = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , * * kwargs ) #2. Hdet if wrap : #Lambda function that ignores its args but still returns correct values func = lambda x , y , z , kfki = 1. : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) [ 0 ] hdet_func = lambda kfki : wrap_and_calc_psf ( xpts , ypts , z , func , kfki = kfki ) else : hdet_func = lambda kfki : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) [ 0 ] ##### inner = [ wts [ a ] * hdet_func ( kfkipts [ a ] ) for a in range ( nkpts ) ] hdet = np . sum ( inner , axis = 0 ) if normalize : hilm /= hilm . sum ( ) hdet /= hdet . sum ( ) for a in range ( x . size ) : hdet [ a ] *= hilm return hdet if normalize else hdet / hdet . sum ( )
12936	def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . __name__ , ) : self [ name ] = blok return blok return decorator
3651	def logger ( name = None , save = False ) : logger = logging . getLogger ( name ) if save : logformat = '%(asctime)s [%(levelname)s] [%(name)s] %(funcName)s: %(message)s (line %(lineno)d)' log_file_path = 'fut.log' # TODO: define logpath open ( log_file_path , 'w' ) . write ( '' ) # remove old logs logger . setLevel ( logging . DEBUG ) logger_handler = logging . FileHandler ( log_file_path ) logger_handler . setFormatter ( logging . Formatter ( logformat ) ) else : logger_handler = NullHandler ( ) logger . addHandler ( logger_handler ) return logger
639	def getBool ( cls , prop ) : value = cls . getInt ( prop ) if value not in ( 0 , 1 ) : raise ValueError ( "Expected 0 or 1, but got %r in config property %s" % ( value , prop ) ) return bool ( value )
3937	def submit_form ( self , form_selector , input_dict ) : logger . info ( 'Submitting form on page %r' , self . _page . url . split ( '?' ) [ 0 ] ) logger . info ( 'Page contains forms: %s' , [ elem . get ( 'id' ) for elem in self . _page . soup . select ( 'form' ) ] ) try : form = self . _page . soup . select ( form_selector ) [ 0 ] except IndexError : raise GoogleAuthError ( 'Failed to find form {!r} in page' . format ( form_selector ) ) logger . info ( 'Page contains inputs: %s' , [ elem . get ( 'id' ) for elem in form . select ( 'input' ) ] ) for selector , value in input_dict . items ( ) : try : form . select ( selector ) [ 0 ] [ 'value' ] = value except IndexError : raise GoogleAuthError ( 'Failed to find input {!r} in form' . format ( selector ) ) try : self . _page = self . _browser . submit ( form , self . _page . url ) self . _page . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Failed to submit form: {}' . format ( e ) )
2674	def init ( src , minimal = False ) : templates_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , 'project_templates' , ) for filename in os . listdir ( templates_path ) : if ( minimal and filename == 'event.json' ) or filename . endswith ( '.pyc' ) : continue dest_path = os . path . join ( templates_path , filename ) if not os . path . isdir ( dest_path ) : copy ( dest_path , src )
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
3961	def update_managed_repos ( force = False ) : log_to_client ( 'Pulling latest updates for all active managed repos:' ) update_specs_repo_and_known_hosts ( ) repos_to_update = get_all_repos ( active_only = True , include_specs_repo = False ) with parallel_task_queue ( ) as queue : log_to_client ( 'Updating managed repos' ) for repo in repos_to_update : if not repo . is_overridden : repo . update_local_repo_async ( queue , force = force )
7765	def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
11000	def _tz ( self , z ) : return ( z - self . param_dict [ 'psf-zslab' ] ) * self . param_dict [ self . zscale ]
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
7570	def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : ## read in paired end read files 4 lines at a time if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) ## a list to store until writing writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) ## write to disk counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
13740	def connect ( self ) : if not self . connected ( ) : self . _ws = create_connection ( self . WS_URI ) message = { 'type' : self . WS_TYPE , 'product_id' : self . WS_PRODUCT_ID } self . _ws . send ( dumps ( message ) ) # There will be only one keep alive thread per client instance with self . _lock : if not self . _thread : thread = Thread ( target = self . _keep_alive_thread , args = [ ] ) thread . start ( )
2178	def authorization_url ( self , url , request_token = None , * * kwargs ) : kwargs [ "oauth_token" ] = request_token or self . _client . client . resource_owner_key log . debug ( "Adding parameters %s to url %s" , kwargs , url ) return add_params_to_uri ( url , kwargs . items ( ) )
11099	def select_by_mtime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . mtime <= max_time return self . select_file ( filters , recursive )
10419	def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
8583	def get_attached_volume ( self , datacenter_id , server_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes/%s' % ( datacenter_id , server_id , volume_id ) ) return response
9588	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } if self . session_id is not None : data . setdefault ( 'session_id' , self . session_id ) data = self . _wrap_el ( data ) res = self . remote_invoker . execute ( command , data ) ret = WebDriverResult . from_object ( res ) ret . raise_for_status ( ) ret . value = self . _unwrap_el ( ret . value ) if not unpack : return ret return ret . value
6980	def _epd_function ( coeffs , fluxes , xcc , ycc , bgv , bge ) : epdf = ( coeffs [ 0 ] + coeffs [ 1 ] * npsin ( 2 * MPI * xcc ) + coeffs [ 2 ] * npcos ( 2 * MPI * xcc ) + coeffs [ 3 ] * npsin ( 2 * MPI * ycc ) + coeffs [ 4 ] * npcos ( 2 * MPI * ycc ) + coeffs [ 5 ] * npsin ( 4 * MPI * xcc ) + coeffs [ 6 ] * npcos ( 4 * MPI * xcc ) + coeffs [ 7 ] * npsin ( 4 * MPI * ycc ) + coeffs [ 8 ] * npcos ( 4 * MPI * ycc ) + coeffs [ 9 ] * bgv + coeffs [ 10 ] * bge ) return epdf
2808	def convert_matmul ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting matmul ...' ) if names == 'short' : tf_name = 'MMUL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) == 1 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) elif len ( inputs ) == 2 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) else : raise AssertionError ( 'Cannot convert matmul layer' )
13138	def http_get_provider ( provider , request_url , params , token_secret , token_cookie = None ) : if not validate_provider ( provider ) : raise InvalidUsage ( 'Provider not supported' ) klass = getattr ( socialauth . providers , provider . capitalize ( ) ) provider = klass ( request_url , params , token_secret , token_cookie ) if provider . status == 302 : ret = dict ( status = 302 , redirect = provider . redirect ) tc = getattr ( provider , 'set_token_cookie' , None ) if tc is not None : ret [ 'set_token_cookie' ] = tc return ret if provider . status == 200 and provider . user_id is not None : ret = dict ( status = 200 , provider_user_id = provider . user_id ) if provider . user_name is not None : ret [ 'provider_user_name' ] = provider . user_name return ret raise InvalidUsage ( 'Invalid request' )
11998	def _encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . _hmac_generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . _aes_encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
2579	def cleanup ( self ) : logger . info ( "DFK cleanup initiated" ) # this check won't detect two DFK cleanups happening from # different threads extremely close in time because of # non-atomic read/modify of self.cleanup_called if self . cleanup_called : raise Exception ( "attempt to clean up DFK when it has already been cleaned-up" ) self . cleanup_called = True self . log_task_states ( ) # Checkpointing takes priority over the rest of the tasks # checkpoint if any valid checkpoint method is specified if self . checkpoint_mode is not None : self . checkpoint ( ) if self . _checkpoint_timer : logger . info ( "Stopping checkpoint timer" ) self . _checkpoint_timer . close ( ) # Send final stats self . usage_tracker . send_message ( ) self . usage_tracker . close ( ) logger . info ( "Terminating flow_control and strategy threads" ) self . flowcontrol . close ( ) for executor in self . executors . values ( ) : if executor . managed : if executor . scaling_enabled : job_ids = executor . provider . resources . keys ( ) executor . scale_in ( len ( job_ids ) ) executor . shutdown ( ) self . time_completed = datetime . datetime . now ( ) if self . monitoring : self . monitoring . send ( MessageType . WORKFLOW_INFO , { 'tasks_failed_count' : self . tasks_failed_count , 'tasks_completed_count' : self . tasks_completed_count , "time_began" : self . time_began , 'time_completed' : self . time_completed , 'workflow_duration' : ( self . time_completed - self . time_began ) . total_seconds ( ) , 'run_id' : self . run_id , 'rundir' : self . run_dir } ) self . monitoring . close ( ) """ if self.logging_server is not None: self.logging_server.terminate() self.logging_server.join() if self.web_app is not None: self.web_app.terminate() self.web_app.join() """ logger . info ( "DFK cleanup complete" )
1015	def _getCellForNewSegment ( self , colIdx ) : # Not fixed size CLA, just choose a cell randomly if self . maxSegmentsPerCell < 0 : if self . cellsPerColumn > 1 : # Don't ever choose the start cell (cell # 0) in each column i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 else : i = 0 return i # Fixed size CLA, choose from among the cells that are below the maximum # number of segments. # NOTE: It is important NOT to always pick the cell with the fewest number # of segments. The reason is that if we always do that, we are more likely # to run into situations where we choose the same set of cell indices to # represent an 'A' in both context 1 and context 2. This is because the # cell indices we choose in each column of a pattern will advance in # lockstep (i.e. we pick cell indices of 1, then cell indices of 2, etc.). candidateCellIdxs = [ ] if self . cellsPerColumn == 1 : minIdx = 0 maxIdx = 0 else : minIdx = 1 # Don't include startCell in the mix maxIdx = self . cellsPerColumn - 1 for i in xrange ( minIdx , maxIdx + 1 ) : numSegs = len ( self . cells [ colIdx ] [ i ] ) if numSegs < self . maxSegmentsPerCell : candidateCellIdxs . append ( i ) # If we found one, return with it. Note we need to use _random to maintain # correspondence with CPP code. if len ( candidateCellIdxs ) > 0 : #candidateCellIdx = random.choice(candidateCellIdxs) candidateCellIdx = ( candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) if self . verbosity >= 5 : print "Cell [%d,%d] chosen for new segment, # of segs is %d" % ( colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) return candidateCellIdx # All cells in the column are full, find a segment to free up candidateSegment = None candidateSegmentDC = 1.0 # For each cell in this column for i in xrange ( minIdx , maxIdx + 1 ) : # For each segment in this cell for s in self . cells [ colIdx ] [ i ] : dc = s . dutyCycle ( ) if dc < candidateSegmentDC : candidateCellIdx = i candidateSegmentDC = dc candidateSegment = s # Free up the least used segment if self . verbosity >= 5 : print ( "Deleting segment #%d for cell[%d,%d] to make room for new " "segment" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) candidateSegment . debugPrint ( ) self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) return candidateCellIdx
10347	def run_rcr ( graph , tag = 'dgxp' ) : # Step 1: Calculate the hypothesis subnetworks (just simple star graphs) hypotheses = defaultdict ( set ) increases = defaultdict ( set ) decreases = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : hypotheses [ u ] . add ( v ) if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : increases [ u ] . add ( v ) elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : decreases [ u ] . add ( v ) # Step 2: Calculate the matching of the data points to the causal relationships #: A dictionary from {tuple controller node: int count of correctly matching observations} correct = defaultdict ( int ) #: A dictionary from {tuple controller node: int count of incorrectly matching observations} contra = defaultdict ( int ) #: A dictionary from {tuple controller node: int count of ambiguous observations} ambiguous = defaultdict ( int ) #: A dictionary from {tuple controller node: int count of missing obvservations} missing = defaultdict ( int ) for controller , downstream_nodes in hypotheses . items ( ) : if len ( downstream_nodes ) < 4 : continue # need enough data to make reasonable calculations! for node in downstream_nodes : if node in increases [ controller ] and node in decreases [ controller ] : ambiguous [ controller ] += 1 elif node in increases [ controller ] : if graph . node [ node ] [ tag ] == 1 : correct [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : contra [ controller ] += 1 elif node in decreases [ controller ] : if graph . node [ node ] [ tag ] == 1 : contra [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : correct [ controller ] += 1 else : missing [ controller ] += 1 # Step 3: Keep only controller nodes who have 4 or more downstream nodes controllers = { controller for controller , downstream_nodes in hypotheses . items ( ) if 4 <= len ( downstream_nodes ) } # Step 4: Calculate concordance scores concordance_scores = { controller : scipy . stats . beta ( 0.5 , correct [ controller ] , contra [ controller ] ) for controller in controllers } # Step 5: Calculate richness scores # TODO # Calculate the population as the union of all downstream nodes for all controllers population = { node for controller in controllers for node in hypotheses [ controller ] } population_size = len ( population ) # Step 6: Export return pandas . DataFrame ( { 'contra' : contra , 'correct' : correct , 'concordance' : concordance_scores } )
8918	def _get_version ( self ) : version = self . _get_param ( param = "version" , allowed_values = allowed_versions [ self . params [ 'service' ] ] , optional = True ) if version is None and self . _get_request_type ( ) != "getcapabilities" : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) else : return version
6825	def deploy_services ( self , site = None ) : verbose = self . verbose r = self . local_renderer if not r . env . manage_configs : return # # target_sites = self.genv.available_sites_by_host.get(hostname, None) self . render_paths ( ) supervisor_services = [ ] if r . env . purge_all_confs : r . sudo ( 'rm -Rf /etc/supervisor/conf.d/*' ) #TODO:check available_sites_by_host and remove dead? self . write_configs ( site = site ) for _site , site_data in self . iter_sites ( site = site , renderer = self . render_paths ) : if verbose : print ( 'deploy_services.site:' , _site ) # Only load site configurations that are allowed for this host. # if target_sites is not None: # assert isinstance(target_sites, (tuple, list)) # if site not in target_sites: # continue for cb in self . genv . _supervisor_create_service_callbacks : if self . verbose : print ( 'cb:' , cb ) ret = cb ( site = _site ) if self . verbose : print ( 'ret:' , ret ) if isinstance ( ret , six . string_types ) : supervisor_services . append ( ret ) elif isinstance ( ret , tuple ) : assert len ( ret ) == 2 conf_name , conf_content = ret if self . dryrun : print ( 'supervisor conf filename:' , conf_name ) print ( conf_content ) self . write_to_file ( conf_content ) self . env . services_rendered = '\n' . join ( supervisor_services ) fn = self . render_to_file ( self . env . config_template ) r . put ( local_path = fn , remote_path = self . env . config_path , use_sudo = True ) # We use supervisorctl to configure supervisor, but this will throw a uselessly vague # error message is supervisor isn't running. if not self . is_running ( ) : self . start ( ) # Reload config and then add and remove as necessary (restarts programs) r . sudo ( 'supervisorctl update' )
329	def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) # alpha and beta X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
157	def noise2d ( self , x , y ) : # Place input coordinates onto grid. stretch_offset = ( x + y ) * STRETCH_CONSTANT_2D xs = x + stretch_offset ys = y + stretch_offset # Floor to get grid coordinates of rhombus (stretched square) super-cell origin. xsb = floor ( xs ) ysb = floor ( ys ) # Skew out to get actual coordinates of rhombus origin. We'll need these later. squish_offset = ( xsb + ysb ) * SQUISH_CONSTANT_2D xb = xsb + squish_offset yb = ysb + squish_offset # Compute grid coordinates relative to rhombus origin. xins = xs - xsb yins = ys - ysb # Sum those together to get a value that determines which region we're in. in_sum = xins + yins # Positions relative to origin point. dx0 = x - xb dy0 = y - yb value = 0 # Contribution (1,0) dx1 = dx0 - 1 - SQUISH_CONSTANT_2D dy1 = dy0 - 0 - SQUISH_CONSTANT_2D attn1 = 2 - dx1 * dx1 - dy1 * dy1 extrapolate = self . _extrapolate2d if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , dx1 , dy1 ) # Contribution (0,1) dx2 = dx0 - 0 - SQUISH_CONSTANT_2D dy2 = dy0 - 1 - SQUISH_CONSTANT_2D attn2 = 2 - dx2 * dx2 - dy2 * dy2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , dx2 , dy2 ) if in_sum <= 1 : # We're inside the triangle (2-Simplex) at (0,0) zins = 1 - in_sum if zins > xins or zins > yins : # (0,0) is one of the closest two triangular vertices if xins > yins : xsv_ext = xsb + 1 ysv_ext = ysb - 1 dx_ext = dx0 - 1 dy_ext = dy0 + 1 else : xsv_ext = xsb - 1 ysv_ext = ysb + 1 dx_ext = dx0 + 1 dy_ext = dy0 - 1 else : # (1,0) and (0,1) are the closest two vertices. xsv_ext = xsb + 1 ysv_ext = ysb + 1 dx_ext = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 1 - 2 * SQUISH_CONSTANT_2D else : # We're inside the triangle (2-Simplex) at (1,1) zins = 2 - in_sum if zins < xins or zins < yins : # (0,0) is one of the closest two triangular vertices if xins > yins : xsv_ext = xsb + 2 ysv_ext = ysb + 0 dx_ext = dx0 - 2 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 + 0 - 2 * SQUISH_CONSTANT_2D else : xsv_ext = xsb + 0 ysv_ext = ysb + 2 dx_ext = dx0 + 0 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 2 - 2 * SQUISH_CONSTANT_2D else : # (1,0) and (0,1) are the closest two vertices. dx_ext = dx0 dy_ext = dy0 xsv_ext = xsb ysv_ext = ysb xsb += 1 ysb += 1 dx0 = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy0 = dy0 - 1 - 2 * SQUISH_CONSTANT_2D # Contribution (0,0) or (1,1) attn0 = 2 - dx0 * dx0 - dy0 * dy0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb , ysb , dx0 , dy0 ) # Extra Vertex attn_ext = 2 - dx_ext * dx_ext - dy_ext * dy_ext if attn_ext > 0 : attn_ext *= attn_ext value += attn_ext * attn_ext * extrapolate ( xsv_ext , ysv_ext , dx_ext , dy_ext ) return value / NORM_CONSTANT_2D
3555	def power_on ( self , timeout_sec = TIMEOUT_SEC ) : # Turn on bluetooth and wait for powered on event to be set. self . _powered_on . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 1 ) if not self . _powered_on . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power on!' )
13037	def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
7340	async def get_media_metadata ( data , path = None ) : if isinstance ( data , bytes ) : media_type = await get_type ( data , path ) else : raise TypeError ( "get_metadata input must be a bytes" ) media_category = get_category ( media_type ) _logger . info ( "media_type: %s, media_category: %s" % ( media_type , media_category ) ) return media_type , media_category
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
1476	def _get_ckptmgr_process ( self ) : ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager' ckptmgr_ram_mb = self . checkpoint_manager_ram / ( 1024 * 1024 ) ckptmgr_cmd = [ os . path . join ( self . heron_java_home , "bin/java" ) , '-Xms%dM' % ckptmgr_ram_mb , '-Xmx%dM' % ckptmgr_ram_mb , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+UseConcMarkSweepGC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . checkpoint_manager_classpath , ckptmgr_main_class , '-t' + self . topology_name , '-i' + self . topology_id , '-c' + self . ckptmgr_ids [ self . shard ] , '-p' + self . checkpoint_manager_port , '-f' + self . stateful_config_file , '-o' + self . override_config_file , '-g' + self . heron_internals_config_file ] retval = { } retval [ self . ckptmgr_ids [ self . shard ] ] = Command ( ckptmgr_cmd , self . shell_env ) return retval
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
1800	def CMOVNO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , src . read ( ) , dest . read ( ) ) )
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
2980	def cmd_logs ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )
7464	def _parse_00 ( ofile ) : with open ( ofile ) as infile : ## read in the results summary from the end of the outfile arr = np . array ( [ " " ] + infile . read ( ) . split ( "Summary of MCMC results\n\n\n" ) [ 1 : ] [ 0 ] . strip ( ) . split ( ) ) ## reshape array rows = 12 cols = ( arr . shape [ 0 ] + 1 ) / rows arr = arr . reshape ( rows , cols ) ## make into labeled data frame df = pd . DataFrame ( data = arr [ 1 : , 1 : ] , columns = arr [ 0 , 1 : ] , index = arr [ 1 : , 0 ] , ) . T return df
6231	def apply_mesh_programs ( self , mesh_programs = None ) : if not mesh_programs : mesh_programs = [ ColorProgram ( ) , TextureProgram ( ) , FallbackProgram ( ) ] for mesh in self . meshes : for mp in mesh_programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , MeshProgram ) : mesh . mesh_program = mp break else : raise ValueError ( "apply() must return a MeshProgram instance, not {}" . format ( type ( instance ) ) ) if not mesh . mesh_program : print ( "WARING: No mesh program applied to '{}'" . format ( mesh . name ) )
191	def blend_alpha ( image_fg , image_bg , alpha , eps = 1e-2 ) : assert image_fg . shape == image_bg . shape assert image_fg . dtype . kind == image_bg . dtype . kind # TODO switch to gate_dtypes() assert image_fg . dtype . name not in [ "float128" ] assert image_bg . dtype . name not in [ "float128" ] # TODO add test for this input_was_2d = ( len ( image_fg . shape ) == 2 ) if input_was_2d : image_fg = np . atleast_3d ( image_fg ) image_bg = np . atleast_3d ( image_bg ) input_was_bool = False if image_fg . dtype . kind == "b" : input_was_bool = True # use float32 instead of float16 here because it seems to be faster image_fg = image_fg . astype ( np . float32 ) image_bg = image_bg . astype ( np . float32 ) alpha = np . array ( alpha , dtype = np . float64 ) if alpha . size == 1 : pass else : if alpha . ndim == 2 : assert alpha . shape == image_fg . shape [ 0 : 2 ] alpha = alpha . reshape ( ( alpha . shape [ 0 ] , alpha . shape [ 1 ] , 1 ) ) elif alpha . ndim == 3 : assert alpha . shape == image_fg . shape or alpha . shape == image_fg . shape [ 0 : 2 ] + ( 1 , ) else : alpha = alpha . reshape ( ( 1 , 1 , - 1 ) ) if alpha . shape [ 2 ] != image_fg . shape [ 2 ] : alpha = np . tile ( alpha , ( 1 , 1 , image_fg . shape [ 2 ] ) ) if not input_was_bool : if np . all ( alpha >= 1.0 - eps ) : return np . copy ( image_fg ) elif np . all ( alpha <= eps ) : return np . copy ( image_bg ) # for efficiency reaons, only test one value of alpha here, even if alpha is much larger assert 0 <= alpha . item ( 0 ) <= 1.0 dt_images = iadt . get_minimal_dtype ( [ image_fg , image_bg ] ) # doing this only for non-float images led to inaccuracies for large floats values isize = dt_images . itemsize * 2 isize = max ( isize , 4 ) # at least 4 bytes (=float32), tends to be faster than float16 dt_blend = np . dtype ( "f%d" % ( isize , ) ) if alpha . dtype != dt_blend : alpha = alpha . astype ( dt_blend ) if image_fg . dtype != dt_blend : image_fg = image_fg . astype ( dt_blend ) if image_bg . dtype != dt_blend : image_bg = image_bg . astype ( dt_blend ) # the following is equivalent to # image_blend = alpha * image_fg + (1 - alpha) * image_bg # but supposedly faster image_blend = image_bg + alpha * ( image_fg - image_bg ) if input_was_bool : image_blend = image_blend > 0.5 else : # skip clip, because alpha is expected to be in range [0.0, 1.0] and both images must have same dtype # dont skip round, because otherwise it is very unlikely to hit the image's max possible value image_blend = iadt . restore_dtypes_ ( image_blend , dt_images , clip = False , round = True ) if input_was_2d : return image_blend [ : , : , 0 ] return image_blend
9194	def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTPBadRequest ( "Missing EPUB in POST body." ) is_pre_publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub_upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from_file ( epub_upload ) except : # noqa: E722 raise httpexceptions . HTTPBadRequest ( 'Format not recognized.' ) # Make a publication entry in the database for status checking # the publication. This also creates publication entries for all # of the content in the EPUB. with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : epub_upload . seek ( 0 ) publication_id , publications = add_publication ( cursor , epub , epub_upload , is_pre_publication ) # Poke at the publication & lookup its state. state , messages = poke_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response_data
13688	def _generate_html_diff ( self , expected_fn , expected_lines , obtained_fn , obtained_lines ) : import difflib differ = difflib . HtmlDiff ( ) return differ . make_file ( fromlines = expected_lines , fromdesc = expected_fn , tolines = obtained_lines , todesc = obtained_fn , )
8559	def update_lan ( self , datacenter_id , lan_id , name = None , public = None , ip_failover = None ) : data = { } if name : data [ 'name' ] = name if public is not None : data [ 'public' ] = public if ip_failover : data [ 'ipFailover' ] = ip_failover response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
6660	def random_forest_error ( forest , X_train , X_test , inbag = None , calibrate = True , memory_constrained = False , memory_limit = None ) : if inbag is None : inbag = calc_inbag ( X_train . shape [ 0 ] , forest ) pred = np . array ( [ tree . predict ( X_test ) for tree in forest ] ) . T pred_mean = np . mean ( pred , 0 ) pred_centered = pred - pred_mean n_trees = forest . n_estimators V_IJ = _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained , memory_limit ) V_IJ_unbiased = _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) # Correct for cases where resampling is done without replacement: if np . max ( inbag ) == 1 : variance_inflation = 1 / ( 1 - np . mean ( inbag ) ) ** 2 V_IJ_unbiased *= variance_inflation if not calibrate : return V_IJ_unbiased if V_IJ_unbiased . shape [ 0 ] <= 20 : print ( "No calibration with n_samples <= 20" ) return V_IJ_unbiased if calibrate : calibration_ratio = 2 n_sample = np . ceil ( n_trees / calibration_ratio ) new_forest = copy . deepcopy ( forest ) new_forest . estimators_ = np . random . permutation ( new_forest . estimators_ ) [ : int ( n_sample ) ] new_forest . n_estimators = int ( n_sample ) results_ss = random_forest_error ( new_forest , X_train , X_test , calibrate = False , memory_constrained = memory_constrained , memory_limit = memory_limit ) # Use this second set of variance estimates # to estimate scale of Monte Carlo noise sigma2_ss = np . mean ( ( results_ss - V_IJ_unbiased ) ** 2 ) delta = n_sample / n_trees sigma2 = ( delta ** 2 + ( 1 - delta ) ** 2 ) / ( 2 * ( 1 - delta ) ** 2 ) * sigma2_ss # Use Monte Carlo noise scale estimate for empirical Bayes calibration V_IJ_calibrated = calibrateEB ( V_IJ_unbiased , sigma2 ) return V_IJ_calibrated
4347	def tempo ( self , factor , audio_type = None , quick = False ) : if not is_number ( factor ) or factor <= 0 : raise ValueError ( "factor must be a positive number" ) if factor < 0.5 or factor > 2 : logger . warning ( "Using an extreme time stretching factor. " "Quality of results will be poor" ) if abs ( factor - 1.0 ) <= 0.1 : logger . warning ( "For this stretch factor, " "the stretch effect has better performance." ) if audio_type not in [ None , 'm' , 's' , 'l' ] : raise ValueError ( "audio_type must be one of None, 'm', 's', or 'l'." ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'tempo' ] if quick : effect_args . append ( '-q' ) if audio_type is not None : effect_args . append ( '-{}' . format ( audio_type ) ) effect_args . append ( '{:f}' . format ( factor ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'tempo' ) return self
3255	def list_granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
9501	def _disassemble ( self , lineno_width = 3 , mark_as_current = False ) : fields = [ ] # Column: Source code line number if lineno_width : if self . starts_line is not None : lineno_fmt = "%%%dd" % lineno_width fields . append ( lineno_fmt % self . starts_line ) else : fields . append ( ' ' * lineno_width ) # Column: Current instruction indicator if mark_as_current : fields . append ( '-->' ) else : fields . append ( ' ' ) # Column: Jump target marker if self . is_jump_target : fields . append ( '>>' ) else : fields . append ( ' ' ) # Column: Instruction offset from start of code sequence fields . append ( repr ( self . offset ) . rjust ( 4 ) ) # Column: Opcode name fields . append ( self . opname . ljust ( 20 ) ) # Column: Opcode argument if self . arg is not None : fields . append ( repr ( self . arg ) . rjust ( 5 ) ) # Column: Opcode argument details if self . argrepr : fields . append ( '(' + self . argrepr + ')' ) return ' ' . join ( fields ) . rstrip ( )
6843	def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
11596	def _rc_keys ( self , pattern = '*' ) : result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
8894	def add_to_deleted_models ( sender , instance = None , * args , * * kwargs ) : if issubclass ( sender , SyncableModel ) : instance . _update_deleted_models ( )
13739	def _keep_alive_thread ( self ) : while True : with self . _lock : if self . connected ( ) : self . _ws . ping ( ) else : self . disconnect ( ) self . _thread = None return sleep ( 30 )
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
3073	def _load_config ( self , client_secrets_file , client_id , client_secret ) : if client_id and client_secret : self . client_id , self . client_secret = client_id , client_secret return if client_secrets_file : self . _load_client_secrets ( client_secrets_file ) return if 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' in self . app . config : self . _load_client_secrets ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' ] ) return try : self . client_id , self . client_secret = ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_ID' ] , self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRET' ] ) except KeyError : raise ValueError ( 'OAuth2 configuration could not be found. Either specify the ' 'client_secrets_file or client_id and client_secret or set ' 'the app configuration variables ' 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE or ' 'GOOGLE_OAUTH2_CLIENT_ID and GOOGLE_OAUTH2_CLIENT_SECRET.' )
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : # assumed to be running an IPython magic from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) # all vectors need to be transposed for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
9807	def teardown ( file ) : # pylint:disable=redefined-builtin config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print_error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
6463	def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( ' %-12s %s' % ( function + ':' , doc ) ) return 0
1653	def CheckGlobalStatic ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Match two lines at a time to support multiline declarations if linenum + 1 < clean_lines . NumLines ( ) and not Search ( r'[;({]' , line ) : line += clean_lines . elided [ linenum + 1 ] . strip ( ) # Check for people declaring static/global STL strings at the top level. # This is dangerous because the C++ language does not guarantee that # globals with constructors are initialized before the first access, and # also because globals can be destroyed when some threads are still running. # TODO(unknown): Generalize this to also find static unique_ptr instances. # TODO(unknown): File bugs for clang-tidy to find these. match = Match ( r'((?:|static +)(?:|const +))(?::*std::)?string( +const)? +' r'([a-zA-Z0-9_:]+)\b(.*)' , line ) # Remove false positives: # - String pointers (as opposed to values). # string *pointer # const string *pointer # string const *pointer # string *const pointer # # - Functions and template specializations. # string Function<Type>(... # string Class<Type>::Method(... # # - Operators. These are matched separately because operator names # cross non-word boundaries, and trying to match both operators # and functions at the same time would decrease accuracy of # matching identifiers. # string Class::operator*() if ( match and not Search ( r'\bstring\b(\s+const)?\s*[\*\&]\s*(const\s+)?\w' , line ) and not Search ( r'\boperator\W' , line ) and not Match ( r'\s*(<.*>)?(::[a-zA-Z0-9_]+)*\s*\(([^"]|$)' , match . group ( 4 ) ) ) : if Search ( r'\bconst\b' , line ) : error ( filename , linenum , 'runtime/string' , 4 , 'For a static/global string constant, use a C style string ' 'instead: "%schar%s %s[]".' % ( match . group ( 1 ) , match . group ( 2 ) or '' , match . group ( 3 ) ) ) else : error ( filename , linenum , 'runtime/string' , 4 , 'Static/global string variables are not permitted.' ) if ( Search ( r'\b([A-Za-z0-9_]*_)\(\1\)' , line ) or Search ( r'\b([A-Za-z0-9_]*_)\(CHECK_NOTNULL\(\1\)\)' , line ) ) : error ( filename , linenum , 'runtime/init' , 4 , 'You seem to be initializing a member variable with itself.' )
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
8967	def step ( self , key , chain ) : if chain == "sending" : self . __previous_sending_chain_length = self . sending_chain_length self . __sending_chain = self . __SendingChain ( key ) if chain == "receiving" : self . __receiving_chain = self . __ReceivingChain ( key )
3284	def handle_error ( self , request , client_address ) : ei = sys . exc_info ( ) e = ei [ 1 ] # Suppress stack trace when client aborts connection disgracefully: # 10053: Software caused connection abort # 10054: Connection reset by peer if e . args [ 0 ] in ( 10053 , 10054 ) : _logger . error ( "*** Caught socket.error: {}" . format ( e ) ) return # This is what BaseHTTPServer.HTTPServer.handle_error does, but with # added thread ID and using stderr _logger . error ( "-" * 40 , file = sys . stderr ) _logger . error ( "<{}> Exception happened during processing of request from {}" . format ( threading . currentThread ( ) . ident , client_address ) ) _logger . error ( client_address , file = sys . stderr ) traceback . print_exc ( ) _logger . error ( "-" * 40 , file = sys . stderr ) _logger . error ( request , file = sys . stderr )
564	def toDict ( self ) : def items2dict ( items ) : """Convert a dict of node spec items to a plain dict Each node spec item object will be converted to a dict of its attributes. The entire items dict will become a dict of dicts (same keys). """ d = { } for k , v in items . items ( ) : d [ k ] = v . __dict__ return d self . invariant ( ) return dict ( description = self . description , singleNodeOnly = self . singleNodeOnly , inputs = items2dict ( self . inputs ) , outputs = items2dict ( self . outputs ) , parameters = items2dict ( self . parameters ) , commands = items2dict ( self . commands ) )
4855	def _update_transmissions ( self , content_metadata_item_map , transmission_map ) : for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmission = transmission_map [ content_id ] transmission . channel_metadata = channel_metadata transmission . save ( )
4931	def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
6680	def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
10158	def get_viewset_transition_action_mixin ( model , * * kwargs ) : instance = model ( ) class Mixin ( object ) : save_after_transition = True transitions = instance . get_all_status_transitions ( ) transition_names = set ( x . name for x in transitions ) for transition_name in transition_names : setattr ( Mixin , transition_name , get_transition_viewset_method ( transition_name , * * kwargs ) ) return Mixin
1568	def invoke_hook_bolt_ack ( self , heron_tuple , process_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_ack_info = BoltAckInfo ( heron_tuple = heron_tuple , acking_task_id = self . get_task_id ( ) , process_latency_ms = process_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_ack ( bolt_ack_info )
9043	def gradient ( self ) : L = self . L self . _grad_Lu [ : ] = 0 for i in range ( len ( self . _tril1 [ 0 ] ) ) : row = self . _tril1 [ 0 ] [ i ] col = self . _tril1 [ 1 ] [ i ] self . _grad_Lu [ row , : , i ] = L [ : , col ] self . _grad_Lu [ : , row , i ] += L [ : , col ] m = len ( self . _tril1 [ 0 ] ) for i in range ( len ( self . _diag [ 0 ] ) ) : row = self . _diag [ 0 ] [ i ] col = self . _diag [ 1 ] [ i ] self . _grad_Lu [ row , : , m + i ] = L [ row , col ] * L [ : , col ] self . _grad_Lu [ : , row , m + i ] += L [ row , col ] * L [ : , col ] return { "Lu" : self . _grad_Lu }
155	def prev_key ( self , key , default = _sentinel ) : item = self . prev_item ( key , default ) return default if item is default else item [ 0 ]
13689	def add_peer ( self , peer ) : if type ( peer ) == list : for i in peer : check_url ( i ) self . PEERS . extend ( peer ) elif type ( peer ) == str : check_url ( peer ) self . PEERS . append ( peer )
2001	def visit_BitVecOr ( self , expression , * operands ) : left = expression . operands [ 0 ] right = expression . operands [ 1 ] if isinstance ( right , BitVecConstant ) : if right . value == 0 : return left elif right . value == left . mask : return right elif isinstance ( left , BitVecOr ) : left_left = left . operands [ 0 ] left_right = left . operands [ 1 ] if isinstance ( right , Constant ) : return BitVecOr ( left_left , ( left_right | right ) , taint = expression . taint ) elif isinstance ( left , BitVecConstant ) : return BitVecOr ( right , left , taint = expression . taint )
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
5821	def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { # Parent params 'service_id' : self . attrs [ 'id' ] , } ver . save ( ) return ver
11098	def select_by_size ( self , min_size = 0 , max_size = 1 << 40 , recursive = True ) : def filters ( p ) : return min_size <= p . size <= max_size return self . select_file ( filters , recursive )
12599	def get_sheet_list ( xl_path : str ) -> List : wb = read_xl ( xl_path ) if hasattr ( wb , 'sheetnames' ) : return wb . sheetnames else : return wb . sheet_names ( )
1823	def SETS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF , 1 , 0 ) )
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
4137	def save_figures ( image_path , fig_count , gallery_conf ) : figure_list = [ ] fig_managers = matplotlib . _pylab_helpers . Gcf . get_all_fig_managers ( ) for fig_mngr in fig_managers : # Set the fig_num figure as the current figure as we can't # save a figure that's not the current figure. fig = plt . figure ( fig_mngr . num ) kwargs = { } to_rgba = matplotlib . colors . colorConverter . to_rgba for attr in [ 'facecolor' , 'edgecolor' ] : fig_attr = getattr ( fig , 'get_' + attr ) ( ) default_attr = matplotlib . rcParams [ 'figure.' + attr ] if to_rgba ( fig_attr ) != to_rgba ( default_attr ) : kwargs [ attr ] = fig_attr current_fig = image_path . format ( fig_count + fig_mngr . num ) fig . savefig ( current_fig , * * kwargs ) figure_list . append ( current_fig ) if gallery_conf . get ( 'find_mayavi_figures' , False ) : from mayavi import mlab e = mlab . get_engine ( ) last_matplotlib_fig_num = len ( figure_list ) total_fig_num = last_matplotlib_fig_num + len ( e . scenes ) mayavi_fig_nums = range ( last_matplotlib_fig_num , total_fig_num ) for scene , mayavi_fig_num in zip ( e . scenes , mayavi_fig_nums ) : current_fig = image_path . format ( mayavi_fig_num ) mlab . savefig ( current_fig , figure = scene ) # make sure the image is not too large scale_image ( current_fig , current_fig , 850 , 999 ) figure_list . append ( current_fig ) mlab . close ( all = True ) return figure_list
13414	def removeLayout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . takeAt ( cnt ) widget = item . widget ( ) if widget is not None : widget . deleteLater ( ) else : '''If sublayout encountered, iterate recursively.''' self . removeLayout ( item . layout ( ) )
10920	def do_levmarq_particles ( s , particles , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , max_iter = 2 , * * kwargs ) : lp = LMParticles ( s , particles , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , max_iter = max_iter , * * kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . get_termination_stats ( )
956	def getArgumentDescriptions ( f ) : # Get the argument names and default values argspec = inspect . getargspec ( f ) # Scan through the docstring to extract documentation for each argument as # follows: # Check the first word of the line, stripping a colon if one is present. # If it matches an argument name: # Take the rest of the line, stripping leading whitespeace # Take each subsequent line if its indentation level is greater than the # initial indentation level # Once the indentation level is back to the original level, look for # another argument docstring = f . __doc__ descriptions = { } if docstring : lines = docstring . split ( '\n' ) i = 0 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : i += 1 continue # Indentation level is index of the first character indentLevel = lines [ i ] . index ( stripped [ 0 ] ) # Get the first word and remove the colon, if present firstWord = stripped . split ( ) [ 0 ] if firstWord . endswith ( ':' ) : firstWord = firstWord [ : - 1 ] if firstWord in argspec . args : # Found an argument argName = firstWord restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) argLines = [ restOfLine ] # Take the next lines as long as they are indented more i += 1 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : # Empty line - stop break if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : # No longer indented far enough - stop break # This line counts too argLines . append ( lines [ i ] . strip ( ) ) i += 1 # Store this description descriptions [ argName ] = ' ' . join ( argLines ) else : # Not an argument i += 1 # Build the list of (argName, description, defaultValue) args = [ ] if argspec . defaults : defaultCount = len ( argspec . defaults ) else : defaultCount = 0 nonDefaultArgCount = len ( argspec . args ) - defaultCount for i , argName in enumerate ( argspec . args ) : if i >= nonDefaultArgCount : defaultValue = argspec . defaults [ i - nonDefaultArgCount ] args . append ( ( argName , descriptions . get ( argName , "" ) , defaultValue ) ) else : args . append ( ( argName , descriptions . get ( argName , "" ) ) ) return args
12009	def _update_dict ( data , default_data , replace_data = False ) : if not data : data = default_data . copy ( ) return data if not isinstance ( data , dict ) : raise TypeError ( 'Value not dict type' ) if len ( data ) > 255 : raise ValueError ( 'More than 255 values defined' ) for i in data . keys ( ) : if not isinstance ( i , int ) : raise TypeError ( 'Index not int type' ) if i < 0 or i > 255 : raise ValueError ( 'Index value out of range' ) if not replace_data : data . update ( default_data ) return data
4832	def course_discovery_api_client ( user , catalog_url ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( _ ( "To get a Catalog API client, this package must be " "installed in an Open edX environment." ) ) jwt = JwtBuilder . create_jwt_for_user ( user ) return EdxRestApiClient ( catalog_url , jwt = jwt )
12597	def _check_xl_path ( xl_path : str ) : xl_path = op . abspath ( op . expanduser ( xl_path ) ) if not op . isfile ( xl_path ) : raise IOError ( "Could not find file in {}." . format ( xl_path ) ) return xl_path , _use_openpyxl_or_xlrf ( xl_path )
2382	def from_resolver ( cls , spec_resolver ) : spec_validators = cls . _get_spec_validators ( spec_resolver ) return validators . extend ( Draft4Validator , spec_validators )
158	def InColorspace ( to_colorspace , from_colorspace = "RGB" , children = None , name = None , deterministic = False , random_state = None ) : return WithColorspace ( to_colorspace , from_colorspace , children , name , deterministic , random_state )
6754	def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
226	def get_top_long_short_abs ( positions , top = 10 ) : positions = positions . drop ( 'cash' , axis = 'columns' ) df_max = positions . max ( ) df_min = positions . min ( ) df_abs_max = positions . abs ( ) . max ( ) df_top_long = df_max [ df_max > 0 ] . nlargest ( top ) df_top_short = df_min [ df_min < 0 ] . nsmallest ( top ) df_top_abs = df_abs_max . nlargest ( top ) return df_top_long , df_top_short , df_top_abs
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
9835	def __general ( self ) : while 1 : # main loop try : tok = self . __peek ( ) # only peek, apply_parser() will consume except DXParserNoTokens : # save previous DXInitObject # (kludge in here as the last level-2 parser usually does not return # via the object parser) if self . currentobject and self . currentobject not in self . objects : self . objects . append ( self . currentobject ) return # stop parsing and finish # decision branches for all level-1 parsers: # (the only way to get out of the lower level parsers!) if tok . iscode ( 'COMMENT' ) : self . set_parser ( 'comment' ) # switch the state elif tok . iscode ( 'WORD' ) and tok . equals ( 'object' ) : self . set_parser ( 'object' ) # switch the state elif self . __parser is self . __general : # Either a level-2 parser screwed up or some level-1 # construct is not implemented. (Note: this elif can # be only reached at the beginning or after comments; # later we never formally switch back to __general # (would create inifinite loop) raise DXParseError ( 'Unknown level-1 construct at ' + str ( tok ) ) self . apply_parser ( )
8156	def create_index ( self , table , field , unique = False , ascending = True ) : if unique : u = "unique " else : u = "" if ascending : a = "asc" else : a = "desc" sql = "create " + u + "index index_" + table + "_" + field + " " sql += "on " + table + "(" + field + " " + a + ")" self . _cur . execute ( sql ) self . _con . commit ( )
961	def initLogger ( obj ) : if inspect . isclass ( obj ) : myClass = obj else : myClass = obj . __class__ logger = logging . getLogger ( "." . join ( [ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) return logger
13456	def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
11429	def record_strip_controlfields ( rec ) : for tag in rec . keys ( ) : if tag [ : 2 ] == '00' and rec [ tag ] [ 0 ] [ 3 ] : del rec [ tag ]
13458	def download_s3 ( bucket_name , file_key , file_path , force = False ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) file_dir = file_path . dirname ( ) file_dir . makedirs ( ) s3_key = bucket . get_key ( file_key ) if file_path . exists ( ) : file_data = file_path . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) # Check the hash. try : s3_md5 = s3_key . etag . replace ( '"' , '' ) except KeyError : pass else : if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) return elif not force : # Check if file on S3 is older than local file. s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( file_path . stat ( ) . st_mtime ) if s3_datetime < local_datetime : info ( "File at %s is less recent than the local version." % ( file_key ) ) return # If it is newer, let's process and upload info ( "Downloading %s..." % ( file_key ) ) try : with open ( file_path , 'w' ) as fo : s3_key . get_contents_to_file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
2009	def concretized_args ( * * policies ) : def concretizer ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : spec = inspect . getfullargspec ( func ) for arg , policy in policies . items ( ) : assert arg in spec . args , "Concretizer argument not found in wrapped function." # index is 0-indexed, but ConcretizeArgument is 1-indexed. However, this is correct # since implementation method is always a bound method (self is param 0) index = spec . args . index ( arg ) if not issymbolic ( args [ index ] ) : continue if not policy : policy = 'SAMPLED' if policy == "ACCOUNTS" : value = args [ index ] world = args [ 0 ] . world #special handler for EVM only policy cond = world . _constraint_to_accounts ( value , ty = 'both' , include_zero = True ) world . constraints . add ( cond ) policy = 'ALL' raise ConcretizeArgument ( index , policy = policy ) return func ( * args , * * kwargs ) wrapper . __signature__ = inspect . signature ( func ) return wrapper return concretizer
1270	def _fire ( self , layers , the_plot ) : # We don't fire if another Marauder fired a bolt just now. if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return the_plot [ 'last_marauder_shot' ] = the_plot . frame # Which Marauder should fire the laser bolt? col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 # Move ourselves just below that Marauder. self . _teleport ( ( row , col ) )
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
6906	def total_proper_motion ( pmra , pmdecl , decl ) : pm = np . sqrt ( pmdecl * pmdecl + pmra * pmra * np . cos ( np . radians ( decl ) ) * np . cos ( np . radians ( decl ) ) ) return pm
8606	def list_group_users ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/users?depth=%s' % ( group_id , str ( depth ) ) ) return response
11860	def all_events ( vars , bn , e ) : if not vars : yield e else : X , rest = vars [ 0 ] , vars [ 1 : ] for e1 in all_events ( rest , bn , e ) : for x in bn . variable_values ( X ) : yield extend ( e1 , X , x )
7008	def _fourier_chisq ( fourierparams , phase , mags , errs ) : f = _fourier_func ( fourierparams , phase , mags ) chisq = npsum ( ( ( mags - f ) * ( mags - f ) ) / ( errs * errs ) ) return chisq
10027	def delete_unused_versions ( self , versions_to_keep = 10 ) : # get versions in use environments = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) environments = environments [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] versions_in_use = [ ] for env in environments : versions_in_use . append ( env [ 'VersionLabel' ] ) # get all versions versions = self . ebs . describe_application_versions ( application_name = self . app_name ) versions = versions [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ] versions = sorted ( versions , reverse = True , key = functools . cmp_to_key ( lambda x , y : ( x [ 'DateCreated' ] > y [ 'DateCreated' ] ) - ( x [ 'DateCreated' ] < y [ 'DateCreated' ] ) ) ) # delete versions in use for version in versions [ versions_to_keep : ] : if version [ 'VersionLabel' ] in versions_in_use : out ( "Not deleting " + version [ "VersionLabel" ] + " because it is in use" ) else : out ( "Deleting unused version: " + version [ "VersionLabel" ] ) self . ebs . delete_application_version ( application_name = self . app_name , version_label = version [ 'VersionLabel' ] ) sleep ( 2 )
782	def jobReactivateRunningJobs ( self ) : # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' ' _eng_allocate_new_workers=TRUE ' ' WHERE status=%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) return
1879	def VEXTRACTF128 ( cpu , dest , src , offset ) : offset = offset . read ( ) dest . write ( Operators . EXTRACT ( src . read ( ) , offset * 128 , ( offset + 1 ) * 128 ) )
12860	def add_period ( self , p , holiday_obj = None ) : if isinstance ( p , ( list , tuple ) ) : return [ BusinessDate . add_period ( self , pd ) for pd in p ] elif isinstance ( p , str ) : period = BusinessPeriod ( p ) else : period = p res = self res = BusinessDate . add_months ( res , period . months ) res = BusinessDate . add_years ( res , period . years ) res = BusinessDate . add_days ( res , period . days ) if period . businessdays : if holiday_obj : res = BusinessDate . add_business_days ( res , period . businessdays , holiday_obj ) else : res = BusinessDate . add_business_days ( res , period . businessdays , period . holiday ) return res
9564	def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) # basic header and record length checks validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) # some simple value checks validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) # a more complicated record check def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
6652	def findProgram ( self , builddir , program ) : # if this is an exact match, do no further checking: if os . path . isfile ( os . path . join ( builddir , program ) ) : logging . info ( 'found %s' % program ) return program exact_matches = [ ] insensitive_matches = [ ] approx_matches = [ ] for path , dirs , files in os . walk ( builddir ) : if program in files : exact_matches . append ( os . path . relpath ( os . path . join ( path , program ) , builddir ) ) continue files_lower = [ f . lower ( ) for f in files ] if program . lower ( ) in files_lower : insensitive_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( program . lower ( ) ) ] ) , builddir ) ) continue # !!! TODO: in the future add approximate string matching (typos, # etc.), for now we just test stripping any paths off program, and # looking for substring matches: pg_basen_lower_noext = os . path . splitext ( os . path . basename ( program ) . lower ( ) ) [ 0 ] for f in files_lower : if pg_basen_lower_noext in f : approx_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( f ) ] ) , builddir ) ) if len ( exact_matches ) == 1 : logging . info ( 'found %s at %s' , program , exact_matches [ 0 ] ) return exact_matches [ 0 ] elif len ( exact_matches ) > 1 : logging . error ( '%s matches multiple executables, please use a full path (one of %s)' % ( program , ', or ' . join ( [ '"' + os . path . join ( m , program ) + '"' for m in exact_matches ] ) ) ) return None # if we have matches with and without a file extension, prefer the # no-file extension version, and discard the others (so we avoid # picking up post-processed files): reduced_approx_matches = [ ] for m in approx_matches : root = os . path . splitext ( m ) [ 0 ] if ( m == root ) or ( root not in approx_matches ) : reduced_approx_matches . append ( m ) approx_matches = reduced_approx_matches for matches in ( insensitive_matches , approx_matches ) : if len ( matches ) == 1 : logging . info ( 'found %s at %s' % ( program , matches [ 0 ] ) ) return matches [ 0 ] elif len ( matches ) > 1 : logging . error ( '%s is similar to several executables found. Please use an exact name:\n%s' % ( program , '\n' . join ( matches ) ) ) return None logging . error ( 'could not find program "%s" to debug' % program ) return None
7689	def piano_roll ( annotation , sr = 22050 , length = None , * * kwargs ) : intervals , pitches = annotation . to_interval_values ( ) # Construct the pitchogram pitch_map = { f : idx for idx , f in enumerate ( np . unique ( pitches ) ) } gram = np . zeros ( ( len ( pitch_map ) , len ( intervals ) ) ) for col , f in enumerate ( pitches ) : gram [ pitch_map [ f ] , col ] = 1 return filter_kwargs ( mir_eval . sonify . time_frequency , gram , pitches , intervals , sr , length = length , * * kwargs )
7525	def draw ( self , show_tip_labels = True , show_node_support = False , use_edge_lengths = False , orient = "right" , print_args = False , * args , * * kwargs ) : ## re-decompose tree for new orient and edges args self . _decompose_tree ( orient = orient , use_edge_lengths = use_edge_lengths ) ## update kwargs with entered args and all other kwargs dwargs = { } dwargs [ "show_tip_labels" ] = show_tip_labels dwargs [ "show_node_support" ] = show_node_support dwargs . update ( kwargs ) ## pass to panel plotter canvas , axes , panel = tree_panel_plot ( self , print_args , * * dwargs ) return canvas , axes , panel
9643	def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] #catch the case where no client is listening try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
10481	def _generateChildren ( self ) : try : children = self . AXChildren except _a11y . Error : return if children : for child in children : yield child
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
2097	def cancel ( self , pk = None , fail_if_not_running = False , * * kwargs ) : # Search for the record if pk not given if not pk : existing_data = self . get ( * * kwargs ) pk = existing_data [ 'id' ] cancel_endpoint = '%s%s/cancel/' % ( self . endpoint , pk ) # Attempt to cancel the job. try : client . post ( cancel_endpoint ) changed = True except exc . MethodNotAllowed : changed = False if fail_if_not_running : raise exc . TowerCLIError ( 'Job not running.' ) # Return a success. return { 'status' : 'canceled' , 'changed' : changed }
10279	def neurommsig_topology ( graph : BELGraph , nodes : List [ BaseEntity ] ) -> float : nodes = list ( nodes ) number_nodes = len ( nodes ) if number_nodes <= 1 : # log.debug('') return 0.0 unnormalized_sum = sum ( u in graph [ v ] for u , v in itt . product ( nodes , repeat = 2 ) if v in graph and u != v ) return unnormalized_sum / ( number_nodes * ( number_nodes - 1.0 ) )
8895	def _with_error_handling ( resp , error , mode , response_format ) : def safe_parse ( r ) : try : return APIWrapper . _parse_resp ( r , response_format ) except ( ValueError , SyntaxError ) as ex : log . error ( ex ) r . parsed = None return r if isinstance ( error , requests . HTTPError ) : if resp . status_code == 400 : # It means that request parameters were rejected by the server, # so we need to enrich standard error message # with 'ValidationErrors' # from the response resp = safe_parse ( resp ) if resp . parsed is not None : parsed_resp = resp . parsed messages = [ ] if response_format == 'xml' and parsed_resp . find ( './ValidationErrors' ) is not None : messages = [ e . find ( './Message' ) . text for e in parsed_resp . findall ( './ValidationErrors/ValidationErrorDto' ) ] elif response_format == 'json' and 'ValidationErrors' in parsed_resp : messages = [ e [ 'Message' ] for e in parsed_resp [ 'ValidationErrors' ] ] error = requests . HTTPError ( '%s: %s' % ( error , '\n\t' . join ( messages ) ) , response = resp ) elif resp . status_code == 429 : error = requests . HTTPError ( '%sToo many requests in the last minute.' % error , response = resp ) if STRICT == mode : raise error elif GRACEFUL == mode : if isinstance ( error , EmptyResponse ) : # Empty response is returned by the API occasionally, # in this case it makes sense to ignore it and retry. log . warning ( error ) resp . parsed = None return resp elif isinstance ( error , requests . HTTPError ) : # Ignoring 'Too many requests' error, # since subsequent retries will come after a delay. if resp . status_code == 429 : # Too many requests log . warning ( error ) return safe_parse ( resp ) else : raise error else : raise error else : # ignore everything, just log it and return whatever response we # have log . error ( error ) return safe_parse ( resp )
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : # Check if siblings exist; don't process the paragraph in that case. if element . next is not None : return element elif element . prev is not None : return element # Remove the Para wrapper from the lone paragraph. # `Plain` is a container that isn't rendered as a paragraph. return Plain ( * element . content )
7143	def new_address ( self , label = None ) : return self . _backend . new_address ( account = self . index , label = label )
1234	def atomic_observe ( self , states , actions , internals , reward , terminal ) : # TODO probably unnecessary here. self . current_terminal = terminal self . current_reward = reward # print('action = {}'.format(actions)) if self . unique_state : states = dict ( state = states ) if self . unique_action : actions = dict ( action = actions ) self . episode = self . model . atomic_observe ( states = states , actions = actions , internals = internals , terminal = self . current_terminal , reward = self . current_reward )
1964	def sys_chroot ( self , path ) : if path not in self . current . memory : return - errno . EFAULT path_s = self . current . read_string ( path ) if not os . path . exists ( path_s ) : return - errno . ENOENT if not os . path . isdir ( path_s ) : return - errno . ENOTDIR return - errno . EPERM
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
4348	def trim ( self , start_time , end_time = None ) : if not is_number ( start_time ) or start_time < 0 : raise ValueError ( "start_time must be a positive number." ) effect_args = [ 'trim' , '{:f}' . format ( start_time ) ] if end_time is not None : if not is_number ( end_time ) or end_time < 0 : raise ValueError ( "end_time must be a positive number." ) if start_time >= end_time : raise ValueError ( "start_time must be smaller than end_time." ) effect_args . append ( '{:f}' . format ( end_time - start_time ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'trim' ) return self
4157	def arma_estimate ( X , P , Q , lag ) : R = CORRELATION ( X , maxlags = lag , norm = 'unbiased' ) R0 = R [ 0 ] #C Estimate the AR parameters (no error weighting is used). #C Number of equation errors is M-Q . MPQ = lag - Q + P N = len ( X ) Y = np . zeros ( N - P , dtype = complex ) for K in range ( 0 , MPQ ) : KPQ = K + Q - P + 1 if KPQ < 0 : Y [ K ] = R [ - KPQ ] . conjugate ( ) if KPQ == 0 : Y [ K ] = R0 if KPQ > 0 : Y [ K ] = R [ KPQ ] # The resize is very important for the normalissation. Y . resize ( lag ) if P <= 4 : res = arcovar_marple ( Y . copy ( ) , P ) #! Eq. (10.12) ar_params = res [ 0 ] else : res = arcovar ( Y . copy ( ) , P ) #! Eq. (10.12) ar_params = res [ 0 ] # the .copy is used to prevent a reference somewhere. this is a bug # to be tracked down. Y . resize ( N - P ) #C Filter the original time series for k in range ( P , N ) : SUM = X [ k ] #SUM += sum([ar_params[j]*X[k-j-1] for j in range(0,P)]) for j in range ( 0 , P ) : SUM = SUM + ar_params [ j ] * X [ k - j - 1 ] #! Eq. (10.17) Y [ k - P ] = SUM # Estimate the MA parameters (a "long" AR of order at least 2*IQ #C is suggested) #Y.resize(N-P) ma_params , rho = ma ( Y , Q , 2 * Q ) #! Eq. (10.3) return ar_params , ma_params , rho
4144	def CHOLESKY ( A , B , method = 'scipy' ) : if method == 'numpy_solver' : X = _numpy_solver ( A , B ) return X elif method == 'numpy' : X , _L = _numpy_cholesky ( A , B ) return X elif method == 'scipy' : import scipy . linalg L = scipy . linalg . cholesky ( A ) X = scipy . linalg . cho_solve ( ( L , False ) , B ) else : raise ValueError ( 'method must be numpy_solver, numpy_cholesky or cholesky_inplace' ) return X
61	def iou ( self , other ) : inters = self . intersection ( other ) if inters is None : return 0.0 else : area_union = self . area + other . area - inters . area return inters . area / area_union if area_union > 0 else 0.0
10170	def set_scheduled ( self ) : with self . _idle_lock : if self . _idle : self . _idle = False return True return False
8063	def do_set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . _vars : self . print_response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . _vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var_changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
2014	def _top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise StackUnderflow ( ) return self . stack [ n - 1 ]
3587	def remove ( self , cbobject ) : with self . _lock : if cbobject in self . _metadata : del self . _metadata [ cbobject ]
11955	def is_dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except ValueError : return False if val > 255 or val < 0 : return False return True
8119	def transform_path ( self , path ) : p = path . __class__ ( ) # Create a new BezierPath. for pt in path : if pt . cmd == "close" : p . closepath ( ) elif pt . cmd == "moveto" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "lineto" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "curveto" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
9380	def get_standardized_timestamp ( timestamp , ts_format ) : if not timestamp : return None if timestamp == 'now' : timestamp = str ( datetime . datetime . now ( ) ) if not ts_format : ts_format = detect_timestamp_format ( timestamp ) try : if ts_format == 'unknown' : logger . error ( 'Unable to determine timestamp format for : %s' , timestamp ) return - 1 elif ts_format == 'epoch' : ts = int ( timestamp ) * 1000 elif ts_format == 'epoch_ms' : ts = timestamp elif ts_format == 'epoch_fraction' : ts = int ( timestamp [ : 10 ] ) * 1000 + int ( timestamp [ 11 : ] ) elif ts_format in ( '%H:%M:%S' , '%H:%M:%S.%f' ) : date_today = str ( datetime . date . today ( ) ) dt_obj = datetime . datetime . strptime ( date_today + ' ' + timestamp , '%Y-%m-%d ' + ts_format ) ts = calendar . timegm ( dt_obj . utctimetuple ( ) ) * 1000 + dt_obj . microsecond / 1000 else : dt_obj = datetime . datetime . strptime ( timestamp , ts_format ) ts = calendar . timegm ( dt_obj . utctimetuple ( ) ) * 1000 + dt_obj . microsecond / 1000 except ValueError : return - 1 return str ( ts )
13585	def add_link ( cls , attr , title = '' , display = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = attr . capitalize ( ) # python scoping is a bit weird with default values, if it isn't # referenced the inner function won't see it, so assign it for use _display = display def _link ( self , obj ) : field_obj = admin_obj_attr ( obj , attr ) if not field_obj : return '' text = _obj_display ( field_obj , _display ) return admin_obj_link ( field_obj , text ) _link . short_description = title _link . allow_tags = True _link . admin_order_field = attr setattr ( cls , fn_name , _link )
3346	def guess_mime_type ( url ) : ( mimetype , _mimeencoding ) = mimetypes . guess_type ( url ) if not mimetype : ext = os . path . splitext ( url ) [ 1 ] mimetype = _MIME_TYPES . get ( ext ) _logger . debug ( "mimetype({}): {}" . format ( url , mimetype ) ) if not mimetype : mimetype = "application/octet-stream" return mimetype
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
884	def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ]
10680	def Cp_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : c = ( self . _B_mag * ( 2 * tau ** 3 + 2 * tau ** 9 / 3 + 2 * tau ** 15 / 5 ) ) / self . _D_mag else : c = ( 2 * tau ** - 5 + 2 * tau ** - 15 / 3 + 2 * tau ** - 25 / 5 ) / self . _D_mag result = R * math . log ( self . beta0_mag + 1 ) * c return result
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
8722	def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s """\n%s ' % ( prefix , prefix ) ) definition += params . replace ( '\n' , '\n%s ' % prefix ) definition += ( '\n%s """' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
908	def handleInputRecord ( self , inputRecord ) : assert inputRecord , "Invalid inputRecord: %r" % inputRecord results = self . __phaseManager . handleInputRecord ( inputRecord ) metrics = self . __metricsMgr . update ( results ) # Execute task-postIter callbacks for cb in self . __userCallbacks [ 'postIter' ] : cb ( self . __model ) results . metrics = metrics # Return the input and predictions for this record return results
6247	def get_program ( self , label : str ) -> moderngl . Program : return self . _project . get_program ( label )
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
5036	def _handle_singular ( cls , enterprise_customer , manage_learners_form ) : form_field_value = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . EMAIL_OR_USERNAME ] email = email_or_username__to__email ( form_field_value ) try : validate_email_to_link ( email , form_field_value , ValidationMessages . INVALID_EMAIL_OR_USERNAME , True ) except ValidationError as exc : manage_learners_form . add_error ( ManageLearnersForm . Fields . EMAIL_OR_USERNAME , exc ) else : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) return [ email ]
12428	def check_directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . _ve_dir ) : os . makedirs ( self . _ve_dir ) if not os . path . exists ( self . _app_dir ) : os . makedirs ( self . _app_dir ) if not os . path . exists ( self . _conf_dir ) : os . makedirs ( self . _conf_dir ) if not os . path . exists ( self . _var_dir ) : os . makedirs ( self . _var_dir ) if not os . path . exists ( self . _log_dir ) : os . makedirs ( self . _log_dir ) if not os . path . exists ( self . _script_dir ) : os . makedirs ( self . _script_dir ) # copy uswgi_params for nginx uwsgi_params = '/etc/nginx/uwsgi_params' if os . path . exists ( uwsgi_params ) : shutil . copy ( uwsgi_params , self . _conf_dir ) else : logging . warning ( 'Unable to find Nginx uwsgi_params. You must manually copy this to {0}.' . format ( self . _conf_dir ) ) # copy mime.types for nginx mime_types = '/etc/nginx/mime.types' if os . path . exists ( mime_types ) : shutil . copy ( mime_types , self . _conf_dir ) self . _include_mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx. You must manually copy this to {0}.' . format ( self . _conf_dir ) )
3751	def TWA ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] : _TWA = ( _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] : _TWA = ( _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _TWA = None else : raise Exception ( 'Failure in in function' ) return _TWA
4593	def receive ( self , msg ) : if self . edit_queue : self . edit_queue . put_edit ( self . _set , msg ) else : self . _set ( msg )
12143	async def _push ( self , * args , * * kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
10340	def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
1851	def RCL ( cpu , dest , src ) : OperandSize = dest . size count = src . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( src . size + 1 ) , OperandSize ) value = dest . read ( ) if isinstance ( tempCount , int ) and tempCount == 0 : # this is a no-op new_val = value dest . write ( new_val ) else : carry = Operators . ITEBV ( OperandSize , cpu . CF , 1 , 0 ) right = value >> ( OperandSize - tempCount ) new_val = ( value << tempCount ) | ( carry << ( tempCount - 1 ) ) | ( right >> 1 ) dest . write ( new_val ) def sf ( v , size ) : return ( v & ( 1 << ( size - 1 ) ) ) != 0 cpu . CF = sf ( value << ( tempCount - 1 ) , OperandSize ) cpu . OF = Operators . ITE ( tempCount == 1 , sf ( new_val , OperandSize ) != cpu . CF , cpu . OF )
2149	def modify ( self , pk = None , create_on_missing = False , * * kwargs ) : # Create the resource if needed. if pk is None and create_on_missing : try : self . get ( * * copy . deepcopy ( kwargs ) ) except exc . NotFound : return self . create ( * * kwargs ) # Modify everything except notification type and configuration config_item = self . _separate ( kwargs ) notification_type = kwargs . pop ( 'notification_type' , None ) debug . log ( 'Modify everything except notification type and' ' configuration' , header = 'details' ) part_result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , * * kwargs ) # Modify notification type and configuration if notification_type is None or notification_type == part_result [ 'notification_type' ] : for item in part_result [ 'notification_configuration' ] : if item not in config_item or not config_item [ item ] : to_add = part_result [ 'notification_configuration' ] [ item ] if not ( to_add == '$encrypted$' and item in Resource . encrypted_fields ) : config_item [ item ] = to_add if notification_type is None : kwargs [ 'notification_type' ] = part_result [ 'notification_type' ] else : kwargs [ 'notification_type' ] = notification_type self . _configuration ( kwargs , config_item ) debug . log ( 'Modify notification type and configuration' , header = 'details' ) result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , * * kwargs ) # Update 'changed' field to give general changed info if 'changed' in result and 'changed' in part_result : result [ 'changed' ] = result [ 'changed' ] or part_result [ 'changed' ] return result
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
6681	def move ( self , source , destination , use_sudo = False ) : func = use_sudo and run_as_root or self . run func ( '/bin/mv {0} {1}' . format ( quote ( source ) , quote ( destination ) ) )
2517	def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
46	def project ( self , from_shape , to_shape ) : xy_proj = project_coords ( [ ( self . x , self . y ) ] , from_shape , to_shape ) return self . deepcopy ( x = xy_proj [ 0 ] [ 0 ] , y = xy_proj [ 0 ] [ 1 ] )
9142	def _sortkey ( self , key = 'uri' , language = 'any' ) : if key == 'uri' : return self . uri else : l = label ( self . labels , language , key == 'sortlabel' ) return l . label . lower ( ) if l else ''
4322	def channels ( self , n_channels ) : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( 'n_channels must be a positive integer.' ) effect_args = [ 'channels' , '{}' . format ( n_channels ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'channels' ) return self
7328	def _get_base_url ( base_url , api , version ) : format_args = { } if "{api}" in base_url : if api == "" : base_url = base_url . replace ( '{api}.' , '' ) else : format_args [ 'api' ] = api if "{version}" in base_url : if version == "" : base_url = base_url . replace ( '/{version}' , '' ) else : format_args [ 'version' ] = version return base_url . format ( api = api , version = version )
12391	def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
2922	def _clear_celery_task_data ( self , my_task ) : # Save history if 'task_id' in my_task . internal_data : # Save history for diagnostics/forensics history = my_task . _get_internal_data ( 'task_history' , [ ] ) history . append ( my_task . _get_internal_data ( 'task_id' ) ) del my_task . internal_data [ 'task_id' ] my_task . _set_internal_data ( task_history = history ) if 'task_state' in my_task . internal_data : del my_task . internal_data [ 'task_state' ] if 'error' in my_task . internal_data : del my_task . internal_data [ 'error' ] if hasattr ( my_task , 'async_call' ) : delattr ( my_task , 'async_call' ) if hasattr ( my_task , 'deserialized' ) : delattr ( my_task , 'deserialized' )
1816	def SETNO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , 1 , 0 ) )
1483	def launch ( self ) : with self . process_lock : current_commands = dict ( map ( ( lambda process : ( process . name , process . command ) ) , self . processes_to_monitor . values ( ) ) ) updated_commands = self . get_commands_to_run ( ) # get the commands to kill, keep and start commands_to_kill , commands_to_keep , commands_to_start = self . get_command_changes ( current_commands , updated_commands ) Log . info ( "current commands: %s" % sorted ( current_commands . keys ( ) ) ) Log . info ( "new commands : %s" % sorted ( updated_commands . keys ( ) ) ) Log . info ( "commands_to_kill: %s" % sorted ( commands_to_kill . keys ( ) ) ) Log . info ( "commands_to_keep: %s" % sorted ( commands_to_keep . keys ( ) ) ) Log . info ( "commands_to_start: %s" % sorted ( commands_to_start . keys ( ) ) ) self . _kill_processes ( commands_to_kill ) self . _start_processes ( commands_to_start ) Log . info ( "Launch complete - processes killed=%s kept=%s started=%s monitored=%s" % ( len ( commands_to_kill ) , len ( commands_to_keep ) , len ( commands_to_start ) , len ( self . processes_to_monitor ) ) )
6512	def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
5376	def outputs_are_present ( outputs ) : # outputs are OutputFileParam (see param_util.py) # If outputs contain a pattern, then there is no way for `dsub` to verify # that *all* output is present. The best that `dsub` can do is to verify # that *some* output was created for each such parameter. for o in outputs : if not o . value : continue if o . recursive : if not folder_exists ( o . value ) : return False else : if not simple_pattern_exists_in_gcs ( o . value ) : return False return True
9615	def elements ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENTS , { 'using' : using , 'value' : value } )
13726	def set_connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
7411	def sample_loci ( self ) : ## store idx of passing loci idxs = np . random . choice ( self . idxs , self . ntests ) ## open handle, make a proper generator to reduce mem with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) ## store data as dict seqdata = { i : "" for i in self . samples } ## put chunks into a list for idx , loc in enumerate ( liter ) : if idx in idxs : ## parse chunk lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } ## add data to concatenated seqdict for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) ## concatenate into a phylip file return seqdata
5490	def discover ( cls ) : file = os . path . join ( Config . config_dir , Config . config_name ) return cls . from_file ( file )
6770	def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , * * kwargs )
11116	def create_package ( self , path = None , name = None , mode = None ) : # check mode assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : mode = 'w:bz2' mode = 'w:' # get root if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path # get name if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext # save repository self . save ( ) # create tar file tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) # walk directory and create empty directories for directory in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : t = tarfile . TarInfo ( directory ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) # walk files and add to tar for file in self . walk_files_relative_path ( ) : tarHandler . add ( os . path . join ( self . __path , file ) , arcname = file ) # save repository .pyrepinfo tarHandler . add ( os . path . join ( self . __path , ".pyrepinfo" ) , arcname = ".pyrepinfo" ) # close tar file tarHandler . close ( )
4465	def save ( filename_audio , filename_jam , jam , strict = True , fmt = 'auto' , * * kwargs ) : y = jam . sandbox . muda . _audio [ 'y' ] sr = jam . sandbox . muda . _audio [ 'sr' ] # First, dump the audio file psf . write ( filename_audio , y , sr , * * kwargs ) # Then dump the jam jam . save ( filename_jam , strict = strict , fmt = fmt )
1896	def _is_sat ( self ) -> bool : logger . debug ( "Solver.check() " ) start = time . time ( ) self . _send ( '(check-sat)' ) status = self . _recv ( ) logger . debug ( "Check took %s seconds (%s)" , time . time ( ) - start , status ) if status not in ( 'sat' , 'unsat' , 'unknown' ) : raise SolverError ( status ) if consider_unknown_as_unsat : if status == 'unknown' : logger . info ( 'Found an unknown core, probably a solver timeout' ) status = 'unsat' if status == 'unknown' : raise SolverUnknown ( status ) return status == 'sat'
3014	def _to_json ( self , strip , to_serialize = None ) : if to_serialize is None : to_serialize = copy . copy ( self . __dict__ ) pkcs12_val = to_serialize . get ( _PKCS12_KEY ) if pkcs12_val is not None : to_serialize [ _PKCS12_KEY ] = base64 . b64encode ( pkcs12_val ) return super ( ServiceAccountCredentials , self ) . _to_json ( strip , to_serialize = to_serialize )
2376	def run ( self , args ) : self . args = self . parse_and_process_args ( args ) if self . args . version : print ( __version__ ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . _load_rule_file ( filename ) if self . args . list : self . list_rules ( ) return 0 if self . args . describe : self . _describe_rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , "other" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) continue if os . path . isdir ( filename ) : self . _process_folder ( filename ) else : self . _process_file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0
7076	def periodrec_worker ( task ) : pfpkl , simbasedir , period_tolerance = task try : return periodicvar_recovery ( pfpkl , simbasedir , period_tolerance = period_tolerance ) except Exception as e : LOGEXCEPTION ( 'periodic var recovery failed for %s' % repr ( task ) ) return None
10584	def remove_account ( self , name ) : acc_to_remove = None for a in self . accounts : if a . name == name : acc_to_remove = a if acc_to_remove is not None : self . accounts . remove ( acc_to_remove )
6372	def accuracy ( self ) : if self . population ( ) == 0 : return float ( 'NaN' ) return ( self . _tp + self . _tn ) / self . population ( )
12819	def _file_size ( self , field ) : size = 0 try : handle = open ( self . _files [ field ] , "r" ) size = os . fstat ( handle . fileno ( ) ) . st_size handle . close ( ) except : size = 0 self . _file_lengths [ field ] = size return self . _file_lengths [ field ]
4557	def contains ( x ) : if isinstance ( x , str ) : x = canonical_name ( x ) return x in _TO_COLOR_USER or x in _TO_COLOR else : x = tuple ( x ) return x in _TO_NAME_USER or x in _TO_NAME
6224	def _gl_look_at ( self , pos , target , up ) : z = vector . normalise ( pos - target ) x = vector . normalise ( vector3 . cross ( vector . normalise ( up ) , z ) ) y = vector3 . cross ( z , x ) translate = matrix44 . create_identity ( ) translate [ 3 ] [ 0 ] = - pos . x translate [ 3 ] [ 1 ] = - pos . y translate [ 3 ] [ 2 ] = - pos . z rotate = matrix44 . create_identity ( ) rotate [ 0 ] [ 0 ] = x [ 0 ] # -- X rotate [ 1 ] [ 0 ] = x [ 1 ] rotate [ 2 ] [ 0 ] = x [ 2 ] rotate [ 0 ] [ 1 ] = y [ 0 ] # -- Y rotate [ 1 ] [ 1 ] = y [ 1 ] rotate [ 2 ] [ 1 ] = y [ 2 ] rotate [ 0 ] [ 2 ] = z [ 0 ] # -- Z rotate [ 1 ] [ 2 ] = z [ 1 ] rotate [ 2 ] [ 2 ] = z [ 2 ] return matrix44 . multiply ( translate , rotate )
9092	def _get_old_entry_identifiers ( namespace : Namespace ) -> Set [ NamespaceEntry ] : return { term . identifier for term in namespace . entries }
3136	def get ( self , * * queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( ) , * * queryparams )
10392	def workflow_all ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , ) -> Mapping [ BaseEntity , List [ Runner ] ] : results = { } for node in get_nodes_by_function ( graph , BIOPROCESS ) : results [ node ] = workflow ( graph , node , key = key , tag = tag , default_score = default_score , runs = runs ) return results
4412	def fetch ( self , key : object , default = None ) : return self . _user_data . get ( key , default )
4956	def get_object ( self , name , description ) : return Activity ( id = X_API_ACTIVITY_COURSE , definition = ActivityDefinition ( name = LanguageMap ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = LanguageMap ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
1900	def get_all_values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , ConstraintSet ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp_cs . new_array ( index_max = expression . index_max , value_bits = expression . value_bits , taint = expression . taint ) . array else : raise NotImplementedError ( f"get_all_values only implemented for {type(expression)} expression type." ) temp_cs . add ( var == expression ) self . _reset ( temp_cs . to_string ( related_to = var ) ) result = [ ] while self . _is_sat ( ) : value = self . _getvalue ( var ) result . append ( value ) self . _assert ( var != value ) if len ( result ) >= maxcnt : if silent : # do not throw an exception if set to silent # Default is not silent, assume user knows # what they are doing and will check the size # of returned vals list (previous smtlib behavior) break else : raise TooManySolutions ( result ) return result
11806	def viterbi_segment ( text , P ) : # best[i] = best probability for text[0:i] # words[i] = best word ending at position i n = len ( text ) words = [ '' ] + list ( text ) best = [ 1.0 ] + [ 0.0 ] * n ## Fill in the vectors best, words via dynamic programming for i in range ( n + 1 ) : for j in range ( 0 , i ) : w = text [ j : i ] if P [ w ] * best [ i - len ( w ) ] >= best [ i ] : best [ i ] = P [ w ] * best [ i - len ( w ) ] words [ i ] = w ## Now recover the sequence of best words sequence = [ ] i = len ( words ) - 1 while i > 0 : sequence [ 0 : 0 ] = [ words [ i ] ] i = i - len ( words [ i ] ) ## Return sequence of best words and overall probability return sequence , best [ - 1 ]
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
7488	def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
3185	def get ( self , store_id , product_id , image_id , * * queryparams ) : self . store_id = store_id self . product_id = product_id self . image_id = image_id return self . _mc_client . _post ( url = self . _build_path ( store_id , 'products' , product_id , 'images' , image_id ) , * * queryparams )
12187	async def handle_message ( self , message , filters ) : data = self . _unpack_message ( message ) logger . debug ( data ) if data . get ( 'type' ) == 'error' : raise SlackApiError ( data . get ( 'error' , { } ) . get ( 'msg' , str ( data ) ) ) elif self . message_is_to_me ( data ) : text = data [ 'text' ] [ len ( self . address_as ) : ] . strip ( ) if text == 'help' : return self . _respond ( channel = data [ 'channel' ] , text = self . _instruction_list ( filters ) , ) elif text == 'version' : return self . _respond ( channel = data [ 'channel' ] , text = self . VERSION , ) for _filter in filters : if _filter . matches ( data ) : logger . debug ( 'Response triggered' ) async for response in _filter : self . _respond ( channel = data [ 'channel' ] , text = response )
12563	def get_rois_centers_of_mass ( vol ) : from scipy . ndimage . measurements import center_of_mass roisvals = np . unique ( vol ) roisvals = roisvals [ roisvals != 0 ] rois_centers = OrderedDict ( ) for r in roisvals : rois_centers [ r ] = center_of_mass ( vol , vol , r ) return rois_centers
13023	def select ( self , sql_string , cols , * args , * * kwargs ) : working_columns = None if kwargs . get ( 'columns' ) is not None : working_columns = kwargs . pop ( 'columns' ) query = self . _assemble_select ( sql_string , cols , * args , * kwargs ) return self . _execute ( query , working_columns = working_columns )
8529	def report ( self ) : self . _output . write ( '\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )
7137	def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . format ( x , options . digits ) , } for _types , fmtr in formatters . items ( ) : if isinstance ( obj , _types ) : return fmtr ( obj ) try : if six . PY2 and isinstance ( obj , six . string_types ) : return str ( obj . encode ( 'utf-8' ) ) return str ( obj ) except : return 'OBJECT'
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
8295	def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
2479	def datetime_iso_format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
9992	def _new_dynspace ( self , name = None , bases = None , formula = None , refs = None , arguments = None , source = None , ) : if name is None : name = self . spacenamer . get_next ( self . namespace ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = RootDynamicSpaceImpl ( parent = self , name = name , formula = formula , refs = refs , source = source , arguments = arguments , ) space . is_derived = False self . _set_space ( space ) if bases : # i.e. not [] dynbase = self . _get_dynamic_base ( bases ) space . _dynbase = dynbase dynbase . _dynamic_subs . append ( space ) return space
7150	def decode ( cls , phrase ) : phrase = phrase . split ( " " ) out = "" for i in range ( len ( phrase ) // 3 ) : word1 , word2 , word3 = phrase [ 3 * i : 3 * i + 3 ] w1 = cls . word_list . index ( word1 ) w2 = cls . word_list . index ( word2 ) % cls . n w3 = cls . word_list . index ( word3 ) % cls . n x = w1 + cls . n * ( ( w2 - w1 ) % cls . n ) + cls . n * cls . n * ( ( w3 - w2 ) % cls . n ) out += endian_swap ( "%08x" % x ) return out
8570	def get_location ( self , location_id , depth = 0 ) : response = self . _perform_request ( '/locations/%s?depth=%s' % ( location_id , depth ) ) return response
5343	def compose_mbox ( projects ) : mbox_archives = '/home/bitergia/mboxes' mailing_lists_projects = [ project for project in projects if 'mailing_lists' in projects [ project ] ] for mailing_lists in mailing_lists_projects : projects [ mailing_lists ] [ 'mbox' ] = [ ] for mailing_list in projects [ mailing_lists ] [ 'mailing_lists' ] : if 'listinfo' in mailing_list : name = mailing_list . split ( 'listinfo/' ) [ 1 ] elif 'mailing-list' in mailing_list : name = mailing_list . split ( 'mailing-list/' ) [ 1 ] else : name = mailing_list . split ( '@' ) [ 0 ] list_new = "%s %s/%s.mbox/%s.mbox" % ( name , mbox_archives , name , name ) projects [ mailing_lists ] [ 'mbox' ] . append ( list_new ) return projects
12974	def compat_convertHashedIndexes ( self , fetchAll = True ) : saver = IndexedRedisSave ( self . mdl ) if fetchAll is True : objs = self . all ( ) saver . compat_convertHashedIndexes ( objs ) else : didWarnOnce = False pks = self . getPrimaryKeys ( ) for pk in pks : obj = self . get ( pk ) if not obj : if didWarnOnce is False : sys . stderr . write ( 'WARNING(once)! An object (type=%s , pk=%d) disappered while ' 'running compat_convertHashedIndexes! This probably means an application ' 'is using the model while converting indexes. This is a very BAD IDEA (tm).' ) didWarnOnce = True continue saver . compat_convertHashedIndexes ( [ obj ] )
8062	def do_help ( self , arg ) : print ( self . response_prompt , file = self . stdout ) return cmd . Cmd . do_help ( self , arg )
9136	def get_modules ( ) -> Mapping : modules = { } for entry_point in iter_entry_points ( group = 'bio2bel' , name = None ) : entry = entry_point . name try : modules [ entry ] = entry_point . load ( ) except VersionConflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except UnknownExtra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except ImportError as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
9193	def _insert_file ( cursor , file , media_type ) : resource_hash = _get_file_sha1 ( file ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( resource_hash , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except ( IndexError , TypeError ) : cursor . execute ( "INSERT INTO files (file, media_type) " "VALUES (%s, %s)" "RETURNING fileid" , ( psycopg2 . Binary ( file . read ( ) ) , media_type , ) ) fileid = cursor . fetchone ( ) [ 0 ] return fileid , resource_hash
7957	def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( " want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( " want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : # pylint: disable=E1103 tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : # SSLSocket.cipher doesn't work on PyPy cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
2117	def convert ( self , value , param , ctx ) : choice = super ( MappedChoice , self ) . convert ( value , param , ctx ) ix = self . choices . index ( choice ) return self . actual_choices [ ix ]
4798	def exists ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . _err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self
9147	def web ( connection , host , port ) : from bio2bel . web . application import create_application app = create_application ( connection = connection ) app . run ( host = host , port = port )
4847	def _load_data ( self , resource , detail_resource = None , resource_id = None , querystring = None , traverse_pagination = False , default = DEFAULT_VALUE_SAFEGUARD , ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } querystring = querystring if querystring else { } cache_key = utils . get_cache_key ( resource = resource , querystring = querystring , traverse_pagination = traverse_pagination , resource_id = resource_id ) response = cache . get ( cache_key ) if not response : # Response is not cached, so make a call. endpoint = getattr ( self . client , resource ) ( resource_id ) endpoint = getattr ( endpoint , detail_resource ) if detail_resource else endpoint response = endpoint . get ( * * querystring ) if traverse_pagination : results = utils . traverse_pagination ( response , endpoint ) response = { 'count' : len ( results ) , 'next' : 'None' , 'previous' : 'None' , 'results' : results , } if response : # Now that we've got a response, cache it. cache . set ( cache_key , response , settings . ENTERPRISE_API_CACHE_TIMEOUT ) return response or default_val
2160	def _format_id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text_type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise MultipleRelatedError ( 'Could not serialize output with id format.' )
12412	def write ( self , chunk , serialize = False , format = None ) : # Ensure we're not closed. self . require_not_closed ( ) if chunk is None : # There is nothing here. return if serialize or format is not None : # Forward to the serializer to serialize the chunk # before it gets written to the response. self . serialize ( chunk , format = format ) return # `serialize` invokes write(...) if type ( chunk ) is six . binary_type : # Update the stream length. self . _length += len ( chunk ) # If passed a byte string, we hope the user encoded it properly. self . _stream . write ( chunk ) elif isinstance ( chunk , six . string_types ) : encoding = self . encoding if encoding is not None : # If passed a string, we can encode it for the user. chunk = chunk . encode ( encoding ) else : # Bail; we don't have an encoding. raise exceptions . InvalidOperation ( 'Attempting to write textual data without an encoding.' ) # Update the stream length. self . _length += len ( chunk ) # Write the encoded data into the byte stream. self . _stream . write ( chunk ) elif isinstance ( chunk , collections . Iterable ) : # If passed some kind of iterator, attempt to recurse into # oblivion. for section in chunk : self . write ( section ) else : # Bail; we have no idea what to do with this. raise exceptions . InvalidOperation ( 'Attempting to write something not recognized.' )
5465	def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
4513	def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
2791	def get_object ( cls , api_token , cert_id ) : certificate = cls ( token = api_token , id = cert_id ) certificate . load ( ) return certificate
12467	def run_hook ( hook , config , quiet = False ) : if not hook : return True if not quiet : print_message ( '== Step 3. Run post-bootstrap hook ==' ) result = not run_cmd ( prepare_args ( hook , config ) , echo = not quiet , fail_silently = True , shell = True ) if not quiet : print_message ( ) return result
6415	def median ( nums ) : nums = sorted ( nums ) mag = len ( nums ) if mag % 2 : mag = int ( ( mag - 1 ) / 2 ) return nums [ mag ] mag = int ( mag / 2 ) med = ( nums [ mag - 1 ] + nums [ mag ] ) / 2 return med if not med . is_integer ( ) else int ( med )
279	def plot_monthly_returns_heatmap ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) monthly_ret_table = monthly_ret_table . unstack ( ) . round ( 3 ) sns . heatmap ( monthly_ret_table . fillna ( 0 ) * 100.0 , annot = True , annot_kws = { "size" : 9 } , alpha = 1.0 , center = 0.0 , cbar = False , cmap = matplotlib . cm . RdYlGn , ax = ax , * * kwargs ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Month' ) ax . set_title ( "Monthly returns (%)" ) return ax
4425	def remove ( self , guild_id ) : if guild_id in self . _players : self . _players [ guild_id ] . cleanup ( ) del self . _players [ guild_id ]
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : # logging.info(" layer %d: %s" % (i, str(layer))) logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
2116	def convert ( self , value , param , ctx ) : # Protect against corner cases of invalid inputs if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) # Read from a file under these cases if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : # Sometimes click.File may return a buffer and not a string return file_obj . read ( ) return file_obj # No file, use given string return value
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) # choose the epoch epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) # choose the period, fourierorder, and amplitude period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) # fix the amplitude if it needs to be flipped if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude # generate the amplitudes and phases of the Fourier components ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] # now that we have our amp and pha components, generate the light curve modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) # resort in original time order timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] # return a dict with everything modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , # these are standard keys that help with later characterization of # variability as a function period, variability amplitude, object mag, # ndet, etc. 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
787	def partitionAtIntervals ( data , intervals ) : assert sum ( intervals ) <= len ( data ) start = 0 for interval in intervals : end = start + interval yield data [ start : end ] start = end raise StopIteration
11192	def item ( proto_dataset_uri , input_file , relpath_in_dataset ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( proto_dataset_uri , config_path = CONFIG_PATH ) if relpath_in_dataset == "" : relpath_in_dataset = os . path . basename ( input_file ) proto_dataset . put_item ( input_file , relpath_in_dataset )
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
262	def plot_returns ( perf_attrib_data , cost = None , ax = None ) : if ax is None : ax = plt . gca ( ) returns = perf_attrib_data [ 'total_returns' ] total_returns_label = 'Total returns' cumulative_returns_less_costs = _cumulative_returns_less_costs ( returns , cost ) if cost is not None : total_returns_label += ' (adjusted)' specific_returns = perf_attrib_data [ 'specific_returns' ] common_returns = perf_attrib_data [ 'common_returns' ] ax . plot ( cumulative_returns_less_costs , color = 'b' , label = total_returns_label ) ax . plot ( ep . cum_returns ( specific_returns ) , color = 'g' , label = 'Cumulative specific returns' ) ax . plot ( ep . cum_returns ( common_returns ) , color = 'r' , label = 'Cumulative common returns' ) if cost is not None : ax . plot ( - ep . cum_returns ( cost ) , color = 'k' , label = 'Cumulative cost spent' ) ax . set_title ( 'Time series of cumulative returns' ) ax . set_ylabel ( 'Returns' ) configure_legend ( ax ) return ax
8388	def merge_configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( "+" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )
840	def closestTrainingPattern ( self , inputPattern , cat ) : dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) for patIdx in sorted : patternCat = self . _categoryList [ patIdx ] # If closest pattern belongs to desired category, return it if patternCat == cat : if self . useSparseMemory : closestPattern = self . _Memory . getRow ( int ( patIdx ) ) else : closestPattern = self . _M [ patIdx ] return closestPattern # No patterns were found! return None
345	def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : path = os . path . join ( path , name ) # Define functions for loading mnist-like data's images and labels. # For convenience, they also download the requested files if needed. def load_mnist_images ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) logging . info ( filepath ) # Read the inputs in Yann LeCun's binary format. with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) # The inputs are vectors now, we reshape them to monochrome 2D images, # following the shape convention: (examples, channels, rows, columns) data = data . reshape ( shape ) # The inputs come as bytes, we convert them to float32 in range [0,1]. # (Actually to range [0, 255/256], for compatibility to the version # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.) return data / np . float32 ( 256 ) def load_mnist_labels ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) # Read the labels in Yann LeCun's binary format. with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) # The labels are vectors of integers now, that's exactly what we want. return data # Download and read the training and test set images and labels. logging . info ( "Load or Download {0} > {1}" . format ( name . upper ( ) , path ) ) X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) # We reserve the last 10000 training examples for validation. X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] # We just return all the arrays in order, as expected in main(). # (It doesn't matter how we do this as long as we can read them again.) X_train = np . asarray ( X_train , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) X_val = np . asarray ( X_val , dtype = np . float32 ) y_val = np . asarray ( y_val , dtype = np . int32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_val , y_val , X_test , y_test
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
6580	def _ensure_started ( self ) : if self . _process and self . _process . poll ( ) is None : return if not getattr ( self , "_cmd" ) : raise RuntimeError ( "Player command is not configured" ) log . debug ( "Starting playback command: %r" , self . _cmd ) self . _process = SilentPopen ( self . _cmd ) self . _post_start ( )
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
13669	def check_readable ( self , timeout ) : rlist , wlist , xlist = select . select ( [ self . _stdout ] , [ ] , [ ] , timeout ) return bool ( len ( rlist ) )
7507	def _renamer ( self , tre ) : ## get the tre with numbered tree tip labels names = tre . get_leaves ( ) ## replace numbered names with snames for name in names : name . name = self . samples [ int ( name . name ) ] ## return with only topology and leaf labels return tre . write ( format = 9 )
12213	def update_field_from_proxy ( field_obj , pref_proxy ) : attr_names = ( 'verbose_name' , 'help_text' , 'default' ) for attr_name in attr_names : setattr ( field_obj , attr_name , getattr ( pref_proxy , attr_name ) )
12483	def filter_list ( lst , pattern ) : if is_fnmatch_regex ( pattern ) and not is_regex ( pattern ) : #use fnmatch log . info ( 'Using fnmatch for {0}' . format ( pattern ) ) filst = fnmatch . filter ( lst , pattern ) else : #use re log . info ( 'Using regex match for {0}' . format ( pattern ) ) filst = match_list ( lst , pattern ) if filst : filst . sort ( ) return filst
1426	def getInstanceJstack ( self , topology_info , instance_id ) : pid_response = yield getInstancePid ( topology_info , instance_id ) try : http_client = tornado . httpclient . AsyncHTTPClient ( ) pid_json = json . loads ( pid_response ) pid = pid_json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/jstack/%s" % ( endpoint , pid ) response = yield http_client . fetch ( url ) Log . debug ( "HTTP call for url: %s" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
9569	def build_message ( self , data ) : if not data : return None return Message ( id = data [ 'message' ] [ 'mid' ] , platform = self . platform , text = data [ 'message' ] [ 'text' ] , user = data [ 'sender' ] [ 'id' ] , timestamp = data [ 'timestamp' ] , raw = data , chat = None , # TODO: Refactor build_messages and Message class )
6166	def my_psd ( x , NFFT = 2 ** 10 , Fs = 1 ) : Px , f = pylab . mlab . psd ( x , NFFT , Fs ) return Px . flatten ( ) , f
3246	def get_managed_policies ( group , * * conn ) : managed_policies = list_attached_group_managed_policies ( group [ 'GroupName' ] , * * conn ) managed_policy_names = [ ] for policy in managed_policies : managed_policy_names . append ( policy [ 'PolicyName' ] ) return managed_policy_names
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
5006	def handle_enterprise_logistration ( backend , user , * * kwargs ) : request = backend . strategy . request enterprise_customer = get_enterprise_customer_for_running_pipeline ( request , { 'backend' : backend . name , 'kwargs' : kwargs } ) if enterprise_customer is None : # This pipeline element is not being activated as a part of an Enterprise logistration return # proceed with the creation of a link between the user and the enterprise customer, then exit. enterprise_customer_user , _ = EnterpriseCustomerUser . objects . update_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enterprise_customer_user . update_session ( request )
5262	def camelcase ( string ) : string = re . sub ( r"^[\-_\.]" , '' , str ( string ) ) if not string : return string return lowercase ( string [ 0 ] ) + re . sub ( r"[\-_\.\s]([a-z])" , lambda matched : uppercase ( matched . group ( 1 ) ) , string [ 1 : ] )
9356	def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
11605	def convert_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
177	def concatenate ( self , other ) : if not isinstance ( other , LineString ) : other = LineString ( other ) return self . deepcopy ( coords = np . concatenate ( [ self . coords , other . coords ] , axis = 0 ) )
11245	def clean_strings ( iterable ) : retval = [ ] for val in iterable : try : retval . append ( val . strip ( ) ) except ( AttributeError ) : retval . append ( val ) return retval
2604	def close ( self ) : if self . reuse : logger . debug ( "Ipcontroller not shutting down: reuse enabled" ) return if self . mode == "manual" : logger . debug ( "Ipcontroller not shutting down: Manual mode" ) return try : pgid = os . getpgid ( self . proc . pid ) os . killpg ( pgid , signal . SIGTERM ) time . sleep ( 0.2 ) os . killpg ( pgid , signal . SIGKILL ) try : self . proc . wait ( timeout = 1 ) x = self . proc . returncode if x == 0 : logger . debug ( "Controller exited with {0}" . format ( x ) ) else : logger . error ( "Controller exited with {0}. May require manual cleanup" . format ( x ) ) except subprocess . TimeoutExpired : logger . warn ( "Ipcontroller process:{0} cleanup failed. May require manual cleanup" . format ( self . proc . pid ) ) except Exception as e : logger . warn ( "Failed to kill the ipcontroller process[{0}]: {1}" . format ( self . proc . pid , e ) )
13662	def atomic_write ( filename ) : f = _tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) # replace the original file with the new temp file (atomic on success) os . replace ( f . name , filename )
1629	def GetHeaderGuardCPPVariable ( filename ) : # Restores original filename in case that cpplint is invoked from Emacs's # flymake. filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) # Replace 'c++' with 'cpp'. filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep # On Windows using directory separator will leave us with # "bogus escape error" unless we properly escape regex. if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
3938	def _parse_sid_response ( res ) : res = json . loads ( list ( ChunkParser ( ) . get_chunks ( res ) ) [ 0 ] ) sid = res [ 0 ] [ 1 ] [ 1 ] gsessionid = res [ 1 ] [ 1 ] [ 0 ] [ 'gsid' ] return ( sid , gsessionid )
576	def tick ( self ) : # Run activities whose time has come for act in self . __activities : if not act . iteratorHolder [ 0 ] : continue try : next ( act . iteratorHolder [ 0 ] ) except StopIteration : act . cb ( ) if act . repeating : act . iteratorHolder [ 0 ] = iter ( xrange ( act . period ) ) else : act . iteratorHolder [ 0 ] = None return True
12684	def query ( self , input = '' , params = { } ) : # Get and construct query parameters # Default parameters payload = { 'input' : input , 'appid' : self . appid } # Additional parameters (from params), formatted for url for key , value in params . items ( ) : # Check if value is list or tuple type (needs to be comma joined) if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value # Catch any issues with connecting to Wolfram Alpha API try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) # Raise Exception (to be returned as error) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
13330	def create ( name_or_path , config ) : click . echo ( 'Creating module {}...' . format ( name_or_path ) , nl = False ) try : module = cpenv . create_module ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) click . echo ( 'Browse to your new module and make some changes.' ) click . echo ( "When you're ready add the module to an environment:" ) click . echo ( ' cpenv module add my_module ./path/to/my_module' ) click . echo ( 'Or track your module on git and add it directly from the repo:' ) click . echo ( ' cpenv module add my_module git@github.com:user/my_module.git' )
3563	def find_descriptor ( self , uuid ) : for desc in self . list_descriptors ( ) : if desc . uuid == uuid : return desc return None
6888	def parallel_epd_lcdir ( lcdir , externalparams , lcfileglob = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # find all the files matching the lcglob in lcdir if lcfileglob is None : lcfileglob = fileglob lclist = sorted ( glob . glob ( os . path . join ( lcdir , lcfileglob ) ) ) return parallel_epd_lclist ( lclist , externalparams , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , epdsmooth_sigclip = epdsmooth_sigclip , epdsmooth_windowsize = epdsmooth_windowsize , epdsmooth_func = epdsmooth_func , epdsmooth_extraparams = epdsmooth_extraparams , nworkers = nworkers , maxworkertasks = maxworkertasks )
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
11027	def raise_for_not_ok_status ( response ) : if response . code != OK : raise HTTPError ( 'Non-200 response code (%s) for url: %s' % ( response . code , uridecode ( response . request . absoluteURI ) ) ) return response
7866	def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : logger . debug ( "expdict.__setitem__({0!r}, {1!r}, {2!r}, {3!r})" . format ( key , value , timeout , timeout_callback ) ) if not timeout : timeout = self . _default_timeout self . _timeouts [ key ] = ( time . time ( ) + timeout , timeout_callback ) return dict . __setitem__ ( self , key , value )
7697	def from_xml ( cls , element ) : if element . tag != ITEM_TAG : raise ValueError ( "{0!r} is not a roster item" . format ( element ) ) try : jid = JID ( element . get ( "jid" ) ) except ValueError : raise BadRequestProtocolError ( u"Bad item JID" ) subscription = element . get ( "subscription" ) ask = element . get ( "ask" ) name = element . get ( "name" ) duplicate_group = False groups = set ( ) for child in element : if child . tag != GROUP_TAG : continue group = child . text if group is None : group = u"" if group in groups : duplicate_group = True else : groups . add ( group ) approved = element . get ( "approved" ) if approved == "true" : approved = True elif approved in ( "false" , None ) : approved = False else : logger . debug ( "RosterItem.from_xml: got unknown 'approved':" " {0!r}, changing to False" . format ( approved ) ) approved = False result = cls ( jid , name , groups , subscription , ask , approved ) result . _duplicate_group = duplicate_group return result
9167	def _make_celery_app ( config ) : # Tack the pyramid config on the celery app for later use. config . registry . celery_app . conf [ 'pyramid_config' ] = config return config . registry . celery_app
4221	def set_keyring ( keyring ) : global _keyring_backend if not isinstance ( keyring , backend . KeyringBackend ) : raise TypeError ( "The keyring must be a subclass of KeyringBackend" ) _keyring_backend = keyring
12565	def get_3D_from_4D ( image , vol_idx = 0 ) : img = check_img ( image ) hdr , aff = get_img_info ( img ) if len ( img . shape ) != 4 : raise AttributeError ( 'Volume in {} does not have 4 dimensions.' . format ( repr_imgs ( img ) ) ) if not 0 <= vol_idx < img . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, ' 'not {}.' . format ( repr_imgs ( img ) , img . shape [ 3 ] , vol_idx ) ) img_data = img . get_data ( ) new_vol = img_data [ : , : , : , vol_idx ] . copy ( ) hdr . set_data_shape ( hdr . get_data_shape ( ) [ : 3 ] ) return new_vol , hdr , aff
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
12290	def init ( username , reponame , setup , force = False , options = None , noinput = False ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) backendmgr = None if setup == 'git+s3' : backendmgr = mgr . get ( what = 'backend' , name = 's3' ) repo = repomgr . init ( username , reponame , force , backendmgr ) # Now bootstrap the datapackage.json metadata file and copy it in... # Insert a gitignore with .dgit directory in the repo. This # directory will be used to store partial results ( handle , gitignore ) = tempfile . mkstemp ( ) with open ( gitignore , 'w' ) as fd : fd . write ( ".dgit" ) # Try to bootstrap. If you cant, cleanup and return try : filename = bootstrap_datapackage ( repo , force , options , noinput ) except Exception as e : repomgr . drop ( repo , [ ] ) os . unlink ( gitignore ) raise e repo . run ( 'add_files' , [ { 'relativepath' : 'datapackage.json' , 'localfullpath' : filename , } , { 'relativepath' : '.gitignore' , 'localfullpath' : gitignore , } , ] ) # Cleanup temp files os . unlink ( filename ) os . unlink ( gitignore ) args = [ '-a' , '-m' , 'Bootstrapped the repo' ] repo . run ( 'commit' , args ) return repo
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
868	def resetCustomConfig ( cls ) : _getLogger ( ) . info ( "Resetting all custom configuration properties; " "caller=%r" , traceback . format_stack ( ) ) # Clear the in-memory settings cache, forcing reload upon subsequent "get" # request. super ( Configuration , cls ) . clear ( ) # Delete the persistent custom configuration store and reset in-memory # custom configuration info _CustomConfigurationFileWrapper . clear ( persistent = True )
5406	def _get_mount_actions ( self , mounts , mnt_datadisk ) : actions_to_add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount_path = mount . docker_path actions_to_add . extend ( [ google_v2_pipelines . build_action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' , 'RUN_IN_BACKGROUND' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) , google_v2_pipelines . build_action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ 'wait' , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) ] ) return actions_to_add
11158	def mirror_to ( self , dst ) : # pragma: no cover self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : # pragma: no cover raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : # pragma: no cover pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
7363	def with_prefix ( self , prefix , strict = False ) : def decorated ( func ) : return EventHandler ( func = func , event = self . event , prefix = prefix , strict = strict ) return decorated
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : # support sampling if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
6526	def parse ( cls , content , is_pyproject = False ) : parsed = pytoml . loads ( content ) if is_pyproject : parsed = parsed . get ( 'tool' , { } ) parsed = parsed . get ( 'tidypy' , { } ) return parsed
11540	def pin_type ( self , pin ) : if type ( pin ) is list : return [ self . pin_type ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_type ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7919	def __from_unicode ( cls , data , check = True ) : parts1 = data . split ( u"/" , 1 ) parts2 = parts1 [ 0 ] . split ( u"@" , 1 ) if len ( parts2 ) == 2 : local = parts2 [ 0 ] domain = parts2 [ 1 ] if check : local = cls . __prepare_local ( local ) domain = cls . __prepare_domain ( domain ) else : local = None domain = parts2 [ 0 ] if check : domain = cls . __prepare_domain ( domain ) if len ( parts1 ) == 2 : resource = parts1 [ 1 ] if check : resource = cls . __prepare_resource ( parts1 [ 1 ] ) else : resource = None if not domain : raise JIDError ( "Domain is required in JID." ) return ( local , domain , resource )
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : # always copy (even when dtype does not change) ret = np . asarray ( arr ) . astype ( dtype ) else : # load data from disk without changing order # Changing order while reading through a memmap is incredibly # inefficient. ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) # In the present cas, np.may_share_memory result is always reliable. if np . may_share_memory ( ret , arr ) and copy : # order-preserving copy ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
5586	def output_cleaned ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : if is_numpy_or_masked_array ( process_data ) : return process_data elif is_numpy_or_masked_array_with_tags ( process_data ) : data , tags = process_data return self . output_cleaned ( data ) , tags elif self . METADATA [ "data_type" ] == "vector" : return list ( process_data )
332	def model_stoch_vol ( data , samples = 2000 , progressbar = True ) : from pymc3 . distributions . timeseries import GaussianRandomWalk with pm . Model ( ) as model : nu = pm . Exponential ( 'nu' , 1. / 10 , testval = 5. ) sigma = pm . Exponential ( 'sigma' , 1. / .02 , testval = .1 ) s = GaussianRandomWalk ( 's' , sigma ** - 2 , shape = len ( data ) ) volatility_process = pm . Deterministic ( 'volatility_process' , pm . math . exp ( - 2 * s ) ) pm . StudentT ( 'r' , nu , lam = volatility_process , observed = data ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
13508	def create_position ( self , params = { } ) : url = "/2/positions/" body = params data = self . _post_resource ( url , body ) return self . position_from_json ( data [ "position" ] )
12601	def _check_cols ( df , col_names ) : for col in col_names : if not hasattr ( df , col ) : raise AttributeError ( "DataFrame does not have a '{}' column, got {}." . format ( col , df . columns ) )
12459	def main ( * args ) : # Create parser, read arguments from direct input or command line with disable_error_handler ( ) : args = parse_args ( args or sys . argv [ 1 : ] ) # Read current config from file and command line arguments config = read_config ( args . config , args ) if config is None : return True bootstrap = config [ __script__ ] # Check pre-requirements if not check_pre_requirements ( bootstrap [ 'pre_requirements' ] ) : return True # Create virtual environment env_args = prepare_args ( config [ 'virtualenv' ] , bootstrap ) if not create_env ( bootstrap [ 'env' ] , env_args , bootstrap [ 'recreate' ] , bootstrap [ 'ignore_activated' ] , bootstrap [ 'quiet' ] ) : # Exit if couldn't create virtual environment return True # And install library or project here pip_args = prepare_args ( config [ 'pip' ] , bootstrap ) if not install ( bootstrap [ 'env' ] , bootstrap [ 'requirements' ] , pip_args , bootstrap [ 'ignore_activated' ] , bootstrap [ 'install_dev_requirements' ] , bootstrap [ 'quiet' ] ) : # Exist if couldn't install requirements into venv return True # Run post-bootstrap hook run_hook ( bootstrap [ 'hook' ] , bootstrap , bootstrap [ 'quiet' ] ) # All OK! if not bootstrap [ 'quiet' ] : print_message ( 'All OK!' ) # False means everything went alright, exit code: 0 return False
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) # this is now a weighted residual taking into account the measurement err return ( pmags - modelmags ) / perrs
7731	def make_join_request ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : self . clear_muc_child ( ) self . muc_child = MucX ( parent = self . xmlnode ) if ( history_maxchars is not None or history_maxstanzas is not None or history_seconds is not None or history_since is not None ) : history = HistoryParameters ( history_maxchars , history_maxstanzas , history_seconds , history_since ) self . muc_child . set_history ( history ) if password is not None : self . muc_child . set_password ( password )
10815	def invite_by_emails ( self , emails ) : assert emails is None or isinstance ( emails , list ) results = [ ] for email in emails : try : user = User . query . filter_by ( email = email ) . one ( ) results . append ( self . invite ( user ) ) except NoResultFound : results . append ( None ) return results
1217	def register_saver_ops ( self ) : variables = self . get_savable_variables ( ) if variables is None or len ( variables ) == 0 : self . _saver = None return base_scope = self . _get_base_variable_scope ( ) variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } self . _saver = tf . train . Saver ( var_list = variables_map , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )
2058	def _dict_diff ( d1 , d2 ) : d = { } for key in set ( d1 ) . intersection ( set ( d2 ) ) : if d2 [ key ] != d1 [ key ] : d [ key ] = d2 [ key ] for key in set ( d2 ) . difference ( set ( d1 ) ) : d [ key ] = d2 [ key ] return d
12517	def extract_datasets ( h5file , h5path = '/' ) : if isinstance ( h5file , str ) : _h5file = h5py . File ( h5file , mode = 'r' ) else : _h5file = h5file _datasets = get_datasets ( _h5file , h5path ) datasets = OrderedDict ( ) try : for ds in _datasets : datasets [ ds . name . split ( '/' ) [ - 1 ] ] = ds [ : ] except : raise RuntimeError ( 'Error reading datasets in {}/{}.' . format ( _h5file . filename , h5path ) ) finally : if isinstance ( h5file , str ) : _h5file . close ( ) return datasets
9486	def ensure_instruction ( instruction : int ) -> bytes : if PY36 : return instruction . to_bytes ( 2 , byteorder = "little" ) else : return instruction . to_bytes ( 1 , byteorder = "little" )
10169	def register_receivers ( app , config ) : for event_name , event_config in config . items ( ) : event_builders = [ obj_or_import_string ( func ) for func in event_config . get ( 'event_builders' , [ ] ) ] signal = obj_or_import_string ( event_config [ 'signal' ] ) signal . connect ( EventEmmiter ( event_name , event_builders ) , sender = app , weak = False )
6300	def get_dirs ( self ) -> List [ str ] : for package in self . packages : yield os . path . join ( package . path , 'resources' )
7413	def plot ( self ) : if self . results_table == None : return "no results found" else : bb = self . results_table . sort_values ( by = [ "ABCD" , "ACBD" ] , ascending = [ False , True ] , ) ## make a barplot import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m
10565	def exclude_filepaths ( filepaths , exclude_patterns = None ) : if not exclude_patterns : return filepaths , [ ] exclude_re = re . compile ( "|" . join ( pattern for pattern in exclude_patterns ) ) included_songs = [ ] excluded_songs = [ ] for filepath in filepaths : if exclude_patterns and exclude_re . search ( filepath ) : excluded_songs . append ( filepath ) else : included_songs . append ( filepath ) return included_songs , excluded_songs
8938	def confluence ( ctx , no_publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) # force a full rebuild if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + '_cf' ] ) if no_publish : cmd . extend ( [ '-Dconfluence_publish=False' ] ) # Build docs notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
7511	def select_samples ( dbsamples , samples , pidx = None ) : ## get index from dbsamples samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
3251	def save ( self , obj , content_type = "application/xml" ) : rest_url = obj . href data = obj . message ( ) headers = { "Content-type" : content_type , "Accept" : content_type } logger . debug ( "{} {}" . format ( obj . save_method , obj . href ) ) resp = self . http_request ( rest_url , method = obj . save_method . lower ( ) , data = data , headers = headers ) if resp . status_code not in ( 200 , 201 ) : raise FailedRequestError ( 'Failed to save to Geoserver catalog: {}, {}' . format ( resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : # get index writers for all enabled formats index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : # if there are indexes to write to, check if output exists tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) # only write entries if there are indexes to write to and output # exists if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) # yield tile for progress information yield tile
4218	def _get_env ( self , env_var ) : value = os . environ . get ( env_var ) if not value : raise ValueError ( 'Missing environment variable:%s' % env_var ) return value
7208	def task_ids ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get task IDs.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for task IDs." ) wf = self . workflow . get ( self . id ) return [ task [ 'id' ] for task in wf [ 'tasks' ] ]
11168	def _add_option ( self , option ) : if option . name in self . options : raise ValueError ( 'name already in use' ) if option . abbreviation in self . abbreviations : raise ValueError ( 'abbreviation already in use' ) if option . name in [ arg . name for arg in self . positional_args ] : raise ValueError ( 'name already in use by a positional argument' ) self . options [ option . name ] = option if option . abbreviation : self . abbreviations [ option . abbreviation ] = option self . option_order . append ( option . name )
10021	def get_environments ( self ) : response = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) return response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ]
9819	def teardown ( self , hooks = True ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . teardown_on_kubernetes ( hooks = hooks ) elif self . is_docker_compose : self . teardown_on_docker_compose ( ) elif self . is_docker : self . teardown_on_docker ( hooks = hooks ) elif self . is_heroku : self . teardown_on_heroku ( hooks = hooks )
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
7395	def get_publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink_set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile_set . all ( ) return render_template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )
3444	def load_json_model ( filename ) : if isinstance ( filename , string_types ) : with open ( filename , "r" ) as file_handle : return model_from_dict ( json . load ( file_handle ) ) else : return model_from_dict ( json . load ( filename ) )
11589	def _rc_rpoplpush ( self , src , dst ) : rpop = self . rpop ( src ) if rpop is not None : self . lpush ( dst , rpop ) return rpop return None
13853	def run ( self ) : if not self . device : return try : data = "" while ( self . do_run ) : try : if ( self . device . inWaiting ( ) > 1 ) : l = self . device . readline ( ) [ : - 2 ] l = l . decode ( "UTF-8" ) if ( l == "[" ) : # start recording data = "[" elif ( l == "]" ) and ( len ( data ) > 4 ) and ( data [ 0 ] == "[" ) : # now parse the input data = data + "]" self . store . register_json ( data ) self . age ( ) elif ( l [ 0 : 3 ] == " {" ) : # this is a data line data = data + " " + l else : # this is a slow interface - give it some time sleep ( 1 ) # then count down.. self . age ( ) except ( UnicodeDecodeError , ValueError ) : # only accepting unicode: throw away the whole bunch data = "" # and count down the exit condition self . age ( ) except serial . serialutil . SerialException : print ( "Could not connect to the serial line at " + self . device_name )
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , * * kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , * * kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
6891	def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
3119	def get_prep_value ( self , value ) : if value is None : return None else : return encoding . smart_text ( base64 . b64encode ( jsonpickle . encode ( value ) . encode ( ) ) )
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
8373	def widget_changed ( self , widget , v ) : # set the appropriate bot var if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v ) # pretty dumb for now elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v ) # pretty dumb for now elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v )
3978	def _get_referenced_libs ( specs ) : active_libs = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for lib in app_spec [ 'depends' ] [ 'libs' ] : active_libs . add ( lib ) return active_libs
7492	def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )
5305	def detect_color_support ( env ) : # noqa if env . get ( 'COLORFUL_DISABLE' , '0' ) == '1' : return NO_COLORS if env . get ( 'COLORFUL_FORCE_8_COLORS' , '0' ) == '1' : return ANSI_8_COLORS if env . get ( 'COLORFUL_FORCE_16_COLORS' , '0' ) == '1' : return ANSI_16_COLORS if env . get ( 'COLORFUL_FORCE_256_COLORS' , '0' ) == '1' : return ANSI_256_COLORS if env . get ( 'COLORFUL_FORCE_TRUE_COLORS' , '0' ) == '1' : return TRUE_COLORS # if we are not a tty if not sys . stdout . isatty ( ) : return NO_COLORS colorterm_env = env . get ( 'COLORTERM' ) if colorterm_env : if colorterm_env in { 'truecolor' , '24bit' } : return TRUE_COLORS if colorterm_env in { '8bit' } : return ANSI_256_COLORS termprog_env = env . get ( 'TERM_PROGRAM' ) if termprog_env : if termprog_env in { 'iTerm.app' , 'Hyper' } : return TRUE_COLORS if termprog_env in { 'Apple_Terminal' } : return ANSI_256_COLORS term_env = env . get ( 'TERM' ) if term_env : if term_env in { 'screen-256' , 'screen-256color' , 'xterm-256' , 'xterm-256color' } : return ANSI_256_COLORS if term_env in { 'screen' , 'xterm' , 'vt100' , 'color' , 'ansi' , 'cygwin' , 'linux' } : return ANSI_16_COLORS if colorterm_env : # if there was no match with $TERM either but we # had one with $COLORTERM, we use it! return ANSI_16_COLORS return ANSI_8_COLORS
10389	def calculate_average_scores_on_subgraphs ( subgraphs : Mapping [ H , BELGraph ] , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , tqdm_kwargs : Optional [ Mapping [ str , Any ] ] = None , ) -> Mapping [ H , Tuple [ float , float , float , float , int , int ] ] : results = { } log . info ( 'calculating results for %d candidate mechanisms using %d permutations' , len ( subgraphs ) , runs ) it = subgraphs . items ( ) if use_tqdm : _tqdm_kwargs = dict ( total = len ( subgraphs ) , desc = 'Candidate mechanisms' ) if tqdm_kwargs : _tqdm_kwargs . update ( tqdm_kwargs ) it = tqdm ( it , * * _tqdm_kwargs ) for node , subgraph in it : number_first_neighbors = subgraph . in_degree ( node ) number_first_neighbors = 0 if isinstance ( number_first_neighbors , dict ) else number_first_neighbors mechanism_size = subgraph . number_of_nodes ( ) runners = workflow ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if 0 == len ( scores ) : results [ node ] = ( None , None , None , None , number_first_neighbors , mechanism_size , ) continue scores = np . array ( scores ) average_score = np . average ( scores ) score_std = np . std ( scores ) med_score = np . median ( scores ) chi_2_stat , norm_p = stats . normaltest ( scores ) results [ node ] = ( average_score , score_std , norm_p , med_score , number_first_neighbors , mechanism_size , ) return results
10760	def from_rectilinear ( cls , x , y , z , formatter = numpy_formatter ) : x = np . asarray ( x , dtype = np . float64 ) y = np . asarray ( y , dtype = np . float64 ) z = np . ma . asarray ( z , dtype = np . float64 ) # Check arguments. if x . ndim != 1 : raise TypeError ( "'x' must be a 1D array but is a {:d}D array" . format ( x . ndim ) ) if y . ndim != 1 : raise TypeError ( "'y' must be a 1D array but is a {:d}D array" . format ( y . ndim ) ) if z . ndim != 2 : raise TypeError ( "'z' must be a 2D array but it a {:d}D array" . format ( z . ndim ) ) if x . size != z . shape [ 1 ] : raise TypeError ( ( "the length of 'x' must be equal to the number of columns in " "'z' but the length of 'x' is {:d} and 'z' has {:d} " "columns" ) . format ( x . size , z . shape [ 1 ] ) ) if y . size != z . shape [ 0 ] : raise TypeError ( ( "the length of 'y' must be equal to the number of rows in " "'z' but the length of 'y' is {:d} and 'z' has {:d} " "rows" ) . format ( y . size , z . shape [ 0 ] ) ) # Convert to curvilinear format and call constructor. y , x = np . meshgrid ( y , x , indexing = 'ij' ) return cls ( x , y , z , formatter )
536	def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance
5333	def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
9280	def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
5403	def _get_localization_env ( self , inputs , user_project ) : # Add variables for paths that need to be localized, for example: # INPUT_COUNT: 1 # INPUT_0: MY_INPUT_FILE # INPUT_RECURSIVE_0: 0 # INPUT_SRC_0: gs://mybucket/mypath/myfile # INPUT_DST_0: /mnt/data/inputs/mybucket/mypath/myfile non_empty_inputs = [ var for var in inputs if var . value ] env = { 'INPUT_COUNT' : str ( len ( non_empty_inputs ) ) } for idx , var in enumerate ( non_empty_inputs ) : env [ 'INPUT_{}' . format ( idx ) ] = var . name env [ 'INPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT_SRC_{}' . format ( idx ) ] = var . value # For wildcard paths, the destination must be a directory dst = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
3528	def get_required_setting ( setting , value_re , invalid_msg ) : try : value = getattr ( settings , setting ) except AttributeError : raise AnalyticalException ( "%s setting: not found" % setting ) if not value : raise AnalyticalException ( "%s setting is not set" % setting ) value = str ( value ) if not value_re . search ( value ) : raise AnalyticalException ( "%s setting: %s: '%s'" % ( setting , invalid_msg , value ) ) return value
1088	def countOf ( a , b ) : count = 0 for i in a : if i == b : count += 1 return count
12968	def delete ( self ) : if self . filters or self . notFilters : return self . mdl . deleter . deleteMultiple ( self . allOnlyIndexedFields ( ) ) return self . mdl . deleter . destroyModel ( )
5030	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , is_passing = False , * * kwargs ) : # pylint: disable=arguments-differ,unused-argument # Degreed expects completion dates of the form 'yyyy-mm-dd'. completed_timestamp = completed_date . strftime ( "%F" ) if isinstance ( completed_date , datetime ) else None if enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) is not None : DegreedLearnerDataTransmissionAudit = apps . get_model ( # pylint: disable=invalid-name 'degreed' , 'DegreedLearnerDataTransmissionAudit' ) # We return two records here, one with the course key and one with the course run id, to account for # uncertainty about the type of content (course vs. course run) that was sent to the integrated channel. return [ DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) , DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = enterprise_enrollment . course_id , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because a Degreed user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
10823	def query_requests ( cls , admin , eager = False ) : # Get direct pending request if hasattr ( admin , 'is_superadmin' ) and admin . is_superadmin : q1 = GroupAdmin . query . with_entities ( GroupAdmin . group_id ) else : q1 = GroupAdmin . query_by_admin ( admin ) . with_entities ( GroupAdmin . group_id ) q2 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q1 ) , ) # Get request from admin groups your are member of q3 = Membership . query_by_user ( user = admin , state = MembershipState . ACTIVE ) . with_entities ( Membership . id_group ) q4 = GroupAdmin . query . filter ( GroupAdmin . admin_type == 'Group' , GroupAdmin . admin_id . in_ ( q3 ) ) . with_entities ( GroupAdmin . group_id ) q5 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q4 ) ) query = q2 . union ( q5 ) return query
1087	def concat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
3668	def K_value ( P = None , Psat = None , phi_l = None , phi_g = None , gamma = None , Poynting = 1 ) : try : if gamma : if phi_l : return gamma * Psat * phi_l * Poynting / ( phi_g * P ) return gamma * Psat * Poynting / P elif phi_l : return phi_l / phi_g return Psat / P except TypeError : raise Exception ( 'Input must consist of one set from (P, Psat, phi_l, \ phi_g, gamma), (P, Psat, gamma), (phi_l, phi_g), (P, Psat)' )
4426	async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
2436	def add_reviewer ( self , doc , reviewer ) : # Each reviewer marks the start of a new review object. # FIXME: this state does not make sense self . reset_reviews ( ) if validations . validate_reviewer ( reviewer ) : doc . add_review ( review . Review ( reviewer = reviewer ) ) return True else : raise SPDXValueError ( 'Review::Reviewer' )
13681	def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
4414	def add_next ( self , requester : int , track : dict ) : self . queue . insert ( 0 , AudioTrack ( ) . build ( track , requester ) )
2680	def get_client ( client , profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : boto3 . setup_default_session ( profile_name = profile_name , aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key , region_name = region , ) return boto3 . client ( client )
8374	def var_added ( self , v ) : self . add_variable ( v ) self . window . set_size_request ( 400 , 35 * len ( self . widgets . keys ( ) ) ) self . window . show_all ( )
12040	def checkOut ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not "__" in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
9269	def version_of_first_item ( self ) : try : sections = read_changelog ( self . options ) return sections [ 0 ] [ "version" ] except ( IOError , TypeError ) : return self . get_temp_tag_for_repo_creation ( )
10360	def is_edge_consistent ( graph , u , v ) : if not graph . has_edge ( u , v ) : raise ValueError ( '{} does not contain an edge ({}, {})' . format ( graph , u , v ) ) return 0 == len ( set ( d [ RELATION ] for d in graph . edge [ u ] [ v ] . values ( ) ) )
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
173	def draw_heatmap_array ( self , image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : heatmap_lines = self . draw_lines_heatmap_array ( image_shape , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) if size_points <= 0 : return heatmap_lines heatmap_points = self . draw_points_heatmap_array ( image_shape , alpha = alpha_points , size = size_points , raise_if_out_of_image = raise_if_out_of_image ) heatmap = np . dstack ( [ heatmap_lines , heatmap_points ] ) return np . max ( heatmap , axis = 2 )
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) # response table re-initialization # for each pin set the mode to input and the last read data value to zero with self . pymata . data_lock : # remove all old entries from existing tables for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) # reinitialize tables for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
392	def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) # adjust meta data adjust_joint_list = [ ] for joint in annos : # TODO : speed up with affine transform adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None
9421	def _read_header ( self , handle ) : header_data = unrarlib . RARHeaderDataEx ( ) try : res = unrarlib . RARReadHeaderEx ( handle , ctypes . byref ( header_data ) ) rarinfo = RarInfo ( header = header_data ) except unrarlib . ArchiveEnd : return None except unrarlib . MissingPassword : raise RuntimeError ( "Archive is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for Archive" ) except unrarlib . UnrarException as e : raise BadRarFile ( str ( e ) ) return rarinfo
12958	def _compat_rem_str_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _compat_get_str_key_for_index ( indexedField , val ) , pk )
3707	def Amgat ( xs , Vms ) : if not none_and_length_check ( [ xs , Vms ] ) : raise Exception ( 'Function inputs are incorrect format' ) return mixing_simple ( xs , Vms )
11073	def set_nested ( data , value , * keys ) : if len ( keys ) == 1 : data [ keys [ 0 ] ] = value else : if keys [ 0 ] not in data : data [ keys [ 0 ] ] = { } set_nested ( data [ keys [ 0 ] ] , value , * keys [ 1 : ] )
5234	def sort_by_modified ( files_or_folders : list ) -> list : return sorted ( files_or_folders , key = os . path . getmtime , reverse = True )
2209	def parse_requirements_alt ( fname = 'requirements.txt' ) : import requirements from os . path import dirname , join , exists require_fpath = join ( dirname ( __file__ ) , fname ) if exists ( require_fpath ) : # Dont use until this handles platform specific dependencies with open ( require_fpath , 'r' ) as file : requires = list ( requirements . parse ( file ) ) packages = [ r . name for r in requires ] return packages return [ ]
2652	def execute_no_wait ( self , cmd , walltime = 2 , envs = { } ) : # Execute the command stdin , stdout , stderr = self . ssh_client . exec_command ( self . prepend_envs ( cmd , envs ) , bufsize = - 1 , timeout = walltime ) return None , stdout , stderr
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : # is numpy array return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
12205	def url_builder ( self , endpoint , * , root = None , params = None , url_params = None ) : if root is None : root = self . ROOT scheme , netloc , path , _ , _ = urlsplit ( root ) return urlunsplit ( ( scheme , netloc , urljoin ( path , endpoint ) , urlencode ( url_params or { } ) , '' , ) ) . format ( * * params or { } )
12069	def save ( abf , fname = None , tag = None , width = 700 , close = True , facecolor = 'w' , resize = True ) : if len ( pylab . gca ( ) . get_lines ( ) ) == 0 : print ( "can't save, no figure!" ) return if resize : pylab . tight_layout ( ) pylab . subplots_adjust ( bottom = .1 ) annotate ( abf ) if tag : fname = abf . outpath + abf . ID + "_" + tag + ".png" inchesX , inchesY = pylab . gcf ( ) . get_size_inches ( ) dpi = width / inchesX if fname : if not os . path . exists ( abf . outpath ) : os . mkdir ( abf . outpath ) print ( " <- saving [%s] at %d DPI (%dx%d)" % ( os . path . basename ( fname ) , dpi , inchesX * dpi , inchesY * dpi ) ) pylab . savefig ( fname , dpi = dpi , facecolor = facecolor ) else : pylab . show ( ) if close : pylab . close ( )
13559	def get_top_assets ( self ) : images = self . get_all_images ( ) [ 0 : 14 ] video = [ ] if supports_video : video = self . eventvideo_set . all ( ) [ 0 : 10 ] return list ( chain ( images , video ) ) [ 0 : 15 ]
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
4177	def window_cosine ( N ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) win = sin ( pi * n / ( N - 1. ) ) return win
6864	def normalized_flux_to_mag ( lcdict , columns = ( 'sap.sap_flux' , 'sap.sap_flux_err' , 'sap.sap_bkg' , 'sap.sap_bkg_err' , 'pdc.pdcsap_flux' , 'pdc.pdcsap_flux_err' ) ) : tess_mag = lcdict [ 'objectinfo' ] [ 'tessmag' ] for key in columns : k1 , k2 = key . split ( '.' ) if 'err' not in k2 : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( tess_mag - 2.5 * np . log10 ( lcdict [ k1 ] [ k2 ] ) ) else : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( - 2.5 * np . log10 ( 1.0 - lcdict [ k1 ] [ k2 ] ) ) return lcdict
8783	def _get_base_network_info ( self , context , network_id , base_net_driver ) : driver_name = base_net_driver . get_name ( ) net_info = { "network_type" : driver_name } LOG . debug ( '_get_base_network_info: %s %s' % ( driver_name , network_id ) ) # If the driver is NVP, we need to look up the lswitch id we should # be attaching to. if driver_name == 'NVP' : LOG . debug ( 'looking up lswitch ids for network %s' % ( network_id ) ) lswitch_ids = base_net_driver . get_lswitch_ids_for_network ( context , network_id ) if not lswitch_ids or len ( lswitch_ids ) > 1 : msg = ( 'lswitch id lookup failed, %s ids found.' % ( len ( lswitch_ids ) ) ) LOG . error ( msg ) raise IronicException ( msg ) lswitch_id = lswitch_ids . pop ( ) LOG . info ( 'found lswitch for network %s: %s' % ( network_id , lswitch_id ) ) net_info [ 'lswitch_id' ] = lswitch_id LOG . debug ( '_get_base_network_info finished: %s %s %s' % ( driver_name , network_id , net_info ) ) return net_info
327	def summarize_paths ( samples , cone_std = ( 1. , 1.5 , 2. ) , starting_value = 1. ) : cum_samples = ep . cum_returns ( samples . T , starting_value = starting_value ) . T cum_mean = cum_samples . mean ( axis = 0 ) cum_std = cum_samples . std ( axis = 0 ) if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] cone_bounds = pd . DataFrame ( columns = pd . Float64Index ( [ ] ) ) for num_std in cone_std : cone_bounds . loc [ : , float ( num_std ) ] = cum_mean + cum_std * num_std cone_bounds . loc [ : , float ( - num_std ) ] = cum_mean - cum_std * num_std return cone_bounds
3103	def do_GET ( self ) : self . send_response ( http_client . OK ) self . send_header ( 'Content-type' , 'text/html' ) self . end_headers ( ) parts = urllib . parse . urlparse ( self . path ) query = _helpers . parse_unique_urlencoded ( parts . query ) self . server . query_params = query self . wfile . write ( b'<html><head><title>Authentication Status</title></head>' ) self . wfile . write ( b'<body><p>The authentication flow has completed.</p>' ) self . wfile . write ( b'</body></html>' )
12011	def extractFile ( self , filename ) : files = [ x for x in self . tableOfContents if x [ 'filename' ] == filename ] if len ( files ) == 0 : raise FileNotFoundException ( ) fileRecord = files [ 0 ] # got here? need to fetch the file size metaheadroom = 1024 # should be enough request = urllib2 . Request ( self . zipURI ) start = fileRecord [ 'filestart' ] end = fileRecord [ 'filestart' ] + fileRecord [ 'compressedsize' ] + metaheadroom request . headers [ 'Range' ] = "bytes=%s-%s" % ( start , end ) handle = urllib2 . urlopen ( request ) # make sure the response is ranged return_range = handle . headers . get ( 'Content-Range' ) if return_range != "bytes %d-%d/%s" % ( start , end , self . filesize ) : raise Exception ( "Ranged requests are not supported for this URI" ) filedata = handle . read ( ) # find start of raw file data zip_n = unpack ( "H" , filedata [ 26 : 28 ] ) [ 0 ] zip_m = unpack ( "H" , filedata [ 28 : 30 ] ) [ 0 ] # check compressed size has_data_descriptor = bool ( unpack ( "H" , filedata [ 6 : 8 ] ) [ 0 ] & 8 ) comp_size = unpack ( "I" , filedata [ 18 : 22 ] ) [ 0 ] if comp_size == 0 and has_data_descriptor : # assume compressed size in the Central Directory is correct comp_size = fileRecord [ 'compressedsize' ] elif comp_size != fileRecord [ 'compressedsize' ] : raise Exception ( "Something went wrong. Directory and file header disagree of compressed file size" ) raw_zip_data = filedata [ 30 + zip_n + zip_m : 30 + zip_n + zip_m + comp_size ] uncompressed_data = "" # can't decompress if stored without compression compression_method = unpack ( "H" , filedata [ 8 : 10 ] ) [ 0 ] if compression_method == 0 : return raw_zip_data dec = zlib . decompressobj ( - zlib . MAX_WBITS ) for chunk in raw_zip_data : rv = dec . decompress ( chunk ) if rv : uncompressed_data = uncompressed_data + rv return uncompressed_data
3692	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : return TWU_a_alpha_common ( T , self . Tc , self . omega , self . a , full = full , quick = quick , method = 'SRK' )
8384	def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : # Rounded rectangle in the given background color. p , h = self . textpath ( self . i ) f = self . fontsize self . _ctx . fill ( self . background ) self . _ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . _w + f , h + f * 1.5 , roundness = 0.2 ) # Fade in/out the current text. alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . _ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . _ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . _ctx . drawpath ( p )
11635	def oauth2_access_parser ( self , raw_access ) : parsed_access = json . loads ( raw_access . content . decode ( 'utf-8' ) ) self . access_token = parsed_access [ 'access_token' ] self . token_type = parsed_access [ 'token_type' ] self . refresh_token = parsed_access [ 'refresh_token' ] self . guid = parsed_access [ 'xoauth_yahoo_guid' ] credentials = { 'access_token' : self . access_token , 'token_type' : self . token_type , 'refresh_token' : self . refresh_token , 'guid' : self . guid } return credentials
13679	def register_json ( self , data ) : j = json . loads ( data ) self . last_data_timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : # prepare the sensor entry container self . data [ v [ self . id_key ] ] = { } # add the mandatory entries self . data [ v [ self . id_key ] ] [ self . id_key ] = v [ self . id_key ] self . data [ v [ self . id_key ] ] [ self . value_key ] = v [ self . value_key ] # add the optional well known entries if provided if self . unit_key in v : self . data [ v [ self . id_key ] ] [ self . unit_key ] = v [ self . unit_key ] if self . threshold_key in v : self . data [ v [ self . id_key ] ] [ self . threshold_key ] = v [ self . threshold_key ] # add any further entries found for k in self . other_keys : if k in v : self . data [ v [ self . id_key ] ] [ k ] = v [ k ] # add the custom sensor time if self . sensor_time_key in v : self . data [ v [ self . sensor_time_key ] ] [ self . sensor_time_key ] = v [ self . sensor_time_key ] # last: add the time the data was received (overwriting any # not properly defined timestamp that was already there) self . data [ v [ self . id_key ] ] [ self . time_key ] = self . last_data_timestamp except KeyError as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except ValueError as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
11053	def _issue_cert ( self , domain ) : def errback ( failure ) : # Don't fail on some of the errors we could get from the ACME # server, rather just log an error so that we can continue with # other domains. failure . trap ( txacme_ServerError ) acme_error = failure . value . message if acme_error . code in [ 'rateLimited' , 'serverInternal' , 'connection' , 'unknownHost' ] : # TODO: Fire off an error to Sentry or something? self . log . error ( 'Error ({code}) issuing certificate for "{domain}": ' '{detail}' , code = acme_error . code , domain = domain , detail = acme_error . detail ) else : # There are more error codes but if they happen then something # serious has gone wrong-- carry on error-ing. return failure d = self . txacme_service . issue_cert ( domain ) return d . addErrback ( errback )
10161	def py_hash ( key , num_buckets ) : b , j = - 1 , 0 if num_buckets < 1 : raise ValueError ( 'num_buckets must be a positive number' ) while j < num_buckets : b = int ( j ) key = ( ( key * long ( 2862933555777941757 ) ) + 1 ) & 0xffffffffffffffff j = float ( b + 1 ) * ( float ( 1 << 31 ) / float ( ( key >> 33 ) + 1 ) ) return int ( b )
459	def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) # disable noise layers feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result
7193	def histogram_match ( self , use_bands , blm_source = None , * * kwargs ) : assert has_rio , "To match image histograms please install rio_hist" data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked_values ( data , 0 ) bounds = self . _reproject ( box ( * self . bounds ) , from_proj = self . proj , to_proj = "EPSG:4326" ) . bounds if blm_source == 'browse' : from gbdxtools . images . browse_image import BrowseImage ref = BrowseImage ( self . cat_id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms_image import TmsImage tms = TmsImage ( zoom = self . _calc_tms_zoom ( self . affine [ 0 ] ) , bbox = bounds , * * kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio_match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( out , * * kwargs ) else : return out
11484	def _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing = False ) : item_id = _create_or_reuse_item ( local_folder , parent_folder_id , reuse_existing ) subdir_contents = sorted ( os . listdir ( local_folder ) ) # for each file in the subdir, add it to the item filecount = len ( subdir_contents ) for ( ind , current_file ) in enumerate ( subdir_contents ) : file_path = os . path . join ( local_folder , current_file ) log_ind = '({0} of {1})' . format ( ind + 1 , filecount ) _create_bitstream ( file_path , current_file , item_id , log_ind ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , item_id )
3198	def start ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'actions/start' ) )
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
10720	def get_parser ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( "package" , choices = arg_map . keys ( ) , help = "designates the package to test" ) parser . add_argument ( "--ignore" , help = "ignore these files" ) return parser
12443	def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : # The specified method is not allowed for the resource # identified by the request URI. # RFC 2616 § 10.4.6 — 405 Method Not Allowed raise http . exceptions . MethodNotAllowed ( allowed )
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
4671	def unlock ( self , pwd ) : if self . store . is_encrypted ( ) : return self . store . unlock ( pwd )
2214	def repr2 ( data , * * kwargs ) : custom_extensions = kwargs . get ( 'extensions' , None ) _return_info = kwargs . get ( '_return_info' , False ) kwargs [ '_root_info' ] = _rectify_root_info ( kwargs . get ( '_root_info' , None ) ) outstr = None _leaf_info = None if custom_extensions : func = custom_extensions . lookup ( data ) if func is not None : outstr = func ( data , * * kwargs ) if outstr is None : if isinstance ( data , dict ) : outstr , _leaf_info = _format_dict ( data , * * kwargs ) elif isinstance ( data , ( list , tuple , set , frozenset ) ) : outstr , _leaf_info = _format_list ( data , * * kwargs ) if outstr is None : # check any globally registered functions for special formatters func = _FORMATTER_EXTENSIONS . lookup ( data ) if func is not None : outstr = func ( data , * * kwargs ) else : outstr = _format_object ( data , * * kwargs ) if _return_info : _leaf_info = _rectify_leaf_info ( _leaf_info ) return outstr , _leaf_info else : return outstr
6514	def get_gender ( self , name , country = None ) : if not self . case_sensitive : name = name . lower ( ) if name not in self . names : return self . unknown_value elif not country : def counter ( country_values ) : country_values = map ( ord , country_values . replace ( " " , "" ) ) return ( len ( country_values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country_values ) ) ) return self . _most_popular_gender ( name , counter ) elif country in self . __class__ . COUNTRIES : index = self . __class__ . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . _most_popular_gender ( name , counter ) else : raise NoCountryError ( "No such country: %s" % country )
1018	def addSynapse ( self , srcCellCol , srcCellIdx , perm ) : self . syns . append ( [ int ( srcCellCol ) , int ( srcCellIdx ) , numpy . float32 ( perm ) ] )
3017	def from_json_keyfile_dict ( cls , keyfile_dict , scopes = '' , token_uri = None , revoke_uri = None ) : return cls . _from_parsed_json_keyfile ( keyfile_dict , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
6842	def force_stop ( self ) : r = self . local_renderer with self . settings ( warn_only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )
12784	def get_xdg_dirs ( self ) : # type: () -> List[str] config_dirs = getenv ( 'XDG_CONFIG_DIRS' , '' ) if config_dirs : self . _log . debug ( 'XDG_CONFIG_DIRS is set to %r' , config_dirs ) output = [ ] for path in reversed ( config_dirs . split ( ':' ) ) : output . append ( join ( path , self . group_name , self . app_name ) ) return output return [ '/etc/xdg/%s/%s' % ( self . group_name , self . app_name ) ]
1268	def _fire ( self , layers , things , the_plot ) : # We don't fire if the player fired another bolt just now. if the_plot . get ( 'last_player_shot' ) == the_plot . frame : return the_plot [ 'last_player_shot' ] = the_plot . frame # We start just above the player. row , col = things [ 'P' ] . position self . _teleport ( ( row - 1 , col ) )
5502	def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force_update ) : if source : source_obj = ctx . obj [ "conf" ] . get_source_by_nick ( source ) if not source_obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source_obj = Source ( source , source ) sources = [ source_obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update_interval = ctx . obj [ "conf" ] . timeline_update_interval ) as cache : force_update = force_update or not cache . is_valid if force_update : tweets = get_remote_tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update_interval ) ) # Behold, almighty list comprehensions! (I might have gone overboard here…) tweets = list ( chain . from_iterable ( [ cache . get_tweets ( source . url ) for source in sources ] ) ) except OSError as e : logger . debug ( e ) tweets = get_remote_tweets ( sources , limit , timeout ) else : tweets = get_remote_tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get_local_tweets ( source , limit ) ) if not tweets : return tweets = sort_and_truncate_tweets ( tweets , sorting , limit ) if pager : click . echo_via_pager ( style_timeline ( tweets , porcelain ) ) else : click . echo ( style_timeline ( tweets , porcelain ) )
346	def load_matt_mahoney_text8_dataset ( path = 'data' ) : path = os . path . join ( path , 'mm_test8' ) logging . info ( "Load or Download matt_mahoney_text8 Dataset> {}" . format ( path ) ) filename = 'text8.zip' url = 'http://mattmahoney.net/dc/' maybe_download_and_extract ( filename , path , url , expected_bytes = 31344016 ) with zipfile . ZipFile ( os . path . join ( path , filename ) ) as f : word_list = f . read ( f . namelist ( ) [ 0 ] ) . split ( ) for idx , _ in enumerate ( word_list ) : word_list [ idx ] = word_list [ idx ] . decode ( ) return word_list
10000	def clear_obj ( self , obj ) : obj_nodes = self . get_nodes_with ( obj ) removed = set ( ) for node in obj_nodes : if self . has_node ( node ) : removed . update ( self . clear_descendants ( node ) ) return removed
6771	def list_required ( self , type = None , service = None ) : # pylint: disable=redefined-builtin from burlap . common import ( required_system_packages , required_python_packages , required_ruby_packages , ) service = ( service or '' ) . strip ( ) . upper ( ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) packages_set = set ( ) packages = [ ] version = self . os_version for _service , satchel in self . all_other_enabled_satchels . items ( ) : _service = _service . strip ( ) . upper ( ) if service and service != _service : continue _new = [ ] if not type or type == SYSTEM : #TODO:deprecated, remove _new . extend ( required_system_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) try : _pkgs = satchel . packager_system_packages if self . verbose : print ( 'pkgs:' ) pprint ( _pkgs , indent = 4 ) for _key in [ ( version . distro , version . release ) , version . distro ] : if self . verbose : print ( 'checking key:' , _key ) if _key in _pkgs : if self . verbose : print ( 'satchel %s requires:' % satchel , _pkgs [ _key ] ) _new . extend ( _pkgs [ _key ] ) break except AttributeError : pass if not type or type == PYTHON : #TODO:deprecated, remove _new . extend ( required_python_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) try : _pkgs = satchel . packager_python_packages for _key in [ ( version . distro , version . release ) , version . distro ] : if _key in _pkgs : _new . extend ( _pkgs [ _key ] ) except AttributeError : pass print ( '_new:' , _new ) if not type or type == RUBY : #TODO:deprecated, remove _new . extend ( required_ruby_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) for _ in _new : if _ in packages_set : continue packages_set . add ( _ ) packages . append ( _ ) if self . verbose : for package in sorted ( packages ) : print ( 'package:' , package ) return packages
8194	def _density ( self ) : return 2.0 * len ( self . edges ) / ( len ( self . nodes ) * ( len ( self . nodes ) - 1 ) )
8944	def pushd ( path ) : saved = os . getcwd ( ) os . chdir ( path ) try : yield saved finally : os . chdir ( saved )
5808	def parse_tls_records ( data ) : pointer = 0 data_len = len ( data ) while pointer < data_len : # Don't try to parse any more once the ChangeCipherSpec is found if data [ pointer : pointer + 1 ] == b'\x14' : break length = int_from_bytes ( data [ pointer + 3 : pointer + 5 ] ) yield ( data [ pointer : pointer + 1 ] , data [ pointer + 1 : pointer + 3 ] , data [ pointer + 5 : pointer + 5 + length ] ) pointer += 5 + length
4831	def get_course_certificate ( self , course_id , username ) : return self . client . certificates ( username ) . courses ( course_id ) . get ( )
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
9804	def activate ( username ) : try : PolyaxonClient ( ) . user . activate_user ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not activate user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "User `{}` was activated successfully." . format ( username ) )
3425	def get_metabolite_compartments ( self ) : warn ( 'use Model.compartments instead' , DeprecationWarning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
3390	def validate ( self , samples ) : samples = np . atleast_2d ( samples ) prob = self . problem if samples . shape [ 1 ] == len ( self . model . reactions ) : S = create_stoichiometric_matrix ( self . model ) b = np . array ( [ self . model . constraints [ m . id ] . lb for m in self . model . metabolites ] ) bounds = np . array ( [ r . bounds for r in self . model . reactions ] ) . T elif samples . shape [ 1 ] == len ( self . model . variables ) : S = prob . equalities b = prob . b bounds = prob . variable_bounds else : raise ValueError ( "Wrong number of columns. samples must have a " "column for each flux or variable defined in the " "model!" ) feasibility = np . abs ( S . dot ( samples . T ) . T - b ) . max ( axis = 1 ) lb_error = ( samples - bounds [ 0 , ] ) . min ( axis = 1 ) ub_error = ( bounds [ 1 , ] - samples ) . min ( axis = 1 ) if ( samples . shape [ 1 ] == len ( self . model . variables ) and prob . inequalities . shape [ 0 ] ) : consts = prob . inequalities . dot ( samples . T ) lb_error = np . minimum ( lb_error , ( consts - prob . bounds [ 0 , ] ) . min ( axis = 1 ) ) ub_error = np . minimum ( ub_error , ( prob . bounds [ 1 , ] - consts ) . min ( axis = 1 ) ) valid = ( ( feasibility < self . feasibility_tol ) & ( lb_error > - self . bounds_tol ) & ( ub_error > - self . bounds_tol ) ) codes = np . repeat ( "" , valid . shape [ 0 ] ) . astype ( np . dtype ( ( str , 3 ) ) ) codes [ valid ] = "v" codes [ lb_error <= - self . bounds_tol ] = np . char . add ( codes [ lb_error <= - self . bounds_tol ] , "l" ) codes [ ub_error <= - self . bounds_tol ] = np . char . add ( codes [ ub_error <= - self . bounds_tol ] , "u" ) codes [ feasibility > self . feasibility_tol ] = np . char . add ( codes [ feasibility > self . feasibility_tol ] , "e" ) return codes
12559	def largest_connected_component ( volume ) : # We use asarray to be able to work with masked arrays. volume = np . asarray ( volume ) labels , num_labels = scn . label ( volume ) if not num_labels : raise ValueError ( 'No non-zero values: no connected components found.' ) if num_labels == 1 : return volume . astype ( np . bool ) label_count = np . bincount ( labels . ravel ( ) . astype ( np . int ) ) # discard the 0 label label_count [ 0 ] = 0 return labels == label_count . argmax ( )
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
13219	def shell ( self , expect = pexpect ) : dsn = self . connection_dsn ( ) log . debug ( 'connection string: %s' % dsn ) child = expect . spawn ( 'psql "%s"' % dsn ) if self . _connect_args [ 'password' ] is not None : child . expect ( 'Password: ' ) child . sendline ( self . _connect_args [ 'password' ] ) child . interact ( )
10326	def canonical_averages ( ps , microcanonical_averages_arrays ) : num_sites = microcanonical_averages_arrays [ 'N' ] num_edges = microcanonical_averages_arrays [ 'M' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_averages_arrays ) ret = dict ( ) ret [ 'ps' ] = ps ret [ 'N' ] = num_sites ret [ 'M' ] = num_edges ret [ 'max_cluster_size' ] = np . empty ( ps . size ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( ps . size , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( ps . size ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( ps . size , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , ps . size ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , ps . size , 2 ) ) for p_index , p in enumerate ( ps ) : binomials = _binomial_pmf ( n = num_edges , p = p ) for key , value in microcanonical_averages_arrays . items ( ) : if len ( key ) <= 1 : continue if key in [ 'max_cluster_size' , 'spanning_cluster' ] : ret [ key ] [ p_index ] = np . sum ( binomials * value ) elif key in [ 'max_cluster_size_ci' , 'spanning_cluster_ci' ] : ret [ key ] [ p_index ] = np . sum ( np . tile ( binomials , ( 2 , 1 ) ) . T * value , axis = 0 ) elif key == 'moments' : ret [ key ] [ : , p_index ] = np . sum ( np . tile ( binomials , ( 5 , 1 ) ) * value , axis = 1 ) elif key == 'moments_ci' : ret [ key ] [ : , p_index ] = np . sum ( np . rollaxis ( np . tile ( binomials , ( 5 , 2 , 1 ) ) , 2 , 1 ) * value , axis = 1 ) else : raise NotImplementedError ( '{}-dimensional array' . format ( value . ndim ) ) return ret
6818	def install_auth_basic_user_file ( self , site = None ) : r = self . local_renderer hostname = self . current_hostname target_sites = self . genv . available_sites_by_host . get ( hostname , None ) for _site , site_data in self . iter_sites ( site = site , setter = self . set_site_specifics ) : if self . verbose : print ( '~' * 80 , file = sys . stderr ) print ( 'Site:' , _site , file = sys . stderr ) print ( 'env.apache_auth_basic:' , r . env . auth_basic , file = sys . stderr ) # Only load site configurations that are allowed for this host. if target_sites is not None : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : continue if not r . env . auth_basic : continue assert r . env . auth_basic_users , 'No apache auth users specified.' for username , password in r . env . auth_basic_users : r . env . auth_basic_username = username r . env . auth_basic_password = password r . env . apache_site = _site r . env . fn = r . format ( r . env . auth_basic_authuserfile ) if self . files . exists ( r . env . fn ) : r . sudo ( 'htpasswd -b {fn} {auth_basic_username} {auth_basic_password}' ) else : r . sudo ( 'htpasswd -b -c {fn} {auth_basic_username} {auth_basic_password}' )
8977	def _file ( self , file ) : if not self . __text_is_expected : file = BytesWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
4085	def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
5750	def downloadURL ( url , filename ) : path_temp_bviewfile = os . path . join ( c . raw_data , c . bview_dir , 'tmp' , filename ) path_bviewfile = os . path . join ( c . raw_data , c . bview_dir , filename ) try : f = urlopen ( url ) except : return False if f . getcode ( ) != 200 : publisher . warning ( '{} unavailable, code: {}' . format ( url , f . getcode ( ) ) ) return False try : with open ( path_temp_bviewfile , 'w' ) as outfile : outfile . write ( f . read ( ) ) os . rename ( path_temp_bviewfile , path_bviewfile ) except : os . remove ( path_temp_bviewfile ) return False return True
9401	def _parse_error ( self , err ) : self . logger . debug ( err ) stack = err . get ( 'stack' , [ ] ) if not err [ 'message' ] . startswith ( 'parse error:' ) : err [ 'message' ] = 'error: ' + err [ 'message' ] errmsg = 'Octave evaluation error:\n%s' % err [ 'message' ] if not isinstance ( stack , StructArray ) : return errmsg errmsg += '\nerror: called from:' for item in stack [ : - 1 ] : errmsg += '\n %(name)s at line %(line)d' % item try : errmsg += ', column %(column)d' % item except Exception : pass return errmsg
9861	def sync_update_info ( self , * _ ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_info ( ) ) loop . run_until_complete ( task )
7729	def get_muc_child ( self ) : if self . muc_child : return self . muc_child if not self . xmlnode . children : return None n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ( n . name , ns_uri ) == ( "x" , MUC_NS ) : self . muc_child = MucX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "x" , MUC_USER_NS ) : self . muc_child = MucUserX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_ADMIN_NS ) : self . muc_child = MucAdminQuery ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_OWNER_NS ) : self . muc_child = MucOwnerX ( n ) return self . muc_child n = n . next
4422	async def seek ( self , pos : int ) : await self . _lavalink . ws . send ( op = 'seek' , guildId = self . guild_id , position = pos )
3552	def _state_changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) # Handle when powered on. if state == 5 : self . _powered_off . clear ( ) self . _powered_on . set ( ) # Handle when powered off. elif state == 4 : self . _powered_on . clear ( ) self . _powered_off . set ( )
6650	def exec_helper ( self , cmd , builddir ) : try : child = subprocess . Popen ( cmd , cwd = builddir ) child . wait ( ) except OSError as e : if e . errno == errno . ENOENT : if cmd [ 0 ] == 'cmake' : return 'CMake is not installed, please follow the installation instructions at http://docs.yottabuild.org/#installing' else : return '%s is not installed' % ( cmd [ 0 ] ) else : return 'command %s failed' % ( cmd ) if child . returncode : return 'command %s failed' % ( cmd )
8690	def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
4915	def with_access_to ( self , request , * args , * * kwargs ) : # pylint: disable=invalid-name,unused-argument self . queryset = self . queryset . order_by ( 'name' ) enterprise_id = self . request . query_params . get ( 'enterprise_id' , None ) enterprise_slug = self . request . query_params . get ( 'enterprise_slug' , None ) enterprise_name = self . request . query_params . get ( 'search' , None ) if enterprise_id is not None : self . queryset = self . queryset . filter ( uuid = enterprise_id ) elif enterprise_slug is not None : self . queryset = self . queryset . filter ( slug = enterprise_slug ) elif enterprise_name is not None : self . queryset = self . queryset . filter ( name__icontains = enterprise_name ) return self . list ( request , * args , * * kwargs )
767	def getMetrics ( self ) : result = { } for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : value = metricObj . getMetric ( ) result [ label ] = value [ 'value' ] return result
4605	def upgrade ( self ) : # pragma: no cover assert callable ( self . blockchain . upgrade_account ) return self . blockchain . upgrade_account ( account = self )
5774	def ecdsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'ec' : raise ValueError ( 'The key specified is not an EC private key' ) return _sign ( private_key , data , hash_algorithm )
659	def populationStability ( vectors , numSamples = None ) : # ---------------------------------------------------------------------- # Calculate the stability numVectors = len ( vectors ) if numSamples is None : numSamples = numVectors - 1 countOn = range ( numVectors - 1 ) else : countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) sigmap = 0.0 for i in countOn : match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) # Ignore reset vectors (all 0's) if match [ 1 ] != 0 : sigmap += float ( match [ 0 ] ) / match [ 1 ] return sigmap / numSamples
766	def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
788	def jobInfoWithModels ( self , jobID ) : # Get a database connection and cursor combinedResults = None with ConnectionFactory . get ( ) as conn : # NOTE: Since we're using a LEFT JOIN on the models table, there need not # be a matching row in the models table, but the matching row from the # jobs table will still be returned (along with all fields from the models # table with values of None in case there were no matchings models) query = ' ' . join ( [ 'SELECT %s.*, %s.*' % ( self . jobsTableName , self . modelsTableName ) , 'FROM %s' % self . jobsTableName , 'LEFT JOIN %s USING(job_id)' % self . modelsTableName , 'WHERE job_id=%s' ] ) conn . cursor . execute ( query , ( jobID , ) ) if conn . cursor . rowcount > 0 : combinedResults = [ ClientJobsDAO . _combineResults ( result , self . _jobs . jobInfoNamedTuple , self . _models . modelInfoNamedTuple ) for result in conn . cursor . fetchall ( ) ] if combinedResults is not None : return combinedResults raise RuntimeError ( "jobID=%s not found within the jobs table" % ( jobID ) )
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
11059	def run ( self , start = True ) : # Fail out if setup wasn't run if not self . is_setup : raise NotSetupError # Start the web server self . webserver . start ( ) first_connect = True try : while self . runnable : if self . reconnect_needed : if not self . sc . rtm_connect ( with_team_state = start ) : return False self . reconnect_needed = False if first_connect : first_connect = False self . plugins . connect ( ) # Get all waiting events - this always returns a list try : events = self . sc . rtm_read ( ) except AttributeError : self . log . exception ( 'Something has failed in the slack rtm library. This is fatal.' ) self . runnable = False events = [ ] except : self . log . exception ( 'Unhandled exception in rtm_read()' ) self . reconnect_needed = True events = [ ] for e in events : try : self . _handle_event ( e ) except KeyboardInterrupt : # Gracefully shutdown self . runnable = False except : self . log . exception ( 'Unhandled exception in event handler' ) sleep ( 0.1 ) except KeyboardInterrupt : # On ctrl-c, just exit pass except : self . log . exception ( 'Unhandled exception' )
5725	def write ( self , mi_cmd_to_write , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True , read_response = True , ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 # Ensure proper type of the mi command if type ( mi_cmd_to_write ) in [ str , unicode ] : pass elif type ( mi_cmd_to_write ) == list : mi_cmd_to_write = "\n" . join ( mi_cmd_to_write ) else : raise TypeError ( "The gdb mi command must a be str or list. Got " + str ( type ( mi_cmd_to_write ) ) ) self . logger . debug ( "writing: %s" , mi_cmd_to_write ) if not mi_cmd_to_write . endswith ( "\n" ) : mi_cmd_to_write_nl = mi_cmd_to_write + "\n" else : mi_cmd_to_write_nl = mi_cmd_to_write if USING_WINDOWS : # select not implemented in windows for pipes # assume it's always ready outputready = [ self . stdin_fileno ] else : _ , outputready , _ = select . select ( [ ] , self . write_list , [ ] , timeout_sec ) for fileno in outputready : if fileno == self . stdin_fileno : # ready to write self . gdb_process . stdin . write ( mi_cmd_to_write_nl . encode ( ) ) # don't forget to flush for Python3, otherwise gdb won't realize there is data # to evaluate, and we won't get a response self . gdb_process . stdin . flush ( ) else : self . logger . error ( "got unexpected fileno %d" % fileno ) if read_response is True : return self . get_gdb_response ( timeout_sec = timeout_sec , raise_error_on_timeout = raise_error_on_timeout ) else : return [ ]
4804	def when_called_with ( self , * some_args , * * some_kwargs ) : if not self . expected : raise TypeError ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some_args , * * some_kwargs ) except BaseException as e : if issubclass ( type ( e ) , self . expected ) : # chain on with exception message as val return AssertionBuilder ( str ( e ) , self . description , self . kind ) else : # got exception, but wrong type, so raise self . _err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , * * some_kwargs ) , type ( e ) . __name__ ) ) # didn't fail as expected, so raise self . _err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , * * some_kwargs ) ) )
7899	def request_configuration_form ( self ) : iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "get" ) iq . new_query ( MUC_OWNER_NS , "query" ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_form_success , self . process_configuration_form_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
10376	def calculate_concordance ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , use_ambiguous : bool = False ) -> float : correct , incorrect , ambiguous , _ = calculate_concordance_helper ( graph , key , cutoff = cutoff ) try : return correct / ( correct + incorrect + ( ambiguous if use_ambiguous else 0 ) ) except ZeroDivisionError : return - 1.0
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
7933	def _connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( "Component JID may have only domain defined" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise ValueError ( "Server or port not given" ) Stream . _connect ( self , server , port , None , self . me )
12351	def restore ( self , image , wait = True ) : return self . _action ( 'restore' , image = image , wait = wait )
9215	def t_istringapostrophe_css_string ( self , t ) : t . lexer . lineno += t . value . count ( '\n' ) return t
13897	def DumpDirHashToStringIO ( directory , stringio , base = '' , exclude = None , include = None ) : import fnmatch import os files = [ ( os . path . join ( directory , i ) , i ) for i in os . listdir ( directory ) ] files = [ i for i in files if os . path . isfile ( i [ 0 ] ) ] for fullname , filename in files : if include is not None : if not fnmatch . fnmatch ( fullname , include ) : continue if exclude is not None : if fnmatch . fnmatch ( fullname , exclude ) : continue md5 = Md5Hex ( fullname ) if base : stringio . write ( '%s/%s=%s\n' % ( base , filename , md5 ) ) else : stringio . write ( '%s=%s\n' % ( filename , md5 ) )
11747	def routes_simple ( self ) : routes = [ ] for bundle in self . _registered_bundles : bundle_path = bundle [ 'path' ] for blueprint in bundle [ 'blueprints' ] : bp_path = blueprint [ 'path' ] for child in blueprint [ 'routes' ] : routes . append ( ( child [ 'endpoint' ] , bundle_path + bp_path + child [ 'path' ] , child [ 'methods' ] ) ) return routes
3940	async def listen ( self ) : retries = 0 # Number of retries attempted so far need_new_sid = True # whether a new SID is needed while retries <= self . _max_retries : # After the first failed retry, back off exponentially longer after # each attempt. if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) # Request a new SID if we don't have one yet, or the previous one # became invalid. if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False # Clear any previous push data, since if there was an error it # could contain garbage. self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : # The connection closed successfully, so reset the number of # retries. retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) # If the request ended with an error, the client must account for # messages being dropped during this time. logger . error ( 'Ran out of retries for long-polling request' )
13536	def prune_list ( self ) : targets = self . descendents_root ( ) try : targets . remove ( self . graph . root ) except ValueError : # root wasn't in the target list, no problem pass targets . append ( self ) return targets
8908	def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
5558	def _flatten_tree ( tree , old_path = None ) : flat_tree = [ ] for key , value in tree . items ( ) : new_path = "/" . join ( [ old_path , key ] ) if old_path else key if isinstance ( value , dict ) and "format" not in value : flat_tree . extend ( _flatten_tree ( value , old_path = new_path ) ) else : flat_tree . append ( ( new_path , value ) ) return flat_tree
1535	def init_topology ( mcs , classname , class_dict ) : if classname == 'Topology' : # Base class can't initialize protobuf return heron_options = TopologyType . get_heron_options_from_env ( ) initial_state = heron_options . get ( "cmdline.topology.initial.state" , "RUNNING" ) tmp_directory = heron_options . get ( "cmdline.topologydefn.tmpdirectory" ) if tmp_directory is None : raise RuntimeError ( "Topology definition temp directory not specified" ) topology_name = heron_options . get ( "cmdline.topology.name" , classname ) topology_id = topology_name + str ( uuid . uuid4 ( ) ) # create protobuf topology = topology_pb2 . Topology ( ) topology . id = topology_id topology . name = topology_name topology . state = topology_pb2 . TopologyState . Value ( initial_state ) topology . topology_config . CopyFrom ( TopologyType . get_topology_config_protobuf ( class_dict ) ) TopologyType . add_bolts_and_spouts ( topology , class_dict ) class_dict [ 'topology_name' ] = topology_name class_dict [ 'topology_id' ] = topology_id class_dict [ 'protobuf_topology' ] = topology class_dict [ 'topologydefn_tmpdir' ] = tmp_directory class_dict [ 'heron_runtime_options' ] = heron_options
6762	def write_pgpass ( self , name = None , site = None , use_sudo = 0 , root = 0 ) : r = self . database_renderer ( name = name , site = site ) root = int ( root ) use_sudo = int ( use_sudo ) r . run ( 'touch {pgpass_path}' ) if '~' in r . env . pgpass_path : r . run ( 'chmod {pgpass_chmod} {pgpass_path}' ) else : r . sudo ( 'chmod {pgpass_chmod} {pgpass_path}' ) if root : r . env . shell_username = r . env . get ( 'db_root_username' , 'postgres' ) r . env . shell_password = r . env . get ( 'db_root_password' , 'password' ) else : r . env . shell_username = r . env . db_user r . env . shell_password = r . env . db_password r . append ( '{db_host}:{port}:*:{shell_username}:{shell_password}' , r . env . pgpass_path , use_sudo = use_sudo )
2055	def ADDW ( cpu , dest , src , add ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc if src . type == 'register' and src . reg in ( 'PC' , 'R15' ) : src = aligned_pc else : src = src . read ( ) dest . write ( src + add . read ( ) )
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
9669	def is_valid_sound ( sound , ts ) : if isinstance ( sound , ( Marker , UnknownSound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
3947	def decode ( message , pblite , ignore_first_item = False ) : if not isinstance ( pblite , list ) : logger . warning ( 'Ignoring invalid message: expected list, got %r' , type ( pblite ) ) return if ignore_first_item : pblite = pblite [ 1 : ] # If the last item of the list is a dict, use it as additional field/value # mappings. This seems to be an optimization added for dealing with really # high field numbers. if pblite and isinstance ( pblite [ - 1 ] , dict ) : extra_fields = { int ( field_number ) : value for field_number , value in pblite [ - 1 ] . items ( ) } pblite = pblite [ : - 1 ] else : extra_fields = { } fields_values = itertools . chain ( enumerate ( pblite , start = 1 ) , extra_fields . items ( ) ) for field_number , value in fields_values : if value is None : continue try : field = message . DESCRIPTOR . fields_by_number [ field_number ] except KeyError : # If the tag number is unknown and the value is non-trivial, log a # message to aid reverse-engineering the missing field in the # message. if value not in [ [ ] , '' , 0 ] : logger . debug ( 'Message %r contains unknown field %s with value ' '%r' , message . __class__ . __name__ , field_number , value ) continue if field . label == FieldDescriptor . LABEL_REPEATED : _decode_repeated_field ( message , field , value ) else : _decode_field ( message , field , value )
5682	def get_closest_stop ( self , lat , lon ) : cur = self . conn . cursor ( ) min_dist = float ( "inf" ) min_stop_I = None rows = cur . execute ( "SELECT stop_I, lat, lon FROM stops" ) for stop_I , lat_s , lon_s in rows : dist_now = wgs84_distance ( lat , lon , lat_s , lon_s ) if dist_now < min_dist : min_dist = dist_now min_stop_I = stop_I return min_stop_I
7065	def delete_ec2_nodes ( instance_id_list , client = None ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . terminate_instances ( InstanceIds = instance_id_list ) return resp
1991	def ls ( self , glob_str ) : path = os . path . join ( self . uri , glob_str ) return [ os . path . split ( s ) [ 1 ] for s in glob . glob ( path ) ]
717	def __getHyperSearchJobIDFilePath ( cls , permWorkDir , outputLabel ) : # Get the base path and figure out the path of the report file. basePath = permWorkDir # Form the name of the output csv file that will contain all the results filename = "%s_HyperSearchJobID.pkl" % ( outputLabel , ) filepath = os . path . join ( basePath , filename ) return filepath
2137	def create ( self , fail_on_found = False , force_on_exists = False , * * kwargs ) : if kwargs . get ( 'parent' , None ) : parent_data = self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs [ 'inventory' ] = parent_data [ 'inventory' ] elif 'inventory' not in kwargs : raise exc . UsageError ( 'To create a group, you must provide a parent inventory or parent group.' ) return super ( Resource , self ) . create ( fail_on_found = fail_on_found , force_on_exists = force_on_exists , * * kwargs )
9785	def bookmark ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : PolyaxonClient ( ) . build_job . bookmark ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job bookmarked." )
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : ## an array to fill with consensus site calls cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) ## iterate over columns for col in xrange ( arr . shape [ 1 ] ) : ## the site of focus carr = arr [ : , col ] ## make mask of N and - sites mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] ## skip if only empties (e.g., N-) if not marr . shape [ 0 ] : cons [ col ] = 78 ## skip if not variable elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] ## estimate variable site call else : ## get allele freqs (first-most, second, third = p, q, r) counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] ## based on biallelic depth bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : ## if depth is too high, reduce to sampled int if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq ## make statistical base call if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) #LOGGER.info("ishet, prob, b1, b2: %s %s %s %s", ishet, prob, base1, base2) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase ## make majrule base call else : #if bidepth >= mindepth_majrule: if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
5539	def read ( self , * * kwargs ) : if self . tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( self . tile . bounds , self . tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( self . tile ) return self . config . output . extract_subset ( input_data_tiles = [ ( output_tile , self . config . output . read ( output_tile ) ) for output_tile in output_tiles ] , out_tile = self . tile , )
9974	def get_mro ( self , space ) : seqs = [ self . get_mro ( base ) for base in self . get_bases ( space ) ] + [ list ( self . get_bases ( space ) ) ] res = [ ] while True : non_empty = list ( filter ( None , seqs ) ) if not non_empty : # Nothing left to process, we're done. res . insert ( 0 , space ) return res for seq in non_empty : # Find merge candidates among seq heads. candidate = seq [ 0 ] not_head = [ s for s in non_empty if candidate in s [ 1 : ] ] if not_head : # Reject the candidate. candidate = None else : break if not candidate : raise TypeError ( "inconsistent hierarchy, no C3 MRO is possible" ) res . append ( candidate ) for seq in non_empty : # Remove candidate. if seq [ 0 ] == candidate : del seq [ 0 ]
10881	def delistify ( a , b = None ) : if isinstance ( b , ( tuple , list , np . ndarray ) ) : if isinstance ( a , ( tuple , list , np . ndarray ) ) : return type ( b ) ( a ) return type ( b ) ( [ a ] ) else : if isinstance ( a , ( tuple , list , np . ndarray ) ) and len ( a ) == 1 : return a [ 0 ] return a return a
7292	def set_post_data ( self ) : self . form . data = self . post_data_dict # Specifically adding list field keys to the form so they are included # in form.cleaned_data after the call to is_valid for field_key , field in self . form . fields . items ( ) : if has_digit ( field_key ) : # We have a list field. base_key = make_key ( field_key , exclude_last_string = True ) # Add new key value with field to form fields so validation # will work correctly for key in self . post_data_dict . keys ( ) : if base_key in key : self . form . fields . update ( { key : field } )
8268	def color ( self , clr = None , d = 0.035 ) : # Revert to grayscale for black, white and grey hues. if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
407	def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )
8914	def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
11061	def send_message ( self , channel , text , thread = None , reply_broadcast = None ) : # This doesn't want the # in the channel name if isinstance ( channel , SlackRoomIMBase ) : channel = channel . id self . log . debug ( "Trying to send to %s: %s" , channel , text ) self . sc . rtm_send_message ( channel , text , thread = thread , reply_broadcast = reply_broadcast )
4566	def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
145	def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )
60	def union ( self , other ) : return BoundingBox ( x1 = min ( self . x1 , other . x1 ) , y1 = min ( self . y1 , other . y1 ) , x2 = max ( self . x2 , other . x2 ) , y2 = max ( self . y2 , other . y2 ) , )
703	def _getStreamDef ( self , modelDescription ) : #-------------------------------------------------------------------------- # Generate the string containing the aggregation settings. aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } # Honor any overrides provided in the stream definition aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) # Do we have any aggregation at all? hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break # Convert the aggFunctionsDict to a list aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
8335	def findPreviousSibling ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findPreviousSiblings , name , attrs , text , * * kwargs )
12043	def algo_exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : # no-op layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
7361	async def _connect ( self ) : logger . debug ( "connecting to the stream" ) await self . client . setup if self . session is None : self . session = self . client . _session kwargs = await self . client . headers . prepare_request ( * * self . kwargs ) request = self . client . error_handler ( self . session . request ) return await request ( timeout = 0 , * * kwargs )
5816	def _read_remaining ( socket ) : output = b'' old_timeout = socket . gettimeout ( ) try : socket . settimeout ( 0.0 ) output += socket . recv ( 8192 ) except ( socket_ . error ) : pass finally : socket . settimeout ( old_timeout ) return output
21	def boolean_flag ( parser , name , default = False , help = None ) : dest = name . replace ( '-' , '_' ) parser . add_argument ( "--" + name , action = "store_true" , default = default , dest = dest , help = help ) parser . add_argument ( "--no-" + name , action = "store_false" , dest = dest )
4227	def _check_old_config_root ( ) : # disable the check - once is enough and avoids infinite loop globals ( ) [ '_check_old_config_root' ] = lambda : None config_file_new = os . path . join ( _config_root_Linux ( ) , 'keyringrc.cfg' ) config_file_old = os . path . join ( _data_root_Linux ( ) , 'keyringrc.cfg' ) if os . path . isfile ( config_file_old ) and not os . path . isfile ( config_file_new ) : msg = ( "Keyring config exists only in the old location " "{config_file_old} and should be moved to {config_file_new} " "to work with this version of keyring." ) raise RuntimeError ( msg . format ( * * locals ( ) ) )
10852	def otsu_threshold ( data , bins = 255 ) : h0 , x0 = np . histogram ( data . ravel ( ) , bins = bins ) h = h0 . astype ( 'float' ) / h0 . sum ( ) #normalize x = 0.5 * ( x0 [ 1 : ] + x0 [ : - 1 ] ) #bin center wk = np . array ( [ h [ : i + 1 ] . sum ( ) for i in range ( h . size ) ] ) #omega_k mk = np . array ( [ sum ( x [ : i + 1 ] * h [ : i + 1 ] ) for i in range ( h . size ) ] ) #mu_k mt = mk [ - 1 ] #mu_T sb = ( mt * wk - mk ) ** 2 / ( wk * ( 1 - wk ) + 1e-15 ) #sigma_b ind = sb . argmax ( ) return 0.5 * ( x0 [ ind ] + x0 [ ind + 1 ] )
12508	def get_3D_coordmap ( img ) : if isinstance ( img , nib . Nifti1Image ) : img = nifti2nipy ( img ) if img . ndim == 4 : from nipy . core . reference . coordinate_map import drop_io_dim cm = drop_io_dim ( img . coordmap , 3 ) else : cm = img . coordmap return cm
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
5177	def resources ( self , type_ = None , title = None , * * kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) return resources
4500	def _follow_next ( self , url ) : response = self . _json ( self . _get ( url ) , 200 ) data = response [ 'data' ] next_url = self . _get_attribute ( response , 'links' , 'next' ) while next_url is not None : response = self . _json ( self . _get ( next_url ) , 200 ) data . extend ( response [ 'data' ] ) next_url = self . _get_attribute ( response , 'links' , 'next' ) return data
8334	def findAllPrevious ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousGenerator , * * kwargs )
5497	def discover ( cls , * args , * * kwargs ) : file = os . path . join ( Cache . cache_dir , Cache . cache_name ) return cls . from_file ( file , * args , * * kwargs )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
5201	def Operate ( self , command , index , op_type ) : OutstationApplication . process_point_value ( 'Operate' , command , index , op_type ) return opendnp3 . CommandStatus . SUCCESS
6258	def update ( self , aspect_ratio = None , fov = None , near = None , far = None ) : self . aspect_ratio = aspect_ratio or self . aspect_ratio self . fov = fov or self . fov self . near = near or self . near self . far = far or self . far self . matrix = Matrix44 . perspective_projection ( self . fov , self . aspect_ratio , self . near , self . far )
6896	def pdw_worker ( task ) : frequency = task [ 0 ] times , modmags = task [ 1 ] , task [ 2 ] fold_time = task [ 3 ] j_range = range ( task [ 4 ] ) keep_threshold_1 = task [ 5 ] keep_threshold_2 = task [ 6 ] phasebinsize = task [ 7 ] try : period = 1.0 / frequency # use the common phaser to phase and sort the mag phased = phase_magseries ( times , modmags , period , fold_time , wrap = False , sort = True ) # bin in phase if requested, this turns this into a sort of PDM method if phasebinsize is not None and phasebinsize > 0 : bphased = pwd_phasebin ( phased [ 'phase' ] , phased [ 'mags' ] , binsize = phasebinsize ) phase_sorted = bphased [ 0 ] mod_mag_sorted = bphased [ 1 ] j_range = range ( len ( mod_mag_sorted ) - 1 ) else : phase_sorted = phased [ 'phase' ] mod_mag_sorted = phased [ 'mags' ] # now calculate the string length rolledmags = nproll ( mod_mag_sorted , 1 ) rolledphases = nproll ( phase_sorted , 1 ) strings = ( ( rolledmags - mod_mag_sorted ) * ( rolledmags - mod_mag_sorted ) + ( rolledphases - phase_sorted ) * ( rolledphases - phase_sorted ) ) strings [ 0 ] = ( ( ( mod_mag_sorted [ 0 ] - mod_mag_sorted [ - 1 ] ) * ( mod_mag_sorted [ 0 ] - mod_mag_sorted [ - 1 ] ) ) + ( ( phase_sorted [ 0 ] - phase_sorted [ - 1 ] + 1 ) * ( phase_sorted [ 0 ] - phase_sorted [ - 1 ] + 1 ) ) ) strlen = npsum ( npsqrt ( strings ) ) if ( keep_threshold_1 < strlen < keep_threshold_2 ) : p_goodflag = True else : p_goodflag = False return ( period , strlen , p_goodflag ) except Exception as e : LOGEXCEPTION ( 'error in DWP' ) return ( period , npnan , False )
9549	def validate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , limit = 0 , context = None , report_unexpected_exceptions = True ) : problems = list ( ) problem_generator = self . ivalidate ( data , expect_header_row , ignore_lines , summarize , context , report_unexpected_exceptions ) for i , p in enumerate ( problem_generator ) : if not limit or i < limit : problems . append ( p ) return problems
2902	def complete_next ( self , pick_up = True , halt_on_manual = True ) : # Try to pick up where we left off. blacklist = [ ] if pick_up and self . last_task is not None : try : iter = Task . Iterator ( self . last_task , Task . READY ) task = next ( iter ) except StopIteration : task = None self . last_task = None if task is not None : if not ( halt_on_manual and task . task_spec . manual ) : if task . complete ( ) : self . last_task = task return True blacklist . append ( task ) # Walk through all ready tasks. for task in Task . Iterator ( self . task_tree , Task . READY ) : for blacklisted_task in blacklist : if task . _is_descendant_of ( blacklisted_task ) : continue if not ( halt_on_manual and task . task_spec . manual ) : if task . complete ( ) : self . last_task = task return True blacklist . append ( task ) # Walk through all waiting tasks. for task in Task . Iterator ( self . task_tree , Task . WAITING ) : task . task_spec . _update ( task ) if not task . _has_state ( Task . WAITING ) : self . last_task = task return True return False
3782	def set_user_methods_P ( self , user_methods_P , forced_P = False ) : # Accept either a string or a list of methods, and whether # or not to only consider the false methods if isinstance ( user_methods_P , str ) : user_methods_P = [ user_methods_P ] # The user's order matters and is retained for use by select_valid_methods self . user_methods_P = user_methods_P self . forced_P = forced_P # Validate that the user's specified methods are actual methods if set ( self . user_methods_P ) . difference ( self . all_methods_P ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods_P and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) # Remove previously selected methods self . method_P = None self . sorted_valid_methods_P = [ ] self . TP_cached = None
13784	def get_tm_session ( session_factory , transaction_manager ) : dbsession = session_factory ( ) zope . sqlalchemy . register ( dbsession , transaction_manager = transaction_manager ) return dbsession
10188	def publish ( self , event_type , events ) : assert event_type in self . events current_queues . queues [ 'stats-{}' . format ( event_type ) ] . publish ( events )
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) # PIL/asarray returns read only array if not img_np . flags [ "WRITEABLE" ] : try : # this seems to no longer work with np 1.16 (or was pillow updated?) img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
971	def _copyAllocatedStates ( self ) : # Get learn states if we need to print them out if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )
9750	def find_x ( path1 ) : libs = os . listdir ( path1 ) for lib_dir in libs : if "doublefann" in lib_dir : return True
6446	def _cond_bb ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 and word [ - suffix_len - 3 : - suffix_len ] != 'met' and word [ - suffix_len - 4 : - suffix_len ] != 'ryst' )
10011	def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
1510	def start_cluster ( cl_args ) : roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] zookeepers = roles [ Role . ZOOKEEPERS ] Log . info ( "Roles:" ) Log . info ( " - Master Servers: %s" % list ( masters ) ) Log . info ( " - Slave Servers: %s" % list ( slaves ) ) Log . info ( " - Zookeeper Servers: %s" % list ( zookeepers ) ) if not masters : Log . error ( "No master servers specified!" ) sys . exit ( - 1 ) if not slaves : Log . error ( "No slave servers specified!" ) sys . exit ( - 1 ) if not zookeepers : Log . error ( "No zookeeper servers specified!" ) sys . exit ( - 1 ) # make sure configs are templated update_config_files ( cl_args ) dist_nodes = list ( masters . union ( slaves ) ) # if just local deployment if not ( len ( dist_nodes ) == 1 and is_self ( dist_nodes [ 0 ] ) ) : distribute_package ( roles , cl_args ) start_master_nodes ( masters , cl_args ) start_slave_nodes ( slaves , cl_args ) start_api_server ( masters , cl_args ) start_heron_tools ( masters , cl_args ) Log . info ( "Heron standalone cluster complete!" )
2547	def add_annotation_type ( self , doc , annotation_type ) : if len ( doc . annotations ) != 0 : if not self . annotation_type_set : if annotation_type . endswith ( 'annotationType_other' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'OTHER' return True elif annotation_type . endswith ( 'annotationType_review' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'REVIEW' return True else : raise SPDXValueError ( 'Annotation::AnnotationType' ) else : raise CardinalityError ( 'Annotation::AnnotationType' ) else : raise OrderError ( 'Annotation::AnnotationType' )
2016	def _store ( self , offset , value , size = 1 ) : self . memory . write_BE ( offset , value , size ) for i in range ( size ) : self . _publish ( 'did_evm_write_memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
1953	def input_from_cons ( constupl , datas ) : def make_chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints_to_constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make_chr ( solver . get_value ( newset , c ) ) return ret
4399	def adsSyncSetTimeoutEx ( port , nMs ) : # type: (int, int) -> None adsSyncSetTimeoutFct = _adsDLL . AdsSyncSetTimeoutEx cms = ctypes . c_long ( nMs ) err_code = adsSyncSetTimeoutFct ( port , cms ) if err_code : raise ADSError ( err_code )
8430	def cmap_d_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) if not isinstance ( colormap , mcolors . ListedColormap ) : raise ValueError ( "For a discrete palette, cmap must be of type " "matplotlib.colors.ListedColormap" ) ncolors = len ( colormap . colors ) def _cmap_d_pal ( n ) : if n > ncolors : raise ValueError ( "cmap `{}` has {} colors you requested {} " "colors." . format ( name , ncolors , n ) ) if ncolors < 256 : return [ mcolors . rgb2hex ( c ) for c in colormap . colors [ : n ] ] else : # Assume these are continuous and get colors equally spaced # intervals e.g. viridis is defined with 256 colors idx = np . linspace ( 0 , ncolors - 1 , n ) . round ( ) . astype ( int ) return [ mcolors . rgb2hex ( colormap . colors [ i ] ) for i in idx ] return _cmap_d_pal
1906	def strlen ( state , s ) : cpu = state . cpu if issymbolic ( s ) : raise ConcretizeArgument ( state . cpu , 1 ) zero_idx = _find_zero ( cpu , state . constraints , s ) ret = zero_idx for offset in range ( zero_idx - 1 , - 1 , - 1 ) : byt = cpu . read_int ( s + offset , 8 ) if issymbolic ( byt ) : ret = ITEBV ( cpu . address_bit_size , byt == 0 , offset , ret ) return ret
13590	def sigma_prime ( self ) : return _np . sqrt ( self . emit / self . beta ( self . E ) )
878	def newPosition ( self , whichVars = None ) : # TODO: incorporate data from choice variables.... # TODO: make sure we're calling this when appropriate. # Get the global best position for this swarm generation globalBestPosition = None # If speculative particles are enabled, use the global best considering # even particles in the current generation. This gives better results # but does not provide repeatable results because it depends on # worker timing if self . _hsObj . _speculativeParticles : genIdx = self . genIdx else : genIdx = self . genIdx - 1 if genIdx >= 0 : ( bestModelId , _ ) = self . _resultsDB . bestModelIdAndErrScore ( self . swarmId , genIdx ) if bestModelId is not None : ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( bestModelId ) globalBestPosition = Particle . getPositionFromState ( particleState ) # Update each variable for ( varName , var ) in self . permuteVars . iteritems ( ) : if whichVars is not None and varName not in whichVars : continue if globalBestPosition is None : var . newPosition ( None , self . _rng ) else : var . newPosition ( globalBestPosition [ varName ] , self . _rng ) # get the new position position = self . getPosition ( ) # Log the new position if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : msg = StringIO . StringIO ( ) print >> msg , "New particle position: \n%s" % ( pprint . pformat ( position , indent = 4 ) ) print >> msg , "Particle variables:" for ( varName , var ) in self . permuteVars . iteritems ( ) : print >> msg , " %s: %s" % ( varName , str ( var ) ) self . logger . debug ( msg . getvalue ( ) ) msg . close ( ) return position
5919	def center_fit ( self , * * kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , '_centfit' , 'xtc' ) ) ) force = kwargs . pop ( 'force' , self . force ) logger . info ( "Centering and fitting trajectory {f!r}..." . format ( * * kwargs ) ) with utilities . in_dir ( self . dirname ) : if not self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : trj_fitandcenter ( * * kwargs ) logger . info ( "Centered and fit trajectory: {o!r}." . format ( * * kwargs ) ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
2569	def construct_end_message ( self ) : app_count = self . dfk . task_count site_count = len ( [ x for x in self . dfk . config . executors if x . managed ] ) app_fails = len ( [ t for t in self . dfk . tasks if self . dfk . tasks [ t ] [ 'status' ] in FINAL_FAILURE_STATES ] ) message = { 'uuid' : self . uuid , 'end' : time . time ( ) , 't_apps' : app_count , 'sites' : site_count , 'c_time' : None , 'failed' : app_fails , 'test' : self . test_mode , } return json . dumps ( message )
1253	def print_state ( self ) : def tile_string ( value ) : """Concert value to string.""" if value > 0 : return '% 5d' % ( 2 ** value , ) return " " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
5405	def _build_user_environment ( self , envs , inputs , outputs , mounts ) : envs = { env . name : env . value for env in envs } envs . update ( providers_util . get_file_environment_variables ( inputs ) ) envs . update ( providers_util . get_file_environment_variables ( outputs ) ) envs . update ( providers_util . get_file_environment_variables ( mounts ) ) return envs
9456	def cancel_scheduled_play ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledPlay/' method = 'POST' return self . request ( path , method , call_params )
3312	def do_MKCOL ( self , environ , start_response ) : path = environ [ "PATH_INFO" ] provider = self . _davProvider # res = provider.get_resource_inst(path, environ) # Do not understand ANY request body entities if util . get_content_length ( environ ) != 0 : self . _fail ( HTTP_MEDIATYPE_NOT_SUPPORTED , "The server does not handle any body content." , ) # Only accept Depth: 0 (but assume this, if omitted) if environ . setdefault ( "HTTP_DEPTH" , "0" ) != "0" : self . _fail ( HTTP_BAD_REQUEST , "Depth must be '0'." ) if provider . exists ( path , environ ) : self . _fail ( HTTP_METHOD_NOT_ALLOWED , "MKCOL can only be executed on an unmapped URL." , ) parentRes = provider . get_resource_inst ( util . get_uri_parent ( path ) , environ ) if not parentRes or not parentRes . is_collection : self . _fail ( HTTP_CONFLICT , "Parent must be an existing collection." ) # TODO: should we check If headers here? # self._evaluate_if_headers(res, environ) # Check for write permissions on the PARENT self . _check_write_permission ( parentRes , "0" , environ ) parentRes . create_collection ( util . get_uri_name ( path ) ) return util . send_status_response ( environ , start_response , HTTP_CREATED )
9023	def write ( self , bytes_ ) : string = bytes_ . decode ( self . _encoding ) self . _file . write ( string )
10455	def check ( self , window_name , object_name ) : # FIXME: Check for object type object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) if object_handle . AXValue == 1 : # Already checked return 1 # AXPress doesn't work with Instruments # So did the following work around self . _grabfocus ( object_handle ) x , y , width , height = self . _getobjectsize ( object_handle ) # Mouse left click on the object # Note: x + width/2, y + height / 2 doesn't work self . generatemouseevent ( x + width / 2 , y + height / 2 , "b1c" ) return 1
8449	def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
9725	async def set_qtm_event ( self , event = None ) : cmd = "event%s" % ( "" if event is None else " " + event ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
6192	def _generate ( num_particles , D , box , rs ) : X0 = rs . rand ( num_particles ) * ( box . x2 - box . x1 ) + box . x1 Y0 = rs . rand ( num_particles ) * ( box . y2 - box . y1 ) + box . y1 Z0 = rs . rand ( num_particles ) * ( box . z2 - box . z1 ) + box . z1 return [ Particle ( D = D , x0 = x0 , y0 = y0 , z0 = z0 ) for x0 , y0 , z0 in zip ( X0 , Y0 , Z0 ) ]
11505	def move_folder ( self , token , folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.folder.move' , parameters ) return response
4179	def window_bartlett_hann ( N ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) a0 = 0.62 a1 = 0.48 a2 = 0.38 win = a0 - a1 * abs ( n / ( N - 1. ) - 0.5 ) - a2 * cos ( 2 * pi * n / ( N - 1. ) ) return win
6625	def availableTags ( self ) : return [ GithubComponentVersion ( '' , t [ 0 ] , t [ 1 ] , self . name , cache_key = _createCacheKey ( 'tag' , t [ 0 ] , t [ 1 ] , self . name ) ) for t in self . _getTags ( ) ]
3397	def extend_model ( self , exchange_reactions = False , demand_reactions = True ) : for rxn in self . universal . reactions : rxn . gapfilling_type = 'universal' new_metabolites = self . universal . metabolites . query ( lambda metabolite : metabolite not in self . model . metabolites ) self . model . add_metabolites ( new_metabolites ) existing_exchanges = [ ] for rxn in self . universal . boundary : existing_exchanges = existing_exchanges + [ met . id for met in list ( rxn . metabolites ) ] for met in self . model . metabolites : if exchange_reactions : # check for exchange reaction in model already if met . id not in existing_exchanges : rxn = self . universal . add_boundary ( met , type = 'exchange_smiley' , lb = - 1000 , ub = 0 , reaction_id = 'EX_{}' . format ( met . id ) ) rxn . gapfilling_type = 'exchange' if demand_reactions : rxn = self . universal . add_boundary ( met , type = 'demand_smiley' , lb = 0 , ub = 1000 , reaction_id = 'DM_{}' . format ( met . id ) ) rxn . gapfilling_type = 'demand' new_reactions = self . universal . reactions . query ( lambda reaction : reaction not in self . model . reactions ) self . model . add_reactions ( new_reactions )
13871	def CanonicalPath ( path ) : path = os . path . normpath ( path ) path = os . path . abspath ( path ) path = os . path . normcase ( path ) return path
6024	def convolve ( self , array ) : if self . shape [ 0 ] % 2 == 0 or self . shape [ 1 ] % 2 == 0 : raise exc . KernelException ( "PSF Kernel must be odd" ) return scipy . signal . convolve2d ( array , self , mode = 'same' )
4109	def mexican ( lb , ub , n ) : if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = ( 1. - x ** 2. ) * ( 2. / ( numpy . sqrt ( 3. ) * pi ** 0.25 ) ) * numpy . exp ( - x ** 2 / 2. ) return psi
12055	def ftp_login ( folder = None ) : pwDir = os . path . realpath ( __file__ ) for i in range ( 3 ) : pwDir = os . path . dirname ( pwDir ) pwFile = os . path . join ( pwDir , "passwd.txt" ) print ( " -- looking for login information in:\n [%s]" % pwFile ) try : with open ( pwFile ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK_askPassword ( "FTP LOGIN" , "enter FTP username" ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( " username:" , username ) print ( " password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
452	def get_layers_with_name ( net , name = "" , verbose = False ) : logging . info ( " [*] geting layers with %s" % name ) layers = [ ] i = 0 for layer in net . all_layers : # logging.info(type(layer.name)) if name in layer . name : layers . append ( layer ) if verbose : logging . info ( " got {:3}: {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) ) ) i = i + 1 return layers
1097	def decode ( in_file , out_file = None , mode = None , quiet = 0 ) : # # Open the input file, if needed. # opened_files = [ ] if in_file == '-' : in_file = sys . stdin elif isinstance ( in_file , basestring ) : in_file = open ( in_file ) opened_files . append ( in_file ) try : # # Read until a begin is encountered or we've exhausted the file # while True : hdr = in_file . readline ( ) if not hdr : raise Error ( 'No valid begin line found in input file' ) if not hdr . startswith ( 'begin' ) : continue hdrfields = hdr . split ( ' ' , 2 ) if len ( hdrfields ) == 3 and hdrfields [ 0 ] == 'begin' : try : int ( hdrfields [ 1 ] , 8 ) break except ValueError : pass if out_file is None : out_file = hdrfields [ 2 ] . rstrip ( ) if os . path . exists ( out_file ) : raise Error ( 'Cannot overwrite existing file: %s' % out_file ) if mode is None : mode = int ( hdrfields [ 1 ] , 8 ) # # Open the output file # if out_file == '-' : out_file = sys . stdout elif isinstance ( out_file , basestring ) : fp = open ( out_file , 'wb' ) try : os . path . chmod ( out_file , mode ) except AttributeError : pass out_file = fp opened_files . append ( out_file ) # # Main decoding loop # s = in_file . readline ( ) while s and s . strip ( ) != 'end' : try : data = binascii . a2b_uu ( s ) except binascii . Error , v : # Workaround for broken uuencoders by /Fredrik Lundh nbytes = ( ( ( ord ( s [ 0 ] ) - 32 ) & 63 ) * 4 + 5 ) // 3 data = binascii . a2b_uu ( s [ : nbytes ] ) if not quiet : sys . stderr . write ( "Warning: %s\n" % v ) out_file . write ( data ) s = in_file . readline ( ) if not s : raise Error ( 'Truncated input file' ) finally : for f in opened_files : f . close ( )
10426	def enrich_internal_unqualified_edges ( graph , subgraph ) : for u , v in itt . combinations ( subgraph , 2 ) : if not graph . has_edge ( u , v ) : continue for k in graph [ u ] [ v ] : if k < 0 : subgraph . add_edge ( u , v , key = k , * * graph [ u ] [ v ] [ k ] )
13243	def weekdays ( self ) : if not self . root . xpath ( 'days' ) : return set ( range ( 7 ) ) return set ( int ( d ) - 1 for d in self . root . xpath ( 'days/day/text()' ) )
1558	def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
8657	def _clean ( zipcode , valid_length = _valid_zipcode_length ) : zipcode = zipcode . split ( "-" ) [ 0 ] # Convert #####-#### to ##### if len ( zipcode ) != valid_length : raise ValueError ( 'Invalid format, zipcode must be of the format: "#####" or "#####-####"' ) if _contains_nondigits ( zipcode ) : raise ValueError ( 'Invalid characters, zipcode may only contain digits and "-".' ) return zipcode
5785	def _raw_read ( self ) : data = self . _raw_bytes try : data += self . _socket . recv ( 8192 ) except ( socket_ . error ) : pass output = data written = libssl . BIO_write ( self . _rbio , data , len ( data ) ) self . _raw_bytes = data [ written : ] return output
9001	def build_SVG_dict ( self ) : zoom = self . _zoom layout = self . _layout builder = self . _builder bbox = list ( map ( lambda f : f * zoom , layout . bounding_box ) ) builder . bounding_box = bbox flip_x = bbox [ 2 ] + bbox [ 0 ] * 2 flip_y = bbox [ 3 ] + bbox [ 1 ] * 2 instructions = list ( layout . walk_instructions ( lambda i : ( flip_x - ( i . x + i . width ) * zoom , flip_y - ( i . y + i . height ) * zoom , i . instruction ) ) ) instructions . sort ( key = lambda x_y_i : x_y_i [ 2 ] . render_z ) for x , y , instruction in instructions : render_z = instruction . render_z z_id = ( "" if not render_z else "-{}" . format ( render_z ) ) layer_id = "row-{}{}" . format ( instruction . row . id , z_id ) def_id = self . _register_instruction_in_defs ( instruction ) scale = self . _symbol_id_to_scale [ def_id ] group = { "@class" : "instruction" , "@id" : "instruction-{}" . format ( instruction . id ) , "@transform" : "translate({},{}),scale({})" . format ( x , y , scale ) } builder . place_svg_use ( def_id , layer_id , group ) builder . insert_defs ( self . _instruction_type_color_to_symbol . values ( ) ) return builder . get_svg_dict ( )
1143	def _bytelist2longBigEndian ( list ) : imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl
9550	def ivalidate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , context = None , report_unexpected_exceptions = True ) : unique_sets = self . _init_unique_sets ( ) # used for unique checks for i , r in enumerate ( data ) : if expect_header_row and i == ignore_lines : # r is the header row for p in self . _apply_header_checks ( i , r , summarize , context ) : yield p elif i >= ignore_lines : # r is a data row skip = False for p in self . _apply_skips ( i , r , summarize , report_unexpected_exceptions , context ) : if p is True : skip = True else : yield p if not skip : for p in self . _apply_each_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p # may yield a problem if an exception is raised for p in self . _apply_value_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_length_checks ( i , r , summarize , context ) : yield p for p in self . _apply_value_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_unique_checks ( i , r , unique_sets , summarize ) : yield p for p in self . _apply_check_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_assert_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_finally_assert_methods ( summarize , report_unexpected_exceptions , context ) : yield p
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
645	def generateCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : coincMatrix0 = SM32 ( int ( nCoinc ) , int ( length ) ) theOnes = numpy . array ( [ 1.0 ] * activity , dtype = numpy . float32 ) for rowIdx in xrange ( nCoinc ) : coinc = numpy . array ( random . sample ( xrange ( length ) , activity ) , dtype = numpy . uint32 ) coinc . sort ( ) coincMatrix0 . setRowFromSparse ( rowIdx , coinc , theOnes ) # This is the right code to use, it's faster, but it derails the unit # testing of the pooling for now. coincMatrix = SM32 ( int ( nCoinc ) , int ( length ) ) coincMatrix . initializeWithFixedNNZR ( activity ) return coincMatrix0
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : # value is a list pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) # try to convert elements to int for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : # value is another dictionary subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
5183	def node ( self , name ) : nodes = self . nodes ( path = name ) return next ( node for node in nodes )
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : # ENTER-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : if self . method : # retest within range if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : # pragma: no cover pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
13251	async def process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) github_url = ltd_product_data [ 'doc_repo' ] github_url = normalize_repo_root_url ( github_url ) repo_slug = parse_repo_slug_from_url ( github_url ) try : metadata_yaml = await download_metadata_yaml ( session , github_url ) except aiohttp . ClientResponseError as err : # metadata.yaml not found; probably not a Sphinx technote logger . debug ( 'Tried to download %s\'s metadata.yaml, got status %d' , ltd_product_data [ 'slug' ] , err . code ) raise NotSphinxTechnoteError ( ) # Extract data from the GitHub API github_query = GitHubQuery . load ( 'technote_repo' ) github_variables = { "orgName" : repo_slug . owner , "repoName" : repo_slug . repo } github_data = await github_request ( session , github_api_token , query = github_query , variables = github_variables ) try : jsonld = reduce_technote_metadata ( github_url , metadata_yaml , github_data , ltd_product_data ) except Exception as exception : message = "Issue building JSON-LD for technote %s" logger . exception ( message , github_url , exception ) raise if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , jsonld ) logger . info ( 'Ingested technote %s into MongoDB' , github_url ) return jsonld
13240	def includes ( self , query_date , query_time = None ) : if self . start_date and query_date < self . start_date : return False if self . end_date and query_date > self . end_date : return False if query_date . weekday ( ) not in self . weekdays : return False if not query_time : return True if query_time >= self . period . start and query_time <= self . period . end : return True return False
6334	def dist ( self , src , tar ) : if src == tar : return 0.0 return self . dist_abs ( src , tar ) / ( len ( src ) + len ( tar ) )
13358	def aot40_vegetation ( df , nb_an ) : return _aot ( df . tshift ( 1 ) , nb_an = nb_an , limite = 80 , mois_debut = 5 , mois_fin = 7 , heure_debut = 8 , heure_fin = 19 )
630	def binSearch ( arr , val ) : i = bisect_left ( arr , val ) if i != len ( arr ) and arr [ i ] == val : return i return - 1
9300	def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , "expected filters type 'dict'" assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" # start with our base query query = self . get_query ( ) assert isinstance ( query , peewee . Query ) # XXX: convert and apply user specified filters #filters = {field.name: cursor[field.name] for field in fields} #query.where( paginator = self . get_paginator ( ) assert isinstance ( paginator , Pagination ) # always include an extra row for next cursor position count += 1 # apply pagination to query pquery = paginator . filter_query ( query , cursor , count ) items = [ item for item in pquery ] # determine next cursor position next_item = items . pop ( 1 ) next_cursor = next_item . to_cursor_ref ( ) ''' # is this field allowed for sort? if field not in self.sort_fields: raise ValueError("Cannot sort on field '{}'".format(field)) ''' return items , next_cursor
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
9682	def set_fan_power ( self , power ) : # Check to make sure the value is a single byte if power > 255 : raise ValueError ( "The fan power should be a single byte (0-255)." ) # Send the command byte and wait 10 ms a = self . cnxn . xfer ( [ 0x42 ] ) [ 0 ] sleep ( 10e-3 ) # Send the next two bytes b = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] c = self . cnxn . xfer ( [ power ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x42 and c == 0x00 else False
13189	async def _upload_to_mongodb ( collection , jsonld ) : document = { 'data' : jsonld } query = { 'data.reportNumber' : jsonld [ 'reportNumber' ] } await collection . update ( query , document , upsert = True , multi = False )
3532	def mixpanel ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MixpanelNode ( )
4532	def clone ( self ) : args = { k : getattr ( self , k ) for k in self . CLONE_ATTRS } args [ 'color_list' ] = copy . copy ( self . color_list ) return self . __class__ ( [ ] , * * args )
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
13558	def get_all_images_count ( self ) : self_imgs = self . image_set . count ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) . count ( ) count = self_imgs + u_images return count
1082	def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . __new__ ( type ( self ) , hour , minute , second , microsecond , tzinfo )
13746	def create_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) now = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) attrs = { 'created_on' : now , 'modified_on' : now , 'count' : start , } if extra_attrs : attrs . update ( extra_attrs ) item = table . new_item ( hash_key = hash_key , attrs = attrs , ) return item
1747	def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
9240	def fetch_events_for_issues_and_pr ( self ) : # Async fetching events: self . fetcher . fetch_events_async ( self . issues , "issues" ) self . fetcher . fetch_events_async ( self . pull_requests , "pull requests" )
5646	def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make_axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . ColorbarBase ( cax , cmap = cmap , norm = norm ) return c
5510	def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum_value : raise ValueError ( "Too many releases" )
509	def stripUnlearnedColumns ( self , activeArray ) : neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] activeArray [ neverLearned ] = 0
6079	def intensities_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . intensities_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
11492	def login_with_api_key ( self , email , api_key , application = 'Default' ) : parameters = dict ( ) parameters [ 'email' ] = BaseDriver . email = email # Cache email parameters [ 'apikey' ] = BaseDriver . apikey = api_key # Cache API key parameters [ 'appname' ] = application response = self . request ( 'midas.login' , parameters ) if 'token' in response : # normal case return response [ 'token' ] if 'mfa_token_id' : # case with multi-factor authentication return response [ 'mfa_token_id' ]
13435	def cut ( self , line ) : result = [ ] line = self . line ( line ) for i , field in enumerate ( self . positions ) : try : index = _setup_index ( field ) try : result += line [ index ] except IndexError : result . append ( self . invalid_pos ) except ValueError : result . append ( str ( field ) ) except TypeError : result . extend ( self . _cut_range ( line , int ( field [ 0 ] ) , i ) ) return '' . join ( result )
12718	def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
3714	def calculate ( self , T , method ) : if method == CRC_INORG_S : Vms = self . CRC_INORG_S_Vm # elif method == GOODMAN: # Vms = Goodman(T, self.Tt, self.rhol_Tt) elif method in self . tabular_data : Vms = self . interpolate ( T , method ) return Vms
11856	def extender ( self , edge ) : ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
8179	def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
7743	def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( "_prepar_io_handler_cb called for {0!r}" . format ( handler ) ) self . _configure_io_handler ( handler ) self . _prepare_sources . pop ( handler , None ) return False
13434	def _setup_index ( index ) : index = int ( index ) if index > 0 : index -= 1 elif index == 0 : # Zero indicies should not be allowed by default. raise ValueError return index
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
3930	def _get_authorization_code ( session , credentials_prompt ) : browser = Browser ( session , OAUTH2_LOGIN_URL ) email = credentials_prompt . get_email ( ) browser . submit_form ( FORM_SELECTOR , { EMAIL_SELECTOR : email } ) password = credentials_prompt . get_password ( ) browser . submit_form ( FORM_SELECTOR , { PASSWORD_SELECTOR : password } ) if browser . has_selector ( TOTP_CHALLENGE_SELECTOR ) : browser . submit_form ( TOTP_CHALLENGE_SELECTOR , { } ) elif browser . has_selector ( PHONE_CHALLENGE_SELECTOR ) : browser . submit_form ( PHONE_CHALLENGE_SELECTOR , { } ) if browser . has_selector ( VERIFICATION_FORM_SELECTOR ) : if browser . has_selector ( TOTP_CODE_SELECTOR ) : input_selector = TOTP_CODE_SELECTOR elif browser . has_selector ( PHONE_CODE_SELECTOR ) : input_selector = PHONE_CODE_SELECTOR else : raise GoogleAuthError ( 'Unknown verification code input' ) verfification_code = credentials_prompt . get_verification_code ( ) browser . submit_form ( VERIFICATION_FORM_SELECTOR , { input_selector : verfification_code } ) try : return browser . get_cookie ( 'oauth_code' ) except KeyError : raise GoogleAuthError ( 'Authorization code cookie not found' )
2984	def set_cors_headers ( resp , options ) : # If CORS has already been evaluated via the decorator, skip if hasattr ( resp , FLASK_CORS_EVALUATED ) : LOG . debug ( 'CORS have been already evaluated, skipping' ) return resp # Some libraries, like OAuthlib, set resp.headers to non Multidict # objects (Werkzeug Headers work as well). This is a problem because # headers allow repeated values. if ( not isinstance ( resp . headers , Headers ) and not isinstance ( resp . headers , MultiDict ) ) : resp . headers = MultiDict ( resp . headers ) headers_to_set = get_cors_headers ( options , request . headers , request . method ) LOG . debug ( 'Settings CORS headers: %s' , str ( headers_to_set ) ) for k , v in headers_to_set . items ( ) : resp . headers . add ( k , v ) return resp
9890	def _boottime_linux ( ) : global __boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : __boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise NotImplementedError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime ) except ( IOError , IndexError ) : return None
7256	def get_strip_metadata ( self , catID ) : self . logger . debug ( 'Retrieving strip catalog metadata' ) url = '%(base_url)s/record/%(catID)s?includeRelationships=false' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) if r . status_code == 200 : return r . json ( ) [ 'properties' ] elif r . status_code == 404 : self . logger . debug ( 'Strip not found: %s' % catID ) r . raise_for_status ( ) else : self . logger . debug ( 'There was a problem retrieving catid: %s' % catID ) r . raise_for_status ( )
4504	def put_edit ( self , f , * args , * * kwds ) : self . put_nowait ( functools . partial ( f , * args , * * kwds ) )
8672	def list_keys ( key_name , max_suggestions , cutoff , jsonify , locked , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase , quiet = jsonify ) try : keys = stash . list ( key_name = key_name , max_suggestions = max_suggestions , cutoff = cutoff , locked_only = locked , key_type = key_type ) except GhostError as ex : sys . exit ( ex ) if jsonify : click . echo ( json . dumps ( keys , indent = 4 , sort_keys = True ) ) elif not keys : click . echo ( 'The stash is empty. Go on, put some keys in there...' ) else : click . echo ( 'Listing all keys...' ) click . echo ( _prettify_list ( keys ) )
9048	def gradient ( self ) : grad = { } for i , f in enumerate ( self . _covariances ) : for varname , g in f . gradient ( ) . items ( ) : grad [ f"{self._name}[{i}].{varname}" ] = g return grad
2156	def launch ( self , monitor = False , wait = False , timeout = None , * * kwargs ) : # This feature only exists for versions 2.2 and up r = client . get ( '/' ) if 'ad_hoc_commands' not in r . json ( ) : raise exc . TowerCLIError ( 'Your host is running an outdated version' 'of Ansible Tower that can not run ' 'ad-hoc commands (2.2 or earlier)' ) # Pop the None arguments because we have no .write() method in # inheritance chain for this type of resource. This is needed self . _pop_none ( kwargs ) # Actually start the command. debug . log ( 'Launching the ad-hoc command.' , header = 'details' ) result = client . post ( self . endpoint , data = kwargs ) command = result . json ( ) command_id = command [ 'id' ] # If we were told to monitor the command once it started, then call # monitor from here. if monitor : return self . monitor ( command_id , timeout = timeout ) elif wait : return self . wait ( command_id , timeout = timeout ) # Return the command ID and other response data answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , command_id ) , ) ) answer . update ( result . json ( ) ) return answer
1113	def _split_line ( self , data_list , line_num , text ) : # if blank line or context separator, just add it to the output list if not line_num : data_list . append ( ( line_num , text ) ) return # if line text doesn't need wrapping, just add it to the output list size = len ( text ) max = self . _wrapcolumn if ( size <= max ) or ( ( size - ( text . count ( '\0' ) * 3 ) ) <= max ) : data_list . append ( ( line_num , text ) ) return # scan text looking for the wrap point, keeping track if the wrap # point is inside markers i = 0 n = 0 mark = '' while n < max and i < size : if text [ i ] == '\0' : i += 1 mark = text [ i ] i += 1 elif text [ i ] == '\1' : i += 1 mark = '' else : i += 1 n += 1 # wrap point is inside text, break it up into separate lines line1 = text [ : i ] line2 = text [ i : ] # if wrap point is inside markers, place end marker at end of first # line and start marker at beginning of second line because each # line will have its own table tag markup around it. if mark : line1 = line1 + '\1' line2 = '\0' + mark + line2 # tack on first line onto the output list data_list . append ( ( line_num , line1 ) ) # use this routine again to wrap the remaining text self . _split_line ( data_list , '>' , line2 )
8779	def _try_allocate ( self , context , segment_id , network_id ) : LOG . info ( "Attempting to allocate segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) ) filter_dict = { "segment_id" : segment_id , "segment_type" : self . segment_type , "do_not_use" : False } available_ranges = db_api . segment_allocation_range_find ( context , scope = db_api . ALL , * * filter_dict ) available_range_ids = [ r [ "id" ] for r in available_ranges ] try : with context . session . begin ( subtransactions = True ) : # Search for any deallocated segment ids for the # given segment. filter_dict = { "deallocated" : True , "segment_id" : segment_id , "segment_type" : self . segment_type , "segment_allocation_range_ids" : available_range_ids } # NOTE(morgabra) We select 100 deallocated segment ids from # the table here, and then choose 1 randomly. This is to help # alleviate the case where an uncaught exception might leave # an allocation active on a remote service but we do not have # a record of it locally. If we *do* end up choosing a # conflicted id, the caller should simply allocate another one # and mark them all as reserved. If a single object has # multiple reservations on the same segment, they will not be # deallocated, and the operator must resolve the conficts # manually. allocations = db_api . segment_allocation_find ( context , lock_mode = True , * * filter_dict ) . limit ( 100 ) . all ( ) if allocations : allocation = random . choice ( allocations ) # Allocate the chosen segment. update_dict = { "deallocated" : False , "deallocated_at" : None , "network_id" : network_id } allocation = db_api . segment_allocation_update ( context , allocation , * * update_dict ) LOG . info ( "Allocated segment %s for network %s " "segment_id %s segment_type %s" % ( allocation [ "id" ] , network_id , segment_id , self . segment_type ) ) return allocation except Exception : LOG . exception ( "Error in segment reallocation." ) LOG . info ( "Cannot find reallocatable segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) )
7462	def encode ( self , obj ) : def hint_tuples ( item ) : """ embeds __tuple__ hinter in json strings """ if isinstance ( item , tuple ) : return { '__tuple__' : True , 'items' : item } if isinstance ( item , list ) : return [ hint_tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint_tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint_tuples ( obj ) )
3821	async def delete_conversation ( self , delete_conversation_request ) : response = hangouts_pb2 . DeleteConversationResponse ( ) await self . _pb_request ( 'conversations/deleteconversation' , delete_conversation_request , response ) return response
9617	def PlugIn ( self ) : ids = self . available_ids ( ) if len ( ids ) == 0 : raise MaxInputsReachedError ( 'Max Inputs Reached' ) self . id = ids [ 0 ] _xinput . PlugIn ( self . id ) while self . id in self . available_ids ( ) : pass
9233	def fetch_commit ( self , event ) : gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ event [ "commit_id" ] ] . get ( ) if rc == 200 : return data self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
2700	def text_rank ( path ) : graph = build_graph ( json_iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks
7688	def pitch_contour ( annotation , sr = 22050 , length = None , * * kwargs ) : # Map contours to lists of observations times = defaultdict ( list ) freqs = defaultdict ( list ) for obs in annotation : times [ obs . value [ 'index' ] ] . append ( obs . time ) freqs [ obs . value [ 'index' ] ] . append ( obs . value [ 'frequency' ] * ( - 1 ) ** ( ~ obs . value [ 'voiced' ] ) ) y_out = 0.0 for ix in times : y_out = y_out + filter_kwargs ( mir_eval . sonify . pitch_contour , np . asarray ( times [ ix ] ) , np . asarray ( freqs [ ix ] ) , fs = sr , length = length , * * kwargs ) if length is None : length = len ( y_out ) return y_out
10864	def _update_type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . _p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
11497	def create_community ( self , token , name , * * kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name optional_keys = [ 'description' , 'uuid' , 'privacy' , 'can_join' ] for key in optional_keys : if key in kwargs : if key == 'can_join' : parameters [ 'canjoin' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.community.create' , parameters ) return response
9509	def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
1164	def start ( self ) : if not self . __initialized : raise RuntimeError ( "thread.__init__() not called" ) if self . __started . is_set ( ) : raise RuntimeError ( "threads can only be started once" ) if __debug__ : self . _note ( "%s.start(): starting thread" , self ) with _active_limbo_lock : _limbo [ self ] = self try : _start_new_thread ( self . __bootstrap , ( ) ) except Exception : with _active_limbo_lock : del _limbo [ self ] raise self . __started . wait ( )
9226	def post_parse ( self ) : if self . result : out = [ ] for pu in self . result : try : out . append ( pu . parse ( self . scope ) ) except SyntaxError as e : self . handle_error ( e , 0 ) self . result = list ( utility . flatten ( out ) )
12134	def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )
863	def Enum ( * args , * * kwargs ) : def getLabel ( cls , val ) : """ Get a string label for the current value of the enum """ return cls . __labels [ val ] def validate ( cls , val ) : """ Returns True if val is a valid value for the enumeration """ return val in cls . __values def getValues ( cls ) : """ Returns a list of all the possible values for this enum """ return list ( cls . __values ) def getLabels ( cls ) : """ Returns a list of all possible labels for this enum """ return list ( cls . __labels . values ( ) ) def getValue ( cls , label ) : """ Returns value given a label """ return cls . __labels [ label ] for arg in list ( args ) + kwargs . keys ( ) : if type ( arg ) is not str : raise TypeError ( "Enum arg {0} must be a string" . format ( arg ) ) if not __isidentifier ( arg ) : raise ValueError ( "Invalid enum value '{0}'. " "'{0}' is not a valid identifier" . format ( arg ) ) #kwargs.update(zip(args, range(len(args)))) kwargs . update ( zip ( args , args ) ) newType = type ( "Enum" , ( object , ) , kwargs ) newType . __labels = dict ( ( v , k ) for k , v in kwargs . iteritems ( ) ) newType . __values = set ( newType . __labels . keys ( ) ) newType . getLabel = functools . partial ( getLabel , newType ) newType . validate = functools . partial ( validate , newType ) newType . getValues = functools . partial ( getValues , newType ) newType . getLabels = functools . partial ( getLabels , newType ) newType . getValue = functools . partial ( getValue , newType ) return newType
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key # Style list fields if ListField in ( field_value . field_type , field_type ) : # Nested lists/embedded docs need special care to get # styles to work out nicely. if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' # Compute number value for list key list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : # Get the base key for our embedded field class valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) # We need to remove the trailing number from the key # so that grouping will occur on the front end when we have a list. embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" # Setting the embedded key correctly allows to visually nest the # embedded documents on the front end. if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) # Style embedded documents if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : # Note, this is copied every time so each widget gets a different class list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
2900	def get_tasks ( self , state = Task . ANY_MASK ) : return [ t for t in Task . Iterator ( self . task_tree , state ) ]
5193	def send_select_and_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command_set , callback , config )
4611	def block_time ( self , block_num ) : return self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( )
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) # Press the non-modifier key self . _sendKey ( keychr , modFlags , globally = globally ) # Release the modifiers self . _releaseModifiers ( modifiers , globally = globally ) # Post the queued keypresses: self . _postQueuedEvents ( )
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
6915	def collection_worker ( task ) : lcfile , outdir , kwargs = task try : fakelcresults = make_fakelc ( lcfile , outdir , * * kwargs ) return fakelcresults except Exception as e : LOGEXCEPTION ( 'could not process %s into a fakelc' % lcfile ) return None
6996	def spline_fit_magseries ( times , mags , errs , period , knotfraction = 0.01 , maxknots = 30 , sigclip = 30.0 , plotfit = False , ignoreinitfail = False , magsarefluxes = False , verbose = True ) : # this is required to fit the spline correctly if errs is None : errs = npfull_like ( mags , 0.005 ) # sigclip the magnitude time series stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip , magsarefluxes = magsarefluxes ) # get rid of zero errs nzind = npnonzero ( serrs ) stimes , smags , serrs = stimes [ nzind ] , smags [ nzind ] , serrs [ nzind ] # phase the mag series phase , pmags , perrs , ptimes , mintime = ( get_phased_quantities ( stimes , smags , serrs , period ) ) # now figure out the number of knots up to max knots (=100) nobs = len ( phase ) nknots = int ( npfloor ( knotfraction * nobs ) ) nknots = maxknots if nknots > maxknots else nknots splineknots = nplinspace ( phase [ 0 ] + 0.01 , phase [ - 1 ] - 0.01 , num = nknots ) # NOTE: newer scipy needs x to be strictly increasing. this means we should # filter out anything that doesn't have np.diff(phase) > 0.0 # FIXME: this needs to be tested phase_diffs_ind = npdiff ( phase ) > 0.0 incphase_ind = npconcatenate ( ( nparray ( [ True ] ) , phase_diffs_ind ) ) phase , pmags , perrs = ( phase [ incphase_ind ] , pmags [ incphase_ind ] , perrs [ incphase_ind ] ) # generate and fit the spline spl = LSQUnivariateSpline ( phase , pmags , t = splineknots , w = 1.0 / perrs ) # calculate the spline fit to the actual phases, the chisq and red-chisq fitmags = spl ( phase ) fitchisq = npsum ( ( ( fitmags - pmags ) * ( fitmags - pmags ) ) / ( perrs * perrs ) ) fitredchisq = fitchisq / ( len ( pmags ) - nknots - 1 ) if verbose : LOGINFO ( 'spline fit done. nknots = %s, ' 'chisq = %.5f, reduced chisq = %.5f' % ( nknots , fitchisq , fitredchisq ) ) # figure out the time of light curve minimum (i.e. the fit epoch) # this is when the fit mag is maximum (i.e. the faintest) # or if magsarefluxes = True, then this is when fit flux is minimum if not magsarefluxes : fitmagminind = npwhere ( fitmags == npmax ( fitmags ) ) else : fitmagminind = npwhere ( fitmags == npmin ( fitmags ) ) if len ( fitmagminind [ 0 ] ) > 1 : fitmagminind = ( fitmagminind [ 0 ] [ 0 ] , ) magseriesepoch = ptimes [ fitmagminind ] # assemble the returndict returndict = { 'fittype' : 'spline' , 'fitinfo' : { 'nknots' : nknots , 'fitmags' : fitmags , 'fitepoch' : magseriesepoch } , 'fitchisq' : fitchisq , 'fitredchisq' : fitredchisq , 'fitplotfile' : None , 'magseries' : { 'times' : ptimes , 'phase' : phase , 'mags' : pmags , 'errs' : perrs , 'magsarefluxes' : magsarefluxes } , } # make the fit plot if required if plotfit and isinstance ( plotfit , str ) : make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = magsarefluxes ) returndict [ 'fitplotfile' ] = plotfit return returndict
10236	def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
623	def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index
7141	def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . accounts [ 0 ] . transfer_multiple ( destinations , priority = priority , payment_id = payment_id , unlock_time = unlock_time , relay = relay )
9990	def get_object ( self , name ) : parts = name . split ( "." ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get_object ( "." . join ( parts ) ) else : return self . _namespace_impl [ child ]
13003	def modify_data ( data ) : with tempfile . NamedTemporaryFile ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to_dict ( include_meta = True ) , default = datetime_handler ) ) f . write ( '\n' ) f . flush ( ) print_success ( "Starting editor" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )
4497	def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
6048	def relocated_grid_stack_from_grid_stack ( self , grid_stack ) : border_grid = grid_stack . regular [ self ] return GridStack ( regular = self . relocated_grid_from_grid_jit ( grid = grid_stack . regular , border_grid = border_grid ) , sub = self . relocated_grid_from_grid_jit ( grid = grid_stack . sub , border_grid = border_grid ) , blurring = None , pix = self . relocated_grid_from_grid_jit ( grid = grid_stack . pix , border_grid = border_grid ) )
5825	def _patch ( self , route , data , headers = None , failure_message = None ) : headers = self . _get_headers ( headers ) response_lambda = ( lambda : requests . patch ( self . _get_qualified_route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check_for_rate_limiting ( response_lambda ( ) , response_lambda ) return self . _handle_response ( response , failure_message )
9144	def drop ( connection , skip ) : for idx , name , manager in _iterate_managers ( connection , skip ) : click . secho ( f'dropping {name}' , fg = 'cyan' , bold = True ) manager . drop_all ( )
8333	def findPrevious ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findAllPrevious , name , attrs , text , * * kwargs )
1736	def parse_num ( source , start , charset ) : while start < len ( source ) and source [ start ] in charset : start += 1 return start
8579	def delete_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'DELETE' ) return response
5189	def main ( ) : # app = MyMaster() app = MyMaster ( log_handler = MyLogger ( ) , listener = AppChannelListener ( ) , soe_handler = SOEHandler ( ) , master_application = MasterApplication ( ) ) _log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be performed at this point. See master_cmd.py for examples. app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
8775	def _discover_via_entrypoints ( self ) : emgr = extension . ExtensionManager ( PLUGIN_EP , invoke_on_load = False ) return ( ( ext . name , ext . plugin ) for ext in emgr )
5070	def format_price ( price , currency = '$' ) : if int ( price ) == price : return '{}{}' . format ( currency , int ( price ) ) return '{}{:0.2f}' . format ( currency , price )
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
6505	def add_properties ( self ) : for property_name in [ p [ 0 ] for p in inspect . getmembers ( self . __class__ ) if isinstance ( p [ 1 ] , property ) ] : self . _results_fields [ property_name ] = getattr ( self , property_name , None )
6723	def get_or_create_ec2_key_pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm_ec2_keypair_name pem_path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get_ec2_connection ( ) kp = conn . get_key_pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : # Note, we only get the private key during creation. # If we don't save it here, it's lost forever. kp = conn . create_key_pair ( name ) open ( pem_path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem_path ) print ( 'Key pair %s created.' % name ) #return kp return pem_path
13178	def cache_func ( prefix , method = False ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : cache_args = args if method : cache_args = args [ 1 : ] cache_key = get_cache_key ( prefix , * cache_args , * * kwargs ) cached_value = cache . get ( cache_key ) if cached_value is None : cached_value = func ( * args , * * kwargs ) cache . set ( cache_key , cached_value ) return cached_value return wrapper return decorator
8301	def dispatch ( self , message , source = None ) : msgtype = "" try : if type ( message [ 0 ] ) == str : # got a single message address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except KeyError , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except IndexError , e : print '%s: %s' % ( e , message ) pass except None , e : print "Exception in" , address , "callback :" , e return
4021	def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
4154	def add_markdown_cell ( self , text ) : markdown_cell = { "cell_type" : "markdown" , "metadata" : { } , "source" : [ rst2md ( text ) ] } self . work_notebook [ "cells" ] . append ( markdown_cell )
235	def compute_cap_exposures ( positions , caps ) : long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) tot_gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) tot_long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) tot_short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) for bucket_name , boundaries in CAP_BUCKETS . items ( ) : in_bucket = positions_wo_cash [ ( caps >= boundaries [ 0 ] ) & ( caps <= boundaries [ 1 ] ) ] gross_bucket = in_bucket . abs ( ) . sum ( axis = 'columns' ) . divide ( tot_gross_exposure ) long_bucket = in_bucket [ in_bucket > 0 ] . sum ( axis = 'columns' ) . divide ( tot_long_exposure ) short_bucket = in_bucket [ in_bucket < 0 ] . sum ( axis = 'columns' ) . divide ( tot_short_exposure ) net_bucket = long_bucket . subtract ( short_bucket ) gross_exposures . append ( gross_bucket ) long_exposures . append ( long_bucket ) short_exposures . append ( short_bucket ) net_exposures . append ( net_bucket ) return long_exposures , short_exposures , gross_exposures , net_exposures
12629	def recursive_glob ( base_directory , regex = '' ) : files = glob ( op . join ( base_directory , regex ) ) for path , dirlist , filelist in os . walk ( base_directory ) : for dir_name in dirlist : files . extend ( glob ( op . join ( path , dir_name , regex ) ) ) return files
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
12424	def loads ( s , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : if isinstance ( s , six . text_type ) : io = StringIO ( s ) else : io = BytesIO ( s ) return load ( fp = io , separator = separator , index_separator = index_separator , cls = cls , list_cls = list_cls , )
5771	def _advapi32_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : algo = certificate_or_public_key . algorithm if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) decrypted_signature = raw_rsa_public_crypt ( certificate_or_public_key , signature ) key_size = certificate_or_public_key . bit_size if not verify_pss_padding ( hash_algorithm , hash_length , key_size , data , decrypted_signature ) : raise SignatureError ( 'Signature is invalid' ) return if algo == 'rsa' and hash_algorithm == 'raw' : padded_plaintext = raw_rsa_public_crypt ( certificate_or_public_key , signature ) try : plaintext = remove_pkcs1v15_signature_padding ( certificate_or_public_key . byte_size , padded_plaintext ) if not constant_compare ( plaintext , data ) : raise ValueError ( ) except ( ValueError ) : raise SignatureError ( 'Signature is invalid' ) return hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( certificate_or_public_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) if algo == 'dsa' : # Windows doesn't use the ASN.1 Sequence for DSA signatures, # so we have to convert it here for the verification to work try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) # Switch the two integers so that the reversal later will # result in the correct order half_len = len ( signature ) // 2 signature = signature [ half_len : ] + signature [ : half_len ] except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) # The CryptoAPI expects signatures to be in little endian byte order, # which is the opposite of other systems, so we must reverse it reversed_signature = signature [ : : - 1 ] res = advapi32 . CryptVerifySignatureW ( hash_handle , reversed_signature , len ( signature ) , certificate_or_public_key . key_handle , null ( ) , 0 ) handle_error ( res ) finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
715	def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : jobID = hyperSearchJob . getJobID ( ) filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) if os . path . exists ( filePath ) : _backupFile ( filePath ) d = dict ( hyperSearchJobID = jobID ) with open ( filePath , "wb" ) as jobIdPickleFile : pickle . dump ( d , jobIdPickleFile )
461	def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) # return [random.randint(min,max) for p in range(0, number)] return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]
7981	def auth_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise LegacyAuthenticationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
1855	def BSF ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , 0 , 1 ) == 1 res = 0 for pos in range ( 1 , src . size ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , Operators . EXTRACT ( value , pos , 1 ) == 1 ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
6264	def quad_2d ( width , height , xpos = 0.0 , ypos = 0.0 ) -> VAO : pos = numpy . array ( [ xpos - width / 2.0 , ypos + height / 2.0 , 0.0 , xpos - width / 2.0 , ypos - height / 2.0 , 0.0 , xpos + width / 2.0 , ypos - height / 2.0 , 0.0 , xpos - width / 2.0 , ypos + height / 2.0 , 0.0 , xpos + width / 2.0 , ypos - height / 2.0 , 0.0 , xpos + width / 2.0 , ypos + height / 2.0 , 0.0 , ] , dtype = numpy . float32 ) normals = numpy . array ( [ 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , ] , dtype = numpy . float32 ) uvs = numpy . array ( [ 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 1.0 , 1.0 , 0.0 , 1.0 , 1.0 , ] , dtype = numpy . float32 ) vao = VAO ( "geometry:quad" , mode = moderngl . TRIANGLES ) vao . buffer ( pos , '3f' , [ "in_position" ] ) vao . buffer ( normals , '3f' , [ "in_normal" ] ) vao . buffer ( uvs , '2f' , [ "in_uv" ] ) return vao
6134	def from_file ( cls , fpath , position = 1 , file_id = None ) : if file_id is None : file_id = fpath with open ( fpath ) as f : code = f . read ( ) file_content = str ( code ) file_metadata = FileMetadata ( file_id , position ) return cls ( file_metadata , file_content )
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) # TODO: ensure unicode data works correctly for python2 args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) # Do vertical padding arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] # Initialize output all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : # Concatenate the new string for lx , line in enumerate ( lines ) : all_lines [ lx ] += line # Find the new maximum horizontal width width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : # Horizontal padding on all but last iter for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) # Clean up trailing whitespace all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
5482	def retry_auth_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in HTTP_AUTH_ERROR_CODES : _print_error ( 'Retrying...' ) return True return False
3986	def update_hosts_file_from_port_spec ( port_spec ) : logging . info ( 'Updating hosts file to match port spec' ) hosts_specs = port_spec [ 'hosts_file' ] current_hosts = config_file . read ( constants . HOSTS_PATH ) cleared_hosts = config_file . remove_current_dusty_config ( current_hosts ) updated_hosts = cleared_hosts + _dusty_hosts_config ( hosts_specs ) config_file . write ( constants . HOSTS_PATH , updated_hosts )
11111	def synchronize ( self , verbose = False ) : if self . __path is None : return # walk directories for dirPath in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , dirPath ) # if directory exist if os . path . isdir ( realPath ) : continue if verbose : warnings . warn ( "%s directory is missing" % realPath ) # loop to get dirInfoDict keys = dirPath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break # remove dirInfoDict directory if existing if dirInfoDict is not None : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is not None : dict . pop ( dirs , keys [ - 1 ] , None ) # walk files for filePath in sorted ( list ( self . walk_files_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , filePath ) # if file exists if os . path . isfile ( realPath ) : continue if verbose : warnings . warn ( "%s file is missing" % realPath ) # loop to get dirInfoDict keys = filePath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break # remove dirInfoDict file if existing if dirInfoDict is not None : files = dict . get ( dirInfoDict , 'files' , None ) if files is not None : dict . pop ( files , keys [ - 1 ] , None )
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
4069	def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) # dict keys of allowed operators for the current condition permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) # transform these into values permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
10633	def get_compound_afrs ( self ) : result = self . _compound_mfrs * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
6663	def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
1592	def add ( self , stream_id , task_ids , grouping , source_comp_name ) : if stream_id not in self . targets : self . targets [ stream_id ] = [ ] self . targets [ stream_id ] . append ( Target ( task_ids , grouping , source_comp_name ) )
597	def _compute ( self , inputs , outputs ) : #if self.topDownMode and (not 'topDownIn' in inputs): # raise RuntimeError("The input topDownIn must be linked in if " # "topDownMode is True") if self . _tfdr is None : raise RuntimeError ( "TM has not been initialized" ) # Conditional compute break self . _conditionalBreak ( ) self . _iterations += 1 # Get our inputs as numpy array buInputVector = inputs [ 'bottomUpIn' ] # Handle reset signal resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 # Position within the current sequence if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] # Perform inference and/or learning tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 # OR'ing together the cells in each column? if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) # Direct logging of non-zero TM outputs if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = " " . join ( [ "%d" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr # Write the bottom up out to our node outputs outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : # Top-down compute outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) # Set output for use with anomaly classification region if in anomalyMode if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : # Reshape so we are dealing with 1D arrays activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ "activeCells" ] . fill ( 0 ) outputs [ "activeCells" ] [ activeIndices ] = 1 outputs [ "predictedActiveCells" ] . fill ( 0 ) outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1
9998	def cellsiter_to_dataframe ( cellsiter , args , drop_allna = True ) : from modelx . core . cells import shareable_parameters if len ( args ) : indexes = shareable_parameters ( cellsiter ) else : indexes = get_all_params ( cellsiter . values ( ) ) result = None for cells in cellsiter . values ( ) : df = cells_to_dataframe ( cells , args ) if drop_allna and df . isnull ( ) . all ( ) . all ( ) : continue # Ignore all NA or empty if df . index . names != [ None ] : if isinstance ( df . index , pd . MultiIndex ) : if _pd_ver < ( 0 , 20 ) : df = _reset_naindex ( df ) df = df . reset_index ( ) missing_params = set ( indexes ) - set ( df ) for params in missing_params : df [ params ] = np . nan if result is None : result = df else : try : result = pd . merge ( result , df , how = "outer" ) except MergeError : # When no common column exists, i.e. all cells are scalars. result = pd . concat ( [ result , df ] , axis = 1 ) except ValueError : # When common columns are not coercible (numeric vs object), # Make the numeric column object type cols = set ( result . columns ) & set ( df . columns ) for col in cols : # When only either of them has object dtype if ( len ( [ str ( frame [ col ] . dtype ) for frame in ( result , df ) if str ( frame [ col ] . dtype ) == "object" ] ) == 1 ) : if str ( result [ col ] . dtype ) == "object" : frame = df else : frame = result frame [ [ col ] ] = frame [ col ] . astype ( "object" ) # Try again result = pd . merge ( result , df , how = "outer" ) if result is None : return pd . DataFrame ( ) else : return result . set_index ( indexes ) if indexes else result
9445	def group_call ( self , call_params ) : path = '/' + self . api_version + '/GroupCall/' method = 'POST' return self . request ( path , method , call_params )
13804	def _generate_token ( self , length = 32 ) : return '' . join ( choice ( ascii_letters + digits ) for x in range ( length ) )
6220	def create ( self ) : dtype = NP_COMPONENT_DTYPE [ self . component_type . value ] data = numpy . frombuffer ( self . buffer . read ( byte_length = self . byte_length , byte_offset = self . byte_offset ) , count = self . count * self . components , dtype = dtype , ) return dtype , data
265	def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )
6010	def load_poisson_noise_map ( poisson_noise_map_path , poisson_noise_map_hdu , pixel_scale , convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image , image , exposure_time_map , convert_from_electrons , gain , convert_from_adus ) : poisson_noise_map_options = sum ( [ convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image ] ) if poisson_noise_map_options == 0 and poisson_noise_map_path is not None : return PoissonNoiseMap . from_fits_with_pixel_scale ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale ) elif poisson_noise_map_from_image : if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if a' 'gain is not supplied to convert from adus' ) return PoissonNoiseMap . from_image_and_exposure_time_map ( pixel_scale = pixel_scale , image = image , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) elif convert_poisson_noise_map_from_weight_map and poisson_noise_map_path is not None : weight_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_poisson_noise_map_from_inverse_noise_map and poisson_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : # Ensure a site is selected. self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
1056	def _slotnames ( cls ) : # Get the value from a cache in the class if possible names = cls . __dict__ . get ( "__slotnames__" ) if names is not None : return names # Not cached -- calculate the value names = [ ] if not hasattr ( cls , "__slots__" ) : # This class has no slots pass else : # Slots found -- gather slot names from all base classes for c in cls . __mro__ : if "__slots__" in c . __dict__ : slots = c . __dict__ [ '__slots__' ] # if class has a single slot, it can be given as a string if isinstance ( slots , basestring ) : slots = ( slots , ) for name in slots : # special descriptors if name in ( "__dict__" , "__weakref__" ) : continue # mangled names elif name . startswith ( '__' ) and not name . endswith ( '__' ) : names . append ( '_%s%s' % ( c . __name__ , name ) ) else : names . append ( name ) # Cache the outcome in the class if at all possible try : cls . __slotnames__ = names except : pass # But don't die if we can't return names
12919	def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
6038	def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
1970	def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
1578	def make_shell_logfile_data_url ( host , shell_port , instance_id , offset , length ) : return "http://%s:%d/filedata/log-files/%s.log.0?offset=%s&length=%s" % ( host , shell_port , instance_id , offset , length )
11771	def AIMAFile ( components , mode = 'r' ) : import utils dir = os . path . dirname ( utils . __file__ ) return open ( apply ( os . path . join , [ dir ] + components ) , mode )
6750	def register ( self ) : self . _set_defaults ( ) all_satchels [ self . name . upper ( ) ] = self manifest_recorder [ self . name ] = self . record_manifest # Register service commands. if self . required_system_packages : required_system_packages [ self . name . upper ( ) ] = self . required_system_packages
9192	def _get_file_sha1 ( file ) : bits = file . read ( ) file . seek ( 0 ) h = hashlib . new ( 'sha1' , bits ) . hexdigest ( ) return h
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
1974	def sys_allocate ( self , cpu , length , isX , addr ) : # TODO: check 4 bytes from addr if addr not in cpu . memory : logger . info ( "ALLOCATE: addr points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT perms = [ 'rw ' , 'rwx' ] [ bool ( isX ) ] try : result = cpu . memory . mmap ( None , length , perms ) except Exception as e : logger . info ( "ALLOCATE exception %s. Returning ENOMEM %r" , str ( e ) , length ) return Decree . CGC_ENOMEM cpu . write_int ( addr , result , 32 ) logger . info ( "ALLOCATE(%d, %s, 0x%08x) -> 0x%08x" % ( length , perms , addr , result ) ) self . syscall_trace . append ( ( "_allocate" , - 1 , length ) ) return 0
10173	def get_bookmark ( self ) : if not Index ( self . aggregation_alias , using = self . client ) . exists ( ) : if not Index ( self . event_index , using = self . client ) . exists ( ) : return datetime . date . today ( ) return self . _get_oldest_event_timestamp ( ) # retrieve the oldest bookmark query_bookmark = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) [ 0 : 1 ] . sort ( { 'date' : { 'order' : 'desc' } } ) bookmarks = query_bookmark . execute ( ) # if no bookmark is found but the index exist, the bookmark was somehow # lost or never written, so restart from the beginning if len ( bookmarks ) == 0 : return self . _get_oldest_event_timestamp ( ) # change it to doc_id_suffix bookmark = datetime . datetime . strptime ( bookmarks [ 0 ] . date , self . doc_id_suffix ) return bookmark
9863	def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
3341	def parse_xml_body ( environ , allow_empty = False ) : # clHeader = environ . get ( "CONTENT_LENGTH" , "" ) . strip ( ) # content_length = -1 # read all of stream if clHeader == "" : # No Content-Length given: read to end of stream # TODO: etree.parse() locks, if input is invalid? # pfroot = etree.parse(environ["wsgi.input"]).getroot() # requestbody = environ["wsgi.input"].read() # TODO: read() should be # called in a loop? requestbody = "" else : try : content_length = int ( clHeader ) if content_length < 0 : raise DAVError ( HTTP_BAD_REQUEST , "Negative content-length." ) except ValueError : raise DAVError ( HTTP_BAD_REQUEST , "content-length is not numeric." ) if content_length == 0 : requestbody = "" else : requestbody = environ [ "wsgi.input" ] . read ( content_length ) environ [ "wsgidav.all_input_read" ] = 1 if requestbody == "" : if allow_empty : return None else : raise DAVError ( HTTP_BAD_REQUEST , "Body must not be empty." ) try : rootEL = etree . fromstring ( requestbody ) except Exception as e : raise DAVError ( HTTP_BAD_REQUEST , "Invalid XML format." , src_exception = e ) # If dumps of the body are desired, then this is the place to do it pretty: if environ . get ( "wsgidav.dump_request_body" ) : _logger . info ( "{} XML request body:\n{}" . format ( environ [ "REQUEST_METHOD" ] , compat . to_native ( xml_to_bytes ( rootEL , pretty_print = True ) ) , ) ) environ [ "wsgidav.dump_request_body" ] = False return rootEL
10283	def count_sources ( edge_iter : EdgeIterator ) -> Counter : return Counter ( u for u , _ , _ in edge_iter )
11321	def update_languages ( self ) : language_fields = record_get_field_instances ( self . record , '041' ) language = "eng" record_delete_fields ( self . record , "041" ) for field in language_fields : subs = field_get_subfields ( field ) if 'a' in subs : language = self . get_config_item ( subs [ 'a' ] [ 0 ] , "languages" ) break new_subs = [ ( 'a' , language ) ] record_add_field ( self . record , "041" , subfields = new_subs )
3479	def _clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid
3091	def locked_delete ( self ) : if self . _cache : self . _cache . delete ( self . _key_name ) self . _delete_entity ( )
12367	def create ( self , name , ip_address ) : return ( self . post ( name = name , ip_address = ip_address ) . get ( self . singular , None ) )
5882	def nodes_to_check ( self , docs ) : nodes_to_check = [ ] for doc in docs : for tag in [ 'p' , 'pre' , 'td' ] : items = self . parser . getElementsByTag ( doc , tag = tag ) nodes_to_check += items return nodes_to_check
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
8453	def clean ( ) : temple . check . in_git_repo ( ) current_branch = _get_current_branch ( ) update_branch = temple . constants . UPDATE_BRANCH_NAME temp_update_branch = temple . constants . TEMP_UPDATE_BRANCH_NAME if current_branch in ( update_branch , temp_update_branch ) : err_msg = ( 'You must change from the "{}" branch since it will be deleted during cleanup' ) . format ( current_branch ) raise temple . exceptions . InvalidCurrentBranchError ( err_msg ) if temple . check . _has_branch ( update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( update_branch ) ) if temple . check . _has_branch ( temp_update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( temp_update_branch ) )
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8839	def missing ( data , * args ) : not_found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) return ret
2052	def LDRD ( cpu , dest1 , dest2 , src , offset = None ) : assert dest1 . type == 'register' assert dest2 . type == 'register' assert src . type == 'memory' mem1 = cpu . read_int ( src . address ( ) , 32 ) mem2 = cpu . read_int ( src . address ( ) + 4 , 32 ) writeback = cpu . _compute_writeback ( src , offset ) dest1 . write ( mem1 ) dest2 . write ( mem2 ) cpu . _cs_hack_ldr_str_writeback ( src , offset , writeback )
429	def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )
1827	def CALL ( cpu , op0 ) : # TODO FIX 64Bit FIX segment proc = op0 . read ( ) cpu . push ( cpu . PC , cpu . address_bit_size ) cpu . PC = proc
1494	def _trigger_timers ( self ) : current = time . time ( ) while len ( self . timer_tasks ) > 0 and ( self . timer_tasks [ 0 ] [ 0 ] - current <= 0 ) : task = heappop ( self . timer_tasks ) [ 1 ] task ( )
12700	def get_i_name ( self , num , is_oai = None ) : if num not in ( 1 , 2 ) : raise ValueError ( "`num` parameter have to be 1 or 2!" ) if is_oai is None : is_oai = self . oai_marc i_name = "ind" if not is_oai else "i" return i_name + str ( num )
12473	def add_extension_if_needed ( filepath , ext , check_if_exists = False ) : if not filepath . endswith ( ext ) : filepath += ext if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) return filepath
7431	def _count_PIS ( seqsamp , N ) : counts = [ Counter ( col ) for col in seqsamp . T if not ( "-" in col or "N" in col ) ] pis = [ i . most_common ( 2 ) [ 1 ] [ 1 ] > 1 for i in counts if len ( i . most_common ( 2 ) ) > 1 ] if sum ( pis ) >= N : return sum ( pis ) else : return 0
7663	def pop_data ( self ) : data = self . data self . data = SortedKeyList ( key = self . _key ) return data
1372	def get_heron_libs ( local_jars ) : heron_lib_dir = get_heron_lib_dir ( ) heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] return heron_libs
5675	def get_shape_distance_between_stops ( self , trip_I , from_stop_seq , to_stop_seq ) : query_template = "SELECT shape_break FROM stop_times WHERE trip_I={trip_I} AND seq={seq} " stop_seqs = [ from_stop_seq , to_stop_seq ] shape_breaks = [ ] for seq in stop_seqs : q = query_template . format ( seq = seq , trip_I = trip_I ) shape_breaks . append ( self . conn . execute ( q ) . fetchone ( ) ) query_template = "SELECT max(d) - min(d) " "FROM shapes JOIN trips ON(trips.shape_id=shapes.shape_id) " "WHERE trip_I={trip_I} AND shapes.seq>={from_stop_seq} AND shapes.seq<={to_stop_seq};" distance_query = query_template . format ( trip_I = trip_I , from_stop_seq = from_stop_seq , to_stop_seq = to_stop_seq ) return self . conn . execute ( distance_query ) . fetchone ( ) [ 0 ]
4308	def _validate_sample_rates ( input_filepath_list , combine_type ) : sample_rates = [ file_info . sample_rate ( f ) for f in input_filepath_list ] if not core . all_equal ( sample_rates ) : raise IOError ( "Input files do not have the same sample rate. The {} combine " "type requires that all files have the same sample rate" . format ( combine_type ) )
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
9223	def away_from_zero_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] >= 3 : p = 10 ** ndigits return float ( math . floor ( ( value * p ) + math . copysign ( 0.5 , value ) ) ) / p else : return round ( value , ndigits )
4862	def save ( self ) : # pylint: disable=arguments-differ course_id = self . validated_data [ 'course_id' ] __ , created = models . EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = self . enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'rest-api-enrollment' , self . enterprise_customer_user . user_id , course_id )
513	def _updateDutyCycles ( self , overlaps , activeColumns ) : overlapArray = numpy . zeros ( self . _numColumns , dtype = realDType ) activeArray = numpy . zeros ( self . _numColumns , dtype = realDType ) overlapArray [ overlaps > 0 ] = 1 activeArray [ activeColumns ] = 1 period = self . _dutyCyclePeriod if ( period > self . _iterationNum ) : period = self . _iterationNum self . _overlapDutyCycles = self . _updateDutyCyclesHelper ( self . _overlapDutyCycles , overlapArray , period ) self . _activeDutyCycles = self . _updateDutyCyclesHelper ( self . _activeDutyCycles , activeArray , period )
7167	def load_intent ( self , name , file_name , reload_cache = False ) : self . intents . load ( name , file_name , reload_cache ) with open ( file_name ) as f : self . padaos . add_intent ( name , f . read ( ) . split ( '\n' ) ) self . must_train = True
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
2344	def create_graph_from_data ( self , data , * * kwargs ) : # Building setup w/ arguments. self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{CUTOFF}' ] = str ( self . cutoff ) self . arguments [ '{VARSEL}' ] = str ( self . variablesel ) . upper ( ) self . arguments [ '{SELMETHOD}' ] = self . var_selection [ self . selmethod ] self . arguments [ '{PRUNING}' ] = str ( self . pruning ) . upper ( ) self . arguments [ '{PRUNMETHOD}' ] = self . var_selection [ self . prunmethod ] self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_cam ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
12560	def large_clusters_mask ( volume , min_cluster_size ) : labels , num_labels = scn . label ( volume ) labels_to_keep = set ( [ i for i in range ( num_labels ) if np . sum ( labels == i ) >= min_cluster_size ] ) clusters_mask = np . zeros_like ( volume , dtype = int ) for l in range ( num_labels ) : if l in labels_to_keep : clusters_mask [ labels == l ] = 1 return clusters_mask
2747	def get_all_regions ( self ) : data = self . get_data ( "regions/" ) regions = list ( ) for jsoned in data [ 'regions' ] : region = Region ( * * jsoned ) region . token = self . token regions . append ( region ) return regions
8707	def exec_file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from_file ( path ) . replace ( '\r' , '' ) . split ( '\n' ) res = '> ' for line in content : line = line . rstrip ( '\n' ) retlines = ( res + self . __exchange ( line ) ) . splitlines ( ) # Log all but the last line res = retlines . pop ( ) for lin in retlines : log . info ( lin ) # last line log . info ( res )
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
6443	def _cond_n ( self , word , suffix_len ) : if len ( word ) - suffix_len >= 3 : if word [ - suffix_len - 3 ] == 's' : if len ( word ) - suffix_len >= 4 : return True else : return True return False
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
11601	def save_model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )
2060	def add ( self , constraint , check = False ) : if isinstance ( constraint , bool ) : constraint = BoolConstant ( constraint ) assert isinstance ( constraint , Bool ) constraint = simplify ( constraint ) # If self._child is not None this constraint set has been forked and a # a derived constraintset may be using this. So we can't add any more # constraints to this one. After the child constraintSet is deleted # we regain the ability to add constraints. if self . _child is not None : raise Exception ( 'ConstraintSet is frozen' ) if isinstance ( constraint , BoolConstant ) : if not constraint . value : logger . info ( "Adding an impossible constant constraint" ) self . _constraints = [ constraint ] else : return self . _constraints . append ( constraint ) if check : from . . . core . smtlib import solver if not solver . check ( self ) : raise ValueError ( "Added an impossible constraint" )
8265	def _interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in _range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
9966	def convert_args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs
1086	def replace ( self , year = None , month = None , day = None , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return datetime . __new__ ( type ( self ) , year , month , day , hour , minute , second , microsecond , tzinfo )
8587	def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ "id": "' + cdrom_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
3086	def _is_ndb ( self ) : # issubclass will fail if one of the arguments is not a class, only # need worry about new-style classes since ndb and db models are # new-style if isinstance ( self . _model , type ) : if _NDB_MODEL is not None and issubclass ( self . _model , _NDB_MODEL ) : return True elif issubclass ( self . _model , db . Model ) : return False raise TypeError ( 'Model class not an NDB or DB model: {0}.' . format ( self . _model ) )
1837	def JGE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , ( cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
7901	def configure_room ( self , form ) : if form . type == "cancel" : return None elif form . type != "submit" : raise ValueError ( "A 'submit' form required to configure a room" ) iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "set" ) query = iq . new_query ( MUC_OWNER_NS , "query" ) form . as_xml ( query ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_success , self . process_configuration_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
10461	def _glob_match ( self , pattern , string ) : # regex flags Multi-line, Unicode, Locale return bool ( re . match ( fnmatch . translate ( pattern ) , string , re . M | re . U | re . L ) )
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
6370	def specificity ( self ) : if self . _tn + self . _fp == 0 : return float ( 'NaN' ) return self . _tn / ( self . _tn + self . _fp )
472	def build_reverse_dictionary ( word_to_id ) : reverse_dictionary = dict ( zip ( word_to_id . values ( ) , word_to_id . keys ( ) ) ) return reverse_dictionary
9063	def unfix ( self , param ) : if param == "delta" : self . _unfix ( "logistic" ) else : self . _fix [ param ] = False
13354	def _pipepager ( text , cmd , color ) : import subprocess env = dict ( os . environ ) # If we're piping to less we might support colors under the # condition that cmd_detail = cmd . rsplit ( '/' , 1 ) [ - 1 ] . split ( ) if color is None and cmd_detail [ 0 ] == 'less' : less_flags = os . environ . get ( 'LESS' , '' ) + ' ' . join ( cmd_detail [ 1 : ] ) if not less_flags : env [ 'LESS' ] = '-R' color = True elif 'r' in less_flags or 'R' in less_flags : color = True if not color : text = strip_ansi ( text ) c = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , env = env ) encoding = get_best_encoding ( c . stdin ) try : c . stdin . write ( text . encode ( encoding , 'replace' ) ) c . stdin . close ( ) except ( IOError , KeyboardInterrupt ) : pass # Less doesn't respect ^C, but catches it for its own UI purposes (aborting # search or other commands inside less). # # That means when the user hits ^C, the parent process (click) terminates, # but less is still alive, paging the output and messing up the terminal. # # If the user wants to make the pager exit on ^C, they should set # `LESS='-K'`. It's not our decision to make. while True : try : c . wait ( ) except KeyboardInterrupt : pass else : break
7497	def nworker ( data , smpchunk , tests ) : ## tell engines to limit threads #numba.config.NUMBA_DEFAULT_NUM_THREADS = 1 ## open the seqarray view, the modified array is in bootsarr with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : ] ## create an N-mask array of all seq cols (this isn't really too slow) nall_mask = seqview [ : ] == 78 ## tried numba compiling everythign below here, but was not faster ## than making nmask w/ axis arg in numpy ## get the input arrays ready rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None #rweights = np.ones(smpchunk.shape[0], dtype=np.float64) rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) #times = [] ## fill arrays with results using numba funcs for idx in xrange ( smpchunk . shape [ 0 ] ) : ## get seqchunk for 4 samples (4, ncols) sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] ## get N-containing columns in 4-array, and invariant sites. nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) ## <- do we need this? ## get matrices if there are any shared SNPs ## returns best-tree index, qscores, and qstats #bidx, qscores, qstats = calculate(seqchunk, maparr[:, 0], nmask, tests) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) ## get weights from the three scores sorted. ## Only save to file if the quartet has information rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats
4584	def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
12999	def hr_diagram_figure ( cluster ) : temps , lums = round_teff_luminosity ( cluster ) x , y = temps , lums colors , color_mapper = hr_diagram_color_helper ( temps ) x_range = [ max ( x ) + max ( x ) * 0.05 , min ( x ) - min ( x ) * 0.05 ] source = ColumnDataSource ( data = dict ( x = x , y = y , color = colors ) ) pf = figure ( y_axis_type = 'log' , x_range = x_range , name = 'hr' , tools = 'box_select,lasso_select,reset,hover' , title = 'H-R Diagram for {0}' . format ( cluster . name ) ) pf . select ( BoxSelectTool ) . select_every_mousemove = False pf . select ( LassoSelectTool ) . select_every_mousemove = False hover = pf . select ( HoverTool ) [ 0 ] hover . tooltips = [ ( "Temperature (Kelvin)" , "@x{0}" ) , ( "Luminosity (solar units)" , "@y{0.00}" ) ] _diagram ( source = source , plot_figure = pf , name = 'hr' , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) return pf
4393	def adsSyncWriteReqEx ( port , address , index_group , index_offset , value , plc_data_type ) : # type: (int, AmsAddr, int, int, Any, Type) -> None sync_write_request = _adsDLL . AdsSyncWriteReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( value . encode ( "utf-8" ) ) data_pointer = data # type: Union[ctypes.c_char_p, ctypes.pointer] data_length = len ( data_pointer . value ) + 1 # type: ignore else : if type ( plc_data_type ) . __name__ == "PyCArrayType" : data = plc_data_type ( * value ) else : data = plc_data_type ( value ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
2012	def instruction ( self ) : # FIXME check if pc points to invalid instruction # if self.pc >= len(self.bytecode): # return InvalidOpcode('Code out of range') # if self.pc in self.invalid: # raise InvalidOpcode('Opcode inside a PUSH immediate') try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) # This serves one of two purposes depending on run-mode: # - Minimize number of files checked by full-content comparison (hash) # - Minimize chances of file handle exhaustion and limit seeking (exact) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
10245	def create_timeline ( year_counter : typing . Counter [ int ] ) -> List [ Tuple [ int , int ] ] : if not year_counter : return [ ] from_year = min ( year_counter ) - 1 until_year = datetime . now ( ) . year + 1 return [ ( year , year_counter . get ( year , 0 ) ) for year in range ( from_year , until_year ) ]
9178	def obtain_licenses ( ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( """\ SELECT combined_row.url, row_to_json(combined_row) FROM ( SELECT "code", "version", "name", "url", "is_valid_for_publication" FROM licenses) AS combined_row""" ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
1723	def translate_file ( input_path , output_path ) : js = get_file_contents ( input_path ) py_code = translate_js ( js ) lib_name = os . path . basename ( output_path ) . split ( '.' ) [ 0 ] head = '__all__ = [%s]\n\n# Don\'t look below, you will not understand this Python code :) I don\'t.\n\n' % repr ( lib_name ) tail = '\n\n# Add lib to the module scope\n%s = var.to_python()' % lib_name out = head + py_code + tail write_file_contents ( output_path , out )
11244	def reformat_css ( input_file , output_file ) : # Number of lines in the file. line_count = get_line_count ( input_file ) # Open source and target files. f = open ( input_file , 'r+' ) output = open ( output_file , 'w' ) # Loop over every line in the file. for line in range ( line_count ) : # Eliminate whitespace at the beginning and end of lines. string = f . readline ( ) . strip ( ) # New lines after { string = re . sub ( '\{' , '{\n' , string ) # New lines after ; string = re . sub ( '; ' , ';' , string ) string = re . sub ( ';' , ';\n' , string ) # Eliminate whitespace before comments string = re . sub ( '} /*' , '}/*' , string ) # New lines after } string = re . sub ( '\}' , '}\n' , string ) # New lines at the end of comments string = re . sub ( '\*/' , '*/\n' , string ) # Write to the output file. output . write ( string ) # Close the files. output . close ( ) f . close ( ) # Indent the css. indent_css ( output_file , output_file ) # Make sure there's a space before every { add_whitespace_before ( "{" , output_file , output_file )
12612	def search_unique ( self , table_name , sample , unique_fields = None ) : return search_unique ( table = self . table ( table_name ) , sample = sample , unique_fields = unique_fields )
4105	def xcorr ( x , y = None , maxlags = None , norm = 'biased' ) : N = len ( x ) if y is None : y = x assert len ( x ) == len ( y ) , 'x and y must have the same length. Add zeros if needed' if maxlags is None : maxlags = N - 1 lags = np . arange ( 0 , 2 * N - 1 ) else : assert maxlags <= N , 'maxlags must be less than data length' lags = np . arange ( N - maxlags - 1 , N + maxlags ) res = np . correlate ( x , y , mode = 'full' ) if norm == 'biased' : Nf = float ( N ) res = res [ lags ] / float ( N ) # do not use /= !! elif norm == 'unbiased' : res = res [ lags ] / ( float ( N ) - abs ( np . arange ( - N + 1 , N ) ) ) [ lags ] elif norm == 'coeff' : Nf = float ( N ) rms = pylab_rms_flat ( x ) * pylab_rms_flat ( y ) res = res [ lags ] / rms / Nf else : res = res [ lags ] lags = np . arange ( - maxlags , maxlags + 1 ) return res , lags
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
5319	def readattr ( path , name ) : try : f = open ( USB_SYS_PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IOError : return None
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
1507	def print_cluster_info ( cl_args ) : parsed_roles = read_and_parse_roles ( cl_args ) masters = list ( parsed_roles [ Role . MASTERS ] ) slaves = list ( parsed_roles [ Role . SLAVES ] ) zookeepers = list ( parsed_roles [ Role . ZOOKEEPERS ] ) cluster = list ( parsed_roles [ Role . CLUSTER ] ) # OrderedDicts are used here so that the key order can be # specified directly info = OrderedDict ( ) info [ 'numNodes' ] = len ( cluster ) info [ 'nodes' ] = cluster roles = OrderedDict ( ) roles [ 'masters' ] = masters roles [ 'slaves' ] = slaves roles [ 'zookeepers' ] = zookeepers urls = OrderedDict ( ) urls [ 'serviceUrl' ] = get_service_url ( cl_args ) urls [ 'heronUi' ] = get_heron_ui_url ( cl_args ) urls [ 'heronTracker' ] = get_heron_tracker_url ( cl_args ) info [ 'roles' ] = roles info [ 'urls' ] = urls print json . dumps ( info , indent = 2 )
601	def addGraph ( self , data , position = 111 , xlabel = None , ylabel = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . plot ( data ) plt . draw ( )
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : # Check for cyclic waits try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) # Check for disconnected tasks if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
4912	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer = self . get_object ( ) # Maintain plus characters in course key. course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = False for catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : contains_course_runs = not course_run_ids or catalog . contains_courses ( course_run_ids ) contains_program_uuids = not program_uuids or catalog . contains_programs ( program_uuids ) if contains_course_runs and contains_program_uuids : contains_content_items = True break return Response ( { 'contains_content_items' : contains_content_items } )
1608	def make_tuple ( stream , tuple_key , values , roots = None ) : component_name = stream . component_name stream_id = stream . id gen_task = roots [ 0 ] . taskid if roots is not None and len ( roots ) > 0 else None return HeronTuple ( id = str ( tuple_key ) , component = component_name , stream = stream_id , task = gen_task , values = values , creation_time = time . time ( ) , roots = roots )
3447	def _as_medium ( exchanges , tolerance = 1e-6 , exports = False ) : LOGGER . debug ( "Formatting medium." ) medium = pd . Series ( ) for rxn in exchanges : export = len ( rxn . reactants ) == 1 flux = rxn . flux if abs ( flux ) < tolerance : continue if export : medium [ rxn . id ] = - flux elif not export : medium [ rxn . id ] = flux if not exports : medium = medium [ medium > 0 ] return medium
362	def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : # We first define a download function, supporting both Python 2 and 3. def _download ( filename , working_directory , url_source ) : progress_bar = progressbar . ProgressBar ( ) def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : if ( totalSize != 0 ) : if not pbar . max_value : totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) pbar . max_value = int ( totalBlocks ) pbar . update ( count , force = True ) filepath = os . path . join ( working_directory , filename ) logging . info ( 'Downloading %s...\n' % filename ) urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) exists_or_mkdir ( working_directory , verbose = False ) filepath = os . path . join ( working_directory , filename ) if not os . path . exists ( filepath ) : _download ( filename , working_directory , url_source ) statinfo = os . stat ( filepath ) logging . info ( 'Succesfully downloaded %s %s bytes.' % ( filename , statinfo . st_size ) ) # , 'bytes.') if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) if ( extract ) : if tarfile . is_tarfile ( filepath ) : logging . info ( 'Trying to extract tar file' ) tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) logging . info ( '... Success!' ) elif zipfile . is_zipfile ( filepath ) : logging . info ( 'Trying to extract zip file' ) with zipfile . ZipFile ( filepath ) as zf : zf . extractall ( working_directory ) logging . info ( '... Success!' ) else : logging . info ( "Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported" ) return filepath
2584	def get_tasks ( self , count ) : tasks = [ ] for i in range ( 0 , count ) : try : x = self . pending_task_queue . get ( block = False ) except queue . Empty : break else : tasks . append ( x ) return tasks
2384	def read_yaml_file ( path , loader = ExtendedSafeLoader ) : with open ( path ) as fh : return load ( fh , loader )
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) # check if service is already registered if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
4405	def parse_line ( line , document = None ) : result = re . match ( line_pattern , line ) if result : _ , lineno , offset , severity , msg = result . groups ( ) lineno = int ( lineno or 1 ) offset = int ( offset or 0 ) errno = 2 if severity == 'error' : errno = 1 diag = { 'source' : 'mypy' , 'range' : { 'start' : { 'line' : lineno - 1 , 'character' : offset } , # There may be a better solution, but mypy does not provide end 'end' : { 'line' : lineno - 1 , 'character' : offset + 1 } } , 'message' : msg , 'severity' : errno } if document : # although mypy does not provide the end of the affected range, we # can make a good guess by highlighting the word that Mypy flagged word = document . word_at_position ( diag [ 'range' ] [ 'start' ] ) if word : diag [ 'range' ] [ 'end' ] [ 'character' ] = ( diag [ 'range' ] [ 'start' ] [ 'character' ] + len ( word ) ) return diag
3066	def _apply_user_agent ( headers , user_agent ) : if user_agent is not None : if 'user-agent' in headers : headers [ 'user-agent' ] = ( user_agent + ' ' + headers [ 'user-agent' ] ) else : headers [ 'user-agent' ] = user_agent return headers
11804	def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : # Remove old val if there was one self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
661	def computeSaturationLevels ( outputs , outputsShape , sparseForm = False ) : # Get the outputs into a SparseBinaryMatrix if not sparseForm : outputs = outputs . reshape ( outputsShape ) spOut = SM32 ( outputs ) else : if len ( outputs ) > 0 : assert ( outputs . max ( ) < outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut = SM32 ( 1 , outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut . setRowFromSparse ( 0 , outputs , [ 1 ] * len ( outputs ) ) spOut . reshape ( outputsShape [ 0 ] , outputsShape [ 1 ] ) # Get the activity in each local region using the nNonZerosPerBox method # This method takes a list of the end row indices and a list of the end # column indices. # We will use regions that are 15x15, which give us about a 1/225 (.4%) resolution # on saturation. regionSize = 15 rows = xrange ( regionSize + 1 , outputsShape [ 0 ] + 1 , regionSize ) cols = xrange ( regionSize + 1 , outputsShape [ 1 ] + 1 , regionSize ) regionSums = spOut . nNonZerosPerBox ( rows , cols ) # Get all the nonzeros out - those are our saturation sums ( locations , values ) = regionSums . tolist ( ) values /= float ( regionSize * regionSize ) sat = list ( values ) # Now, to compute which are the inner regions, we will only take the ones that # are surrounded by activity above, below, left and right innerSat = [ ] locationSet = set ( locations ) for ( location , value ) in itertools . izip ( locations , values ) : ( row , col ) = location if ( row - 1 , col ) in locationSet and ( row , col - 1 ) in locationSet and ( row + 1 , col ) in locationSet and ( row , col + 1 ) in locationSet : innerSat . append ( value ) return ( sat , innerSat )
3597	def list ( self , cat , ctr = None , nb_results = None , offset = None ) : path = LIST_URL + "?c=3&cat={}" . format ( requests . utils . quote ( cat ) ) if ctr is not None : path += "&ctr={}" . format ( requests . utils . quote ( ctr ) ) if nb_results is not None : path += "&n={}" . format ( requests . utils . quote ( str ( nb_results ) ) ) if offset is not None : path += "&o={}" . format ( requests . utils . quote ( str ( offset ) ) ) data = self . executeRequestApi2 ( path ) clusters = [ ] docs = [ ] if ctr is None : # list subcategories for pf in data . preFetch : for cluster in pf . response . payload . listResponse . doc : clusters . extend ( cluster . child ) return [ c . docid for c in clusters ] else : apps = [ ] for d in data . payload . listResponse . doc : # categories for c in d . child : # sub-category for a in c . child : # app apps . append ( utils . parseProtobufObj ( a ) ) return apps
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
11051	def listen_events ( self , reconnects = 0 ) : self . log . info ( 'Listening for events from Marathon...' ) self . _attached = False def on_finished ( result , reconnects ) : # If the callback fires then the HTTP request to the event stream # went fine, but the persistent connection for the SSE stream was # dropped. Just reconnect for now- if we can't actually connect # then the errback will fire rather. self . log . warn ( 'Connection lost listening for events, ' 'reconnecting... ({reconnects} so far)' , reconnects = reconnects ) reconnects += 1 return self . listen_events ( reconnects ) def log_failure ( failure ) : self . log . failure ( 'Failed to listen for events' , failure ) return failure return self . marathon_client . get_events ( { 'event_stream_attached' : self . _sync_on_event_stream_attached , 'api_post_event' : self . _sync_on_api_post_event } ) . addCallbacks ( on_finished , log_failure , callbackArgs = [ reconnects ] )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
6768	def update ( self ) : packager = self . packager if packager == APT : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update' ) elif packager == YUM : self . sudo ( 'yum update' ) else : raise Exception ( 'Unknown packager: %s' % ( packager , ) )
8316	def parse_images ( self , markup , treshold = 6 ) : images = [ ] m = re . findall ( self . re [ "image" ] , markup ) for p in m : p = self . parse_balanced_image ( p ) img = p . split ( "|" ) path = img [ 0 ] . replace ( "[[Image:" , "" ) . strip ( ) description = u"" links = { } properties = [ ] if len ( img ) > 1 : img = "|" . join ( img [ 1 : ] ) links = self . parse_links ( img ) properties = self . plain ( img ) . split ( "|" ) description = u"" # Best guess: an image description is normally # longer than six characters, properties like # "thumb" and "right" are less than six characters. if len ( properties [ - 1 ] ) > treshold : description = properties [ - 1 ] properties = properties [ : - 1 ] img = WikipediaImage ( path , description , links , properties ) images . append ( img ) markup = markup . replace ( p , "" ) return images , markup . strip ( )
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
4329	def flanger ( self , delay = 0 , depth = 2 , regen = 0 , width = 71 , speed = 0.5 , shape = 'sine' , phase = 25 , interp = 'linear' ) : if not is_number ( delay ) or delay < 0 or delay > 30 : raise ValueError ( "delay must be a number between 0 and 30." ) if not is_number ( depth ) or depth < 0 or depth > 10 : raise ValueError ( "depth must be a number between 0 and 10." ) if not is_number ( regen ) or regen < - 95 or regen > 95 : raise ValueError ( "regen must be a number between -95 and 95." ) if not is_number ( width ) or width < 0 or width > 100 : raise ValueError ( "width must be a number between 0 and 100." ) if not is_number ( speed ) or speed < 0.1 or speed > 10 : raise ValueError ( "speed must be a number between 0.1 and 10." ) if shape not in [ 'sine' , 'triangle' ] : raise ValueError ( "shape must be one of 'sine' or 'triangle'." ) if not is_number ( phase ) or phase < 0 or phase > 100 : raise ValueError ( "phase must be a number between 0 and 100." ) if interp not in [ 'linear' , 'quadratic' ] : raise ValueError ( "interp must be one of 'linear' or 'quadratic'." ) effect_args = [ 'flanger' , '{:f}' . format ( delay ) , '{:f}' . format ( depth ) , '{:f}' . format ( regen ) , '{:f}' . format ( width ) , '{:f}' . format ( speed ) , '{}' . format ( shape ) , '{:f}' . format ( phase ) , '{}' . format ( interp ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'flanger' ) return self
11587	def object ( self , infotype , key ) : redisent = self . redises [ self . _getnodenamefor ( key ) + '_slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
8689	def _construct_key ( self , values ) : key = { } for column , value in zip ( self . keys . columns , values ) : key . update ( { column . name : value } ) return key
10879	def wrap_and_calc_psf ( xpts , ypts , zpts , func , * * kwargs ) : #1. Checking that everything is hunky-dory: for t in [ xpts , ypts , zpts ] : if len ( t . shape ) != 1 : raise ValueError ( 'xpts,ypts,zpts must be 1D.' ) dx = 1 if xpts [ 0 ] == 0 else 0 dy = 1 if ypts [ 0 ] == 0 else 0 xg , yg , zg = np . meshgrid ( xpts , ypts , zpts , indexing = 'ij' ) xs , ys , zs = [ pts . size for pts in [ xpts , ypts , zpts ] ] to_return = np . zeros ( [ 2 * xs - dx , 2 * ys - dy , zs ] ) #2. Calculate: up_corner_psf = func ( xg , yg , zg , * * kwargs ) to_return [ xs - dx : , ys - dy : , : ] = up_corner_psf . copy ( ) #x>0, y>0 if dx == 0 : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ : : - 1 , : , : ] . copy ( ) #x<0, y>0 else : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ - 1 : 0 : - 1 , : , : ] . copy ( ) #x<0, y>0 if dy == 0 : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , : : - 1 , : ] . copy ( ) #x>0, y<0 else : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , - 1 : 0 : - 1 , : ] . copy ( ) #x>0, y<0 if ( dx == 0 ) and ( dy == 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , : : - 1 , : ] . copy ( ) #x<0,y<0 elif ( dx == 0 ) and ( dy != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) #x<0,y<0 elif ( dy == 0 ) and ( dx != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , : : - 1 , : ] . copy ( ) #x<0,y<0 else : #dx==1 and dy==1 to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) #x<0,y<0 return to_return
9469	def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
8157	def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
13519	def prop_power ( self , propulsion_eff = 0.7 , sea_margin = 0.2 ) : PP = ( 1 + sea_margin ) * self . resistance ( ) * self . speed / propulsion_eff return PP
3681	def GWP ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in GWP_data . index : methods . append ( IPCC100 ) if not pd . isnull ( GWP_data . at [ CASRN , 'SAR 100yr' ] ) : methods . append ( IPCC100SAR ) methods . append ( IPCC20 ) methods . append ( IPCC500 ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IPCC100 : return float ( GWP_data . at [ CASRN , '100yr GWP' ] ) elif Method == IPCC100SAR : return float ( GWP_data . at [ CASRN , 'SAR 100yr' ] ) elif Method == IPCC20 : return float ( GWP_data . at [ CASRN , '20yr GWP' ] ) elif Method == IPCC500 : return float ( GWP_data . at [ CASRN , '500yr GWP' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
7642	def _conversion ( target , source ) : def register ( func ) : '''This decorator registers func as mapping source to target''' __CONVERSION__ [ target ] [ source ] = func return func return register
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
7017	def parallel_concat_worker ( task ) : lcbasedir , objectid , kwargs = task try : return concat_write_pklc ( lcbasedir , objectid , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed LC concatenation for %s in %s' % ( objectid , lcbasedir ) ) return None
1322	def MoveToCenter ( self ) -> bool : if self . IsTopLevel ( ) : rect = self . BoundingRectangle screenWidth , screenHeight = GetScreenSize ( ) x , y = ( screenWidth - rect . width ( ) ) // 2 , ( screenHeight - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return SetWindowPos ( self . NativeWindowHandle , SWP . HWND_Top , x , y , 0 , 0 , SWP . SWP_NoSize ) return False
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' # calculate the totla size of the packet incl. header typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) # first write out how much data is there as the header packet += HeronProtocol . pack_int ( datasize ) # next write the type string packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename # reqid packet += reqid . pack ( ) # add the proto packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
12481	def get_rcfile_variable_value ( var_name , app_name , section_name = None ) : cfg = get_rcfile_section ( app_name , section_name ) if var_name in cfg : raise KeyError ( 'Option {} not found in {} ' 'section.' . format ( var_name , section_name ) ) return cfg [ var_name ]
3281	def compute_digest_response ( self , realm , user_name , method , uri , nonce , cnonce , qop , nc , environ ) : def md5h ( data ) : return md5 ( compat . to_bytes ( data ) ) . hexdigest ( ) def md5kd ( secret , data ) : return md5h ( secret + ":" + data ) A1 = self . domain_controller . digest_auth_user ( realm , user_name , environ ) if not A1 : return False A2 = method + ":" + uri if qop : res = md5kd ( A1 , nonce + ":" + nc + ":" + cnonce + ":" + qop + ":" + md5h ( A2 ) ) else : res = md5kd ( A1 , nonce + ":" + md5h ( A2 ) ) return res
7077	def parallel_periodicvar_recovery ( simbasedir , period_tolerance = 1.0e-3 , liststartind = None , listmaxobjects = None , nworkers = None ) : # figure out the periodfinding pickles directory pfpkldir = os . path . join ( simbasedir , 'periodfinding' ) if not os . path . exists ( pfpkldir ) : LOGERROR ( 'no "periodfinding" subdirectory in %s, can\'t continue' % simbasedir ) return None # find all the periodfinding pickles pfpkl_list = glob . glob ( os . path . join ( pfpkldir , '*periodfinding*pkl*' ) ) if len ( pfpkl_list ) > 0 : if liststartind : pfpkl_list = pfpkl_list [ liststartind : ] if listmaxobjects : pfpkl_list = pfpkl_list [ : listmaxobjects ] tasks = [ ( x , simbasedir , period_tolerance ) for x in pfpkl_list ] pool = mp . Pool ( nworkers ) results = pool . map ( periodrec_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { x [ 'objectid' ] : x for x in results if x is not None } actual_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and x [ 'actual_vartype' ] in PERIODIC_VARTYPES ) ] , dtype = np . unicode_ ) recovered_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'actual' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_twice_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'twice' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_half_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'half' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) all_objectids = [ x [ 'objectid' ] for x in results ] outdict = { 'simbasedir' : os . path . abspath ( simbasedir ) , 'objectids' : all_objectids , 'period_tolerance' : period_tolerance , 'actual_periodicvars' : actual_periodicvars , 'recovered_periodicvars' : recovered_periodicvars , 'alias_twice_periodicvars' : alias_twice_periodicvars , 'alias_half_periodicvars' : alias_half_periodicvars , 'details' : resdict } outfile = os . path . join ( simbasedir , 'periodicvar-recovery.pkl' ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict else : LOGERROR ( 'no periodfinding result pickles found in %s, can\'t continue' % pfpkldir ) return None
6958	def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
3928	def _replace_words ( replacements , string ) : output_lines = [ ] for line in string . split ( '\n' ) : output_words = [ ] for word in line . split ( ' ' ) : new_word = replacements . get ( word , word ) output_words . append ( new_word ) output_lines . append ( output_words ) return '\n' . join ( ' ' . join ( output_words ) for output_words in output_lines )
6619	def poll ( self ) : finished_procs = [ p for p in self . running_procs if p . poll ( ) is not None ] self . running_procs = collections . deque ( [ p for p in self . running_procs if p not in finished_procs ] ) for proc in finished_procs : stdout , stderr = proc . communicate ( ) ## proc.communicate() returns (stdout, stderr) when ## self.pipe = True. Otherwise they are (None, None) finished_pids = [ p . pid for p in finished_procs ] self . finished_pids . extend ( finished_pids ) logger = logging . getLogger ( __name__ ) messages = 'Running: {}, Finished: {}' . format ( len ( self . running_procs ) , len ( self . finished_pids ) ) logger . info ( messages ) return finished_pids
12676	def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
2944	def accept_message ( self , message ) : assert not self . read_only self . refresh_waiting_tasks ( ) self . do_engine_steps ( ) for my_task in Task . Iterator ( self . task_tree , Task . WAITING ) : my_task . task_spec . accept_message ( my_task , message )
2972	def from_dict ( name , values ) : # determine the number of instances of this container count = 1 count_value = values . get ( 'count' , 1 ) if isinstance ( count_value , int ) : count = max ( count_value , 1 ) def with_index ( name , idx ) : if name and idx : return '%s_%d' % ( name , idx ) return name def get_instance ( n , idx = None ) : return BlockadeContainerConfig ( with_index ( n , idx ) , values [ 'image' ] , command = values . get ( 'command' ) , links = values . get ( 'links' ) , volumes = values . get ( 'volumes' ) , publish_ports = values . get ( 'ports' ) , expose_ports = values . get ( 'expose' ) , environment = values . get ( 'environment' ) , hostname = values . get ( 'hostname' ) , dns = values . get ( 'dns' ) , start_delay = values . get ( 'start_delay' , 0 ) , neutral = values . get ( 'neutral' , False ) , holy = values . get ( 'holy' , False ) , container_name = with_index ( values . get ( 'container_name' ) , idx ) , cap_add = values . get ( 'cap_add' ) ) if count == 1 : yield get_instance ( name ) else : for idx in range ( 1 , count + 1 ) : # TODO: configurable name/index format yield get_instance ( name , idx )
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
8182	def remove_edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
6801	def load_db_set ( self , name , r = None ) : r = r or self db_set = r . genv . db_sets . get ( name , { } ) r . genv . update ( db_set )
853	def appendRecord ( self , record ) : assert self . _file is not None assert self . _mode == self . _FILE_WRITE_MODE assert isinstance ( record , ( list , tuple ) ) , "unexpected record type: " + repr ( type ( record ) ) assert len ( record ) == self . _fieldCount , "len(record): %s, fieldCount: %s" % ( len ( record ) , self . _fieldCount ) # Write header if needed if self . _recordCount == 0 : # Write the header names , types , specials = zip ( * self . getFields ( ) ) for line in names , types , specials : self . _writer . writerow ( line ) # Keep track of sequences, make sure time flows forward self . _updateSequenceInfo ( record ) line = [ self . _adapters [ i ] ( f ) for i , f in enumerate ( record ) ] self . _writer . writerow ( line ) self . _recordCount += 1
527	def _getColumnNeighborhood ( self , centerColumn ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions ) else : return topology . neighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions )
3675	def charge ( self ) : try : if not self . rdkitmol : return charge_from_formula ( self . formula ) else : return Chem . GetFormalCharge ( self . rdkitmol ) except : return charge_from_formula ( self . formula )
3629	def horiz_div ( col_widths , horiz , vert , padding ) : horizs = [ horiz * w for w in col_widths ] div = '' . join ( [ padding * horiz , vert , padding * horiz ] ) return div . join ( horizs )
1449	def get_all_zk_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "zookeeper" ) for location in state_locations : name = location [ 'name' ] hostport = location [ 'hostport' ] hostportlist = [ ] for hostportpair in hostport . split ( ',' ) : host = None port = None if ':' in hostport : hostandport = hostportpair . split ( ':' ) if len ( hostandport ) == 2 : host = hostandport [ 0 ] port = int ( hostandport [ 1 ] ) if not host or not port : raise Exception ( "Hostport for %s must be of the format 'host:port'." % ( name ) ) hostportlist . append ( ( host , port ) ) tunnelhost = location [ 'tunnelhost' ] rootpath = location [ 'rootpath' ] LOG . info ( "Connecting to zk hostports: " + str ( hostportlist ) + " rootpath: " + rootpath ) state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) state_managers . append ( state_manager ) return state_managers
4121	def twosided_2_centerdc ( data ) : N = len ( data ) # could us int() or // in python 3 newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
6492	def _process_facet_terms ( facet_terms ) : elastic_facets = { } for facet in facet_terms : facet_term = { "field" : facet } if facet_terms [ facet ] : for facet_option in facet_terms [ facet ] : facet_term [ facet_option ] = facet_terms [ facet ] [ facet_option ] elastic_facets [ facet ] = { "terms" : facet_term } return elastic_facets
7571	def revcomp ( sequence ) : sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
6016	def signal_to_noise_map ( self ) : signal_to_noise_map = np . divide ( self . image , self . noise_map ) signal_to_noise_map [ signal_to_noise_map < 0 ] = 0 return signal_to_noise_map
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
10398	def score_leaves ( self ) -> Set [ BaseEntity ] : leaves = set ( self . iter_leaves ( ) ) if not leaves : log . warning ( 'no leaves.' ) return set ( ) for leaf in leaves : self . graph . nodes [ leaf ] [ self . tag ] = self . calculate_score ( leaf ) log . log ( 5 , 'chomping %s' , leaf ) return leaves
9891	def _uptime_amiga ( ) : global __boottime try : __boottime = os . stat ( 'RAM:' ) . st_ctime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
12150	def htmlFor ( self , fname ) : if os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.jpg' , '.png' ] : html = '<a href="%s"><img src="%s"></a>' % ( fname , fname ) if "_tif_" in fname : html = html . replace ( '<img ' , '<img class="datapic micrograph"' ) if "_plot_" in fname : html = html . replace ( '<img ' , '<img class="datapic intrinsic" ' ) if "_experiment_" in fname : html = html . replace ( '<img ' , '<img class="datapic experiment" ' ) elif os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.html' , '.htm' ] : html = 'LINK: %s' % fname else : html = '<br>Not sure how to show: [%s]</br>' % fname return html
9210	def capture ( target_url , user_agent = "archiveis (https://github.com/pastpages/archiveis)" , proxies = { } ) : # Put together the URL that will save our request domain = "http://archive.vn" save_url = urljoin ( domain , "/submit/" ) # Configure the request headers headers = { 'User-Agent' : user_agent , "host" : "archive.vn" , } # Request a unique identifier for our activity logger . debug ( "Requesting {}" . format ( domain + "/" ) ) get_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , ) if proxies : get_kwargs [ 'proxies' ] = proxies response = requests . get ( domain + "/" , * * get_kwargs ) response . raise_for_status ( ) # It will need to be parsed from the homepage response headers html = str ( response . content ) try : unique_id = html . split ( 'name="submitid' , 1 ) [ 1 ] . split ( 'value="' , 1 ) [ 1 ] . split ( '"' , 1 ) [ 0 ] logger . debug ( "Unique identifier: {}" . format ( unique_id ) ) except IndexError : logger . warn ( "Unable to extract unique identifier from archive.is. Submitting without it." ) unique_id = None # Send the capture request to archive.is with the unique id included data = { "url" : target_url , "anyway" : 1 , } if unique_id : data . update ( { "submitid" : unique_id } ) post_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , data = data ) if proxies : post_kwargs [ 'proxies' ] = proxies logger . debug ( "Requesting {}" . format ( save_url ) ) response = requests . post ( save_url , * * post_kwargs ) response . raise_for_status ( ) # There are a couple ways the header can come back if 'Refresh' in response . headers : memento = str ( response . headers [ 'Refresh' ] ) . split ( ';url=' ) [ 1 ] logger . debug ( "Memento from Refresh header: {}" . format ( memento ) ) return memento if 'Location' in response . headers : memento = response . headers [ 'Location' ] logger . debug ( "Memento from Location header: {}" . format ( memento ) ) return memento logger . debug ( "Memento not found in response headers. Inspecting history." ) for i , r in enumerate ( response . history ) : logger . debug ( "Inspecting history request #{}" . format ( i ) ) logger . debug ( r . headers ) if 'Location' in r . headers : memento = r . headers [ 'Location' ] logger . debug ( "Memento from the Location header of {} history response: {}" . format ( i + 1 , memento ) ) return memento # If there's nothing at this point, throw an error logger . error ( "No memento returned by archive.is" ) logger . error ( "Status code: {}" . format ( response . status_code ) ) logger . error ( response . headers ) logger . error ( response . text ) raise Exception ( "No memento returned by archive.is" )
7714	def add_item ( self , jid , name = None , groups = None , callback = None , error_callback = None ) : # pylint: disable=R0913 if jid in self . roster : raise ValueError ( "{0!r} already in the roster" . format ( jid ) ) item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
1063	def readheaders ( self ) : self . dict = { } self . unixfrom = '' self . headers = lst = [ ] self . status = '' headerseen = "" firstline = 1 startofline = unread = tell = None if hasattr ( self . fp , 'unread' ) : unread = self . fp . unread elif self . seekable : tell = self . fp . tell while 1 : if tell : try : startofline = tell ( ) except IOError : startofline = tell = None self . seekable = 0 line = self . fp . readline ( ) if not line : self . status = 'EOF in headers' break # Skip unix From name time lines if firstline and line . startswith ( 'From ' ) : self . unixfrom = self . unixfrom + line continue firstline = 0 if headerseen and line [ 0 ] in ' \t' : # It's a continuation line. lst . append ( line ) x = ( self . dict [ headerseen ] + "\n " + line . strip ( ) ) self . dict [ headerseen ] = x . strip ( ) continue elif self . iscomment ( line ) : # It's a comment. Ignore it. continue elif self . islast ( line ) : # Note! No pushback here! The delimiter line gets eaten. break headerseen = self . isheader ( line ) if headerseen : # It's a legal header line, save it. lst . append ( line ) self . dict [ headerseen ] = line [ len ( headerseen ) + 1 : ] . strip ( ) continue elif headerseen is not None : # An empty header name. These aren't allowed in HTTP, but it's # probably a benign mistake. Don't add the header, just keep # going. continue else : # It's not a header line; throw it back and stop here. if not self . dict : self . status = 'No headers' else : self . status = 'Non-header line where header expected' # Try to undo the read. if unread : unread ( line ) elif tell : self . fp . seek ( startofline ) else : self . status = self . status + '; bad seek' break
4397	def adsSyncAddDeviceNotificationReqEx ( port , adr , data_name , pNoteAttrib , callback , user_handle = None ) : # type: (int, AmsAddr, str, NotificationAttrib, Callable, int) -> Tuple[int, int] global callback_store if NOTEFUNC is None : raise TypeError ( "Callback function type can't be None" ) adsSyncAddDeviceNotificationReqFct = _adsDLL . AdsSyncAddDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) hnl = adsSyncReadWriteReqEx2 ( port , adr , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING ) nIndexGroup = ctypes . c_ulong ( ADSIGRP_SYM_VALBYHND ) nIndexOffset = ctypes . c_ulong ( hnl ) attrib = pNoteAttrib . notificationAttribStruct ( ) pNotification = ctypes . c_ulong ( ) nHUser = ctypes . c_ulong ( hnl ) if user_handle is not None : nHUser = ctypes . c_ulong ( user_handle ) adsSyncAddDeviceNotificationReqFct . argtypes = [ ctypes . c_ulong , ctypes . POINTER ( SAmsAddr ) , ctypes . c_ulong , ctypes . c_ulong , ctypes . POINTER ( SAdsNotificationAttrib ) , NOTEFUNC , ctypes . c_ulong , ctypes . POINTER ( ctypes . c_ulong ) , ] adsSyncAddDeviceNotificationReqFct . restype = ctypes . c_long def wrapper ( addr , notification , user ) : # type: (AmsAddr, SAdsNotificationHeader, int) -> Callable[[SAdsNotificationHeader, str], None] return callback ( notification , data_name ) c_callback = NOTEFUNC ( wrapper ) err_code = adsSyncAddDeviceNotificationReqFct ( port , pAmsAddr , nIndexGroup , nIndexOffset , ctypes . byref ( attrib ) , c_callback , nHUser , ctypes . byref ( pNotification ) , ) if err_code : raise ADSError ( err_code ) callback_store [ pNotification . value ] = c_callback return ( pNotification . value , hnl )
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
7471	def build_tmp_h5 ( data , samples ) : ## get samples and names, sorted snames = [ i . name for i in samples ] snames . sort ( ) ## Build an array for quickly indexing consens reads from catg files. ## save as a npy int binary file. uhandle = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) ## send as first async1 job get_seeds_and_hits ( uhandle , bseeds , snames )
4966	def clean ( self ) : cleaned_data = super ( ManageLearnersForm , self ) . clean ( ) # Here we take values from `data` (and not `cleaned_data`) as we need raw values - field clean methods # might "invalidate" the value and set it to None, while all we care here is if it was provided at all or not email_or_username = self . data . get ( self . Fields . EMAIL_OR_USERNAME , None ) bulk_upload_csv = self . files . get ( self . Fields . BULK_UPLOAD , None ) if not email_or_username and not bulk_upload_csv : raise ValidationError ( ValidationMessages . NO_FIELDS_SPECIFIED ) if email_or_username and bulk_upload_csv : raise ValidationError ( ValidationMessages . BOTH_FIELDS_SPECIFIED ) if email_or_username : mode = self . Modes . MODE_SINGULAR else : mode = self . Modes . MODE_BULK cleaned_data [ self . Fields . MODE ] = mode cleaned_data [ self . Fields . NOTIFY ] = self . clean_notify ( ) self . _validate_course ( ) self . _validate_program ( ) if self . data . get ( self . Fields . PROGRAM , None ) and self . data . get ( self . Fields . COURSE , None ) : raise ValidationError ( ValidationMessages . COURSE_AND_PROGRAM_ERROR ) return cleaned_data
4510	def get ( name = None ) : if name is None or name == 'default' : return _DEFAULT_PALETTE if isinstance ( name , str ) : return PROJECT_PALETTES . get ( name ) or BUILT_IN_PALETTES . get ( name )
7828	def _new_from_xml ( cls , xmlnode ) : label = from_utf8 ( xmlnode . prop ( "label" ) ) child = xmlnode . children value = None for child in xml_element_ns_iter ( xmlnode . children , DATAFORM_NS ) : if child . name == "value" : value = from_utf8 ( child . getContent ( ) ) break if value is None : raise BadRequestProtocolError ( "No value in <option/> element" ) return cls ( value , label )
1256	def setup_hooks ( self ) : hooks = list ( ) # Checkpoint saver hook if self . saver_spec is not None and ( self . execution_type == 'single' or self . distributed_spec [ 'task_index' ] == 0 ) : self . saver_directory = self . saver_spec [ 'directory' ] hooks . append ( tf . train . CheckpointSaverHook ( checkpoint_dir = self . saver_directory , save_secs = self . saver_spec . get ( 'seconds' , None if 'steps' in self . saver_spec else 600 ) , save_steps = self . saver_spec . get ( 'steps' ) , # Either one or the other has to be set. saver = None , # None since given via 'scaffold' argument. checkpoint_basename = self . saver_spec . get ( 'basename' , 'model.ckpt' ) , scaffold = self . scaffold , listeners = None ) ) else : self . saver_directory = None # Stop at step hook # hooks.append(tf.train.StopAtStepHook( # num_steps=???, # This makes more sense, if load and continue training. # last_step=None # Either one or the other has to be set. # )) # # Step counter hook # hooks.append(tf.train.StepCounterHook( # every_n_steps=counter_config.get('steps', 100), # Either one or the other has to be set. # every_n_secs=counter_config.get('secs'), # Either one or the other has to be set. # output_dir=None, # None since given via 'summary_writer' argument. # summary_writer=summary_writer # )) # Other available hooks: # tf.train.FinalOpsHook(final_ops, final_ops_feed_dict=None) # tf.train.GlobalStepWaiterHook(wait_until_step) # tf.train.LoggingTensorHook(tensors, every_n_iter=None, every_n_secs=None) # tf.train.NanTensorHook(loss_tensor, fail_on_nan_loss=True) # tf.train.ProfilerHook(save_steps=None, save_secs=None, output_dir='', show_dataflow=True, show_memory=False) return hooks
5040	def get_users_by_email ( cls , emails ) : users = User . objects . filter ( email__in = emails ) present_emails = users . values_list ( 'email' , flat = True ) missing_emails = list ( set ( emails ) - set ( present_emails ) ) return users , missing_emails
7334	async def _chunked_upload ( self , media , media_size , path = None , media_type = None , media_category = None , chunk_size = 2 ** 20 , * * params ) : if isinstance ( media , bytes ) : media = io . BytesIO ( media ) chunk = media . read ( chunk_size ) is_coro = asyncio . iscoroutine ( chunk ) if is_coro : chunk = await chunk if media_type is None : media_metadata = await utils . get_media_metadata ( chunk , path ) media_type , media_category = media_metadata elif media_category is None : media_category = utils . get_category ( media_type ) response = await self . upload . media . upload . post ( command = "INIT" , total_bytes = media_size , media_type = media_type , media_category = media_category , * * params ) media_id = response [ 'media_id' ] i = 0 while chunk : if is_coro : req = self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk , _ = await asyncio . gather ( media . read ( chunk_size ) , req ) else : await self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk = media . read ( chunk_size ) i += 1 status = await self . upload . media . upload . post ( command = "FINALIZE" , media_id = media_id ) if 'processing_info' in status : while status [ 'processing_info' ] . get ( 'state' ) != "succeeded" : processing_info = status [ 'processing_info' ] if processing_info . get ( 'state' ) == "failed" : error = processing_info . get ( 'error' , { } ) message = error . get ( 'message' , str ( status ) ) raise exceptions . MediaProcessingError ( data = status , message = message , * * params ) delay = processing_info [ 'check_after_secs' ] await asyncio . sleep ( delay ) status = await self . upload . media . upload . get ( command = "STATUS" , media_id = media_id , * * params ) return response
2320	def check_cuda_devices ( ) : import ctypes # Some constants taken from cuda.h CUDA_SUCCESS = 0 libnames = ( 'libcuda.so' , 'libcuda.dylib' , 'cuda.dll' ) for libname in libnames : try : cuda = ctypes . CDLL ( libname ) except OSError : continue else : break else : # raise OSError("could not load any of: " + ' '.join(libnames)) return 0 nGpus = ctypes . c_int ( ) error_str = ctypes . c_char_p ( ) result = cuda . cuInit ( 0 ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) # print("cuInit failed with error code %d: %s" % (result, error_str.value.decode())) return 0 result = cuda . cuDeviceGetCount ( ctypes . byref ( nGpus ) ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) # print("cuDeviceGetCount failed with error code %d: %s" % (result, error_str.value.decode())) return 0 # print("Found %d device(s)." % nGpus.value) return nGpus . value
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
2008	def _deserialize_int ( data , nbytes = 32 , padding = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True ) value = Operators . SEXTEND ( value , nbytes * 8 , ( nbytes + padding ) * 8 ) if not issymbolic ( value ) : # sign bit on if value & ( 1 << ( nbytes * 8 - 1 ) ) : value = - ( ( ( ~ value ) + 1 ) & ( ( 1 << ( nbytes * 8 ) ) - 1 ) ) return value
13810	def MakeDescriptor ( desc_proto , package = '' , build_file_if_cpp = True , syntax = None ) : if api_implementation . Type ( ) == 'cpp' and build_file_if_cpp : # The C++ implementation requires all descriptors to be backed by the same # definition in the C++ descriptor pool. To do this, we build a # FileDescriptorProto with the same definition as this descriptor and build # it into the pool. from typy . google . protobuf import descriptor_pb2 file_descriptor_proto = descriptor_pb2 . FileDescriptorProto ( ) file_descriptor_proto . message_type . add ( ) . MergeFrom ( desc_proto ) # Generate a random name for this proto file to prevent conflicts with any # imported ones. We need to specify a file name so the descriptor pool # accepts our FileDescriptorProto, but it is not important what that file # name is actually set to. proto_name = str ( uuid . uuid4 ( ) ) if package : file_descriptor_proto . name = os . path . join ( package . replace ( '.' , '/' ) , proto_name + '.proto' ) file_descriptor_proto . package = package else : file_descriptor_proto . name = proto_name + '.proto' _message . default_pool . Add ( file_descriptor_proto ) result = _message . default_pool . FindFileByName ( file_descriptor_proto . name ) if _USE_C_DESCRIPTORS : return result . message_types_by_name [ desc_proto . name ] full_message_name = [ desc_proto . name ] if package : full_message_name . insert ( 0 , package ) # Create Descriptors for enum types enum_types = { } for enum_proto in desc_proto . enum_type : full_name = '.' . join ( full_message_name + [ enum_proto . name ] ) enum_desc = EnumDescriptor ( enum_proto . name , full_name , None , [ EnumValueDescriptor ( enum_val . name , ii , enum_val . number ) for ii , enum_val in enumerate ( enum_proto . value ) ] ) enum_types [ full_name ] = enum_desc # Create Descriptors for nested types nested_types = { } for nested_proto in desc_proto . nested_type : full_name = '.' . join ( full_message_name + [ nested_proto . name ] ) # Nested types are just those defined inside of the message, not all types # used by fields in the message, so no loops are possible here. nested_desc = MakeDescriptor ( nested_proto , package = '.' . join ( full_message_name ) , build_file_if_cpp = False , syntax = syntax ) nested_types [ full_name ] = nested_desc fields = [ ] for field_proto in desc_proto . field : full_name = '.' . join ( full_message_name + [ field_proto . name ] ) enum_desc = None nested_desc = None if field_proto . HasField ( 'type_name' ) : type_name = field_proto . type_name full_type_name = '.' . join ( full_message_name + [ type_name [ type_name . rfind ( '.' ) + 1 : ] ] ) if full_type_name in nested_types : nested_desc = nested_types [ full_type_name ] elif full_type_name in enum_types : enum_desc = enum_types [ full_type_name ] # Else type_name references a non-local type, which isn't implemented field = FieldDescriptor ( field_proto . name , full_name , field_proto . number - 1 , field_proto . number , field_proto . type , FieldDescriptor . ProtoTypeToCppProtoType ( field_proto . type ) , field_proto . label , None , nested_desc , enum_desc , None , False , None , options = field_proto . options , has_default_value = False ) fields . append ( field ) desc_name = '.' . join ( full_message_name ) return Descriptor ( desc_proto . name , desc_name , None , None , fields , list ( nested_types . values ( ) ) , list ( enum_types . values ( ) ) , [ ] , options = desc_proto . options )
11959	def _check_nm ( nm , notation ) : # Convert to decimal, and check if it's in the list of valid netmasks. _NM_CHECK_FUNCT = { NM_DOT : _dot_to_dec , NM_HEX : _hex_to_dec , NM_BIN : _bin_to_dec , NM_OCT : _oct_to_dec , NM_DEC : _dec_to_dec_long } try : dec = _NM_CHECK_FUNCT [ notation ] ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
336	def run_model ( model , returns_train , returns_test = None , bmark = None , samples = 500 , ppc = False , progressbar = True ) : if model == 'alpha_beta' : model , trace = model_returns_t_alpha_beta ( returns_train , bmark , samples , progressbar = progressbar ) elif model == 't' : model , trace = model_returns_t ( returns_train , samples , progressbar = progressbar ) elif model == 'normal' : model , trace = model_returns_normal ( returns_train , samples , progressbar = progressbar ) elif model == 'best' : model , trace = model_best ( returns_train , returns_test , samples = samples , progressbar = progressbar ) else : raise NotImplementedError ( 'Model {} not found.' 'Use alpha_beta, t, normal, or best.' . format ( model ) ) if ppc : ppc_samples = pm . sample_ppc ( trace , samples = samples , model = model , size = len ( returns_test ) , progressbar = progressbar ) return trace , ppc_samples [ 'returns' ] return trace
4608	def nolist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ ] , account = self )
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
4370	def emit ( self , event , * args , * * kwargs ) : callback = kwargs . pop ( 'callback' , None ) if kwargs : raise ValueError ( "emit() only supports positional argument, to stay " "compatible with the Socket.IO protocol. You can " "however pass in a dictionary as the first argument" ) pkt = dict ( type = "event" , name = event , args = args , endpoint = self . ns_name ) if callback : # By passing 'data', we indicate that we *want* an explicit ack # by the client code, not an automatic as with send(). pkt [ 'ack' ] = 'data' pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
1285	def footnote_item ( self , key , text ) : back = ( '<a href="#fnref-%s" class="footnote">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id="fn-%s">%s</li>\n' % ( escape ( key ) , text ) return html
12732	def join ( self , shape , body_a , body_b = None , name = None , * * kwargs ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) shape = shape . lower ( ) if name is None : name = '{}^{}^{}' . format ( ba . name , shape , bb . name if bb else '' ) self . _joints [ name ] = Joint . build ( shape , name , self , body_a = ba , body_b = bb , * * kwargs ) return self . _joints [ name ]
12042	def XMLtoPython ( xmlStr = r"C:\Apps\pythonModules\GSTemp.xml" ) : #TODO: this absolute file path crazy stuff needs to stop! if os . path . exists ( xmlStr ) : with open ( xmlStr ) as f : xmlStr = f . read ( ) print ( xmlStr ) print ( "DONE" ) return
2732	def get_records ( self , params = None ) : if params is None : params = { } # URL https://api.digitalocean.com/v2/domains/[NAME]/records/ records = [ ] data = self . get_data ( "domains/%s/records/" % self . name , type = GET , params = params ) for record_data in data [ 'domain_records' ] : record = Record ( domain_name = self . name , * * record_data ) record . token = self . token records . append ( record ) return records
3929	def get_auth ( credentials_prompt , refresh_token_cache , manual_login = False ) : with requests . Session ( ) as session : session . headers = { 'user-agent' : USER_AGENT } try : logger . info ( 'Authenticating with refresh token' ) refresh_token = refresh_token_cache . get ( ) if refresh_token is None : raise GoogleAuthError ( "Refresh token not found" ) access_token = _auth_with_refresh_token ( session , refresh_token ) except GoogleAuthError as e : logger . info ( 'Failed to authenticate using refresh token: %s' , e ) logger . info ( 'Authenticating with credentials' ) if manual_login : authorization_code = ( credentials_prompt . get_authorization_code ( ) ) else : authorization_code = _get_authorization_code ( session , credentials_prompt ) access_token , refresh_token = _auth_with_code ( session , authorization_code ) refresh_token_cache . set ( refresh_token ) logger . info ( 'Authentication successful' ) return _get_session_cookies ( session , access_token )
12251	def get_all_keys ( self , * args , * * kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 0 ] if len ( args ) else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_all_keys ( * args , * * kwargs )
11520	def search ( self , search , token = None ) : parameters = dict ( ) parameters [ 'search' ] = search if token : parameters [ 'token' ] = token response = self . request ( 'midas.resource.search' , parameters ) return response
8796	def serialize_groups ( self , groups ) : rules = [ ] for group in groups : rules . extend ( self . serialize_rules ( group . rules ) ) return rules
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
9697	def check_nonce ( self , request , oauth_request ) : oauth_nonce = oauth_request [ 'oauth_nonce' ] oauth_timestamp = oauth_request [ 'oauth_timestamp' ] return check_nonce ( request , oauth_request , oauth_nonce , oauth_timestamp )
11556	def enable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
5092	def _login ( self , email , password ) : response = requests . post ( urljoin ( self . ENDPOINT , 'sessions' ) , json = { 'email' : email , 'password' : password , 'platform' : 'ios' , 'token' : binascii . hexlify ( os . urandom ( 64 ) ) . decode ( 'utf8' ) } , headers = self . _headers ) response . raise_for_status ( ) access_token = response . json ( ) [ 'access_token' ] self . _headers [ 'Authorization' ] = 'Token token=%s' % access_token
11318	def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
9824	def update ( ctx , name , description , tags , private ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description if private is not None : update_dict [ 'is_public' ] = not private tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . update_project ( user , project_name , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project updated." ) get_project_details ( response )
4259	def read_markdown ( filename ) : global MD # Use utf-8-sig codec to remove BOM if it is present. This is only possible # this way prior to feeding the text to the markdown parser (which would # also default to pure utf-8) with open ( filename , 'r' , encoding = 'utf-8-sig' ) as f : text = f . read ( ) if MD is None : MD = Markdown ( extensions = [ 'markdown.extensions.meta' , 'markdown.extensions.tables' ] , output_format = 'html5' ) else : MD . reset ( ) # When https://github.com/Python-Markdown/markdown/pull/672 # will be available, this can be removed. MD . Meta = { } # Mark HTML with Markup to prevent jinja2 autoescaping output = { 'description' : Markup ( MD . convert ( text ) ) } try : meta = MD . Meta . copy ( ) except AttributeError : pass else : output [ 'meta' ] = meta try : output [ 'title' ] = MD . Meta [ 'title' ] [ 0 ] except KeyError : pass return output
10933	def check_completion ( self ) : terminate = False term_dict = self . get_termination_stats ( get_cos = self . costol is not None ) terminate |= np . all ( np . abs ( term_dict [ 'delta_vals' ] ) < self . paramtol ) terminate |= ( term_dict [ 'delta_err' ] < self . errtol ) terminate |= ( term_dict [ 'exp_err' ] < self . exptol ) terminate |= ( term_dict [ 'frac_err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term_dict [ 'model_cosine' ] ) return terminate
7486	def run_cutadapt ( data , subsamples , lbview ) : ## choose cutadapt function based on datatype start = time . time ( ) printstr = " processing reads | {} | s2 |" finished = 0 rawedits = { } ## sort subsamples so that the biggest files get submitted first subsamples . sort ( key = lambda x : x . stats . reads_raw , reverse = True ) LOGGER . info ( [ i . stats . reads_raw for i in subsamples ] ) ## send samples to cutadapt filtering if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_single , * ( data , sample ) ) ## wait for all to finish while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break ## collect results, report failures, and store stats. async = sample.name for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) ## if single cleanup is easy if "pair" not in data . paramsdict [ "datatype" ] : parse_single_results ( data , data . samples [ async ] , res ) else : parse_pair_results ( data , data . samples [ async ] , res ) else : print ( " found an error in step2; see ipyrad_log.txt" ) LOGGER . error ( "error in run_cutadapt(): %s" , rawedits [ async ] . exception ( ) )
7241	def aoi ( self , * * kwargs ) : g = self . _parse_geoms ( * * kwargs ) if g is None : return self else : return self [ g ]
8640	def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1101	def context_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : prefix = dict ( insert = '+ ' , delete = '- ' , replace = '! ' , equal = ' ' ) started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True # fromdate = '\t{}'.format(fromfiledate) if fromfiledate else '' fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' # todate = '\t{}'.format(tofiledate) if tofiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' # yield '*** {}{}{}'.format(fromfile, fromdate, lineterm) yield '*** %s%s%s' % ( fromfile , fromdate , lineterm ) # yield '--- {}{}{}'.format(tofile, todate, lineterm) yield '--- %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] yield '***************' + lineterm file1_range = _format_range_context ( first [ 1 ] , last [ 2 ] ) # yield '*** {} ****{}'.format(file1_range, lineterm) yield '*** %s ****%s' % ( file1_range , lineterm ) if any ( tag in ( 'replace' , 'delete' ) for tag , _ , _ , _ , _ in group ) : for tag , i1 , i2 , _ , _ in group : if tag != 'insert' : for line in a [ i1 : i2 ] : yield prefix [ tag ] + line file2_range = _format_range_context ( first [ 3 ] , last [ 4 ] ) # yield '--- {} ----{}'.format(file2_range, lineterm) yield '--- %s ----%s' % ( file2_range , lineterm ) if any ( tag in ( 'replace' , 'insert' ) for tag , _ , _ , _ , _ in group ) : for tag , _ , _ , j1 , j2 in group : if tag != 'delete' : for line in b [ j1 : j2 ] : yield prefix [ tag ] + line
11275	def check_pid ( pid , debug ) : try : # A Kill of 0 is to check if the PID is active. It won't kill the process os . kill ( pid , 0 ) if debug > 1 : print ( "Script has a PIDFILE where the process is still running" ) return True except OSError : if debug > 1 : print ( "Script does not appear to be running" ) return False
4828	def is_enrolled ( self , username , course_run_id ) : enrollment = self . get_course_enrollment ( username , course_run_id ) return enrollment is not None and enrollment . get ( 'is_active' , False )
2062	def declarations ( self ) : declarations = GetDeclarations ( ) for a in self . constraints : try : declarations . visit ( a ) except RuntimeError : # TODO: (defunct) move recursion management out of PickleSerializer if sys . getrecursionlimit ( ) >= PickleSerializer . MAX_RECURSION : raise Exception ( f'declarations recursion limit surpassed {PickleSerializer.MAX_RECURSION}, aborting' ) new_limit = sys . getrecursionlimit ( ) + PickleSerializer . DEFAULT_RECURSION if new_limit <= PickleSerializer . DEFAULT_RECURSION : sys . setrecursionlimit ( new_limit ) return self . declarations return declarations . result
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : # n.b. gevent will monkey patch time . sleep ( interval_seconds ) slept = True break if not slept : break
8395	def show_help ( ) : print ( """\ Manage local config files from masters in edx_lint. Commands: """ ) for cmd in [ write_main , check_main , list_main ] : print ( cmd . __doc__ . lstrip ( "\n" ) )
1145	def enterabs ( self , time , priority , action , argument ) : event = Event ( time , priority , action , argument ) heapq . heappush ( self . _queue , event ) return event
1380	def get_version_number ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : for line in release_info : trunks = line [ : - 1 ] . split ( ' ' ) if trunks [ 0 ] == 'heron.build.version' : return trunks [ - 1 ] . replace ( "'" , "" ) return 'unknown'
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : # Fixes Python3 error "TypeError: Unicode-objects must be encoded before hashing". h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
7873	def get_all_payload ( self , specialize = False ) : if self . _payload is None : self . decode_payload ( specialize ) elif specialize : for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ i ] = payload return list ( self . _payload )
1061	def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
9413	def _setup_log ( ) : try : handler = logging . StreamHandler ( stream = sys . stdout ) except TypeError : # pragma: no cover handler = logging . StreamHandler ( strm = sys . stdout ) log = get_log ( ) log . addHandler ( handler ) log . setLevel ( logging . INFO ) log . propagate = False
2118	def convert ( self , value , param , ctx ) : resource = tower_cli . get_resource ( self . resource_name ) # Ensure that None is passed through without trying to # do anything. if value is None : return None # If we were already given an integer, do nothing. # This ensures that the convert method is idempotent. if isinstance ( value , int ) : return value # Do we have a string that contains only digits? # If so, then convert it to an integer and return it. if re . match ( r'^[\d]+$' , value ) : return int ( value ) # Special case to allow disassociations if value == 'null' : return value # Okay, we have a string. Try to do a name-based lookup on the # resource, and return back the ID that we get from that. # # This has the chance of erroring out, which is fine. try : debug . log ( 'The %s field is given as a name; ' 'looking it up.' % param . name , header = 'details' ) lookup_data = { resource . identity [ - 1 ] : value } rel = resource . get ( * * lookup_data ) except exc . MultipleResults : raise exc . MultipleRelatedError ( 'Cannot look up {0} exclusively by name, because multiple {0} ' 'objects exist with that name.\n' 'Please send an ID. You can get the ID for the {0} you want ' 'with:\n' ' tower-cli {0} list --name "{1}"' . format ( self . resource_name , value ) , ) except exc . TowerCLIError as ex : raise exc . RelatedError ( 'Could not get %s. %s' % ( self . resource_name , str ( ex ) ) ) # Done! Return the ID. return rel [ 'id' ]
683	def getZeroedOutEncoding ( self , n ) : assert all ( field . numRecords > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) return encoding
1029	def b64decode ( s , altchars = None ) : if altchars is not None : s = s . translate ( string . maketrans ( altchars [ : 2 ] , '+/' ) ) try : return binascii . a2b_base64 ( s ) except binascii . Error , msg : # Transform this exception for consistency raise TypeError ( msg )
5217	def check_hours ( tickers , tz_exch , tz_loc = DEFAULT_TZ ) -> pd . DataFrame : cols = [ 'Trading_Day_Start_Time_EOD' , 'Trading_Day_End_Time_EOD' ] con , _ = create_connection ( ) hours = con . ref ( tickers = tickers , flds = cols ) cur_dt = pd . Timestamp ( 'today' ) . strftime ( '%Y-%m-%d ' ) hours . loc [ : , 'local' ] = hours . value . astype ( str ) . str [ : - 3 ] hours . loc [ : , 'exch' ] = pd . DatetimeIndex ( cur_dt + hours . value . astype ( str ) ) . tz_localize ( tz_loc ) . tz_convert ( tz_exch ) . strftime ( '%H:%M' ) hours = pd . concat ( [ hours . set_index ( [ 'ticker' , 'field' ] ) . exch . unstack ( ) . loc [ : , cols ] , hours . set_index ( [ 'ticker' , 'field' ] ) . local . unstack ( ) . loc [ : , cols ] , ] , axis = 1 ) hours . columns = [ 'Exch_Start' , 'Exch_End' , 'Local_Start' , 'Local_End' ] return hours
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
5623	def path_exists ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : urlopen ( path ) . info ( ) return True except HTTPError as e : if e . code == 404 : return False else : raise elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return True else : return False else : logger . debug ( "%s exists: %s" , path , os . path . exists ( path ) ) return os . path . exists ( path )
3913	def _on_event ( self , conv_event ) : if isinstance ( conv_event , hangups . ChatMessageEvent ) : self . _typing_statuses [ conv_event . user_id ] = ( hangups . TYPING_TYPE_STOPPED ) self . _update ( )
2139	def associate ( self , group , parent , * * kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _assoc ( 'children' , parent_id , group_id )
3131	def merge_results ( x , y ) : z = x . copy ( ) for key , value in y . items ( ) : if isinstance ( value , list ) and isinstance ( z . get ( key ) , list ) : z [ key ] += value else : z [ key ] = value return z
2050	def _swap_mode ( self ) : assert self . mode in ( cs . CS_MODE_ARM , cs . CS_MODE_THUMB ) if self . mode == cs . CS_MODE_ARM : self . mode = cs . CS_MODE_THUMB else : self . mode = cs . CS_MODE_ARM
11466	def ls ( self , folder = '' ) : current_folder = self . _ftp . pwd ( ) self . cd ( folder ) contents = [ ] self . _ftp . retrlines ( 'LIST' , lambda a : contents . append ( a ) ) files = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( '-' ) , contents ) folders = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( 'd' ) , contents ) files = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , files ) folders = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , folders ) self . _ftp . cwd ( current_folder ) return files , folders
11561	def play_tone ( self , pin , tone_command , frequency , duration ) : # convert the integer values to bytes if tone_command == self . TONE_TONE : # duration is specified if duration : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , duration & 0x7f , ( duration >> 7 ) & 0x7f ] else : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , 0 , 0 ] self . _command_handler . digital_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_MODE ] = self . TONE # turn off tone else : data = [ tone_command , pin ] self . _command_handler . send_sysex ( self . _command_handler . TONE_PLAY , data )
3064	def update_query_params ( uri , params ) : parts = urllib . parse . urlparse ( uri ) query_params = parse_unique_urlencoded ( parts . query ) query_params . update ( params ) new_query = urllib . parse . urlencode ( query_params ) new_parts = parts . _replace ( query = new_query ) return urllib . parse . urlunparse ( new_parts )
7935	def _auth ( self ) : if self . authenticated : self . __logger . debug ( "_auth: already authenticated" ) return self . __logger . debug ( "doing handshake..." ) hash_value = self . _compute_handshake ( ) n = common_root . newTextChild ( None , "handshake" , hash_value ) self . _write_node ( n ) n . unlinkNode ( ) n . freeNode ( ) self . __logger . debug ( "handshake hash sent." )
9803	def set ( verbose , # pylint:disable=redefined-builtin host , http_port , ws_port , use_https , verify_ssl ) : _config = GlobalConfigManager . get_config_or_default ( ) if verbose is not None : _config . verbose = verbose if host is not None : _config . host = host if http_port is not None : _config . http_port = http_port if ws_port is not None : _config . ws_port = ws_port if use_https is not None : _config . use_https = use_https if verify_ssl is False : _config . verify_ssl = verify_ssl GlobalConfigManager . set_config ( _config ) Printer . print_success ( 'Config was updated.' ) # Reset cli config CliConfigManager . purge ( )
8256	def _darkest ( self ) : min , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for clr in self : if clr . r + clr . g + clr . b < n : min , n = clr , clr . r + clr . g + clr . b return min
9886	def _call_multi_fortran_z ( self , names , data_types , rec_nums , dim_sizes , input_type_code , func , epoch = False , data_offset = None , epoch16 = False ) : # isolate input type code variables from total supplied types idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : # read all data of a given type at once max_rec = rec_nums [ idx ] . max ( ) sub_names = np . array ( names ) [ idx ] sub_sizes = dim_sizes [ idx ] status , data = func ( self . fname , sub_names . tolist ( ) , sub_sizes , sub_sizes . sum ( ) , max_rec , len ( sub_names ) ) if status == 0 : # account for quirks of CDF data storage for certain types if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset if epoch : # account for difference in seconds between # CDF epoch and python's epoch, leap year in there # (datetime(1971,1,2) - # datetime(1,1,1)).total_seconds()*1000 data -= 62167219200000 data = data . astype ( '<M8[ms]' ) if epoch16 : data [ 0 : : 2 , : ] -= 62167219200 data = data [ 0 : : 2 , : ] * 1E9 + data [ 1 : : 2 , : ] / 1.E3 data = data . astype ( 'datetime64[ns]' ) sub_sizes /= 2 # all data of a type has been loaded and tweaked as necessary # parse through returned array to break out the individual variables # as appropriate self . _process_return_multi_z ( data , sub_names , sub_sizes ) else : raise IOError ( fortran_cdf . statusreporter ( status ) )
212	def from_uint8 ( arr_uint8 , shape , min_value = 0.0 , max_value = 1.0 ) : arr_0to1 = arr_uint8 . astype ( np . float32 ) / 255.0 return HeatmapsOnImage . from_0to1 ( arr_0to1 , shape , min_value = min_value , max_value = max_value )
3769	def none_and_length_check ( all_inputs , length = None ) : if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
7327	def parse_int_list ( string ) : integers = [ ] for comma_part in string . split ( "," ) : for substring in comma_part . split ( " " ) : if len ( substring ) == 0 : continue if "-" in substring : left , right = substring . split ( "-" ) left_val = int ( left . strip ( ) ) right_val = int ( right . strip ( ) ) integers . extend ( range ( left_val , right_val + 1 ) ) else : integers . append ( int ( substring . strip ( ) ) ) return integers
3260	def get_layergroup ( self , name , workspace = None ) : layergroups = self . get_layergroups ( names = name , workspaces = workspace ) return self . _return_first_item ( layergroups )
2380	def _load_rule_file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load_source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
167	def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )
416	def find_top_dataset ( self , dataset_name = None , sort = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) d = self . db . Dataset . find_one ( filter = kwargs , sort = sort ) if d is not None : dataset_id = d [ 'dataset_id' ] else : print ( "[Database] FAIL! Cannot find dataset: {}" . format ( kwargs ) ) return False try : dataset = self . _deserialization ( self . dataset_fs . get ( dataset_id ) . read ( ) ) pc = self . db . Dataset . find ( kwargs ) print ( "[Database] Find one dataset SUCCESS, {} took: {}s" . format ( kwargs , round ( time . time ( ) - s , 2 ) ) ) # check whether more datasets match the requirement dataset_id_list = pc . distinct ( 'dataset_id' ) n_dataset = len ( dataset_id_list ) if n_dataset != 1 : print ( " Note that there are {} datasets match the requirement" . format ( n_dataset ) ) return dataset except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
8925	def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
4827	def get_course_enrollment ( self , username , course_id ) : endpoint = getattr ( self . client . enrollment , '{username},{course_id}' . format ( username = username , course_id = course_id ) ) try : result = endpoint . get ( ) except HttpNotFoundError : # This enrollment data endpoint returns a 404 if either the username or course_id specified isn't valid LOGGER . error ( 'Course enrollment details not found for invalid username or course; username=[%s], course=[%s]' , username , course_id ) return None # This enrollment data endpoint returns an empty string if the username and course_id is valid, but there's # no matching enrollment found if not result : LOGGER . info ( 'Failed to find course enrollment details for user [%s] and course [%s]' , username , course_id ) return None return result
3278	def get_resource_inst ( self , path , environ ) : _logger . info ( "get_resource_inst('%s')" % path ) self . _count_get_resource_inst += 1 root = RootCollection ( environ ) return root . resolve ( "" , path )
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) # TODO: we should check if Python is at least 3.7 and sort by kwargs # keys otherwise. Should we use hash_data for key generation if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : # nocover # We can sort because they keys are gaurenteed to be strings kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
7979	def _try_auth ( self ) : if self . authenticated : self . __logger . debug ( "try_auth: already authenticated" ) return self . __logger . debug ( "trying auth: %r" % ( self . _auth_methods_left , ) ) if not self . _auth_methods_left : raise LegacyAuthenticationError ( "No allowed authentication methods available" ) method = self . _auth_methods_left [ 0 ] if method . startswith ( "sasl:" ) : return ClientStream . _try_auth ( self ) elif method not in ( "plain" , "digest" ) : self . _auth_methods_left . pop ( 0 ) self . __logger . debug ( "Skipping unknown auth method: %s" % method ) return self . _try_auth ( ) elif self . available_auth_methods is not None : if method in self . available_auth_methods : self . _auth_methods_left . pop ( 0 ) self . auth_method_used = method if method == "digest" : self . _digest_auth_stage2 ( self . auth_stanza ) else : self . _plain_auth_stage2 ( self . auth_stanza ) self . auth_stanza = None return else : self . __logger . debug ( "Skipping unavailable auth method: %s" % method ) else : self . _auth_stage1 ( )
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
7886	def emit_stanza ( self , element ) : if not self . _head_emitted : raise RuntimeError ( ".emit_head() must be called first." ) string = self . _emit_element ( element , level = 1 , declared_prefixes = self . _root_prefixes ) return remove_evil_characters ( string )
12292	def annotate_metadata_code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching_files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching_files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( OrderedDict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess_type ( absf ) [ 0 ] ) , ( 'sha256' , compute_sha256 ( absf ) ) ] ) )
11665	def _build_indices ( X , flann_args ) : # TODO: should probably multithread this logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( * * flann_args ) idx . build_index ( bag ) return indices
4635	def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive_from_seed ( s )
10974	def new ( ) : form = GroupForm ( request . form ) if form . validate_on_submit ( ) : try : group = Group . create ( admins = [ current_user ] , * * form . data ) flash ( _ ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) except IntegrityError : flash ( _ ( 'Group creation failure' ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , )
4661	def broadcast ( self , tx = None ) : if tx : # If tx is provided, we broadcast the tx return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
5721	def _convert_schemas ( mapping , schemas ) : schemas = deepcopy ( schemas ) for schema in schemas : for fk in schema . get ( 'foreignKeys' , [ ] ) : resource = fk [ 'reference' ] [ 'resource' ] if resource != 'self' : if resource not in mapping : message = 'Not resource "%s" for foreign key "%s"' message = message % ( resource , fk ) raise ValueError ( message ) fk [ 'reference' ] [ 'resource' ] = mapping [ resource ] return schemas
9893	def _uptime_plan9 ( ) : # Apparently Plan 9 only has Python 2.2, which I'm not prepared to # support. Maybe some Linuxes implement /dev/time, though, someone was # talking about it somewhere. try : # The time file holds one 32-bit number representing the sec- # onds since start of epoch and three 64-bit numbers, repre- # senting nanoseconds since start of epoch, clock ticks, and # clock frequency. # -- cons(3) f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
7500	def get_spans ( maparr , spans ) : ## start at 0, finds change at 1-index of map file bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) ## read through marr and record when locus id changes for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans
7727	def get_items ( self ) : if not self . xmlnode . children : return [ ] ret = [ ] n = self . xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != self . ns : pass elif n . name == "item" : ret . append ( MucItem ( n ) ) elif n . name == "status" : ret . append ( MucStatus ( n ) ) # FIXME: alt,decline,invite,password n = n . next return ret
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
11040	def write ( self , path , * * data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
1140	def fill ( text , width = 70 , * * kwargs ) : w = TextWrapper ( width = width , * * kwargs ) return w . fill ( text )
4913	def courses ( self , request , pk = None ) : # pylint: disable=invalid-name,unused-argument enterprise_customer = self . get_object ( ) self . check_object_permissions ( request , enterprise_customer ) self . ensure_data_exists ( request , enterprise_customer . catalog , error_message = "No catalog is associated with Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) # We have handled potential error cases and are now ready to call out to the Catalog API. catalog_api = CourseCatalogApiClient ( request . user , enterprise_customer . site ) courses = catalog_api . get_paginated_catalog_courses ( enterprise_customer . catalog , request . GET ) # An empty response means that there was a problem fetching data from Catalog API, since # a Catalog with no courses has a non empty response indicating that there are no courses. self . ensure_data_exists ( request , courses , error_message = ( "Unable to fetch API response for catalog courses for " "Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) ) serializer = serializers . EnterpriseCatalogCoursesReadOnlySerializer ( courses ) # Add enterprise related context for the courses. serializer . update_enterprise_courses ( enterprise_customer , catalog_id = enterprise_customer . catalog ) return get_paginated_response ( serializer . data , request )
11624	def from_devanagari ( self , data ) : from indic_transliteration import sanscript return sanscript . transliterate ( data = data , _from = sanscript . DEVANAGARI , _to = self . name )
13480	def _str_replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
2641	def shutdown ( self ) : self . is_alive = False logging . debug ( "Waking management thread" ) self . incoming_q . put ( None ) # Wake up the thread self . _queue_management_thread . join ( ) # Force join logging . debug ( "Exiting thread" ) self . worker . join ( ) return True
6309	def draw ( self , time , frametime , target ) : for effect in self . effects : value = effect . rocket_timeline_track . time_value ( time ) if value > 0.5 : effect . draw ( time , frametime , target )
2098	def relaunch ( self , pk = None , * * kwargs ) : # Search for the record if pk not given if not pk : existing_data = self . get ( * * kwargs ) pk = existing_data [ 'id' ] relaunch_endpoint = '%s%s/relaunch/' % ( self . endpoint , pk ) data = { } # Attempt to relaunch the job. answer = { } try : result = client . post ( relaunch_endpoint , data = data ) . json ( ) if 'id' in result : answer . update ( result ) answer [ 'changed' ] = True except exc . MethodNotAllowed : answer [ 'changed' ] = False # Return the answer. return answer
5648	def _write_stop_to_stop_network_edges ( net , file_name , data = True , fmt = None ) : if fmt is None : fmt = "edg" if fmt == "edg" : if data : networkx . write_edgelist ( net , file_name , data = True ) else : networkx . write_edgelist ( net , file_name ) elif fmt == "csv" : with open ( file_name , 'w' ) as f : # writing out the header edge_iter = net . edges_iter ( data = True ) _ , _ , edg_data = next ( edge_iter ) edg_data_keys = list ( sorted ( edg_data . keys ( ) ) ) header = ";" . join ( [ "from_stop_I" , "to_stop_I" ] + edg_data_keys ) f . write ( header ) for from_node_I , to_node_I , data in net . edges_iter ( data = True ) : f . write ( "\n" ) values = [ str ( from_node_I ) , str ( to_node_I ) ] data_values = [ ] for key in edg_data_keys : if key == "route_I_counts" : route_I_counts_string = str ( data [ key ] ) . replace ( " " , "" ) [ 1 : - 1 ] data_values . append ( route_I_counts_string ) else : data_values . append ( str ( data [ key ] ) ) all_values = values + data_values f . write ( ";" . join ( all_values ) )
12139	def from_pattern ( cls , pattern , filetype = None , key = 'filename' , root = None , ignore = [ ] ) : filepattern = FilePattern ( key , pattern , root = root ) if FileInfo . filetype and filetype is None : filetype = FileInfo . filetype elif filetype is None : raise Exception ( "The filetype argument must be supplied unless " "an appropriate default has been specified as " "FileInfo.filetype" ) return FileInfo ( filepattern , key , filetype , ignore = ignore )
9279	def from_decimal ( number , width = 1 ) : text = [ ] if not isinstance ( number , int_type ) : raise TypeError ( "Expected number to be int, got %s" , type ( number ) ) elif not isinstance ( width , int_type ) : raise TypeError ( "Expected width to be int, got %s" , type ( number ) ) elif number < 0 : raise ValueError ( "Expected number to be positive integer" ) elif number > 0 : max_n = ceil ( log ( number ) / log ( 91 ) ) for n in _range ( int ( max_n ) , - 1 , - 1 ) : quotient , number = divmod ( number , 91 ** n ) text . append ( chr ( 33 + quotient ) ) return "" . join ( text ) . lstrip ( '!' ) . rjust ( max ( 1 , width ) , '!' )
10322	def spanning_1d_chain ( length ) : ret = nx . grid_graph ( dim = [ int ( length + 2 ) ] ) ret . node [ 0 ] [ 'span' ] = 0 ret [ 0 ] [ 1 ] [ 'span' ] = 0 ret . node [ length + 1 ] [ 'span' ] = 1 ret [ length ] [ length + 1 ] [ 'span' ] = 1 return ret
11690	def get_area ( self , geojson ) : geojson = json . load ( open ( geojson , 'r' ) ) self . area = Polygon ( geojson [ 'features' ] [ 0 ] [ 'geometry' ] [ 'coordinates' ] [ 0 ] )
13538	def get_location ( self , location_id ) : url = "/2/locations/%s" % location_id return self . location_from_json ( self . _get_resource ( url ) [ "location" ] )
12768	def load_skeleton ( self , filename , pid_params = None ) : self . skeleton = skeleton . Skeleton ( self ) self . skeleton . load ( filename , color = ( 0.3 , 0.5 , 0.9 , 0.8 ) ) if pid_params : self . skeleton . set_pid_params ( * * pid_params ) self . skeleton . erp = 0.1 self . skeleton . cfm = 0
3925	def _update_tabs ( self ) : text = [ ] for num , widget in enumerate ( self . _widgets ) : palette = ( 'active_tab' if num == self . _tab_index else 'inactive_tab' ) text += [ ( palette , ' {} ' . format ( self . _widget_title [ widget ] ) ) , ( 'tab_background' , ' ' ) , ] self . _tabs . set_text ( text ) self . _frame . contents [ 'body' ] = ( self . _widgets [ self . _tab_index ] , None )
1120	def pformat ( o , indent = 1 , width = 80 , depth = None ) : return PrettyPrinter ( indent = indent , width = width , depth = depth ) . pformat ( o )
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) # Based on what Hangouts for Chrome does over 2 requests, this is # trimmed down to 1 request that includes the bare minimum to make # things work. services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
8980	def temporary_path ( self , extension = "" ) : path = NamedTemporaryFile ( delete = False , suffix = extension ) . name self . path ( path ) return path
3387	def _is_redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility_tol # Avoid zero variances extra_col = matrix [ : , 0 ] + 1 # Avoid zero rows being correlated with constant rows extra_col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c_ [ matrix , extra_col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
13542	def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
13254	def tz ( self ) : if not self . _tz : self . _tz = tzlocal . get_localzone ( ) . zone return self . _tz
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) # TODO: better integration if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
8609	def list_resources ( self , resource_type = None , depth = 1 ) : if resource_type is not None : response = self . _perform_request ( '/um/resources/%s?depth=%s' % ( resource_type , str ( depth ) ) ) else : response = self . _perform_request ( '/um/resources?depth=' + str ( depth ) ) return response
10955	def get_update_io_tiles ( self , params , values ) : # get the affected area of the model image otile = self . get_update_tile ( params , values ) if otile is None : return [ None ] * 3 ptile = self . get_padding_size ( otile ) or util . Tile ( 0 , dim = otile . dim ) otile = util . Tile . intersection ( otile , self . oshape ) if ( otile . shape <= 0 ) . any ( ) : raise UpdateError ( "update triggered invalid tile size" ) if ( ptile . shape < 0 ) . any ( ) or ( ptile . shape > self . oshape . shape ) . any ( ) : raise UpdateError ( "update triggered invalid padding tile size" ) # now remove the part of the tile that is outside the image and pad the # interior part with that overhang. reflect the necessary padding back # into the image itself for the outer slice which we will call outer outer = otile . pad ( ( ptile . shape + 1 ) // 2 ) inner , outer = outer . reflect_overhang ( self . oshape ) iotile = inner . translate ( - outer . l ) outer = util . Tile . intersection ( outer , self . oshape ) inner = util . Tile . intersection ( inner , self . oshape ) return outer , inner , iotile
9462	def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
5767	def _advapi32_interpret_rsa_key_blob ( bit_size , blob_struct , blob ) : len1 = bit_size // 8 len2 = bit_size // 16 prime1_offset = len1 prime2_offset = prime1_offset + len2 exponent1_offset = prime2_offset + len2 exponent2_offset = exponent1_offset + len2 coefficient_offset = exponent2_offset + len2 private_exponent_offset = coefficient_offset + len2 public_exponent = blob_struct . rsapubkey . pubexp modulus = int_from_bytes ( blob [ 0 : prime1_offset ] [ : : - 1 ] ) prime1 = int_from_bytes ( blob [ prime1_offset : prime2_offset ] [ : : - 1 ] ) prime2 = int_from_bytes ( blob [ prime2_offset : exponent1_offset ] [ : : - 1 ] ) exponent1 = int_from_bytes ( blob [ exponent1_offset : exponent2_offset ] [ : : - 1 ] ) exponent2 = int_from_bytes ( blob [ exponent2_offset : coefficient_offset ] [ : : - 1 ] ) coefficient = int_from_bytes ( blob [ coefficient_offset : private_exponent_offset ] [ : : - 1 ] ) private_exponent = int_from_bytes ( blob [ private_exponent_offset : private_exponent_offset + len1 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'public_key' : keys . RSAPublicKey ( { 'modulus' : modulus , 'public_exponent' : public_exponent , } ) , } ) rsa_private_key = keys . RSAPrivateKey ( { 'version' : 'two-prime' , 'modulus' : modulus , 'public_exponent' : public_exponent , 'private_exponent' : private_exponent , 'prime1' : prime1 , 'prime2' : prime2 , 'exponent1' : exponent1 , 'exponent2' : exponent2 , 'coefficient' : coefficient , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'private_key' : rsa_private_key , } ) return ( public_key_info , private_key_info )
8010	def webhook_handler ( * event_types ) : # First expand all wildcards and verify the event types are valid event_types_to_register = set ( ) for event_type in event_types : # Always convert to lowercase event_type = event_type . lower ( ) if "*" in event_type : # expand it for t in WEBHOOK_EVENT_TYPES : if fnmatch ( t , event_type ) : event_types_to_register . add ( t ) elif event_type not in WEBHOOK_EVENT_TYPES : raise ValueError ( "Unknown webhook event: %r" % ( event_type ) ) else : event_types_to_register . add ( event_type ) # Now register them def decorator ( func ) : for event_type in event_types_to_register : WEBHOOK_SIGNALS [ event_type ] . connect ( func ) return func return decorator
7226	def paint ( self ) : # TODO Figure out why i cant use some of these props snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , #'line-cap': self.cap, #'line-join': self.join, 'line-width' : VectorStyle . get_style_value ( self . width ) , #'line-gap-width': self.gap_width, #'line-blur': self.blur, } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
4419	async def play_previous ( self ) : if not self . previous : raise NoPreviousTrack self . queue . insert ( 0 , self . previous ) await self . play ( ignore_shuffle = True )
10825	def search ( cls , query , q ) : query = query . join ( User ) . filter ( User . email . like ( '%{0}%' . format ( q ) ) , ) return query
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return # Find current git branch. branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
1793	def MUL ( cpu , src ) : size = src . size reg_name_low , reg_name_high = { 8 : ( 'AL' , 'AH' ) , 16 : ( 'AX' , 'DX' ) , 32 : ( 'EAX' , 'EDX' ) , 64 : ( 'RAX' , 'RDX' ) } [ size ] res = ( Operators . ZEXTEND ( cpu . read_register ( reg_name_low ) , 256 ) * Operators . ZEXTEND ( src . read ( ) , 256 ) ) cpu . write_register ( reg_name_low , Operators . EXTRACT ( res , 0 , size ) ) cpu . write_register ( reg_name_high , Operators . EXTRACT ( res , size , size ) ) cpu . OF = Operators . EXTRACT ( res , size , size ) != 0 cpu . CF = cpu . OF
13841	def _ConsumeSingleByteString ( self ) : text = self . token if len ( text ) < 1 or text [ 0 ] not in _QUOTES : raise self . _ParseError ( 'Expected string but found: %r' % ( text , ) ) if len ( text ) < 2 or text [ - 1 ] != text [ 0 ] : raise self . _ParseError ( 'String missing ending quote: %r' % ( text , ) ) try : result = text_encoding . CUnescape ( text [ 1 : - 1 ] ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
1930	def add ( self , name : str , default = None , description : str = None ) : if name in self . _vars : raise ConfigError ( f"{self.name}.{name} already defined." ) if name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default ) self . _vars [ name ] = v
9970	def _get_col_index ( name ) : index = string . ascii_uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
2554	def setdocument ( self , doc ) : # assume that a document is correct in the subtree if self . document != doc : self . document = doc for i in self . children : if not isinstance ( i , dom_tag ) : return i . setdocument ( doc )
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
2246	def symlink ( real_path , link_path , overwrite = False , verbose = 0 ) : path = normpath ( real_path ) link = normpath ( link_path ) if not os . path . isabs ( path ) : # if path is not absolute it must be specified relative to link if _can_symlink ( ) : path = os . path . relpath ( path , os . path . dirname ( link ) ) else : # nocover # On windows, we need to use absolute paths path = os . path . abspath ( path ) if verbose : print ( 'Symlink: {path} -> {link}' . format ( path = path , link = link ) ) if islink ( link ) : if verbose : print ( '... already exists' ) pointed = _readlink ( link ) if pointed == path : if verbose > 1 : print ( '... and points to the right place' ) return link if verbose > 1 : if not exists ( link ) : print ( '... but it is broken and points somewhere else: {}' . format ( pointed ) ) else : print ( '... but it points somewhere else: {}' . format ( pointed ) ) if overwrite : util_io . delete ( link , verbose = verbose > 1 ) elif exists ( link ) : if _win32_links is None : if verbose : print ( '... already exists, but its a file. This will error.' ) raise FileExistsError ( 'cannot overwrite a physical path: "{}"' . format ( path ) ) else : # nocover if verbose : print ( '... already exists, and is either a file or hard link. ' 'Assuming it is a hard link. ' 'On non-win32 systems this would error.' ) if _win32_links is None : os . symlink ( path , link ) else : # nocover _win32_links . _symlink ( path , link , overwrite = overwrite , verbose = verbose ) return link
11445	def _clean_xml ( self , path_to_xml ) : try : if os . path . isfile ( path_to_xml ) : tree = ET . parse ( path_to_xml ) root = tree . getroot ( ) else : root = ET . fromstring ( path_to_xml ) except Exception , e : self . logger . error ( "Could not read OAI XML, aborting filter!" ) raise e strip_xml_namespace ( root ) return root
3175	def create ( self , campaign_id , data , * * queryparams ) : self . campaign_id = campaign_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) response = self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'feedback' ) , data = data , * * queryparams ) if response is not None : self . feedback_id = response [ 'feedback_id' ] else : self . feedback_id = None return response
13584	def _obj_display ( obj , display = '' ) : result = '' if not display : result = str ( obj ) else : template = Template ( display ) context = Context ( { 'obj' : obj } ) result = template . render ( context ) return result
13741	def cached_httpbl_exempt ( view_func ) : # We could just do view_func.cached_httpbl_exempt = True, but decorators # are nicer if they don't have side-effects, so we return a new # function. def wrapped_view ( * args , * * kwargs ) : return view_func ( * args , * * kwargs ) wrapped_view . cached_httpbl_exempt = True return wraps ( view_func , assigned = available_attrs ( view_func ) ) ( wrapped_view )
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
9784	def stop ( ctx , yes ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _build ) ) : click . echo ( 'Existing without stopping build job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . build_job . stop ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job is being stopped." )
7784	def got_it ( self , value , state = "new" ) : if not self . active : return item = CacheItem ( self . address , value , self . _item_freshness_period , self . _item_expiration_period , self . _item_purge_period , state ) self . _object_handler ( item . address , item . value , item . state ) self . cache . add_item ( item ) self . _deactivate ( )
5290	def construct_inlines ( self ) : inline_formsets = [ ] for inline_class in self . get_inlines ( ) : inline_instance = inline_class ( self . model , self . request , self . object , self . kwargs , self ) inline_formset = inline_instance . construct_formset ( ) inline_formsets . append ( inline_formset ) return inline_formsets
2100	def read ( self , * args , * * kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , * * kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
10345	def get_merged_namespace_names ( locations , check_keywords = True ) : resources = { location : get_bel_resource ( location ) for location in locations } if check_keywords : resource_keywords = set ( config [ 'Namespace' ] [ 'Keyword' ] for config in resources . values ( ) ) if 1 != len ( resource_keywords ) : raise ValueError ( 'Tried merging namespaces with different keywords: {}' . format ( resource_keywords ) ) result = { } for resource in resources : result . update ( resource [ 'Values' ] ) return result
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
9916	def validate_is_primary ( self , is_primary ) : # TODO: Setting 'is_primary' to 'False' should probably not be # allowed. if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) # Special MathML handling ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : # We are doing our own prettyfication as etree pretty_print is too insane. ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
7519	def write_snps_map ( data ) : ## grab map data from tmparr start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ "maparr" ] [ : ] ## get last data end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] ## write to map file (this is too slow...) outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : ## build to list line = maparr [ idx , : ] #print(line) outchunk . append ( "{}\trad{}_snp{}\t{}\t{}\n" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) ## clear list if not idx % 10000 : out . write ( "" . join ( outchunk ) ) outchunk = [ ] ## write remaining out . write ( "" . join ( outchunk ) ) LOGGER . debug ( "finished writing snps_map in: %s" , time . time ( ) - start )
7856	def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) except ValueError , e : self . error ( e )
402	def maxnorm_regularizer ( scale = 1.0 ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) # if scale >= 1.: # raise ValueError('Setting a scale greater than 1 on a regularizer: %g' % # scale) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def mn ( weights , name = 'max_regularizer' ) : """Applies max-norm regularization to weights.""" with tf . name_scope ( name ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) # if tf.__version__ <= '0.12': # standard_ops_fn = standard_ops.mul # else: standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_max ( standard_ops . abs ( weights ) ) , name = scope ) return mn
6738	def get_last_modified_timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string_types ) : return ignore_str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore_str = ' ' . join ( "! -name '%s'" % _ for _ in ignore ) cmd = 'find "' + path + '" ' + ignore_str + ' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -f 1 -d " "' #'find '+path+' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -d " " -f1 ret = subprocess . check_output ( cmd , shell = True ) # Note, we round now to avoid rounding errors later on where some formatters # use different decimal contexts. try : ret = round ( float ( ret ) , 2 ) except ValueError : return return ret
5242	def market_exact ( self , session , start_time : str , end_time : str ) -> Session : if session not in self . exch : return SessNA ss = self . exch [ session ] same_day = ss [ 0 ] < ss [ - 1 ] if not start_time : s_time = ss [ 0 ] else : s_time = param . to_hour ( start_time ) if same_day : s_time = max ( s_time , ss [ 0 ] ) if not end_time : e_time = ss [ - 1 ] else : e_time = param . to_hour ( end_time ) if same_day : e_time = min ( e_time , ss [ - 1 ] ) if same_day and ( s_time > e_time ) : return SessNA return Session ( start_time = s_time , end_time = e_time )
3638	def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
8306	def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
1047	def format_list ( extracted_list ) : list = [ ] for filename , lineno , name , line in extracted_list : item = ' File "%s", line %d, in %s\n' % ( filename , lineno , name ) if line : item = item + ' %s\n' % line . strip ( ) list . append ( item ) return list
12073	def _trace_summary ( self ) : for ( i , ( val , args ) ) in enumerate ( self . trace ) : if args is StopIteration : info = "Terminated" else : pprint = ',' . join ( '{' + ',' . join ( '%s=%r' % ( k , v ) for ( k , v ) in arg . items ( ) ) + '}' for arg in args ) info = ( "exploring arguments [%s]" % pprint ) if i == 0 : print ( "Step %d: Initially %s." % ( i , info ) ) else : print ( "Step %d: %s after receiving input(s) %s." % ( i , info . capitalize ( ) , val ) )
4686	def encrypt ( self , message ) : if not message : return None nonce = str ( random . getrandbits ( 64 ) ) try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( self . from_account [ "options" ] [ "memo_key" ] ) except KeyNotFound : # if all fails, raise exception raise MissingKeyError ( "Memo private key {} for {} could not be found" . format ( self . from_account [ "options" ] [ "memo_key" ] , self . from_account [ "name" ] ) ) if not memo_wif : raise MissingKeyError ( "Memo key for %s missing!" % self . from_account [ "name" ] ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix enc = memo . encode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( self . to_account [ "options" ] [ "memo_key" ] , prefix = self . chain_prefix ) , nonce , message , ) return { "message" : enc , "nonce" : nonce , "from" : self . from_account [ "options" ] [ "memo_key" ] , "to" : self . to_account [ "options" ] [ "memo_key" ] , }
7762	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) msg = Message ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond , subject = self . _subject , body = self . _body , thread = self . _thread ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : msg . add_payload ( payload . copy ( ) ) return msg
9520	def make_random_contigs ( contigs , length , outfile , name_by_letters = False , prefix = '' , seed = None , first_number = 1 ) : random . seed ( a = seed ) fout = utils . open_file_write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters_index = 0 for i in range ( contigs ) : if name_by_letters : name = letters [ letters_index ] letters_index += 1 if letters_index == len ( letters ) : letters_index = 0 else : name = str ( i + first_number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
6378	def manhattan ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Manhattan ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
5144	def _load ( self , config ) : if isinstance ( config , six . string_types ) : try : config = json . loads ( config ) except ValueError : pass if not isinstance ( config , dict ) : raise TypeError ( 'config block must be an istance ' 'of dict or a valid NetJSON string' ) return config
7275	def seek ( self , relative_position ) : self . _player_interface . Seek ( Int64 ( 1000.0 * 1000 * relative_position ) ) self . seekEvent ( self , relative_position )
7586	def _get_boots ( arr , nboots ) : ## hold results (nboots, [dstat, ]) boots = np . zeros ( ( nboots , ) ) ## iterate to fill boots for bidx in xrange ( nboots ) : ## sample with replacement lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst ## return bootarr return boots
8660	def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
10406	def canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_m2' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size_mean' , 'float64' ) , ( 'max_cluster_size_m2' , 'float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_m2' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
4865	def validate_username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) return value
8701	def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( ) except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) # joseph's stability fix: step to compute torques, then reset the # skeleton to the start of the step, and then step using computed # torques. thus any numerical errors between the body states after # stepping using angle constraints will be removed, because we # will be stepping the model using the computed torques. self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
7683	def display_multi ( annotations , fig_kw = None , meta = True , * * kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) # Filter down to coercable annotations first display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break # If there are no displayable annotations, fail here if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , * * fig_kw ) # MPL is stupid when making singleton subplots. # We catch this and make it always iterable. if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , * * kwargs ) return fig , axs
12884	def field_type ( self ) : if not self . model : return 'JSON' database = self . model . _meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , PostgresqlDatabase ) : return 'JSON' return 'TEXT'
7379	def _get ( self , text ) : if self . strict : match = self . prog . match ( text ) if match : cmd = match . group ( ) if cmd in self : return cmd else : words = self . prog . findall ( text ) for word in words : if word in self : return word
10550	def find_results ( project_id , * * kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'result' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : # pragma: no cover raise
87	def is_integer_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . integer )
6360	def needleman_wunsch ( src , tar , gap_cost = 1 , sim_func = sim_ident ) : return NeedlemanWunsch ( ) . dist_abs ( src , tar , gap_cost , sim_func )
4520	def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
5002	def _assign_enterprise_role_to_users ( self , _get_batch_method , options , is_feature_role = False ) : role_name = options [ 'role' ] batch_limit = options [ 'batch_limit' ] batch_sleep = options [ 'batch_sleep' ] batch_offset = options [ 'batch_offset' ] current_batch_index = batch_offset users_batch = _get_batch_method ( batch_offset , batch_offset + batch_limit ) role_class = SystemWideEnterpriseRole role_assignment_class = SystemWideEnterpriseUserRoleAssignment if is_feature_role : role_class = EnterpriseFeatureRole role_assignment_class = EnterpriseFeatureUserRoleAssignment enterprise_role = role_class . objects . get ( name = role_name ) while users_batch . count ( ) > 0 : for index , user in enumerate ( users_batch ) : LOGGER . info ( 'Processing user with index %s and id %s' , current_batch_index + index , user . id ) role_assignment_class . objects . get_or_create ( user = user , role = enterprise_role ) sleep ( batch_sleep ) current_batch_index += len ( users_batch ) users_batch = _get_batch_method ( current_batch_index , current_batch_index + batch_limit )
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/task_00009.p.gz' return ret
1685	def RepositoryName ( self ) : fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) # If the user specified a repository path, it exists, and the file is # contained in it, use the specified repository path if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : # allow case insensitive compare on Windows if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : # If there's a .svn file in the current directory, we recursively look # up the directory tree for the top of the SVN checkout root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by # searching up from the current path. root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Don't know what to do; header guard warnings may be wrong... return fullname
7349	def parse_token ( response ) : items = response . split ( "&" ) items = [ item . split ( "=" ) for item in items ] return { key : value for key , value in items }
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) # 1. Setup the forum. self . forum . on_start_game ( ) # 2. Setup the world. with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) # 3. Setup the actors. Because this is done after the forum and the # world have been setup, this signals to the actors that they can # send messages and query the game world as usual. num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
2537	def set_pkg_source_info ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_source_info_set : self . package_source_info_set = True doc . package . source_info = text return True else : raise CardinalityError ( 'Package::SourceInfo' )
1843	def JNS ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . SF , target . read ( ) , cpu . PC )
9626	def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 # The provided module/preview does not exist in the index. return preview . detail_view ( request )
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
358	def load_npy_to_any ( path = '' , name = 'file.npy' ) : file_path = os . path . join ( path , name ) try : return np . load ( file_path ) . item ( ) except Exception : return np . load ( file_path ) raise Exception ( "[!] Fail to load %s" % file_path )
4711	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.block.env: invalid SSH environment" ) return 1 block = cij . env_to_dict ( PREFIX , REQUIRED ) block [ "DEV_PATH" ] = "/dev/%s" % block [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , block ) return 0
9463	def conference_deaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceDeaf/' method = 'POST' return self . request ( path , method , call_params )
13184	def row_to_dict ( cls , row ) : comment_code = row [ 3 ] if comment_code . lower ( ) == 'na' : comment_code = '' comp1 = row [ 4 ] if comp1 . lower ( ) == 'na' : comp1 = '' comp2 = row [ 5 ] if comp2 . lower ( ) == 'na' : comp2 = '' chart = row [ 6 ] if chart . lower ( ) == 'na' : chart = '' notes = row [ 7 ] if notes . lower ( ) == 'na' : notes = '' return { 'name' : row [ 0 ] , 'date' : row [ 1 ] , 'magnitude' : row [ 2 ] , 'comment_code' : comment_code , 'comp1' : comp1 , 'comp2' : comp2 , 'chart' : chart , 'notes' : notes , }
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
10484	def _matchOther ( self , obj , * * kwargs ) : if obj is not None : # Need to check that the returned UI element wasn't destroyed first: if self . _findFirstR ( * * kwargs ) : return obj . _match ( * * kwargs ) return False
6897	def _periodicfeatures_worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get_periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )
10444	def getobjectproperty ( self , window_name , object_name , prop ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : # During the test, when the window closed and reopened # ErrorInvalidUIElement exception will be thrown self . _windows = { } # Call the method again, after updating apps obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) if obj_info and prop != "obj" and prop in obj_info : if prop == "class" : # ldtp_class_type are compatible with Linux and Windows class name # If defined class name exist return that, # else return as it is return ldtp_class_type . get ( obj_info [ prop ] , obj_info [ prop ] ) else : return obj_info [ prop ] raise LdtpServerException ( 'Unknown property "%s" in %s' % ( prop , object_name ) )
1548	def init_rotating_logger ( level , logfile , max_files , max_bytes ) : logging . basicConfig ( ) root_logger = logging . getLogger ( ) log_format = "[%(asctime)s] [%(levelname)s] %(filename)s: %(message)s" root_logger . setLevel ( level ) handler = RotatingFileHandler ( logfile , maxBytes = max_bytes , backupCount = max_files ) handler . setFormatter ( logging . Formatter ( fmt = log_format , datefmt = date_format ) ) root_logger . addHandler ( handler ) for handler in root_logger . handlers : root_logger . debug ( "Associated handlers - " + str ( handler ) ) if isinstance ( handler , logging . StreamHandler ) : root_logger . debug ( "Removing StreamHandler: " + str ( handler ) ) root_logger . handlers . remove ( handler )
3151	def update ( self , list_id , webhook_id , data ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) , data = data )
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
8939	def _zipped ( self , docs_base ) : with pushd ( docs_base ) : with tempfile . NamedTemporaryFile ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip_name = shutil . make_archive ( ziphandle . name , 'zip' ) notify . info ( "Uploading {:.1f} MiB from '{}' to '{}'..." . format ( os . path . getsize ( zip_name ) / 1024.0 , zip_name , self . target ) ) with io . open ( zip_name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )
10649	def get_component ( self , name ) : return [ c for c in self . components if c . name == name ] [ 0 ]
9683	def toggle_laser ( self , state ) : # Send the command byte and wait 10 ms a = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 10e-3 ) # If state is true, turn the laser ON, else OFF if state : b = self . cnxn . xfer ( [ 0x02 ] ) [ 0 ] else : b = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x03 else False
1621	def FindNextMultiLineCommentEnd ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : return lineix lineix += 1 return len ( lines )
11362	def convert_html_subscripts_to_latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$_{\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
4871	def create ( self , validated_data ) : ret = [ ] for attrs in validated_data : if 'non_field_errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
8527	def add_child ( self , child ) : if not isinstance ( child , ChildMixin ) : raise TypeError ( 'Requires instance of TreeElement. ' 'Got {}' . format ( type ( child ) ) ) child . parent = self self . _children . append ( child )
1559	def component_id ( self ) : if isinstance ( self . _component_id , HeronComponentSpec ) : if self . _component_id . name is None : # HeronComponentSpec instance's name attribute might not be available until # TopologyType metaclass finally sets it. This statement is to support __eq__(), # __hash__() and __str__() methods with safety, as raising Exception is not # appropriate this case. return "<No name available for HeronComponentSpec yet, uuid: %s>" % self . _component_id . uuid return self . _component_id . name elif isinstance ( self . _component_id , str ) : return self . _component_id else : raise ValueError ( "Component Id for this GlobalStreamId is not properly set: <%s:%s>" % ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) )
2095	def monitor ( self , pk , parent_pk = None , timeout = None , interval = 0.5 , outfile = sys . stdout , * * kwargs ) : # If we do not have the unified job info, infer it from parent if pk is None : pk = self . last_job_data ( parent_pk , * * kwargs ) [ 'id' ] job_endpoint = '%s%s/' % ( self . unified_job_type , pk ) # Pause until job is in running state self . wait ( pk , exit_on = [ 'running' , 'successful' ] , outfile = outfile ) # Loop initialization start = time . time ( ) start_line = 0 result = client . get ( job_endpoint ) . json ( ) click . echo ( '\033[0;91m------Starting Standard Out Stream------\033[0m' , nl = 2 , file = outfile ) # Poll the Ansible Tower instance for status and content, and print standard out to the out file while not result [ 'failed' ] and result [ 'status' ] != 'successful' : result = client . get ( job_endpoint ) . json ( ) # Put the process to sleep briefly. time . sleep ( interval ) # Make request to get standard out content = self . lookup_stdout ( pk , start_line , full = False ) # In the first moments of running the job, the standard out # may not be available yet if not content . startswith ( "Waiting for results" ) : line_count = len ( content . splitlines ( ) ) start_line += line_count click . echo ( content , nl = 0 , file = outfile ) if timeout and time . time ( ) - start > timeout : raise exc . Timeout ( 'Monitoring aborted due to timeout.' ) # Special final line for closure with workflow jobs if self . endpoint == '/workflow_jobs/' : click . echo ( self . lookup_stdout ( pk , start_line , full = True ) , nl = 1 ) click . echo ( '\033[0;91m------End of Standard Out Stream--------\033[0m' , nl = 2 , file = outfile ) if result [ 'failed' ] : raise exc . JobFailure ( 'Job failed.' ) # Return the job ID and other response data answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , pk ) ) ) answer . update ( result ) # Make sure to return ID of resource and not update number relevant for project creation and update if parent_pk : answer [ 'id' ] = parent_pk else : answer [ 'id' ] = pk return answer
11686	def get_user_details ( user_id ) : reasons = [ ] try : url = OSM_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : user_data = user_request . content xml_data = ET . fromstring ( user_data ) . getchildren ( ) [ 0 ] . getchildren ( ) changesets = [ i for i in xml_data if i . tag == 'changesets' ] [ 0 ] blocks = [ i for i in xml_data if i . tag == 'blocks' ] [ 0 ] if int ( changesets . get ( 'count' ) ) <= 5 : reasons . append ( 'New mapper' ) elif int ( changesets . get ( 'count' ) ) <= 30 : url = MAPBOX_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : mapping_days = int ( user_request . json ( ) . get ( 'extra' ) . get ( 'mapping_days' ) ) if mapping_days <= 5 : reasons . append ( 'New mapper' ) if int ( blocks . getchildren ( ) [ 0 ] . get ( 'count' ) ) > 1 : reasons . append ( 'User has multiple blocks' ) except Exception as e : message = 'Could not verify user of the changeset: {}, {}' print ( message . format ( user_id , str ( e ) ) ) return reasons
432	def save_images ( images , size , image_path = '_temp.png' ) : if len ( images . shape ) == 3 : # Greyscale [batch, h, w] --> [batch, h, w, 1] images = images [ : , : , : , np . newaxis ] def merge ( images , size ) : h , w = images . shape [ 1 ] , images . shape [ 2 ] img = np . zeros ( ( h * size [ 0 ] , w * size [ 1 ] , 3 ) , dtype = images . dtype ) for idx , image in enumerate ( images ) : i = idx % size [ 1 ] j = idx // size [ 1 ] img [ j * h : j * h + h , i * w : i * w + w , : ] = image return img def imsave ( images , size , path ) : if np . max ( images ) <= 1 and ( - 1 <= np . min ( images ) < 0 ) : images = ( ( images + 1 ) * 127.5 ) . astype ( np . uint8 ) elif np . max ( images ) <= 1 and np . min ( images ) >= 0 : images = ( images * 255 ) . astype ( np . uint8 ) return imageio . imwrite ( path , merge ( images , size ) ) if len ( images ) > size [ 0 ] * size [ 1 ] : raise AssertionError ( "number of images should be equal or less than size[0] * size[1] {}" . format ( len ( images ) ) ) return imsave ( images , size , image_path )
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
1502	def template_scheduler_yaml ( cl_args , masters ) : single_master = masters [ 0 ] scheduler_config_actual = "%s/standalone/scheduler.yaml" % cl_args [ "config_path" ] scheduler_config_template = "%s/standalone/templates/scheduler.template.yaml" % cl_args [ "config_path" ] template_file ( scheduler_config_template , scheduler_config_actual , { "<scheduler_uri>" : "http://%s:4646" % single_master } )
1390	def get_status ( self ) : status = None if self . physical_plan and self . physical_plan . topology : status = self . physical_plan . topology . state if status == 1 : return "Running" elif status == 2 : return "Paused" elif status == 3 : return "Killed" else : return "Unknown"
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
12204	def auto_constraints ( self , component = None ) : if not component : for table in self . tables : self . auto_constraints ( table ) return if not component . tableSchema . primaryKey : idcol = component . get_column ( term_uri ( 'id' ) ) if idcol : component . tableSchema . primaryKey = [ idcol . name ] self . _auto_foreign_keys ( component ) try : table_type = self . get_tabletype ( component ) except ValueError : # New component is not a known CLDF term, so cannot add components # automatically. TODO: We might me able to infer some based on # `xxxReference` column properties? return # auto-add foreign keys targetting the new component: for table in self . tables : self . _auto_foreign_keys ( table , component = component , table_type = table_type )
7377	def _user_headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = set ( headers . keys ( ) ) if h . get ( 'Authorization' , False ) : keys -= { 'Authorization' } for key in keys : h [ key ] = headers [ key ] return h
10843	def pending ( self ) : pending_updates = [ ] url = PATHS [ 'GET_PENDING' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : pending_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __pending = pending_updates return self . __pending
6353	def _normalize_lang_attrs ( self , text , strip ) : uninitialized = - 1 # all 1's attrib = uninitialized while '[' in text : bracket_start = text . find ( '[' ) bracket_end = text . find ( ']' , bracket_start ) if bracket_end == - 1 : raise ValueError ( 'No closing square bracket: text=(' + text + ') strip=(' + text_type ( strip ) + ')' ) attrib &= int ( text [ bracket_start + 1 : bracket_end ] ) text = text [ : bracket_start ] + text [ bracket_end + 1 : ] if attrib == uninitialized or strip : return text elif attrib == 0 : # means that the attributes were incompatible and there is no # alternative here return '[0]' return text + '[' + str ( attrib ) + ']'
2495	def handle_pkg_optional_fields ( self , package , package_node ) : self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . versionInfo , 'version' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . packageFileName , 'file_name' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . supplier , 'supplier' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . originator , 'originator' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . sourceInfo , 'source_info' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . licenseComments , 'license_comment' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . summary , 'summary' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . description , 'description' ) if package . has_optional_field ( 'check_sum' ) : checksum_node = self . create_checksum_node ( package . check_sum ) self . graph . add ( ( package_node , self . spdx_namespace . checksum , checksum_node ) ) if package . has_optional_field ( 'homepage' ) : homepage_node = URIRef ( self . to_special_value ( package . homepage ) ) homepage_triple = ( package_node , self . doap_namespace . homepage , homepage_node ) self . graph . add ( homepage_triple )
1489	def save_file ( self , obj ) : # pylint: disable=too-many-branches try : import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute except ImportError : import io as pystringIO # pylint: disable=reimported if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : raise pickle . PicklingError ( "Cannot pickle files that do not map to an actual file" ) if obj is sys . stdout : return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) if obj is sys . stderr : return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) if obj is sys . stdin : raise pickle . PicklingError ( "Cannot pickle standard input" ) if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : raise pickle . PicklingError ( "Cannot pickle files that map to tty objects" ) if 'r' not in obj . mode : raise pickle . PicklingError ( "Cannot pickle files that are not opened for reading" ) name = obj . name try : fsize = os . stat ( name ) . st_size except OSError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be stat" % name ) if obj . closed : #create an empty closed string io retval = pystringIO . StringIO ( "" ) retval . close ( ) elif not fsize : #empty file retval = pystringIO . StringIO ( "" ) try : tmpfile = file ( name ) tst = tmpfile . read ( 1 ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) tmpfile . close ( ) if tst != '' : raise pickle . PicklingError ( "Cannot pickle file %s as it does not appear to map to a physical, real file" % name ) else : try : tmpfile = file ( name ) contents = tmpfile . read ( ) tmpfile . close ( ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) retval = pystringIO . StringIO ( contents ) curloc = obj . tell ( ) retval . seek ( curloc ) retval . name = name self . save ( retval ) self . memoize ( obj )
435	def tsne_embedding ( embeddings , reverse_dictionary , plot_only = 500 , second = 5 , saveable = False , name = 'tsne' , fig_idx = 9862 ) : import matplotlib . pyplot as plt def plot_with_labels ( low_dim_embs , labels , figsize = ( 18 , 18 ) , second = 5 , saveable = True , name = 'tsne' , fig_idx = 9862 ) : if low_dim_embs . shape [ 0 ] < len ( labels ) : raise AssertionError ( "More labels than embeddings" ) if saveable is False : plt . ion ( ) plt . figure ( fig_idx ) plt . figure ( figsize = figsize ) # in inches for i , label in enumerate ( labels ) : x , y = low_dim_embs [ i , : ] plt . scatter ( x , y ) plt . annotate ( label , xy = ( x , y ) , xytext = ( 5 , 2 ) , textcoords = 'offset points' , ha = 'right' , va = 'bottom' ) if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second ) try : from sklearn . manifold import TSNE from six . moves import xrange tsne = TSNE ( perplexity = 30 , n_components = 2 , init = 'pca' , n_iter = 5000 ) # plot_only = 500 low_dim_embs = tsne . fit_transform ( embeddings [ : plot_only , : ] ) labels = [ reverse_dictionary [ i ] for i in xrange ( plot_only ) ] plot_with_labels ( low_dim_embs , labels , second = second , saveable = saveable , name = name , fig_idx = fig_idx ) except ImportError : _err = "Please install sklearn and matplotlib to visualize embeddings." tl . logging . error ( _err ) raise ImportError ( _err )
351	def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , "wb" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : # filter out keep-alive new chunks f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = "https://docs.google.com/uc?export=download" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )
5098	def graph2dict ( g , return_dict_of_dict = True ) : if not isinstance ( g , nx . DiGraph ) : g = QueueNetworkDiGraph ( g ) dict_of_dicts = nx . to_dict_of_dicts ( g ) if return_dict_of_dict : return dict_of_dicts else : return { k : list ( val . keys ( ) ) for k , val in dict_of_dicts . items ( ) }
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
3041	def access_token_expired ( self ) : if self . invalid : return True if not self . token_expiry : return False now = _UTCNOW ( ) if now >= self . token_expiry : logger . info ( 'access_token is expired. Now: %s, token_expiry: %s' , now , self . token_expiry ) return True return False
13041	def pipe_worker ( pipename , filename , object_type , query , format_string , unique = False ) : print_notification ( "[{}] Starting pipe" . format ( pipename ) ) object_type = object_type ( ) try : while True : uniq = set ( ) # Remove the previous file if it exists if os . path . exists ( filename ) : os . remove ( filename ) # Create the named pipe os . mkfifo ( filename ) # This function will block until a process opens it with open ( filename , 'w' ) as pipe : print_success ( "[{}] Providing data" . format ( pipename ) ) # Search the database objects = object_type . search ( * * query ) for obj in objects : data = fmt . format ( format_string , * * obj . to_dict ( ) ) if unique : if not data in uniq : uniq . add ( data ) pipe . write ( data + '\n' ) else : pipe . write ( data + '\n' ) os . unlink ( filename ) except KeyboardInterrupt : print_notification ( "[{}] Shutting down named pipe" . format ( pipename ) ) except Exception as e : print_error ( "[{}] Error: {}, stopping named pipe" . format ( e , pipename ) ) finally : os . remove ( filename )
1713	def ConstructObject ( self , py_obj ) : obj = self . NewObject ( ) for k , v in py_obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
11831	def path ( self ) : node , path_back = self , [ ] while node : path_back . append ( node ) node = node . parent return list ( reversed ( path_back ) )
5631	def long_description ( ) : import argparse parser = argparse . ArgumentParser ( ) parser . add_argument ( '--doc' , dest = "doc" , action = "store_true" , default = False ) args , sys . argv = parser . parse_known_args ( sys . argv ) if args . doc : import doc2md , pypandoc md = doc2md . doc2md ( doc2md . __doc__ , "doc2md" , toc = False ) long_description = pypandoc . convert ( md , 'rst' , format = 'md' ) else : return None
8100	def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
6791	def loaddata ( self , path , site = None ) : site = site or self . genv . SITE r = self . local_renderer r . env . _loaddata_path = path for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : try : self . set_db ( site = _site ) r . env . SITE = _site r . sudo ( 'export SITE={SITE}; export ROLE={ROLE}; ' 'cd {project_dir}; ' '{manage_cmd} loaddata {_loaddata_path}' ) except KeyError : pass
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
9046	def rsolve ( A , y ) : from numpy_sugar . linalg import rsolve as _rsolve try : beta = _rsolve ( A , y ) except LinAlgError : msg = "Could not converge to solve Ax=y." msg += " Setting x to zero." warnings . warn ( msg , RuntimeWarning ) beta = zeros ( A . shape [ 0 ] ) return beta
4944	def get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) : # Prevent circular imports. DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) # pylint: disable=invalid-name return DataSharingConsent . objects . proxied_get ( username = username , course_id = course_id , enterprise_customer__uuid = enterprise_customer_uuid )
95	def quokka_segmentation_map ( size = None , extract = None ) : # TODO get rid of this deferred import from imgaug . augmentables . segmaps import SegmentationMapOnImage with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) xx = [ ] yy = [ ] for kp_dict in json_dict [ "polygons" ] [ 0 ] [ "keypoints" ] : x = kp_dict [ "x" ] y = kp_dict [ "y" ] xx . append ( x ) yy . append ( y ) img_seg = np . zeros ( ( 643 , 960 , 1 ) , dtype = np . float32 ) rr , cc = skimage . draw . polygon ( np . array ( yy ) , np . array ( xx ) , shape = img_seg . shape ) img_seg [ rr , cc ] = 1.0 if extract is not None : bb = _quokka_normalize_extract ( extract ) img_seg = bb . extract_from_image ( img_seg ) segmap = SegmentationMapOnImage ( img_seg , shape = img_seg . shape [ 0 : 2 ] + ( 3 , ) ) if size is not None : shape_resized = _compute_resized_shape ( img_seg . shape , size ) segmap = segmap . resize ( shape_resized [ 0 : 2 ] ) segmap . shape = tuple ( shape_resized [ 0 : 2 ] ) + ( 3 , ) return segmap
517	def _bumpUpWeakColumns ( self ) : weakColumns = numpy . where ( self . _overlapDutyCycles < self . _minOverlapDutyCycles ) [ 0 ] for columnIndex in weakColumns : perm = self . _permanences [ columnIndex ] . astype ( realDType ) maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] perm [ maskPotential ] += self . _synPermBelowStimulusInc self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False )
9290	def _socket_readlines ( self , blocking = False ) : try : self . sock . setblocking ( 0 ) except socket . error as e : self . logger . error ( "socket error when setblocking(0): %s" % str ( e ) ) raise ConnectionDrop ( "connection dropped" ) while True : short_buf = b'' newline = b'\r\n' select . select ( [ self . sock ] , [ ] , [ ] , None if blocking else 0 ) try : short_buf = self . sock . recv ( 4096 ) # sock.recv returns empty if the connection drops if not short_buf : self . logger . error ( "socket.recv(): returned empty" ) raise ConnectionDrop ( "connection dropped" ) except socket . error as e : self . logger . error ( "socket error on recv(): %s" % str ( e ) ) if "Resource temporarily unavailable" in str ( e ) : if not blocking : if len ( self . buf ) == 0 : break self . buf += short_buf while newline in self . buf : line , self . buf = self . buf . split ( newline , 1 ) yield line
3193	def update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
1886	def _hook_xfer_mem ( self , uc , access , address , size , value , data ) : assert access in ( UC_MEM_WRITE , UC_MEM_READ , UC_MEM_FETCH ) if access == UC_MEM_WRITE : self . _cpu . write_int ( address , value , size * 8 ) # If client code is attempting to read a value, we need to bring it # in from Manticore state. If we try to mem_write it here, Unicorn # will segfault. We add the value to a list of things that need to # be written, and ask to restart the emulation. elif access == UC_MEM_READ : value = self . _cpu . read_bytes ( address , size ) if address in self . _should_be_written : return True self . _should_be_written [ address ] = value self . _should_try_again = True return False return True
6476	def set_text ( self , point , text ) : if not self . option . legend : return if not isinstance ( point , Point ) : point = Point ( point ) for offset , char in enumerate ( str ( text ) ) : self . screen . canvas [ point . y ] [ point . x + offset ] = char
8753	def partition_vifs ( xapi_client , interfaces , security_group_states ) : added = [ ] updated = [ ] removed = [ ] for vif in interfaces : # Quark should not action on isonet vifs in regions that use FLIP if ( 'floating_ip' in CONF . QUARK . environment_capabilities and is_isonet_vif ( vif ) ) : continue vif_has_groups = vif in security_group_states if vif . tagged and vif_has_groups and security_group_states [ vif ] [ sg_cli . SECURITY_GROUP_ACK ] : # Already ack'd these groups and VIF is tagged, reapply. # If it's not tagged, fall through and have it self-heal continue if vif . tagged : if vif_has_groups : updated . append ( vif ) else : removed . append ( vif ) else : if vif_has_groups : added . append ( vif ) # if not tagged and no groups, skip return added , updated , removed
12746	def as_flat_array ( iterables ) : arr = [ ] for x in iterables : arr . extend ( x ) return np . array ( arr )
2599	def uncan ( obj , g = None ) : import_needed = False for cls , uncanner in iteritems ( uncan_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif isinstance ( obj , cls ) : return uncanner ( obj , g ) if import_needed : # perform uncan_map imports, then try again # this will usually only happen once _import_mapping ( uncan_map , _original_uncan_map ) return uncan ( obj , g ) return obj
9982	def remove_decorator ( source : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break if node . decorator_list : deco_first = node . decorator_list [ 0 ] deco_last = node . decorator_list [ - 1 ] line_first = atok . tokens [ deco_first . first_token . index - 1 ] . start [ 0 ] line_last = atok . tokens [ deco_last . last_token . index + 1 ] . start [ 0 ] lines = lines [ : line_first - 1 ] + lines [ line_last : ] return "\n" . join ( lines ) + "\n"
1100	def unified_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True # fromdate = '\t{}'.format(fromfiledate) if fromfiledate else '' fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' # todate = '\t{}'.format(tofiledate) if tofiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' # yield '--- {}{}{}'.format(fromfile, fromdate, lineterm) yield '--- %s%s%s' % ( fromfile , fromdate , lineterm ) # yield '+++ {}{}{}'.format(tofile, todate, lineterm) yield '+++ %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] file1_range = _format_range_unified ( first [ 1 ] , last [ 2 ] ) file2_range = _format_range_unified ( first [ 3 ] , last [ 4 ] ) # yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm) yield '@@ -%s +%s @@%s' % ( file1_range , file2_range , lineterm ) for tag , i1 , i2 , j1 , j2 in group : if tag == 'equal' : for line in a [ i1 : i2 ] : yield ' ' + line continue if tag in ( 'replace' , 'delete' ) : for line in a [ i1 : i2 ] : yield '-' + line if tag in ( 'replace' , 'insert' ) : for line in b [ j1 : j2 ] : yield '+' + line
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) # Purge caching ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
5107	def fetch_data ( self , return_header = False ) : qdata = [ ] for d in self . data . values ( ) : qdata . extend ( d ) dat = np . zeros ( ( len ( qdata ) , 6 ) ) if len ( qdata ) > 0 : dat [ : , : 5 ] = np . array ( qdata ) dat [ : , 5 ] = self . edge [ 2 ] dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] dat = np . array ( [ tuple ( d ) for d in dat ] , dtype = dType ) dat = np . sort ( dat , order = 'a' ) dat = np . array ( [ tuple ( d ) for d in dat ] ) if return_header : return dat , 'arrival,service,departure,num_queued,num_total,q_id' return dat
4587	def stop ( self ) : if self . is_running : log . info ( 'Stopping' ) self . is_running = False self . __class__ . _INSTANCE = None try : self . thread and self . thread . stop ( ) except : log . error ( 'Error stopping thread' ) traceback . print_exc ( ) self . thread = None return True
1814	def SETNG ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , 1 , 0 ) )
5338	def __upload_title ( self , kibiter_major ) : if kibiter_major == "6" : resource = ".kibana/doc/projectname" data = { "projectname" : { "name" : self . project_name } } mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for dashboard title" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for dashboard title." ) logger . error ( res . json ( ) ) logger . debug ( "Uploading dashboard title" ) res = self . grimoire_con . post ( url , data = json . dumps ( data ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create dashboard title." ) logger . error ( res . json ( ) )
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
12633	def calculate_file_distances ( dicom_files , field_weights = None , dist_method_cls = None , * * kwargs ) : if dist_method_cls is None : dist_method = LevenshteinDicomFileDistance ( field_weights ) else : try : dist_method = dist_method_cls ( field_weights = field_weights , * * kwargs ) except : log . exception ( 'Could not instantiate {} object with field_weights ' 'and {}' . format ( dist_method_cls , kwargs ) ) dist_dtype = np . float16 n_files = len ( dicom_files ) try : file_dists = np . zeros ( ( n_files , n_files ) , dtype = dist_dtype ) except MemoryError as mee : import scipy . sparse file_dists = scipy . sparse . lil_matrix ( ( n_files , n_files ) , dtype = dist_dtype ) for idxi in range ( n_files ) : dist_method . set_dicom_file1 ( dicom_files [ idxi ] ) for idxj in range ( idxi + 1 , n_files ) : dist_method . set_dicom_file2 ( dicom_files [ idxj ] ) if idxi != idxj : file_dists [ idxi , idxj ] = dist_method . transform ( ) return file_dists
2621	def security_group ( self , vpc ) : sg = vpc . create_security_group ( GroupName = "private-subnet" , Description = "security group for remote executors" ) ip_ranges = [ { 'CidrIp' : '10.0.0.0/16' } ] # Allows all ICMP in, all TCP and UDP in within VPC in_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'ICMP' , 'FromPort' : - 1 , 'ToPort' : - 1 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 22 , 'ToPort' : 22 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } ] # Allows all TCP out, all TCP and UDP out within VPC out_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , ] sg . authorize_ingress ( IpPermissions = in_permissions ) sg . authorize_egress ( IpPermissions = out_permissions ) self . sg_id = sg . id return sg
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
6098	def luminosity_within_ellipse_in_units ( self , major_axis , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( major_axis , dim . Length ) : major_axis = dim . Length ( major_axis , 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
13566	def plot ( * args , ax = None , * * kwargs ) : if ax is None : fig , ax = _setup_axes ( ) pl = ax . plot ( * args , * * kwargs ) if _np . shape ( args ) [ 0 ] > 1 : if type ( args [ 1 ] ) is not str : min_x = min ( args [ 0 ] ) max_x = max ( args [ 0 ] ) ax . set_xlim ( ( min_x , max_x ) ) return pl
707	def _engineServicesRunning ( ) : process = subprocess . Popen ( [ "ps" , "aux" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise RuntimeError ( "Unable to check for running client job manager" ) # See if the CJM is running running = False for line in stdout . split ( "\n" ) : if "python" in line and "clientjobmanager.client_job_manager" in line : running = True break return running
315	def gross_lev ( positions ) : exposure = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) return exposure / positions . sum ( axis = 1 )
12837	def init_async ( self , loop = None ) : self . _loop = loop or asyncio . get_event_loop ( ) self . _async_lock = asyncio . Lock ( loop = loop ) # FIX: SQLITE in memory database if not self . database == ':memory:' : self . _state = ConnectionLocal ( )
11595	def _rc_renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
3481	def read_sbml_model ( filename , number = float , f_replace = F_REPLACE , set_missing_bounds = False , * * kwargs ) : try : doc = _get_doc_from_filename ( filename ) return _sbml_to_model ( doc , number = number , f_replace = f_replace , set_missing_bounds = set_missing_bounds , * * kwargs ) except IOError as e : raise e except Exception : LOGGER . error ( traceback . print_exc ( ) ) raise CobraSBMLError ( "Something went wrong reading the SBML model. Most likely the SBML" " model is not valid. Please check that your model is valid using " "the `cobra.io.sbml.validate_sbml_model` function or via the " "online validator at http://sbml.org/validator .\n" "\t`(model, errors) = validate_sbml_model(filename)`" "\nIf the model is valid and cannot be read please open an issue " "at https://github.com/opencobra/cobrapy/issues ." )
47	def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
6266	def stop ( self ) -> float : self . stop_time = time . time ( ) return self . stop_time - self . start_time - self . offset
3624	def decode ( geohash ) : lat , lon , lat_err , lon_err = decode_exactly ( geohash ) # Format to the number of decimals that are known lats = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lat_err ) ) ) ) - 1 , lat ) lons = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lon_err ) ) ) ) - 1 , lon ) if '.' in lats : lats = lats . rstrip ( '0' ) if '.' in lons : lons = lons . rstrip ( '0' ) return lats , lons
10320	def _microcanonical_average_moments ( moments , alpha ) : ret = dict ( ) runs = moments . shape [ 0 ] sqrt_n = np . sqrt ( runs ) moments_sample_mean = moments . mean ( axis = 0 ) ret [ 'moments' ] = moments_sample_mean moments_sample_std = moments . std ( axis = 0 , ddof = 1 ) ret [ 'moments_ci' ] = np . empty ( ( 5 , 2 ) ) for k in range ( 5 ) : if moments_sample_std [ k ] : old_settings = np . seterr ( all = 'raise' ) ret [ 'moments_ci' ] [ k ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = moments_sample_mean [ k ] , scale = moments_sample_std [ k ] / sqrt_n ) np . seterr ( * * old_settings ) else : ret [ 'moments_ci' ] [ k ] = ( moments_sample_mean [ k ] * np . ones ( 2 ) ) return ret
13872	def StandardizePath ( path , strip = False ) : path = path . replace ( SEPARATOR_WINDOWS , SEPARATOR_UNIX ) if strip : path = path . rstrip ( SEPARATOR_UNIX ) return path
6295	def index_buffer ( self , buffer , index_element_size = 4 ) : if not type ( buffer ) in [ moderngl . Buffer , numpy . ndarray , bytes ] : raise VAOError ( "buffer parameter must be a moderngl.Buffer, numpy.ndarray or bytes instance" ) if isinstance ( buffer , numpy . ndarray ) : buffer = self . ctx . buffer ( buffer . tobytes ( ) ) if isinstance ( buffer , bytes ) : buffer = self . ctx . buffer ( data = buffer ) self . _index_buffer = buffer self . _index_element_size = index_element_size
12157	def list_order_by ( l , firstItems ) : l = list ( l ) for item in firstItems [ : : - 1 ] : #backwards if item in l : l . remove ( item ) l . insert ( 0 , item ) return l
7267	def run ( self , * args , * * kw ) : log . debug ( '[operator] run "{}" with arguments: {}' . format ( self . __class__ . __name__ , args ) ) if self . kind == OperatorTypes . ATTRIBUTE : return self . match ( self . ctx ) else : return self . run_matcher ( * args , * * kw )
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
12741	def _parse_persons ( self , datafield , subfield , roles = [ "aut" ] ) : # parse authors parsed_persons = [ ] raw_persons = self . get_subfields ( datafield , subfield ) for person in raw_persons : # check if person have at least one of the roles specified in # 'roles' parameter of function other_subfields = person . other_subfields if "4" in other_subfields and roles != [ "any" ] : person_roles = other_subfields [ "4" ] # list of role parameters relevant = any ( map ( lambda role : role in roles , person_roles ) ) # skip non-relevant persons if not relevant : continue # result of .strip() is string, so ind1/2 in MARCSubrecord are lost ind1 = person . i1 ind2 = person . i2 person = person . strip ( ) name = "" second_name = "" surname = "" title = "" # here it gets nasty - there is lot of options in ind1/ind2 # parameters if ind1 == "1" and ind2 == " " : if "," in person : surname , name = person . split ( "," , 1 ) elif " " in person : surname , name = person . split ( " " , 1 ) else : surname = person if "c" in other_subfields : title = "," . join ( other_subfields [ "c" ] ) elif ind1 == "0" and ind2 == " " : name = person . strip ( ) if "b" in other_subfields : second_name = "," . join ( other_subfields [ "b" ] ) if "c" in other_subfields : surname = "," . join ( other_subfields [ "c" ] ) elif ind1 == "1" and ind2 == "0" or ind1 == "0" and ind2 == "0" : name = person . strip ( ) if "c" in other_subfields : title = "," . join ( other_subfields [ "c" ] ) parsed_persons . append ( Person ( name . strip ( ) , second_name . strip ( ) , surname . strip ( ) , title . strip ( ) ) ) return parsed_persons
4088	def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : asyncio . ensure_future ( fn ( * args , * * kwargs ) ) return wrapper return outer_decorator
9277	def parse ( packet ) : if not isinstance ( packet , string_type_parse ) : raise TypeError ( "Expected packet to be str/unicode/bytes, got %s" , type ( packet ) ) if len ( packet ) == 0 : raise ParseError ( "packet is empty" , packet ) # attempt to detect encoding if isinstance ( packet , bytes ) : packet = _unicode_packet ( packet ) packet = packet . rstrip ( "\r\n" ) logger . debug ( "Parsing: %s" , packet ) # split into head and body try : ( head , body ) = packet . split ( ':' , 1 ) except : raise ParseError ( "packet has no body" , packet ) if len ( body ) == 0 : raise ParseError ( "packet body is empty" , packet ) parsed = { 'raw' : packet , } # parse head try : parsed . update ( parse_header ( head ) ) except ParseError as msg : raise ParseError ( str ( msg ) , packet ) # parse body packet_type = body [ 0 ] body = body [ 1 : ] if len ( body ) == 0 and packet_type != '>' : raise ParseError ( "packet body is empty after packet type character" , packet ) # attempt to parse the body try : _try_toparse_body ( packet_type , body , parsed ) # capture ParseErrors and attach the packet except ( UnknownFormat , ParseError ) as exp : exp . packet = packet raise # if we fail all attempts to parse, try beacon packet if 'format' not in parsed : if not re . match ( r"^(AIR.*|ALL.*|AP.*|BEACON|CQ.*|GPS.*|DF.*|DGPS.*|" "DRILL.*|DX.*|ID.*|JAVA.*|MAIL.*|MICE.*|QST.*|QTH.*|" "RTCM.*|SKY.*|SPACE.*|SPC.*|SYM.*|TEL.*|TEST.*|TLM.*|" "WX.*|ZIP.*|UIDIGI)$" , parsed [ 'to' ] ) : raise UnknownFormat ( "format is not supported" , packet ) parsed . update ( { 'format' : 'beacon' , 'text' : packet_type + body , } ) logger . debug ( "Parsed ok." ) return parsed
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 # May raise a permission denied self . check_object_permissions ( self . request , obj ) return obj
3889	async def fire ( self , * args , * * kwargs ) : logger . debug ( 'Fired {}' . format ( self ) ) for observer in self . _observers : gen = observer ( * args , * * kwargs ) if asyncio . iscoroutinefunction ( observer ) : await gen
2969	def _sm_stop_from_pain ( self , * args , * * kwargs ) : _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _do_reset_all ( )
8556	def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
12466	def run_cmd ( cmd , echo = False , fail_silently = False , * * kwargs ) : out , err = None , None if echo : cmd_str = cmd if isinstance ( cmd , string_types ) else ' ' . join ( cmd ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = sys . stdout , sys . stderr print_message ( '$ {0}' . format ( cmd_str ) ) else : out , err = get_temp_streams ( ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = out , err try : retcode = subprocess . call ( cmd , * * kwargs ) except subprocess . CalledProcessError as err : if fail_silently : return False print_error ( str ( err ) if IS_PY3 else unicode ( err ) ) # noqa finally : if out : out . close ( ) if err : err . close ( ) if retcode and echo and not fail_silently : print_error ( 'Command {0!r} returned non-zero exit status {1}' . format ( cmd_str , retcode ) ) return retcode
1288	def tf_baseline_loss ( self , states , internals , reward , update , reference = None ) : if self . baseline_mode == 'states' : loss = self . baseline . loss ( states = states , internals = internals , reward = reward , update = update , reference = reference ) elif self . baseline_mode == 'network' : loss = self . baseline . loss ( states = self . network . apply ( x = states , internals = internals , update = update ) , internals = internals , reward = reward , update = update , reference = reference ) regularization_loss = self . baseline . regularization_loss ( ) if regularization_loss is not None : loss += regularization_loss return loss
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
4606	def whitelist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "white" ] , account = self )
8351	def handle_pi ( self , text ) : if text [ : 3 ] == "xml" : text = u"xml version='1.0' encoding='%SOUP-ENCODING%'" self . _toStringSubclass ( text , ProcessingInstruction )
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
3977	def _expand_libs_in_libs ( specs ) : for lib_name , lib_spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib_spec and 'libs' in lib_spec [ 'depends' ] : lib_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , lib_name , specs , 'libs' )
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
4769	def is_instance_of ( self , some_class ) : try : if not isinstance ( self . val , some_class ) : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some_class . __name__ ) ) except TypeError : raise TypeError ( 'given arg must be a class' ) return self
290	def plot_rolling_beta ( returns , factor_returns , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) ax . set_title ( "Rolling portfolio beta to " + str ( factor_returns . name ) ) ax . set_ylabel ( 'Beta' ) rb_1 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) rb_1 . plot ( color = 'steelblue' , lw = 3 , alpha = 0.6 , ax = ax , * * kwargs ) rb_2 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 12 ) rb_2 . plot ( color = 'grey' , lw = 3 , alpha = 0.4 , ax = ax , * * kwargs ) ax . axhline ( rb_1 . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_xlabel ( '' ) ax . legend ( [ '6-mo' , '12-mo' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_ylim ( ( - 1.0 , 1.0 ) ) return ax
10547	def find_taskruns ( project_id , * * kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'taskrun' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : return res except : # pragma: no cover raise
11923	def parse ( self ) : if exists ( self . filepath ) : content = open ( self . filepath ) . read ( ) . decode ( charset ) else : content = "" try : config = toml . loads ( content ) except toml . TomlSyntaxError : raise ConfigSyntaxError return config
7053	def _read_pklc ( lcfile ) : if lcfile . endswith ( '.gz' ) : try : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
10196	def aggregate_events ( aggregations , start_date = None , end_date = None , update_bookmark = True ) : start_date = dateutil_parse ( start_date ) if start_date else None end_date = dateutil_parse ( end_date ) if end_date else None results = [ ] for a in aggregations : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) results . append ( aggregator . run ( start_date , end_date , update_bookmark ) ) return results
9801	def config ( list ) : # pylint:disable=redefined-builtin if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
6978	def read_kepler_pklc ( picklefile ) : if picklefile . endswith ( '.gz' ) : infd = gzip . open ( picklefile , 'rb' ) else : infd = open ( picklefile , 'rb' ) try : with infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( picklefile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % picklefile ) return lcdict
3204	def create ( self , data ) : if 'operations' not in data : raise KeyError ( 'The batch must have operations' ) for op in data [ 'operations' ] : if 'method' not in op : raise KeyError ( 'The batch operation must have a method' ) if op [ 'method' ] not in [ 'GET' , 'POST' , 'PUT' , 'PATCH' , 'DELETE' ] : raise ValueError ( 'The batch operation method must be one of "GET", "POST", "PUT", "PATCH", ' 'or "DELETE", not {0}' . format ( op [ 'method' ] ) ) if 'path' not in op : raise KeyError ( 'The batch operation must have a path' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
2741	def remove_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
11949	def configure_custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and isinstance ( c , types . ClassType ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) # Check for valid identifiers kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) result = c ( * * kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
10499	def waitFor ( self , timeout , notification , * * kwargs ) : return self . _waitFor ( timeout , notification , * * kwargs )
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : # pylint: disable=protected-access sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
9712	def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : # if item >= heap[0], it will be popped immediately after pushed item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
4295	def less_than_version ( value ) : items = list ( map ( int , str ( value ) . split ( '.' ) ) ) if len ( items ) == 1 : items . append ( 0 ) items [ 1 ] += 1 if value == '1.11' : return '2.0' else : return '.' . join ( map ( str , items ) )
4237	def get_attached_devices_2 ( self ) : _LOGGER . info ( "Get attached devices 2" ) success , response = self . _make_request ( SERVICE_DEVICE_INFO , "GetAttachDevice2" ) if not success : return None success , devices_node = _find_node ( response . text , ".//GetAttachDevice2Response/NewAttachDevice" ) if not success : return None xml_devices = devices_node . findall ( "Device" ) devices = [ ] for d in xml_devices : ip = _xml_get ( d , 'IP' ) name = _xml_get ( d , 'Name' ) mac = _xml_get ( d , 'MAC' ) signal = _convert ( _xml_get ( d , 'SignalStrength' ) , int ) link_type = _xml_get ( d , 'ConnectionType' ) link_rate = _xml_get ( d , 'Linkspeed' ) allow_or_block = _xml_get ( d , 'AllowOrBlock' ) device_type = _convert ( _xml_get ( d , 'DeviceType' ) , int ) device_model = _xml_get ( d , 'DeviceModel' ) ssid = _xml_get ( d , 'SSID' ) conn_ap_mac = _xml_get ( d , 'ConnAPMAC' ) devices . append ( Device ( name , ip , mac , link_type , signal , link_rate , allow_or_block , device_type , device_model , ssid , conn_ap_mac ) ) return devices
12078	def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
3090	def locked_put ( self , credentials ) : entity = self . _model . get_or_insert ( self . _key_name ) setattr ( entity , self . _property_name , credentials ) entity . put ( ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) )
1353	def make_error_response ( self , message ) : response = self . make_response ( constants . RESPONSE_STATUS_FAILURE ) response [ constants . RESPONSE_KEY_MESSAGE ] = message return response
13758	def split_path ( path ) : result_parts = [ ] #todo: check loops while path != "/" : parts = os . path . split ( path ) if parts [ 1 ] == path : result_parts . insert ( 0 , parts [ 1 ] ) break elif parts [ 0 ] == path : result_parts . insert ( 0 , parts [ 0 ] ) break else : path = parts [ 0 ] result_parts . insert ( 0 , parts [ 1 ] ) return result_parts
12980	def deleteMultipleByPks ( self , pks ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : return self . deleteByPk ( pks [ 0 ] ) objs = self . mdl . objects . getMultipleOnlyIndexedFields ( pks ) return self . deleteMultiple ( objs )
8346	def findAll ( self , name = None , attrs = { } , recursive = True , text = None , limit = None , * * kwargs ) : generator = self . recursiveChildGenerator if not recursive : generator = self . childGenerator return self . _findAll ( name , attrs , text , limit , generator , * * kwargs )
13180	def _get_column_nums_from_args ( columns ) : nums = [ ] for c in columns : for p in c . split ( ',' ) : p = p . strip ( ) try : c = int ( p ) nums . append ( c ) except ( TypeError , ValueError ) : start , ignore , end = p . partition ( '-' ) try : start = int ( start ) end = int ( end ) except ( TypeError , ValueError ) : raise ValueError ( 'Did not understand %r, expected digit-digit' % c ) inc = 1 if start < end else - 1 nums . extend ( range ( start , end + inc , inc ) ) # The user will pass us 1-based indexes, but we need to use # 0-based indexing with the row. return [ n - 1 for n in nums ]
3665	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Cplms = [ i ( T ) for i in self . HeatCapacityLiquids ] return mixing_simple ( zs , Cplms ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) Cpl = Laliberte_heat_capacity ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return property_mass_to_molar ( Cpl , MW ) else : raise Exception ( 'Method not valid' )
3649	def messages ( self ) : method = 'GET' url = 'activeMessage' rc = self . __request__ ( method , url ) # try: # return rc['activeMessage'] # except: # raise UnknownError('Invalid activeMessage response') # is it even possible? return rc [ 'activeMessage' ]
11982	def is_valid_ip ( self , ip ) : if not isinstance ( ip , ( IPv4Address , CIDR ) ) : if str ( ip ) . find ( '/' ) == - 1 : ip = IPv4Address ( ip ) else : # Support for CIDR strings/objects, an idea of Nicola Novello. ip = CIDR ( ip ) if isinstance ( ip , IPv4Address ) : if ip < self . _first_ip or ip > self . _last_ip : return False elif isinstance ( ip , CIDR ) : # NOTE: manage /31 networks; 127.0.0.1/31 is considered to # be included in 127.0.0.1/8. if ip . _nm . _ip_dec == 0xFFFFFFFE and self . _nm . _ip_dec != 0xFFFFFFFE : compare_to_first = self . _net_ip . _ip_dec compare_to_last = self . _bc_ip . _ip_dec else : compare_to_first = self . _first_ip . _ip_dec compare_to_last = self . _last_ip . _ip_dec if ip . _first_ip . _ip_dec < compare_to_first or ip . _last_ip . _ip_dec > compare_to_last : return False return True
7145	def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( destinations , priority , payment_id , unlock_time , account = self . index , relay = relay )
384	def parse_darknet_ann_str_to_list ( annotations ) : annotations = annotations . split ( "\n" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann
686	def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
7633	def __get_dtype ( typespec ) : if 'type' in typespec : return __TYPE_MAP__ . get ( typespec [ 'type' ] , np . object_ ) elif 'enum' in typespec : # Enums map to objects return np . object_ elif 'oneOf' in typespec : # Recurse types = [ __get_dtype ( v ) for v in typespec [ 'oneOf' ] ] # If they're not all equal, return object if all ( [ t == types [ 0 ] for t in types ] ) : return types [ 0 ] return np . object_
10661	def convert_compound ( mass , source , target , element ) : # Perform the conversion. target_mass_fraction = element_mass_fraction ( target , element ) if target_mass_fraction == 0.0 : # If target_formula does not contain element, just return 0.0. return 0.0 else : source_mass_fraction = element_mass_fraction ( source , element ) return mass * source_mass_fraction / target_mass_fraction
2295	def predict_proba ( self , a , b , * * kwargs ) : estimators = { 'entropy' : lambda x , y : eval_entropy ( y ) - eval_entropy ( x ) , 'integral' : integral_approx_estimator } ref_measures = { 'gaussian' : lambda x : standard_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'uniform' : lambda x : min_max_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'None' : lambda x : x } ref_measure = ref_measures [ kwargs . get ( 'refMeasure' , 'gaussian' ) ] estimator = estimators [ kwargs . get ( 'estimator' , 'entropy' ) ] a = ref_measure ( a ) b = ref_measure ( b ) return estimator ( a , b )
3815	async def _on_receive_array ( self , array ) : if array [ 0 ] == 'noop' : pass # This is just a keep-alive, ignore it. else : wrapper = json . loads ( array [ 0 ] [ 'p' ] ) # Wrapper appears to be a Protocol Buffer message, but encoded via # field numbers as dictionary keys. Since we don't have a parser # for that, parse it ad-hoc here. if '3' in wrapper : # This is a new client_id. self . _client_id = wrapper [ '3' ] [ '2' ] logger . info ( 'Received new client_id: %r' , self . _client_id ) # Once client_id is received, the channel is ready to have # services added. await self . _add_channel_services ( ) if '2' in wrapper : pblite_message = json . loads ( wrapper [ '2' ] [ '2' ] ) if pblite_message [ 0 ] == 'cbu' : # This is a (Client)BatchUpdate containing StateUpdate # messages. batch_update = hangouts_pb2 . BatchUpdate ( ) pblite . decode ( batch_update , pblite_message , ignore_first_item = True ) for state_update in batch_update . state_update : logger . debug ( 'Received StateUpdate:\n%s' , state_update ) header = state_update . state_update_header self . _active_client_state = header . active_client_state await self . on_state_update . fire ( state_update ) else : logger . info ( 'Ignoring message: %r' , pblite_message [ 0 ] )
917	def info ( self , msg , * args , * * kwargs ) : self . _baseLogger . info ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
2447	def set_pkg_vers ( self , doc , version ) : self . assert_package_exists ( ) if not self . package_vers_set : self . package_vers_set = True doc . package . version = version return True else : raise CardinalityError ( 'Package::Version' )
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
10195	def register_templates ( ) : event_templates = [ current_stats . _events_config [ e ] [ 'templates' ] for e in current_stats . _events_config ] aggregation_templates = [ current_stats . _aggregations_config [ a ] [ 'templates' ] for a in current_stats . _aggregations_config ] return event_templates + aggregation_templates
10377	def one_sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
2475	def set_lic_name ( self , doc , name ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_name_set : self . extr_lic_name_set = True if validations . validate_extr_lic_name ( name ) : self . extr_lic ( doc ) . full_name = name return True else : raise SPDXValueError ( 'ExtractedLicense::Name' ) else : raise CardinalityError ( 'ExtractedLicense::Name' ) else : raise OrderError ( 'ExtractedLicense::Name' )
3424	def get_solution ( model , reactions = None , metabolites = None , raise_error = False ) : check_solver_status ( model . solver . status , raise_error = raise_error ) if reactions is None : reactions = model . reactions if metabolites is None : metabolites = model . metabolites rxn_index = list ( ) fluxes = empty ( len ( reactions ) ) reduced = empty ( len ( reactions ) ) var_primals = model . solver . primal_values shadow = empty ( len ( metabolites ) ) if model . solver . is_integer : reduced . fill ( nan ) shadow . fill ( nan ) for ( i , rxn ) in enumerate ( reactions ) : rxn_index . append ( rxn . id ) fluxes [ i ] = var_primals [ rxn . id ] - var_primals [ rxn . reverse_id ] met_index = [ met . id for met in metabolites ] else : var_duals = model . solver . reduced_costs for ( i , rxn ) in enumerate ( reactions ) : forward = rxn . id reverse = rxn . reverse_id rxn_index . append ( forward ) fluxes [ i ] = var_primals [ forward ] - var_primals [ reverse ] reduced [ i ] = var_duals [ forward ] - var_duals [ reverse ] met_index = list ( ) constr_duals = model . solver . shadow_prices for ( i , met ) in enumerate ( metabolites ) : met_index . append ( met . id ) shadow [ i ] = constr_duals [ met . id ] return Solution ( model . solver . objective . value , model . solver . status , Series ( index = rxn_index , data = fluxes , name = "fluxes" ) , Series ( index = rxn_index , data = reduced , name = "reduced_costs" ) , Series ( index = met_index , data = shadow , name = "shadow_prices" ) )
9921	def validate_key ( self , key ) : if not models . PasswordResetToken . valid_tokens . filter ( key = key ) . exists ( ) : raise serializers . ValidationError ( _ ( "The provided reset token does not exist, or is expired." ) ) return key
9503	def remove_contained_in_list ( l ) : i = 0 l . sort ( ) while i < len ( l ) - 1 : if l [ i + 1 ] . contains ( l [ i ] ) : l . pop ( i ) elif l [ i ] . contains ( l [ i + 1 ] ) : l . pop ( i + 1 ) else : i += 1
13403	def acceptedUser ( self , logType ) : from urllib2 import urlopen , URLError , HTTPError import json isApproved = False userName = str ( self . logui . userName . text ( ) ) if userName == "" : return False # Must have a user name to submit entry if logType == "MCC" : networkFault = False data = [ ] log_url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=" + userName try : data = urlopen ( log_url , None , 5 ) . read ( ) data = json . loads ( data ) except URLError as error : print ( "URLError: " + str ( error . reason ) ) networkFault = True except HTTPError as error : print ( "HTTPError: " + str ( error . reason ) ) networkFault = True # If network fails, ask user to verify if networkFault : msgBox = QMessageBox ( ) msgBox . setText ( "Cannot connect to MCC Log Server!" ) msgBox . setInformativeText ( "Use entered User name anyway?" ) msgBox . setStandardButtons ( QMessageBox . Ok | QMessageBox . Cancel ) msgBox . setDefaultButton ( QMessageBox . Ok ) if msgBox . exec_ ( ) == QMessageBox . Ok : isApproved = True if data != [ ] and ( data is not None ) : isApproved = True else : isApproved = True return isApproved
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : # Initial logbook menu can add additional menus, all others can only remove themselves. QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
13338	def redirect_resolver ( resolver , path ) : if not os . path . exists ( path ) : raise ResolveError if os . path . isfile ( path ) : path = os . path . dirname ( path ) for root , _ , _ in walk_up ( path ) : if is_redirecting ( root ) : env_paths = redirect_to_env_paths ( unipath ( root , '.cpenv' ) ) r = Resolver ( * env_paths ) return r . resolve ( ) raise ResolveError
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
11732	def registerGoodClass ( self , class_ ) : # Class itself added to "good" list self . _valid_classes . append ( class_ ) # Recurse into any inner classes for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
497	def _constructClassificationRecord ( self , inputs ) : # Count the number of unpredicted columns allSPColumns = inputs [ "spBottomUpOut" ] activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] score = anomaly . computeRawAnomalyScore ( activeSPColumns , self . _prevPredictedColumns ) spSize = len ( allSPColumns ) allTPCells = inputs [ 'tpTopDownOut' ] tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) classificationVector = numpy . array ( [ ] ) if self . classificationVectorType == 1 : # Classification Vector: [---TM Cells---] classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = inputs [ "tpLrnActiveStateT" ] . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . classificationVectorType == 2 : # Classification Vecotr: [---SP---|---(TM-SP)----] classificationVector = numpy . zeros ( spSize + spSize ) if activeSPColumns . shape [ 0 ] > 0 : classificationVector [ activeSPColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeSPColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . classificationVectorType ) ) # Store the state for next time step numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = allTPCells . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = self . _iteration , #__numRunCalls called #at beginning of model.run anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
13494	def write ( args ) : logging . info ( "Writing configure file: %s" % args . config_file ) if args . config_file is None : return #Let's add each attribute of 'args' to the configure file config = cparser . ConfigParser ( ) config . add_section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( "_" ) ] : if p in IGNORE_ARGS : continue #We ignore some attributes value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config_file , 'w' ) as f : config . write ( f )
7600	def get_popular_tournaments ( self , * * params : keys ) : url = self . api . POPULAR + '/tournament' return self . _get_model ( url , PartialTournament , * * params )
2073	def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
9029	def _step ( self , row , position , passed ) : if row in passed or not self . _row_should_be_placed ( row , position ) : return self . _place_row ( row , position ) passed = [ row ] + passed # print("{}{} at\t{} {}".format(" " * len(passed), row, position, # passed)) for i , produced_mesh in enumerate ( row . produced_meshes ) : self . _expand_produced_mesh ( produced_mesh , i , position , passed ) for i , consumed_mesh in enumerate ( row . consumed_meshes ) : self . _expand_consumed_mesh ( consumed_mesh , i , position , passed )
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
4738	def warn ( txt ) : print ( "%s# %s%s%s" % ( PR_WARN_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
1341	def imagenet_example ( shape = ( 224 , 224 ) , data_format = 'channels_last' ) : assert len ( shape ) == 2 assert data_format in [ 'channels_first' , 'channels_last' ] from PIL import Image path = os . path . join ( os . path . dirname ( __file__ ) , 'example.png' ) image = Image . open ( path ) image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) image = image [ : , : , : 3 ] assert image . shape == shape + ( 3 , ) if data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) return image , 282
3862	def _get_default_delivery_medium ( self ) : medium_options = ( self . _conversation . self_conversation_state . delivery_medium_option ) try : default_medium = medium_options [ 0 ] . delivery_medium except IndexError : logger . warning ( 'Conversation %r has no delivery medium' , self . id_ ) default_medium = hangouts_pb2 . DeliveryMedium ( medium_type = hangouts_pb2 . DELIVERY_MEDIUM_BABEL ) for medium_option in medium_options : if medium_option . current_default : default_medium = medium_option . delivery_medium return default_medium
3138	def get ( self , app_id , * * queryparams ) : self . app_id = app_id return self . _mc_client . _get ( url = self . _build_path ( app_id ) , * * queryparams )
1883	def new_symbolic_value ( self , nbits , label = None , taint = frozenset ( ) ) : assert nbits in ( 1 , 4 , 8 , 16 , 32 , 64 , 128 , 256 ) avoid_collisions = False if label is None : label = 'val' avoid_collisions = True expr = self . _constraints . new_bitvec ( nbits , name = label , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) return expr
3219	def get_network_acls ( vpc , * * conn ) : route_tables = describe_network_acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) nacl_ids = [ ] for r in route_tables : nacl_ids . append ( r [ "NetworkAclId" ] ) return nacl_ids
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
12116	def loadResults ( resultsFile ) : with open ( resultsFile ) as f : raw = f . read ( ) . split ( "\n" ) foldersByDay = { } for line in raw : folder = line . split ( '"' ) [ 1 ] + "\\" line = [ ] + line . split ( '"' ) [ 2 ] . split ( ", " ) for day in line [ 1 : ] : if not day in foldersByDay : foldersByDay [ day ] = [ ] foldersByDay [ day ] = foldersByDay [ day ] + [ folder ] nActiveDays = len ( foldersByDay ) dayFirst = sorted ( foldersByDay . keys ( ) ) [ 0 ] dayLast = sorted ( foldersByDay . keys ( ) ) [ - 1 ] dayFirst = datetime . datetime . strptime ( dayFirst , "%Y-%m-%d" ) dayLast = datetime . datetime . strptime ( dayLast , "%Y-%m-%d" ) nDays = ( dayLast - dayFirst ) . days + 1 emptyDays = 0 for deltaDays in range ( nDays ) : day = dayFirst + datetime . timedelta ( days = deltaDays ) stamp = datetime . datetime . strftime ( day , "%Y-%m-%d" ) if not stamp in foldersByDay : foldersByDay [ stamp ] = [ ] emptyDays += 1 percActive = nActiveDays / nDays * 100 print ( "%d of %d days were active (%.02f%%)" % ( nActiveDays , nDays , percActive ) ) return foldersByDay
2746	def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
11001	def _kpad ( self , field , finalshape , zpad = False , norm = True ) : currshape = np . array ( field . shape ) if any ( finalshape < currshape ) : raise IndexError ( "PSF tile size is less than minimum support size" ) d = finalshape - currshape # fix off-by-one issues when going odd to even tile sizes o = d % 2 d = np . floor_divide ( d , 2 ) if not zpad : o [ 0 ] = 0 axes = None pad = tuple ( ( d [ i ] + o [ i ] , d [ i ] ) for i in [ 0 , 1 , 2 ] ) rpsf = np . pad ( field , pad , mode = 'constant' , constant_values = 0 ) rpsf = np . fft . ifftshift ( rpsf , axes = axes ) kpsf = fft . rfftn ( rpsf , * * fftkwargs ) if norm : kpsf /= kpsf [ 0 , 0 , 0 ] return kpsf
9874	def aggregate ( l ) : tree = radix . Radix ( ) for item in l : try : tree . add ( item ) except ( ValueError ) as err : raise Exception ( "ERROR: invalid IP prefix: {}" . format ( item ) ) return aggregate_tree ( tree ) . prefixes ( )
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) # if Froude number is out of interpolation range, nearest extrapolation is used return Cr
7404	def below ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved below instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = self . get_ordering_queryset ( ) . filter ( order__gt = ref . order ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) or 0 else : o = ref . order self . to ( o )
11666	def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) # need to throw away the closest neighbor, which will always be self # thus K=1 corresponds to column 1 in the result array which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
12445	def options ( self , request , response ) : # Gather a list available HTTP/1.1 methods for this URI. response [ 'Allowed' ] = ', ' . join ( self . meta . http_allowed_methods ) # All CORS handling is done for every HTTP/1.1 method. # No more handling is neccesary; set the response to 200 and return. response . status = http . client . OK
9763	def upload ( sync = True ) : # pylint:disable=assign-to-new-keyword project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
708	def runWithJsonFile ( expJsonFilePath , options , outputLabel , permWorkDir ) : if "verbosityCount" in options : verbosity = options [ "verbosityCount" ] del options [ "verbosityCount" ] else : verbosity = 1 _setupInterruptHandling ( ) with open ( expJsonFilePath , "r" ) as jsonFile : expJsonConfig = json . loads ( jsonFile . read ( ) ) outDir = os . path . dirname ( expJsonFilePath ) return runWithConfig ( expJsonConfig , options , outDir = outDir , outputLabel = outputLabel , permWorkDir = permWorkDir , verbosity = verbosity )
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
4603	def copy ( self ) : return self . __class__ ( amount = self [ "amount" ] , asset = self [ "asset" ] . copy ( ) , blockchain_instance = self . blockchain , )
13204	def _parse_documentclass ( self ) : command = LatexCommand ( 'documentclass' , { 'name' : 'options' , 'required' : False , 'bracket' : '[' } , { 'name' : 'class_name' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no documentclass' ) self . _document_options = [ ] try : content = parsed [ 'options' ] self . _document_options = [ opt . strip ( ) for opt in content . split ( ',' ) ] except KeyError : self . _logger . warning ( 'lsstdoc has no documentclass options' ) self . _document_options = [ ]
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
8020	async def websocket_accept ( self , message , stream_name ) : is_first = not self . applications_accepting_frames self . applications_accepting_frames . add ( stream_name ) # accept the connection after the first upstream application accepts. if is_first : await self . accept ( )
8005	def from_xml ( self , xmlnode ) : if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:x:delay element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != DELAY_NS or xmlnode . name != "x" : raise ValueError ( "XML node is not a jabber:x:delay element" ) stamp = xmlnode . prop ( "stamp" ) if stamp . endswith ( "Z" ) : stamp = stamp [ : - 1 ] if "-" in stamp : stamp = stamp . split ( "-" , 1 ) [ 0 ] try : tm = time . strptime ( stamp , "%Y%m%dT%H:%M:%S" ) except ValueError : raise BadRequestProtocolError ( "Bad timestamp" ) tm = tm [ 0 : 8 ] + ( 0 , ) self . timestamp = datetime . datetime . fromtimestamp ( time . mktime ( tm ) ) delay_from = from_utf8 ( xmlnode . prop ( "from" ) ) if delay_from : try : self . delay_from = JID ( delay_from ) except JIDError : raise JIDMalformedProtocolError ( "Bad JID in the jabber:x:delay 'from' attribute" ) else : self . delay_from = None self . reason = from_utf8 ( xmlnode . getContent ( ) )
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) # Chop off initial '@' alias = self . _field [ 1 : ] self . _alias = alias return self
12331	def get_tree ( gitdir = "." ) : cmd = [ "git" , "log" , "--all" , "--branches" , '--pretty=format:{ "commit": "%H", "abbreviated_commit": "%h", "tree": "%T", "abbreviated_tree": "%t", "parent": "%P", "abbreviated_parent": "%p", "refs": "%d", "encoding": "%e", "subject": "%s", "sanitized_subject_line": "%f", "commit_notes": "", "author": { "name": "%aN", "email": "%aE", "date": "%ai" }, "commiter": { "name": "%cN", "email": "%cE", "date": "%ci" }},' ] output = run ( cmd ) lines = output . split ( "\n" ) content = "" history = [ ] for l in lines : try : revisedcontent = content + l if revisedcontent . count ( '"' ) % 2 == 0 : j = json . loads ( revisedcontent [ : - 1 ] ) if "Notes added by" in j [ 'subject' ] : content = "" continue history . append ( j ) content = "" else : content = revisedcontent except Exception as e : print ( "Error while parsing record" ) print ( revisedcontent ) content = "" # Order by time. First commit first... history . reverse ( ) # changes = get_change ( ) for i in range ( len ( history ) ) : abbrev_commit = history [ i ] [ 'abbreviated_commit' ] if abbrev_commit not in changes : raise Exception ( "Missing changes for " + abbrev_commit ) history [ i ] [ 'changes' ] = changes [ abbrev_commit ] [ 'changes' ] return history
5332	def get_panels ( config ) : task = TaskPanels ( config ) task . execute ( ) task = TaskPanelsMenu ( config ) task . execute ( ) logging . info ( "Panels creation finished!" )
8340	def toEncoding ( self , s , encoding = None ) : if isinstance ( s , unicode ) : if encoding : s = s . encode ( encoding ) elif isinstance ( s , str ) : if encoding : s = s . encode ( encoding ) else : s = unicode ( s ) else : if encoding : s = self . toEncoding ( str ( s ) , encoding ) else : s = unicode ( s ) return s
7962	def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
10004	def rename ( self , name ) : if is_valid_name ( name ) : if name not in self . system . models : self . name = name return True # Rename success else : # Model name already exists return False else : raise ValueError ( "Invalid name '%s'." % name )
12529	def load_command_table ( self , args ) : #pylint: disable=too-many-statements # Need an empty client for the select and upload operations with CommandSuperGroup ( __name__ , self , 'rcctl.custom_cluster#{}' ) as super_group : with super_group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with CommandSuperGroup ( __name__ , self , 'rcctl.custom_reliablecollections#{}' , client_factory = client_create ) as super_group : with super_group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query_reliabledictionary' ) group . command ( 'execute' , 'execute_reliabledictionary' ) group . command ( 'schema' , 'get_reliabledictionary_schema' ) group . command ( 'list' , 'get_reliabledictionary_list' ) group . command ( 'type-schema' , 'get_reliabledictionary_type_schema' ) with ArgumentsContext ( self , 'dictionary' ) as ac : ac . argument ( 'application_name' , options_list = [ '--application-name' , '-a' ] ) ac . argument ( 'service_name' , options_list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary_name' , options_list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output_file' , options_list = [ '--output-file' , '-out' ] ) ac . argument ( 'input_file' , options_list = [ '--input-file' , '-in' ] ) ac . argument ( 'query_string' , options_list = [ '--query-string' , '-q' ] ) ac . argument ( 'type_name' , options_list = [ '--type-name' , '-t' ] ) return OrderedDict ( self . command_table )
10607	def run ( self ) : self . prepare_to_run ( ) for i in range ( 0 , self . period_count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )
944	def getCheckpointParentDir ( experimentDir ) : baseDir = os . path . join ( experimentDir , "savedmodels" ) baseDir = os . path . abspath ( baseDir ) return baseDir
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) # normalize extension url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
204	def from_heatmaps ( heatmaps , class_indices = None , nb_classes = None ) : if class_indices is None : return SegmentationMapOnImage ( heatmaps . arr_0to1 , shape = heatmaps . shape ) else : ia . do_assert ( nb_classes is not None ) ia . do_assert ( min ( class_indices ) >= 0 ) ia . do_assert ( max ( class_indices ) < nb_classes ) ia . do_assert ( len ( class_indices ) == heatmaps . arr_0to1 . shape [ 2 ] ) arr_0to1 = heatmaps . arr_0to1 arr_0to1_full = np . zeros ( ( arr_0to1 . shape [ 0 ] , arr_0to1 . shape [ 1 ] , nb_classes ) , dtype = np . float32 ) for heatmap_channel , mapped_channel in enumerate ( class_indices ) : arr_0to1_full [ : , : , mapped_channel ] = arr_0to1 [ : , : , heatmap_channel ] return SegmentationMapOnImage ( arr_0to1_full , shape = heatmaps . shape )
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] # transform glob patterns to regular expressions # print("Includes ", includes) includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : # print("Looking at ", files) # exclude dirs # dirs[:] = [os.path.join(root, d) for d in dirs] dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] # exclude/include files files = [ f for f in files if not re . match ( excludes , f ) ] #print("Files after excludes", files) #print(includes) files = [ f for f in files if re . match ( includes , f ) ] #print("Files after includes", files) files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
8551	def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response
5534	def execute ( self , process_tile , raise_nodata = False ) : if self . config . mode not in [ "memory" , "continue" , "overwrite" ] : raise ValueError ( "process mode must be memory, continue or overwrite" ) if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif isinstance ( process_tile , BufferedTile ) : pass else : raise TypeError ( "process_tile must be tuple or BufferedTile" ) if process_tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( process_tile ) return self . _execute ( process_tile , raise_nodata = raise_nodata )
2099	def batch_update ( self , pk = None , * * kwargs ) : res = self . get ( pk = pk , * * kwargs ) url = self . endpoint + '%d/%s/' % ( res [ 'id' ] , 'update_inventory_sources' ) return client . post ( url , data = { } ) . json ( )
9616	def is_displayed ( target ) : is_displayed = getattr ( target , 'is_displayed' , None ) if not is_displayed or not callable ( is_displayed ) : raise TypeError ( 'Target has no attribute \'is_displayed\' or not callable' ) if not is_displayed ( ) : raise WebDriverException ( 'element not visible' )
8053	def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = ShoebotCmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( _ ( "Connected" ) ) GObject . io_add_watch ( conn , GObject . IO_IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + "\n" ) self . shell . stdout . flush ( ) return True
10579	def add_to ( self , other ) : # Add another package. if type ( other ) is MaterialPackage : # Packages of the same material. if self . material == other . material : self . compound_masses += other . compound_masses # Packages of different materials. else : for compound in other . material . compounds : if compound not in self . material . compounds : raise Exception ( "Packages of '" + other . material . name + "' cannot be added to packages of '" + self . material . name + "'. The compound '" + compound + "' was not found in '" + self . material . name + "'." ) self . add_to ( ( compound , other . get_compound_mass ( compound ) ) ) # Add the specified mass of the specified compound. elif self . _is_compound_mass_tuple ( other ) : # Added material variables. compound = other [ 0 ] compound_index = self . material . get_compound_index ( compound ) mass = other [ 1 ] # Create the result package. self . compound_masses [ compound_index ] += mass # If not one of the above, it must be an invalid argument. else : raise TypeError ( 'Invalid addition argument.' )
12015	def define_spotsignal ( self ) : client = kplr . API ( ) star = client . star ( self . kic ) lcs = star . get_light_curves ( short_cadence = False ) time , flux , ferr , qual = [ ] , [ ] , [ ] , [ ] for lc in lcs : with lc . open ( ) as f : hdu_data = f [ 1 ] . data time . append ( hdu_data [ "time" ] ) flux . append ( hdu_data [ "pdcsap_flux" ] ) ferr . append ( hdu_data [ "pdcsap_flux_err" ] ) qual . append ( hdu_data [ "sap_quality" ] ) tout = np . array ( [ ] ) fout = np . array ( [ ] ) eout = np . array ( [ ] ) for i in range ( len ( flux ) ) : t = time [ i ] [ qual [ i ] == 0 ] f = flux [ i ] [ qual [ i ] == 0 ] e = ferr [ i ] [ qual [ i ] == 0 ] t = t [ np . isfinite ( f ) ] e = e [ np . isfinite ( f ) ] f = f [ np . isfinite ( f ) ] e /= np . median ( f ) f /= np . median ( f ) tout = np . append ( tout , t [ 50 : ] + 54833 ) fout = np . append ( fout , f [ 50 : ] ) eout = np . append ( eout , e [ 50 : ] ) self . spot_signal = np . zeros ( 52 ) for i in range ( len ( self . times ) ) : if self . times [ i ] < 55000 : self . spot_signal [ i ] = 1.0 else : self . spot_signal [ i ] = fout [ np . abs ( self . times [ i ] - tout ) == np . min ( np . abs ( self . times [ i ] - tout ) ) ]
6961	def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 ] , int ) : # python3 byte strings for x , y in zip ( a , b ) : result |= x ^ y else : # python2 for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0
9814	def start ( ctx , file , u ) : # pylint:disable=redefined-builtin specification = None job_config = None if file : specification = check_polyaxonfile ( file , log = False ) . specification # Check if we need to upload if u : ctx . invoke ( upload , sync = False ) if specification : # pylint:disable=protected-access check_polyaxonfile_kind ( specification = specification , kind = specification . _NOTEBOOK ) job_config = specification . parsed_data user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . start_notebook ( user , project_name , job_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not start notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 200 : Printer . print_header ( "A notebook for this project is already running on:" ) click . echo ( get_notebook_url ( user , project_name ) ) sys . exit ( 0 ) if response . status_code != 201 : Printer . print_error ( 'Something went wrong, Notebook was not created.' ) sys . exit ( 1 ) Printer . print_success ( 'Notebook is being deployed for project `{}`' . format ( project_name ) ) clint . textui . puts ( "It may take some time before you can access the notebook.\n" ) clint . textui . puts ( "Your notebook will be available on:\n" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( get_notebook_url ( user , project_name ) )
5167	def __intermediate_dns_search ( self , uci , address ) : # allow override if 'dns_search' in uci : return uci [ 'dns_search' ] # ignore if "proto" is none if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
4213	def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
13075	def create_blueprint ( self ) : self . register_plugins ( ) self . blueprint = Blueprint ( self . name , "nemo" , url_prefix = self . prefix , template_folder = self . template_folder , static_folder = self . static_folder , static_url_path = self . static_url_path ) for url , name , methods , instance in self . _urls : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) , methods = methods ) for url , name , methods , instance in self . _semantic_url : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) + "_semantic" , methods = methods ) self . register_assets ( ) self . register_filters ( ) # We extend the loading list by the instance value self . __templates_namespaces__ . extend ( self . __instance_templates__ ) # We generate a template loader for namespace , directory in self . __templates_namespaces__ [ : : - 1 ] : if namespace not in self . __template_loader__ : self . __template_loader__ [ namespace ] = [ ] self . __template_loader__ [ namespace ] . append ( jinja2 . FileSystemLoader ( op . abspath ( directory ) ) ) self . blueprint . jinja_loader = jinja2 . PrefixLoader ( { namespace : jinja2 . ChoiceLoader ( paths ) for namespace , paths in self . __template_loader__ . items ( ) } , "::" ) if self . cache is not None : for func , instance in self . cached : setattr ( instance , func . __name__ , self . cache . memoize ( ) ( func ) ) return self . blueprint
572	def rCopy ( d , f = identityConversion , discardNoneKeys = True , deepCopy = True ) : # Optionally deep copy the dict. if deepCopy : d = copy . deepcopy ( d ) newDict = { } toCopy = [ ( k , v , newDict , ( ) ) for k , v in d . iteritems ( ) ] while len ( toCopy ) > 0 : k , v , d , prevKeys = toCopy . pop ( ) prevKeys = prevKeys + ( k , ) if isinstance ( v , dict ) : d [ k ] = dict ( ) toCopy [ 0 : 0 ] = [ ( innerK , innerV , d [ k ] , prevKeys ) for innerK , innerV in v . iteritems ( ) ] else : #print k, v, prevKeys newV = f ( v , prevKeys ) if not discardNoneKeys or newV is not None : d [ k ] = newV return newDict
831	def decode ( self , encoded , parentFieldName = '' ) : fieldsDict = dict ( ) fieldsOrder = [ ] # What is the effective parent name? if parentFieldName == '' : parentName = self . name else : parentName = "%s.%s" % ( parentFieldName , self . name ) if self . encoders is not None : # Merge decodings of all child encoders together for i in xrange ( len ( self . encoders ) ) : # Get the encoder and the encoded output ( name , encoder , offset ) = self . encoders [ i ] if i < len ( self . encoders ) - 1 : nextOffset = self . encoders [ i + 1 ] [ 2 ] else : nextOffset = self . width fieldOutput = encoded [ offset : nextOffset ] ( subFieldsDict , subFieldsOrder ) = encoder . decode ( fieldOutput , parentFieldName = parentName ) fieldsDict . update ( subFieldsDict ) fieldsOrder . extend ( subFieldsOrder ) return ( fieldsDict , fieldsOrder )
2971	def dependency_sorted ( containers ) : if not isinstance ( containers , collections . Mapping ) : containers = dict ( ( c . name , c ) for c in containers ) container_links = dict ( ( name , set ( c . links . keys ( ) ) ) for name , c in containers . items ( ) ) sorted_names = _resolve ( container_links ) return [ containers [ name ] for name in sorted_names ]
3540	def status_printer ( ) : last_len = [ 0 ] def p ( s ) : s = next ( spinner ) + ' ' + s len_s = len ( s ) output = '\r' + s + ( ' ' * max ( last_len [ 0 ] - len_s , 0 ) ) sys . stdout . write ( output ) sys . stdout . flush ( ) last_len [ 0 ] = len_s return p
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
7906	def __groupchat_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : self . __logger . debug ( "groupchat message from unknown source" ) return False rs . process_groupchat_message ( stanza ) return True
11189	def edit ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) try : # Python2 compatibility. readme_content = unicode ( readme_content , "utf-8" ) except NameError : pass edited_content = click . edit ( readme_content ) if edited_content is not None : _validate_and_put_readme ( dataset , edited_content ) click . secho ( "Updated readme " , nl = False , fg = "green" ) else : click . secho ( "Did not update readme " , nl = False , fg = "red" ) click . secho ( dataset_uri )
1246	def disconnect ( self ) : # If we are not connected, return error. if not self . socket : logging . warning ( "No active socket to close!" ) return # Close our socket. self . socket . close ( ) self . socket = None
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
7086	def _single_true ( iterable ) : # return True if exactly one true found iterator = iter ( iterable ) # consume from "i" until first true or it's exhausted has_true = any ( iterator ) # carry on consuming until another true value / exhausted has_another_true = any ( iterator ) return has_true and not has_another_true
11979	def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise ValueError ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , IPv4Address ) : self . _ip = ip else : self . _ip = IPv4Address ( ip ) if isinstance ( netmask , IPv4NetMask ) : self . _nm = netmask else : self . _nm = IPv4NetMask ( netmask ) ipl = int ( self . _ip ) nml = int ( self . _nm ) base_add = ipl & nml self . _ip_num = 0xFFFFFFFF - 1 - nml # NOTE: quite a mess. # This's here to handle /32 (-1) and /31 (0) netmasks. if self . _ip_num in ( - 1 , 0 ) : if self . _ip_num == - 1 : self . _ip_num = 1 else : self . _ip_num = 2 self . _net_ip = None self . _bc_ip = None self . _first_ip_dec = base_add self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) if self . _ip_num == 1 : last_ip_dec = self . _first_ip_dec else : last_ip_dec = self . _first_ip_dec + 1 self . _last_ip = IPv4Address ( last_ip_dec , notation = IP_DEC ) return self . _net_ip = IPv4Address ( base_add , notation = IP_DEC ) self . _bc_ip = IPv4Address ( base_add + self . _ip_num + 1 , notation = IP_DEC ) self . _first_ip_dec = base_add + 1 self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) self . _last_ip = IPv4Address ( base_add + self . _ip_num , notation = IP_DEC )
9551	def _init_unique_sets ( self ) : ks = dict ( ) for t in self . _unique_checks : key = t [ 0 ] ks [ key ] = set ( ) # empty set return ks
1783	def ADC ( cpu , dest , src ) : cpu . _ADD ( dest , src , carry = True )
4053	def all_collections ( self , collid = None ) : all_collections = [ ] def subcoll ( clct ) : """ recursively add collections to a flat master list """ all_collections . append ( clct ) if clct [ "meta" ] . get ( "numCollections" , 0 ) > 0 : # add collection to master list & recur with all child # collections [ subcoll ( c ) for c in self . everything ( self . collections_sub ( clct [ "data" ] [ "key" ] ) ) ] # select all top-level collections or a specific collection and # children if collid : toplevel = [ self . collection ( collid ) ] else : toplevel = self . everything ( self . collections_top ( ) ) [ subcoll ( collection ) for collection in toplevel ] return all_collections
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
9898	def _initfile ( path , data = "dict" ) : data = { } if data . lower ( ) == "dict" else [ ] # The file will need to be created if it doesn't exist if not os . path . exists ( path ) : # The file doesn't exist # Raise exception if the directory that should contain the file doesn't # exist dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IOError ( ( "Could not initialize empty JSON file in non-existant " "directory '{}'" ) . format ( os . path . dirname ( path ) ) ) # Write an empty file there with open ( path , "w" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : # The file is empty with open ( path , "w" ) as f : json . dump ( data , f ) else : # The file exists and contains content return False
6613	def receive_finished ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . poll ( )
5936	def col ( self , c ) : m = self . COLOUR . search ( c ) if not m : self . logger . fatal ( "Cannot parse colour specification %r." , c ) raise ParseError ( "XPM reader: Cannot parse colour specification {0!r}." . format ( c ) ) value = m . group ( 'value' ) color = m . group ( 'symbol' ) self . logger . debug ( "%s: %s %s\n" , c . strip ( ) , color , value ) return color , value
3006	def get_storage ( request ) : storage_model = oauth2_settings . storage_model user_property = oauth2_settings . storage_model_user_property credentials_property = oauth2_settings . storage_model_credentials_property if storage_model : module_name , class_name = storage_model . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) storage_model_class = getattr ( module , class_name ) return storage . DjangoORMStorage ( storage_model_class , user_property , request . user , credentials_property ) else : # use session return dictionary_storage . DictionaryStorage ( request . session , key = _CREDENTIALS_KEY )
5541	def hillshade ( self , elevation , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : return commons_hillshade . hillshade ( elevation , self , azimuth , altitude , z , scale )
9435	def _read_a_packet ( file_h , hdrp , layers = 0 ) : raw_packet_header = file_h . read ( 16 ) if not raw_packet_header or len ( raw_packet_header ) != 16 : return None # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the packet header if hdrp [ 0 ] . byteorder == 'big' : packet_header = struct . unpack ( '>IIII' , raw_packet_header ) else : packet_header = struct . unpack ( '<IIII' , raw_packet_header ) ( timestamp , timestamp_us , capture_len , packet_len ) = packet_header raw_packet_data = file_h . read ( capture_len ) if not raw_packet_data or len ( raw_packet_data ) != capture_len : return None if layers > 0 : layers -= 1 raw_packet = linklayer . clookup ( hdrp [ 0 ] . ll_type ) ( raw_packet_data , layers = layers ) else : raw_packet = raw_packet_data packet = pcap_packet ( hdrp , timestamp , timestamp_us , capture_len , packet_len , raw_packet ) return packet
12976	def compat_convertHashedIndexes ( self , objs , conn = None ) : if conn is None : conn = self . _get_connection ( ) # Do one pipeline per object. # XXX: Maybe we should do the whole thing in one pipeline? fields = [ ] # A list of the indexed fields # Iterate now so we do this once instead of per-object. for indexedField in self . indexedFields : origField = self . fields [ indexedField ] # Check if type supports configurable hashIndex, and if not skip it. if 'hashIndex' not in origField . __class__ . __new__ . __code__ . co_varnames : continue if indexedField . hashIndex is True : hashingField = origField regField = origField . copy ( ) regField . hashIndex = False else : regField = origField # Maybe copy should allow a dict of override params? hashingField = origField . copy ( ) hashingField . hashIndex = True fields . append ( ( origField , regField , hashingField ) ) objDicts = [ obj . asDict ( True , forStorage = True ) for obj in objs ] # Iterate over all values. Remove the possibly stringed index, the possibly hashed index, and then put forth the hashed index. for objDict in objDicts : pipeline = conn . pipeline ( ) pk = objDict [ '_id' ] for origField , regField , hashingField in fields : val = objDict [ indexedField ] # Remove the possibly stringed index self . _rem_id_from_index ( regField , pk , val , pipeline ) # Remove the possibly hashed index self . _rem_id_from_index ( hashingField , pk , val , pipeline ) # Add the new (hashed or unhashed) form. self . _add_id_to_index ( origField , pk , val , pipeline ) # Launch all at once pipeline . execute ( )
13755	def write_to_file ( file_path , contents , encoding = "utf-8" ) : with codecs . open ( file_path , "w" , encoding ) as f : f . write ( contents )
9056	def sample ( self , random_state = None ) : from numpy_sugar import epsilon from numpy_sugar . linalg import sum2diag from numpy_sugar . random import multivariate_normal if random_state is None : random_state = RandomState ( ) m = self . _mean . value ( ) K = self . _cov . value ( ) . copy ( ) sum2diag ( K , + epsilon . small , out = K ) return self . _lik . sample ( multivariate_normal ( m , K , random_state ) , random_state )
5381	def _datetime_to_utc_int ( date ) : if date is None : return None # Convert localized datetime to a UTC integer epoch = dsub_util . replace_timezone ( datetime . utcfromtimestamp ( 0 ) , pytz . utc ) return ( date - epoch ) . total_seconds ( )
13596	def get ( f , key , default = None ) : if key in f . keys ( ) : val = f [ key ] . value if default is None : return val else : if _np . shape ( val ) == _np . shape ( default ) : return val return default
4403	def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
13388	def ttl ( self , response ) : if response . code != 200 : return 0 if not self . request . method in [ 'GET' , 'HEAD' , 'OPTIONS' ] : return 0 try : pragma = self . request . headers [ 'pragma' ] if pragma == 'no-cache' : return 0 except KeyError : pass try : cache_control = self . request . headers [ 'cache-control' ] # no caching options for option in [ 'private' , 'no-cache' , 'no-store' , 'must-revalidate' , 'proxy-revalidate' ] : if cache_control . find ( option ) : return 0 # further parsing to get a ttl options = parse_cache_control ( cache_control ) try : return int ( options [ 's-maxage' ] ) except KeyError : pass try : return int ( options [ 'max-age' ] ) except KeyError : pass if 's-maxage' in options : max_age = options [ 's-maxage' ] if max_age < ttl : ttl = max_age if 'max-age' in options : max_age = options [ 'max-age' ] if max_age < ttl : ttl = max_age return ttl except KeyError : pass try : expires = self . request . headers [ 'expires' ] return time . mktime ( time . strptime ( expires , '%a, %d %b %Y %H:%M:%S' ) ) - time . time ( ) except KeyError : pass
175	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : assert color is not None assert alpha is not None assert size is not None color_lines = color_lines if color_lines is not None else np . float32 ( color ) color_points = color_points if color_points is not None else np . float32 ( color ) * 0.5 alpha_lines = alpha_lines if alpha_lines is not None else np . float32 ( alpha ) alpha_points = alpha_points if alpha_points is not None else np . float32 ( alpha ) size_lines = size_lines if size_lines is not None else size size_points = size_points if size_points is not None else size * 3 image = self . draw_lines_on_image ( image , color = np . array ( color_lines ) . astype ( np . uint8 ) , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) image = self . draw_points_on_image ( image , color = np . array ( color_points ) . astype ( np . uint8 ) , alpha = alpha_points , size = size_points , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image
10243	def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
2409	def dump_model_to_file ( prompt_string , feature_ext , classifier , text , score , model_path ) : model_file = { 'prompt' : prompt_string , 'extractor' : feature_ext , 'model' : classifier , 'text' : text , 'score' : score } pickle . dump ( model_file , file = open ( model_path , "w" ) )
3132	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . list_id = response [ 'id' ] else : self . list_id = None return response
2326	def orient_graph ( self , df_data , graph , nb_runs = 6 , printout = None , * * kwargs ) : if type ( graph ) == nx . DiGraph : edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) ] oriented_edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) not in list ( graph . edges ( ) ) ] for a in edges : if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) : edges . remove ( a ) output = nx . DiGraph ( ) for i in oriented_edges : output . add_edge ( * i ) elif type ( graph ) == nx . Graph : edges = list ( graph . edges ( ) ) output = nx . DiGraph ( ) else : raise TypeError ( "Data type not understood." ) res = [ ] for idx , ( a , b ) in enumerate ( edges ) : weight = self . predict_proba ( df_data [ a ] . values . reshape ( ( - 1 , 1 ) ) , df_data [ b ] . values . reshape ( ( - 1 , 1 ) ) , idx = idx , nb_runs = nb_runs , * * kwargs ) if weight > 0 : # a causes b output . add_edge ( a , b , weight = weight ) else : output . add_edge ( b , a , weight = abs ( weight ) ) if printout is not None : res . append ( [ str ( a ) + '-' + str ( b ) , weight ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) for node in list ( df_data . columns . values ) : if node not in output . nodes ( ) : output . add_node ( node ) return output
10803	def tk ( self , k , x ) : weights = np . diag ( np . ones ( k + 1 ) ) [ k ] return np . polynomial . chebyshev . chebval ( self . _x2c ( x ) , weights )
65	def clip_out_of_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] ia . do_assert ( height > 0 ) ia . do_assert ( width > 0 ) eps = np . finfo ( np . float32 ) . eps x1 = np . clip ( self . x1 , 0 , width - eps ) x2 = np . clip ( self . x2 , 0 , width - eps ) y1 = np . clip ( self . y1 , 0 , height - eps ) y2 = np . clip ( self . y2 , 0 , height - eps ) return self . copy ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 , label = self . label )
11769	def printf ( format , * args ) : sys . stdout . write ( str ( format ) % args ) return if_ ( args , lambda : args [ - 1 ] , lambda : format )
8545	def get_datacenter_by_name ( self , name , depth = 1 ) : all_data_centers = self . list_datacenters ( depth = depth ) [ 'items' ] data_center = find_item_by_name ( all_data_centers , lambda i : i [ 'properties' ] [ 'name' ] , name ) if not data_center : raise NameError ( "No data center found with name " "containing '{name}'." . format ( name = name ) ) if len ( data_center ) > 1 : raise NameError ( "Found {n} data centers with the name '{name}': {names}" . format ( n = len ( data_center ) , name = name , names = ", " . join ( d [ 'properties' ] [ 'name' ] for d in data_center ) ) ) return data_center [ 0 ]
3422	def get_context ( obj ) : try : return obj . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass try : return obj . _model . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass return None
341	def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : # pylint: disable=global-variable-not-assigned global _level_names # pylint: enable=global-variable-not-assigned # Record current time now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) # Severity string severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , # month now_tuple [ 2 ] , # day now_tuple [ 3 ] , # hour now_tuple [ 4 ] , # min now_tuple [ 5 ] , # sec now_microsecond , _get_thread_id ( ) , basename , line ) return s
5071	def get_configuration_value_for_site ( site , key , default = None ) : if hasattr ( site , 'configuration' ) : return site . configuration . get_value ( key , default ) return default
4689	def init_aes ( shared_secret , nonce ) : " Shared Secret " ss = hashlib . sha512 ( unhexlify ( shared_secret ) ) . digest ( ) " Seed " seed = bytes ( str ( nonce ) , "ascii" ) + hexlify ( ss ) seed_digest = hexlify ( hashlib . sha512 ( seed ) . digest ( ) ) . decode ( "ascii" ) " AES " key = unhexlify ( seed_digest [ 0 : 64 ] ) iv = unhexlify ( seed_digest [ 64 : 96 ] ) return AES . new ( key , AES . MODE_CBC , iv )
1167	def _dump_registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . __module__ , cls . __name__ ) print >> file , "Inv.counter: %s" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( "_abc_" ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
11017	def signed_number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number_str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number_str
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) # pylint: disable=invalid-name try : return EnterpriseCustomer . objects . get ( uuid = uuid ) # pylint: disable=no-member except EnterpriseCustomer . DoesNotExist : return None
308	def plot_prob_profit_trade ( round_trips , ax = None ) : x = np . linspace ( 0 , 1. , 500 ) round_trips [ 'profitable' ] = round_trips . pnl > 0 dist = sp . stats . beta ( round_trips . profitable . sum ( ) , ( ~ round_trips . profitable ) . sum ( ) ) y = dist . pdf ( x ) lower_perc = dist . ppf ( .025 ) upper_perc = dist . ppf ( .975 ) lower_plot = dist . ppf ( .001 ) upper_plot = dist . ppf ( .999 ) if ax is None : ax = plt . subplot ( ) ax . plot ( x , y ) ax . axvline ( lower_perc , color = '0.5' ) ax . axvline ( upper_perc , color = '0.5' ) ax . set_xlabel ( 'Probability of making a profitable decision' ) ax . set_ylabel ( 'Belief' ) ax . set_xlim ( lower_plot , upper_plot ) ax . set_ylim ( ( 0 , y . max ( ) + 1. ) ) return ax
11450	def get_date ( self , filename ) : try : self . document = parse ( filename ) return self . _get_date ( ) except DateNotFoundException : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
1096	def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
5073	def get_request_value ( request , key , default = None ) : if request . method in [ 'GET' , 'DELETE' ] : return request . query_params . get ( key , request . data . get ( key , default ) ) return request . data . get ( key , request . query_params . get ( key , default ) )
2252	def iterable ( obj , strok = False ) : try : iter ( obj ) except Exception : return False else : return strok or not isinstance ( obj , six . string_types )
1536	def get_heron_options_from_env ( ) : heron_options_raw = os . environ . get ( "HERON_OPTIONS" ) if heron_options_raw is None : raise RuntimeError ( "HERON_OPTIONS environment variable not found" ) options = { } for option_line in heron_options_raw . replace ( "%%%%" , " " ) . split ( ',' ) : key , sep , value = option_line . partition ( "=" ) if sep : options [ key ] = value else : raise ValueError ( "Invalid HERON_OPTIONS part %r" % option_line ) return options
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , * * kwargs ) return wrapper
10280	def bond_task ( perc_graph_result , seeds , ps , convolution_factors_tasks_iterator ) : # restore the list of convolution factors tasks convolution_factors_tasks = list ( convolution_factors_tasks_iterator ) return reduce ( percolate . hpc . bond_reduce , map ( bond_run , itertools . repeat ( perc_graph_result ) , seeds , itertools . repeat ( ps ) , itertools . repeat ( convolution_factors_tasks ) , ) )
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
2616	def initialize_boto_client ( self ) : self . session = self . create_session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance_states = { } self . vpc_id = 0 self . sg_id = 0 self . sn_ids = [ ]
2756	def get_all_tags ( self ) : data = self . get_data ( "tags" ) return [ Tag ( token = self . token , * * tag ) for tag in data [ 'tags' ] ]
8763	def update_security_group_rule ( context , id , security_group_rule ) : LOG . info ( "update_security_group_rule for tenant %s" % ( context . tenant_id ) ) new_rule = security_group_rule [ "security_group_rule" ] # Only allow updatable fields new_rule = _filter_update_security_group_rule ( new_rule ) with context . session . begin ( ) : rule = db_api . security_group_rule_find ( context , id = id , scope = db_api . ONE ) if not rule : raise sg_ext . SecurityGroupRuleNotFound ( id = id ) db_rule = db_api . security_group_rule_update ( context , rule , * * new_rule ) group_id = db_rule . group_id group = db_api . security_group_find ( context , id = group_id , scope = db_api . ONE ) if not group : raise sg_ext . SecurityGroupNotFound ( id = group_id ) if group : _perform_async_update_rule ( context , group_id , group , rule . id , RULE_UPDATE ) return v . _make_security_group_rule_dict ( db_rule )
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
11765	def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : x , y = move n = 0 # n is number of moves in row while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 # Because we counted move itself twice return n >= self . k
10365	def has_translocation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , TRANSLOCATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
3328	def get_lock ( self , token , key = None ) : assert key in ( None , "type" , "scope" , "depth" , "owner" , "root" , "timeout" , "principal" , "token" , ) lock = self . storage . get ( token ) if key is None or lock is None : return lock return lock [ key ]
3108	def locked_get ( self ) : query = { self . key_name : self . key_value } entities = self . model_class . objects . filter ( * * query ) if len ( entities ) > 0 : credential = getattr ( entities [ 0 ] , self . property_name ) if getattr ( credential , 'set_store' , None ) is not None : credential . set_store ( self ) return credential else : return None
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
1471	def setup ( executor ) : # pylint: disable=unused-argument def signal_handler ( signal_to_handle , frame ) : # We would do nothing here but just exit # Just catch the SIGTERM and then cleanup(), registered with atexit, would invoke Log . info ( 'signal_handler invoked with signal %s' , signal_to_handle ) executor . stop_state_manager_watches ( ) sys . exit ( signal_to_handle ) def cleanup ( ) : """Handler to trigger when receiving the SIGTERM signal Do cleanup inside this method, including: 1. Terminate all children processes """ Log . info ( 'Executor terminated; exiting all process in executor.' ) # Kill child processes first and wait for log collection to finish for pid in executor . processes_to_monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) # We would not wait or check whether process spawned dead or not os . killpg ( 0 , signal . SIGTERM ) # Redirect stdout and stderr to files in append mode # The filename format is heron-executor-<container_id>.stdxxx shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) # POSIX prohibits the change of the process group ID of a session leader if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) # create new process group, become its leader Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal_handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
2266	def map_vals ( func , dict_ ) : if not hasattr ( func , '__call__' ) : func = func . __getitem__ keyval_list = [ ( key , func ( val ) ) for key , val in six . iteritems ( dict_ ) ] dictclass = OrderedDict if isinstance ( dict_ , OrderedDict ) else dict newdict = dictclass ( keyval_list ) # newdict = type(dict_)(keyval_list) return newdict
2832	def get_platform_pwm ( * * keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPi_PWM_Adapter ( RPi . GPIO , * * keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . PWM return BBIO_PWM_Adapter ( Adafruit_BBIO . PWM , * * keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
13057	def get_locale ( self ) : best_match = request . accept_languages . best_match ( [ 'de' , 'fr' , 'en' , 'la' ] ) if best_match is None : if len ( request . accept_languages ) > 0 : best_match = request . accept_languages [ 0 ] [ 0 ] [ : 2 ] else : return self . __default_lang__ lang = self . __default_lang__ if best_match == "de" : lang = "ger" elif best_match == "fr" : lang = "fre" elif best_match == "en" : lang = "eng" elif best_match == "la" : lang = "lat" return lang
779	def _getOneMatchingRowNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames ) : rows = self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = 1 ) if rows : assert len ( rows ) == 1 , repr ( len ( rows ) ) result = rows [ 0 ] else : result = None return result
1762	def push_bytes ( self , data , force = False ) : self . STACK -= len ( data ) self . write_bytes ( self . STACK , data , force ) return self . STACK
4745	def dev_get_chunk ( dev_name , state , pugrp = None , punit = None ) : rprt = dev_get_rprt ( dev_name , pugrp , punit ) if not rprt : return None return next ( ( d for d in rprt if d [ "cs" ] == state ) , None )
6858	def database_exists ( name , * * kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = query ( "SHOW DATABASES LIKE '%(name)s';" % { 'name' : name } , * * kwargs ) return res . succeeded and ( res == name )
3884	def from_conv_part_data ( conv_part_data , self_user_id ) : user_id = UserID ( chat_id = conv_part_data . id . chat_id , gaia_id = conv_part_data . id . gaia_id ) return User ( user_id , conv_part_data . fallback_name , None , None , [ ] , ( self_user_id == user_id ) or ( self_user_id is None ) )
11693	def label_suspicious ( self , reason ) : self . suspicion_reasons . append ( reason ) self . is_suspect = True
10679	def S ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : # Create a modified exponent to analytically integrate Cp(T)/T # instead of Cp(T). e_modified = e - 1.0 if e_modified == - 1.0 : result += c * math . log ( lT / Tref ) else : e_mod = e_modified + 1.0 result += c * ( lT ** e_mod - Tref ** e_mod ) / e_mod return result
10266	def collapse_consistent_edges ( graph : BELGraph ) : for u , v in graph . edges ( ) : relation = pair_is_consistent ( graph , u , v ) if not relation : continue edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges ) graph . add_edge ( u , v , attr_dict = { RELATION : relation } )
1661	def ExpectingFunctionArgs ( clean_lines , linenum ) : line = clean_lines . elided [ linenum ] return ( Match ( r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(' , line ) or ( linenum >= 2 and ( Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\((?:\S+,)?\s*$' , clean_lines . elided [ linenum - 1 ] ) or Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\(\s*$' , clean_lines . elided [ linenum - 2 ] ) or Search ( r'\bstd::m?function\s*\<\s*$' , clean_lines . elided [ linenum - 1 ] ) ) ) )
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , * * self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) # return response.text return GetReferenceDataResponse . deserialize ( response . json ( ) )
12258	def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
6511	def _eat_name_line ( self , line ) : if line [ 0 ] not in "#=" : parts = line . split ( ) country_values = line [ 30 : - 1 ] name = map_name ( parts [ 1 ] ) if not self . case_sensitive : name = name . lower ( ) if parts [ 0 ] == "M" : self . _set ( name , u"male" , country_values ) elif parts [ 0 ] == "1M" or parts [ 0 ] == "?M" : self . _set ( name , u"mostly_male" , country_values ) elif parts [ 0 ] == "F" : self . _set ( name , u"female" , country_values ) elif parts [ 0 ] == "1F" or parts [ 0 ] == "?F" : self . _set ( name , u"mostly_female" , country_values ) elif parts [ 0 ] == "?" : self . _set ( name , self . unknown_value , country_values ) else : raise "Not sure what to do with a sex of %s" % parts [ 0 ]
463	def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : text = "[TL] Open tensorboard, go to localhost:" + str ( port ) + " to access" text2 = " not yet supported by this function (tl.ops.open_tb)" if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : tl . logging . info ( "[TL] Log reportory was created at %s" % log_dir ) if _platform == "linux" or _platform == "linux2" : raise NotImplementedError ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( sys . prefix + " | python -m tensorflow.tensorboard --logdir=" + log_dir + " --port=" + str ( port ) , shell = True ) # open tensorboard in localhost:6006/ or whatever port you chose elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( _platform + text2 )
8319	def parse_tables ( self , markup ) : tables = [ ] m = re . findall ( self . re [ "table" ] , markup ) for chunk in m : table = WikipediaTable ( ) table . properties = chunk . split ( "\n" ) [ 0 ] . strip ( "{|" ) . strip ( ) self . connect_table ( table , chunk , markup ) # Tables start with "{|". # On the same line can be properties, e.g. {| border="1" # The table heading starts with "|+". # A new row in the table starts with "|-". # The end of the table is marked with "|}". row = None for chunk in chunk . split ( "\n" ) : chunk = chunk . strip ( ) if chunk . startswith ( "|+" ) : title = self . plain ( chunk . strip ( "|+" ) ) table . title = title elif chunk . startswith ( "|-" ) : if row : row . properties = chunk . strip ( "|-" ) . strip ( ) table . append ( row ) row = None elif chunk . startswith ( "|}" ) : pass elif chunk . startswith ( "|" ) or chunk . startswith ( "!" ) : row = self . parse_table_row ( chunk , row ) # Append the last row. if row : table . append ( row ) if len ( table ) > 0 : tables . append ( table ) return tables
11731	def _compress ( self , input_str ) : compressed_bits = cStringIO . StringIO ( ) f = gzip . GzipFile ( fileobj = compressed_bits , mode = 'wb' ) f . write ( input_str ) f . close ( ) return compressed_bits . getvalue ( )
7564	def get_client ( cluster_id , profile , engines , timeout , cores , quiet , spacer , * * kwargs ) : ## save stds for later, we're gonna hide them to prevent external printing save_stdout = sys . stdout save_stderr = sys . stderr sys . stdout = cStringIO . StringIO ( ) sys . stderr = cStringIO . StringIO ( ) ## get cluster_info print string connection_string = "{}establishing parallel connection:" . format ( spacer ) ## wrapped search for ipcluster try : ## are we looking for a running ipcluster instance? if profile not in [ None , "default" ] : args = { 'profile' : profile , "timeout" : timeout } else : clusterargs = [ cluster_id , profile , timeout ] argnames = [ "cluster_id" , "profile" , "timeout" ] args = { key : value for key , value in zip ( argnames , clusterargs ) } ## get connection within timeout window of wait time and hide messages ipyclient = ipp . Client ( * * args ) sys . stdout = save_stdout sys . stderr = save_stderr ## check that all engines have connected if ( engines == "MPI" ) or ( "ipyrad-cli-" in cluster_id ) : if not quiet : print ( connection_string ) for _ in range ( 6000 ) : initid = len ( ipyclient ) time . sleep ( 0.01 ) ## If MPI then wait for all engines to start so we can report ## how many cores are on each host. If Local then only wait for ## one engine to be ready and then just go. if ( engines == "MPI" ) or ( "ipyrad-cli-" in cluster_id ) : ## wait for cores to be connected if cores : time . sleep ( 0.1 ) if initid == cores : break if initid : time . sleep ( 3 ) if len ( ipyclient ) == initid : break else : if cores : if initid == cores : break else : if initid : break except KeyboardInterrupt as inst : ## ensure stdout is reset even if Exception was raised sys . stdout = save_stdout sys . stderr = save_stderr raise inst ## This is raised if ipcluster is not running ------------ except IOError as inst : ## ensure stdout is reset even if Exception was raised sys . stdout = save_stdout sys . stderr = save_stderr if "ipyrad-cli-" in cluster_id : raise IPyradWarningExit ( NO_IPCLUSTER_CLI ) else : raise IPyradWarningExit ( NO_IPCLUSTER_API ) except ( ipp . TimeoutError , ipp . NoEnginesRegistered ) as inst : ## raised by ipp if no connection file is found for 'nwait' seconds sys . stdout = save_stdout sys . stderr = save_stderr raise inst except Exception as inst : ## if any other exceptions were missed... sys . stdout = save_stdout sys . stderr = save_stderr raise inst finally : ## ensure that no matter what we reset the stds sys . stdout = save_stdout sys . stderr = save_stderr return ipyclient
6953	def bootstrap_falsealarmprob ( lspinfo , times , mags , errs , nbootstrap = 250 , magsarefluxes = False , sigclip = 10.0 , npeaks = None ) : # figure out how many periods to work on if ( npeaks and ( 0 < npeaks < len ( lspinfo [ 'nbestperiods' ] ) ) ) : nperiods = npeaks else : LOGWARNING ( 'npeaks not specified or invalid, ' 'getting FAP for all %s periodogram peaks' % len ( lspinfo [ 'nbestperiods' ] ) ) nperiods = len ( lspinfo [ 'nbestperiods' ] ) nbestperiods = lspinfo [ 'nbestperiods' ] [ : nperiods ] nbestpeaks = lspinfo [ 'nbestlspvals' ] [ : nperiods ] # get rid of nans first and sigclip stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) allpeaks = [ ] allperiods = [ ] allfaps = [ ] alltrialbestpeaks = [ ] # make sure there are enough points to calculate a spectrum if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : for ind , period , peak in zip ( range ( len ( nbestperiods ) ) , nbestperiods , nbestpeaks ) : LOGINFO ( 'peak %s: running %s trials...' % ( ind + 1 , nbootstrap ) ) trialbestpeaks = [ ] for _trial in range ( nbootstrap ) : # get a scrambled index tindex = np . random . randint ( 0 , high = mags . size , size = mags . size ) # get the kwargs dict out of the lspinfo if 'kwargs' in lspinfo : kwargs = lspinfo [ 'kwargs' ] # update the kwargs with some local stuff kwargs . update ( { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } ) else : kwargs = { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } # run the periodogram with scrambled mags and errs # and the appropriate keyword arguments lspres = LSPMETHODS [ lspinfo [ 'method' ] ] ( times , mags [ tindex ] , errs [ tindex ] , * * kwargs ) trialbestpeaks . append ( lspres [ 'bestlspval' ] ) trialbestpeaks = np . array ( trialbestpeaks ) alltrialbestpeaks . append ( trialbestpeaks ) # calculate the FAP for a trial peak j = FAP[j] = # (1.0 + sum(trialbestpeaks[i] > peak[j]))/(ntrialbestpeaks + 1) if lspinfo [ 'method' ] != 'pdm' : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks > peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) # for PDM, we're looking for a peak smaller than the best peak # because values closer to 0.0 are more significant else : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks < peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) LOGINFO ( 'FAP for peak %s, period: %.6f = %.3g' % ( ind + 1 , period , falsealarmprob ) ) allpeaks . append ( peak ) allperiods . append ( period ) allfaps . append ( falsealarmprob ) return { 'peaks' : allpeaks , 'periods' : allperiods , 'probabilities' : allfaps , 'alltrialbestpeaks' : alltrialbestpeaks } else : LOGERROR ( 'not enough mag series points to calculate periodogram' ) return None
496	def _classifyState ( self , state ) : # Record is before wait period do not classifiy if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG # Update the label based on classifications newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) # Add threshold classification label if above threshold, else if # classified to add the auto threshold classification. if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel # Make all entries unique labelList = list ( set ( labelList ) ) # If both above threshold and auto classified above - remove auto label if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return # Update state's labeling state . anomalyLabel = labelList # Update KNN Classifier with new labeling if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )
1346	def gradient ( self , image , label ) : _ , gradient = self . predictions_and_gradient ( image , label ) return gradient
5012	def get_inactive_sap_learners ( self ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : # Create a new session with a valid token self . session . close ( ) self . _create_session ( ) sap_search_student_url = '{sapsf_base_url}/{search_students_path}?$filter={search_filter}' . format ( sapsf_base_url = self . enterprise_configuration . sapsf_base_url . rstrip ( '/' ) , search_students_path = self . global_sap_config . search_student_api_path . rstrip ( '/' ) , search_filter = 'criteria/isActive eq False&$select=studentID' , ) all_inactive_learners = self . _call_search_students_recursively ( sap_search_student_url , all_inactive_learners = [ ] , page_size = 500 , start_at = 0 ) return all_inactive_learners
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
13482	def sphinx_make ( * targets ) : sh ( 'make %s' % ' ' . join ( targets ) , cwd = options . paved . docs . path )
10947	def reset ( self ) : inds = list ( range ( self . state . obj_get_positions ( ) . shape [ 0 ] ) ) self . _rad_nms = self . state . param_particle_rad ( inds ) self . _pos_nms = self . state . param_particle_pos ( inds ) self . _initial_rad = np . copy ( self . state . state [ self . _rad_nms ] ) self . _initial_pos = np . copy ( self . state . state [ self . _pos_nms ] ) . reshape ( ( - 1 , 3 ) ) self . param_vals [ self . rscale_mask ] = 0
11863	def show_approx ( self , numfmt = '%.3g' ) : return ', ' . join ( [ ( '%s: ' + numfmt ) % ( v , p ) for ( v , p ) in sorted ( self . prob . items ( ) ) ] )
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
10006	def clear_obj ( self , obj ) : removed = self . cellgraph . clear_obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
2285	def predict ( self , df_data , graph = None , * * kwargs ) : if graph is None : return self . create_graph_from_data ( df_data , * * kwargs ) elif isinstance ( graph , nx . DiGraph ) : return self . orient_directed_graph ( df_data , graph , * * kwargs ) elif isinstance ( graph , nx . Graph ) : return self . orient_undirected_graph ( df_data , graph , * * kwargs ) else : print ( 'Unknown Graph type' ) raise ValueError
8040	def is_public ( self ) : # Check if we are a setter/deleter method, and mark as private if so. for decorator in self . decorators : # Given 'foo', match 'foo.bar' but not 'foobar' or 'sfoo' if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
4598	def read_from ( self , data , pad = 0 ) : for i in range ( self . BEGIN , self . END + 1 ) : index = self . index ( i , len ( data ) ) yield pad if index is None else data [ index ]
1041	def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
2015	def _rollback ( self ) : last_pc , last_gas , last_instruction , last_arguments , fee , allocated = self . _checkpoint_data self . _push_arguments ( last_arguments ) self . _gas = last_gas self . _pc = last_pc self . _allocated = allocated self . _checkpoint_data = None
11869	def color_from_hls ( hue , light , sat ) : if light > 0.95 : #too bright, let's just switch to white return 256 elif light < 0.05 : #too dark, let's shut it off return - 1 else : hue = ( - hue + 1 + 2.0 / 3.0 ) % 1 # invert and translate by 2/3 return int ( floor ( hue * 256 ) )
5120	def next_event_description ( self ) : if self . _fancy_heap . size == 0 : event_type = 'Nothing' edge_index = None else : s = [ q . _key ( ) for q in self . edge2queue ] s . sort ( ) e = s [ 0 ] [ 1 ] q = self . edge2queue [ e ] event_type = 'Arrival' if q . next_event_description ( ) == 1 else 'Departure' edge_index = q . edge [ 2 ] return event_type , edge_index
10033	def join_phonemes ( * args ) : # Normalize arguments as onset, nucleus, coda. if len ( args ) == 1 : # tuple of (onset, nucleus[, coda]) args = args [ 0 ] if len ( args ) == 2 : args += ( CODAS [ 0 ] , ) try : onset , nucleus , coda = args except ValueError : raise TypeError ( 'join_phonemes() takes at most 3 arguments' ) offset = ( ( ONSETS . index ( onset ) * NUM_NUCLEUSES + NUCLEUSES . index ( nucleus ) ) * NUM_CODAS + CODAS . index ( coda ) ) return unichr ( FIRST_HANGUL_OFFSET + offset )
2755	def get_ssh_key ( self , ssh_key_id ) : return SSHKey . get_object ( api_token = self . token , ssh_key_id = ssh_key_id )
13852	def age ( self ) : # 0 means this composer will never decompose if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
6643	def getExtraIncludes ( self ) : if 'extraIncludes' in self . description : return [ os . path . normpath ( x ) for x in self . description [ 'extraIncludes' ] ] else : return [ ]
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
12899	def get_play_status ( self ) : status = yield from self . handle_int ( self . API . get ( 'status' ) ) return self . PLAY_STATES . get ( status )
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
135	def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot reorder polygon points, because it contains no points." ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( "Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )
6354	def _apply_rule_if_compat ( self , phonetic , target , language_arg ) : candidate = phonetic + target if '[' not in candidate : # no attributes so we need test no further return candidate # expand the result, converting incompatible attributes to [0] candidate = self . _expand_alternates ( candidate ) candidate_array = candidate . split ( '|' ) # drop each alternative that has incompatible attributes candidate = '' found = False for i in range ( len ( candidate_array ) ) : this_candidate = candidate_array [ i ] if language_arg != 1 : this_candidate = self . _normalize_lang_attrs ( this_candidate + '[' + str ( language_arg ) + ']' , False ) if this_candidate != '[0]' : found = True if candidate : candidate += '|' candidate += this_candidate # return false if no compatible alternatives remain if not found : return None # return the result of applying the rule if '|' in candidate : candidate = '(' + candidate + ')' return candidate
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
3100	def loadfile ( filename , cache = None ) : _SECRET_NAMESPACE = 'oauth2client:secrets#ns' if not cache : return _loadfile ( filename ) obj = cache . get ( filename , namespace = _SECRET_NAMESPACE ) if obj is None : client_type , client_info = _loadfile ( filename ) obj = { client_type : client_info } cache . set ( filename , obj , namespace = _SECRET_NAMESPACE ) return next ( six . iteritems ( obj ) )
3752	def STEL ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _STEL = None else : raise Exception ( 'Failure in in function' ) return _STEL
454	def initialize_rnn_state ( state , feed_dict = None ) : if isinstance ( state , LSTMStateTuple ) : c = state . c . eval ( feed_dict = feed_dict ) h = state . h . eval ( feed_dict = feed_dict ) return c , h else : new_state = state . eval ( feed_dict = feed_dict ) return new_state
802	def modelsGetParams ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % ( type ( modelIDs ) , ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getParamsNamedTuple . _fields ] ) # NOTE: assertion will also fail when modelIDs contains duplicates assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) # Return the params and params hashes as a namedtuple return [ self . _models . getParamsNamedTuple . _make ( r ) for r in rows ]
1804	def SAHF ( cpu ) : eflags_size = 32 val = cpu . AH & 0xD5 | 0x02 cpu . EFLAGS = Operators . ZEXTEND ( val , eflags_size )
2688	def update_extend ( dst , src ) : for k , v in src . items ( ) : existing = dst . setdefault ( k , [ ] ) for x in v : if x not in existing : existing . append ( x )
9148	def actions ( connection ) : session = _make_session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
381	def zca_whitening ( x , principal_components ) : flatx = np . reshape ( x , ( x . size ) ) # tl.logging.info(principal_components.shape, x.shape) # ((28160, 28160), (160, 176, 1)) # flatx = np.reshape(x, (x.shape)) # flatx = np.reshape(x, (x.shape[0], )) # tl.logging.info(flatx.shape) # (160, 176, 1) whitex = np . dot ( flatx , principal_components ) x = np . reshape ( whitex , ( x . shape [ 0 ] , x . shape [ 1 ] , x . shape [ 2 ] ) ) return x
7708	def handle_got_features_event ( self , event ) : server_features = set ( ) logger . debug ( "Checking roster-related features" ) if event . features . find ( FEATURE_ROSTERVER ) is not None : logger . debug ( " Roster versioning available" ) server_features . add ( "versioning" ) if event . features . find ( FEATURE_APPROVALS ) is not None : logger . debug ( " Subscription pre-approvals available" ) server_features . add ( "pre-approvals" ) self . server_features = server_features
9416	def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) # Bootstrap a MatlabObject from scipy.io # From https://github.com/scipy/scipy/blob/93a0ea9e5d4aba1f661b6bb0e18f9c2d1fce436a/scipy/io/matlab/mio5.py#L435-L443 # and https://github.com/scipy/scipy/blob/93a0ea9e5d4aba1f661b6bb0e18f9c2d1fce436a/scipy/io/matlab/mio5_params.py#L224 dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
10464	def verifymenucheck ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) try : if menu_handle . AXMenuItemMarkChar : # Checked return 1 except atomac . _a11y . Error : pass except LdtpServerException : pass return 0
4145	def speriodogram ( x , NFFT = None , detrend = True , sampling = 1. , scale_by_freq = True , window = 'hamming' , axis = 0 ) : x = np . array ( x ) # array with 1 dimension case if x . ndim == 1 : axis = 0 r = x . shape [ 0 ] w = Window ( r , window ) #same size as input data w = w . data # matrix case elif x . ndim == 2 : logging . debug ( '2D array. each row is a 1D array' ) [ r , c ] = x . shape w = np . array ( [ Window ( r , window ) . data for this in range ( c ) ] ) . reshape ( r , c ) if NFFT is None : NFFT = len ( x ) isreal = np . isrealobj ( x ) if detrend == True : m = np . mean ( x , axis = axis ) else : m = 0 if isreal == True : if x . ndim == 2 : res = ( abs ( rfft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( rfft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r else : if x . ndim == 2 : res = ( abs ( fft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( fft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r if scale_by_freq is True : df = sampling / float ( NFFT ) res *= 2 * np . pi / df if x . ndim == 1 : return res . transpose ( ) else : return res
9075	def sendMultiPart ( smtp , gpg_context , sender , recipients , subject , text , attachments ) : sent = 0 for to in recipients : if not to . startswith ( '<' ) : uid = '<%s>' % to else : uid = to if not checkRecipient ( gpg_context , uid ) : continue msg = MIMEMultipart ( ) msg [ 'From' ] = sender msg [ 'To' ] = to msg [ 'Subject' ] = subject msg [ "Date" ] = formatdate ( localtime = True ) msg . preamble = u'This is an email in encrypted multipart format.' attach = MIMEText ( str ( gpg_context . encrypt ( text . encode ( 'utf-8' ) , uid , always_trust = True ) ) ) attach . set_charset ( 'UTF-8' ) msg . attach ( attach ) for attachment in attachments : with open ( attachment , 'rb' ) as fp : attach = MIMEBase ( 'application' , 'octet-stream' ) attach . set_payload ( str ( gpg_context . encrypt_file ( fp , uid , always_trust = True ) ) ) attach . add_header ( 'Content-Disposition' , 'attachment' , filename = basename ( '%s.pgp' % attachment ) ) msg . attach ( attach ) # TODO: need to catch exception? # yes :-) we need to adjust the status accordingly (>500 so it will be destroyed) smtp . begin ( ) smtp . sendmail ( sender , to , msg . as_string ( ) ) smtp . quit ( ) sent += 1 return sent
6437	def dist_abs ( self , src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : # Calculate the eudex hashes and XOR them xored = eudex ( src , max_length = max_length ) ^ eudex ( tar , max_length = max_length ) # Simple hamming distance (all bits are equal) if not weights : binary = bin ( xored ) distance = binary . count ( '1' ) if normalized : return distance / ( len ( binary ) - 2 ) return distance # If weights is a function, it should create a generator, # which we now use to populate a list if callable ( weights ) : weights = weights ( ) elif weights == 'exponential' : weights = Eudex . gen_exponential ( ) elif weights == 'fibonacci' : weights = Eudex . gen_fibonacci ( ) if isinstance ( weights , GeneratorType ) : weights = [ next ( weights ) for _ in range ( max_length ) ] [ : : - 1 ] # Sum the weighted hamming distance distance = 0 max_distance = 0 while ( xored or normalized ) and weights : max_distance += 8 * weights [ - 1 ] distance += bin ( xored & 0xFF ) . count ( '1' ) * weights . pop ( ) xored >>= 8 if normalized : distance /= max_distance return distance
3558	def discover ( cls , device , timeout_sec = TIMEOUT_SEC ) : device . discover ( cls . SERVICES , cls . CHARACTERISTICS , timeout_sec )
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
13774	def format ( self , record ) : record_fields = record . __dict__ . copy ( ) self . _set_exc_info ( record_fields ) event_name = 'default' if record_fields . get ( 'event_name' ) : event_name = record_fields . pop ( 'event_name' ) log_level = 'INFO' if record_fields . get ( 'log_level' ) : log_level = record_fields . pop ( 'log_level' ) [ record_fields . pop ( k ) for k in record_fields . keys ( ) if k not in self . fields ] defaults = self . defaults . copy ( ) fields = self . fields . copy ( ) fields . update ( record_fields ) filtered_fields = { } for k , v in fields . iteritems ( ) : if v is not None : filtered_fields [ k ] = v defaults . update ( { 'event_timestamp' : self . _get_now ( ) , 'event_name' : event_name , 'log_level' : log_level , 'fields' : filtered_fields } ) return json . dumps ( defaults , default = self . json_default )
11306	def store_providers ( self , provider_data ) : if not hasattr ( provider_data , '__iter__' ) : raise OEmbedException ( 'Autodiscovered response not iterable' ) provider_pks = [ ] for provider in provider_data : if 'endpoint' not in provider or 'matches' not in provider : continue resource_type = provider . get ( 'type' ) if resource_type not in RESOURCE_TYPES : continue stored_provider , created = StoredProvider . objects . get_or_create ( wildcard_regex = provider [ 'matches' ] ) if created : stored_provider . endpoint_url = relative_to_full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored_provider . resource_type = resource_type stored_provider . save ( ) provider_pks . append ( stored_provider . pk ) return StoredProvider . objects . filter ( pk__in = provider_pks )
3134	def update ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id ) , data = data )
6356	def dist_strcmp95 ( src , tar , long_strings = False ) : return Strcmp95 ( ) . dist ( src , tar , long_strings )
386	def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : if coords is None : coords = [ ] def _flip ( im , coords ) : im = flip_axis ( im , axis = 1 , is_random = False ) coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) if is_rescale : if is_center : # x_center' = 1 - x x = 1. - coord [ 0 ] else : # x_center' = 1 - x - w x = 1. - coord [ 0 ] - coord [ 2 ] else : if is_center : # x' = im.width - x x = im . shape [ 1 ] - coord [ 0 ] else : # x' = im.width - x - w x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) return im , coords_new if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : return _flip ( im , coords ) else : return im , coords else : return _flip ( im , coords )
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : # get user language for user from language store defined in # NOTIFICATION_LANGUAGE_MODULE setting try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : # activate the user's language activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True # reset environment to original language activate ( current_language ) return sent
11297	def _check_for_exceptions ( self , resp , multiple_rates ) : if resp [ 'rCode' ] != 100 : raise exceptions . get_exception_for_code ( resp [ 'rCode' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . ZipTaxNoResults ( 'No results found' ) if len ( results ) > 1 and not multiple_rates : # It's fine if all the taxes are the same rates = [ result [ 'taxSales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . ZipTaxMultipleResults ( 'Multiple results found but requested only one' )
8765	def opt_args_decorator ( func ) : @ wraps ( func ) def wrapped_dec ( * args , * * kwargs ) : if len ( args ) == 1 and len ( kwargs ) == 0 and callable ( args [ 0 ] ) : # actual decorated function return func ( args [ 0 ] ) else : # decorator arguments return lambda realf : func ( realf , * args , * * kwargs ) return wrapped_dec
5938	def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
2787	def attach ( self , droplet_id , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "attach" , "droplet_id" : droplet_id , "region" : region } )
3427	def add_metabolites ( self , metabolite_list ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] if len ( metabolite_list ) == 0 : return None # First check whether the metabolites exist in the model metabolite_list = [ x for x in metabolite_list if x . id not in self . metabolites ] bad_ids = [ m for m in metabolite_list if not isinstance ( m . id , string_types ) or len ( m . id ) < 1 ] if len ( bad_ids ) != 0 : raise ValueError ( 'invalid identifiers in {}' . format ( repr ( bad_ids ) ) ) for x in metabolite_list : x . _model = self self . metabolites += metabolite_list # from cameo ... to_add = [ ] for met in metabolite_list : if met . id not in self . constraints : constraint = self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) to_add += [ constraint ] self . add_cons_vars ( to_add ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __isub__ , metabolite_list ) ) for x in metabolite_list : # Do we care? context ( partial ( setattr , x , '_model' , None ) )
12920	def reload ( self ) : if len ( self ) == 0 : return [ ] ret = [ ] for obj in self : res = None try : res = obj . reload ( ) except Exception as e : res = e ret . append ( res ) return ret
7092	def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , LatLng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , LatLng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ LatLng ( * p ) for p in change [ 'items' ] ] self . addAll ( [ bridge . encode ( c ) for c in points ] ) elif op == '__setitem__' : self . set ( change [ 'index' ] , LatLng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise NotImplementedError ( "Unsupported change operation {}" . format ( op ) )
5501	def remove_tweets ( self , url ) : try : del self . cache [ url ] self . mark_updated ( ) return True except KeyError : return False
13745	def get_table ( self ) : if hasattr ( self , '_table' ) : table = self . _table else : try : table = self . conn . get_table ( self . get_table_name ( ) ) except boto . exception . DynamoDBResponseError : if self . auto_create_table : table = self . create_table ( ) else : raise self . _table = table return table
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(([^,]+).*?\)''' ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
5803	def extract_chain ( server_handshake_bytes ) : output = [ ] chain_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0b' : chain_bytes = message_data break if chain_bytes : break if chain_bytes : # The first 3 bytes are the cert chain length pointer = 3 while pointer < len ( chain_bytes ) : cert_length = int_from_bytes ( chain_bytes [ pointer : pointer + 3 ] ) cert_start = pointer + 3 cert_end = cert_start + cert_length pointer = cert_end cert_bytes = chain_bytes [ cert_start : cert_end ] output . append ( Certificate . load ( cert_bytes ) ) return output
10051	def post ( self , pid , record , action ) : record = getattr ( record , action ) ( pid = pid ) db . session . commit ( ) # Refresh the PID and record metadata db . session . refresh ( pid ) db . session . refresh ( record . model ) post_action . send ( current_app . _get_current_object ( ) , action = action , pid = pid , deposit = record ) response = self . make_response ( pid , record , 202 if action == 'publish' else 201 ) endpoint = '.{0}_item' . format ( pid . pid_type ) location = url_for ( endpoint , pid_value = pid . pid_value , _external = True ) response . headers . extend ( dict ( Location = location ) ) return response
6533	def get_project_config ( project_path , use_cache = True ) : return get_local_config ( project_path , use_cache = use_cache ) or get_user_config ( project_path , use_cache = use_cache ) or get_default_config ( )
326	def simulate_paths ( is_returns , num_days , starting_value = 1 , num_samples = 1000 , random_seed = None ) : samples = np . empty ( ( num_samples , num_days ) ) seed = np . random . RandomState ( seed = random_seed ) for i in range ( num_samples ) : samples [ i , : ] = is_returns . sample ( num_days , replace = True , random_state = seed ) return samples
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
7842	def set_category ( self , category ) : if not category : raise ValueError ( "Category is required in DiscoIdentity" ) category = unicode ( category ) self . xmlnode . setProp ( "category" , category . encode ( "utf-8" ) )
10907	def twoslice ( field , center = None , size = 6.0 , cmap = 'bone_r' , vmin = 0 , vmax = 1 , orientation = 'vertical' , figpad = 1.09 , off = 0.01 ) : center = center or [ i // 2 for i in field . shape ] slices = [ ] for i , c in enumerate ( center ) : blank = [ np . s_ [ : ] ] * len ( center ) blank [ i ] = c slices . append ( tuple ( blank ) ) z , y , x = [ float ( i ) for i in field . shape ] w = float ( x + z ) h = float ( y + z ) def show ( field , ax , slicer , transpose = False ) : tmp = field [ slicer ] if not transpose else field [ slicer ] . T ax . imshow ( tmp , cmap = cmap , interpolation = 'nearest' , vmin = vmin , vmax = vmax ) ax . set_xticks ( [ ] ) ax . set_yticks ( [ ] ) ax . grid ( 'off' ) if orientation . startswith ( 'v' ) : # rect = l,b,w,h log . info ( '{} {} {} {} {} {}' . format ( x , y , z , w , h , x / h ) ) r = x / h q = y / h f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * r , size * f ) ) ax1 = fig . add_axes ( ( off , f * ( 1 - q ) + 2 * off , f , f * q ) ) ax2 = fig . add_axes ( ( off , off , f , f * ( 1 - q ) ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 1 ] ) else : # rect = l,b,w,h r = y / w q = x / w f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * f , size * r ) ) ax1 = fig . add_axes ( ( off , off , f * q , f ) ) ax2 = fig . add_axes ( ( 2 * off + f * q , off , f * ( 1 - q ) , f ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 2 ] , transpose = True ) return fig , ax1 , ax2
746	def anomalyAddLabel ( self , start , end , labelName ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . addLabel ( start , end , labelName )
8533	def is_isomorphic_to ( self , other ) : return ( isinstance ( other , self . __class__ ) and len ( self . fields ) == len ( other . fields ) and all ( a . is_isomorphic_to ( b ) for a , b in zip ( self . fields , other . fields ) ) )
13684	def post ( self , url , params = { } , files = None ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . post ( self . host + url , data = params , files = files ) return self . json_parse ( response . content ) except RequestException as e : return self . json_parse ( e . args )
1435	def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )
11163	def mtime ( self ) : try : return self . _stat . st_mtime except : # pragma: no cover self . _stat = self . stat ( ) return self . mtime
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
8834	def less ( a , b , * args ) : types = set ( [ type ( a ) , type ( b ) ] ) if float in types or int in types : try : a , b = float ( a ) , float ( b ) except TypeError : # NaN return False return a < b and ( not args or less ( b , * args ) )
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : # Return if not collecting stats if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 # Compute the prediction score, how well the prediction from the last # time step predicted the current bottom-up input ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) # Store the stats that don't depend on burn-in stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 # If we are passed the burn-in period, update the accumulated stats # Here's what various burn-in values mean: # 0: try to predict the first element of each sequence and all subsequent # 1: try to predict the second element of each sequence and all subsequent # etc. if stats [ 'nInfersSinceReset' ] <= self . burnIn : return # Burn-in related stats stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : # Collect cell confidences for every cell that correctly predicted current # bottom up input. Normalize confidence across each column cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] # Update cell confidence histogram: add column-normalized confidence # scores to the histogram self . _internalStats [ 'confHistogram' ] += cc
7664	def to_samples ( self , times , confidence = False ) : times = np . asarray ( times ) if times . ndim != 1 or np . any ( times < 0 ) : raise ParameterError ( 'times must be 1-dimensional and non-negative' ) idx = np . argsort ( times ) samples = times [ idx ] values = [ list ( ) for _ in samples ] confidences = [ list ( ) for _ in samples ] for obs in self . data : start = np . searchsorted ( samples , obs . time ) end = np . searchsorted ( samples , obs . time + obs . duration , side = 'right' ) for i in range ( start , end ) : values [ idx [ i ] ] . append ( obs . value ) confidences [ idx [ i ] ] . append ( obs . confidence ) if confidence : return values , confidences else : return values
4548	def fill_rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : _draw_fast_vline ( setter , i , y , h , color , aa )
12816	def _send_to_consumer ( self , block ) : self . _consumer . write ( block ) self . _sent += len ( block ) if self . _callback : self . _callback ( self . _sent , self . length )
4254	def time_zone_by_country_and_region ( country_code , region_code = None ) : timezone = country_dict . get ( country_code ) if not timezone : return None if isinstance ( timezone , str ) : return timezone return timezone . get ( region_code )
13812	def FindMethodByName ( self , name ) : for method in self . methods : if name == method . name : return method return None
13497	def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
11656	def fit_transform ( self , X , y = None , * * params ) : X = as_features ( X , stack = True ) X_new = self . transformer . fit_transform ( X . stacked_features , y , * * params ) return self . _gather_outputs ( X , X_new )
3999	def copy_to_local ( local_path , remote_name , remote_path , demote = True ) : if not container_path_exists ( remote_name , remote_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( remote_path , remote_name ) ) temp_identifier = str ( uuid . uuid1 ( ) ) copy_path_inside_container ( remote_name , remote_path , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) ) vm_path = os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) is_dir = vm_path_is_directory ( vm_path ) sync_local_path_from_vm ( local_path , vm_path , demote = demote , is_dir = is_dir )
10009	def check_mro ( self , bases ) : try : self . add_node ( "temp" ) for base in bases : nx . DiGraph . add_edge ( self , base , "temp" ) result = self . get_mro ( "temp" ) [ 1 : ] finally : self . remove_node ( "temp" ) return result
10978	def leave ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_leave ( current_user ) : try : group . remove_member ( current_user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You have successfully left %(group_name)s group.' , group_name = group . name ) , 'success' ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You cannot leave the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
10805	def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
11315	def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) # remove any spaces before/after if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
5147	def generate ( self ) : tar_bytes = BytesIO ( ) tar = tarfile . open ( fileobj = tar_bytes , mode = 'w' ) self . _generate_contents ( tar ) self . _process_files ( tar ) tar . close ( ) tar_bytes . seek ( 0 ) # set pointer to beginning of stream # `mtime` parameter of gzip file must be 0, otherwise any checksum operation # would return a different digest even when content is the same. # to achieve this we must use the python `gzip` library because the `tarfile` # library does not seem to offer the possibility to modify the gzip `mtime`. gzip_bytes = BytesIO ( ) gz = gzip . GzipFile ( fileobj = gzip_bytes , mode = 'wb' , mtime = 0 ) gz . write ( tar_bytes . getvalue ( ) ) gz . close ( ) gzip_bytes . seek ( 0 ) # set pointer to beginning of stream return gzip_bytes
13401	def removeLogbook ( self , menu = None ) : if self . logMenuCount > 1 and menu is not None : menu . removeMenu ( ) self . logMenus . remove ( menu ) self . logMenuCount -= 1
12334	def sudo ( self , password = None ) : if self . username == 'root' : raise ValueError ( 'Already root user' ) password = self . validate_password ( password ) stdin , stdout , stderr = self . exec_command ( 'sudo su' ) stdin . write ( "%s\n" % password ) stdin . flush ( ) errors = stderr . read ( ) if errors : raise ValueError ( errors )
7801	def handle_authorized ( self , event ) : stream = event . stream if not stream : return if not stream . initiator : return if stream . features is None : return element = stream . features . find ( SESSION_TAG ) if element is None : return logger . debug ( "Establishing IM session" ) stanza = Iq ( stanza_type = "set" ) payload = XMLPayload ( ElementTree . Element ( SESSION_TAG ) ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _session_success , self . _session_error ) stream . send ( stanza )
5019	def validate_image_extension ( value ) : config = get_app_config ( ) ext = os . path . splitext ( value . name ) [ 1 ] if config and not ext . lower ( ) in config . valid_image_extensions : raise ValidationError ( _ ( "Unsupported file extension." ) )
12777	def resorted ( values ) : if not values : return values values = sorted ( values ) # look for first word first_word = next ( ( cnt for cnt , val in enumerate ( values ) if val and not val [ 0 ] . isdigit ( ) ) , None ) # if not found, just return the values if first_word is None : return values words = values [ first_word : ] numbers = values [ : first_word ] return words + numbers
7322	def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : # Relative paths are relative to the template's parent directory attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) # Check that the attachment exists if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
5659	def _validate_danglers ( self ) : for query , warning in zip ( DANGLER_QUERIES , DANGLER_WARNINGS ) : dangler_count = self . gtfs . execute_custom_query ( query ) . fetchone ( ) [ 0 ] if dangler_count > 0 : if self . verbose : print ( str ( dangler_count ) + " " + warning ) self . warnings_container . add_warning ( warning , self . location , count = dangler_count )
12773	def _step_to_marker_frame ( self , frame_no , dt = None ) : # update the positions and velocities of the markers. self . markers . detach ( ) self . markers . reposition ( frame_no ) self . markers . attach ( frame_no ) # detect collisions. self . ode_space . collide ( None , self . on_collision ) # record the state of each skeleton body. states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) # yield the current simulation state to our caller. yield states # update the ode world. self . ode_world . step ( dt or self . dt ) # clear out contact joints to prepare for the next frame. self . ode_contactgroup . empty ( )
11537	def set_pin_direction ( self , pin , direction ) : if type ( pin ) is list : for p in pin : self . set_pin_direction ( p , direction ) return pin_id = self . _pin_mapping . get ( pin , None ) if pin_id and type ( direction ) is ahio . Direction : self . _set_pin_direction ( pin_id , direction ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
7980	def auth_timeout ( self ) : self . lock . acquire ( ) try : self . __logger . debug ( "Timeout while waiting for jabber:iq:auth result" ) if self . _auth_methods_left : self . _auth_methods_left . pop ( 0 ) finally : self . lock . release ( )
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : # Shuffling the list of boids ensures fluid movement. # If you need the boids to retain their position in the list # each update, set the shuffled parameter to False. from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 # cohesion m2 = 1.0 # separation m3 = 1.0 # alignment m4 = 1.0 # goal # The flock scatters randomly with a Boids.scatter chance. # This means their cohesion (m1) is reversed, # and their joint alignment (m3) is dimished, # causing boids to oscillate in confusion. # Setting Boids.scatter(chance=0) ensures they never scatter. if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 # A flock can have a goal defined with Boids.goal(x,y,z), # a place of interest to flock around. if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : # A boid that is perching will continue to do so # until Boid._perch_t reaches zero. if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
4821	def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
12027	def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) #it should be in the first 30k of the file f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) # the whole protocol filename protocolID = protocol . split ( " " ) [ 0 ] # just the first number return protocolID
9867	async def rt_unsubscribe ( self ) : if self . _subscription_id is None : _LOGGER . error ( "Not subscribed." ) return await self . _tibber_control . sub_manager . unsubscribe ( self . _subscription_id )
7517	def snpcount_numba ( superints , snpsarr ) : ## iterate over all loci for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : ## make new array catg = np . zeros ( 4 , dtype = np . int16 ) ## a list for only catgs ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : #C catg [ 0 ] += 1 elif ncol [ idx ] == 65 : #A catg [ 1 ] += 1 elif ncol [ idx ] == 84 : #T catg [ 2 ] += 1 elif ncol [ idx ] == 71 : #G catg [ 3 ] += 1 elif ncol [ idx ] == 82 : #R catg [ 1 ] += 1 #A catg [ 3 ] += 1 #G elif ncol [ idx ] == 75 : #K catg [ 2 ] += 1 #T catg [ 3 ] += 1 #G elif ncol [ idx ] == 83 : #S catg [ 0 ] += 1 #C catg [ 3 ] += 1 #G elif ncol [ idx ] == 89 : #Y catg [ 0 ] += 1 #C catg [ 2 ] += 1 #T elif ncol [ idx ] == 87 : #W catg [ 1 ] += 1 #A catg [ 2 ] += 1 #T elif ncol [ idx ] == 77 : #M catg [ 0 ] += 1 #C catg [ 1 ] += 1 #A ## get second most common site catg . sort ( ) ## if invariant e.g., [0, 0, 0, 9], then nothing (" ") if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
1828	def RET ( cpu , * operands ) : # TODO FIX 64Bit FIX segment N = 0 if len ( operands ) > 0 : N = operands [ 0 ] . read ( ) cpu . PC = cpu . pop ( cpu . address_bit_size ) cpu . STACK += N
6972	def _epd_residual2 ( coeffs , times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual
13292	def json_attributes ( self , vfuncs = None ) : vfuncs = vfuncs or [ ] js = { 'global' : { } } for k in self . ncattrs ( ) : js [ 'global' ] [ k ] = self . getncattr ( k ) for varname , var in self . variables . items ( ) : js [ varname ] = { } for k in var . ncattrs ( ) : z = var . getncattr ( k ) try : assert not np . isnan ( z ) . all ( ) js [ varname ] [ k ] = z except AssertionError : js [ varname ] [ k ] = None except TypeError : js [ varname ] [ k ] = z for vf in vfuncs : try : js [ varname ] . update ( vfuncs ( var ) ) except BaseException : logger . exception ( "Could not apply custom variable attribue function" ) return json . loads ( json . dumps ( js , cls = BasicNumpyEncoder ) )
9548	def add_unique_check ( self , key , code = UNIQUE_CHECK_FAILED , message = MESSAGES [ UNIQUE_CHECK_FAILED ] ) : if isinstance ( key , basestring ) : assert key in self . _field_names , 'unexpected field name: %s' % key else : for f in key : assert f in self . _field_names , 'unexpected field name: %s' % key t = key , code , message self . _unique_checks . append ( t )
6039	def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )
9535	def get_complete_version ( version = None ) : if version is None : from django_cryptography import VERSION as version else : assert len ( version ) == 5 assert version [ 3 ] in ( 'alpha' , 'beta' , 'rc' , 'final' ) return version
4734	def round_data ( filter_data ) : for index , _ in enumerate ( filter_data ) : filter_data [ index ] [ 0 ] = round ( filter_data [ index ] [ 0 ] / 100.0 ) * 100.0 return filter_data
13247	def get_lsst_bibtex ( bibtex_filenames = None ) : logger = logging . getLogger ( __name__ ) if bibtex_filenames is None : # Default lsst-texmf bibliography files bibtex_names = KNOWN_LSSTTEXMF_BIB_NAMES else : # Sanitize filenames (remove extensions, path) bibtex_names = [ ] for filename in bibtex_filenames : name = os . path . basename ( os . path . splitext ( filename ) [ 0 ] ) if name not in KNOWN_LSSTTEXMF_BIB_NAMES : logger . warning ( '%r is not a known lsst-texmf bib file' , name ) continue bibtex_names . append ( name ) # names of bibtex files not in cache uncached_names = [ name for name in bibtex_names if name not in _LSSTTEXMF_BIB_CACHE ] if len ( uncached_names ) > 0 : # Download bibtex and put into the cache loop = asyncio . get_event_loop ( ) future = asyncio . ensure_future ( _download_lsst_bibtex ( uncached_names ) ) loop . run_until_complete ( future ) for name , text in zip ( bibtex_names , future . result ( ) ) : _LSSTTEXMF_BIB_CACHE [ name ] = text return { name : _LSSTTEXMF_BIB_CACHE [ name ] for name in bibtex_names }
1461	def import_and_get_class ( path_to_pex , python_class_name ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) Log . debug ( "In import_and_get_class with cls_name: %s" % python_class_name ) split = python_class_name . split ( '.' ) from_path = '.' . join ( split [ : - 1 ] ) import_name = python_class_name . split ( '.' ) [ - 1 ] Log . debug ( "From path: %s, import name: %s" % ( from_path , import_name ) ) # Resolve duplicate package suffix problem (heron.), if the top level package name is heron if python_class_name . startswith ( "heron." ) : try : mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) return getattr ( mod , import_name ) except : Log . error ( "Could not resolve class %s with special handling" % python_class_name ) mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) Log . debug ( "Imported module: %s" % str ( mod ) ) return getattr ( mod , import_name )
4049	def fulltext_item ( self , itemkey , * * kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
12435	def parse ( cls , path ) : # Iterate through the available patterns. for resource , pattern in cls . meta . patterns : # Attempt to match the path. match = re . match ( pattern , path ) if match is not None : # Found something. return resource , match . groupdict ( ) , match . string [ match . end ( ) : ] # No patterns at all; return unsuccessful. return None if not cls . meta . patterns else False
1163	def wait ( self , timeout = None ) : with self . __cond : if not self . __flag : self . __cond . wait ( timeout ) return self . __flag
4576	def color_cmp ( a , b ) : if a == b : return 0 a , b = rgb_to_hsv ( a ) , rgb_to_hsv ( b ) return - 1 if a < b else 1
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
3230	def service_list ( service = None , key_name = None , * * kwargs ) : resp_list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) if key_name and key_name in resp : resp_list . extend ( resp [ key_name ] ) else : resp_list . append ( resp ) # Not all list calls have a list_next if hasattr ( service , 'list_next' ) : req = service . list_next ( previous_request = req , previous_response = resp ) else : req = None return resp_list
8756	def delete_tenant_quota ( context , tenant_id ) : tenant_quotas = context . session . query ( Quota ) tenant_quotas = tenant_quotas . filter_by ( tenant_id = tenant_id ) tenant_quotas . delete ( )
9039	def _dump_to_file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )
9525	def sort_by_name ( infile , outfile ) : seqs = { } file_to_dict ( infile , seqs ) #seqs = list(seqs.values()) #seqs.sort() fout = utils . open_file_write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : # lets store all kind of credential data into this dict if credentials is not None : self . credentials = credentials else : self . credentials = { } # set the user from CLI or file if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] # set the password from CLI or file if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] # if both user and password is set, # 1. encode to base 64 for basic auth if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
6929	def trapezoid_transit_func ( transitparams , times , mags , errs , get_ntransitpoints = False ) : ( transitperiod , transitepoch , transitdepth , transitduration , ingressduration ) = transitparams # generate the phases iphase = ( times - transitepoch ) / transitperiod iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) halftransitduration = transitduration / 2.0 bottomlevel = zerolevel - transitdepth slope = transitdepth / ingressduration # the four contact points of the eclipse firstcontact = 1.0 - halftransitduration secondcontact = firstcontact + ingressduration thirdcontact = halftransitduration - ingressduration fourthcontact = halftransitduration ## the phase indices ## # during ingress ingressind = ( phase > firstcontact ) & ( phase < secondcontact ) # at transit bottom bottomind = ( phase > secondcontact ) | ( phase < thirdcontact ) # during egress egressind = ( phase > thirdcontact ) & ( phase < fourthcontact ) # count the number of points in transit in_transit_points = ingressind | bottomind | egressind n_transit_points = np . sum ( in_transit_points ) # set the mags modelmags [ ingressind ] = zerolevel - slope * ( phase [ ingressind ] - firstcontact ) modelmags [ bottomind ] = bottomlevel modelmags [ egressind ] = bottomlevel + slope * ( phase [ egressind ] - thirdcontact ) if get_ntransitpoints : return modelmags , phase , ptimes , pmags , perrs , n_transit_points else : return modelmags , phase , ptimes , pmags , perrs
6114	def resized_scaled_array_from_array ( self , new_shape , new_centre_pixels = None , new_centre_arcsec = None ) : if new_centre_pixels is None and new_centre_arcsec is None : new_centre = ( - 1 , - 1 ) # In Numba, the input origin must be the same image type as the origin, thus we cannot # pass 'None' and instead use the tuple (-1, -1). elif new_centre_pixels is not None and new_centre_arcsec is None : new_centre = new_centre_pixels elif new_centre_pixels is None and new_centre_arcsec is not None : new_centre = self . arc_second_coordinates_to_pixel_coordinates ( arc_second_coordinates = new_centre_arcsec ) else : raise exc . DataException ( 'You have supplied two centres (pixels and arc-seconds) to the resize scaled' 'array function' ) return self . new_with_array ( array = array_util . resized_array_2d_from_array_2d_and_resized_shape ( array_2d = self , resized_shape = new_shape , origin = new_centre ) )
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
3833	async def modify_otr_status ( self , modify_otr_status_request ) : response = hangouts_pb2 . ModifyOTRStatusResponse ( ) await self . _pb_request ( 'conversations/modifyotrstatus' , modify_otr_status_request , response ) return response
3749	def calculate_P ( self , T , P , method ) : if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
3803	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return mixing_simple ( zs , ks ) elif method == DIPPR_9H : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return DIPPR9H ( ws , ks ) elif method == FILIPPOV : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return Filippov ( ws , ks ) elif method == MAGOMEDOV : k_w = self . ThermalConductivityLiquids [ self . index_w ] ( T , P ) ws = list ( ws ) ws . pop ( self . index_w ) return thermal_conductivity_Magomedov ( T , P , ws , self . wCASs , k_w ) else : raise Exception ( 'Method not valid' )
8477	def run ( self ) : self . checkProperties ( ) self . debug ( "[*] Iniciando escaneo de AtomShields con las siguientes propiedades. . . " ) self . showScanProperties ( ) self . loadConfig ( ) # Init time counter init_ts = datetime . now ( ) # Execute plugins cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . executeCheckers ( ) os . chdir ( cwd ) # Finish time counter end_ts = datetime . now ( ) duration = '{}' . format ( end_ts - init_ts ) # Process and set issues for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . saveIssue , value ) else : self . saveIssue ( value ) # Execute reports print "" self . executeReports ( ) # Print summary output. self . debug ( "" ) self . debug ( "Duration: {t}" . format ( t = duration ) ) self . showSummary ( ) return self . issues
7170	def train ( self , debug = True , force = False , single_thread = False , timeout = 20 ) : if not self . must_train and not force : return self . padaos . compile ( ) self . train_thread = Thread ( target = self . _train , kwargs = dict ( debug = debug , single_thread = single_thread , timeout = timeout ) , daemon = True ) self . train_thread . start ( ) self . train_thread . join ( timeout ) self . must_train = False return not self . train_thread . is_alive ( )
4953	def ready ( self ) : from enterprise . signals import handle_user_post_save from django . db . models . signals import pre_migrate , post_save post_save . connect ( handle_user_post_save , sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID ) pre_migrate . connect ( self . _disconnect_user_post_save_for_migrations )
8874	def _touch ( fname , mode = 0o666 , dir_fd = None , * * kwargs ) : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fname , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fname , dir_fd = None if os . supports_fd else dir_fd , * * kwargs , )
7622	def tempo ( ref , est , * * kwargs ) : ref = coerce_annotation ( ref , 'tempo' ) est = coerce_annotation ( est , 'tempo' ) ref_tempi = np . asarray ( [ o . value for o in ref ] ) ref_weight = ref . data [ 0 ] . confidence est_tempi = np . asarray ( [ o . value for o in est ] ) return mir_eval . tempo . evaluate ( ref_tempi , ref_weight , est_tempi , * * kwargs )
10030	def execute ( helper , config , args ) : old_env_name = args . old_environment new_env_name = args . new_environment # swap C-Names out ( "Assuming that {} is the currently active environment..." . format ( old_env_name ) ) out ( "Swapping environment cnames: {} will become active, {} will become inactive." . format ( new_env_name , old_env_name ) ) helper . swap_environment_cnames ( old_env_name , new_env_name ) helper . wait_for_environments ( [ old_env_name , new_env_name ] , status = 'Ready' , include_deleted = False )
9124	def belanno ( keyword : str , file : TextIO ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belanno ( graph , file = file , )
4935	def chunks ( dictionary , chunk_size ) : iterable = iter ( dictionary ) for __ in range ( 0 , len ( dictionary ) , chunk_size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk_size ) }
8240	def tetrad ( clr , angle = 90 ) : clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( angle ) if clr . brightness < 0.5 : c . brightness += 0.2 else : c . brightness -= - 0.2 colors . append ( c ) c = clr . rotate_ryb ( angle * 2 ) if clr . brightness < 0.5 : c . brightness += 0.1 else : c . brightness -= - 0.1 colors . append ( c ) colors . append ( clr . rotate_ryb ( angle * 3 ) . lighten ( 0.1 ) ) return colors
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
7155	def raw ( prompt , * args , * * kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'type' , str ) default = kwargs . get ( 'default' , '' ) with stdout_redirected ( sys . stderr ) : while True : try : if kwargs . get ( 'secret' , False ) : answer = getpass . getpass ( prompt ) elif sys . version_info < ( 3 , 0 ) : answer = raw_input ( prompt ) else : answer = input ( prompt ) if not answer : answer = default if answer == go_back : raise QuestionnaireGoBack return type_ ( answer ) except ValueError : eprint ( '\n`{}` is not a valid `{}`\n' . format ( answer , type_ ) )
13880	def MoveFile ( source_filename , target_filename ) : _AssertIsLocal ( source_filename ) _AssertIsLocal ( target_filename ) import shutil shutil . move ( source_filename , target_filename )
6862	def drop_views ( self , name = None , site = None ) : r = self . database_renderer result = r . sudo ( "mysql --batch -v -h {db_host} " #"-u {db_root_username} -p'{db_root_password}' " "-u {db_user} -p'{db_password}' " "--execute=\"SELECT GROUP_CONCAT(CONCAT(TABLE_SCHEMA,'.',table_name) SEPARATOR ', ') AS views " "FROM INFORMATION_SCHEMA.views WHERE TABLE_SCHEMA = '{db_name}' ORDER BY table_name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db_view_list = result [ 0 ] #cmd = ("mysql -v -h {db_host} -u {db_root_username} -p'{db_root_password}' " \ r . sudo ( "mysql -v -h {db_host} -u {db_user} -p'{db_password}' " "--execute=\"DROP VIEW {db_view_list} CASCADE;\"" )
3920	def set_focus ( self , position ) : self . _focus_position = position self . _modified ( ) # If we set focus to anywhere but the last position, the user if # scrolling up: try : self . next_position ( position ) except IndexError : self . _is_scrolling = False else : self . _is_scrolling = True
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( * * self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
1173	def insort_right ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if x < a [ mid ] : hi = mid else : lo = mid + 1 a . insert ( lo , x )
4968	def _validate_program ( self ) : program = self . cleaned_data . get ( self . Fields . PROGRAM ) if not program : return course_runs = get_course_runs_from_program ( program ) try : client = CourseCatalogApiClient ( self . _user , self . _enterprise_customer . site ) available_modes = client . get_common_course_modes ( course_runs ) course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . FAILED_TO_OBTAIN_COURSE_MODES . format ( program_title = program . get ( "title" ) ) ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) if course_mode not in available_modes : raise ValidationError ( ValidationMessages . COURSE_MODE_NOT_AVAILABLE . format ( mode = course_mode , program_title = program . get ( "title" ) , modes = ", " . join ( available_modes ) ) )
5459	def find_task_descriptor ( self , task_id ) : # It is not guaranteed that the index will be task_id - 1 when --tasks is # used with a min/max range. for task_descriptor in self . task_descriptors : if task_descriptor . task_metadata . get ( 'task-id' ) == task_id : return task_descriptor return None
11056	def ensure_backrefs ( obj , fields = None ) : for ref in _collect_refs ( obj , fields ) : updated = ref [ 'value' ] . _update_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , ) if updated : logging . debug ( 'Updated reference {}:{}:{}:{}:{}' . format ( obj . _name , obj . _primary_key , ref [ 'field_name' ] , ref [ 'value' ] . _name , ref [ 'value' ] . _primary_key , ) )
6422	def _synoname_strip_punct ( self , word ) : stripped = '' for char in word : if char not in set ( ',-./:;"&\'()!{|}?$%*+<=>[\\]^_`~' ) : stripped += char return stripped . strip ( )
742	def readFromFile ( cls , f , packed = True ) : # Get capnproto schema from instance schema = cls . getSchema ( ) # Read from file if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) # Return first-class instance initialized from proto obj return cls . read ( proto )
9711	def heappush_max ( heap , item ) : heap . append ( item ) _siftdown_max ( heap , 0 , len ( heap ) - 1 )
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , * * kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
1428	def build_extra_args_dict ( cl_args ) : # Check parameters component_parallelism = cl_args [ 'component_parallelism' ] runtime_configs = cl_args [ 'runtime_config' ] container_number = cl_args [ 'container_number' ] # Users need to provide either (component-parallelism || container_number) or runtime-config if ( component_parallelism and runtime_configs ) or ( container_number and runtime_configs ) : raise Exception ( "(component-parallelism or container_num) and runtime-config " + "can't be updated at the same time" ) dict_extra_args = { } nothing_set = True if component_parallelism : dict_extra_args . update ( { 'component_parallelism' : component_parallelism } ) nothing_set = False if container_number : dict_extra_args . update ( { 'container_number' : container_number } ) nothing_set = False if runtime_configs : dict_extra_args . update ( { 'runtime_config' : runtime_configs } ) nothing_set = False if nothing_set : raise Exception ( "Missing arguments --component-parallelism or --runtime-config or --container-number" ) if cl_args [ 'dry_run' ] : dict_extra_args . update ( { 'dry_run' : True } ) if 'dry_run_format' in cl_args : dict_extra_args . update ( { 'dry_run_format' : cl_args [ "dry_run_format" ] } ) return dict_extra_args
7647	def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
7694	def _sasl_authenticate ( self , stream , username , authzid ) : if not stream . initiator : raise SASLAuthenticationFailed ( "Only initiating entity start" " SASL authentication" ) if stream . features is None or not self . peer_sasl_mechanisms : raise SASLNotAvailable ( "Peer doesn't support SASL" ) props = dict ( stream . auth_properties ) if not props . get ( "service-domain" ) and ( stream . peer and stream . peer . domain ) : props [ "service-domain" ] = stream . peer . domain if username is not None : props [ "username" ] = username if authzid is not None : props [ "authzid" ] = authzid if "password" in self . settings : props [ "password" ] = self . settings [ "password" ] props [ "available_mechanisms" ] = self . peer_sasl_mechanisms enabled = sasl . filter_mechanism_list ( self . settings [ 'sasl_mechanisms' ] , props , self . settings [ 'insecure_auth' ] ) if not enabled : raise SASLNotAvailable ( "None of SASL mechanism selected can be used" ) props [ "enabled_mechanisms" ] = enabled mechanism = None for mech in enabled : if mech in self . peer_sasl_mechanisms : mechanism = mech break if not mechanism : raise SASLMechanismNotAvailable ( "Peer doesn't support any of" " our SASL mechanisms" ) logger . debug ( "Our mechanism: {0!r}" . format ( mechanism ) ) stream . auth_method_used = mechanism self . authenticator = sasl . client_authenticator_factory ( mechanism ) initial_response = self . authenticator . start ( props ) if not isinstance ( initial_response , sasl . Response ) : raise SASLAuthenticationFailed ( "SASL initiation failed" ) element = ElementTree . Element ( AUTH_TAG ) element . set ( "mechanism" , mechanism ) if initial_response . data : if initial_response . encode : element . text = initial_response . encode ( ) else : element . text = initial_response . data stream . write_element ( element )
7445	def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 4: Joint estimation of error rate and heterozygosity" ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 4 , force ) : raise IPyradError ( FIRST_RUN_3 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS_EXIST . format ( len ( samples ) ) ) return ## send to function assemble . jointestimate . run ( self , samples , force , ipyclient )
4663	def new_tx ( self , * args , * * kwargs ) : builder = self . transactionbuilder_class ( * args , blockchain_instance = self , * * kwargs ) self . _txbuffers . append ( builder ) return builder
11535	def available_drivers ( ) : global __modules global __available if type ( __modules ) is not list : __modules = list ( __modules ) if not __available : __available = [ d . ahioDriverInfo . NAME for d in __modules if d . ahioDriverInfo . AVAILABLE ] return __available
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
9370	def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] # get the 11th digit of the INN weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 # get the 12th digit of the INN weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
7700	def verify_roster_set ( self , fix = False , settings = None ) : # pylint: disable=R0912 try : self . _verify ( ( None , u"remove" ) , fix ) except ValueError , err : raise BadRequestProtocolError ( unicode ( err ) ) if self . ask : if fix : self . ask = None else : raise BadRequestProtocolError ( "'ask' in roster set" ) if self . approved : if fix : self . approved = False else : raise BadRequestProtocolError ( "'approved' in roster set" ) if settings is None : settings = XMPPSettings ( ) name_length_limit = settings [ "roster_name_length_limit" ] if self . name and len ( self . name ) > name_length_limit : raise NotAcceptableProtocolError ( u"Roster item name too long" ) group_length_limit = settings [ "roster_group_name_length_limit" ] for group in self . groups : if not group : raise NotAcceptableProtocolError ( u"Roster group name empty" ) if len ( group ) > group_length_limit : raise NotAcceptableProtocolError ( u"Roster group name too long" ) if self . _duplicate_group : raise BadRequestProtocolError ( u"Item group duplicated" )
1892	def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) # Z3 was removed from the system in the middle of operation raise Z3NotFoundError # TODO(mark) don't catch this exception in two places # run solver specific initializations for cfg in self . _init : self . _send ( cfg )
7535	def muscle_chunker ( data , sample ) : ## log our location for debugging LOGGER . info ( "inside muscle_chunker" ) ## only chunk up denovo data, refdata has its own chunking method which ## makes equal size chunks, instead of uneven chunks like in denovo if data . paramsdict [ "assembly_method" ] != "reference" : ## get the number of clusters clustfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) with iter ( gzip . open ( clustfile , 'rb' ) ) as clustio : nloci = sum ( 1 for i in clustio if "//" in i ) // 2 #tclust = clustio.read().count("//")//2 optim = ( nloci // 20 ) + ( nloci % 20 ) LOGGER . info ( "optim for align chunks: %s" , optim ) ## write optim clusters to each tmp file clustio = gzip . open ( clustfile , 'rb' ) inclusts = iter ( clustio . read ( ) . strip ( ) . split ( "//\n//\n" ) ) ## splitting loci so first file is smaller and last file is bigger inc = optim // 10 for idx in range ( 10 ) : ## how big is this chunk? this = optim + ( idx * inc ) left = nloci - this if idx == 9 : ## grab everything left grabchunk = list ( itertools . islice ( inclusts , int ( 1e9 ) ) ) else : ## grab next chunks-worth of data grabchunk = list ( itertools . islice ( inclusts , this ) ) nloci = left ## write the chunk to file tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" . format ( idx ) ) with open ( tmpfile , 'wb' ) as out : out . write ( "//\n//\n" . join ( grabchunk ) ) ## write the chunk to file #grabchunk = list(itertools.islice(inclusts, left)) #if grabchunk: # tmpfile = os.path.join(data.tmpdir, sample.name+"_chunk_9.ali") # with open(tmpfile, 'a') as out: # out.write("\n//\n//\n".join(grabchunk)) clustio . close ( )
116	def map_batches_async ( self , batches , chunksize = None , callback = None , error_callback = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map_async ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize , callback = callback , error_callback = error_callback )
4374	def get_messages_payload ( self , socket , timeout = None ) : try : msgs = socket . get_multiple_client_msgs ( timeout = timeout ) data = self . encode_payload ( msgs ) except Empty : data = "" return data
3414	def model_to_dict ( model , sort = False ) : obj = OrderedDict ( ) obj [ "metabolites" ] = list ( map ( metabolite_to_dict , model . metabolites ) ) obj [ "reactions" ] = list ( map ( reaction_to_dict , model . reactions ) ) obj [ "genes" ] = list ( map ( gene_to_dict , model . genes ) ) obj [ "id" ] = model . id _update_optional ( model , obj , _OPTIONAL_MODEL_ATTRIBUTES , _ORDERED_OPTIONAL_MODEL_KEYS ) if sort : get_id = itemgetter ( "id" ) obj [ "metabolites" ] . sort ( key = get_id ) obj [ "reactions" ] . sort ( key = get_id ) obj [ "genes" ] . sort ( key = get_id ) return obj
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
1786	def CMPXCHG8B ( cpu , dest ) : size = dest . size cmp_reg_name_l = { 64 : 'EAX' , 128 : 'RAX' } [ size ] cmp_reg_name_h = { 64 : 'EDX' , 128 : 'RDX' } [ size ] src_reg_name_l = { 64 : 'EBX' , 128 : 'RBX' } [ size ] src_reg_name_h = { 64 : 'ECX' , 128 : 'RCX' } [ size ] # EDX:EAX or RDX:RAX cmph = cpu . read_register ( cmp_reg_name_h ) cmpl = cpu . read_register ( cmp_reg_name_l ) srch = cpu . read_register ( src_reg_name_h ) srcl = cpu . read_register ( src_reg_name_l ) cmp0 = Operators . CONCAT ( size , cmph , cmpl ) src0 = Operators . CONCAT ( size , srch , srcl ) arg_dest = dest . read ( ) cpu . ZF = arg_dest == cmp0 dest . write ( Operators . ITEBV ( size , cpu . ZF , Operators . CONCAT ( size , srch , srcl ) , arg_dest ) ) cpu . write_register ( cmp_reg_name_l , Operators . ITEBV ( size // 2 , cpu . ZF , cmpl , Operators . EXTRACT ( arg_dest , 0 , size // 2 ) ) ) cpu . write_register ( cmp_reg_name_h , Operators . ITEBV ( size // 2 , cpu . ZF , cmph , Operators . EXTRACT ( arg_dest , size // 2 , size // 2 ) ) )
4244	def _get_record ( self , ipnum ) : seek_country = self . _seek_country ( ipnum ) if seek_country == self . _databaseSegments : return { } read_length = ( 2 * self . _recordLength - 1 ) * self . _databaseSegments try : self . _lock . acquire ( ) self . _fp . seek ( seek_country + read_length , os . SEEK_SET ) buf = self . _fp . read ( const . FULL_RECORD_LENGTH ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) record = { 'dma_code' : 0 , 'area_code' : 0 , 'metro_code' : None , 'postal_code' : None } latitude = 0 longitude = 0 char = ord ( buf [ 0 ] ) record [ 'country_code' ] = const . COUNTRY_CODES [ char ] record [ 'country_code3' ] = const . COUNTRY_CODES3 [ char ] record [ 'country_name' ] = const . COUNTRY_NAMES [ char ] record [ 'continent' ] = const . CONTINENT_NAMES [ char ] def read_data ( buf , pos ) : cur = pos while buf [ cur ] != '\0' : cur += 1 return cur , buf [ pos : cur ] if cur > pos else None offset , record [ 'region_code' ] = read_data ( buf , 1 ) offset , record [ 'city' ] = read_data ( buf , offset + 1 ) offset , record [ 'postal_code' ] = read_data ( buf , offset + 1 ) offset = offset + 1 for j in range ( 3 ) : latitude += ( ord ( buf [ offset + j ] ) << ( j * 8 ) ) for j in range ( 3 ) : longitude += ( ord ( buf [ offset + j + 3 ] ) << ( j * 8 ) ) record [ 'latitude' ] = ( latitude / 10000.0 ) - 180.0 record [ 'longitude' ] = ( longitude / 10000.0 ) - 180.0 if self . _databaseType in ( const . CITY_EDITION_REV1 , const . CITY_EDITION_REV1_V6 ) : if record [ 'country_code' ] == 'US' : dma_area = 0 for j in range ( 3 ) : dma_area += ord ( buf [ offset + j + 6 ] ) << ( j * 8 ) record [ 'dma_code' ] = int ( floor ( dma_area / 1000 ) ) record [ 'area_code' ] = dma_area % 1000 record [ 'metro_code' ] = const . DMA_MAP . get ( record [ 'dma_code' ] ) params = ( record [ 'country_code' ] , record [ 'region_code' ] ) record [ 'time_zone' ] = time_zone_by_country_and_region ( * params ) return record
9481	def to_bytes ( self , previous : bytes ) : # First, validate the lengths. if len ( self . conditions ) != len ( self . body ) : raise exc . CompileError ( "Conditions and body length mismatch!" ) bc = b"" prev_len = len ( previous ) # Loop over the conditions and bodies for condition , body in zip ( self . conditions , self . body ) : # Generate the conditional data. cond_bytecode = condition . to_bytecode ( previous ) bc += cond_bytecode # Complex calculation. First, generate the bytecode for all tokens in the body. Then # we calculate the len() of that. We create a POP_JUMP_IF_FALSE operation that jumps # to the instructions after the body code + 3 for the pop call. This is done for all # chained IF calls, as if it was an elif call. Else calls are not possible to be # auto-generated, but it is possible to emulate them using an elif call that checks # for the opposite of the above IF. # Call the _compile_func method from compiler to compile the body. body_bc = compiler . compile_bytecode ( body ) bdyl = len ( body_bc ) # Add together the lengths. gen_len = prev_len + len ( cond_bytecode ) + bdyl + 1 # Generate the POP_JUMP_IF_FALSE instruction bc += generate_simple_call ( tokens . POP_JUMP_IF_FALSE , gen_len ) # Add the body_bc bc += body_bc # Update previous_len prev_len = len ( previous ) + len ( bc ) return bc
10625	def _calculate_Hfr_coal ( self , T ) : m_C = 0 # kg/h m_H = 0 # kg/h m_O = 0 # kg/h m_N = 0 # kg/h m_S = 0 # kg/h Hfr = 0.0 # kWh/h for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) formula = compound . split ( '[' ) [ 0 ] if stoich . element_mass_fraction ( formula , 'C' ) == 1.0 : m_C += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'H' ) == 1.0 : m_H += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'O' ) == 1.0 : m_O += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'N' ) == 1.0 : m_N += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'S' ) == 1.0 : m_S += self . _compound_mfrs [ index ] else : dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr += dHfr m_total = m_C + m_H + m_O + m_N + m_S # kg/h y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg Hdaf = H - H298 + self . _DH298 # kWh/kg Hdaf *= m_total # kWh/h Hfr += Hdaf return Hfr
9433	def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the file header if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , * * kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
7741	def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : tag = self . _io_sources . pop ( handler , None ) if tag is not None : glib . source_remove ( tag ) if not prepared : self . _unprepared_handlers [ handler ] = fileno if fileno is None : logger . debug ( " {0!r}.fileno() is None, not polling" . format ( handler ) ) return events = 0 if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= glib . IO_IN | glib . IO_ERR if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= glib . IO_OUT | glib . IO_HUP | glib . IO_ERR if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) glib . io_add_watch ( fileno , events , self . _io_callback , handler )
3546	def _characteristics_discovered ( self , service ) : # Characteristics for the specified service were discovered. Update # set of discovered services and signal when all have been discovered. self . _discovered_services . add ( service ) if self . _discovered_services >= set ( self . _peripheral . services ( ) ) : # Found all the services characteristics, finally time to fire the # service discovery complete event. self . _discovered . set ( )
2521	def p_file_type ( self , f_term , predicate ) : try : for _ , _ , ftype in self . graph . triples ( ( f_term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set_file_type ( self . doc , ftype ) except SPDXValueError : self . value_error ( 'FILE_TYPE' , ftype ) except CardinalityError : self . more_than_one_error ( 'file type' )
2386	def create_model_path ( model_path ) : if not model_path . startswith ( "/" ) and not model_path . startswith ( "models/" ) : model_path = "/" + model_path if not model_path . startswith ( "models" ) : model_path = "models" + model_path if not model_path . endswith ( ".p" ) : model_path += ".p" return model_path
3429	def add_boundary ( self , metabolite , type = "exchange" , reaction_id = None , lb = None , ub = None , sbo_term = None ) : ub = CONFIGURATION . upper_bound if ub is None else ub lb = CONFIGURATION . lower_bound if lb is None else lb types = { "exchange" : ( "EX" , lb , ub , sbo_terms [ "exchange" ] ) , "demand" : ( "DM" , 0 , ub , sbo_terms [ "demand" ] ) , "sink" : ( "SK" , lb , ub , sbo_terms [ "sink" ] ) } if type == "exchange" : external = find_external_compartment ( self ) if metabolite . compartment != external : raise ValueError ( "The metabolite is not an external metabolite" " (compartment is `%s` but should be `%s`). " "Did you mean to add a demand or sink? " "If not, either change its compartment or " "rename the model compartments to fix this." % ( metabolite . compartment , external ) ) if type in types : prefix , lb , ub , default_term = types [ type ] if reaction_id is None : reaction_id = "{}_{}" . format ( prefix , metabolite . id ) if sbo_term is None : sbo_term = default_term if reaction_id is None : raise ValueError ( "Custom types of boundary reactions require a custom " "identifier. Please set the `reaction_id`." ) if reaction_id in self . reactions : raise ValueError ( "Boundary reaction '{}' already exists." . format ( reaction_id ) ) name = "{} {}" . format ( metabolite . name , type ) rxn = Reaction ( id = reaction_id , name = name , lower_bound = lb , upper_bound = ub ) rxn . add_metabolites ( { metabolite : - 1 } ) if sbo_term : rxn . annotation [ "sbo" ] = sbo_term self . add_reactions ( [ rxn ] ) return rxn
2195	def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
11036	def _check_request_results ( self , results ) : responses = [ ] failed_endpoints = [ ] for index , result_tuple in enumerate ( results ) : success , result = result_tuple if success : responses . append ( result ) else : endpoint = self . endpoints [ index ] self . log . failure ( 'Failed to make a request to a marathon-lb instance: ' '{endpoint}' , result , LogLevel . error , endpoint = endpoint ) responses . append ( None ) failed_endpoints . append ( endpoint ) if len ( failed_endpoints ) == len ( self . endpoints ) : raise RuntimeError ( 'Failed to make a request to all marathon-lb instances' ) if failed_endpoints : self . log . error ( 'Failed to make a request to {x}/{y} marathon-lb instances: ' '{endpoints}' , x = len ( failed_endpoints ) , y = len ( self . endpoints ) , endpoints = failed_endpoints ) return responses
12692	def write_tersoff_potential ( parameters ) : lines = [ ] for ( e1 , e2 , e3 ) , params in parameters . items ( ) : if len ( params ) != 14 : raise ValueError ( 'tersoff three body potential expects 14 parameters' ) lines . append ( ' ' . join ( [ e1 , e2 , e3 ] + [ '{:16.8g}' . format ( _ ) for _ in params ] ) ) return '\n' . join ( lines )
2924	def _predict ( self , my_task , seen = None , looked_ahead = 0 ) : if my_task . _is_finished ( ) : return if seen is None : seen = [ ] elif self in seen : return if not my_task . _is_finished ( ) : self . _predict_hook ( my_task ) if not my_task . _is_definite ( ) : if looked_ahead + 1 >= self . lookahead : return seen . append ( self ) for child in my_task . children : child . task_spec . _predict ( child , seen [ : ] , looked_ahead + 1 )
4417	async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
8891	def deserialize ( cls , dict_model ) : kwargs = { } for f in cls . _meta . concrete_fields : if f . attname in dict_model : kwargs [ f . attname ] = dict_model [ f . attname ] return cls ( * * kwargs )
3534	def olark ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OlarkNode ( )
6833	def version ( self ) : r = self . local_renderer with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : res = r . local ( 'vagrant --version' , capture = True ) if res . failed : return None line = res . splitlines ( ) [ - 1 ] version = re . match ( r'Vagrant (?:v(?:ersion )?)?(.*)' , line ) . group ( 1 ) return tuple ( _to_int ( part ) for part in version . split ( '.' ) )
1440	def register_metrics ( self , context ) : sys_config = system_config . get_sys_config ( ) interval = float ( sys_config [ constants . HERON_METRICS_EXPORT_INTERVAL_SEC ] ) collector = context . get_metrics_collector ( ) super ( ComponentMetrics , self ) . register_metrics ( collector , interval )
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
12154	def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return "%.02f ms" % ( took * 1000.0 ) elif took < 60 : return "%.02f s" % ( took ) else : return "%.02f min" % ( took / 60.0 )
3240	def get_group_policy_document ( group_name , policy_name , client = None , * * kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , * * kwargs ) [ 'PolicyDocument' ]
5538	def _extract ( self , in_tile = None , in_data = None , out_tile = None ) : return self . config . output . extract_subset ( input_data_tiles = [ ( in_tile , in_data ) ] , out_tile = out_tile )
7767	def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . setup_stanza_handlers ( handlers , "post-auth" )
11598	def prepare ( self ) : # Create a collection for the attributes and elements of # this instance. attributes , elements = OrderedDict ( ) , [ ] # Initialize the namespace map. nsmap = dict ( [ self . meta . namespace ] ) # Iterate through all declared items. for name , item in self . _items . items ( ) : if isinstance ( item , Attribute ) : # Prepare the item as an attribute. attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : # Update the nsmap. nsmap . update ( [ item . namespace ] ) # Prepare the item as an element. elements . append ( item ) # Return the collected attributes and elements return attributes , elements , nsmap
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
8350	def _toStringSubclass ( self , text , subclass ) : self . endData ( ) self . handle_data ( text ) self . endData ( subclass )
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
11159	def execute_pyfile ( self , py_exe = None ) : # pragma: no cover import subprocess self . assert_is_dir_and_exists ( ) if py_exe is None : if six . PY2 : py_exe = "python2" elif six . PY3 : py_exe = "python3" for p in self . select_by_ext ( ".py" ) : subprocess . Popen ( '%s "%s"' % ( py_exe , p . abspath ) )
5035	def get_pending_users_queryset ( self , search_keyword , customer_uuid ) : queryset = PendingEnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) if search_keyword is not None : queryset = queryset . filter ( user_email__icontains = search_keyword ) return queryset
6629	def read ( self , filenames ) : for fn in filenames : try : self . configs [ fn ] = ordered_json . load ( fn ) except IOError : self . configs [ fn ] = OrderedDict ( ) except Exception as e : self . configs [ fn ] = OrderedDict ( ) logging . warning ( "Failed to read settings file %s, it will be ignored. The error was: %s" , fn , e )
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : # pragma: no cover raise
8000	def fix_out_stanza ( self , stanza ) : StreamBase . fix_out_stanza ( self , stanza ) if self . initiator : if stanza . from_jid : stanza . from_jid = None else : if not stanza . from_jid : stanza . from_jid = self . me
6940	def _gaussian ( x , amp , loc , std ) : return amp * np . exp ( - ( ( x - loc ) * ( x - loc ) ) / ( 2.0 * std * std ) )
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
5773	def dsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'dsa' : raise ValueError ( 'The key specified is not a DSA private key' ) return _sign ( private_key , data , hash_algorithm )
11936	def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
13080	def register ( self ) : if self . app is not None : if not self . blueprint : self . blueprint = self . create_blueprint ( ) self . app . register_blueprint ( self . blueprint ) if self . cache is None : # We register a fake cache extension. setattr ( self . app . jinja_env , "_fake_cache_extension" , self ) self . app . jinja_env . add_extension ( FakeCacheExtension ) return self . blueprint return None
12397	def gen_method_keys ( self , * args , * * kwargs ) : token = args [ 0 ] for mro_type in type ( token ) . __mro__ [ : - 1 ] : name = mro_type . __name__ yield name
821	def compute ( slidingWindow , total , newVal , windowSize ) : if len ( slidingWindow ) == windowSize : total -= slidingWindow . pop ( 0 ) slidingWindow . append ( newVal ) total += newVal return float ( total ) / len ( slidingWindow ) , slidingWindow , total
1790	def IDIV ( cpu , src ) : reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ src . size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ src . size ] dividend = Operators . CONCAT ( src . size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = src . read ( ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) dst_size = src . size * 2 divisor = Operators . SEXTEND ( divisor , src . size , dst_size ) mask = ( 1 << dst_size ) - 1 sign_mask = 1 << ( dst_size - 1 ) dividend_sign = ( dividend & sign_mask ) != 0 divisor_sign = ( divisor & sign_mask ) != 0 if isinstance ( divisor , int ) : if divisor_sign : divisor = ( ( ~ divisor ) + 1 ) & mask divisor = - divisor if isinstance ( dividend , int ) : if dividend_sign : dividend = ( ( ~ dividend ) + 1 ) & mask dividend = - dividend quotient = Operators . SDIV ( dividend , divisor ) if ( isinstance ( dividend , int ) and isinstance ( dividend , int ) ) : # handle the concrete case remainder = dividend - ( quotient * divisor ) else : # symbolic case -- optimize via SREM remainder = Operators . SREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , src . size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , src . size ) )
6683	def require ( self , path = None , contents = None , source = None , url = None , md5 = None , use_sudo = False , owner = None , group = '' , mode = None , verify_remote = True , temp_dir = '/tmp' ) : func = use_sudo and run_as_root or self . run # 1) Only a path is given if path and not ( contents or source or url ) : assert path if not self . is_file ( path ) : func ( 'touch "%(path)s"' % locals ( ) ) # 2) A URL is specified (path is optional) elif url : if not path : path = os . path . basename ( urlparse ( url ) . path ) if not self . is_file ( path ) or md5 and self . md5sum ( path ) != md5 : func ( 'wget --progress=dot:mega "%(url)s" -O "%(path)s"' % locals ( ) ) # 3) A local filename, or a content string, is specified else : if source : assert not contents t = None else : fd , source = mkstemp ( ) t = os . fdopen ( fd , 'w' ) t . write ( contents ) t . close ( ) if verify_remote : # Avoid reading the whole file into memory at once digest = hashlib . md5 ( ) f = open ( source , 'rb' ) try : while True : d = f . read ( BLOCKSIZE ) if not d : break digest . update ( d ) finally : f . close ( ) else : digest = None if ( not self . is_file ( path , use_sudo = use_sudo ) or ( verify_remote and self . md5sum ( path , use_sudo = use_sudo ) != digest . hexdigest ( ) ) ) : with self . settings ( hide ( 'running' ) ) : self . put ( local_path = source , remote_path = path , use_sudo = use_sudo , temp_dir = temp_dir ) if t is not None : os . unlink ( source ) # Ensure correct owner if use_sudo and owner is None : owner = 'root' if ( owner and self . get_owner ( path , use_sudo ) != owner ) or ( group and self . get_group ( path , use_sudo ) != group ) : func ( 'chown %(owner)s:%(group)s "%(path)s"' % locals ( ) ) # Ensure correct mode if use_sudo and mode is None : mode = oct ( 0o666 & ~ int ( self . umask ( use_sudo = True ) , base = 8 ) ) if mode and self . get_mode ( path , use_sudo ) != mode : func ( 'chmod %(mode)s "%(path)s"' % locals ( ) )
3348	def remove_members ( self , to_remove ) : if isinstance ( to_remove , string_types ) or hasattr ( to_remove , "id" ) : warn ( "need to pass in a list" ) to_remove = [ to_remove ] self . _members . difference_update ( to_remove )
13073	def r_assets ( self , filetype , asset ) : if filetype in self . assets and asset in self . assets [ filetype ] and self . assets [ filetype ] [ asset ] : return send_from_directory ( directory = self . assets [ filetype ] [ asset ] , filename = asset ) abort ( 404 )
11816	def score ( self , code ) : text = permutation_decode ( self . ciphertext , code ) logP = ( sum ( [ log ( self . Pwords [ word ] ) for word in words ( text ) ] ) + sum ( [ log ( self . P1 [ c ] ) for c in text ] ) + sum ( [ log ( self . P2 [ b ] ) for b in bigrams ( text ) ] ) ) return exp ( logP )
1170	def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = " " * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )
10888	def coords ( self , norm = False , form = 'broadcast' ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . arange ( self . l [ i ] , self . r [ i ] ) / norm [ i ] for i in range ( self . dim ) ) return self . _format_vector ( v , form = form )
9268	def detect_link_tag_time ( self , tag ) : # if tag is nil - set current time newer_tag_time = self . get_time_of_tag ( tag ) if tag else datetime . datetime . now ( ) # if it's future release tag - set this value if tag [ "name" ] == self . options . unreleased_label and self . options . future_release : newer_tag_name = self . options . future_release newer_tag_link = self . options . future_release elif tag [ "name" ] is not self . options . unreleased_label : # put unreleased label if there is no name for the tag newer_tag_name = tag [ "name" ] newer_tag_link = newer_tag_name else : newer_tag_name = self . options . unreleased_label newer_tag_link = "HEAD" return [ newer_tag_link , newer_tag_name , newer_tag_time ]
397	def cross_entropy ( output , target , name = None ) : if name is None : raise Exception ( "Please give a unique name to tl.cost.cross_entropy for TF1.0+" ) return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
2983	def cmd_events ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . json : outf = None _write = puts if opts . output is not None : outf = open ( opts . output , "w" ) _write = outf . write try : delim = "" logs = b . get_audit ( ) . read_logs ( as_json = False ) _write ( '{"events": [' ) _write ( os . linesep ) for l in logs : _write ( delim + l ) delim = "," + os . linesep _write ( os . linesep ) _write ( ']}' ) finally : if opts . output is not None : outf . close ( ) else : puts ( colored . blue ( columns ( [ "EVENT" , 10 ] , [ "TARGET" , 16 ] , [ "STATUS" , 8 ] , [ "TIME" , 16 ] , [ "MESSAGE" , 25 ] ) ) ) logs = b . get_audit ( ) . read_logs ( as_json = True ) for l in logs : puts ( columns ( [ l [ 'event' ] , 10 ] , [ str ( [ str ( t ) for t in l [ 'targets' ] ] ) , 16 ] , [ l [ 'status' ] , 8 ] , [ str ( l [ 'timestamp' ] ) , 16 ] , [ l [ 'message' ] , 25 ] ) )
12890	def handle_text ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return doc . value . c8_array . text or None
9723	async def save ( self , filename , overwrite = False ) : cmd = "save %s%s" % ( filename , " overwrite" if overwrite else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
6851	def get_public_ip ( self ) : r = self . local_renderer ret = r . run ( r . env . get_public_ip_command ) or '' ret = ret . strip ( ) print ( 'ip:' , ret ) return ret
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
5203	def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
8744	def get_floatingip ( context , id , fields = None ) : LOG . info ( 'get_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . FLOATING , '_deallocated' : False } floating_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , * * filters ) if not floating_ip : raise q_exc . FloatingIpNotFound ( id = id ) return v . _make_floating_ip_dict ( floating_ip )
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , * * kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , * * kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
1260	def restore_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) component . restore ( sess = self . session , save_path = save_path )
13631	def _adaptToResource ( self , result ) : if result is None : return NotFound ( ) spinneretResource = ISpinneretResource ( result , None ) if spinneretResource is not None : return SpinneretResource ( spinneretResource ) renderable = IRenderable ( result , None ) if renderable is not None : return _RenderableResource ( renderable ) resource = IResource ( result , None ) if resource is not None : return resource if isinstance ( result , URLPath ) : return Redirect ( str ( result ) ) return result
9621	def gamepad ( self ) : state = _xinput_state ( ) _xinput . XInputGetState ( self . ControllerID - 1 , pointer ( state ) ) self . dwPacketNumber = state . dwPacketNumber return state . XINPUT_GAMEPAD
4947	def send_course_completion_statement ( lrs_configuration , user , course_overview , course_grade ) : user_details = LearnerInfoSerializer ( user ) course_details = CourseInfoSerializer ( course_overview ) statement = LearnerCourseCompletionStatement ( user , course_overview , user_details . data , course_details . data , course_grade , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
919	def error ( self , msg , * args , * * kwargs ) : self . _baseLogger . error ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
1457	def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
9260	def include_issues_by_labels ( self , all_issues ) : included_by_labels = self . filter_by_include_labels ( all_issues ) wo_labels = self . filter_wo_labels ( all_issues ) il = set ( [ f [ "number" ] for f in included_by_labels ] ) wl = set ( [ w [ "number" ] for w in wo_labels ] ) filtered_issues = [ ] for issue in all_issues : if issue [ "number" ] in il or issue [ "number" ] in wl : filtered_issues . append ( issue ) return filtered_issues
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
7877	def _bind_success ( self , stanza ) : # pylint: disable-msg=R0201 payload = stanza . get_payload ( ResourceBindingPayload ) jid = payload . jid if not jid : raise BadRequestProtocolError ( u"<jid/> element mising in" " the bind response" ) self . stream . me = jid self . stream . event ( AuthorizedEvent ( self . stream . me ) )
1815	def SETNLE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) )
9235	def parse ( data ) : sections = re . compile ( "^## .+$" , re . MULTILINE ) . split ( data ) headings = re . findall ( "^## .+?$" , data , re . MULTILINE ) sections . pop ( 0 ) parsed = [ ] def func ( h , s ) : p = parse_heading ( h ) p [ "content" ] = s parsed . append ( p ) list ( map ( func , headings , sections ) ) return parsed
6273	def load ( self , meta : ResourceDescription ) -> Any : self . _check_meta ( meta ) self . resolve_loader ( meta ) return meta . loader_cls ( meta ) . load ( )
13405	def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
1929	def process_config_values ( parser : argparse . ArgumentParser , args : argparse . Namespace ) : # First, load a local config file, if passed or look for one in pwd if it wasn't. load_overrides ( args . config ) # Get a list of defined config vals. If these are passed on the command line, # update them in their correct group, not in the cli group defined_vars = list ( get_config_keys ( ) ) command_line_args = vars ( args ) # Bring in the options keys into args config_cli_args = get_group ( 'cli' ) # Place all command line args into the cli group (for saving in the workspace). If # the value is set on command line, then it takes precedence; otherwise we try to # read it from the config file's cli group. for k in command_line_args : default = parser . get_default ( k ) set_val = getattr ( args , k ) if default is not set_val : if k not in defined_vars : config_cli_args . update ( k , value = set_val ) else : # Update a var's native group group_name , key = k . split ( '.' ) group = get_group ( group_name ) setattr ( group , key , set_val ) else : if k in config_cli_args : setattr ( args , k , getattr ( config_cli_args , k ) )
4677	def getMemoKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) key = self . getPrivateKeyForPublicKey ( account [ "options" ] [ "memo_key" ] ) if key : return key return False
9030	def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
3173	def create_or_update ( self , store_id , customer_id , data ) : self . store_id = store_id self . customer_id = customer_id if 'id' not in data : raise KeyError ( 'The store customer must have an id' ) if 'email_address' not in data : raise KeyError ( 'Each store customer must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'opt_in_status' not in data : raise KeyError ( 'The store customer must have an opt_in_status' ) if data [ 'opt_in_status' ] not in [ True , False ] : raise TypeError ( 'The opt_in_status must be True or False' ) return self . _mc_client . _put ( url = self . _build_path ( store_id , 'customers' , customer_id ) , data = data )
12721	def hi_stops ( self , hi_stops ) : _set_params ( self . ode_obj , 'HiStop' , hi_stops , self . ADOF + self . LDOF )
11171	def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = _autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . _wrap_labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
13712	def invalidate_cache ( self ) : if self . _use_cache : self . _cache_version += 1 self . _cache . increment ( 'cached_httpbl_{0}_version' . format ( self . _api_key ) )
5455	def numeric_task_id ( task_id ) : # This function exists to support the legacy "task-id" format in the "google" # provider. Google labels originally could not be numeric. When the google # provider is completely replaced by the google-v2 provider, this function can # go away. if task_id is not None : if task_id . startswith ( 'task-' ) : return int ( task_id [ len ( 'task-' ) : ] ) else : return int ( task_id )
1235	def from_spec ( spec , kwargs ) : agent = util . get_object ( obj = spec , predefined_objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent
4781	def is_between ( self , low , high ) : val_type = type ( self . val ) self . _validate_between_args ( val_type , low , high ) if self . val < low or self . val > high : if val_type is datetime . datetime : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self
1888	def must_be_true ( self , constraints , expression ) -> bool : solutions = self . get_all_values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]
8008	def activate ( self ) : obj = self . find_paypal_object ( ) if obj . state == enums . BillingPlanState . CREATED : success = obj . activate ( ) if not success : raise PaypalApiError ( "Failed to activate plan: %r" % ( obj . error ) ) # Resync the updated data to the database self . get_or_update_from_api_data ( obj , always_sync = True ) return obj
2773	def create ( self , * args , * * kwargs ) : rules_dict = [ rule . __dict__ for rule in self . forwarding_rules ] params = { 'name' : self . name , 'region' : self . region , 'forwarding_rules' : rules_dict , 'redirect_http_to_https' : self . redirect_http_to_https } if self . droplet_ids and self . tag : raise ValueError ( 'droplet_ids and tag are mutually exclusive args' ) elif self . tag : params [ 'tag' ] = self . tag else : params [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : params [ 'algorithm' ] = self . algorithm if self . health_check : params [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : params [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ data = self . get_data ( 'load_balancers/' , type = POST , params = params ) if data : self . id = data [ 'load_balancer' ] [ 'id' ] self . ip = data [ 'load_balancer' ] [ 'ip' ] self . algorithm = data [ 'load_balancer' ] [ 'algorithm' ] self . health_check = HealthCheck ( * * data [ 'load_balancer' ] [ 'health_check' ] ) self . sticky_sessions = StickySesions ( * * data [ 'load_balancer' ] [ 'sticky_sessions' ] ) self . droplet_ids = data [ 'load_balancer' ] [ 'droplet_ids' ] self . status = data [ 'load_balancer' ] [ 'status' ] self . created_at = data [ 'load_balancer' ] [ 'created_at' ] return self
6280	def clear_values ( self , red = 0.0 , green = 0.0 , blue = 0.0 , alpha = 0.0 , depth = 1.0 ) : self . clear_color = ( red , green , blue , alpha ) self . clear_depth = depth
10020	def environment_exists ( self , env_name ) : response = self . ebs . describe_environments ( application_name = self . app_name , environment_names = [ env_name ] , include_deleted = False ) return len ( response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] ) > 0 and response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'
13525	def error ( code : int , * args , * * kwargs ) -> HedgehogCommandError : # TODO add proper error code if code == FAILED_COMMAND and len ( args ) >= 1 and args [ 0 ] == "Emergency Shutdown activated" : return EmergencyShutdown ( * args , * * kwargs ) return _errors [ code ] ( * args , * * kwargs )
7220	def to_geotiff ( arr , path = './output.tif' , proj = None , spec = None , bands = None , * * kwargs ) : assert has_rasterio , "To create geotiff images please install rasterio" try : img_md = arr . rda . metadata [ "image" ] x_size = img_md [ "tileXSize" ] y_size = img_md [ "tileYSize" ] except ( AttributeError , KeyError ) : x_size = kwargs . get ( "chunk_size" , 256 ) y_size = kwargs . get ( "chunk_size" , 256 ) try : tfm = kwargs [ 'transform' ] if 'transform' in kwargs else arr . affine except : tfm = None dtype = arr . dtype . name if arr . dtype . name != 'int8' else 'uint8' if spec is not None and spec . lower ( ) == 'rgb' : if bands is None : bands = arr . _rgb_bands # skip if already DRA'ed if not arr . options . get ( 'dra' ) : # add the RDA HistogramDRA op to get a RGB 8-bit image from gbdxtools . rda . interface import RDA rda = RDA ( ) dra = rda . HistogramDRA ( arr ) # Reset the bounds and select the bands on the new Dask arr = dra . aoi ( bbox = arr . bounds ) arr = arr [ bands , ... ] . astype ( np . uint8 ) dtype = 'uint8' else : if bands is not None : arr = arr [ bands , ... ] meta = { 'width' : arr . shape [ 2 ] , 'height' : arr . shape [ 1 ] , 'count' : arr . shape [ 0 ] , 'dtype' : dtype , 'driver' : 'GTiff' , 'transform' : tfm } if proj is not None : meta [ "crs" ] = { 'init' : proj } if "tiled" in kwargs and kwargs [ "tiled" ] : meta . update ( blockxsize = x_size , blockysize = y_size , tiled = "yes" ) with rasterio . open ( path , "w" , * * meta ) as dst : writer = rio_writer ( dst ) result = store ( arr , writer , compute = False ) result . compute ( scheduler = threaded_get ) return path
7545	def calculate_depths ( data , samples , lbview ) : ## send jobs to be processed on engines start = time . time ( ) printstr = " calculating depths | {} | s5 |" recaljobs = { } maxlens = [ ] for sample in samples : recaljobs [ sample . name ] = lbview . apply ( recal_hidepth , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in recaljobs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures and collect results modsamples = [ ] for sample in samples : if not recaljobs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , recaljobs [ sample . name ] . exception ( ) ) else : modsample , _ , maxlen , _ , _ = recaljobs [ sample . name ] . result ( ) modsamples . append ( modsample ) maxlens . append ( maxlen ) ## reset global maxlen if something changed data . _hackersonly [ "max_fragment_length" ] = int ( max ( maxlens ) ) + 4 return samples
3161	def get ( self , conversation_id , * * queryparams ) : self . conversation_id = conversation_id return self . _mc_client . _get ( url = self . _build_path ( conversation_id ) , * * queryparams )
3589	def set_color ( self , r , g , b ) : # See more details on the bulb's protocol from this guide: # https://learn.adafruit.com/reverse-engineering-a-bluetooth-low-energy-light-bulb/overview command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
9243	def find_closed_date_by_commit ( self , issue ) : if not issue . get ( 'events' ) : return # if it's PR -> then find "merged event", in case # of usual issue -> find closed date compare_string = "merged" if 'merged_at' in issue else "closed" # reverse! - to find latest closed event. (event goes in date order) # if it were reopened and closed again. issue [ 'events' ] . reverse ( ) found_date = False for event in issue [ 'events' ] : if event [ "event" ] == compare_string : self . set_date_from_event ( event , issue ) found_date = True break if not found_date : # TODO: assert issues, that remain without # 'actual_date' hash for some reason. print ( "\nWARNING: Issue without 'actual_date':" " #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) )
12798	def _fetch ( self , method , url = None , post_data = None , parse_data = True , key = None , parameters = None , listener = None , full_return = False ) : headers = self . get_headers ( ) headers [ "Content-Type" ] = "application/json" handlers = [ ] debuglevel = int ( self . _settings [ "debug" ] ) handlers . append ( urllib2 . HTTPHandler ( debuglevel = debuglevel ) ) if hasattr ( httplib , "HTTPS" ) : handlers . append ( urllib2 . HTTPSHandler ( debuglevel = debuglevel ) ) handlers . append ( urllib2 . HTTPCookieProcessor ( cookielib . CookieJar ( ) ) ) password_url = self . _get_password_url ( ) if password_url and "Authorization" not in headers : pwd_manager = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) pwd_manager . add_password ( None , password_url , self . _settings [ "user" ] , self . _settings [ "password" ] ) handlers . append ( HTTPBasicAuthHandler ( pwd_manager ) ) opener = urllib2 . build_opener ( * handlers ) if post_data is not None : post_data = json . dumps ( post_data ) uri = self . _url ( url , parameters ) request = RESTRequest ( uri , method = method , headers = headers ) if post_data is not None : request . add_data ( post_data ) response = None try : response = opener . open ( request ) body = response . read ( ) if password_url and password_url not in self . _settings [ "authorizations" ] and request . has_header ( "Authorization" ) : self . _settings [ "authorizations" ] [ password_url ] = request . get_header ( "Authorization" ) except urllib2 . HTTPError as e : if e . code == 401 : raise AuthenticationError ( "Access denied while trying to access %s" % uri ) elif e . code == 404 : raise ConnectionError ( "URL not found: %s" % uri ) else : raise except urllib2 . URLError as e : raise ConnectionError ( "Error while fetching from %s: %s" % ( uri , e ) ) finally : if response : response . close ( ) opener . close ( ) data = None if parse_data : if not key : key = string . split ( url , "/" ) [ 0 ] data = self . parse ( body , key ) if full_return : info = response . info ( ) if response else None status = int ( string . split ( info [ "status" ] ) [ 0 ] ) if ( info and "status" in info ) else None return { "success" : ( status >= 200 and status < 300 ) , "data" : data , "info" : info , "body" : body } return data
2901	def complete_task_from_id ( self , task_id ) : if task_id is None : raise WorkflowException ( self . spec , 'task_id is None' ) for task in self . task_tree : if task . id == task_id : return task . complete ( ) msg = 'A task with the given task_id (%s) was not found' % task_id raise WorkflowException ( self . spec , msg )
2804	def convert_clip ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting clip ...' ) if params [ 'min' ] == 0 : print ( "using ReLU({0})" . format ( params [ 'max' ] ) ) layer = keras . layers . ReLU ( max_value = params [ 'max' ] ) else : def target_layer ( x , vmin = params [ 'min' ] , vmax = params [ 'max' ] ) : import tensorflow as tf return tf . clip_by_value ( x , vmin , vmax ) layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = layer ( layers [ inputs [ 0 ] ] )
325	def rolling_sharpe ( returns , rolling_sharpe_window ) : return returns . rolling ( rolling_sharpe_window ) . mean ( ) / returns . rolling ( rolling_sharpe_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
4696	def env ( ) : if cij . ssh . env ( ) : cij . err ( "board.env: invalid SSH environment" ) return 1 board = cij . env_to_dict ( PREFIX , REQUIRED ) # Verify REQUIRED variables if board is None : cij . err ( "board.env: invalid BOARD environment" ) return 1 board [ "CLASS" ] = "_" . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ "IDENT" ] = "-" . join ( [ board [ "CLASS" ] , board [ "ALIAS" ] ] ) cij . env_export ( PREFIX , EXPORTED , board ) # Export EXPORTED variables return 0
12596	def _openpyxl_read_xl ( xl_path : str ) : try : wb = load_workbook ( filename = xl_path , read_only = True ) except : raise else : return wb
11756	def prop_symbols ( x ) : if not isinstance ( x , Expr ) : return [ ] elif is_prop_symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop_symbols ( arg ) ) )
1731	def call ( self , this , args = ( ) ) : if self . is_native : _args = SpaceTuple ( args ) # we have to do that unfortunately to pass all the necessary info to the funcs _args . space = self . space return self . code ( this , _args ) # must return valid js object - undefined, null, float, unicode, bool, or PyJs else : return self . space . exe . _call ( self , this , args )
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
4482	def storages ( self ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
11105	def sync_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if not self . _keepSynchronized : r = func ( self , * args , * * kwargs ) else : state = self . _load_state ( ) #print("-----------> ",state, self.state) if state is None : r = func ( self , * args , * * kwargs ) elif state == self . state : r = func ( self , * args , * * kwargs ) else : warnings . warn ( "Repository at '%s' is out of date. Need to load it again to avoid conflict." % self . path ) r = None return r return wrapper
8568	def get_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics/%s?depth=%s' % ( datacenter_id , loadbalancer_id , nic_id , str ( depth ) ) ) return response
8294	def clique ( graph , id ) : clique = [ id ] for n in graph . nodes : friend = True for id in clique : if n . id == id or graph . edge ( n . id , id ) == None : friend = False break if friend : clique . append ( n . id ) return clique
5272	def _find_lcs ( self , node , stringIdxs ) : nodes = [ self . _find_lcs ( n , stringIdxs ) for ( n , _ ) in node . transition_links if n . generalized_idxs . issuperset ( stringIdxs ) ] if nodes == [ ] : return node deepestNode = max ( nodes , key = lambda n : n . depth ) return deepestNode
5692	def update ( self , new_labels , departure_time_backup = None ) : if self . _closed : raise RuntimeError ( "Profile is closed, no updates can be made" ) try : departure_time = next ( iter ( new_labels ) ) . departure_time except StopIteration : departure_time = departure_time_backup self . _check_dep_time_is_valid ( departure_time ) for new_label in new_labels : assert ( new_label . departure_time == departure_time ) dep_time_index = self . dep_times_to_index [ departure_time ] if dep_time_index > 0 : # Departure time is modified in order to not pass on labels which are not Pareto-optimal when departure time is ignored. mod_prev_labels = [ label . get_copy_with_specified_departure_time ( departure_time ) for label in self . _label_bags [ dep_time_index - 1 ] ] else : mod_prev_labels = list ( ) mod_prev_labels += self . _label_bags [ dep_time_index ] walk_label = self . _get_label_to_target ( departure_time ) if walk_label : new_labels = new_labels + [ walk_label ] new_frontier = merge_pareto_frontiers ( new_labels , mod_prev_labels ) self . _label_bags [ dep_time_index ] = new_frontier return True
1721	def limited ( func ) : def f ( standard = False , * * args ) : insert_pos = len ( inline_stack . names ) # in case line is longer than limit we will have to insert the lval at current position # this is because calling func will change inline_stack. # we cant use inline_stack.require here because we dont know whether line overflows yet res = func ( * * args ) if len ( res ) > LINE_LEN_LIMIT : name = inline_stack . require ( 'LONG' ) inline_stack . names . pop ( ) inline_stack . names . insert ( insert_pos , name ) res = 'def %s(var=var):\n return %s\n' % ( name , res ) inline_stack . define ( name , res ) return name + '()' else : return res f . __dict__ [ 'standard' ] = func return f
312	def downside_risk ( returns , required_return = 0 , period = DAILY ) : return ep . downside_risk ( returns , required_return = required_return , period = period )
12006	def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
5626	def write_json ( path , params ) : logger . debug ( "write %s to %s" , params , path ) if path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) logger . debug ( "upload %s" , key ) bucket . put_object ( Key = key , Body = json . dumps ( params , sort_keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort_keys = True , indent = 4 )
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
3742	def StielPolar ( Tc = None , Pc = None , omega = None , CASRN = '' , Method = None , AvailableMethods = False ) : def list_methods ( ) : methods = [ ] if Tc and Pc and omega : methods . append ( 'DEFINITION' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc * 0.6 ) if not P : factor = None else : Pr = P / Pc factor = log10 ( Pr ) + 1.70 * omega + 1.552 elif Method == 'NONE' : factor = None else : raise Exception ( 'Failure in in function' ) return factor
12945	def hasSameValues ( self , other , cascadeObject = True ) : if self . FIELDS != other . FIELDS : return False oga = object . __getattribute__ for field in self . FIELDS : thisVal = oga ( self , field ) otherVal = oga ( other , field ) if thisVal != otherVal : return False if cascadeObject is True and issubclass ( field . __class__ , IRForeignLinkFieldBase ) : if thisVal and thisVal . isFetched ( ) : if otherVal and otherVal . isFetched ( ) : theseForeign = thisVal . getObjs ( ) othersForeign = otherVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if not theseForeign [ i ] . hasSameValues ( othersForeign [ i ] ) : return False else : theseForeign = thisVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if theseForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False else : if otherVal and otherVal . isFetched ( ) : othersForeign = otherVal . getObjs ( ) for i in range ( len ( othersForeign ) ) : if othersForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False return True
13360	def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env_data = yaml . load ( f . read ( ) ) if env_data : for env in env_data : self . add ( VirtualEnvironment ( env [ 'root' ] ) )
11681	def send ( self , command , timeout = 5 ) : logger . info ( u'Sending %s' % command ) _ , writable , __ = select . select ( [ ] , [ self . sock ] , [ ] , timeout ) if not writable : raise SendTimeoutError ( ) writable [ 0 ] . sendall ( command + '\n' )
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
9371	def password ( at_least = 6 , at_most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at_least = at_least , at_most = at_most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
8727	def daily_at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) # convert when to the next datetime matching this time when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when < now ( ) : when += daily return cls . at_time ( cls . _localize ( when ) , daily , target )
6404	def get_feature ( vector , feature ) : # :param bool binary: if False, -1, 0, & 1 represent -, 0, & + # if True, only binary oppositions are allowed: # 0 & 1 represent - & + and 0s are mapped to - if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) # each feature mask contains two bits, one each for - and + mask = _FEATURE_MASK [ feature ] # the lower bit represents + pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) # 0 elif masked == mask : retvec . append ( 2 ) # +/- elif masked & pos_mask : retvec . append ( 1 ) # + else : retvec . append ( - 1 ) # - return retvec
2035	def SLOAD ( self , offset ) : storage_address = self . address self . _publish ( 'will_evm_read_storage' , storage_address , offset ) value = self . world . get_storage_data ( storage_address , offset ) self . _publish ( 'did_evm_read_storage' , storage_address , offset , value ) return value
10822	def query_invitations ( cls , user , eager = False ) : if eager : eager = [ Membership . group ] return cls . query_by_user ( user , state = MembershipState . PENDING_USER , eager = eager )
725	def get ( self , number ) : if not number in self . _patterns : raise IndexError ( "Invalid number" ) return self . _patterns [ number ]
7942	def _got_addresses ( self , name , port , addrs ) : with self . lock : if not addrs : if self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _dst_addrs = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Could not resolve address record for {0!r}" . format ( name ) ) self . _dst_addrs = [ ( family , ( addr , port ) ) for ( family , addr ) in addrs ] self . _set_state ( "connect" )
1142	def _long2bytesBigEndian ( n , blocksize = 0 ) : # After much testing, this algorithm was deemed to be the fastest. s = b'' pack = struct . pack while n > 0 : s = pack ( '>I' , n & 0xffffffff ) + s n = n >> 32 # Strip off leading zeros. for i in range ( len ( s ) ) : if s [ i ] != '\000' : break else : # Only happens when n == 0. s = '\000' i = 0 s = s [ i : ] # Add back some pad bytes. This could be done more efficiently # w.r.t. the de-padding being done above, but sigh... if blocksize > 0 and len ( s ) % blocksize : s = ( blocksize - len ( s ) % blocksize ) * '\000' + s return s
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
9502	def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2_pos = 0 for l in l1 : while l2_pos < len ( l2 ) and l2 [ l2_pos ] . end < l . start : l2_pos += 1 if l2_pos == len ( l2 ) : break while l2_pos < len ( l2 ) and l . intersects ( l2 [ l2_pos ] ) : out . append ( l . intersection ( l2 [ l2_pos ] ) ) l2_pos += 1 l2_pos = max ( 0 , l2_pos - 1 ) return out
7909	def __presence_available ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_available_presence ( MucPresence ( stanza ) ) return True
5504	def relative_datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created_at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created_at ) , tense )
11498	def get_community_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
7992	def _send_stream_error ( self , condition ) : if self . _output_state is "closed" : return if self . _output_state in ( None , "restart" ) : self . _send_stream_start ( ) element = StreamErrorElement ( condition ) . as_xml ( ) self . transport . send_element ( element ) self . transport . disconnect ( ) self . _output_state = "closed"
13615	def write ( ) : click . echo ( "Fantastic. Let's get started. " ) title = click . prompt ( "What's the title?" ) # Make sure that title doesn't exist. url = slugify ( title ) url = click . prompt ( "What's the URL?" , default = url ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url ) scaffold_piece ( title , url )
13158	def insert ( cls , cur , table : str , values : dict ) : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) query = cls . _insert_string . format ( table , keys , value_place_holder [ : - 1 ] ) yield from cur . execute ( query , tuple ( values . values ( ) ) ) return ( yield from cur . fetchone ( ) )
6774	def deploy ( self , site = None ) : r = self . local_renderer self . deploy_logrotate ( ) cron_crontabs = [ ] # if self.verbose: # print('hostname: "%s"' % (hostname,), file=sys.stderr) for _site , site_data in self . iter_sites ( site = site ) : r . env . cron_stdout_log = r . format ( r . env . stdout_log_template ) r . env . cron_stderr_log = r . format ( r . env . stderr_log_template ) r . sudo ( 'touch {cron_stdout_log}' ) r . sudo ( 'touch {cron_stderr_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stdout_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stderr_log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs_selected:' , self . env . crontabs_selected , file = sys . stderr ) for selected_crontab in self . env . crontabs_selected : lines = self . env . crontabs_available . get ( selected_crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron_crontabs . append ( r . format ( line ) ) if not cron_crontabs : return cron_crontabs = self . env . crontab_headers + cron_crontabs cron_crontabs . append ( '\n' ) r . env . crontabs_rendered = '\n' . join ( cron_crontabs ) fn = self . write_to_file ( content = r . env . crontabs_rendered ) print ( 'fn:' , fn ) r . env . put_remote_path = r . put ( local_path = fn ) if isinstance ( r . env . put_remote_path , ( tuple , list ) ) : r . env . put_remote_path = r . env . put_remote_path [ 0 ] r . sudo ( 'crontab -u {cron_user} {put_remote_path}' )
1914	def enqueue ( self , state ) : # save the state to secondary storage state_id = self . _workspace . save_state ( state ) self . put ( state_id ) self . _publish ( 'did_enqueue_state' , state_id , state ) return state_id
10634	def get_compound_afr ( self , compound ) : index = self . material . get_compound_index ( compound ) return stoich . amount ( compound , self . _compound_mfrs [ index ] )
7778	def __from_xml ( self , data ) : ns = get_node_ns ( data ) if ns and ns . getContent ( ) != VCARD_NS : raise ValueError ( "Not in the %r namespace" % ( VCARD_NS , ) ) if data . name != "vCard" : raise ValueError ( "Bad root element name: %r" % ( data . name , ) ) n = data . children dns = get_node_ns ( data ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and dns and ns . getContent ( ) != dns . getContent ( ) ) : n = n . next continue if not self . components . has_key ( n . name ) : n = n . next continue cl , tp = self . components [ n . name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( n . name ) : raise ValueError ( "Duplicate %s" % ( n . name , ) ) try : self . content [ n . name ] = cl ( n . name , n ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( n . name ) : self . content [ n . name ] = [ ] try : self . content [ n . name ] . append ( cl ( n . name , n ) ) except Empty : pass n = n . next
1737	def parse_exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False # we need at least one dig after exponent while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : # This is the common part of all message types payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } # Depending on the message type add the appropriate fields if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
11919	def index_row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup_url_kwarg ] ] . to_frame ( ) . T
8882	def predict_proba ( self , X ) : # Check is fit had been called check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) # Check that X have correct shape X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix )
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : # we don't want this node continue if "formula" not in sakefile [ target ] : # that means this is a meta target for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , * * data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , * * sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) # normalize all paths in output for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] # normalize all paths in dependencies for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
544	def _updateModelDBResults ( self ) : # ----------------------------------------------------------------------- # Get metrics metrics = self . _getMetrics ( ) # ----------------------------------------------------------------------- # Extract report metrics that match the requested report REs reportDict = dict ( [ ( k , metrics [ k ] ) for k in self . _reportMetricLabels ] ) # ----------------------------------------------------------------------- # Extract the report item that matches the optimize key RE # TODO cache optimizedMetricLabel sooner metrics = self . _getMetrics ( ) optimizeDict = dict ( ) if self . _optimizeKeyPattern is not None : optimizeDict [ self . _optimizedMetricLabel ] = metrics [ self . _optimizedMetricLabel ] # ----------------------------------------------------------------------- # Update model results results = json . dumps ( ( metrics , optimizeDict ) ) self . _jobsDAO . modelUpdateResults ( self . _modelID , results = results , metricValue = optimizeDict . values ( ) [ 0 ] , numRecords = ( self . _currentRecordIndex + 1 ) ) self . _logger . debug ( "Model Results: modelID=%s; numRecords=%s; results=%s" % ( self . _modelID , self . _currentRecordIndex + 1 , results ) ) return
7709	def handle_authorized_event ( self , event ) : self . server = event . authorized_jid . bare ( ) if "versioning" in self . server_features : if self . roster is not None and self . roster . version is not None : version = self . roster . version else : version = u"" else : version = None self . request_roster ( version )
8986	def _instructions_changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in_row = self . _parser . instruction_in_row ( self , instruction ) self . instructions [ index ] = in_row else : instruction . transfer_to_row ( self )
9514	def is_complete_orf ( self ) : if len ( self ) % 3 != 0 or len ( self ) < 6 : return False orfs = self . orfs ( ) complete_orf = intervals . Interval ( 0 , len ( self ) - 1 ) for orf in orfs : if orf == complete_orf : return True return False
11594	def _rc_rename ( self , src , dst ) : if src == dst : return self . rename ( src + "{" + src + "}" , src ) if not self . exists ( src ) : return self . rename ( src + "{" + src + "}" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) # Handle keys with an expire time set kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )
12001	def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
6669	def task_or_dryrun ( * args , * * kwargs ) : invoked = bool ( not args or kwargs ) task_class = kwargs . pop ( "task_class" , WrappedCallableTask ) # if invoked: # func, args = args[0], () # else: func , args = args [ 0 ] , ( ) def wrapper ( func ) : return task_class ( func , * args , * * kwargs ) wrapper . is_task_or_dryrun = True wrapper . wrapped = func return wrapper if invoked else wrapper ( func )
127	def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( "Cannot compute the polygon's area because it contains less than three points." ) poly = self . to_shapely_polygon ( ) return poly . area
3310	def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
6641	def hasDependencyRecursively ( self , name , target = None , test_dependencies = False ) : # checking dependencies recursively isn't entirely straightforward, so # use the existing method to resolve them all before checking: dependencies = self . getDependenciesRecursive ( target = target , test = test_dependencies ) return ( name in dependencies )
6903	def angle_wrap ( angle , radians = False ) : if radians : wrapped = angle % ( 2.0 * pi_value ) if wrapped < 0.0 : wrapped = 2.0 * pi_value + wrapped else : wrapped = angle % 360.0 if wrapped < 0.0 : wrapped = 360.0 + wrapped return wrapped
3574	def peripheral_didUpdateValueForDescriptor_error_ ( self , peripheral , descriptor , error ) : logger . debug ( 'peripheral_didUpdateValueForDescriptor_error called' ) # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated descriptor value. device = device_list ( ) . get ( peripheral ) if device is not None : device . _descriptor_changed ( descriptor )
12371	def logon ( self , username , password ) : if self . _token : self . logoff ( ) try : response = self . __makerequest ( 'logon' , email = username , password = password ) except FogBugzAPIError : e = sys . exc_info ( ) [ 1 ] raise FogBugzLogonError ( e ) self . _token = response . token . string if type ( self . _token ) == CData : self . _token = self . _token . encode ( 'utf-8' )
711	def _iterModels ( modelIDs ) : class ModelInfoIterator ( object ) : """ModelInfo iterator implementation class """ # Maximum number of ModelInfo elements to load into cache whenever # cache empties __CACHE_LIMIT = 1000 debug = False def __init__ ( self , modelIDs ) : """ Parameters: ---------------------------------------------------------------------- modelIDs: a sequence of Nupic model identifiers for which this iterator will return _NupicModelInfo instances. NOTE: The returned instances are NOT guaranteed to be in the same order as the IDs in modelIDs sequence. retval: nothing """ # Make our own copy in case caller changes model id list during iteration self . __modelIDs = tuple ( modelIDs ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __init__; numModelIDs=%s" % len ( self . __modelIDs ) ) self . __nextIndex = 0 self . __modelCache = collections . deque ( ) return def __iter__ ( self ) : """Iterator Protocol function Parameters: ---------------------------------------------------------------------- retval: self """ return self def next ( self ) : """Iterator Protocol function Parameters: ---------------------------------------------------------------------- retval: A _NupicModelInfo instance or raises StopIteration to signal end of iteration. """ return self . __getNext ( ) def __getNext ( self ) : """Implementation of the next() Iterator Protocol function. When the modelInfo cache becomes empty, queries Nupic and fills the cache with the next set of NupicModelInfo instances. Parameters: ---------------------------------------------------------------------- retval: A _NupicModelInfo instance or raises StopIteration to signal end of iteration. """ if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __getNext(); modelCacheLen=%s" % ( len ( self . __modelCache ) ) ) if not self . __modelCache : self . __fillCache ( ) if not self . __modelCache : raise StopIteration ( ) return self . __modelCache . popleft ( ) def __fillCache ( self ) : """Queries Nupic and fills an empty modelInfo cache with the next set of _NupicModelInfo instances Parameters: ---------------------------------------------------------------------- retval: nothing """ assert ( not self . __modelCache ) # Assemble a list of model IDs to look up numModelIDs = len ( self . __modelIDs ) if self . __modelIDs else 0 if self . __nextIndex >= numModelIDs : return idRange = self . __nextIndex + self . __CACHE_LIMIT if idRange > numModelIDs : idRange = numModelIDs lookupIDs = self . __modelIDs [ self . __nextIndex : idRange ] self . __nextIndex += ( idRange - self . __nextIndex ) # Query Nupic for model info of all models in the look-up list # NOTE: the order of results may not be the same as lookupIDs infoList = _clientJobsDB ( ) . modelsInfo ( lookupIDs ) assert len ( infoList ) == len ( lookupIDs ) , "modelsInfo returned %s elements; expected %s." % ( len ( infoList ) , len ( lookupIDs ) ) # Create _NupicModelInfo instances and add them to cache for rawInfo in infoList : modelInfo = _NupicModelInfo ( rawInfo = rawInfo ) self . __modelCache . append ( modelInfo ) assert len ( self . __modelCache ) == len ( lookupIDs ) , "Added %s elements to modelCache; expected %s." % ( len ( self . __modelCache ) , len ( lookupIDs ) ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: Leaving __fillCache(); modelCacheLen=%s" % ( len ( self . __modelCache ) , ) ) return ModelInfoIterator ( modelIDs )
8965	def whichgen ( command , path = None , verbose = 0 , exts = None ) : # pylint: disable=too-many-branches, too-many-statements matches = [ ] if path is None : using_given_path = 0 path = os . environ . get ( "PATH" , "" ) . split ( os . pathsep ) if sys . platform . startswith ( "win" ) : path . insert ( 0 , os . curdir ) # implied by Windows shell else : using_given_path = 1 # Windows has the concept of a list of extensions (PATHEXT env var). if sys . platform . startswith ( "win" ) : if exts is None : exts = os . environ . get ( "PATHEXT" , "" ) . split ( os . pathsep ) # If '.exe' is not in exts then obviously this is Win9x and # or a bogus PATHEXT, then use a reasonable default. for ext in exts : if ext . lower ( ) == ".exe" : break else : exts = [ '.COM' , '.EXE' , '.BAT' ] elif not isinstance ( exts , list ) : raise TypeError ( "'exts' argument must be a list or None" ) else : if exts is not None : raise WhichError ( "'exts' argument is not supported on platform '%s'" % sys . platform ) exts = [ ] # File name cannot have path separators because PATH lookup does not # work that way. if os . sep in command or os . altsep and os . altsep in command : pass else : for i , dir_name in enumerate ( path ) : # On windows the dir_name *could* be quoted, drop the quotes if sys . platform . startswith ( "win" ) and len ( dir_name ) >= 2 and dir_name [ 0 ] == '"' and dir_name [ - 1 ] == '"' : dir_name = dir_name [ 1 : - 1 ] for ext in [ '' ] + exts : abs_name = os . path . abspath ( os . path . normpath ( os . path . join ( dir_name , command + ext ) ) ) if os . path . isfile ( abs_name ) : if using_given_path : from_where = "from given path element %d" % i elif not sys . platform . startswith ( "win" ) : from_where = "from PATH element %d" % i elif i == 0 : from_where = "from current directory" else : from_where = "from PATH element %d" % ( i - 1 ) match = _cull ( ( abs_name , from_where ) , matches , verbose ) if match : if verbose : yield match else : yield match [ 0 ] match = _get_registered_executable ( command ) if match is not None : match = _cull ( match , matches , verbose ) if match : if verbose : yield match else : yield match [ 0 ]
2067	def reverse_dummies ( self , X , mapping ) : out_cols = X . columns . values . tolist ( ) mapped_columns = [ ] for switch in mapping : col = switch . get ( 'col' ) mod = switch . get ( 'mapping' ) insert_at = out_cols . index ( mod . columns [ 0 ] ) X . insert ( insert_at , col , 0 ) positive_indexes = mod . index [ mod . index > 0 ] for i in range ( positive_indexes . shape [ 0 ] ) : existing_col = mod . columns [ i ] val = positive_indexes [ i ] X . loc [ X [ existing_col ] == 1 , col ] = val mapped_columns . append ( existing_col ) X . drop ( mod . columns , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
10431	def selectrowpartialmatch ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) for cell in object_handle . AXRows : if re . search ( row_text , cell . AXChildren [ 0 ] . AXValue ) : if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : # Selected pass return 1 raise LdtpServerException ( u"Unable to select row: %s" % row_text )
5960	def _tcorrel ( self , nstep = 100 , * * kwargs ) : t = self . array [ 0 , : : nstep ] r = gromacs . collections . Collection ( [ numkit . timeseries . tcorrel ( t , Y , nstep = 1 , * * kwargs ) for Y in self . array [ 1 : , : : nstep ] ] ) return r
1328	def channel_axis ( self , batch ) : axis = self . __model . channel_axis ( ) if not batch : axis = axis - 1 return axis
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) # e.g., 'task_00009' ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) # e.g., 'results/task_00009/result.p.gz' return ret
8814	def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
1566	def invoke_hook_spout_fail ( self , message_id , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_fail_info = SpoutFailInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_fail ( spout_fail_info )
6046	def array_2d_from_array_1d ( self , padded_array_1d ) : padded_array_2d = self . map_to_2d_keep_padded ( padded_array_1d ) pad_size_0 = self . mask . shape [ 0 ] - self . image_shape [ 0 ] pad_size_1 = self . mask . shape [ 1 ] - self . image_shape [ 1 ] return ( padded_array_2d [ pad_size_0 // 2 : self . mask . shape [ 0 ] - pad_size_0 // 2 , pad_size_1 // 2 : self . mask . shape [ 1 ] - pad_size_1 // 2 ] )
3220	def get_client ( service , service_type = 'client' , * * conn_args ) : client_details = choose_client ( service ) user_agent = get_user_agent ( * * conn_args ) if client_details : if client_details [ 'client_type' ] == 'cloud' : client = get_gcp_client ( mod_name = client_details [ 'module_name' ] , pkg_name = conn_args . get ( 'pkg_name' , 'google.cloud' ) , key_file = conn_args . get ( 'key_file' , None ) , project = conn_args [ 'project' ] , user_agent = user_agent ) else : client = get_google_client ( mod_name = client_details [ 'module_name' ] , key_file = conn_args . get ( 'key_file' , None ) , user_agent = user_agent , api_version = conn_args . get ( 'api_version' , 'v1' ) ) else : # There is no client known for this service. We can try the standard API. try : client = get_google_client ( mod_name = service , key_file = conn_args . get ( 'key_file' , None ) , user_agent = user_agent , api_version = conn_args . get ( 'api_version' , 'v1' ) ) except Exception as e : raise e return client_details , client
11286	def flush ( self , line ) : # TODO -- maybe use echo? sys . stdout . write ( line ) sys . stdout . flush ( )
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } # GET /api/projects/0.1/jobs/ response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1700	def consume ( self , consume_function ) : from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet consume_streamlet = ConsumeStreamlet ( consume_function , self ) self . _add_child ( consume_streamlet ) return
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
5448	def make_param ( self , name , raw_uri , disk_size ) : if raw_uri . startswith ( 'https://www.googleapis.com/compute' ) : # Full Image URI should look something like: # https://www.googleapis.com/compute/v1/projects/<project>/global/images/ # But don't validate further, should the form of a valid image URI # change (v1->v2, for example) docker_path = self . _parse_image_uri ( raw_uri ) return job_model . PersistentDiskMountParam ( name , raw_uri , docker_path , disk_size , disk_type = None ) elif raw_uri . startswith ( 'file://' ) : local_path , docker_path = self . _parse_local_mount_uri ( raw_uri ) return job_model . LocalMountParam ( name , raw_uri , docker_path , local_path ) elif raw_uri . startswith ( 'gs://' ) : docker_path = self . _parse_gcs_uri ( raw_uri ) return job_model . GCSMountParam ( name , raw_uri , docker_path ) else : raise ValueError ( 'Mount parameter {} must begin with valid prefix.' . format ( raw_uri ) )
7573	def progressbar ( njobs , finished , msg = "" , spacer = " " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
69	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : image = np . copy ( image ) if copy else image for bb in self . bounding_boxes : image = bb . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = False , raise_if_out_of_image = raise_if_out_of_image , thickness = thickness ) return image
5632	def unindent ( lines ) : try : # Determine minimum indentation: indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
7036	def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , samplespec = None , limitspec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : # turn the input into a param dict coords = '%.5f %.5f %.1f' % ( center_ra , center_decl , radiusarcmin ) params = { 'coords' : coords } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done # we won't wait for the LC ZIP to complete if email_when_done = True if email_when_done : download_data = False # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # hit the server api_url = '%s/api/conesearch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) # check the status of the search status = searchresult [ 0 ] # now we'll check if we want to download the data if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
4352	def leave ( self , room ) : self . socket . rooms . remove ( self . _get_room_name ( room ) )
6125	def plot_image ( image , plot_origin = True , mask = None , extract_array_from_mask = False , zoom_around_mask = False , should_plot_border = False , positions = None , as_subplot = False , units = 'arcsec' , kpc_per_arcsec = None , figsize = ( 7 , 7 ) , aspect = 'square' , cmap = 'jet' , norm = 'linear' , norm_min = None , norm_max = None , linthresh = 0.05 , linscale = 0.01 , cb_ticksize = 10 , cb_fraction = 0.047 , cb_pad = 0.01 , cb_tick_values = None , cb_tick_labels = None , title = 'Image' , titlesize = 16 , xlabelsize = 16 , ylabelsize = 16 , xyticksize = 16 , mask_pointsize = 10 , position_pointsize = 30 , grid_pointsize = 1 , output_path = None , output_format = 'show' , output_filename = 'image' ) : origin = get_origin ( array = image , plot_origin = plot_origin ) array_plotters . plot_array ( array = image , origin = origin , mask = mask , extract_array_from_mask = extract_array_from_mask , zoom_around_mask = zoom_around_mask , should_plot_border = should_plot_border , positions = positions , as_subplot = as_subplot , units = units , kpc_per_arcsec = kpc_per_arcsec , figsize = figsize , aspect = aspect , cmap = cmap , norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale , cb_ticksize = cb_ticksize , cb_fraction = cb_fraction , cb_pad = cb_pad , cb_tick_values = cb_tick_values , cb_tick_labels = cb_tick_labels , title = title , titlesize = titlesize , xlabelsize = xlabelsize , ylabelsize = ylabelsize , xyticksize = xyticksize , mask_pointsize = mask_pointsize , position_pointsize = position_pointsize , grid_pointsize = grid_pointsize , output_path = output_path , output_format = output_format , output_filename = output_filename )
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
1068	def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist
1520	def get_remote_home ( host , cl_args ) : cmd = "echo ~" if not is_self ( host ) : cmd = ssh_remote_execute ( cmd , host , cl_args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get home path for remote host %s with output:\n%s" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
11261	def resplit ( prev , pattern , * args , * * kw ) : maxsplit = 0 if 'maxsplit' not in kw else kw . pop ( 'maxsplit' ) pattern_obj = re . compile ( pattern , * args , * * kw ) for s in prev : yield pattern_obj . split ( s , maxsplit = maxsplit )
9666	def write_dot_file ( G , filename ) : with io . open ( filename , "w" ) as fh : fh . write ( "strict digraph DependencyDiagram {\n" ) edge_list = G . edges ( ) node_list = set ( G . nodes ( ) ) if edge_list : for edge in sorted ( edge_list ) : source , targ = edge node_list = node_list - set ( source ) node_list = node_list - set ( targ ) line = '"{}" -> "{}";\n' fh . write ( line . format ( source , targ ) ) # draw nodes with no links if node_list : for node in sorted ( node_list ) : line = '"{}"\n' . format ( node ) fh . write ( line ) fh . write ( "}" )
13223	def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
11694	def full_analysis ( self ) : self . count ( ) self . verify_words ( ) self . verify_user ( ) if self . review_requested == 'yes' : self . label_suspicious ( 'Review requested' )
3794	def setup_a_alpha_and_derivatives ( self , i , T = None ) : self . a , self . m , self . Tc = self . ais [ i ] , self . ms [ i ] , self . Tcs [ i ]
6478	def _normalised_numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
3140	def get ( self , folder_id , * * queryparams ) : self . folder_id = folder_id return self . _mc_client . _get ( url = self . _build_path ( folder_id ) , * * queryparams )
811	def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : # Sanity checking if not isinstance ( statsInfo , dict ) : raise RuntimeError ( "statsInfo must be a dict -- " "found '%s' instead" % type ( statsInfo ) ) filename = resource_filename ( "nupic.datafiles" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) # Use cached stats if found AND if it has the right data if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , "rb" ) ) except : # Ok to ignore errors -- we will just re-generate the file print "Warning: unable to load stats for %s -- " "will regenerate" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print "generateStats: re-generating stats file %s because " "keys %s are not available" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print "Generating statistics for file '%s' with filters '%s'" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters # Convert collector description to collector object stats = [ ] for field in statsInfo : # field = key from statsInfo if statsInfo [ field ] == "number" : # This wants a field name e.g. consumption and the field type as the value statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == "category" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( "Unknown stats type '%s' for field '%s'" % ( statsInfo [ field ] , field ) ) # Now collect the stats if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor # Assemble the results and return r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , "wb" ) pickle . dump ( r , f ) f . close ( ) # caller may need to know name of cached file r [ "_filename" ] = statsFilename return r
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
6637	def unpublish ( self , registry = None ) : return registry_access . unpublish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , registry = registry )
4084	def get_newest_possible_languagetool_version ( ) : java_path = find_executable ( 'java' ) if not java_path : # Just ignore this and assume an old version of Java. It might not be # found because of a PATHEXT-related issue # (https://bugs.python.org/issue2200). return JAVA_6_COMPATIBLE_VERSION output = subprocess . check_output ( [ java_path , '-version' ] , stderr = subprocess . STDOUT , universal_newlines = True ) java_version = parse_java_version ( output ) if java_version >= ( 1 , 8 ) : return LATEST_VERSION elif java_version >= ( 1 , 7 ) : return JAVA_7_COMPATIBLE_VERSION elif java_version >= ( 1 , 6 ) : warn ( 'language-check would be able to use a newer version of ' 'LanguageTool if you had Java 7 or newer installed' ) return JAVA_6_COMPATIBLE_VERSION else : raise SystemExit ( 'You need at least Java 6 to use language-check' )
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
1183	def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context
9700	def worker ( wrapped , dkwargs , hash_value = None , * args , * * kwargs ) : if "event" not in dkwargs : msg = "djwebhooks.decorators.redis_hook requires an 'event' argument in the decorator." raise TypeError ( msg ) event = dkwargs [ 'event' ] if "owner" not in kwargs : msg = "djwebhooks.senders.redis_callable requires an 'owner' argument in the decorated function." raise TypeError ( msg ) owner = kwargs [ 'owner' ] if "identifier" not in kwargs : msg = "djwebhooks.senders.orm_callable requires an 'identifier' argument in the decorated function." raise TypeError ( msg ) identifier = kwargs [ 'identifier' ] senderobj = DjangoRQSenderable ( wrapped , dkwargs , hash_value , WEBHOOK_ATTEMPTS , * args , * * kwargs ) # Add the webhook object just so it's around # TODO - error handling if this can't be found senderobj . webhook_target = WebhookTarget . objects . get ( event = event , owner = owner , identifier = identifier ) # Get the target url and add it senderobj . url = senderobj . webhook_target . target_url # Get the payload. This overides the senderobj.payload property. senderobj . payload = senderobj . get_payload ( ) # Get the creator and add it to the payload. senderobj . payload [ 'owner' ] = getattr ( kwargs [ 'owner' ] , WEBHOOK_OWNER_FIELD ) # get the event and add it to the payload senderobj . payload [ 'event' ] = dkwargs [ 'event' ] return senderobj . send ( )
7112	def serve ( self , port = 62000 ) : from http . server import HTTPServer , CGIHTTPRequestHandler os . chdir ( self . log_folder ) httpd = HTTPServer ( ( '' , port ) , CGIHTTPRequestHandler ) print ( "Starting LanguageBoard on port: " + str ( httpd . server_port ) ) webbrowser . open ( 'http://0.0.0.0:{}' . format ( port ) ) httpd . serve_forever ( )
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
5621	def tile_to_zoom_level ( tile , dst_pyramid = None , matching_method = "gdal" , precision = 8 ) : def width_height ( bounds ) : try : l , b , r , t = reproject_geometry ( box ( * bounds ) , src_crs = tile . crs , dst_crs = dst_pyramid . crs ) . bounds except ValueError : raise TopologicalError ( "bounds cannot be translated into target CRS" ) return r - l , t - b if tile . tp . crs == dst_pyramid . crs : return tile . zoom else : if matching_method == "gdal" : # use rasterio/GDAL method to calculate default warp target properties transform , width , height = calculate_default_transform ( tile . tp . crs , dst_pyramid . crs , tile . width , tile . height , * tile . bounds ) # this is the resolution the tile would have in destination TilePyramid CRS tile_resolution = round ( transform [ 0 ] , precision ) elif matching_method == "min" : # calculate the minimum pixel size from the four tile corner pixels l , b , r , t = tile . bounds x = tile . pixel_x_size y = tile . pixel_y_size res = [ ] for bounds in [ ( l , t - y , l + x , t ) , # left top ( l , b , l + x , b + y ) , # left bottom ( r - x , b , r , b + y ) , # right bottom ( r - x , t - y , r , t ) # right top ] : try : w , h = width_height ( bounds ) res . extend ( [ w , h ] ) except TopologicalError : logger . debug ( "pixel outside of destination pyramid" ) if res : tile_resolution = round ( min ( res ) , precision ) else : raise TopologicalError ( "tile outside of destination pyramid" ) else : raise ValueError ( "invalid method given: %s" , matching_method ) logger . debug ( "we are looking for a zoom level interpolating to %s resolution" , tile_resolution ) zoom = 0 while True : td_resolution = round ( dst_pyramid . pixel_x_size ( zoom ) , precision ) if td_resolution <= tile_resolution : break zoom += 1 logger . debug ( "target zoom for %s: %s (%s)" , tile_resolution , zoom , td_resolution ) return zoom
12807	def incoming ( self , messages ) : if self . _observers : campfire = self . _room . get_campfire ( ) for message in messages : for observer in self . _observers : observer ( Message ( campfire , message ) )
500	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
3941	async def _fetch_channel_sid ( self ) : logger . info ( 'Requesting new gsessionid and SID...' ) # Set SID and gsessionid to None so they aren't sent in by send_maps. self . _sid_param = None self . _gsessionid_param = None res = await self . send_maps ( [ ] ) self . _sid_param , self . _gsessionid_param = _parse_sid_response ( res . body ) logger . info ( 'New SID: {}' . format ( self . _sid_param ) ) logger . info ( 'New gsessionid: {}' . format ( self . _gsessionid_param ) )
12748	def load_skel ( self , source , * * kwargs ) : logging . info ( '%s: parsing skeleton configuration' , source ) if hasattr ( source , 'read' ) : p = parser . parse ( source , self . world , self . jointgroup , * * kwargs ) else : with open ( source ) as handle : p = parser . parse ( handle , self . world , self . jointgroup , * * kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
8987	def last_produced_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . produces_meshes ( ) : return instruction . last_produced_mesh raise IndexError ( "{} produces no meshes" . format ( self ) )
2660	def _start_local_queue_process ( self ) : comm_q = Queue ( maxsize = 10 ) self . queue_proc = Process ( target = interchange . starter , args = ( comm_q , ) , kwargs = { "client_ports" : ( self . outgoing_q . port , self . incoming_q . port , self . command_client . port ) , "worker_ports" : self . worker_ports , "worker_port_range" : self . worker_port_range , "logdir" : "{}/{}" . format ( self . run_dir , self . label ) , "suppress_failure" : self . suppress_failure , "heartbeat_threshold" : self . heartbeat_threshold , "poll_period" : self . poll_period , "logging_level" : logging . DEBUG if self . worker_debug else logging . INFO } , ) self . queue_proc . start ( ) try : ( worker_task_port , worker_result_port ) = comm_q . get ( block = True , timeout = 120 ) except queue . Empty : logger . error ( "Interchange has not completed initialization in 120s. Aborting" ) raise Exception ( "Interchange failed to start" ) self . worker_task_url = "tcp://{}:{}" . format ( self . address , worker_task_port ) self . worker_result_url = "tcp://{}:{}" . format ( self . address , worker_result_port )
2339	def weighted_mean_and_std ( values , weights ) : average = np . average ( values , weights = weights , axis = 0 ) variance = np . dot ( weights , ( values - average ) ** 2 ) / weights . sum ( ) # Fast and numerically precise return ( average , np . sqrt ( variance ) )
4850	def _transmit_create ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . create_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _create_transmissions ( chunk )
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
7212	def layers ( self ) : layers = [ self . _layer_def ( style ) for style in self . styles ] return layers
7392	def adjust_angles ( self , start_node , start_angle , end_node , end_angle ) : start_group = self . find_node_group_membership ( start_node ) end_group = self . find_node_group_membership ( end_node ) if start_group == 0 and end_group == len ( self . nodes . keys ( ) ) - 1 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) elif start_group == len ( self . nodes . keys ( ) ) - 1 and end_group == 0 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) elif start_group < end_group : if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) elif end_group < start_group : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) return start_angle , end_angle
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : # TODO : speed up with affine transform new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
6160	def eye_plot ( x , L , S = 0 ) : plt . figure ( figsize = ( 6 , 4 ) ) idx = np . arange ( 0 , L + 1 ) plt . plot ( idx , x [ S : S + L + 1 ] , 'b' ) k_max = int ( ( len ( x ) - S ) / L ) - 1 for k in range ( 1 , k_max ) : plt . plot ( idx , x [ S + k * L : S + L + 1 + k * L ] , 'b' ) plt . grid ( ) plt . xlabel ( 'Time Index - n' ) plt . ylabel ( 'Amplitude' ) plt . title ( 'Eye Plot' ) return 0
941	def _reportCommandLineUsageErrorAndExit ( parser , message ) : print parser . get_usage ( ) print message sys . exit ( 1 )
2618	def write_state_file ( self ) : fh = open ( 'awsproviderstate.json' , 'w' ) state = { } state [ 'vpcID' ] = self . vpc_id state [ 'sgID' ] = self . sg_id state [ 'snIDs' ] = self . sn_ids state [ 'instances' ] = self . instances state [ "instanceState" ] = self . instance_states fh . write ( json . dumps ( state , indent = 4 ) )
9397	def run ( self ) : print ( 'Oct2Py speed test' ) print ( '*' * 20 ) time . sleep ( 1 ) print ( 'Raw speed: ' ) avg = timeit . timeit ( self . raw_speed , number = 10 ) / 10 print ( ' {0:0.01f} usec per loop' . format ( avg * 1e6 ) ) sides = [ 1 , 10 , 100 , 1000 ] runs = [ 10 , 10 , 10 , 5 ] for ( side , nruns ) in zip ( sides , runs ) : self . array = np . reshape ( np . arange ( side ** 2 ) , ( - 1 ) ) print ( 'Put {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_put , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) print ( 'Get {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_get , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) self . octave . exit ( ) print ( '*' * 20 ) print ( 'Test complete!' )
6330	def tf ( self , term ) : if ' ' in term : raise ValueError ( 'tf can only calculate the term frequency of individual words' ) tcount = self . get_count ( term ) if tcount == 0 : return 0.0 return 1 + log10 ( tcount )
675	def _getMetrics ( self ) : metric = None if self . metrics is not None : metric = self . metrics ( self . _currentRecordIndex + 1 ) elif self . metricValue is not None : metric = self . metricValue else : raise RuntimeError ( 'No metrics or metric value specified for dummy model' ) return { self . _optimizeKeyPattern : metric }
9758	def restart ( ctx , copy , file , u ) : # pylint:disable=redefined-builtin config = None update_code = None if file : config = rhea . read ( file ) # Check if we need to upload if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : if copy : response = PolyaxonClient ( ) . experiment . copy ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was copied with id {}' . format ( response . id ) ) else : response = PolyaxonClient ( ) . experiment . restart ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was restarted with id {}' . format ( response . id ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
12059	def TK_message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )
12310	def pull_stream ( self , uri , * * kwargs ) : return self . protocol . execute ( 'pullStream' , uri = uri , * * kwargs )
12821	def _str_to_path ( s , result_type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result_type is text_type : return s . decode ( 'ascii' ) elif isinstance ( s , text_type ) and result_type is bytes : return s . encode ( 'ascii' ) return s
6504	def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
1741	def add_inputs ( self , xs ) : states = [ ] cur = self for x in xs : cur = cur . add_input ( x ) states . append ( cur ) return states
4463	def jam_pack ( jam , * * kwargs ) : if not hasattr ( jam . sandbox , 'muda' ) : # If there's no mudabox, create one jam . sandbox . muda = jams . Sandbox ( history = [ ] , state = [ ] , version = dict ( muda = version , librosa = librosa . __version__ , jams = jams . __version__ , pysoundfile = psf . __version__ ) ) elif not isinstance ( jam . sandbox . muda , jams . Sandbox ) : # If there is a muda entry, but it's not a sandbox, coerce it jam . sandbox . muda = jams . Sandbox ( * * jam . sandbox . muda ) jam . sandbox . muda . update ( * * kwargs ) return jam
13328	def add ( path ) : click . echo ( '\nAdding {} to cache......' . format ( path ) , nl = False ) try : r = cpenv . resolve ( path ) except Exception as e : click . echo ( bold_red ( 'FAILED' ) ) click . echo ( e ) return if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . add ( r . resolved [ 0 ] ) EnvironmentCache . save ( ) click . echo ( bold_green ( 'OK!' ) )
11760	def retract ( self , sentence ) : for c in conjuncts ( to_cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )
11687	def changeset_info ( changeset ) : keys = [ tag . attrib . get ( 'k' ) for tag in changeset . getchildren ( ) ] keys += [ 'id' , 'user' , 'uid' , 'bbox' , 'created_at' ] values = [ tag . attrib . get ( 'v' ) for tag in changeset . getchildren ( ) ] values += [ changeset . get ( 'id' ) , changeset . get ( 'user' ) , changeset . get ( 'uid' ) , get_bounds ( changeset ) , changeset . get ( 'created_at' ) ] return dict ( zip ( keys , values ) )
11648	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) self . train_ = X memory = get_memory ( self . memory ) lo , = memory . cache ( scipy . linalg . eigvalsh ) ( X , eigvals = ( 0 , 0 ) ) self . shift_ = max ( self . min_eig - lo , 0 ) return self
7029	def generalized_lsp_value_notau ( times , mags , errs , omega ) : one_over_errs2 = 1.0 / ( errs * errs ) W = npsum ( one_over_errs2 ) wi = one_over_errs2 / W sin_omegat = npsin ( omega * times ) cos_omegat = npcos ( omega * times ) sin2_omegat = sin_omegat * sin_omegat cos2_omegat = cos_omegat * cos_omegat sincos_omegat = sin_omegat * cos_omegat # calculate some more sums and terms Y = npsum ( wi * mags ) C = npsum ( wi * cos_omegat ) S = npsum ( wi * sin_omegat ) YpY = npsum ( wi * mags * mags ) YpC = npsum ( wi * mags * cos_omegat ) YpS = npsum ( wi * mags * sin_omegat ) CpC = npsum ( wi * cos2_omegat ) # SpS = npsum( wi*sin2_omegat ) CpS = npsum ( wi * sincos_omegat ) # the final terms YY = YpY - Y * Y YC = YpC - Y * C YS = YpS - Y * S CC = CpC - C * C SS = 1 - CpC - S * S # use SpS = 1 - CpC CS = CpS - C * S # P(omega) = (SS*YC*YC + CC*YS*YS - 2.0*CS*YC*YS)/(YY*D) # D(omega) = CC*SS - CS*CS Domega = CC * SS - CS * CS lspval = ( SS * YC * YC + CC * YS * YS - 2.0 * CS * YC * YS ) / ( YY * Domega ) return lspval
8692	def init ( self ) : # ignore 400 (IndexAlreadyExistsException) when creating an index self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
8617	def _underscore_to_camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return "" . join ( next ( c ) ( x ) if x else '_' for x in value . split ( "_" ) )
13477	def NonUniformImage ( x , y , z , ax = None , fig = None , cmap = None , alpha = None , scalex = True , scaley = True , add_cbar = True , * * kwargs ) : if ax is None and fig is None : fig , ax = _setup_axes ( ) elif ax is None : ax = fig . gca ( ) elif fig is None : fig = ax . get_figure ( ) norm = kwargs . get ( 'norm' , None ) im = _mplim . NonUniformImage ( ax , * * kwargs ) vmin = kwargs . pop ( 'vmin' , _np . min ( z ) ) vmax = kwargs . pop ( 'vmax' , _np . max ( z ) ) # im.set_clim(vmin=vmin, vmax=vmax) if cmap is not None : im . set_cmap ( cmap ) m = _cm . ScalarMappable ( cmap = im . get_cmap ( ) , norm = norm ) m . set_array ( z ) if add_cbar : cax , cb = _cb ( ax = ax , im = m , fig = fig ) if alpha is not None : im . set_alpha ( alpha ) im . set_data ( x , y , z ) ax . images . append ( im ) if scalex : xmin = min ( x ) xmax = max ( x ) ax . set_xlim ( xmin , xmax ) if scaley : ymin = min ( y ) ymax = max ( y ) ax . set_ylim ( ymin , ymax ) return _SI ( im = im , cb = cb , cax = cax )
12019	def find_imports ( self , pbds ) : # List of types used, but not defined imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) # Clumpsy, but enought for now for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import_file in self . imports : self . lines . insert ( 2 , 'import "{}";' . format ( import_file ) )
3861	def add_event ( self , event_ ) : conv_event = self . _wrap_event ( event_ ) if conv_event . id_ not in self . _events_dict : self . _events . append ( conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : # If this happens, there's probably a bug. logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return None return conv_event
2172	def new_state ( self ) : try : self . _state = self . state ( ) log . debug ( "Generated new state %s." , self . _state ) except TypeError : self . _state = self . state log . debug ( "Re-using previously supplied state %s." , self . _state ) return self . _state
237	def compute_volume_exposures ( shares_held , volumes , percentile ) : shares_held = shares_held . replace ( 0 , np . nan ) shares_longed = shares_held [ shares_held > 0 ] shares_shorted = - 1 * shares_held [ shares_held < 0 ] shares_grossed = shares_held . abs ( ) longed_frac = shares_longed . divide ( volumes ) shorted_frac = shares_shorted . divide ( volumes ) grossed_frac = shares_grossed . divide ( volumes ) # NOTE: To work around a bug in `quantile` with nan-handling in # pandas 0.18, use np.nanpercentile by applying to each row of # the dataframe. This is fixed in pandas 0.19. # # longed_threshold = 100*longed_frac.quantile(percentile, axis='columns') # shorted_threshold = 100*shorted_frac.quantile(percentile, axis='columns') # grossed_threshold = 100*grossed_frac.quantile(percentile, axis='columns') longed_threshold = 100 * longed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) shorted_threshold = 100 * shorted_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) grossed_threshold = 100 * grossed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) return longed_threshold , shorted_threshold , grossed_threshold
12955	def _add_id_to_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_key_for_index ( indexedField , val ) , pk )
3323	def set_last_modified ( self , dest_path , time_stamp , dry_run ) : # Translate time from RFC 1123 to seconds since epoch format secs = util . parse_time_string ( time_stamp ) if not dry_run : os . utime ( self . _file_path , ( secs , secs ) ) return True
6171	def freq_resp ( self , mode = 'dB' , fs = 8000 , ylim = [ - 100 , 2 ] ) : iir_d . freqz_resp_cas_list ( [ self . sos ] , mode , fs = fs ) pylab . grid ( ) pylab . ylim ( ylim )
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid # pprint(logList) return logList
7134	def run_cell ( self , cell ) : globals = self . ipy_shell . user_global_ns locals = self . ipy_shell . user_ns globals . update ( { "__ipy_scope__" : None , } ) try : with redirect_stdout ( self . stdout ) : self . run ( cell , globals , locals ) except : self . code_error = True if self . options . debug : raise BdbQuit finally : self . finalize ( )
8893	def calculate_uuid ( self ) : # raise an error if no inputs to the UUID calculation were specified if self . uuid_input_fields is None : raise NotImplementedError ( """You must define either a 'uuid_input_fields' attribute (with a tuple of field names) or override the 'calculate_uuid' method, on models that inherit from UUIDModelMixin. If you want a fully random UUID, you can set 'uuid_input_fields' to the string 'RANDOM'.""" ) # if the UUID has been set to be random, return a random UUID if self . uuid_input_fields == "RANDOM" : return uuid . uuid4 ( ) . hex # if we got this far, uuid_input_fields should be a tuple assert isinstance ( self . uuid_input_fields , tuple ) , "'uuid_input_fields' must either be a tuple or the string 'RANDOM'" # calculate the input to the UUID function hashable_input_vals = [ ] for field in self . uuid_input_fields : new_value = getattr ( self , field ) if new_value : hashable_input_vals . append ( str ( new_value ) ) hashable_input = ":" . join ( hashable_input_vals ) # if all the values were falsey, just return a random UUID, to avoid collisions if not hashable_input : return uuid . uuid4 ( ) . hex # compute the UUID as a function of the input values return sha2_uuid ( hashable_input )
8871	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : column = l . find ( self . literal ) if column != - 1 : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}, col {}' . format ( str ( truePosition + 1 ) , column ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) return truePosition # No Match found self . failed = True raise DirectiveException ( self )
10926	def do_run_1 ( self ) : while not self . check_terminate ( ) : self . _has_run = True self . _run1 ( ) self . _num_iter += 1 self . _inner_run_counter += 1
10092	def _parse_response ( self , response ) : if not self . _raise_errors : return response is_4xx_error = str ( response . status_code ) [ 0 ] == '4' is_5xx_error = str ( response . status_code ) [ 0 ] == '5' content = response . content if response . status_code == 403 : raise AuthenticationError ( content ) elif is_4xx_error : raise APIError ( content ) elif is_5xx_error : raise ServerError ( content ) return response
3883	def from_entity ( entity , self_user_id ) : user_id = UserID ( chat_id = entity . id . chat_id , gaia_id = entity . id . gaia_id ) return User ( user_id , entity . properties . display_name , entity . properties . first_name , entity . properties . photo_url , entity . properties . email , ( self_user_id == user_id ) or ( self_user_id is None ) )
2580	def checkpoint ( self , tasks = None ) : with self . checkpoint_lock : checkpoint_queue = None if tasks : checkpoint_queue = tasks else : checkpoint_queue = self . tasks checkpoint_dir = '{0}/checkpoint' . format ( self . run_dir ) checkpoint_dfk = checkpoint_dir + '/dfk.pkl' checkpoint_tasks = checkpoint_dir + '/tasks.pkl' if not os . path . exists ( checkpoint_dir ) : try : os . makedirs ( checkpoint_dir ) except FileExistsError : pass with open ( checkpoint_dfk , 'wb' ) as f : state = { 'rundir' : self . run_dir , 'task_count' : self . task_count } pickle . dump ( state , f ) count = 0 with open ( checkpoint_tasks , 'ab' ) as f : for task_id in checkpoint_queue : if not self . tasks [ task_id ] [ 'checkpoint' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None : hashsum = self . tasks [ task_id ] [ 'hashsum' ] if not hashsum : continue t = { 'hash' : hashsum , 'exception' : None , 'result' : None } try : # Asking for the result will raise an exception if # the app had failed. Should we even checkpoint these? # TODO : Resolve this question ? r = self . memoizer . hash_lookup ( hashsum ) . result ( ) except Exception as e : t [ 'exception' ] = e else : t [ 'result' ] = r # We are using pickle here since pickle dumps to a file in 'ab' # mode behave like a incremental log. pickle . dump ( t , f ) count += 1 self . tasks [ task_id ] [ 'checkpoint' ] = True logger . debug ( "Task {} checkpointed" . format ( task_id ) ) self . checkpointed_tasks += count if count == 0 : if self . checkpointed_tasks == 0 : logger . warn ( "No tasks checkpointed so far in this run. Please ensure caching is enabled" ) else : logger . debug ( "No tasks checkpointed in this pass." ) else : logger . info ( "Done checkpointing {} tasks" . format ( count ) ) return checkpoint_dir
13607	def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX_BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e
285	def plot_drawdown_underwater ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , * * kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
10899	def update ( self , value = 0 ) : self . _deltas . append ( time . time ( ) ) self . value = value self . _percent = 100.0 * self . value / self . num if self . bar : self . _bars = self . _bar_symbol * int ( np . round ( self . _percent / 100. * self . _barsize ) ) if ( len ( self . _deltas ) < 2 ) or ( self . _deltas [ - 1 ] - self . _deltas [ - 2 ] ) > 1e-1 : self . _estimate_time ( ) self . _draw ( ) if self . value == self . num : self . end ( )
651	def sameTMParams ( tp1 , tp2 ) : result = True for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , "minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , "permanenceMax" , "globalDecay" , "activationThreshold" , "doPooling" , "segUpdateValidDuration" , "burnIn" , "pamLength" , "maxAge" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
11518	def generate_upload_token ( self , token , item_id , filename , checksum = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemid' ] = item_id parameters [ 'filename' ] = filename if checksum is not None : parameters [ 'checksum' ] = checksum response = self . request ( 'midas.upload.generatetoken' , parameters ) return response [ 'token' ]
11155	def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
192	def SimplexNoiseAlpha ( first = None , second = None , per_channel = False , size_px_max = ( 2 , 16 ) , upscale_method = None , iterations = ( 1 , 3 ) , aggregation_method = "max" , sigmoid = True , sigmoid_thresh = None , name = None , deterministic = False , random_state = None ) : upscale_method_default = iap . Choice ( [ "nearest" , "linear" , "cubic" ] , p = [ 0.05 , 0.6 , 0.35 ] ) sigmoid_thresh_default = iap . Normal ( 0.0 , 5.0 ) noise = iap . SimplexNoise ( size_px_max = size_px_max , upscale_method = upscale_method if upscale_method is not None else upscale_method_default ) if iterations != 1 : noise = iap . IterativeNoiseAggregator ( noise , iterations = iterations , aggregation_method = aggregation_method ) if sigmoid is False or ( ia . is_single_number ( sigmoid ) and sigmoid <= 0.01 ) : noise = iap . Sigmoid . create_for_noise ( noise , threshold = sigmoid_thresh if sigmoid_thresh is not None else sigmoid_thresh_default , activated = sigmoid ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return AlphaElementwise ( factor = noise , first = first , second = second , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
550	def __checkCancelation ( self ) : # Update a hadoop job counter at least once every 600 seconds so it doesn't # think our map task is dead print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" # See if the job got cancelled jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
8772	def _remove_default_tz_bindings ( self , context , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_remove_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_remove_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . remove ( context , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
6951	def jhk_to_sdssz ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSZ_JHK , SDSSZ_JH , SDSSZ_JK , SDSSZ_HK , SDSSZ_J , SDSSZ_H , SDSSZ_K )
748	def _anomalyCompute ( self ) : inferenceType = self . getInferenceType ( ) inferences = { } sp = self . _getSPRegion ( ) score = None if inferenceType == InferenceType . NontemporalAnomaly : score = sp . getOutputData ( "anomalyScore" ) [ 0 ] #TODO move from SP to Anomaly ? elif inferenceType == InferenceType . TemporalAnomaly : tm = self . _getTPRegion ( ) if sp is not None : activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] else : sensor = self . _getSensorRegion ( ) activeColumns = sensor . getOutputData ( 'dataOut' ) . nonzero ( ) [ 0 ] if not self . _predictedFieldName in self . _input : raise ValueError ( "Expected predicted field '%s' in input row, but was not found!" % self . _predictedFieldName ) # Calculate the anomaly score using the active columns # and previous predicted columns. score = tm . getOutputData ( "anomalyScore" ) [ 0 ] # Calculate the classifier's output and use the result as the anomaly # label. Stores as string of results. # TODO: make labels work with non-SP models if sp is not None : self . _getAnomalyClassifier ( ) . setParameter ( "activeColumnCount" , len ( activeColumns ) ) self . _getAnomalyClassifier ( ) . prepareInputs ( ) self . _getAnomalyClassifier ( ) . compute ( ) labels = self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabelResults ( ) inferences [ InferenceElement . anomalyLabel ] = "%s" % labels inferences [ InferenceElement . anomalyScore ] = score return inferences
10272	def get_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT for node in graph : if is_unweighted_source ( graph , node , key ) : yield node
3327	def refresh ( self , token , timeout = None ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT return self . storage . refresh ( token , timeout )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : # guard conditions assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
7844	def set_type ( self , item_type ) : if not item_type : raise ValueError ( "Type is required in DiscoIdentity" ) item_type = unicode ( item_type ) self . xmlnode . setProp ( "type" , item_type . encode ( "utf-8" ) )
3550	def list_characteristics ( self ) : paths = self . _props . Get ( _SERVICE_INTERFACE , 'Characteristics' ) return map ( BluezGattCharacteristic , get_provider ( ) . _get_objects_by_path ( paths ) )
7751	def process_message ( self , stanza ) : stanza_type = stanza . stanza_type if stanza_type is None : stanza_type = "normal" if self . __try_handlers ( self . _message_handlers , stanza , stanza_type = stanza_type ) : return True if stanza_type not in ( "error" , "normal" ) : # try 'normal' handler additionaly to the regular handler return self . __try_handlers ( self . _message_handlers , stanza , stanza_type = "normal" ) return False
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
2254	def argunique ( items , key = None ) : # yield from unique(range(len(items)), key=lambda i: items[i]) if key is None : return unique ( range ( len ( items ) ) , key = lambda i : items [ i ] ) else : return unique ( range ( len ( items ) ) , key = lambda i : key ( items [ i ] ) )
577	def rUpdate ( original , updates ) : # Keep a list of the sub-dictionaries that need to be updated to avoid having # to use recursion (which could fail for dictionaries with a lot of nesting. dictPairs = [ ( original , updates ) ] while len ( dictPairs ) > 0 : original , updates = dictPairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dictPairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v
12016	def model_uncert ( self ) : Y = self . photometry_array . T Y /= np . median ( Y , axis = 1 ) [ : , None ] C = np . median ( Y , axis = 0 ) nstars , nobs = np . shape ( Y ) Z = np . empty ( ( nstars , 4 ) ) qs = self . qs . astype ( int ) for s in range ( 4 ) : Z [ : , s ] = np . median ( ( Y / C ) [ : , qs == s ] , axis = 1 ) resid2 = ( Y - Z [ : , qs ] * C ) ** 2 z = Z [ : , qs ] trend = z * C [ None , : ] lnS = np . log ( np . nanmedian ( resid2 , axis = 0 ) ) jitter = np . log ( 0.1 * np . nanmedian ( np . abs ( np . diff ( Y , axis = 1 ) ) ) ) cal_ferr = np . sqrt ( np . exp ( 2 * ( jitter / trend ) ) + z ** 2 * np . exp ( lnS ) [ None , : ] ) self . modeled_uncert = cal_ferr self . target_uncert = cal_ferr [ 0 ]
3120	def value_to_string ( self , obj ) : value = self . _get_val_from_obj ( obj ) return self . get_prep_value ( value )
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : ## do not allow bad vals if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
8385	def write_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to write." ) return 1 filename = argv [ 0 ] resource_name = "files/" + filename tweaks_name = amend_filename ( filename , "_tweaks" ) if not pkg_resources . resource_exists ( "edx_lint" , resource_name ) : print ( u"Don't have file %r to write." % filename ) return 2 if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if not tef . validate ( ) : bak_name = amend_filename ( filename , "_backup" ) print ( u"Your copy of %s seems to have been edited, renaming it to %s" % ( filename , bak_name ) ) if os . path . exists ( bak_name ) : print ( u"A previous %s exists, deleting it" % bak_name ) os . remove ( bak_name ) os . rename ( filename , bak_name ) print ( u"Reading edx_lint/files/%s" % filename ) cfg = configparser . RawConfigParser ( ) resource_string = pkg_resources . resource_string ( "edx_lint" , resource_name ) . decode ( "utf8" ) # pkg_resources always reads binary data (in both python2 and python3). # ConfigParser.read_string only exists in python3, so we have to wrap the string # from pkg_resources in a cStringIO so that we can pass it into ConfigParser.readfp. if six . PY2 : cfg . readfp ( cStringIO ( resource_string ) , resource_name ) else : cfg . read_string ( resource_string , resource_name ) # pylint: disable=no-member if os . path . exists ( tweaks_name ) : print ( u"Applying local tweaks from %s" % tweaks_name ) cfg_tweaks = configparser . RawConfigParser ( ) cfg_tweaks . read ( [ tweaks_name ] ) merge_configs ( cfg , cfg_tweaks ) print ( u"Writing %s" % filename ) output_text = cStringIO ( ) output_text . write ( WARNING_HEADER . format ( filename = filename , tweaks_name = tweaks_name ) ) cfg . write ( output_text ) out_tef = TamperEvidentFile ( filename ) if six . PY2 : output_bytes = output_text . getvalue ( ) else : output_bytes = output_text . getvalue ( ) . encode ( "utf8" ) out_tef . write ( output_bytes ) return 0
1408	def _is_continue_to_work ( self ) : if not self . _is_topology_running ( ) : return False max_spout_pending = self . pplan_helper . context . get_cluster_config ( ) . get ( api_constants . TOPOLOGY_MAX_SPOUT_PENDING ) if not self . acking_enabled and self . output_helper . is_out_queue_available ( ) : return True elif self . acking_enabled and self . output_helper . is_out_queue_available ( ) and len ( self . in_flight_tuples ) < max_spout_pending : return True elif self . acking_enabled and not self . in_stream . is_empty ( ) : return True else : return False
1902	def summarized_name ( self , name ) : components = name . split ( '.' ) prefix = '.' . join ( c [ 0 ] for c in components [ : - 1 ] ) return f'{prefix}.{components[-1]}'
4719	def tsuite_setup ( trun , declr , enum ) : suite = copy . deepcopy ( TESTSUITE ) # Setup the test-suite suite [ "name" ] = declr . get ( "name" ) if suite [ "name" ] is None : cij . err ( "rnr:tsuite_setup: no testsuite is given" ) return None suite [ "alias" ] = declr . get ( "alias" ) suite [ "ident" ] = "%s_%d" % ( suite [ "name" ] , enum ) suite [ "res_root" ] = os . sep . join ( [ trun [ "conf" ] [ "OUTPUT" ] , suite [ "ident" ] ] ) suite [ "aux_root" ] = os . sep . join ( [ suite [ "res_root" ] , "_aux" ] ) suite [ "evars" ] . update ( copy . deepcopy ( trun [ "evars" ] ) ) suite [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) # Initialize os . makedirs ( suite [ "res_root" ] ) os . makedirs ( suite [ "aux_root" ] ) # Setup testsuite-hooks suite [ "hooks" ] = hooks_setup ( trun , suite , declr . get ( "hooks" ) ) # Forward from declaration suite [ "hooks_pr_tcase" ] = declr . get ( "hooks_pr_tcase" , [ ] ) suite [ "fname" ] = "%s.suite" % suite [ "name" ] suite [ "fpath" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTSUITES" ] , suite [ "fname" ] ] ) # # Load testcases from .suite file OR from declaration # tcase_fpaths = [ ] # Load testcase fpaths if os . path . exists ( suite [ "fpath" ] ) : # From suite-file suite_lines = ( l . strip ( ) for l in open ( suite [ "fpath" ] ) . read ( ) . splitlines ( ) ) tcase_fpaths . extend ( ( l for l in suite_lines if len ( l ) > 1 and l [ 0 ] != "#" ) ) else : # From declaration tcase_fpaths . extend ( declr . get ( "testcases" , [ ] ) ) # NOTE: fix duplicates; allow them # NOTE: Currently hot-fixed here if len ( set ( tcase_fpaths ) ) != len ( tcase_fpaths ) : cij . err ( "rnr:suite: failed: duplicate tcase in suite not supported" ) return None for tcase_fname in tcase_fpaths : # Setup testcases tcase = tcase_setup ( trun , suite , tcase_fname ) if not tcase : cij . err ( "rnr:suite: failed: tcase_setup" ) return None suite [ "testcases" ] . append ( tcase ) return suite
10526	def get_google_playlist_songs ( self , playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : logger . info ( "Loading Google Music playlist songs..." ) google_playlist = self . get_google_playlist ( playlist ) if not google_playlist : return [ ] , [ ] playlist_song_ids = [ track [ 'trackId' ] for track in google_playlist [ 'tracks' ] ] playlist_songs = [ song for song in self . api . get_all_songs ( ) if song [ 'id' ] in playlist_song_ids ] matched_songs , filtered_songs = filter_google_songs ( playlist_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Filtered {0} Google playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} Google playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
7282	def translate_value ( document_field , form_value ) : value = form_value if isinstance ( document_field , ReferenceField ) : value = document_field . document_type . objects . get ( id = form_value ) if form_value else None return value
2395	def quadratic_weighted_kappa ( rater_a , rater_b , min_rating = None , max_rating = None ) : assert ( len ( rater_a ) == len ( rater_b ) ) rater_a = [ int ( a ) for a in rater_a ] rater_b = [ int ( b ) for b in rater_b ] if min_rating is None : min_rating = min ( rater_a + rater_b ) if max_rating is None : max_rating = max ( rater_a + rater_b ) conf_mat = confusion_matrix ( rater_a , rater_b , min_rating , max_rating ) num_ratings = len ( conf_mat ) num_scored_items = float ( len ( rater_a ) ) hist_rater_a = histogram ( rater_a , min_rating , max_rating ) hist_rater_b = histogram ( rater_b , min_rating , max_rating ) numerator = 0.0 denominator = 0.0 if ( num_ratings > 1 ) : for i in range ( num_ratings ) : for j in range ( num_ratings ) : expected_count = ( hist_rater_a [ i ] * hist_rater_b [ j ] / num_scored_items ) d = pow ( i - j , 2.0 ) / pow ( num_ratings - 1 , 2.0 ) numerator += d * conf_mat [ i ] [ j ] / num_scored_items denominator += d * expected_count / num_scored_items return 1.0 - numerator / denominator else : return 1.0
9882	def alpha ( reliability_data = None , value_counts = None , value_domain = None , level_of_measurement = 'interval' , dtype = np . float64 ) : if ( reliability_data is None ) == ( value_counts is None ) : raise ValueError ( "Either reliability_data or value_counts must be provided, but not both." ) # Don't know if it's a list or numpy array. If it's the latter, the truth value is ambiguous. So, ask for None. if value_counts is None : if type ( reliability_data ) is not np . ndarray : reliability_data = np . array ( reliability_data ) value_domain = value_domain or np . unique ( reliability_data [ ~ np . isnan ( reliability_data ) ] ) value_counts = _reliability_data_to_value_counts ( reliability_data , value_domain ) else : # elif reliability_data is None if value_domain : assert value_counts . shape [ 1 ] == len ( value_domain ) , "The value domain should be equal to the number of columns of value_counts." else : value_domain = tuple ( range ( value_counts . shape [ 1 ] ) ) distance_metric = _distance_metric ( level_of_measurement ) o = _coincidences ( value_counts , value_domain , dtype = dtype ) n_v = np . sum ( o , axis = 0 ) n = np . sum ( n_v ) e = _random_coincidences ( value_domain , n , n_v ) d = _distances ( value_domain , distance_metric , n_v ) return 1 - np . sum ( o * d ) / np . sum ( e * d )
8561	def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) ) return response
1114	def _collect_lines ( self , diffs ) : fromlist , tolist , flaglist = [ ] , [ ] , [ ] # pull from/to data and flags from mdiff style iterator for fromdata , todata , flag in diffs : try : # store HTML markup of the lines into the lists fromlist . append ( self . _format_line ( 0 , flag , * fromdata ) ) tolist . append ( self . _format_line ( 1 , flag , * todata ) ) except TypeError : # exceptions occur for lines where context separators go fromlist . append ( None ) tolist . append ( None ) flaglist . append ( flag ) return fromlist , tolist , flaglist
13215	def dump ( self , name , filename ) : if not self . exists ( name ) : raise DatabaseError ( 'database %s does not exist!' ) log . info ( 'dumping %s to %s' % ( name , filename ) ) self . _run_cmd ( 'pg_dump' , '--verbose' , '--blobs' , '--format=custom' , '--file=%s' % filename , name )
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) # documentation in # https://cloud.google.com/storage/docs/json_api/v1/objects/list request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
7	def observation_placeholder ( ob_space , batch_size = None , name = 'Ob' ) : assert isinstance ( ob_space , Discrete ) or isinstance ( ob_space , Box ) or isinstance ( ob_space , MultiDiscrete ) , 'Can only deal with Discrete and Box observation spaces for now' dtype = ob_space . dtype if dtype == np . int8 : dtype = np . uint8 return tf . placeholder ( shape = ( batch_size , ) + ob_space . shape , dtype = dtype , name = name )
12267	def check_grad ( f_df , xref , stepsize = 1e-6 , tol = 1e-6 , width = 15 , style = 'round' , out = sys . stdout ) : CORRECT = u'\x1b[32m\N{CHECK MARK}\x1b[0m' INCORRECT = u'\x1b[31m\N{BALLOT X}\x1b[0m' obj , grad = wrap ( f_df , xref , size = 0 ) x0 = destruct ( xref ) df = grad ( x0 ) # header out . write ( tp . header ( [ "Numerical" , "Analytic" , "Error" ] , width = width , style = style ) + "\n" ) out . flush ( ) # helper function to parse a number def parse_error ( number ) : # colors failure = "\033[91m" passing = "\033[92m" warning = "\033[93m" end = "\033[0m" base = "{}{:0.3e}{}" # correct if error < 0.1 * tol : return base . format ( passing , error , end ) # warning elif error < tol : return base . format ( warning , error , end ) # failure else : return base . format ( failure , error , end ) # check each dimension num_errors = 0 for j in range ( x0 . size ) : # take a small step in one dimension dx = np . zeros ( x0 . size ) dx [ j ] = stepsize # compute the centered difference formula df_approx = ( obj ( x0 + dx ) - obj ( x0 - dx ) ) / ( 2 * stepsize ) df_analytic = df [ j ] # absolute error abs_error = np . linalg . norm ( df_approx - df_analytic ) # relative error error = abs_error if np . allclose ( abs_error , 0 ) else abs_error / ( np . linalg . norm ( df_analytic ) + np . linalg . norm ( df_approx ) ) num_errors += error >= tol errstr = CORRECT if error < tol else INCORRECT out . write ( tp . row ( [ df_approx , df_analytic , parse_error ( error ) + ' ' + errstr ] , width = width , style = style ) + "\n" ) out . flush ( ) out . write ( tp . bottom ( 3 , width = width , style = style ) + "\n" ) return num_errors
1181	def group ( self , * args ) : if len ( args ) == 0 : args = ( 0 , ) grouplist = [ ] for group in args : grouplist . append ( self . _get_slice ( self . _get_index ( group ) , None ) ) if len ( grouplist ) == 1 : return grouplist [ 0 ] else : return tuple ( grouplist )
4415	def add_at ( self , index : int , requester : int , track : dict ) : self . queue . insert ( min ( index , len ( self . queue ) - 1 ) , AudioTrack ( ) . build ( track , requester ) )
2792	def load ( self ) : data = self . get_data ( "certificates/%s" % self . id ) certificate = data [ "certificate" ] for attr in certificate . keys ( ) : setattr ( self , attr , certificate [ attr ] ) return self
11675	def bare ( self ) : if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
3340	def make_complete_url ( environ , localUri = None ) : url = environ [ "wsgi.url_scheme" ] + "://" if environ . get ( "HTTP_HOST" ) : url += environ [ "HTTP_HOST" ] else : url += environ [ "SERVER_NAME" ] if environ [ "wsgi.url_scheme" ] == "https" : if environ [ "SERVER_PORT" ] != "443" : url += ":" + environ [ "SERVER_PORT" ] else : if environ [ "SERVER_PORT" ] != "80" : url += ":" + environ [ "SERVER_PORT" ] url += compat . quote ( environ . get ( "SCRIPT_NAME" , "" ) ) if localUri is None : url += compat . quote ( environ . get ( "PATH_INFO" , "" ) ) if environ . get ( "QUERY_STRING" ) : url += "?" + environ [ "QUERY_STRING" ] else : url += localUri # TODO: quote? return url
7895	def set_subject ( self , subject ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "groupchat" , subject = subject ) self . manager . stream . send ( m )
4575	def hsv2rgb_360 ( hsv ) : h , s , v = hsv r , g , b = colorsys . hsv_to_rgb ( h / 360.0 , s , v ) return ( int ( r * 255.0 ) , int ( g * 255.0 ) , int ( b * 255.0 ) )
9903	def is_configured ( self , project , * * kwargs ) : params = self . get_option return bool ( params ( 'server_host' , project ) and params ( 'server_port' , project ) )
4980	def get ( self , request , enterprise_uuid , course_id ) : enrollment_course_mode = request . GET . get ( 'course_mode' ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) # Redirect the learner to LMS dashboard in case no course mode is # provided as query parameter `course_mode` if not enrollment_course_mode : return redirect ( LMS_DASHBOARD_URL ) enrollment_api_client = EnrollmentApiClient ( ) course_modes = enrollment_api_client . get_course_modes ( course_id ) # Verify that the request user belongs to the enterprise against the # provided `enterprise_uuid`. enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_customer . uuid ) if not course_modes : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTHCE000' log_message = ( 'No course_modes for course_id {course_id} for enterprise_catalog_uuid ' '{enterprise_catalog_uuid}.' 'The following error was presented to ' 'user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_catalog_uuid = enterprise_catalog_uuid , course_id = course_id , error_code = error_code ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'slug' ] == enrollment_course_mode : selected_course_mode = course_mode break if not selected_course_mode : return redirect ( LMS_DASHBOARD_URL ) # Create the Enterprise backend database records for this course # enrollment __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) DataSharingConsent . objects . update_or_create ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer_user . enterprise_customer , defaults = { 'granted' : True } , ) audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' , 'honor' ] ) if selected_course_mode [ 'slug' ] in audit_modes : # In case of Audit course modes enroll the learner directly through # enrollment API client and redirect the learner to dashboard. enrollment_api_client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode [ 'slug' ] ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) # redirect the enterprise learner to the ecommerce flow in LMS # Note: LMS start flow automatically detects the paid mode premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
8403	def squish_infinite ( x , range = ( 0 , 1 ) ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) x [ x == - np . inf ] = range [ 0 ] x [ x == np . inf ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
8969	def encryptMessage ( self , message , ad = None ) : if ad == None : ad = self . __ad # Prepare the header for this message header = Header ( self . pub , self . __skr . sending_chain_length , self . __skr . previous_sending_chain_length ) # Encrypt the message ciphertext = self . __aead . encrypt ( message , self . __skr . nextEncryptionKey ( ) , self . _makeAD ( header , ad ) ) return { "header" : header , "ciphertext" : ciphertext }
