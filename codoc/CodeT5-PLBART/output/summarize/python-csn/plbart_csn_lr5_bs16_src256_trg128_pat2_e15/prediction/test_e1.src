7414	def plot_pairwise_dist ( self , labels = None , ax = None , cmap = None , cdict = None , metric = "euclidean" ) : allele_counts = self . genotypes . to_n_alt ( ) dist = allel . pairwise_distance ( allele_counts , metric = metric ) if not ax : fig = plt . figure ( figsize = ( 5 , 5 ) ) ax = fig . add_subplot ( 1 , 1 , 1 ) if isinstance ( labels , bool ) : if labels : labels = list ( self . samples_vcforder ) elif isinstance ( labels , type ( None ) ) : pass else : ## If not bool or None (default), then check to make sure the list passed in ## is the right length if not len ( labels ) == len ( self . samples_vcforder ) : raise IPyradError ( LABELS_LENGTH_ERROR . format ( len ( labels ) , len ( self . samples_vcforder ) ) ) allel . plot . pairwise_distance ( dist , labels = labels , ax = ax , colorbar = False )
7090	def _lclist_parallel_worker ( task ) : lcf , columns , lcformat , lcformatdir , lcndetkey = task # get the bits needed for lcformat handling # NOTE: we re-import things in this worker function because sometimes # functions can't be pickled correctly for passing them to worker functions # in a processing pool try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # we store the full path of the light curve lcobjdict = { 'lcfname' : os . path . abspath ( lcf ) } try : # read the light curve in lcdict = readerfunc ( lcf ) # this should handle lists/tuples being returned by readerfunc # we assume that the first element is the actual lcdict # FIXME: figure out how to not need this assumption if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] # insert all of the columns for colkey in columns : if '.' in colkey : getkey = colkey . split ( '.' ) else : getkey = [ colkey ] try : thiscolval = _dict_get ( lcdict , getkey ) except Exception as e : LOGWARNING ( 'column %s does not exist for %s' % ( colkey , lcf ) ) thiscolval = np . nan # update the lcobjdict with this value lcobjdict [ getkey [ - 1 ] ] = thiscolval except Exception as e : LOGEXCEPTION ( 'could not figure out columns for %s' % lcf ) # insert all of the columns as nans for colkey in columns : if '.' in colkey : getkey = colkey . split ( '.' ) else : getkey = [ colkey ] thiscolval = np . nan # update the lclistdict with this value lcobjdict [ getkey [ - 1 ] ] = thiscolval # now get the actual ndets; this excludes nans and infs for dk in lcndetkey : try : if '.' in dk : getdk = dk . split ( '.' ) else : getdk = [ dk ] ndetcol = _dict_get ( lcdict , getdk ) actualndets = ndetcol [ np . isfinite ( ndetcol ) ] . size lcobjdict [ '%s.ndet' % getdk [ - 1 ] ] = actualndets except Exception as e : lcobjdict [ '%s.ndet' % getdk [ - 1 ] ] = np . nan return lcobjdict
3205	def all ( self , get_all = False , * * queryparams ) : self . batch_id = None self . operation_status = None if get_all : return self . _iterate ( url = self . _build_path ( ) , * * queryparams ) else : return self . _mc_client . _get ( url = self . _build_path ( ) , * * queryparams )
9070	def value ( self ) : from numpy_sugar . linalg import ddot , sum2diag if self . _cache [ "value" ] is not None : return self . _cache [ "value" ] scale = exp ( self . logscale ) delta = 1 / ( 1 + exp ( - self . logitdelta ) ) v0 = scale * ( 1 - delta ) v1 = scale * delta mu = self . eta / self . tau n = len ( mu ) if self . _QS is None : K = zeros ( ( n , n ) ) else : Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] K = dot ( ddot ( Q0 , S0 ) , Q0 . T ) A = sum2diag ( sum2diag ( v0 * K , v1 ) , 1 / self . tau ) m = mu - self . mean ( ) v = - n * log ( 2 * pi ) v -= slogdet ( A ) [ 1 ] v -= dot ( m , solve ( A , m ) ) self . _cache [ "value" ] = v / 2 return self . _cache [ "value" ]
2976	def cmd_status ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . status ( ) print_containers ( containers , opts . json )
9589	def init ( self ) : resp = self . _execute ( Command . NEW_SESSION , { 'desiredCapabilities' : self . desired_capabilities } , False ) resp . raise_for_status ( ) self . session_id = str ( resp . session_id ) self . capabilities = resp . value
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } # call Fortran that grabs all of the basic stats on all of the # zVariables in one go. info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] # only looking at first possible extra dimension out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
6143	def DSP_capture_add_samples_stereo ( self , new_data_left , new_data_right ) : self . capture_sample_count = self . capture_sample_count + len ( new_data_left ) + len ( new_data_right ) if self . Tcapture > 0 : self . data_capture_left = np . hstack ( ( self . data_capture_left , new_data_left ) ) self . data_capture_right = np . hstack ( ( self . data_capture_right , new_data_right ) ) if ( len ( self . data_capture_left ) > self . Ncapture ) : self . data_capture_left = self . data_capture_left [ - self . Ncapture : ] if ( len ( self . data_capture_right ) > self . Ncapture ) : self . data_capture_right = self . data_capture_right [ - self . Ncapture : ]
13005	def bruteforce ( users , domain , password , host ) : cs = CredentialSearch ( use_pipe = False ) print_notification ( "Connecting to {}" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = "{}\\{}" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print_success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find_object ( user . username , password , domain = domain , host_ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host_ip = host , type = "plaintext" , port = 389 ) credential . add_tag ( tag ) credential . save ( ) # Add a tag to the user object, so we dont have to bruteforce it again. user . add_tag ( tag ) user . save ( ) else : print_error ( "Fail for: {}:{}" . format ( user . username , password ) )
11980	def set_ip ( self , ip ) : self . set ( ip = ip , netmask = self . _nm )
1225	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) # Memory self . memory = Memory . from_spec ( spec = self . memory_spec , kwargs = dict ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , summary_labels = self . summary_labels ) ) # Optimizer self . optimizer = Optimizer . from_spec ( spec = self . optimizer_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) # TensorFlow functions self . fn_discounted_cumulative_reward = tf . make_template ( name_ = 'discounted-cumulative-reward' , func_ = self . tf_discounted_cumulative_reward , custom_getter_ = custom_getter ) self . fn_reference = tf . make_template ( name_ = 'reference' , func_ = self . tf_reference , custom_getter_ = custom_getter ) self . fn_loss_per_instance = tf . make_template ( name_ = 'loss-per-instance' , func_ = self . tf_loss_per_instance , custom_getter_ = custom_getter ) self . fn_regularization_losses = tf . make_template ( name_ = 'regularization-losses' , func_ = self . tf_regularization_losses , custom_getter_ = custom_getter ) self . fn_loss = tf . make_template ( name_ = 'loss' , func_ = self . tf_loss , custom_getter_ = custom_getter ) self . fn_optimization = tf . make_template ( name_ = 'optimization' , func_ = self . tf_optimization , custom_getter_ = custom_getter ) self . fn_import_experience = tf . make_template ( name_ = 'import-experience' , func_ = self . tf_import_experience , custom_getter_ = custom_getter ) return custom_getter
11571	def set_bit_map ( self , shape , color ) : for row in range ( 0 , 8 ) : data = shape [ row ] # shift data into buffer bit_mask = 0x80 for column in range ( 0 , 8 ) : if data & bit_mask : self . set_pixel ( row , column , color , True ) bit_mask >>= 1 self . output_entire_buffer ( )
2887	def _try_disconnect ( self , ref ) : with self . lock : weak = [ s [ 0 ] for s in self . weak_subscribers ] try : index = weak . index ( ref ) except ValueError : # subscriber was already removed by a call to disconnect() pass else : self . weak_subscribers . pop ( index )
9981	def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
12340	def compress_blocking ( image , delete_tif = False , folder = None , force = False ) : debug ( 'compressing {}' . format ( image ) ) try : new_filename , extension = os . path . splitext ( image ) # remove last occurrence of .ome new_filename = new_filename . rsplit ( '.ome' , 1 ) [ 0 ] # if compressed file should be put in specified folder if folder : basename = os . path . basename ( new_filename ) new_filename = os . path . join ( folder , basename + '.png' ) else : new_filename = new_filename + '.png' # check if png exists if os . path . isfile ( new_filename ) and not force : compressed_images . append ( new_filename ) msg = "Aborting compress, PNG already" " exists: {}" . format ( new_filename ) raise AssertionError ( msg ) if extension != '.tif' : msg = "Aborting compress, not a TIFF: {}" . format ( image ) raise AssertionError ( msg ) # open image, load and close file pointer img = Image . open ( image ) fptr = img . fp # keep file pointer, for closing img . load ( ) # load img-data before switching mode, also closes fp # get tags and save them as json tags = img . tag . as_dict ( ) with open ( new_filename [ : - 4 ] + '.json' , 'w' ) as f : if img . mode == 'P' : # keep palette tags [ 'palette' ] = img . getpalette ( ) json . dump ( tags , f ) # check if image is palette-mode if img . mode == 'P' : # switch to luminance to keep data intact debug ( 'palette-mode switched to luminance' ) img . mode = 'L' if img . mode == 'I;16' : # https://github.com/python-pillow/Pillow/issues/1099 img = img . convert ( mode = 'I' ) # compress/save debug ( 'saving to {}' . format ( new_filename ) ) img . save ( new_filename ) fptr . close ( ) # windows bug Pillow if delete_tif : os . remove ( image ) except ( IOError , AssertionError ) as e : # print error - continue print ( 'leicaexperiment {}' . format ( e ) ) return '' return new_filename
5455	def numeric_task_id ( task_id ) : # This function exists to support the legacy "task-id" format in the "google" # provider. Google labels originally could not be numeric. When the google # provider is completely replaced by the google-v2 provider, this function can # go away. if task_id is not None : if task_id . startswith ( 'task-' ) : return int ( task_id [ len ( 'task-' ) : ] ) else : return int ( task_id )
8334	def findAllPrevious ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousGenerator , * * kwargs )
7569	def fullcomp ( seq ) : ## this is surely not the most efficient... seq = seq . replace ( "A" , 'u' ) . replace ( 'T' , 'v' ) . replace ( 'C' , 'p' ) . replace ( 'G' , 'z' ) . replace ( 'u' , 'T' ) . replace ( 'v' , 'A' ) . replace ( 'p' , 'G' ) . replace ( 'z' , 'C' ) ## No complement for S & W b/c complements are S & W, respectively seq = seq . replace ( 'R' , 'u' ) . replace ( 'K' , 'v' ) . replace ( 'Y' , 'b' ) . replace ( 'M' , 'o' ) . replace ( 'u' , 'Y' ) . replace ( 'v' , 'M' ) . replace ( 'b' , 'R' ) . replace ( 'o' , 'K' ) seq = seq . replace ( 'r' , 'u' ) . replace ( 'k' , 'v' ) . replace ( 'y' , 'b' ) . replace ( 'm' , 'o' ) . replace ( 'u' , 'y' ) . replace ( 'v' , 'm' ) . replace ( 'b' , 'r' ) . replace ( 'o' , 'k' ) return seq
2070	def basen_to_integer ( self , X , cols , base ) : out_cols = X . columns . values . tolist ( ) for col in cols : col_list = [ col0 for col0 in out_cols if str ( col0 ) . startswith ( str ( col ) ) ] insert_at = out_cols . index ( col_list [ 0 ] ) if base == 1 : value_array = np . array ( [ int ( col0 . split ( '_' ) [ - 1 ] ) for col0 in col_list ] ) else : len0 = len ( col_list ) value_array = np . array ( [ base ** ( len0 - 1 - i ) for i in range ( len0 ) ] ) X . insert ( insert_at , col , np . dot ( X [ col_list ] . values , value_array . T ) ) X . drop ( col_list , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
3147	def _iterate ( self , url , * * queryparams ) : # fields as a query string parameter should be a string with # comma-separated substring values to pass along to # self._mc_client._get(). It should also contain total_items whenever # the parameter is employed, which is forced here. if 'fields' in queryparams : if 'total_items' not in queryparams [ 'fields' ] . split ( ',' ) : queryparams [ 'fields' ] += ',total_items' # Remove offset and count if provided in queryparams # to avoid 'multiple values for keyword argument' TypeError queryparams . pop ( "offset" , None ) queryparams . pop ( "count" , None ) # Fetch results from mailchimp, up to first 1000 result = self . _mc_client . _get ( url = url , offset = 0 , count = 1000 , * * queryparams ) total = result [ 'total_items' ] # Fetch further results if necessary if total > 1000 : for offset in range ( 1 , int ( total / 1000 ) + 1 ) : result = merge_results ( result , self . _mc_client . _get ( url = url , offset = int ( offset * 1000 ) , count = 1000 , * * queryparams ) ) return result else : # Further results not necessary return result
4865	def validate_username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) return value
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
12972	def getOnlyFields ( self , pk , fields , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hmget ( key , fields ) if type ( res ) != list or not len ( res ) : return None objDict = { } numFields = len ( fields ) i = 0 anyNotNone = False while i < numFields : objDict [ fields [ i ] ] = res [ i ] if res [ i ] != None : anyNotNone = True i += 1 if anyNotNone is False : return None objDict [ '_id' ] = pk ret = self . _redisResultToObj ( objDict ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
1161	def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( "%s.acquire(%s): blocked waiting, value=%s" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( "%s.acquire: success, value=%s" , self , self . __value ) rc = True return rc
326	def simulate_paths ( is_returns , num_days , starting_value = 1 , num_samples = 1000 , random_seed = None ) : samples = np . empty ( ( num_samples , num_days ) ) seed = np . random . RandomState ( seed = random_seed ) for i in range ( num_samples ) : samples [ i , : ] = is_returns . sample ( num_days , replace = True , random_state = seed ) return samples
5299	def get_context_data ( self , * * kwargs ) : data = super ( BaseCalendarMonthView , self ) . get_context_data ( * * kwargs ) year = self . get_year ( ) month = self . get_month ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) cal = Calendar ( self . get_first_of_week ( ) ) month_calendar = [ ] now = datetime . datetime . utcnow ( ) date_lists = defaultdict ( list ) multidate_objs = [ ] for obj in data [ 'object_list' ] : obj_date = self . get_start_date ( obj ) end_date_field = self . get_end_date_field ( ) if end_date_field : end_date = self . get_end_date ( obj ) if end_date and end_date != obj_date : multidate_objs . append ( { 'obj' : obj , 'range' : [ x for x in daterange ( obj_date , end_date ) ] } ) continue # We don't put multi-day events in date_lists date_lists [ obj_date ] . append ( obj ) for week in cal . monthdatescalendar ( date . year , date . month ) : week_range = set ( daterange ( week [ 0 ] , week [ 6 ] ) ) week_events = [ ] for val in multidate_objs : intersect_length = len ( week_range . intersection ( val [ 'range' ] ) ) if intersect_length : # Event happens during this week slot = 1 width = intersect_length # How many days is the event during this week? nowrap_previous = True # Does the event continue from the previous week? nowrap_next = True # Does the event continue to the next week? if val [ 'range' ] [ 0 ] >= week [ 0 ] : slot = 1 + ( val [ 'range' ] [ 0 ] - week [ 0 ] ) . days else : nowrap_previous = False if val [ 'range' ] [ - 1 ] > week [ 6 ] : nowrap_next = False week_events . append ( { 'event' : val [ 'obj' ] , 'slot' : slot , 'width' : width , 'nowrap_previous' : nowrap_previous , 'nowrap_next' : nowrap_next , } ) week_calendar = { 'events' : week_events , 'date_list' : [ ] , } for day in week : week_calendar [ 'date_list' ] . append ( { 'day' : day , 'events' : date_lists [ day ] , 'today' : day == now . date ( ) , 'is_current_month' : day . month == date . month , } ) month_calendar . append ( week_calendar ) data [ 'calendar' ] = month_calendar data [ 'weekdays' ] = [ DAYS [ x ] for x in cal . iterweekdays ( ) ] data [ 'month' ] = date data [ 'next_month' ] = self . get_next_month ( date ) data [ 'previous_month' ] = self . get_previous_month ( date ) return data
6772	def install_required ( self , type = None , service = None , list_only = 0 , * * kwargs ) : # pylint: disable=redefined-builtin r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
1084	def time ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond )
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
5627	def read_json ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : return json . loads ( urlopen ( path ) . read ( ) . decode ( ) ) except HTTPError : raise FileNotFoundError ( "%s not found" , path ) elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return json . loads ( obj . get ( ) [ 'Body' ] . read ( ) . decode ( ) ) raise FileNotFoundError ( "%s not found" , path ) else : try : with open ( path , "r" ) as src : return json . loads ( src . read ( ) ) except : raise FileNotFoundError ( "%s not found" , path )
13673	def add_file ( self , * args ) : for file_path in args : self . files . append ( FilePath ( file_path , self ) )
12404	def bump ( self , bump_reqs = None , * * kwargs ) : bumps = { } for existing_req in sorted ( self . requirements ( ) , key = lambda r : r . project_name ) : if bump_reqs and existing_req . project_name not in bump_reqs : continue bump_reqs . check ( existing_req ) try : bump = self . _bump ( existing_req , bump_reqs . get ( existing_req . project_name ) ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if bump_reqs and bump_reqs . get ( existing_req . project_name ) and all ( r . required_by is None for r in bump_reqs . get ( existing_req . project_name ) ) : raise else : log . warn ( e ) for reqs in bump_reqs . required_requirements ( ) . values ( ) : name = reqs [ 0 ] . project_name if name not in bumps and self . should_add ( name ) : try : bump = self . _bump ( None , reqs ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if all ( r . required_by is None for r in reqs ) : raise else : log . warn ( e ) self . bumps . update ( bumps . values ( ) ) return bumps . values ( )
13307	def gmb ( a , b ) : return np . exp ( np . log ( a ) . mean ( ) - np . log ( b ) . mean ( ) )
4905	def create_course_completion ( self , user_id , payload ) : # pylint: disable=unused-argument return self . _post ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
6627	def _raiseUnavailableFor401 ( message ) : def __raiseUnavailableFor401 ( fn ) : def wrapped ( * args , * * kwargs ) : try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTPError as e : if e . response . status_code == requests . codes . unauthorized : raise access_common . Unavailable ( message ) else : raise return wrapped return __raiseUnavailableFor401
4723	def trun_setup ( conf ) : declr = None try : with open ( conf [ "TESTPLAN_FPATH" ] ) as declr_fd : declr = yaml . safe_load ( declr_fd ) except AttributeError as exc : cij . err ( "rnr: %r" % exc ) if not declr : return None trun = copy . deepcopy ( TRUN ) trun [ "ver" ] = cij . VERSION trun [ "conf" ] = copy . deepcopy ( conf ) trun [ "res_root" ] = conf [ "OUTPUT" ] trun [ "aux_root" ] = os . sep . join ( [ trun [ "res_root" ] , "_aux" ] ) trun [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( trun [ "aux_root" ] ) hook_names = declr . get ( "hooks" , [ ] ) if "lock" not in hook_names : hook_names = [ "lock" ] + hook_names if hook_names [ 0 ] != "lock" : return None # Setup top-level hooks trun [ "hooks" ] = hooks_setup ( trun , trun , hook_names ) for enum , declr in enumerate ( declr [ "testsuites" ] ) : # Setup testsuites tsuite = tsuite_setup ( trun , declr , enum ) if tsuite is None : cij . err ( "main::FAILED: setting up tsuite: %r" % tsuite ) return 1 trun [ "testsuites" ] . append ( tsuite ) trun [ "progress" ] [ "UNKN" ] += len ( tsuite [ "testcases" ] ) return trun
10697	def color_run ( start_color , end_color , step_count , inclusive = True , to_color = True ) : if isinstance ( start_color , Color ) : start_color = start_color . rgb if isinstance ( end_color , Color ) : end_color = end_color . rgb step = tuple ( ( end_color [ i ] - start_color [ i ] ) / step_count for i in range ( 3 ) ) add = lambda x , y : tuple ( sum ( z ) for z in zip ( x , y ) ) mult = lambda x , y : tuple ( y * z for z in x ) run = [ add ( start_color , mult ( step , i ) ) for i in range ( 1 , step_count ) ] if inclusive : run = [ start_color ] + run + [ end_color ] return run if not to_color else [ Color ( c ) for c in run ]
5212	def bdh ( tickers , flds = None , start_date = None , end_date = 'today' , adjust = None , * * kwargs ) -> pd . DataFrame : logger = logs . get_logger ( bdh , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) # Dividend adjustments if isinstance ( adjust , str ) and adjust : if adjust == 'all' : kwargs [ 'CshAdjNormal' ] = True kwargs [ 'CshAdjAbnormal' ] = True kwargs [ 'CapChg' ] = True else : kwargs [ 'CshAdjNormal' ] = 'normal' in adjust or 'dvd' in adjust kwargs [ 'CshAdjAbnormal' ] = 'abn' in adjust or 'dvd' in adjust kwargs [ 'CapChg' ] = 'split' in adjust con , _ = create_connection ( ) elms = assist . proc_elms ( * * kwargs ) ovrds = assist . proc_ovrds ( * * kwargs ) if isinstance ( tickers , str ) : tickers = [ tickers ] if flds is None : flds = [ 'Last_Price' ] if isinstance ( flds , str ) : flds = [ flds ] e_dt = utils . fmt_dt ( end_date , fmt = '%Y%m%d' ) if start_date is None : start_date = pd . Timestamp ( e_dt ) - relativedelta ( months = 3 ) s_dt = utils . fmt_dt ( start_date , fmt = '%Y%m%d' ) logger . info ( f'loading historical data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) logger . debug ( f'\nflds={flds}\nelms={elms}\novrds={ovrds}\nstart_date={s_dt}\nend_date={e_dt}' ) res = con . bdh ( tickers = tickers , flds = flds , elms = elms , ovrds = ovrds , start_date = s_dt , end_date = e_dt ) res . index . name = None if ( len ( flds ) == 1 ) and kwargs . get ( 'keep_one' , False ) : return res . xs ( flds [ 0 ] , axis = 1 , level = 1 ) return res
2422	def str_from_text ( text ) : REGEX = re . compile ( '<text>((.|\n)+)</text>' , re . UNICODE ) match = REGEX . match ( text ) if match : return match . group ( 1 ) else : return None
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
9373	def is_valid_url ( url ) : regex = re . compile ( r'^(?:http|ftp)s?://' r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|' r'localhost|' r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' r'(?::\d+)?' r'(?:/?|[/?]\S+)$' , re . IGNORECASE ) if regex . match ( url ) : logger . info ( "URL given as config" ) return True else : return False
349	def load_wmt_en_fr_dataset ( path = 'data' ) : path = os . path . join ( path , 'wmt_en_fr' ) # URLs for WMT data. _WMT_ENFR_TRAIN_URL = "http://www.statmt.org/wmt10/" _WMT_ENFR_DEV_URL = "http://www.statmt.org/wmt15/" def gunzip_file ( gz_path , new_path ) : """Unzips from gz_path into new_path.""" logging . info ( "Unpacking %s to %s" % ( gz_path , new_path ) ) with gzip . open ( gz_path , "rb" ) as gz_file : with open ( new_path , "wb" ) as new_file : for line in gz_file : new_file . write ( line ) def get_wmt_enfr_train_set ( path ) : """Download the WMT en-fr training corpus to directory unless it's there.""" filename = "training-giga-fren.tar" maybe_download_and_extract ( filename , path , _WMT_ENFR_TRAIN_URL , extract = True ) train_path = os . path . join ( path , "giga-fren.release2.fixed" ) gunzip_file ( train_path + ".fr.gz" , train_path + ".fr" ) gunzip_file ( train_path + ".en.gz" , train_path + ".en" ) return train_path def get_wmt_enfr_dev_set ( path ) : """Download the WMT en-fr training corpus to directory unless it's there.""" filename = "dev-v2.tgz" dev_file = maybe_download_and_extract ( filename , path , _WMT_ENFR_DEV_URL , extract = False ) dev_name = "newstest2013" dev_path = os . path . join ( path , "newstest2013" ) if not ( gfile . Exists ( dev_path + ".fr" ) and gfile . Exists ( dev_path + ".en" ) ) : logging . info ( "Extracting tgz file %s" % dev_file ) with tarfile . open ( dev_file , "r:gz" ) as dev_tar : fr_dev_file = dev_tar . getmember ( "dev/" + dev_name + ".fr" ) en_dev_file = dev_tar . getmember ( "dev/" + dev_name + ".en" ) fr_dev_file . name = dev_name + ".fr" # Extract without "dev/" prefix. en_dev_file . name = dev_name + ".en" dev_tar . extract ( fr_dev_file , path ) dev_tar . extract ( en_dev_file , path ) return dev_path logging . info ( "Load or Download WMT English-to-French translation > {}" . format ( path ) ) train_path = get_wmt_enfr_train_set ( path ) dev_path = get_wmt_enfr_dev_set ( path ) return train_path , dev_path
5417	def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( * * values )
11106	def get_pickling_errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , "__getstate__" ) : state = obj . __getstate__ ( ) #elif hasattr(obj, "__dict__"): # state = obj.__dict__ else : return None #try: # state = obj.__getstate__() #except AttributeError as e: # #state = obj.__dict__ # return str(e) if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . PicklingError as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get_pickling_errors ( state [ i ] , seen ) return result
11810	def index_document ( self , text , url ) : ## For now, use first line for title title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
1129	def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ "newline" ] ) def rule ( parser ) : result = parser . _accept ( "newline" ) if result is unmatched : return result return [ ] return rule
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
3238	def get_role_managed_policy_documents ( role , client = None , * * kwargs ) : policies = get_role_managed_policies ( role , force_client = client ) policy_names = ( policy [ 'name' ] for policy in policies ) delayed_gmpd_calls = ( delayed ( get_managed_policy_document ) ( policy [ 'arn' ] , force_client = client ) for policy in policies ) policy_documents = Parallel ( n_jobs = 20 , backend = "threading" ) ( delayed_gmpd_calls ) return dict ( zip ( policy_names , policy_documents ) )
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
2924	def _predict ( self , my_task , seen = None , looked_ahead = 0 ) : if my_task . _is_finished ( ) : return if seen is None : seen = [ ] elif self in seen : return if not my_task . _is_finished ( ) : self . _predict_hook ( my_task ) if not my_task . _is_definite ( ) : if looked_ahead + 1 >= self . lookahead : return seen . append ( self ) for child in my_task . children : child . task_spec . _predict ( child , seen [ : ] , looked_ahead + 1 )
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
11798	def suppose ( self , var , value ) : self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
9867	async def rt_unsubscribe ( self ) : if self . _subscription_id is None : _LOGGER . error ( "Not subscribed." ) return await self . _tibber_control . sub_manager . unsubscribe ( self . _subscription_id )
9051	def poisson_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) link = LogLink ( ) lik = PoissonProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
296	def plot_sector_allocations ( returns , sector_alloc , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) sector_alloc . plot ( title = 'Sector allocation over time' , alpha = 0.5 , ax = ax , * * kwargs ) box = ax . get_position ( ) ax . set_position ( [ box . x0 , box . y0 + box . height * 0.1 , box . width , box . height * 0.9 ] ) # Put a legend below current axis ax . legend ( loc = 'upper center' , frameon = True , framealpha = 0.5 , bbox_to_anchor = ( 0.5 , - 0.14 ) , ncol = 5 ) ax . set_xlim ( ( sector_alloc . index [ 0 ] , sector_alloc . index [ - 1 ] ) ) ax . set_ylabel ( 'Exposure by sector' ) ax . set_xlabel ( '' ) return ax
8087	def font ( self , fontpath = None , fontsize = None ) : if fontpath is not None : self . _canvas . fontfile = fontpath else : return self . _canvas . fontfile if fontsize is not None : self . _canvas . fontsize = fontsize
3752	def STEL ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _STEL = None else : raise Exception ( 'Failure in in function' ) return _STEL
1215	def _wait_state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . _execute ( dict ( key = 0 ) ) return state , terminal , reward
12000	def _sign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = get_random_bytes ( algorithm [ 'salt_size' ] ) key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _encode ( data , algorithm , key ) return data + key_salt
1453	def add_key ( self , key ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer )
3531	def is_internal_ip ( context , prefix = None ) : try : request = context [ 'request' ] remote_ip = request . META . get ( 'HTTP_X_FORWARDED_FOR' , '' ) if not remote_ip : remote_ip = request . META . get ( 'REMOTE_ADDR' , '' ) if not remote_ip : return False internal_ips = None if prefix is not None : internal_ips = getattr ( settings , '%s_INTERNAL_IPS' % prefix , None ) if internal_ips is None : internal_ips = getattr ( settings , 'ANALYTICAL_INTERNAL_IPS' , None ) if internal_ips is None : internal_ips = getattr ( settings , 'INTERNAL_IPS' , None ) return remote_ip in ( internal_ips or [ ] ) except ( KeyError , AttributeError ) : return False
6576	def populate_fields ( api_client , instance , data ) : for key , value in instance . __class__ . _fields . items ( ) : default = getattr ( value , "default" , None ) newval = data . get ( value . field , default ) if isinstance ( value , SyntheticField ) : newval = value . formatter ( api_client , data , newval ) setattr ( instance , key , newval ) continue model_class = getattr ( value , "model" , None ) if newval and model_class : if isinstance ( newval , list ) : newval = model_class . from_json_list ( api_client , newval ) else : newval = model_class . from_json ( api_client , newval ) if newval and value . formatter : newval = value . formatter ( api_client , newval ) setattr ( instance , key , newval )
1438	def update_received_packet ( self , received_pkt_size_bytes ) : self . update_count ( self . RECEIVED_PKT_COUNT ) self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes )
7522	def concat_vcf ( data , names , full ) : ## open handle and write headers if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) ## get vcf chunks vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) ## concatenate if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) ## what order do users want? The order in the original ref file? ## Sorted by the size of chroms? that is the order in faidx. ## If reference mapping then it's nice to sort the vcf data by ## CHROM and POS. This is doing a very naive sort right now, so the ## CHROM will be ordered, but not the pos within each chrom. if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : ## Some unix sorting magic to get POS sorted within CHROM ## First you sort by POS (-k 2,2), then you do a `stable` sort ## by CHROM. You end up with POS ordered and grouped correctly by CHROM ## but relatively unordered CHROMs (locus105 will be before locus11). cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) # initialize `TemporalMemoryMonitorMixin` attributes tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
6713	def install_setuptools ( python_cmd = 'python' , use_sudo = True ) : setuptools_version = package_version ( 'setuptools' , python_cmd ) distribute_version = package_version ( 'distribute' , python_cmd ) if setuptools_version is None : _install_from_scratch ( python_cmd , use_sudo ) else : if distribute_version is None : _upgrade_from_setuptools ( python_cmd , use_sudo ) else : _upgrade_from_distribute ( python_cmd , use_sudo )
10447	def activatewindow ( self , window_name ) : window_handle = self . _get_window_handle ( window_name ) self . _grabfocus ( window_handle ) return 1
6548	def move_to ( self , ypos , xpos ) : # the screen's co-ordinates are 1 based, but the command is 0 based xpos -= 1 ypos -= 1 self . exec_command ( "MoveCursor({0}, {1})" . format ( ypos , xpos ) . encode ( "ascii" ) )
10517	def setmax ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 1 return 1
8375	def var_deleted ( self , v ) : widget = self . widgets [ v . name ] # widgets are all in a single container .. parent = widget . get_parent ( ) self . container . remove ( parent ) del self . widgets [ v . name ] self . window . set_size_request ( 400 , 35 * len ( self . widgets . keys ( ) ) ) self . window . show_all ( )
13015	def match ( self , uri ) : absolute_uri = self . __absolute__ ( uri ) return absolute_uri . startswith ( self . __path__ ) and op . exists ( absolute_uri )
9507	def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
155	def prev_key ( self , key , default = _sentinel ) : item = self . prev_item ( key , default ) return default if item is default else item [ 0 ]
402	def maxnorm_regularizer ( scale = 1.0 ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) # if scale >= 1.: # raise ValueError('Setting a scale greater than 1 on a regularizer: %g' % # scale) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def mn ( weights , name = 'max_regularizer' ) : """Applies max-norm regularization to weights.""" with tf . name_scope ( name ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) # if tf.__version__ <= '0.12': # standard_ops_fn = standard_ops.mul # else: standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_max ( standard_ops . abs ( weights ) ) , name = scope ) return mn
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( * * filters ) entity = query . first ( ) if not entity : entity = self . model_class ( * * filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
12838	async def async_connect ( self ) : if self . _async_lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . _async_lock : self . connect ( True ) return self . _state . conn
2236	def timestamp ( method = 'iso8601' ) : if method == 'iso8601' : # ISO 8601 # datetime.datetime.utcnow().isoformat() # datetime.datetime.now().isoformat() # utcnow tz_hour = time . timezone // 3600 utc_offset = str ( tz_hour ) if tz_hour < 0 else '+' + str ( tz_hour ) stamp = time . strftime ( '%Y-%m-%dT%H%M%S' ) + utc_offset return stamp else : raise ValueError ( 'only iso8601 is accepted for now' )
12064	def lazygo ( watchFolder = '../abfs/' , reAnalyze = False , rebuildSite = False , keepGoing = True , matching = False ) : abfsKnown = [ ] while True : print ( ) pagesNeeded = [ ] for fname in glob . glob ( watchFolder + "/*.abf" ) : ID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if not fname in abfsKnown : if os . path . exists ( fname . replace ( ".abf" , ".rsv" ) ) : #TODO: or something like this continue if matching and not matching in fname : continue abfsKnown . append ( fname ) if os . path . exists ( os . path . dirname ( fname ) + "/swhlab4/" + os . path . basename ( fname ) . replace ( ".abf" , "_info.pkl" ) ) and reAnalyze == False : print ( "already analyzed" , os . path . basename ( fname ) ) if rebuildSite : pagesNeeded . append ( ID ) else : handleNewABF ( fname ) pagesNeeded . append ( ID ) if len ( pagesNeeded ) : print ( " -- rebuilding index page" ) indexing . genIndex ( os . path . dirname ( fname ) , forceIDs = pagesNeeded ) if not keepGoing : return for i in range ( 50 ) : print ( '.' , end = '' ) time . sleep ( .2 )
11352	def _fill_text ( self , text , width , indent ) : lines = [ ] for line in text . splitlines ( False ) : if line : # https://docs.python.org/2/library/textwrap.html lines . extend ( textwrap . wrap ( line . strip ( ) , width , initial_indent = indent , subsequent_indent = indent ) ) else : lines . append ( line ) text = "\n" . join ( lines ) return text
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
722	def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records
7378	def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance ( k , str ) : msg = "%s: key must be a string" % self . __class__ . __name__ raise ValueError ( msg ) if not k . startswith ( self . prefix ) : k = self . prefix + k return func ( self , k , * args ) return decorated
121	def get_batch ( self ) : if self . all_finished ( ) : return None batch_str = self . queue_result . get ( ) batch = pickle . loads ( batch_str ) if batch is not None : return batch else : self . nb_workers_finished += 1 if self . nb_workers_finished >= self . nb_workers : try : self . queue_source . get ( timeout = 0.001 ) # remove the None from the source queue except QueueEmpty : pass return None else : return self . get_batch ( )
13198	def read ( cls , root_tex_path ) : # Read and normalize the TeX source, replacing macros with content root_dir = os . path . dirname ( root_tex_path ) tex_source = read_tex_file ( root_tex_path ) tex_macros = get_macros ( tex_source ) tex_source = replace_macros ( tex_source , tex_macros ) return cls ( tex_source , root_dir = root_dir )
11447	def _login ( self , session , get_request = False ) : req = session . post ( self . _login_url , data = self . _logindata ) if _LOGIN_ERROR_STRING in req . text or req . status_code == 403 or req . url == _LOGIN_URL : err_mess = "YesssSMS: login failed, username or password wrong" if _LOGIN_LOCKED_MESS in req . text : err_mess += ", page says: " + _LOGIN_LOCKED_MESS_ENG self . _suspended = True raise self . AccountSuspendedError ( err_mess ) raise self . LoginError ( err_mess ) self . _suspended = False # login worked return ( session , req ) if get_request else session
13002	def _filter_cluster_data ( self ) : min_temp = self . temperature_range_slider . value [ 0 ] max_temp = self . temperature_range_slider . value [ 1 ] temp_mask = np . logical_and ( self . cluster . catalog [ 'temperature' ] >= min_temp , self . cluster . catalog [ 'temperature' ] <= max_temp ) min_lum = self . luminosity_range_slider . value [ 0 ] max_lum = self . luminosity_range_slider . value [ 1 ] lum_mask = np . logical_and ( self . cluster . catalog [ 'luminosity' ] >= min_lum , self . cluster . catalog [ 'luminosity' ] <= max_lum ) selected_mask = np . isin ( self . cluster . catalog [ 'id' ] , self . selection_ids ) filter_mask = temp_mask & lum_mask & selected_mask self . filtered_data = self . cluster . catalog [ filter_mask ] . data self . source . data = { 'id' : list ( self . filtered_data [ 'id' ] ) , 'temperature' : list ( self . filtered_data [ 'temperature' ] ) , 'luminosity' : list ( self . filtered_data [ 'luminosity' ] ) , 'color' : list ( self . filtered_data [ 'color' ] ) } logging . debug ( "Selected data is now: %s" , self . filtered_data )
9596	def add_cookie ( self , cookie_dict ) : if not isinstance ( cookie_dict , dict ) : raise TypeError ( 'Type of the cookie must be a dict.' ) if not cookie_dict . get ( 'name' , None ) or not cookie_dict . get ( 'value' , None ) : raise KeyError ( 'Missing required keys, \'name\' and \'value\' must be provided.' ) self . _execute ( Command . ADD_COOKIE , { 'cookie' : cookie_dict } )
4952	def get_no_record_response ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) data = { self . REQUIRED_PARAM_USERNAME : username , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER : enterprise_customer_uuid , self . CONSENT_EXISTS : False , self . CONSENT_GRANTED : False , self . CONSENT_REQUIRED : False , } if course_id : data [ self . REQUIRED_PARAM_COURSE_ID ] = course_id if program_uuid : data [ self . REQUIRED_PARAM_PROGRAM_UUID ] = program_uuid return Response ( data , status = HTTP_200_OK )
10218	def plot_summary_axes ( graph : BELGraph , lax , rax , logx = True ) : ntc = count_functions ( graph ) etc = count_relations ( graph ) df = pd . DataFrame . from_dict ( dict ( ntc ) , orient = 'index' ) df_ec = pd . DataFrame . from_dict ( dict ( etc ) , orient = 'index' ) df . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = lax ) lax . set_title ( 'Number of nodes: {}' . format ( graph . number_of_nodes ( ) ) ) df_ec . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = rax ) rax . set_title ( 'Number of edges: {}' . format ( graph . number_of_edges ( ) ) )
5046	def _enroll_users ( cls , request , enterprise_customer , emails , mode , course_id = None , program_details = None , notify = True ) : pending_messages = [ ] if course_id : succeeded , pending , failed = cls . enroll_users_in_course ( enterprise_customer = enterprise_customer , course_id = course_id , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : enterprise_customer . notify_enrolled_learners ( catalog_api_user = request . user , course_id = course_id , users = all_successes , ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , course_id ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , course_id ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , course_id ) ) if program_details : succeeded , pending , failed = cls . enroll_users_in_program ( enterprise_customer = enterprise_customer , program_details = program_details , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : cls . notify_program_learners ( enterprise_customer = enterprise_customer , program_details = program_details , users = all_successes ) program_identifier = program_details . get ( 'title' , program_details . get ( 'uuid' , _ ( 'the program' ) ) ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , program_identifier ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , program_identifier ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , program_identifier ) ) cls . send_messages ( request , pending_messages )
3333	def dynamic_instantiate_middleware ( name , args , expand = None ) : def _expand ( v ) : """Replace some string templates with defined values.""" if expand and compat . is_basestring ( v ) and v . lower ( ) in expand : return expand [ v ] return v try : the_class = dynamic_import_class ( name ) inst = None if type ( args ) in ( tuple , list ) : args = tuple ( map ( _expand , args ) ) inst = the_class ( * args ) else : assert type ( args ) is dict args = { k : _expand ( v ) for k , v in args . items ( ) } inst = the_class ( * * args ) _logger . debug ( "Instantiate {}({}) => {}" . format ( name , args , inst ) ) except Exception : _logger . exception ( "ERROR: Instantiate {}({}) => {}" . format ( name , args , inst ) ) return inst
10265	def collapse_entrez_equivalencies ( graph : BELGraph ) : relation_filter = build_relation_predicate ( EQUIVALENT_TO ) source_namespace_filter = build_source_namespace_filter ( [ 'EGID' , 'EG' , 'ENTREZ' ] ) edge_predicates = [ relation_filter , source_namespace_filter , ] _collapse_edge_passing_predicates ( graph , edge_predicates = edge_predicates )
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
8137	def brightness ( self , value = 1.0 ) : b = ImageEnhance . Brightness ( self . img ) self . img = b . enhance ( value )
11768	def weighted_sampler ( seq , weights ) : totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
13481	def _regex_replacement ( self , target , replacement ) : match = re . compile ( target ) self . data = match . sub ( replacement , self . data )
5406	def _get_mount_actions ( self , mounts , mnt_datadisk ) : actions_to_add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount_path = mount . docker_path actions_to_add . extend ( [ google_v2_pipelines . build_action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' , 'RUN_IN_BACKGROUND' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) , google_v2_pipelines . build_action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ 'wait' , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) ] ) return actions_to_add
10810	def get_by_name ( cls , name ) : try : return cls . query . filter_by ( name = name ) . one ( ) except NoResultFound : return None
6673	def is_link ( self , path , use_sudo = False ) : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
835	def clear ( self ) : self . _Memory = None self . _numPatterns = 0 self . _M = None self . _categoryList = [ ] self . _partitionIdList = [ ] self . _partitionIdMap = { } self . _finishedLearning = False self . _iterationIdx = - 1 # Fixed capacity KNN if self . maxStoredPatterns > 0 : assert self . useSparseMemory , ( "Fixed capacity KNN is implemented only " "in the sparse memory mode" ) self . fixedCapacity = True self . _categoryRecencyList = [ ] else : self . fixedCapacity = False # Cached value of the store prototype sizes self . _protoSizes = None # Used by PCA self . _s = None self . _vt = None self . _nc = None self . _mean = None # Used by Network Builder self . _specificIndexTraining = False self . _nextTrainingIndices = None
8057	def do_restart ( self , line ) : self . bot . _frame = 0 self . bot . _namespace . clear ( ) self . bot . _namespace . update ( self . bot . _initial_namespace )
2359	def t_intnumber ( self , t ) : t . value = int ( t . value ) t . type = 'NUMBER' return t
12926	def as_dict ( self ) : self_as_dict = dict ( ) self_as_dict [ 'sequence' ] = self . sequence if hasattr ( self , 'frequency' ) : self_as_dict [ 'frequency' ] = self . frequency return self_as_dict
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None # Loop so we don't return None when there are items, if item is deleted between getting key and getting obj while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
6809	def configure_camera ( self ) : #TODO:check per OS? Works on Raspbian Jessie r = self . local_renderer if self . env . camera_enabled : r . pc ( 'Enabling camera.' ) #TODO:fix, doesn't work on Ubuntu, which uses commented-out values # Set start_x=1 #r.sudo('if grep "start_x=0" /boot/config.txt; then sed -i "s/start_x=0/start_x=1/g" /boot/config.txt; fi') #r.sudo('if grep "start_x" /boot/config.txt; then true; else echo "start_x=1" >> /boot/config.txt; fi') r . enable_attr ( filename = '/boot/config.txt' , key = 'start_x' , value = 1 , use_sudo = True , ) # Set gpu_mem=128 # r.sudo('if grep "gpu_mem" /boot/config.txt; then true; else echo "gpu_mem=128" >> /boot/config.txt; fi') r . enable_attr ( filename = '/boot/config.txt' , key = 'gpu_mem' , value = r . env . gpu_mem , use_sudo = True , ) # Compile the Raspberry Pi binaries. #https://github.com/raspberrypi/userland r . run ( 'cd ~; git clone https://github.com/raspberrypi/userland.git; cd userland; ./buildme' ) r . run ( 'touch ~/.bash_aliases' ) #r.run("echo 'PATH=$PATH:/opt/vc/bin\nexport PATH' >> ~/.bash_aliases") r . append ( r'PATH=$PATH:/opt/vc/bin\nexport PATH' , '~/.bash_aliases' ) #r.run("echo 'LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/vc/lib\nexport LD_LIBRARY_PATH' >> ~/.bash_aliases") r . append ( r'LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/vc/lib\nexport LD_LIBRARY_PATH' , '~/.bash_aliases' ) r . run ( 'source ~/.bashrc' ) r . sudo ( 'ldconfig' ) # Allow our user to access the video device. r . sudo ( "echo 'SUBSYSTEM==\"vchiq\",GROUP=\"video\",MODE=\"0660\"' > /etc/udev/rules.d/10-vchiq-permissions.rules" ) r . sudo ( "usermod -a -G video {user}" ) r . reboot ( wait = 300 , timeout = 60 ) self . test_camera ( ) else : r . disable_attr ( filename = '/boot/config.txt' , key = 'start_x' , use_sudo = True , ) r . disable_attr ( filename = '/boot/config.txt' , key = 'gpu_mem' , use_sudo = True , ) r . reboot ( wait = 300 , timeout = 60 )
9615	def elements ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENTS , { 'using' : using , 'value' : value } )
7853	def identity_is ( self , item_category , item_type = None ) : if not item_category : raise ValueError ( "bad category" ) if not item_type : type_expr = u"" elif '"' not in item_type : type_expr = u' and @type="%s"' % ( item_type , ) elif "'" not in type : type_expr = u" and @type='%s'" % ( item_type , ) else : raise ValueError ( "Invalid type name" ) if '"' not in item_category : expr = u'd:identity[@category="%s"%s]' % ( item_category , type_expr ) elif "'" not in item_category : expr = u"d:identity[@category='%s'%s]" % ( item_category , type_expr ) else : raise ValueError ( "Invalid category name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : # I already have the inverted fields I need anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : # I am creating the inverted fields then...need output file path: output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) # setup the commands to be called invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) # add fsl_sub before the commands if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd # create the inverse fields if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to anatomical space if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to functional space if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
13692	def broadcast_tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK_NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )
2105	def version ( ) : # Print out the current version of Tower CLI. click . echo ( 'Tower CLI %s' % __version__ ) # Print out the current API version of the current code base. click . echo ( 'API %s' % CUR_API_VERSION ) # Attempt to connect to the Ansible Tower server. # If we succeed, print a version; if not, generate a failure. try : r = client . get ( '/config/' ) except RequestException as ex : raise exc . TowerCLIError ( 'Could not connect to Ansible Tower.\n%s' % six . text_type ( ex ) ) config = r . json ( ) license = config . get ( 'license_info' , { } ) . get ( 'license_type' , 'open' ) if license == 'open' : server_type = 'AWX' else : server_type = 'Ansible Tower' click . echo ( '%s %s' % ( server_type , config [ 'version' ] ) ) # Print out Ansible version of server click . echo ( 'Ansible %s' % config [ 'ansible_version' ] )
13839	def ConsumeFloat ( self ) : try : result = ParseFloat ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
4823	def _sort_course_modes ( self , modes ) : def slug_weight ( mode ) : """ Assign a weight to the course mode dictionary based on the position of its slug in the sorting list. """ sorting_slugs = COURSE_MODE_SORT_ORDER sorting_slugs_size = len ( sorting_slugs ) if mode [ 'slug' ] in sorting_slugs : return sorting_slugs_size - sorting_slugs . index ( mode [ 'slug' ] ) return 0 # Sort slug weights in descending order return sorted ( modes , key = slug_weight , reverse = True )
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
13333	def path_resolver ( resolver , path ) : path = unipath ( path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
9654	def take_shas_of_all_files ( G , settings ) : global ERROR_FN sprint = settings [ "sprint" ] error = settings [ "error" ] ERROR_FN = error sha_dict = { } all_files = [ ] for target in G . nodes ( data = True ) : sprint ( "About to take shas of files in target '{}'" . format ( target [ 0 ] ) , level = "verbose" ) if 'dependencies' in target [ 1 ] : sprint ( "It has dependencies" , level = "verbose" ) deplist = [ ] for dep in target [ 1 ] [ 'dependencies' ] : glist = glob . glob ( dep ) if glist : for oneglob in glist : deplist . append ( oneglob ) else : deplist . append ( dep ) target [ 1 ] [ 'dependencies' ] = list ( deplist ) for dep in target [ 1 ] [ 'dependencies' ] : sprint ( " - {}" . format ( dep ) , level = "verbose" ) all_files . append ( dep ) if 'output' in target [ 1 ] : sprint ( "It has outputs" , level = "verbose" ) for out in acts . get_all_outputs ( target [ 1 ] ) : sprint ( " - {}" . format ( out ) , level = "verbose" ) all_files . append ( out ) if len ( all_files ) : sha_dict [ 'files' ] = { } # check if files exist and de-dupe extant_files = [ ] for item in all_files : if item not in extant_files and os . path . isfile ( item ) : extant_files . append ( item ) pool = Pool ( ) results = pool . map ( get_sha , extant_files ) pool . close ( ) pool . join ( ) for fn , sha in zip ( extant_files , results ) : sha_dict [ 'files' ] [ fn ] = { 'sha' : sha } return sha_dict sprint ( "No dependencies" , level = "verbose" )
9395	def _set_scores ( self ) : anom_scores = { } self . _compute_derivatives ( ) derivatives_ema = utils . compute_ema ( self . smoothing_factor , self . derivatives ) for i , ( timestamp , value ) in enumerate ( self . time_series_items ) : anom_scores [ timestamp ] = abs ( self . derivatives [ i ] - derivatives_ema [ i ] ) stdev = numpy . std ( anom_scores . values ( ) ) if stdev : for timestamp in anom_scores . keys ( ) : anom_scores [ timestamp ] /= stdev self . anom_scores = TimeSeries ( self . _denoise_scores ( anom_scores ) )
9592	def set_window_position ( self , x , y , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_POSITION , { 'x' : int ( x ) , 'y' : int ( y ) , 'window_handle' : window_handle } )
273	def to_utc ( df ) : try : df . index = df . index . tz_localize ( 'UTC' ) except TypeError : df . index = df . index . tz_convert ( 'UTC' ) return df
6000	def pix_to_regular ( self ) : pix_to_regular = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . regular_to_pix ) : pix_to_regular [ pix_pixel ] . append ( regular_pixel ) return pix_to_regular
13533	def descendents ( self ) : visited = set ( [ ] ) self . _depth_descend ( self , visited ) try : visited . remove ( self ) except KeyError : # we weren't descendent of ourself, that's ok pass return list ( visited )
7179	def lib2to3_unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype_hgext import apply_job_security code = apply_job_security ( code ) return code
12372	def chop ( list_ , n ) : # could look into itertools also, might be implemented there size = len ( list_ ) each = size // n if each == 0 : return [ list_ ] chopped = [ ] for i in range ( n ) : start = i * each end = ( i + 1 ) * each if i == ( n - 1 ) : # make sure we get all items, let last worker do a litte more end = size chopped . append ( list_ [ start : end ] ) return chopped
10074	def fetch_published ( self ) : pid_type = self [ '_deposit' ] [ 'pid' ] [ 'type' ] pid_value = self [ '_deposit' ] [ 'pid' ] [ 'value' ] resolver = Resolver ( pid_type = pid_type , object_type = 'rec' , getter = partial ( self . published_record_class . get_record , with_deleted = True ) ) return resolver . resolve ( pid_value )
13259	def combine ( self , members , output_file , dimension = None , start_index = None , stop_index = None , stride = None ) : nco = None try : nco = Nco ( ) except BaseException : # This is not necessarily an import error (could be wrong PATH) raise ImportError ( "NCO not found. The NCO python bindings are required to use 'Collection.combine'." ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : # A member DotDoct was passed in, we only need the paths members = [ m . path for m in members ] options = [ '-4' ] # NetCDF4 options += [ '-L' , '3' ] # Level 3 compression options += [ '-h' ] # Don't append to the history global attribute if dimension is not None : if start_index is None : start_index = 0 if stop_index is None : stop_index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start_index , stop_index , stride ) ] nco . ncrcat ( input = members , output = output_file , options = options )
3625	def encode ( latitude , longitude , precision = 12 ) : lat_interval , lon_interval = ( - 90.0 , 90.0 ) , ( - 180.0 , 180.0 ) geohash = [ ] bits = [ 16 , 8 , 4 , 2 , 1 ] bit = 0 ch = 0 even = True while len ( geohash ) < precision : if even : mid = ( lon_interval [ 0 ] + lon_interval [ 1 ] ) / 2 if longitude > mid : ch |= bits [ bit ] lon_interval = ( mid , lon_interval [ 1 ] ) else : lon_interval = ( lon_interval [ 0 ] , mid ) else : mid = ( lat_interval [ 0 ] + lat_interval [ 1 ] ) / 2 if latitude > mid : ch |= bits [ bit ] lat_interval = ( mid , lat_interval [ 1 ] ) else : lat_interval = ( lat_interval [ 0 ] , mid ) even = not even if bit < 4 : bit += 1 else : geohash += __base32 [ ch ] bit = 0 ch = 0 return '' . join ( geohash )
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : # chrome <=55 cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : # chrome >=56 cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
10418	def get_variants_to_controllers ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants_of ( graph , node , modifications ) for controller , variant , data in graph . in_edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL_RELATIONS : rv [ variant ] . add ( controller ) return rv
11354	def record_add_field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield_value = '' ) : if controlfield_value : doc = etree . Element ( "controlfield" , attrib = { "tag" : tag , } ) doc . text = unicode ( controlfield_value ) else : doc = etree . Element ( "datafield" , attrib = { "tag" : tag , "ind1" : ind1 , "ind2" : ind2 , } ) for code , value in subfields : field = etree . SubElement ( doc , "subfield" , attrib = { "code" : code } ) field . text = value rec . append ( doc ) return rec
4095	def AICc ( N , rho , k , norm = True ) : from numpy import log , array p = k #todo check convention. agrees with octave res = log ( rho ) + 2. * ( p + 1 ) / ( N - p - 2 ) return res
5233	def all_folders ( path_name , keyword = '' , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword : folders = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/*{keyword}*' ) if os . path . isdir ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : folders = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isdir ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : folders = filter_by_dates ( folders , date_fmt = date_fmt ) return folders
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
5945	def convert_aa_code ( x ) : if len ( x ) == 1 : return amino_acid_codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse_aa_codes [ x . upper ( ) ] else : raise ValueError ( "Can only convert 1-letter or 3-letter amino acid codes, " "not %r" % x )
3068	def wrap_http_for_auth ( credentials , http ) : orig_request_method = http . request # The closure that will replace 'httplib2.Http.request'. def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if not credentials . access_token : _LOGGER . info ( 'Attempting refresh to obtain ' 'initial access_token' ) credentials . _refresh ( orig_request_method ) # Clone and modify the request headers to add the appropriate # Authorization header. headers = _initialize_headers ( headers ) credentials . apply ( headers ) _apply_user_agent ( headers , credentials . user_agent ) body_stream_position = None # Check if the body is a file-like stream. if all ( getattr ( body , stream_prop , None ) for stream_prop in _STREAM_PROPERTIES ) : body_stream_position = body . tell ( ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) # A stored token may expire between the time it is retrieved and # the time the request is made, so we may need to try twice. max_refresh_attempts = 2 for refresh_attempt in range ( max_refresh_attempts ) : if resp . status not in REFRESH_STATUS_CODES : break _LOGGER . info ( 'Refreshing due to a %s (attempt %s/%s)' , resp . status , refresh_attempt + 1 , max_refresh_attempts ) credentials . _refresh ( orig_request_method ) credentials . apply ( headers ) if body_stream_position is not None : body . seek ( body_stream_position ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) return resp , content # Replace the request method with our own closure. http . request = new_request # Set credentials as a property of the request method. http . request . credentials = credentials
12102	def _launch_process_group ( self , process_commands , streams_path ) : processes = { } def check_complete_processes ( wait = False ) : """ Returns True if a process completed, False otherwise. Optionally allows waiting for better performance (avoids sleep-poll cycle if possible). """ result = False # list creates copy of keys, as dict is modified in loop for proc in list ( processes ) : if wait : proc . wait ( ) if proc . poll ( ) is not None : # process is done, free up slot self . debug ( "Process %d exited with code %d." % ( processes [ proc ] [ 'tid' ] , proc . poll ( ) ) ) processes [ proc ] [ 'stdout' ] . close ( ) processes [ proc ] [ 'stderr' ] . close ( ) del processes [ proc ] result = True return result for cmd , tid in process_commands : self . debug ( "Starting process %d..." % tid ) job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_handle = open ( os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) , "wb" ) stderr_handle = open ( os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) , "wb" ) proc = subprocess . Popen ( cmd , stdout = stdout_handle , stderr = stderr_handle ) processes [ proc ] = { 'tid' : tid , 'stdout' : stdout_handle , 'stderr' : stderr_handle } if self . max_concurrency : # max_concurrency reached, wait until more slots available while len ( processes ) >= self . max_concurrency : if not check_complete_processes ( len ( processes ) == 1 ) : time . sleep ( 0.1 ) # Wait for all processes to complete while len ( processes ) > 0 : if not check_complete_processes ( True ) : time . sleep ( 0.1 )
11062	def send_im ( self , user , text ) : if isinstance ( user , SlackUser ) : user = user . id channelid = self . _find_im_channel ( user ) else : channelid = user . id self . send_message ( channelid , text )
10729	def _handle_base_case ( klass , symbol ) : def the_func ( value , variant = 0 ) : """ Base case. :param int variant: variant level for this object :returns: a tuple of a dbus object and the variant level :rtype: dbus object * int """ ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( 0 , variant ) return ( klass ( value , variant_level = obj_level ) , func_level ) return lambda : ( the_func , symbol )
3664	def calculate ( self , T , method ) : if method == PERRY151 : Cp = ( self . PERRY151_const + self . PERRY151_lin * T + self . PERRY151_quadinv / T ** 2 + self . PERRY151_quad * T ** 2 ) * calorie elif method == CRCSTD : Cp = self . CRCSTD_Cp elif method == LASTOVKA_S : Cp = Lastovka_solid ( T , self . similarity_variable ) Cp = property_mass_to_molar ( Cp , self . MW ) elif method in self . tabular_data : Cp = self . interpolate ( T , method ) return Cp
9256	def issues_to_log ( self , issues , pull_requests ) : log = "" sections_a , issues_a = self . parse_by_sections ( issues , pull_requests ) for section , s_issues in sections_a . items ( ) : log += self . generate_sub_section ( s_issues , section ) log += self . generate_sub_section ( issues_a , self . options . issue_prefix ) return log
13191	def json_struct_to_xml ( json_obj , root , custom_namespace = None ) : if isinstance ( root , ( str , unicode ) ) : if root . startswith ( '!' ) : root = etree . Element ( '{%s}%s' % ( NS_PROTECTED , root [ 1 : ] ) ) elif root . startswith ( '+' ) : if not custom_namespace : raise Exception ( "JSON fields starts with +, but no custom namespace provided" ) root = etree . Element ( '{%s}%s' % ( custom_namespace , root [ 1 : ] ) ) else : root = etree . Element ( root ) if root . tag in ( 'attachments' , 'grouped_events' , 'media_files' ) : for link in json_obj : root . append ( json_link_to_xml ( link ) ) elif isinstance ( json_obj , ( str , unicode ) ) : root . text = json_obj elif isinstance ( json_obj , ( int , float ) ) : root . text = unicode ( json_obj ) elif isinstance ( json_obj , dict ) : if frozenset ( json_obj . keys ( ) ) == frozenset ( ( 'type' , 'coordinates' ) ) : root . append ( geojson_to_gml ( json_obj ) ) else : for key , val in json_obj . items ( ) : if key == 'url' or key . endswith ( '_url' ) : el = json_link_to_xml ( val , json_link_key_to_xml_rel ( key ) ) else : el = json_struct_to_xml ( val , key , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif isinstance ( json_obj , list ) : tag_name = root . tag if tag_name . endswith ( 'ies' ) : tag_name = tag_name [ : - 3 ] + 'y' elif tag_name . endswith ( 's' ) : tag_name = tag_name [ : - 1 ] for val in json_obj : el = json_struct_to_xml ( val , tag_name , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif json_obj is None : return None else : raise NotImplementedError return root
2082	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : uj_res = get_resource ( 'unified_job' ) # Filters # - limit search to jobs spawned as part of this workflow job # - order in the order in which they should add to the list # - only include final job states query_params = ( ( 'unified_job_node__workflow_job' , pk ) , ( 'order_by' , 'finished' ) , ( 'status__in' , 'successful,failed,error' ) ) jobs_list = uj_res . list ( all_pages = True , query = query_params ) if jobs_list [ 'count' ] == 0 : return '' return_content = ResSubcommand ( uj_res ) . _format_human ( jobs_list ) lines = return_content . split ( '\n' ) if not full : lines = lines [ : - 1 ] N = len ( lines ) start_range = start_line if start_line is None : start_range = 0 elif start_line > N : start_range = N end_range = end_line if end_line is None or end_line > N : end_range = N lines = lines [ start_range : end_range ] return_content = '\n' . join ( lines ) if len ( lines ) > 0 : return_content += '\n' return return_content
11951	def _import_config ( config_file ) : # get config file path jocker_lgr . debug ( 'config file is: {0}' . format ( config_file ) ) # append to path for importing try : jocker_lgr . debug ( 'importing config...' ) with open ( config_file , 'r' ) as c : return yaml . safe_load ( c . read ( ) ) except IOError as ex : jocker_lgr . error ( str ( ex ) ) raise RuntimeError ( 'cannot access config file' ) except yaml . parser . ParserError as ex : jocker_lgr . error ( 'invalid yaml file: {0}' . format ( ex ) ) raise RuntimeError ( 'invalid yaml file' )
10579	def add_to ( self , other ) : # Add another package. if type ( other ) is MaterialPackage : # Packages of the same material. if self . material == other . material : self . compound_masses += other . compound_masses # Packages of different materials. else : for compound in other . material . compounds : if compound not in self . material . compounds : raise Exception ( "Packages of '" + other . material . name + "' cannot be added to packages of '" + self . material . name + "'. The compound '" + compound + "' was not found in '" + self . material . name + "'." ) self . add_to ( ( compound , other . get_compound_mass ( compound ) ) ) # Add the specified mass of the specified compound. elif self . _is_compound_mass_tuple ( other ) : # Added material variables. compound = other [ 0 ] compound_index = self . material . get_compound_index ( compound ) mass = other [ 1 ] # Create the result package. self . compound_masses [ compound_index ] += mass # If not one of the above, it must be an invalid argument. else : raise TypeError ( 'Invalid addition argument.' )
6360	def needleman_wunsch ( src , tar , gap_cost = 1 , sim_func = sim_ident ) : return NeedlemanWunsch ( ) . dist_abs ( src , tar , gap_cost , sim_func )
11406	def records_identical ( rec1 , rec2 , skip_005 = True , ignore_field_order = False , ignore_subfield_order = False , ignore_duplicate_subfields = False , ignore_duplicate_controlfields = False ) : rec1_keys = set ( rec1 . keys ( ) ) rec2_keys = set ( rec2 . keys ( ) ) if skip_005 : rec1_keys . discard ( "005" ) rec2_keys . discard ( "005" ) if rec1_keys != rec2_keys : return False for key in rec1_keys : if ignore_duplicate_controlfields and key . startswith ( '00' ) : if set ( field [ 3 ] for field in rec1 [ key ] ) != set ( field [ 3 ] for field in rec2 [ key ] ) : return False continue rec1_fields = rec1 [ key ] rec2_fields = rec2 [ key ] if len ( rec1_fields ) != len ( rec2_fields ) : # They already differs in length... return False if ignore_field_order : # We sort the fields, first by indicators and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) else : # We sort the fields, first by indicators, then by global position # and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) for field1 , field2 in zip ( rec1_fields , rec2_fields ) : if ignore_duplicate_subfields : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or set ( field1 [ 0 ] ) != set ( field2 [ 0 ] ) : return False elif ignore_subfield_order : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or sorted ( field1 [ 0 ] ) != sorted ( field2 [ 0 ] ) : return False elif field1 [ : 4 ] != field2 [ : 4 ] : return False return True
6719	def what_requires ( self , name ) : r = self . local_renderer r . env . name = name r . local ( 'pipdeptree -p {name} --reverse' )
12022	def check_parent_boundary ( self ) : for line in self . lines : for parent_feature in line [ 'parents' ] : ok = False for parent_line in parent_feature : if parent_line [ 'start' ] <= line [ 'start' ] and line [ 'end' ] <= parent_line [ 'end' ] : ok = True break if not ok : self . add_line_error ( line , { 'message' : 'This feature is not contained within the feature boundaries of parent: {0:s}: {1:s}' . format ( parent_feature [ 0 ] [ 'attributes' ] [ 'ID' ] , ',' . join ( [ '({0:s}, {1:d}, {2:d})' . format ( line [ 'seqid' ] , line [ 'start' ] , line [ 'end' ] ) for line in parent_feature ] ) ) , 'error_type' : 'BOUNDS' , 'location' : 'parent_boundary' } )
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : # A tenant can't delete subnets on provider network raise n_exc . NotAuthorized ( subnet_id = id ) else : # Raise a NotFound here because the foreign tenant # does not have to know about other tenant's subnet # existence. raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
6082	def deflections_of_galaxies_from_grid ( grid , galaxies ) : if len ( galaxies ) > 0 : deflections = sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( grid ) , galaxies ) ) else : deflections = np . full ( ( grid . shape [ 0 ] , 2 ) , 0.0 ) if isinstance ( grid , grids . SubGrid ) : return np . asarray ( [ grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 0 ] ) , grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 1 ] ) ] ) . T return deflections
5702	def route_frequencies ( gtfs , results_by_mode = False ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT f.route_I, type, frequency FROM routes as r" " JOIN" " (SELECT route_I, COUNT(route_I) as frequency" " FROM" " (SELECT date, route_I, trip_I" " FROM day_stop_times" " WHERE date = '{day}'" " GROUP by route_I, trip_I)" " GROUP BY route_I) as f" " ON f.route_I = r.route_I" " ORDER BY frequency DESC" . format ( day = day ) ) return pd . DataFrame ( gtfs . execute_custom_query_pandas ( query ) )
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] # if you don't provide a version number, only the latest # will be included in the response body if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
12652	def generate_config ( output_directory ) : if not op . isdir ( output_directory ) : os . makedirs ( output_directory ) config_file = op . join ( output_directory , "config.ini" ) open_file = open ( config_file , "w" ) open_file . write ( "[BOOL]\nManualNIfTIConv=0\n" ) open_file . close ( ) return config_file
11847	def delete_thing ( self , thing ) : try : self . things . remove ( thing ) except ValueError , e : print e print " in Environment delete_thing" print " Thing to be removed: %s at %s" % ( thing , thing . location ) print " from list: %s" % [ ( thing , thing . location ) for thing in self . things ] if thing in self . agents : self . agents . remove ( thing )
10861	def param_particle_pos ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' ] ]
3207	def delete ( self , batch_id ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _delete ( url = self . _build_path ( batch_id ) )
104	def pad_to_aspect_ratio ( arr , aspect_ratio , mode = "constant" , cval = 0 , return_pad_amounts = False ) : pad_top , pad_right , pad_bottom , pad_left = compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) arr_padded = pad ( arr , top = pad_top , right = pad_right , bottom = pad_bottom , left = pad_left , mode = mode , cval = cval ) if return_pad_amounts : return arr_padded , ( pad_top , pad_right , pad_bottom , pad_left ) else : return arr_padded
11735	def sanitize_path ( path ) : if path == '/' : # Nothing to do, just return return path if path [ : 1 ] != '/' : raise InvalidPath ( 'The path must start with a slash' ) # Deduplicate slashes in path path = re . sub ( r'/+' , '/' , path ) # Strip trailing slashes and return return path . rstrip ( '/' )
11580	def _string_data ( self , data ) : print ( "_string_data:" ) string_to_print = [ ] for i in data [ : : 2 ] : string_to_print . append ( chr ( i ) ) print ( "" . join ( string_to_print ) )
1879	def VEXTRACTF128 ( cpu , dest , src , offset ) : offset = offset . read ( ) dest . write ( Operators . EXTRACT ( src . read ( ) , offset * 128 , ( offset + 1 ) * 128 ) )
7642	def _conversion ( target , source ) : def register ( func ) : '''This decorator registers func as mapping source to target''' __CONVERSION__ [ target ] [ source ] = func return func return register
177	def concatenate ( self , other ) : if not isinstance ( other , LineString ) : other = LineString ( other ) return self . deepcopy ( coords = np . concatenate ( [ self . coords , other . coords ] , axis = 0 ) )
2718	def remove_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] # Extracting data from the Droplet object resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __remove_resources ( resources ) return False
2388	def spell_correct ( string ) : # Create a temp file so that aspell could be used # By default, tempfile will delete this file when the file handle is closed. f = tempfile . NamedTemporaryFile ( mode = 'w' ) f . write ( string ) f . flush ( ) f_path = os . path . abspath ( f . name ) try : p = os . popen ( aspell_path + " -a < " + f_path + " --sug-mode=ultra" ) # Aspell returns a list of incorrect words with the above flags incorrect = p . readlines ( ) p . close ( ) except Exception : log . exception ( "aspell process failed; could not spell check" ) # Return original string if aspell fails return string , 0 , string finally : f . close ( ) incorrect_words = list ( ) correct_spelling = list ( ) for i in range ( 1 , len ( incorrect ) ) : if ( len ( incorrect [ i ] ) > 10 ) : #Reformat aspell output to make sense match = re . search ( ":" , incorrect [ i ] ) if hasattr ( match , "start" ) : begstring = incorrect [ i ] [ 2 : match . start ( ) ] begmatch = re . search ( " " , begstring ) begword = begstring [ 0 : begmatch . start ( ) ] sugstring = incorrect [ i ] [ match . start ( ) + 2 : ] sugmatch = re . search ( "," , sugstring ) if hasattr ( sugmatch , "start" ) : sug = sugstring [ 0 : sugmatch . start ( ) ] incorrect_words . append ( begword ) correct_spelling . append ( sug ) #Create markup based on spelling errors newstring = string markup_string = string already_subbed = [ ] for i in range ( 0 , len ( incorrect_words ) ) : sub_pat = r"\b" + incorrect_words [ i ] + r"\b" sub_comp = re . compile ( sub_pat ) newstring = re . sub ( sub_comp , correct_spelling [ i ] , newstring ) if incorrect_words [ i ] not in already_subbed : markup_string = re . sub ( sub_comp , '<bs>' + incorrect_words [ i ] + "</bs>" , markup_string ) already_subbed . append ( incorrect_words [ i ] ) return newstring , len ( incorrect_words ) , markup_string
11435	def _validate_record_field_positions_global ( record ) : all_fields = [ ] for tag , fields in record . items ( ) : previous_field_position_global = - 1 for field in fields : if field [ 4 ] < previous_field_position_global : return ( "Non ascending global field positions in tag '%s'." % tag ) previous_field_position_global = field [ 4 ] if field [ 4 ] in all_fields : return ( "Duplicate global field position '%d' in tag '%s'" % ( field [ 4 ] , tag ) )
2852	def mpsse_set_clock ( self , clock_hz , adaptive = False , three_phase = False ) : # Disable clock divisor by 5 to enable faster speeds on FT232H. self . _write ( '\x8A' ) # Turn on/off adaptive clocking. if adaptive : self . _write ( '\x96' ) else : self . _write ( '\x97' ) # Turn on/off three phase clock (needed for I2C). # Also adjust the frequency for three-phase clocking as specified in section 2.2.4 # of this document: # http://www.ftdichip.com/Support/Documents/AppNotes/AN_255_USB%20to%20I2C%20Example%20using%20the%20FT232H%20and%20FT201X%20devices.pdf if three_phase : self . _write ( '\x8C' ) else : self . _write ( '\x8D' ) # Compute divisor for requested clock. # Use equation from section 3.8.1 of: # http://www.ftdichip.com/Support/Documents/AppNotes/AN_108_Command_Processor_for_MPSSE_and_MCU_Host_Bus_Emulation_Modes.pdf # Note equation is using 60mhz master clock instead of 12mhz. divisor = int ( math . ceil ( ( 30000000.0 - float ( clock_hz ) ) / float ( clock_hz ) ) ) & 0xFFFF if three_phase : divisor = int ( divisor * ( 2.0 / 3.0 ) ) logger . debug ( 'Setting clockspeed with divisor value {0}' . format ( divisor ) ) # Send command to set divisor from low and high byte values. self . _write ( str ( bytearray ( ( 0x86 , divisor & 0xFF , ( divisor >> 8 ) & 0xFF ) ) ) )
13107	def remove_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )
1941	def map_memory_callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( "Mapping Memory @" , hex ( address ) if type ( address ) is int else "0x??" , hr_size ( size ) , "-" , perms , "-" , f"{name}:{hex(offset) if name else ''}" , "->" , hex ( result ) ) ) ) self . _emu . mem_map ( address , size , convert_permissions ( perms ) ) self . copy_memory ( address , size )
12608	def _query_sample ( sample , operators = '__eq__' ) : if isinstance ( operators , str ) : operators = [ operators ] * len ( sample ) if len ( sample ) != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( sample ) , operators ) ) queries = [ ] for i , fn in enumerate ( sample ) : fv = sample [ fn ] op = operators [ i ] queries . append ( _build_query ( field_name = fn , field_value = fv , operator = op ) ) return _concat_queries ( queries , operators = '__and__' )
3258	def get_resources ( self , names = None , stores = None , workspaces = None ) : stores = self . get_stores ( names = stores , workspaces = workspaces ) resources = [ ] for s in stores : try : resources . extend ( s . get_resources ( ) ) except FailedRequestError : continue if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if resources and names : return ( [ resource for resource in resources if resource . name in names ] ) return resources
10071	def pid ( self ) : pid = self . deposit_fetcher ( self . id , self ) return PersistentIdentifier . get ( pid . pid_type , pid . pid_value )
10508	def log ( self , message , level = logging . DEBUG ) : if _ldtp_debug : print ( message ) self . logger . log ( level , str ( message ) ) return 1
6824	def restart ( self ) : n = 60 sleep_n = int ( self . env . max_restart_wait_minutes / 10. * 60 ) for _ in xrange ( n ) : self . stop ( ) if self . dryrun or not self . is_running ( ) : break print ( 'Waiting for supervisor to stop (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) self . start ( ) for _ in xrange ( n ) : if self . dryrun or self . is_running ( ) : return print ( 'Waiting for supervisor to start (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) raise Exception ( 'Failed to restart service %s!' % self . name )
3429	def add_boundary ( self , metabolite , type = "exchange" , reaction_id = None , lb = None , ub = None , sbo_term = None ) : ub = CONFIGURATION . upper_bound if ub is None else ub lb = CONFIGURATION . lower_bound if lb is None else lb types = { "exchange" : ( "EX" , lb , ub , sbo_terms [ "exchange" ] ) , "demand" : ( "DM" , 0 , ub , sbo_terms [ "demand" ] ) , "sink" : ( "SK" , lb , ub , sbo_terms [ "sink" ] ) } if type == "exchange" : external = find_external_compartment ( self ) if metabolite . compartment != external : raise ValueError ( "The metabolite is not an external metabolite" " (compartment is `%s` but should be `%s`). " "Did you mean to add a demand or sink? " "If not, either change its compartment or " "rename the model compartments to fix this." % ( metabolite . compartment , external ) ) if type in types : prefix , lb , ub , default_term = types [ type ] if reaction_id is None : reaction_id = "{}_{}" . format ( prefix , metabolite . id ) if sbo_term is None : sbo_term = default_term if reaction_id is None : raise ValueError ( "Custom types of boundary reactions require a custom " "identifier. Please set the `reaction_id`." ) if reaction_id in self . reactions : raise ValueError ( "Boundary reaction '{}' already exists." . format ( reaction_id ) ) name = "{} {}" . format ( metabolite . name , type ) rxn = Reaction ( id = reaction_id , name = name , lower_bound = lb , upper_bound = ub ) rxn . add_metabolites ( { metabolite : - 1 } ) if sbo_term : rxn . annotation [ "sbo" ] = sbo_term self . add_reactions ( [ rxn ] ) return rxn
5691	def _check_dep_time_is_valid ( self , dep_time ) : assert dep_time <= self . _min_dep_time , "Labels should be entered in decreasing order of departure time." dep_time_index = self . dep_times_to_index [ dep_time ] if self . _min_dep_time < float ( 'inf' ) : min_dep_index = self . dep_times_to_index [ self . _min_dep_time ] assert min_dep_index == dep_time_index or ( min_dep_index == dep_time_index - 1 ) , "dep times should be ordered sequentially" else : assert dep_time_index is 0 , "first dep_time index should be zero (ensuring that all connections are properly handled)" self . _min_dep_time = dep_time
12253	def _delete_key_internal ( self , * args , * * kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . _delete_key_internal ( * args , * * kwargs )
6401	def _undouble ( self , word ) : if ( len ( word ) > 1 and word [ - 1 ] == word [ - 2 ] and word [ - 1 ] in { 'd' , 'k' , 't' } ) : return word [ : - 1 ] return word
10377	def one_sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
4515	def drawLine ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None , aa = False ) : md . draw_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc , aa )
4142	def _numpy_cholesky ( A , B ) : L = numpy . linalg . cholesky ( A ) # A=L*numpy.transpose(L).conjugate() # Ly = b y = numpy . linalg . solve ( L , B ) # Ux = y x = numpy . linalg . solve ( L . transpose ( ) . conjugate ( ) , y ) return x , L
166	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( self . coords , from_shape , to_shape ) return self . copy ( coords = coords_proj )
5520	def check_codes ( self , expected_codes , received_code , info ) : if not any ( map ( received_code . matches , expected_codes ) ) : raise errors . StatusCodeError ( expected_codes , received_code , info )
9992	def _new_dynspace ( self , name = None , bases = None , formula = None , refs = None , arguments = None , source = None , ) : if name is None : name = self . spacenamer . get_next ( self . namespace ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = RootDynamicSpaceImpl ( parent = self , name = name , formula = formula , refs = refs , source = source , arguments = arguments , ) space . is_derived = False self . _set_space ( space ) if bases : # i.e. not [] dynbase = self . _get_dynamic_base ( bases ) space . _dynbase = dynbase dynbase . _dynamic_subs . append ( space ) return space
12536	def get_dcm_reader ( store_metadata = True , header_fields = None ) : if not store_metadata : return lambda fpath : fpath if header_fields is None : build_dcm = lambda fpath : DicomFile ( fpath ) else : dicom_header = namedtuple ( 'DicomHeader' , header_fields ) build_dcm = lambda fpath : dicom_header . _make ( DicomFile ( fpath ) . get_attributes ( header_fields ) ) return build_dcm
13086	def get ( self , section , key ) : try : return self . config . get ( section , key ) except configparser . NoSectionError : pass except configparser . NoOptionError : pass return self . defaults [ section ] [ key ]
4426	async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
5612	def prepare_array ( data , masked = True , nodata = 0 , dtype = "int16" ) : # input is iterable if isinstance ( data , ( list , tuple ) ) : return _prepare_iterable ( data , masked , nodata , dtype ) # special case if a 2D single band is provided elif isinstance ( data , np . ndarray ) and data . ndim == 2 : data = ma . expand_dims ( data , axis = 0 ) # input is a masked array if isinstance ( data , ma . MaskedArray ) : return _prepare_masked ( data , masked , nodata , dtype ) # input is a NumPy array elif isinstance ( data , np . ndarray ) : if masked : return ma . masked_values ( data . astype ( dtype , copy = False ) , nodata , copy = False ) else : return data . astype ( dtype , copy = False ) else : raise ValueError ( "data must be array, masked array or iterable containing arrays." )
12606	def search_unique ( table , sample , unique_fields = None ) : if unique_fields is None : unique_fields = list ( sample . keys ( ) ) query = _query_data ( sample , field_names = unique_fields , operators = '__eq__' ) items = table . search ( query ) if len ( items ) == 1 : return items [ 0 ] if len ( items ) == 0 : return None raise MoreThanOneItemError ( 'Expected to find zero or one items, but found ' '{} items.' . format ( len ( items ) ) )
9816	def check ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) check = False if self . is_kubernetes : check = self . check_for_kubernetes ( ) elif self . is_docker_compose : check = self . check_for_docker_compose ( ) elif self . is_docker : check = self . check_for_docker ( ) elif self . is_heroku : check = self . check_for_heroku ( ) if not check : raise PolyaxonDeploymentConfigError ( 'Deployment `{}` is not valid' . format ( self . deployment_type ) )
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
9328	def get ( self , url , headers = None , params = None ) : merged_headers = self . _merge_headers ( headers ) if "Accept" not in merged_headers : merged_headers [ "Accept" ] = MEDIA_TYPE_TAXII_V20 accept = merged_headers [ "Accept" ] resp = self . session . get ( url , headers = merged_headers , params = params ) resp . raise_for_status ( ) content_type = resp . headers [ "Content-Type" ] if not self . valid_content_type ( content_type = content_type , accept = accept ) : msg = "Unexpected Response. Got Content-Type: '{}' for Accept: '{}'" raise TAXIIServiceException ( msg . format ( content_type , accept ) ) return _to_json ( resp )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
8814	def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
944	def getCheckpointParentDir ( experimentDir ) : baseDir = os . path . join ( experimentDir , "savedmodels" ) baseDir = os . path . abspath ( baseDir ) return baseDir
9349	def date ( past = False , min_delta = 0 , max_delta = 20 ) : timedelta = dt . timedelta ( days = _delta ( past , min_delta , max_delta ) ) return dt . date . today ( ) + timedelta
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
2426	def set_doc_spdx_id ( self , doc , doc_spdx_id_line ) : if not self . doc_spdx_id_set : if doc_spdx_id_line == 'SPDXRef-DOCUMENT' : doc . spdx_id = doc_spdx_id_line self . doc_spdx_id_set = True return True else : raise SPDXValueError ( 'Document::SPDXID' ) else : raise CardinalityError ( 'Document::SPDXID' )
5136	def get_class_traits ( klass ) : # FIXME: gracefully handle errors here or in the caller? source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : # FIXME: handle other kinds of assignments? if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
8184	def update ( self , iterations = 10 ) : # The graph fades in when initially constructed. self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) # Iterates over the graph's layout. # Each step the graph's bounds are recalculated # and a number of iterations are processed, # more and more as the layout progresses. if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) # Calculate the absolute center of the graph. min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
6503	def find_matches ( strings , words , length_hoped ) : lower_words = [ w . lower ( ) for w in words ] def has_match ( string ) : """ Do any of the words match within the string """ lower_string = string . lower ( ) for test_word in lower_words : if test_word in lower_string : return True return False shortened_strings = [ textwrap . wrap ( s ) for s in strings ] short_string_list = list ( chain . from_iterable ( shortened_strings ) ) matches = [ ms for ms in short_string_list if has_match ( ms ) ] cumulative_len = 0 break_at = None for idx , match in enumerate ( matches ) : cumulative_len += len ( match ) if cumulative_len >= length_hoped : break_at = idx break return matches [ 0 : break_at ]
7687	def multi_segment ( annotation , sr = 22050 , length = None , * * kwargs ) : # Pentatonic scale, because why not PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
11652	def get_version ( self ) : if ( self . name is not None and self . version is not None and self . version . startswith ( ":versiontools:" ) ) : return ( self . __get_live_version ( ) or self . __get_frozen_version ( ) or self . __fail_to_get_any_version ( ) ) else : return self . __base . get_version ( self )
1178	def sub ( self , repl , string , count = 0 ) : return self . _subx ( repl , string , count , False )
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) # check the number of lines in the input xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None # turn the input into a param dict params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done # we won't wait for the LC ZIP to complete if email_when_done = True if email_when_done : download_data = False # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # hit the server api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) # check the status of the search status = searchresult [ 0 ] # now we'll check if we want to download the data if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
4614	def awaitTxConfirmation ( self , transaction , limit = 10 ) : counter = 10 for block in self . blocks ( ) : counter += 1 for tx in block [ "transactions" ] : if sorted ( tx [ "signatures" ] ) == sorted ( transaction [ "signatures" ] ) : return tx if counter > limit : raise Exception ( "The operation has not been added after 10 blocks!" )
9371	def password ( at_least = 6 , at_most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at_least = at_least , at_most = at_most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
4144	def CHOLESKY ( A , B , method = 'scipy' ) : if method == 'numpy_solver' : X = _numpy_solver ( A , B ) return X elif method == 'numpy' : X , _L = _numpy_cholesky ( A , B ) return X elif method == 'scipy' : import scipy . linalg L = scipy . linalg . cholesky ( A ) X = scipy . linalg . cho_solve ( ( L , False ) , B ) else : raise ValueError ( 'method must be numpy_solver, numpy_cholesky or cholesky_inplace' ) return X
323	def gen_drawdown_table ( returns , top = 10 ) : df_cum = ep . cum_returns ( returns , 1.0 ) drawdown_periods = get_top_drawdowns ( returns , top = top ) df_drawdowns = pd . DataFrame ( index = list ( range ( top ) ) , columns = [ 'Net drawdown in %' , 'Peak date' , 'Valley date' , 'Recovery date' , 'Duration' ] ) for i , ( peak , valley , recovery ) in enumerate ( drawdown_periods ) : if pd . isnull ( recovery ) : df_drawdowns . loc [ i , 'Duration' ] = np . nan else : df_drawdowns . loc [ i , 'Duration' ] = len ( pd . date_range ( peak , recovery , freq = 'B' ) ) df_drawdowns . loc [ i , 'Peak date' ] = ( peak . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Valley date' ] = ( valley . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) if isinstance ( recovery , float ) : df_drawdowns . loc [ i , 'Recovery date' ] = recovery else : df_drawdowns . loc [ i , 'Recovery date' ] = ( recovery . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Net drawdown in %' ] = ( ( df_cum . loc [ peak ] - df_cum . loc [ valley ] ) / df_cum . loc [ peak ] ) * 100 df_drawdowns [ 'Peak date' ] = pd . to_datetime ( df_drawdowns [ 'Peak date' ] ) df_drawdowns [ 'Valley date' ] = pd . to_datetime ( df_drawdowns [ 'Valley date' ] ) df_drawdowns [ 'Recovery date' ] = pd . to_datetime ( df_drawdowns [ 'Recovery date' ] ) return df_drawdowns
5941	def transform_args ( self , * args , * * kwargs ) : newargs = self . _combineargs ( * args , * * kwargs ) return self . _build_arg_list ( * * newargs )
659	def populationStability ( vectors , numSamples = None ) : # ---------------------------------------------------------------------- # Calculate the stability numVectors = len ( vectors ) if numSamples is None : numSamples = numVectors - 1 countOn = range ( numVectors - 1 ) else : countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) sigmap = 0.0 for i in countOn : match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) # Ignore reset vectors (all 0's) if match [ 1 ] != 0 : sigmap += float ( match [ 0 ] ) / match [ 1 ] return sigmap / numSamples
7463	def depthplot ( data , samples = None , dims = ( None , None ) , canvas = ( None , None ) , xmax = 50 , log = False , outprefix = None , use_maxdepth = False ) : ## select samples to be plotted, requires depths info if not samples : samples = data . samples . keys ( ) samples . sort ( ) subsamples = OrderedDict ( [ ( i , data . samples [ i ] ) for i in samples ] ) ## get canvas dimensions based on n-samples if any ( dims ) : ## user-supplied dimensions (...) print ( "userdims" ) else : if len ( subsamples ) <= 4 : ## set dimension to N samples dims = ( 1 , len ( subsamples ) ) else : dims = ( len ( subsamples ) / 4 , 4 ) ## create canvas if any ( canvas ) : print ( "usercanvas" ) canvas = toyplot . Canvas ( width = canvas [ 0 ] , height = canvas [ 1 ] ) else : canvas = toyplot . Canvas ( width = 200 * dims [ 1 ] , height = 150 * dims [ 0 ] ) ## get all of the data arrays for panel , sample in enumerate ( subsamples ) : ## statistical called bins statdat = subsamples [ sample ] . depths statdat = statdat [ statdat >= data . paramsdict [ "mindepth_statistical" ] ] if use_maxdepth : statdat = { i : j for ( i , j ) in statdat if i < data . paramsdict [ "maxdepth" ] } sdat = np . histogram ( statdat , range ( 50 ) ) ## majrule called bins statdat = subsamples [ sample ] . depths statdat = statdat [ statdat < data . paramsdict [ "mindepth_statistical" ] ] statdat = statdat [ statdat >= data . paramsdict [ "mindepth_majrule" ] ] if use_maxdepth : statdat = statdat [ statdat < data . paramsdict [ "maxdepth" ] ] mdat = np . histogram ( statdat , range ( 50 ) ) ## excluded bins tots = data . samples [ sample ] . depths tots = tots [ tots < data . paramsdict [ "mindepth_majrule" ] ] if use_maxdepth : tots = tots [ tots < data . paramsdict [ "maxdepth" ] ] edat = np . histogram ( tots , range ( 50 ) ) ## fill in each panel of canvas with a sample axes = canvas . cartesian ( grid = ( dims [ 0 ] , dims [ 1 ] , panel ) , gutter = 25 ) axes . x . domain . xmax = xmax axes . label . text = sample if log : axes . y . scale = "log" # heights = np.column_stack((sdat,mdat,edat)) axes . bars ( sdat ) axes . bars ( edat ) axes . bars ( mdat ) ## return objects to be saved... if outprefix : toyplot . html . render ( canvas , fobj = outprefix + ".html" ) toyplot . svg . render ( canvas , fobj = outprefix + ".svg" )
5244	def missing_info ( * * kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , '_' ) info = utils . to_str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
2983	def cmd_events ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . json : outf = None _write = puts if opts . output is not None : outf = open ( opts . output , "w" ) _write = outf . write try : delim = "" logs = b . get_audit ( ) . read_logs ( as_json = False ) _write ( '{"events": [' ) _write ( os . linesep ) for l in logs : _write ( delim + l ) delim = "," + os . linesep _write ( os . linesep ) _write ( ']}' ) finally : if opts . output is not None : outf . close ( ) else : puts ( colored . blue ( columns ( [ "EVENT" , 10 ] , [ "TARGET" , 16 ] , [ "STATUS" , 8 ] , [ "TIME" , 16 ] , [ "MESSAGE" , 25 ] ) ) ) logs = b . get_audit ( ) . read_logs ( as_json = True ) for l in logs : puts ( columns ( [ l [ 'event' ] , 10 ] , [ str ( [ str ( t ) for t in l [ 'targets' ] ] ) , 16 ] , [ l [ 'status' ] , 8 ] , [ str ( l [ 'timestamp' ] ) , 16 ] , [ l [ 'message' ] , 25 ] ) )
864	def makeDirectoryFromAbsolutePath ( absDirPath ) : assert os . path . isabs ( absDirPath ) try : os . makedirs ( absDirPath ) except OSError , e : if e . errno != os . errno . EEXIST : raise return absDirPath
453	def get_variables_with_name ( name = None , train_only = True , verbose = False ) : if name is None : raise Exception ( "please input a name" ) logging . info ( " [*] geting variables with %s" % name ) # tvar = tf.trainable_variables() if train_only else tf.all_variables() if train_only : t_vars = tf . trainable_variables ( ) else : t_vars = tf . global_variables ( ) d_vars = [ var for var in t_vars if name in var . name ] if verbose : for idx , v in enumerate ( d_vars ) : logging . info ( " got {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) return d_vars
9901	def _updateType ( self ) : data = self . _data ( ) # Change type if needed if isinstance ( data , dict ) and isinstance ( self , ListFile ) : self . __class__ = DictFile elif isinstance ( data , list ) and isinstance ( self , DictFile ) : self . __class__ = ListFile
5695	def import_ ( self , conn ) : if self . print_progress : print ( 'Beginning' , self . __class__ . __name__ ) # what is this mystical self._conn ? self . _conn = conn self . create_table ( conn ) # This does insertions if self . mode in ( 'all' , 'import' ) and self . fname and self . exists ( ) and self . table not in ignore_tables : self . insert_data ( conn ) # This makes indexes in the DB. if self . mode in ( 'all' , 'index' ) and hasattr ( self , 'index' ) : self . create_index ( conn ) # Any post-processing to be done after the full import. if self . mode in ( 'all' , 'import' ) and hasattr ( self , 'post_import' ) : self . run_post_import ( conn ) # Commit it all conn . commit ( )
4514	def fillCircle ( self , x0 , y0 , r , color = None ) : md . fill_circle ( self . set , x0 , y0 , r , color )
12249	def get_key ( self , * args , * * kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , { } ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_key ( * args , * * kwargs )
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : # always copy (even when dtype does not change) ret = np . asarray ( arr ) . astype ( dtype ) else : # load data from disk without changing order # Changing order while reading through a memmap is incredibly # inefficient. ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) # In the present cas, np.may_share_memory result is always reliable. if np . may_share_memory ( ret , arr ) and copy : # order-preserving copy ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
13351	def add_files ( self , filelist , * * kwargs ) : # check filelist is list type if not isinstance ( filelist , list ) : raise TypeError ( "request the list type." ) for file in filelist : self . add_file ( file )
2157	def _auto_help_text ( self , help_text ) : # Delete API docs if there are any. api_doc_delimiter = '=====API DOCS=====' begin_api_doc = help_text . find ( api_doc_delimiter ) if begin_api_doc >= 0 : end_api_doc = help_text . rfind ( api_doc_delimiter ) + len ( api_doc_delimiter ) help_text = help_text [ : begin_api_doc ] + help_text [ end_api_doc : ] # Convert the word "object" to the appropriate type of # object being modified (e.g. user, organization). an_prefix = ( 'a' , 'e' , 'i' , 'o' ) if not self . resource_name . lower ( ) . startswith ( an_prefix ) : help_text = help_text . replace ( 'an object' , 'a %s' % self . resource_name ) if self . resource_name . lower ( ) . endswith ( 'y' ) : help_text = help_text . replace ( 'objects' , '%sies' % self . resource_name [ : - 1 ] , ) help_text = help_text . replace ( 'object' , self . resource_name ) # Convert some common Python terms to their CLI equivalents. help_text = help_text . replace ( 'keyword argument' , 'option' ) help_text = help_text . replace ( 'raise an exception' , 'abort with an error' ) # Convert keyword arguments specified in docstrings enclosed # by backticks to switches. for match in re . findall ( r'`([\w_]+)`' , help_text ) : option = '--%s' % match . replace ( '_' , '-' ) help_text = help_text . replace ( '`%s`' % match , option ) # Done; return the new help text. return help_text
1273	def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
11463	def connect ( self ) : self . _ftp . connect ( ) self . _ftp . login ( user = self . _username , passwd = self . _passwd )
13555	def delete_shifts ( self , shifts ) : url = "/2/shifts/?%s" % urlencode ( { 'ids' : "," . join ( str ( s ) for s in shifts ) } ) data = self . _delete_resource ( url ) return data
13273	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) # Let the base class default method raise the TypeError return json . JSONEncoder ( self , obj )
8164	def inheritFromContext ( self , ignore = ( ) ) : for canvas_attr , grob_attr in STATES . items ( ) : if canvas_attr in ignore : continue setattr ( self , grob_attr , getattr ( self . _bot . _canvas , canvas_attr ) )
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
4019	def _dusty_vm_exists ( ) : existing_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'vms' ] ) for line in existing_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
7026	def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax' , 'parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : # this was generated using the awesome query generator at: # https://gea.esac.esa.int/archive/ # NOTE: here we don't resolve the table name right away. this is because # some of the GAIA mirrors use different table names, so we leave the table # name to be resolved by the lower level tap_query function. this is done by # the {{table}} construct. query = ( "select {columns}, " "(DISTANCE(POINT('ICRS', " "{{table}}.ra, {{table}}.dec), " "POINT('ICRS', {ra_center:.5f}, {decl_center:.5f})))*3600.0 " "AS dist_arcsec " "from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "CIRCLE('ICRS',{ra_center:.5f},{decl_center:.5f}," "{search_radius:.6f}))=1 " "{extra_filter_str}" "ORDER by dist_arcsec asc " ) if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( ra_center = racenter , decl_center = declcenter , search_radius = searchradiusarcsec / 3600.0 , extra_filter_str = extra_filter_str , columns = ', ' . join ( columns ) ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
12916	def prune ( self , regex = r".*" ) : return filetree ( self . root , ignore = self . ignore , regex = regex )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
2826	def convert_tanh ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting tanh ...' ) if names == 'short' : tf_name = 'TANH' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) tanh = keras . layers . Activation ( 'tanh' , name = tf_name ) layers [ scope_name ] = tanh ( layers [ inputs [ 0 ] ] )
10252	def highlight_edges ( graph : BELGraph , edges = None , color : Optional [ str ] = None ) -> None : color = color or EDGE_HIGHLIGHT_DEFAULT_COLOR for u , v , k , d in edges if edges is not None else graph . edges ( keys = True , data = True ) : graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ] = color
1539	def add_bolt ( self , name , bolt_cls , par , inputs , config = None , optional_outputs = None ) : bolt_spec = bolt_cls . spec ( name = name , par = par , inputs = inputs , config = config , optional_outputs = optional_outputs ) self . add_spec ( bolt_spec ) return bolt_spec
7139	def spend_key ( self ) : key = self . _backend . spend_key ( ) if key == numbers . EMPTY_KEY : return None return key
44	def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
13402	def selectedLogs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . logMenus ) ) : logType = self . logMenus [ i ] . selectedType ( ) log = self . logMenus [ i ] . selectedProgram ( ) if logType == "MCC" : if log not in mcclogs : mcclogs . append ( log ) elif logType == "Physics" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs
11595	def _rc_renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
13433	def showfig ( fig , aspect = "auto" ) : ax = fig . gca ( ) # Swap y axis if needed alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set_aspect ( aspect ) fig . show ( )
12733	def move_next_to ( self , body_a , body_b , offset_a , offset_b ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) if ba is None : return bb . relative_offset_to_world ( offset_b ) if bb is None : return ba . relative_offset_to_world ( offset_a ) anchor = ba . relative_offset_to_world ( offset_a ) offset = bb . relative_offset_to_world ( offset_b ) bb . position = bb . position + anchor - offset return anchor
5640	def remove_all_trips_fully_outside_buffer ( db_conn , center_lat , center_lon , buffer_km , update_secondary_data = True ) : distance_function_str = add_wgs84_distance_function_to_db ( db_conn ) stops_within_buffer_query_sql = "SELECT stop_I FROM stops WHERE CAST(" + distance_function_str + "(lat, lon, {lat} , {lon}) AS INT) < {d_m}" . format ( lat = float ( center_lat ) , lon = float ( center_lon ) , d_m = int ( 1000 * buffer_km ) ) select_all_trip_Is_where_stop_I_is_within_buffer_sql = "SELECT distinct(trip_I) FROM stop_times WHERE stop_I IN (" + stops_within_buffer_query_sql + ")" trip_Is_to_remove_sql = "SELECT trip_I FROM trips WHERE trip_I NOT IN ( " + select_all_trip_Is_where_stop_I_is_within_buffer_sql + ")" trip_Is_to_remove = pandas . read_sql ( trip_Is_to_remove_sql , db_conn ) [ "trip_I" ] . values trip_Is_to_remove_string = "," . join ( [ str ( trip_I ) for trip_I in trip_Is_to_remove ] ) remove_all_trips_fully_outside_buffer_sql = "DELETE FROM trips WHERE trip_I IN (" + trip_Is_to_remove_string + ")" remove_all_stop_times_where_trip_I_fully_outside_buffer_sql = "DELETE FROM stop_times WHERE trip_I IN (" + trip_Is_to_remove_string + ")" db_conn . execute ( remove_all_trips_fully_outside_buffer_sql ) db_conn . execute ( remove_all_stop_times_where_trip_I_fully_outside_buffer_sql ) delete_stops_not_in_stop_times_and_not_as_parent_stop ( db_conn ) db_conn . execute ( DELETE_ROUTES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) db_conn . execute ( DELETE_DAYS_ENTRIES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_DAY_TRIPS2_ENTRIES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_CALENDAR_ENTRIES_FOR_NON_REFERENCE_SERVICE_IS_SQL ) db_conn . execute ( DELETE_CALENDAR_DATES_ENTRIES_FOR_NON_REFERENCE_SERVICE_IS_SQL ) db_conn . execute ( DELETE_FREQUENCIES_ENTRIES_NOT_PRESENT_IN_TRIPS ) db_conn . execute ( DELETE_AGENCIES_NOT_REFERENCED_IN_ROUTES_SQL ) if update_secondary_data : update_secondary_data_copies ( db_conn )
509	def stripUnlearnedColumns ( self , activeArray ) : neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] activeArray [ neverLearned ] = 0
5313	def translate_style ( style , colormode , colorpalette ) : style_parts = iter ( style . split ( '_' ) ) ansi_start_sequence = [ ] ansi_end_sequence = [ ] try : # consume all modifiers part = None for mod_part in style_parts : part = mod_part if part not in ansi . MODIFIERS : break # all modifiers have been consumed mod_start_code , mod_end_code = resolve_modifier_to_ansi_code ( part , colormode ) ansi_start_sequence . append ( mod_start_code ) ansi_end_sequence . append ( mod_end_code ) else : # we've consumed all parts, thus we can exit raise StopIteration ( ) # next part has to be a foreground color or the 'on' keyword # which means we have to consume background colors if part != 'on' : ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . FOREGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) # consume the required 'on' keyword after the foreground color next ( style_parts ) # next part has to be the background color part = next ( style_parts ) ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . BACKGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) except StopIteration : # we've consumed all parts of the styling string pass # construct and return ANSI escape code sequence return '' . join ( ansi_start_sequence ) , '' . join ( ansi_end_sequence )
10563	def compare_song_collections ( src_songs , dst_songs ) : def gather_field_values ( song ) : return tuple ( ( _normalize_metadata ( song [ field ] ) for field in _filter_comparison_fields ( song ) ) ) dst_songs_criteria = { gather_field_values ( _normalize_song ( dst_song ) ) for dst_song in dst_songs } return [ src_song for src_song in src_songs if gather_field_values ( _normalize_song ( src_song ) ) not in dst_songs_criteria ]
3383	def __build_problem ( self ) : # Set up the mathematical problem prob = constraint_matrices ( self . model , zero_tol = self . feasibility_tol ) # check if there any non-zero equality constraints equalities = prob . equalities b = prob . b bounds = np . atleast_2d ( prob . bounds ) . T var_bounds = np . atleast_2d ( prob . variable_bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility_tol ) fixed_non_zero = np . abs ( prob . variable_bounds [ : , 1 ] ) > self . feasibility_tol fixed_non_zero &= prob . variable_fixed # check if there are any non-zero fixed variables, add them as # equalities to the stoichiometric matrix if any ( fixed_non_zero ) : n_fixed = fixed_non_zero . sum ( ) rows = np . zeros ( ( n_fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n_fixed ) , np . where ( fixed_non_zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var_b = prob . variable_bounds [ : , 1 ] b = np . hstack ( [ b , var_b [ fixed_non_zero ] ] ) homogeneous = False # Set up a projection that can cast point into the nullspace nulls = nullspace ( equalities ) # convert bounds to a matrix and add variable bounds as well return Problem ( equalities = shared_np_array ( equalities . shape , equalities ) , b = shared_np_array ( b . shape , b ) , inequalities = shared_np_array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared_np_array ( bounds . shape , bounds ) , variable_fixed = shared_np_array ( prob . variable_fixed . shape , prob . variable_fixed , integer = True ) , variable_bounds = shared_np_array ( var_bounds . shape , var_bounds ) , nullspace = shared_np_array ( nulls . shape , nulls ) , homogeneous = homogeneous )
5500	def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
2863	def ping ( self ) : self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) ] ) self . _i2c_stop ( ) response = self . _transaction_end ( ) if len ( response ) != 1 : raise RuntimeError ( 'Expected 1 response byte but received {0} byte(s).' . format ( len ( response ) ) ) return ( ( response [ 0 ] & 0x01 ) == 0x00 )
11259	def grep ( prev , pattern , * args , * * kw ) : inv = False if 'inv' not in kw else kw . pop ( 'inv' ) pattern_obj = re . compile ( pattern , * args , * * kw ) for data in prev : if bool ( inv ) ^ bool ( pattern_obj . match ( data ) ) : yield data
13144	def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
2051	def MRC ( cpu , coprocessor , opcode1 , dest , coprocessor_reg_n , coprocessor_reg_m , opcode2 ) : assert coprocessor . type == 'coprocessor' assert opcode1 . type == 'immediate' assert opcode2 . type == 'immediate' assert dest . type == 'register' imm_coprocessor = coprocessor . read ( ) imm_opcode1 = opcode1 . read ( ) imm_opcode2 = opcode2 . read ( ) coprocessor_n_name = coprocessor_reg_n . read ( ) coprocessor_m_name = coprocessor_reg_m . read ( ) if 15 == imm_coprocessor : # MMU if 0 == imm_opcode1 : if 13 == coprocessor_n_name : if 3 == imm_opcode2 : dest . write ( cpu . regfile . read ( 'P15_C13' ) ) return raise NotImplementedError ( "MRC: unimplemented combination of coprocessor, opcode, and coprocessor register" )
9316	def _to_json ( resp ) : try : return resp . json ( ) except ValueError as e : # Maybe better to report the original request URL? six . raise_from ( InvalidJSONError ( "Invalid JSON was received from " + resp . request . url ) , e )
7487	def concat_multiple_inputs ( data , sample ) : ## if more than one tuple in fastq list if len ( sample . files . fastqs ) > 1 : ## create a cat command to append them all (doesn't matter if they ## are gzipped, cat still works). Grab index 0 of tuples for R1s. cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . fastqs ] isgzip = ".gz" if not sample . files . fastqs [ 0 ] [ 0 ] . endswith ( ".gz" ) : isgzip = "" ## write to new concat handle conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concat.fq{}" . format ( isgzip ) ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: {}, {}" . format ( cmd1 , res1 ) ) ## Only set conc2 if R2 actually exists conc2 = 0 if "pair" in data . paramsdict [ "datatype" ] : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . fastqs ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concat.fq{}" . format ( isgzip ) ) with open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "Error concatenating fastq files. Make sure all " + "these files exist: {}\nError message: {}" . format ( cmd2 , proc2 . returncode ) ) ## store new file handles sample . files . concat = [ ( conc1 , conc2 ) ] return sample . files . concat
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
2308	def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
13846	def __get_numbered_paths ( filepath ) : format = '%s (%%d)%s' % splitext_files_only ( filepath ) return map ( lambda n : format % n , itertools . count ( 1 ) )
1388	def num_instances ( self ) : num = 0 # Get all the components components = self . spouts ( ) + self . bolts ( ) # Get instances for each worker for component in components : config = component . comp . config for kvs in config . kvs : if kvs . key == api_constants . TOPOLOGY_COMPONENT_PARALLELISM : num += int ( kvs . value ) break return num
13865	def fromts ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : # This can happen if the relative path is a URL continue # # Prepare the target path targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass # print(sourcepath," => ", targetpath) print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : # Actual gross book is the same thing as the algo's GMV # We want our denom to be avg(AGB previous, AGB current) AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) # Since the first value of pd.rolling returns NaN, we # set our "day 0" AGB to 0. denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
7872	def add_payload ( self , payload ) : if self . _payload is None : self . decode_payload ( ) if isinstance ( payload , ElementClass ) : self . _payload . append ( XMLPayload ( payload ) ) elif isinstance ( payload , StanzaPayload ) : self . _payload . append ( payload ) else : raise TypeError ( "Bad payload type" ) self . _dirty = True
6408	def lmean ( nums ) : if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
6013	def load_background_sky_map ( background_sky_map_path , background_sky_map_hdu , pixel_scale ) : if background_sky_map_path is not None : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = background_sky_map_path , hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) else : return None
4690	def encode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Checksum " raw = bytes ( message , "utf8" ) checksum = hashlib . sha256 ( raw ) . digest ( ) raw = checksum [ 0 : 4 ] + raw " Padding " raw = _pad ( raw , 16 ) " Encryption " return hexlify ( aes . encrypt ( raw ) ) . decode ( "ascii" )
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
8980	def temporary_path ( self , extension = "" ) : path = NamedTemporaryFile ( delete = False , suffix = extension ) . name self . path ( path ) return path
4136	def _plots_are_current ( src_file , image_file ) : first_image_file = image_file . format ( 1 ) has_image = os . path . exists ( first_image_file ) src_file_changed = check_md5sum_change ( src_file ) return has_image and not src_file_changed
9167	def _make_celery_app ( config ) : # Tack the pyramid config on the celery app for later use. config . registry . celery_app . conf [ 'pyramid_config' ] = config return config . registry . celery_app
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
11424	def record_find_field ( rec , tag , field , strict = False ) : try : _check_field_validity ( field ) except InvenioBibRecordFieldError : raise for local_position , field1 in enumerate ( rec . get ( tag , [ ] ) ) : if _compare_fields ( field , field1 , strict ) : return ( field1 [ 4 ] , local_position ) return ( None , None )
4777	def is_empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . _err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
7003	def collect_nonperiodic_features ( featuresdir , magcol , outfile , pklglob = 'varfeatures-*.pkl' , featurestouse = NONPERIODIC_FEATURES_TO_COLLECT , maxobjects = None , labeldict = None , labeltype = 'binary' , ) : # list of input pickles generated by varfeatures in lcproc.py pklist = glob . glob ( os . path . join ( featuresdir , pklglob ) ) if maxobjects : pklist = pklist [ : maxobjects ] # fancy progress bar with tqdm if present if TQDM : listiterator = tqdm ( pklist ) else : listiterator = pklist # go through all the varfeatures arrays feature_dict = { 'objectids' : [ ] , 'magcol' : magcol , 'availablefeatures' : [ ] } LOGINFO ( 'collecting features for magcol: %s' % magcol ) for pkl in listiterator : with open ( pkl , 'rb' ) as infd : varf = pickle . load ( infd ) # update the objectid list objectid = varf [ 'objectid' ] if objectid not in feature_dict [ 'objectids' ] : feature_dict [ 'objectids' ] . append ( objectid ) thisfeatures = varf [ magcol ] if featurestouse and len ( featurestouse ) > 0 : featurestoget = featurestouse else : featurestoget = NONPERIODIC_FEATURES_TO_COLLECT # collect all the features for this magcol/objectid combination for feature in featurestoget : # update the global feature list if necessary if ( ( feature not in feature_dict [ 'availablefeatures' ] ) and ( feature in thisfeatures ) ) : feature_dict [ 'availablefeatures' ] . append ( feature ) feature_dict [ feature ] = [ ] if feature in thisfeatures : feature_dict [ feature ] . append ( thisfeatures [ feature ] ) # now that we've collected all the objects and their features, turn the list # into arrays, and then concatenate them for feat in feature_dict [ 'availablefeatures' ] : feature_dict [ feat ] = np . array ( feature_dict [ feat ] ) feature_dict [ 'objectids' ] = np . array ( feature_dict [ 'objectids' ] ) feature_array = np . column_stack ( [ feature_dict [ feat ] for feat in feature_dict [ 'availablefeatures' ] ] ) feature_dict [ 'features_array' ] = feature_array # if there's a labeldict available, use it to generate a label array. this # feature collection is now a training set. if isinstance ( labeldict , dict ) : labelarray = np . zeros ( feature_dict [ 'objectids' ] . size , dtype = np . int64 ) # populate the labels for each object in the training set for ind , objectid in enumerate ( feature_dict [ 'objectids' ] ) : if objectid in labeldict : # if this is a binary classifier training set, convert bools to # ones and zeros if labeltype == 'binary' : if labeldict [ objectid ] : labelarray [ ind ] = 1 # otherwise, use the actual class label integer elif labeltype == 'classes' : labelarray [ ind ] = labeldict [ objectid ] feature_dict [ 'labels_array' ] = labelarray feature_dict [ 'kwargs' ] = { 'pklglob' : pklglob , 'featurestouse' : featurestouse , 'maxobjects' : maxobjects , 'labeltype' : labeltype } # write the info to the output pickle with open ( outfile , 'wb' ) as outfd : pickle . dump ( feature_dict , outfd , pickle . HIGHEST_PROTOCOL ) # return the feature_dict return feature_dict
1850	def LOOPNZ ( cpu , target ) : counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter != 0 , ( cpu . PC + target . read ( ) ) & ( ( 1 << target . size ) - 1 ) , cpu . PC + cpu . instruction . size )
10720	def get_parser ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( "package" , choices = arg_map . keys ( ) , help = "designates the package to test" ) parser . add_argument ( "--ignore" , help = "ignore these files" ) return parser
13058	def transform ( self , work , xml , objectId , subreference = None ) : # We check first that we don't have if str ( objectId ) in self . _transform : func = self . _transform [ str ( objectId ) ] else : func = self . _transform [ "default" ] # If we have a string, it means we get a XSL filepath if isinstance ( func , str ) : with open ( func ) as f : xslt = etree . XSLT ( etree . parse ( f ) ) return etree . tostring ( xslt ( xml ) , encoding = str , method = "html" , xml_declaration = None , pretty_print = False , with_tail = True , standalone = None ) # If we have a function, it means we return the result of the function elif isinstance ( func , Callable ) : return func ( work , xml , objectId , subreference ) # If we have None, it means we just give back the xml elif func is None : return etree . tostring ( xml , encoding = str )
12677	def escape ( to_escape , safe = SAFE , escape_char = ESCAPE_CHAR , allow_collisions = False ) : if isinstance ( to_escape , bytes ) : # always work on text to_escape = to_escape . decode ( 'utf8' ) if not isinstance ( safe , set ) : safe = set ( safe ) if allow_collisions : safe . add ( escape_char ) elif escape_char in safe : # escape char can't be in safe list safe . remove ( escape_char ) chars = [ ] for c in to_escape : if c in safe : chars . append ( c ) else : chars . append ( _escape_char ( c , escape_char ) ) return u'' . join ( chars )
10434	def getcellvalue ( self , window_name , object_name , row_index , column = 0 ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] count = len ( cell . AXChildren ) if column < 0 or column > count : raise LdtpServerException ( 'Column index out of range: %d' % column ) obj = cell . AXChildren [ column ] if not re . search ( "AXColumn" , obj . AXRole ) : obj = cell . AXChildren [ column ] return obj . AXValue
5482	def retry_auth_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in HTTP_AUTH_ERROR_CODES : _print_error ( 'Retrying...' ) return True return False
11720	def get_directory ( self , path_to_directory , timeout = 30 , backoff = 0.4 , max_wait = 4 ) : response = None started_at = None time_elapsed = 0 i = 0 while time_elapsed < timeout : response = self . _get ( '{0}.zip' . format ( path_to_directory ) ) if response : break else : if started_at is None : started_at = time . time ( ) time . sleep ( min ( backoff * ( 2 ** i ) , max_wait ) ) i += 1 time_elapsed = time . time ( ) - started_at return response
11547	def url ( self ) : if len ( self . drivers ) > 0 : return self . drivers [ 0 ] . url else : return self . _url
941	def _reportCommandLineUsageErrorAndExit ( parser , message ) : print parser . get_usage ( ) print message sys . exit ( 1 )
2798	def rename ( self , new_name ) : return self . get_data ( "images/%s" % self . id , type = PUT , params = { "name" : new_name } )
12065	def gain ( abf ) : Ys = np . nan_to_num ( swhlab . ap . getAvgBySweep ( abf , 'freq' ) ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , Ys , '.-' , ms = 20 , alpha = .5 , color = 'b' ) pylab . axhline ( 0 , alpha = .5 , lw = 2 , color = 'r' , ls = "--" ) pylab . margins ( .1 , .1 )
11393	def relative_to_full ( url , example_url ) : if re . match ( 'https?:\/\/' , url ) : return url domain = get_domain ( example_url ) if domain : return '%s%s' % ( domain , url ) return url
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
3541	def do_apply ( mutation_pk , dict_synonyms , backup ) : filename , mutation_id = filename_and_mutation_id_from_pk ( int ( mutation_pk ) ) update_line_numbers ( filename ) context = Context ( mutation_id = mutation_id , filename = filename , dict_synonyms = dict_synonyms , ) mutate_file ( backup = backup , context = context , ) if context . number_of_performed_mutations == 0 : raise RuntimeError ( 'No mutations performed.' )
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
2138	def list ( self , root = False , * * kwargs ) : # Option to list children of a parent group if kwargs . get ( 'parent' , None ) : self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs . pop ( 'parent' ) # Sanity check: If we got `--root` and no inventory, that's an error. if root and not kwargs . get ( 'inventory' , None ) : raise exc . UsageError ( 'The --root option requires specifying an inventory also.' ) # If we are tasked with getting root groups, do that. if root : inventory_id = kwargs [ 'inventory' ] r = client . get ( '/inventories/%d/root_groups/' % inventory_id ) return r . json ( ) # Return the superclass implementation. return super ( Resource , self ) . list ( * * kwargs )
4062	def show_condition_operators ( self , condition ) : # dict keys of allowed operators for the current condition permitted_operators = self . savedsearch . conditions_operators . get ( condition ) # transform these into values permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
5653	def execute ( cur , * args ) : stmt = args [ 0 ] if len ( args ) > 1 : stmt = stmt . replace ( '%' , '%%' ) . replace ( '?' , '%r' ) print ( stmt % ( args [ 1 ] ) ) return cur . execute ( * args )
5172	def _auto_client_files ( cls , client , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : files = [ ] if ca_path and ca_contents : client [ 'ca' ] = ca_path files . append ( dict ( path = ca_path , contents = ca_contents , mode = DEFAULT_FILE_MODE ) ) if cert_path and cert_contents : client [ 'cert' ] = cert_path files . append ( dict ( path = cert_path , contents = cert_contents , mode = DEFAULT_FILE_MODE ) ) if key_path and key_contents : client [ 'key' ] = key_path files . append ( dict ( path = key_path , contents = key_contents , mode = DEFAULT_FILE_MODE , ) ) return files
6198	def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( " Number of particles:" , self . num_particles ) print ( " Number of time steps:" , self . n_samples ) print ( " Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( " Emission array (float32): %.1f MB" % em_size ) print ( " Position array (float32): %.1f MB " % pos_size )
4297	def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) # positionals._option_string_actions for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
1823	def SETS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF , 1 , 0 ) )
8503	def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get_key ( ) , default )
3675	def charge ( self ) : try : if not self . rdkitmol : return charge_from_formula ( self . formula ) else : return Chem . GetFormalCharge ( self . rdkitmol ) except : return charge_from_formula ( self . formula )
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
8915	def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
7750	def __try_handlers ( self , handler_list , stanza , stanza_type = None ) : # pylint: disable=W0212 if stanza_type is None : stanza_type = stanza . stanza_type payload = stanza . get_all_payload ( ) classes = [ p . __class__ for p in payload ] keys = [ ( p . __class__ , p . handler_key ) for p in payload ] for handler in handler_list : type_filter = handler . _pyxmpp_stanza_handled [ 1 ] class_filter = handler . _pyxmpp_payload_class_handled extra_filter = handler . _pyxmpp_payload_key if type_filter != stanza_type : continue if class_filter : if extra_filter is None and class_filter not in classes : continue if extra_filter and ( class_filter , extra_filter ) not in keys : continue response = handler ( stanza ) if self . _process_handler_result ( response ) : return True return False
1728	def to_arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
1892	def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) # Z3 was removed from the system in the middle of operation raise Z3NotFoundError # TODO(mark) don't catch this exception in two places # run solver specific initializations for cfg in self . _init : self . _send ( cfg )
9448	def hangup_call ( self , call_params ) : path = '/' + self . api_version + '/HangupCall/' method = 'POST' return self . request ( path , method , call_params )
2805	def convert_elementwise_add ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_add ...' ) if 'broadcast' in params : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : layer = tf . add ( x [ 0 ] , x [ 1 ] ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( [ layers [ inputs [ 0 ] ] , layers [ inputs [ 1 ] ] ] ) else : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) add = keras . layers . Add ( name = tf_name ) layers [ scope_name ] = add ( [ model0 , model1 ] )
5031	def get ( self , request , template_id , view_type ) : template = get_object_or_404 ( EnrollmentNotificationEmailTemplate , pk = template_id ) if view_type not in self . view_type_contexts : return HttpResponse ( status = 404 ) base_context = self . view_type_contexts [ view_type ] . copy ( ) base_context . update ( { 'user_name' : self . get_user_name ( request ) } ) return HttpResponse ( template . render_html_template ( base_context ) , content_type = 'text/html' )
10811	def query_by_names ( cls , names ) : assert isinstance ( names , list ) return cls . query . filter ( cls . name . in_ ( names ) )
10780	def diffusion ( diffusion_constant = 0.2 , exposure_time = 0.05 , samples = 200 ) : radius = 5 psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) # create a base image of one particle s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) # add up a bunch of trajectories finalimage = 0 * s0 . get_model_image ( ) [ s0 . inner ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( samples ) : offset = np . sqrt ( 6 * diffusion_constant * exposure_time ) * np . random . randn ( 3 ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage += s0 . get_model_image ( ) [ s0 . inner ] position += s0 . obj . pos [ 0 ] finalimage /= float ( samples ) position /= float ( samples ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) # measure the true inferred parameters return s , finalimage , position
2884	def connect ( self , callback , * args , * * kwargs ) : if self . is_connected ( callback ) : raise AttributeError ( 'callback is already connected' ) if self . hard_subscribers is None : self . hard_subscribers = [ ] self . hard_subscribers . append ( ( callback , args , kwargs ) )
6319	def initial_sanity_check ( self ) : # Check for python module collision self . try_import ( self . project_name ) # Is the name a valid identifier? self . validate_name ( self . project_name ) # Make sure we don't mess with existing directories if os . path . exists ( self . project_name ) : print ( "Directory {} already exist. Aborting." . format ( self . project_name ) ) return False if os . path . exists ( 'manage.py' ) : print ( "A manage.py file already exist in the current directory. Aborting." ) return False return True
7885	def _emit_element ( self , element , level , declared_prefixes ) : declarations = { } declared_prefixes = dict ( declared_prefixes ) name = element . tag prefixed = self . _make_prefixed ( name , True , declared_prefixes , declarations ) start_tag = u"<{0}" . format ( prefixed ) end_tag = u"</{0}>" . format ( prefixed ) for name , value in element . items ( ) : prefixed = self . _make_prefixed ( name , False , declared_prefixes , declarations ) start_tag += u' {0}={1}' . format ( prefixed , quoteattr ( value ) ) declarations = self . _make_ns_declarations ( declarations , declared_prefixes ) if declarations : start_tag += u" " + declarations children = [ ] for child in element : children . append ( self . _emit_element ( child , level + 1 , declared_prefixes ) ) if not children and not element . text : start_tag += u"/>" end_tag = u"" text = u"" else : start_tag += u">" if level > 0 and element . text : text = escape ( element . text ) else : text = u"" if level > 1 and element . tail : tail = escape ( element . tail ) else : tail = u"" return start_tag + text + u'' . join ( children ) + end_tag + tail
4131	def get_docstring_and_rest ( filename ) : with open ( filename ) as f : content = f . read ( ) node = ast . parse ( content ) if not isinstance ( node , ast . Module ) : raise TypeError ( "This function only supports modules. " "You provided {0}" . format ( node . __class__ . __name__ ) ) if node . body and isinstance ( node . body [ 0 ] , ast . Expr ) and isinstance ( node . body [ 0 ] . value , ast . Str ) : docstring_node = node . body [ 0 ] docstring = docstring_node . value . s # This get the content of the file after the docstring last line # Note: 'maxsplit' argument is not a keyword argument in python2 rest = content . split ( '\n' , docstring_node . lineno ) [ - 1 ] return docstring , rest else : raise ValueError ( ( 'Could not find docstring in file "{0}". ' 'A docstring is required by sphinx-gallery' ) . format ( filename ) )
5928	def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
4932	def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
8694	def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] # TODO: modifying argv is no good sys . argv = testargs # resuse miniterm on main function miniterm . main ( )
12147	def scan ( self ) : t1 = cm . timeit ( ) self . files1 = cm . list_to_lowercase ( sorted ( os . listdir ( self . folder1 ) ) ) self . files2 = cm . list_to_lowercase ( sorted ( os . listdir ( self . folder2 ) ) ) self . files1abf = [ x for x in self . files1 if x . endswith ( ".abf" ) ] self . files1abf = cm . list_to_lowercase ( cm . abfSort ( self . files1abf ) ) self . IDs = [ x [ : - 4 ] for x in self . files1abf ] self . log . debug ( "folder1 has %d files" , len ( self . files1 ) ) self . log . debug ( "folder1 has %d abfs" , len ( self . files1abf ) ) self . log . debug ( "folder2 has %d files" , len ( self . files2 ) ) self . log . debug ( "scanning folders took %s" , cm . timeit ( t1 ) )
5807	def parse_session_info ( server_handshake_bytes , client_handshake_bytes ) : protocol = None cipher_suite = None compression = False session_id = None session_ticket = None server_session_id = None client_session_id = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : # Ensure we are working with a ServerHello message if message_type != b'\x02' : continue protocol = { b'\x03\x00' : "SSLv3" , b'\x03\x01' : "TLSv1" , b'\x03\x02' : "TLSv1.1" , b'\x03\x03' : "TLSv1.2" , b'\x03\x04' : "TLSv1.3" , } [ message_data [ 0 : 2 ] ] session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : server_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_bytes = message_data [ cipher_suite_start : cipher_suite_start + 2 ] cipher_suite = CIPHER_SUITE_MAP [ cipher_suite_bytes ] compression_start = cipher_suite_start + 2 compression = message_data [ compression_start : compression_start + 1 ] != b'\x00' extensions_length_start = compression_start + 1 extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "new" break break for record_type , _ , record_data in parse_tls_records ( client_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : # Ensure we are working with a ClientHello message if message_type != b'\x01' : continue session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : client_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_length = int_from_bytes ( message_data [ cipher_suite_start : cipher_suite_start + 2 ] ) compression_start = cipher_suite_start + 2 + cipher_suite_length compression_length = int_from_bytes ( message_data [ compression_start : compression_start + 1 ] ) # On subsequent requests, the session ticket will only be seen # in the ClientHello message if server_session_id is None and session_ticket is None : extensions_length_start = compression_start + 1 + compression_length extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "reused" break break if server_session_id is not None : if client_session_id is None : session_id = "new" else : if client_session_id != server_session_id : session_id = "new" else : session_id = "reused" return { "protocol" : protocol , "cipher_suite" : cipher_suite , "compression" : compression , "session_id" : session_id , "session_ticket" : session_ticket , }
4155	def save_file ( self ) : with open ( self . write_file , 'w' ) as out_nb : json . dump ( self . work_notebook , out_nb , indent = 2 )
1148	def _keep_alive ( x , memo ) : try : memo [ id ( memo ) ] . append ( x ) except KeyError : # aha, this is the first one :-) memo [ id ( memo ) ] = [ x ]
4585	def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
3213	def get_access_details ( self , key = None ) : if key in self . _CACHE_STATS : return self . _CACHE_STATS [ 'access_stats' ] [ key ] else : return self . _CACHE_STATS [ 'access_stats' ]
6446	def _cond_bb ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 and word [ - suffix_len - 3 : - suffix_len ] != 'met' and word [ - suffix_len - 4 : - suffix_len ] != 'ryst' )
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
9138	def drop_all ( self , check_first : bool = True ) : self . _metadata . drop_all ( self . engine , checkfirst = check_first ) self . _store_drop ( )
1400	def extract_logical_plan ( self , topology ) : logicalPlan = { "spouts" : { } , "bolts" : { } , } # Add spouts. for spout in topology . spouts ( ) : spoutName = spout . comp . name spoutType = "default" spoutSource = "NA" spoutVersion = "NA" spoutConfigs = spout . comp . config . kvs for kvs in spoutConfigs : if kvs . key == "spout.type" : spoutType = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.source" : spoutSource = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.version" : spoutVersion = javaobj . loads ( kvs . serialized_value ) spoutPlan = { "config" : convert_pb_kvs ( spoutConfigs , include_non_primitives = False ) , "type" : spoutType , "source" : spoutSource , "version" : spoutVersion , "outputs" : [ ] } for outputStream in list ( spout . outputs ) : spoutPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) logicalPlan [ "spouts" ] [ spoutName ] = spoutPlan # Add bolts. for bolt in topology . bolts ( ) : boltName = bolt . comp . name boltPlan = { "config" : convert_pb_kvs ( bolt . comp . config . kvs , include_non_primitives = False ) , "outputs" : [ ] , "inputs" : [ ] } for outputStream in list ( bolt . outputs ) : boltPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) for inputStream in list ( bolt . inputs ) : boltPlan [ "inputs" ] . append ( { "stream_name" : inputStream . stream . id , "component_name" : inputStream . stream . component_name , "grouping" : topology_pb2 . Grouping . Name ( inputStream . gtype ) } ) logicalPlan [ "bolts" ] [ boltName ] = boltPlan return logicalPlan
2150	def delete ( self , pk = None , fail_on_missing = False , * * kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . delete ( pk = pk , fail_on_missing = fail_on_missing , * * kwargs )
13249	def get_url_from_entry ( entry ) : if 'url' in entry . fields : return entry . fields [ 'url' ] elif entry . type . lower ( ) == 'docushare' : return 'https://ls.st/' + entry . fields [ 'handle' ] elif 'adsurl' in entry . fields : return entry . fields [ 'adsurl' ] elif 'doi' in entry . fields : return 'https://doi.org/' + entry . fields [ 'doi' ] else : raise NoEntryUrlError ( )
4751	def run ( self , shell = True , cmdline = False , echo = True ) : if env ( ) : return 1 cmd = [ "fio" ] + self . __parse_parms ( ) if cmdline : cij . emph ( "cij.fio.run: shell: %r, cmd: %r" % ( shell , cmd ) ) return cij . ssh . command ( cmd , shell , echo )
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : # calculate normalization constant total = 0 for i in pos : total += counts [ i ] total = float ( total ) # set included positions to normalized probability for i in pos : x [ i ] = counts [ i ] / total # If we don't have a set of positions, assume there's only one position else : x [ pos ] = 1 return x
9601	def wait_for_element ( self , using , value , timeout = 10000 , interval = 1000 , asserter = is_displayed ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for_element ( ctx , using , value ) : el = ctx . element ( using , value ) asserter ( el ) return el return _wait_for_element ( self , using , value )
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
7690	def sonify ( annotation , sr = 22050 , duration = None , * * kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) # If the annotation can be directly sonified, try that first if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , * * kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , * * kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
12020	def fasta_dict_to_file ( fasta_dict , fasta_file , line_char_limit = None ) : fasta_fp = fasta_file if isinstance ( fasta_file , str ) : fasta_fp = open ( fasta_file , 'wb' ) for key in fasta_dict : seq = fasta_dict [ key ] [ 'seq' ] if line_char_limit : seq = '\n' . join ( [ seq [ i : i + line_char_limit ] for i in range ( 0 , len ( seq ) , line_char_limit ) ] ) fasta_fp . write ( u'{0:s}\n{1:s}\n' . format ( fasta_dict [ key ] [ 'header' ] , seq ) )
12053	def getIDsFromFiles ( files ) : if type ( files ) is str : files = glob . glob ( files + "/*.*" ) IDs = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] IDs . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( IDs )
372	def shift ( x , wrg = 0.1 , hrg = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if is_random : tx = np . random . uniform ( - hrg , hrg ) * h ty = np . random . uniform ( - wrg , wrg ) * w else : tx , ty = hrg * h , wrg * w translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) transform_matrix = translation_matrix # no need to do offset x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
8051	def _darkest ( self ) : rgb , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for r , g , b in self : if r + g + b < n : rgb , n = ( r , g , b ) , r + g + b return rgb
1339	def batch_crossentropy ( label , logits ) : assert logits . ndim == 2 # for numerical reasons we subtract the max logit # (mathematically it doesn't matter!) # otherwise exp(logits) might become too large or too small logits = logits - np . max ( logits , axis = 1 , keepdims = True ) e = np . exp ( logits ) s = np . sum ( e , axis = 1 ) ces = np . log ( s ) - logits [ : , label ] return ces
2060	def add ( self , constraint , check = False ) : if isinstance ( constraint , bool ) : constraint = BoolConstant ( constraint ) assert isinstance ( constraint , Bool ) constraint = simplify ( constraint ) # If self._child is not None this constraint set has been forked and a # a derived constraintset may be using this. So we can't add any more # constraints to this one. After the child constraintSet is deleted # we regain the ability to add constraints. if self . _child is not None : raise Exception ( 'ConstraintSet is frozen' ) if isinstance ( constraint , BoolConstant ) : if not constraint . value : logger . info ( "Adding an impossible constant constraint" ) self . _constraints = [ constraint ] else : return self . _constraints . append ( constraint ) if check : from . . . core . smtlib import solver if not solver . check ( self ) : raise ValueError ( "Added an impossible constraint" )
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
9126	def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
4540	def advance_permutation ( a , increasing = True , forward = True ) : if not forward : a . reverse ( ) cmp = operator . lt if increasing else operator . gt try : i = next ( i for i in reversed ( range ( len ( a ) - 1 ) ) if cmp ( a [ i ] , a [ i + 1 ] ) ) j = next ( j for j in reversed ( range ( i + 1 , len ( a ) ) ) if cmp ( a [ i ] , a [ j ] ) ) except StopIteration : # This is the lexicographically last permutation. if forward : a . reverse ( ) return False a [ i ] , a [ j ] = a [ j ] , a [ i ] a [ i + 1 : ] = reversed ( a [ i + 1 : ] ) if not forward : a . reverse ( ) return True
8691	def put ( self , key ) : self . client . write ( self . _key_path ( key [ 'name' ] ) , * * key ) return self . _key_path ( key [ 'name' ] )
6977	def kepler_lcdict_to_pkl ( lcdict , outfile = None ) : if not outfile : outfile = '%s-keplc.pkl' % lcdict [ 'objectid' ] . replace ( ' ' , '-' ) # we're using pickle.HIGHEST_PROTOCOL here, this will make Py3 pickles # unreadable for Python 2.7 with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return os . path . abspath ( outfile )
8461	def get_cookiecutter_config ( template , default_config = None , version = None ) : default_config = default_config or { } config_dict = cc_config . get_user_config ( ) repo_dir , _ = cc_repository . determine_repo_dir ( template = template , abbreviations = config_dict [ 'abbreviations' ] , clone_to_dir = config_dict [ 'cookiecutters_dir' ] , checkout = version , no_input = True ) context_file = os . path . join ( repo_dir , 'cookiecutter.json' ) context = cc_generate . generate_context ( context_file = context_file , default_context = { * * config_dict [ 'default_context' ] , * * default_config } ) return repo_dir , cc_prompt . prompt_for_config ( context )
5932	def scale_impropers ( mol , impropers , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_impropers = [ ] for im in mol . impropers : atypes = ( im . atom1 . get_atomtype ( ) , im . atom2 . get_atomtype ( ) , im . atom3 . get_atomtype ( ) , im . atom4 . get_atomtype ( ) ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] # special-case: this is a [ dihedral ] override in molecule block, continue and don't match if im . gromacs [ 'param' ] != [ ] : for p in im . gromacs [ 'param' ] : p [ 'kpsi' ] *= scale new_impropers . append ( im ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , im . gromacs [ 'func' ] ) if ( key in impropers ) : for i , imt in enumerate ( impropers [ key ] ) : imA = copy . deepcopy ( im ) param = copy . deepcopy ( imt . gromacs [ 'param' ] ) # Only check the first dihedral in a list if not impropers [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kpsi' ] *= scale imA . gromacs [ 'param' ] = param if i == 0 : imA . comment = "; banned lines {0} found={1}\n ; parameters for types {2}-{3}-{4}-{5}-9 at LINE({6})\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if imt . line in banned_lines else 0 , imt . atype1 , imt . atype2 , imt . atype3 , imt . atype4 , imt . line ) new_impropers . append ( imA ) break #assert(len(mol.impropers) == new_impropers) mol . impropers = new_impropers return mol
11112	def load_repository ( self , path ) : # try to open if path . strip ( ) in ( '' , '.' ) : path = os . getcwd ( ) repoPath = os . path . realpath ( os . path . expanduser ( path ) ) if not self . is_repository ( repoPath ) : raise Exception ( "no repository found in '%s'" % str ( repoPath ) ) # get pyrepinfo path repoInfoPath = os . path . join ( repoPath , ".pyrepinfo" ) try : fd = open ( repoInfoPath , 'rb' ) except Exception as e : raise Exception ( "unable to open repository file(%s)" % e ) # before doing anything try to lock repository # can't decorate with @acquire_lock because this will point to old repository # path or to current working directory which might not be the path anyways L = Locker ( filePath = None , lockPass = str ( uuid . uuid1 ( ) ) , lockPath = os . path . join ( repoPath , ".pyreplock" ) ) acquired , code = L . acquire_lock ( ) # check if acquired. if not acquired : warnings . warn ( "code %s. Unable to aquire the lock when calling 'load_repository'. You may try again!" % ( code , ) ) return try : # unpickle file try : repo = pickle . load ( fd ) except Exception as e : fd . close ( ) raise Exception ( "unable to pickle load repository (%s)" % e ) finally : fd . close ( ) # check if it's a PyrepInfo instance if not isinstance ( repo , Repository ) : raise Exception ( ".pyrepinfo in '%s' is not a repository instance." % s ) else : # update info path self . __reset_repository ( ) self . __update_repository ( repo ) self . __path = repoPath # set timestamp self . __state = self . _get_or_create_state ( ) except Exception as e : L . release_lock ( ) raise Exception ( e ) finally : L . release_lock ( ) # set loaded repo locker path to L because repository have been moved to another directory self . __locker = L # return return self
2202	def ensure_app_cache_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_cache_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
1456	def valid_path ( path ) : # check if the suffic of classpath suffix exists as directory if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False # check if the classpath entry is a directory Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : # check if the classpath entry is a file Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
5079	def strip_html_tags ( text , allowed_tags = None ) : if text is None : return if allowed_tags is None : allowed_tags = ALLOWED_TAGS return bleach . clean ( text , tags = allowed_tags , attributes = [ 'id' , 'class' , 'style' , 'href' , 'title' ] , strip = True )
2998	def marketYesterdayDF ( token = '' , version = '' ) : x = marketYesterday ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . DataFrame ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
706	def runModel ( self , modelID , jobID , modelParams , modelParamsHash , jobsDAO , modelCheckpointGUID ) : # We're going to make an assumption that if we're not using streams, that # we also don't need checkpoints saved. For now, this assumption is OK # (if there are no streams, we're typically running on a single machine # and just save models to files) but we may want to break this out as # a separate controllable parameter in the future if not self . _createCheckpoints : modelCheckpointGUID = None # Register this model in our database self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = None , completed = False , completionReason = None , matured = False , numRecords = 0 ) # Get the structured params, which we pass to the base description structuredParams = modelParams [ 'structuredParams' ] if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( "Running Model. \nmodelParams: %s, \nmodelID=%s, " % ( pprint . pformat ( modelParams , indent = 4 ) , modelID ) ) # Record time.clock() so that we can report on cpu time cpuTimeStart = time . clock ( ) # Run the experiment. This will report the results back to the models # database for us as well. logLevel = self . logger . getEffectiveLevel ( ) try : if self . _dummyModel is None or self . _dummyModel is False : ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( modelID = modelID , jobID = jobID , baseDescription = self . _baseDescription , params = structuredParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) else : dummyParams = dict ( self . _dummyModel ) dummyParams [ 'permutationParams' ] = structuredParams if self . _dummyModelParamsFunc is not None : permInfo = dict ( structuredParams ) permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) ( cmpReason , cmpMsg ) = runDummyModel ( modelID = modelID , jobID = jobID , params = dummyParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) # Write out the completion reason and message jobsDAO . modelSetCompleted ( modelID , completionReason = cmpReason , completionMsg = cmpMsg , cpuTime = time . clock ( ) - cpuTimeStart ) except InvalidConnectionException , e : self . logger . warn ( "%s" , e )
13024	def get ( self , pk ) : if type ( pk ) == str : # Probably an int, give it a shot try : pk = int ( pk ) except ValueError : pass return self . select ( "SELECT {0} FROM " + self . table + " WHERE " + self . pk + " = {1};" , self . columns , pk )
3906	async def _on_connect ( self ) : self . _user_list , self . _conv_list = ( await hangups . build_user_conversation_list ( self . _client ) ) self . _conv_list . on_event . add_observer ( self . _on_event ) # show the conversation menu conv_picker = ConversationPickerWidget ( self . _conv_list , self . on_select_conversation , self . _keys ) self . _tabbed_window = TabbedWindowWidget ( self . _keys ) self . _tabbed_window . set_tab ( conv_picker , switch = True , title = 'Conversations' ) self . _urwid_loop . widget = self . _tabbed_window
2036	def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) #refund = Operators.ITEBV(256, # previous_value != 0, # Operators.ITEBV(256, value != 0, 0, GSTORAGEREFUND), # 0) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
6331	def encode ( self , word , terminator = '\0' ) : if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
469	def create_vocab ( sentences , word_counts_output_file , min_word_count = 1 ) : tl . logging . info ( "Creating vocabulary." ) counter = Counter ( ) for c in sentences : counter . update ( c ) # tl.logging.info('c',c) tl . logging . info ( " Total words: %d" % len ( counter ) ) # Filter uncommon words and sort by descending count. word_counts = [ x for x in counter . items ( ) if x [ 1 ] >= min_word_count ] word_counts . sort ( key = lambda x : x [ 1 ] , reverse = True ) word_counts = [ ( "<PAD>" , 0 ) ] + word_counts # 1st id should be reserved for padding # tl.logging.info(word_counts) tl . logging . info ( " Words in vocabulary: %d" % len ( word_counts ) ) # Write out the word counts file. with tf . gfile . FastGFile ( word_counts_output_file , "w" ) as f : f . write ( "\n" . join ( [ "%s %d" % ( w , c ) for w , c in word_counts ] ) ) tl . logging . info ( " Wrote vocabulary file: %s" % word_counts_output_file ) # Create the vocabulary dictionary. reverse_vocab = [ x [ 0 ] for x in word_counts ] unk_id = len ( reverse_vocab ) vocab_dict = dict ( [ ( x , y ) for ( y , x ) in enumerate ( reverse_vocab ) ] ) vocab = SimpleVocabulary ( vocab_dict , unk_id ) return vocab
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
10944	def reset ( self , new_region_size = None , do_calc_size = True , new_damping = None , new_max_mem = None ) : if new_region_size is not None : self . region_size = new_region_size if new_max_mem != None : self . max_mem = new_max_mem if do_calc_size : self . region_size = calc_particle_group_region_size ( self . state , region_size = self . region_size , max_mem = self . max_mem ) self . stats = [ ] self . particle_groups = separate_particles_into_groups ( self . state , self . region_size , doshift = 'rand' ) if new_damping is not None : self . _kwargs . update ( { 'damping' : new_damping } ) if self . save_J : if len ( self . particle_groups ) > 90 : CLOG . warn ( 'Attempting to create many open files. Consider increasing max_mem and/or region_size to avoid crashes.' ) self . _tempfiles = [ ] self . _has_saved_J = [ ] for a in range ( len ( self . particle_groups ) ) : #TemporaryFile is automatically deleted for _ in [ 'j' , 'tile' ] : self . _tempfiles . append ( tempfile . TemporaryFile ( dir = os . getcwd ( ) ) ) self . _has_saved_J . append ( False )
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue # Strip previous schema if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) # wow, such security token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
9000	def unique ( iterables ) : included_elements = set ( ) def included ( element ) : result = element in included_elements included_elements . add ( element ) return result return [ element for elements in iterables for element in elements if not included ( element ) ]
9017	def _pattern ( self , base ) : rows = self . _rows ( base . get ( ROWS , [ ] ) ) self . _finish_inheritance ( ) self . _finish_instructions ( ) self . _connect_rows ( base . get ( CONNECTIONS , [ ] ) ) id_ = self . _to_id ( base [ ID ] ) name = base [ NAME ] return self . new_pattern ( id_ , name , rows )
5907	def edit_txt ( filename , substitutions , newname = None ) : if newname is None : newname = filename # No sanity checks (figure out later how to give decent diagnostics). # Filter out any rules that have None in replacement. _substitutions = [ { 'lRE' : re . compile ( str ( lRE ) ) , 'sRE' : re . compile ( str ( sRE ) ) , 'repl' : repl } for lRE , sRE , repl in substitutions if repl is not None ] with tempfile . TemporaryFile ( ) as target : with open ( filename , 'rb' ) as src : logger . info ( "editing txt = {0!r} ({1:d} substitutions)" . format ( filename , len ( substitutions ) ) ) for line in src : line = line . decode ( "utf-8" ) keep_line = True for subst in _substitutions : m = subst [ 'lRE' ] . match ( line ) if m : # apply substition to this line? logger . debug ( 'match: ' + line . rstrip ( ) ) if subst [ 'repl' ] is False : # special rule: delete line keep_line = False else : # standard replacement line = subst [ 'sRE' ] . sub ( str ( subst [ 'repl' ] ) , line ) logger . debug ( 'replaced: ' + line . rstrip ( ) ) if keep_line : target . write ( line . encode ( 'utf-8' ) ) else : logger . debug ( "Deleting line %r" , line ) target . seek ( 0 ) with open ( newname , 'wb' ) as final : shutil . copyfileobj ( target , final ) logger . info ( "edited txt = {newname!r}" . format ( * * vars ( ) ) )
2935	def parse_node ( self , node ) : if node . get ( 'id' ) in self . parsed_nodes : return self . parsed_nodes [ node . get ( 'id' ) ] ( node_parser , spec_class ) = self . parser . _get_parser_class ( node . tag ) if not node_parser or not spec_class : raise ValidationException ( "There is no support implemented for this task type." , node = node , filename = self . filename ) np = node_parser ( self , spec_class , node ) task_spec = np . parse_node ( ) return task_spec
3579	def clear_cached_data ( self ) : # Go through and remove any device that isn't currently connected. for device in self . list_devices ( ) : # Skip any connected device. if device . is_connected : continue # Remove this device. First get the adapter associated with the device. adapter = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , device . _adapter ) , _ADAPTER_INTERFACE ) # Now call RemoveDevice on the adapter to remove the device from # bluez's DBus hierarchy. adapter . RemoveDevice ( device . _device . object_path )
3099	def _validate_clientsecrets ( clientsecrets_dict ) : _INVALID_FILE_FORMAT_MSG = ( 'Invalid file format. See ' 'https://developers.google.com/api-client-library/' 'python/guide/aaa_client_secrets' ) if clientsecrets_dict is None : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG ) try : ( client_type , client_info ) , = clientsecrets_dict . items ( ) except ( ValueError , AttributeError ) : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG + ' ' 'Expected a JSON object with a single property for a "web" or ' '"installed" application' ) if client_type not in VALID_CLIENT : raise InvalidClientSecretsError ( 'Unknown client type: {0}.' . format ( client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'required' ] : if prop_name not in client_info : raise InvalidClientSecretsError ( 'Missing property "{0}" in a client type of "{1}".' . format ( prop_name , client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'string' ] : if client_info [ prop_name ] . startswith ( '[[' ) : raise InvalidClientSecretsError ( 'Property "{0}" is not configured.' . format ( prop_name ) ) return client_type , client_info
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } # name_migration_map -> object_migration_map # Based on the name mapping in name_migration_map build an object to # object mapping to be used in the replacing of variables # inv: object_migration_map's keys should ALWAYS be external/foreign # expressions, and its values should ALWAYS be internal/local expressions object_migration_map = { } #List of foreign vars used in expression foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : # If a variable with the same name was previously migrated if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : # foreign_var was not found in the local declared variables nor # any variable with the same name was previously migrated # let's make a new unique internal name for it migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) # Create and declare a new variable of given type if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : # Note that we are discarding the ArrayProxy encapsulation new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) # Update the var to var mapping object_migration_map [ foreign_var ] = new_var # Update the name to name mapping name_migration_map [ foreign_var . name ] = new_var . name # Actually replace each appearance of migrated variables by the new ones migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
10215	def rank_subgraph_by_node_filter ( graph : BELGraph , node_predicates : Union [ NodePredicate , Iterable [ NodePredicate ] ] , annotation : str = 'Subgraph' , reverse : bool = True , ) -> List [ Tuple [ str , int ] ] : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) r2 = count_dict_values ( r1 ) # TODO use instead: r2.most_common() return sorted ( r2 . items ( ) , key = itemgetter ( 1 ) , reverse = reverse )
8918	def _get_version ( self ) : version = self . _get_param ( param = "version" , allowed_values = allowed_versions [ self . params [ 'service' ] ] , optional = True ) if version is None and self . _get_request_type ( ) != "getcapabilities" : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) else : return version
2600	def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
2781	def create ( self ) : input_params = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } data = self . get_data ( "domains/%s/records" % ( self . domain ) , type = POST , params = input_params , ) if data : self . id = data [ 'domain_record' ] [ 'id' ]
6040	def regular_data_1d_from_sub_data_1d ( self , sub_array_1d ) : return np . multiply ( self . sub_grid_fraction , sub_array_1d . reshape ( - 1 , self . sub_grid_length ) . sum ( axis = 1 ) )
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
1920	def decree ( cls , path , concrete_start = '' , * * kwargs ) : try : return cls ( _make_decree ( path , concrete_start ) , * * kwargs ) except KeyError : # FIXME(mark) magic parsing for DECREE should raise better error raise Exception ( f'Invalid binary: {path}' )
5944	def isstream ( obj ) : signature_methods = ( "close" , ) alternative_methods = ( ( "read" , "readline" , "readlines" ) , ( "write" , "writeline" , "writelines" ) ) # Must have ALL the signature methods for m in signature_methods : if not hasmethod ( obj , m ) : return False # Must have at least one complete set of alternative_methods alternative_results = [ numpy . all ( [ hasmethod ( obj , m ) for m in alternatives ] ) for alternatives in alternative_methods ] return numpy . any ( alternative_results )
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
7952	def wait_for_writability ( self ) : with self . lock : while True : if self . _state in ( "closing" , "closed" , "aborted" ) : return False if self . _socket and bool ( self . _write_queue ) : return True self . _write_queue_cond . wait ( ) return False
8329	def findNext ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findAllNext , name , attrs , text , * * kwargs )
13206	def _parse_doc_ref ( self ) : command = LatexCommand ( 'setDocRef' , { 'name' : 'handle' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no setDocRef' ) self . _handle = None self . _series = None self . _serial = None return self . _handle = parsed [ 'handle' ] try : self . _series , self . _serial = self . _handle . split ( '-' , 1 ) except ValueError : self . _logger . warning ( 'lsstdoc handle cannot be parsed into ' 'series and serial: %r' , self . _handle ) self . _series = None self . _serial = None
4483	def create_file ( self , path , fp , force = False , update = False ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) # all paths are assumed to be absolute path = norm_remote_path ( path ) directory , fname = os . path . split ( path ) directories = directory . split ( os . path . sep ) # navigate to the right parent object for our file parent = self for directory in directories : # skip empty directory names if directory : parent = parent . create_folder ( directory , exist_ok = True ) url = parent . _new_file_url # When uploading a large file (>a few MB) that already exists # we sometimes get a ConnectionError instead of a status == 409. connection_error = False # peek at the file to check if it is an empty file which needs special # handling in requests. If we pass a file like object to data that # turns out to be of length zero then no file is created on the OSF. # See: https://github.com/osfclient/osfclient/pull/135 if file_empty ( fp ) : response = self . _put ( url , params = { 'name' : fname } , data = b'' ) else : try : response = self . _put ( url , params = { 'name' : fname } , data = fp ) except ConnectionError : connection_error = True if connection_error or response . status_code == 409 : if not force and not update : # one-liner to get file size from file pointer from # https://stackoverflow.com/a/283719/2680824 file_size_bytes = get_local_file_size ( fp ) large_file_cutoff = 2 ** 20 # 1 MB in bytes if connection_error and file_size_bytes < large_file_cutoff : msg = ( "There was a connection error which might mean {} " + "already exists. Try again with the `--force` flag " + "specified." ) . format ( path ) raise RuntimeError ( msg ) else : # note in case of connection error, we are making an inference here raise FileExistsError ( path ) else : # find the upload URL for the file we are trying to update for file_ in self . files : if norm_remote_path ( file_ . path ) == path : if not force : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : # If the hashes are equal and force is False, # we're done here break # in the process of attempting to upload the file we # moved through it -> reset read position to beginning # of the file fp . seek ( 0 ) file_ . update ( fp ) break else : raise RuntimeError ( "Could not create a new file at " "({}) nor update it." . format ( path ) )
8319	def parse_tables ( self , markup ) : tables = [ ] m = re . findall ( self . re [ "table" ] , markup ) for chunk in m : table = WikipediaTable ( ) table . properties = chunk . split ( "\n" ) [ 0 ] . strip ( "{|" ) . strip ( ) self . connect_table ( table , chunk , markup ) # Tables start with "{|". # On the same line can be properties, e.g. {| border="1" # The table heading starts with "|+". # A new row in the table starts with "|-". # The end of the table is marked with "|}". row = None for chunk in chunk . split ( "\n" ) : chunk = chunk . strip ( ) if chunk . startswith ( "|+" ) : title = self . plain ( chunk . strip ( "|+" ) ) table . title = title elif chunk . startswith ( "|-" ) : if row : row . properties = chunk . strip ( "|-" ) . strip ( ) table . append ( row ) row = None elif chunk . startswith ( "|}" ) : pass elif chunk . startswith ( "|" ) or chunk . startswith ( "!" ) : row = self . parse_table_row ( chunk , row ) # Append the last row. if row : table . append ( row ) if len ( table ) > 0 : tables . append ( table ) return tables
1512	def distribute_package ( roles , cl_args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl_args [ "heron_dir" ] , tar_file ) ) make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) dist_nodes = masters . union ( slaves ) scp_package ( tar_file , dist_nodes , cl_args )
7706	def load_roster ( self , source ) : try : tree = ElementTree . parse ( source ) except ElementTree . ParseError , err : raise ValueError ( "Invalid roster format: {0}" . format ( err ) ) roster = Roster . from_xml ( tree . getroot ( ) ) for item in roster : item . verify_roster_result ( True ) self . roster = roster
7085	def precess_coordinates ( ra , dec , epoch_one , epoch_two , jd = None , mu_ra = 0.0 , mu_dec = 0.0 , outscalar = False ) : raproc , decproc = np . radians ( ra ) , np . radians ( dec ) if ( ( mu_ra != 0.0 ) and ( mu_dec != 0.0 ) and jd ) : jd_epoch_one = JD2000 + ( epoch_one - epoch_two ) * 365.25 raproc = ( raproc + ( jd - jd_epoch_one ) * mu_ra * MAS_P_YR_TO_RAD_P_DAY / np . cos ( decproc ) ) decproc = decproc + ( jd - jd_epoch_one ) * mu_dec * MAS_P_YR_TO_RAD_P_DAY ca = np . cos ( raproc ) cd = np . cos ( decproc ) sa = np . sin ( raproc ) sd = np . sin ( decproc ) if epoch_one != epoch_two : t1 = 1.0e-3 * ( epoch_two - epoch_one ) t2 = 1.0e-3 * ( epoch_one - 2000.0 ) a = ( t1 * ARCSEC_TO_RADIANS * ( 23062.181 + t2 * ( 139.656 + 0.0139 * t2 ) + t1 * ( 30.188 - 0.344 * t2 + 17.998 * t1 ) ) ) b = t1 * t1 * ARCSEC_TO_RADIANS * ( 79.280 + 0.410 * t2 + 0.205 * t1 ) + a c = ( ARCSEC_TO_RADIANS * t1 * ( 20043.109 - t2 * ( 85.33 + 0.217 * t2 ) + t1 * ( - 42.665 - 0.217 * t2 - 41.833 * t2 ) ) ) sina , sinb , sinc = np . sin ( a ) , np . sin ( b ) , np . sin ( c ) cosa , cosb , cosc = np . cos ( a ) , np . cos ( b ) , np . cos ( c ) precmatrix = np . matrix ( [ [ cosa * cosb * cosc - sina * sinb , sina * cosb + cosa * sinb * cosc , cosa * sinc ] , [ - cosa * sinb - sina * cosb * cosc , cosa * cosb - sina * sinb * cosc , - sina * sinc ] , [ - cosb * sinc , - sinb * sinc , cosc ] ] ) precmatrix = precmatrix . transpose ( ) x = ( np . matrix ( [ cd * ca , cd * sa , sd ] ) ) . transpose ( ) x2 = precmatrix * x outra = np . arctan2 ( x2 [ 1 ] , x2 [ 0 ] ) outdec = np . arcsin ( x2 [ 2 ] ) outradeg = np . rad2deg ( outra ) outdecdeg = np . rad2deg ( outdec ) if outradeg < 0.0 : outradeg = outradeg + 360.0 if outscalar : return float ( outradeg ) , float ( outdecdeg ) else : return outradeg , outdecdeg else : # if the epochs are the same and no proper motion, this will be the same # as the input values. if the epochs are the same, but there IS proper # motion (and a given JD), then these will be perturbed from the input # values of ra, dec by the appropriate amount of motion return np . degrees ( raproc ) , np . degrees ( decproc )
13366	def apply ( f , obj , * args , * * kwargs ) : return vectorize ( f ) ( obj , * args , * * kwargs )
13323	def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d} {:%d} {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
7251	def batch_workflow_status ( self , batch_workflow_id ) : self . logger . debug ( 'Get status of batch workflow: ' + batch_workflow_id ) url = '%(base_url)s/batch_workflows/%(batch_id)s' % { 'base_url' : self . base_url , 'batch_id' : batch_workflow_id } r = self . gbdx_connection . get ( url ) return r . json ( )
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
6404	def get_feature ( vector , feature ) : # :param bool binary: if False, -1, 0, & 1 represent -, 0, & + # if True, only binary oppositions are allowed: # 0 & 1 represent - & + and 0s are mapped to - if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) # each feature mask contains two bits, one each for - and + mask = _FEATURE_MASK [ feature ] # the lower bit represents + pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) # 0 elif masked == mask : retvec . append ( 2 ) # +/- elif masked & pos_mask : retvec . append ( 1 ) # + else : retvec . append ( - 1 ) # - return retvec
8527	def add_child ( self , child ) : if not isinstance ( child , ChildMixin ) : raise TypeError ( 'Requires instance of TreeElement. ' 'Got {}' . format ( type ( child ) ) ) child . parent = self self . _children . append ( child )
11095	def select_by_ext ( self , ext , recursive = True ) : ext = [ ext . strip ( ) . lower ( ) for ext in ensure_list ( ext ) ] def filters ( p ) : return p . suffix . lower ( ) in ext return self . select_file ( filters , recursive )
3289	def get_resource_inst ( self , path , environ ) : self . _count_get_resource_inst += 1 # HG expects the resource paths without leading '/' localHgPath = path . strip ( "/" ) rev = None cmd , rest = util . pop_path ( path ) if cmd == "" : return VirtualCollection ( path , environ , "root" , [ "edit" , "released" , "archive" ] ) elif cmd == "edit" : localHgPath = rest . strip ( "/" ) rev = None elif cmd == "released" : localHgPath = rest . strip ( "/" ) rev = "tip" elif cmd == "archive" : if rest == "/" : # Browse /archive: return a list of revision folders: loglist = self . _get_log ( limit = 10 ) members = [ compat . to_native ( l [ "local_id" ] ) for l in loglist ] return VirtualCollection ( path , environ , "Revisions" , members ) revid , rest = util . pop_path ( rest ) try : int ( revid ) except Exception : # Tried to access /archive/anyname return None # Access /archive/19 rev = revid localHgPath = rest . strip ( "/" ) else : return None # read mercurial repo into request cache cache = self . _get_repo_info ( environ , rev ) if localHgPath in cache [ "filedict" ] : # It is a version controlled file return HgResource ( path , False , environ , rev , localHgPath ) if localHgPath in cache [ "dirinfos" ] or localHgPath == "" : # It is an existing folder return HgResource ( path , True , environ , rev , localHgPath ) return None
704	def _okToExit ( self ) : # Send an update status periodically to the JobTracker so that it doesn't # think this worker is dead. print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" # Any immature models still running? if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) # Sleep for a bit, no need to check for orphaned models very often time . sleep ( 5.0 * random . random ( ) ) return False # All particles have matured, send a STOP signal to any that are still # running. ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) # Update the HsState to get the accurate field contributions. self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) # Update the results field with the new field contributions. jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } # Update the fieldContributions field. if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
5524	def jenks_breaks ( values , nb_class ) : if not isinstance ( values , Iterable ) or isinstance ( values , ( str , bytes ) ) : raise TypeError ( "A sequence of numbers is expected" ) if isinstance ( nb_class , float ) and int ( nb_class ) == nb_class : nb_class = int ( nb_class ) if not isinstance ( nb_class , int ) : raise TypeError ( "Number of class have to be a positive integer: " "expected an instance of 'int' but found {}" . format ( type ( nb_class ) ) ) nb_values = len ( values ) if np and isinstance ( values , np . ndarray ) : values = values [ np . argwhere ( np . isfinite ( values ) ) . reshape ( - 1 ) ] else : values = [ i for i in values if isfinite ( i ) ] if len ( values ) != nb_values : warnings . warn ( 'Invalid values encountered (NaN or Inf) were ignored' ) nb_values = len ( values ) if nb_class >= nb_values or nb_class < 2 : raise ValueError ( "Number of class have to be an integer " "greater than 2 and " "smaller than the number of values to use" ) return jenks . _jenks_breaks ( values , nb_class )
6683	def require ( self , path = None , contents = None , source = None , url = None , md5 = None , use_sudo = False , owner = None , group = '' , mode = None , verify_remote = True , temp_dir = '/tmp' ) : func = use_sudo and run_as_root or self . run # 1) Only a path is given if path and not ( contents or source or url ) : assert path if not self . is_file ( path ) : func ( 'touch "%(path)s"' % locals ( ) ) # 2) A URL is specified (path is optional) elif url : if not path : path = os . path . basename ( urlparse ( url ) . path ) if not self . is_file ( path ) or md5 and self . md5sum ( path ) != md5 : func ( 'wget --progress=dot:mega "%(url)s" -O "%(path)s"' % locals ( ) ) # 3) A local filename, or a content string, is specified else : if source : assert not contents t = None else : fd , source = mkstemp ( ) t = os . fdopen ( fd , 'w' ) t . write ( contents ) t . close ( ) if verify_remote : # Avoid reading the whole file into memory at once digest = hashlib . md5 ( ) f = open ( source , 'rb' ) try : while True : d = f . read ( BLOCKSIZE ) if not d : break digest . update ( d ) finally : f . close ( ) else : digest = None if ( not self . is_file ( path , use_sudo = use_sudo ) or ( verify_remote and self . md5sum ( path , use_sudo = use_sudo ) != digest . hexdigest ( ) ) ) : with self . settings ( hide ( 'running' ) ) : self . put ( local_path = source , remote_path = path , use_sudo = use_sudo , temp_dir = temp_dir ) if t is not None : os . unlink ( source ) # Ensure correct owner if use_sudo and owner is None : owner = 'root' if ( owner and self . get_owner ( path , use_sudo ) != owner ) or ( group and self . get_group ( path , use_sudo ) != group ) : func ( 'chown %(owner)s:%(group)s "%(path)s"' % locals ( ) ) # Ensure correct mode if use_sudo and mode is None : mode = oct ( 0o666 & ~ int ( self . umask ( use_sudo = True ) , base = 8 ) ) if mode and self . get_mode ( path , use_sudo ) != mode : func ( 'chmod %(mode)s "%(path)s"' % locals ( ) )
975	def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : # Create a new representation that has exactly w-1 overlapping bits # as the min representation self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : # Recursively create all the indices above and then this index self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : # Create a new representation that has exactly w-1 overlapping bits # as the max representation self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : # Recursively create all the indices below and then this index self . _createBucket ( index - 1 ) self . _createBucket ( index )
2698	def write_dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
3356	def extend ( self , iterable ) : # Sometimes during initialization from an older pickle, _dict # will not have initialized yet, because the initialization class was # left unspecified. This is an issue because unpickling calls # DictList.extend, which requires the presence of _dict. Therefore, # the issue is caught and addressed here. if not hasattr ( self , "_dict" ) or self . _dict is None : self . _dict = { } _dict = self . _dict current_length = len ( self ) list . extend ( self , iterable ) for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : the_id = obj . id if the_id not in _dict : _dict [ the_id ] = i else : # undo the extend and raise an error self = self [ : current_length ] self . _check ( the_id ) # if the above succeeded, then the id must be present # twice in the list being added raise ValueError ( "id '%s' at index %d is non-unique. " "Is it present twice?" % ( str ( the_id ) , i ) )
6406	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 return ( len ( src ) / len ( tar ) if len ( src ) < len ( tar ) else len ( tar ) / len ( src ) )
2420	def write_document ( document , out , validate = True ) : messages = [ ] messages = document . validate ( messages ) if validate and messages : raise InvalidDocumentError ( messages ) # Write out document information out . write ( '# Document Information\n\n' ) write_value ( 'SPDXVersion' , str ( document . version ) , out ) write_value ( 'DataLicense' , document . data_license . identifier , out ) write_value ( 'DocumentName' , document . name , out ) write_value ( 'SPDXID' , 'SPDXRef-DOCUMENT' , out ) write_value ( 'DocumentNamespace' , document . namespace , out ) if document . has_comment : write_text_value ( 'DocumentComment' , document . comment , out ) for doc_ref in document . ext_document_references : doc_ref_str = ' ' . join ( [ doc_ref . external_document_id , doc_ref . spdx_document_uri , doc_ref . check_sum . identifier + ':' + doc_ref . check_sum . value ] ) write_value ( 'ExternalDocumentRef' , doc_ref_str , out ) write_separators ( out ) # Write out creation info write_creation_info ( document . creation_info , out ) write_separators ( out ) # Writesorted reviews for review in sorted ( document . reviews ) : write_review ( review , out ) write_separators ( out ) #Write sorted annotations for annotation in sorted ( document . annotations ) : write_annotation ( annotation , out ) write_separators ( out ) # Write out package info write_package ( document . package , out ) write_separators ( out ) out . write ( '# Extracted Licenses\n\n' ) for lic in sorted ( document . extracted_licenses ) : write_extracted_licenses ( lic , out ) write_separators ( out )
10909	def missing_particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : # create a base image of one particle s = init . create_two_particle_state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )
5578	def driver_from_file ( input_file ) : file_ext = os . path . splitext ( input_file ) [ 1 ] . split ( "." ) [ 1 ] if file_ext not in _file_ext_to_driver ( ) : raise MapcheteDriverError ( "no driver could be found for file extension %s" % file_ext ) driver = _file_ext_to_driver ( ) [ file_ext ] if len ( driver ) > 1 : warnings . warn ( DeprecationWarning ( "more than one driver for file found, taking %s" % driver [ 0 ] ) ) return driver [ 0 ]
8049	def run ( self ) : # Is there any reason not to call load_source here? if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst_prefix , rst_fail_load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( StringIO ( self . source ) , self . filename ) except SyntaxError as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except AllError : msg = "%s%03i %s" % ( rst_prefix , rst_fail_all , "Failed to parse __all__ entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : # People can use flake8-docstrings to report missing # docstrings continue try : # Note we use the PEP257 trim algorithm to remove the # leading whitespace from each line - this avoids false # positive severe error "Unexpected section title." unindented = trim ( dequote_docstring ( definition . docstring ) ) # Off load RST validation to reStructuredText-lint # which calls docutils internally. # TODO: Should we pass the Python filename as filepath? rst_errors = list ( rst_lint . lint ( unindented ) ) except Exception as err : # e.g. UnicodeDecodeError msg = "%s%03i %s" % ( rst_prefix , rst_fail_lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst_error in rst_errors : # TODO - make this a configuration option? if rst_error . level <= 1 : continue # Levels: # # 0 - debug --> we don't receive these # 1 - info --> RST1## codes # 2 - warning --> RST2## codes # 3 - error --> RST3## codes # 4 - severe --> RST4## codes # # Map the string to a unique code: msg = rst_error . message . split ( "\n" , 1 ) [ 0 ] code = code_mapping ( rst_error . level , msg ) assert code < 100 , code code += 100 * rst_error . level msg = "%s%03i %s" % ( rst_prefix , code , msg ) # This will return the line number by combining the # start of the docstring with the offet within it. # We don't know the column number, leaving as zero. yield definition . start + rst_error . line , 0 , msg , type ( self )
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) # Populate diagnostic info self . _creationTracebackString = traceback . format_stack ( ) # Check for concurrency violation if self . _clsNumOutstanding >= g_max_concurrency : # NOTE: It's possible for _clsNumOutstanding to be greater than # len(_clsOutstandingInstances) if concurrency check was enabled after # unrelease allocations. errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) # Add self to tracked instance set self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
837	def getDistances ( self , inputPattern ) : dist = self . _getDistances ( inputPattern ) return ( dist , self . _categoryList )
8810	def delete_segment_allocation_range ( context , sa_id ) : LOG . info ( "delete_segment_allocation_range %s for tenant %s" % ( sa_id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : sa_range = db_api . segment_allocation_range_find ( context , id = sa_id , scope = db_api . ONE ) if not sa_range : raise q_exc . SegmentAllocationRangeNotFound ( segment_allocation_range_id = sa_id ) _delete_segment_allocation_range ( context , sa_range )
12117	def ndist ( data , Xs ) : sigma = np . sqrt ( np . var ( data ) ) center = np . average ( data ) curve = mlab . normpdf ( Xs , center , sigma ) curve *= len ( data ) * HIST_RESOLUTION return curve
10354	def get_subgraph_by_node_search ( graph : BELGraph , query : Strings ) -> BELGraph : nodes = search_node_names ( graph , query ) return get_subgraph_by_induction ( graph , nodes )
6255	def load ( self ) : self . meta . resolved_path = self . find_data ( self . meta . path ) if not self . meta . resolved_path : raise ImproperlyConfigured ( "Data file '{}' not found" . format ( self . meta . path ) ) print ( "Loading:" , self . meta . path ) with open ( self . meta . resolved_path , 'r' ) as fd : return fd . read ( )
4569	def dump ( data , file = sys . stdout , use_yaml = None , * * kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , * * kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , * * kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
12251	def get_all_keys ( self , * args , * * kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 0 ] if len ( args ) else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_all_keys ( * args , * * kwargs )
9957	def setup_ipython ( self ) : if self . is_ipysetup : return from ipykernel . kernelapp import IPKernelApp self . shell = IPKernelApp . instance ( ) . shell # None in PyCharm console if not self . shell and is_ipython ( ) : self . shell = get_ipython ( ) if self . shell : shell_class = type ( self . shell ) shell_class . default_showtraceback = shell_class . showtraceback shell_class . showtraceback = custom_showtraceback self . is_ipysetup = True else : raise RuntimeError ( "IPython shell not found." )
1560	def register_metric ( self , name , metric , time_bucket_in_sec ) : collector = self . get_metrics_collector ( ) collector . register_metric ( name , metric , time_bucket_in_sec )
1026	def encode ( input , output , quotetabs , header = 0 ) : if b2a_qp is not None : data = input . read ( ) odata = b2a_qp ( data , quotetabs = quotetabs , header = header ) output . write ( odata ) return def write ( s , output = output , lineEnd = '\n' ) : # RFC 1521 requires that the line ending in a space or tab must have # that trailing character encoded. if s and s [ - 1 : ] in ' \t' : output . write ( s [ : - 1 ] + quote ( s [ - 1 ] ) + lineEnd ) elif s == '.' : output . write ( quote ( s ) + lineEnd ) else : output . write ( s + lineEnd ) prevline = None while 1 : line = input . readline ( ) if not line : break outline = [ ] # Strip off any readline induced trailing newline stripped = '' if line [ - 1 : ] == '\n' : line = line [ : - 1 ] stripped = '\n' # Calculate the un-length-limited encoded line for c in line : if needsquoting ( c , quotetabs , header ) : c = quote ( c ) if header and c == ' ' : outline . append ( '_' ) else : outline . append ( c ) # First, write out the previous line if prevline is not None : write ( prevline ) # Now see if we need any soft line breaks because of RFC-imposed # length limitations. Then do the thisline->prevline dance. thisline = EMPTYSTRING . join ( outline ) while len ( thisline ) > MAXLINESIZE : # Don't forget to include the soft line break `=' sign in the # length calculation! write ( thisline [ : MAXLINESIZE - 1 ] , lineEnd = '=\n' ) thisline = thisline [ MAXLINESIZE - 1 : ] # Write out the current line prevline = thisline # Write out the last line, without a trailing newline if prevline is not None : write ( prevline , lineEnd = stripped )
9203	def render ( node , strict = False ) : if isinstance ( node , list ) : return render_list ( node ) elif isinstance ( node , dict ) : return render_node ( node , strict = strict ) else : raise NotImplementedError ( "You tried to render a %s. Only list and dicts can be rendered." % node . __class__ . __name__ )
11858	def make_factor ( var , e , bn ) : node = bn . variable_node ( var ) vars = [ X for X in [ var ] + node . parents if X not in e ] cpt = dict ( ( event_values ( e1 , vars ) , node . p ( e1 [ var ] , e1 ) ) for e1 in all_events ( vars , bn , e ) ) return Factor ( vars , cpt )
2515	def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
2244	def argflag ( key , argv = None ) : if argv is None : # nocover argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key flag = any ( k in argv for k in keys ) return flag
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
2997	def marketOhlcDF ( token = '' , version = '' ) : x = marketOhlc ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . io . json . json_normalize ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
4742	def env_export ( prefix , exported , env ) : for exp in exported : ENV [ "_" . join ( [ prefix , exp ] ) ] = env [ exp ]
8038	def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass # Following assumes any variable messages take the format # of 'Fixed text "variable text".' only: # e.g. 'Unknown directive type "req".' # ---> 'Unknown directive type' # e.g. 'Unknown interpreted text role "need".' # ---> 'Unknown interpreted text role' if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
11760	def retract ( self , sentence ) : for c in conjuncts ( to_cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )
7018	def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_concat_worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
6138	def set_default_sim_param ( self , * args , * * kwargs ) : if len ( args ) is 1 and isinstance ( args [ 0 ] , SimulationParameter ) : self . __default_param = args [ 0 ] else : self . __default_param = SimulationParameter ( * args , * * kwargs ) return
5786	def _raw_write ( self ) : data_available = libssl . BIO_ctrl_pending ( self . _wbio ) if data_available == 0 : return b'' to_read = min ( self . _buffer_size , data_available ) read = libssl . BIO_read ( self . _wbio , self . _bio_write_buffer , to_read ) to_write = bytes_from_buffer ( self . _bio_write_buffer , read ) output = to_write while len ( to_write ) : raise_disconnect = False try : sent = self . _socket . send ( to_write ) except ( socket_ . error ) as e : # Handle ECONNRESET and EPIPE if e . errno == 104 or e . errno == 32 : raise_disconnect = True else : raise if raise_disconnect : raise_disconnection ( ) to_write = to_write [ sent : ] if len ( to_write ) : self . select_write ( ) return output
8614	def create_volume ( self , datacenter_id , volume ) : data = ( json . dumps ( self . _create_volume_dict ( volume ) ) ) response = self . _perform_request ( url = '/datacenters/%s/volumes' % datacenter_id , method = 'POST' , data = data ) return response
11820	def as_dict ( self , default = None ) : settings = SettingDict ( queryset = self , default = default ) return settings
9431	def dostime_to_timetuple ( dostime ) : dostime = dostime >> 16 dostime = dostime & 0xffff day = dostime & 0x1f month = ( dostime >> 5 ) & 0xf year = 1980 + ( dostime >> 9 ) second = 2 * ( dostime & 0x1f ) minute = ( dostime >> 5 ) & 0x3f hour = dostime >> 11 return ( year , month , day , hour , minute , second )
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : """ Any conversion from ES result syntax into our search engine syntax """ translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : """ Any conversion from ES facet syntax into our search engine sytax """ terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
9917	def validate ( self , data ) : user = self . _confirmation . email . user if ( app_settings . EMAIL_VERIFICATION_PASSWORD_REQUIRED and not user . check_password ( data [ "password" ] ) ) : raise serializers . ValidationError ( _ ( "The provided password is invalid." ) ) # Add email to returned data data [ "email" ] = self . _confirmation . email . email return data
11947	def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
1213	def _run_single ( self , thread_id , agent , environment , deterministic = False , max_episode_timesteps = - 1 , episode_finished = None , testing = False , sleep = None ) : # figure out whether we are using the deprecated way of "episode_finished" reporting old_episode_finished = False if episode_finished is not None and len ( getargspec ( episode_finished ) . args ) == 1 : old_episode_finished = True episode = 0 # Run this single worker (episode loop) as long as global count thresholds have not been reached. while not self . should_stop : state = environment . reset ( ) agent . reset ( ) self . global_timestep , self . global_episode = agent . timestep , agent . episode episode_reward = 0 # Time step (within episode) loop time_step = 0 time_start = time . time ( ) while True : action , internals , states = agent . act ( states = state , deterministic = deterministic , buffered = False ) reward = 0 for repeat in xrange ( self . repeat_actions ) : state , terminal , step_reward = environment . execute ( action = action ) reward += step_reward if terminal : break if not testing : # agent.observe(reward=reward, terminal=terminal) # Insert everything at once. agent . atomic_observe ( states = state , actions = action , internals = internals , reward = reward , terminal = terminal ) if sleep is not None : time . sleep ( sleep ) time_step += 1 episode_reward += reward if terminal or time_step == max_episode_timesteps : break # Abort the episode (discard its results) when global says so. if self . should_stop : return self . global_timestep += time_step # Avoid race condition where order in episode_rewards won't match order in episode_timesteps. self . episode_list_lock . acquire ( ) self . episode_rewards . append ( episode_reward ) self . episode_timesteps . append ( time_step ) self . episode_times . append ( time . time ( ) - time_start ) self . episode_list_lock . release ( ) if episode_finished is not None : # old way of calling episode_finished if old_episode_finished : summary_data = { "thread_id" : thread_id , "episode" : episode , "timestep" : time_step , "episode_reward" : episode_reward } if not episode_finished ( summary_data ) : return # New way with BasicRunner (self) and thread-id. elif not episode_finished ( self , thread_id ) : return episode += 1
1783	def ADC ( cpu , dest , src ) : cpu . _ADD ( dest , src , carry = True )
13237	def next_interval ( self , after = None ) : if after is None : after = timezone . now ( ) after = self . to_timezone ( after ) return next ( self . intervals ( range_start = after ) , None )
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) # Note that we convert positions to percentages *after* the checks # above, since get_turnover() expects positions in dollars. positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
10325	def _binomial_pmf ( n , p ) : n = int ( n ) ret = np . empty ( n + 1 ) nmax = int ( np . round ( p * n ) ) ret [ nmax ] = 1.0 old_settings = np . seterr ( under = 'ignore' ) # seterr to known value for i in range ( nmax + 1 , n + 1 ) : ret [ i ] = ret [ i - 1 ] * ( n - i + 1.0 ) / i * p / ( 1.0 - p ) for i in range ( nmax - 1 , - 1 , - 1 ) : ret [ i ] = ret [ i + 1 ] * ( i + 1.0 ) / ( n - i ) * ( 1.0 - p ) / p np . seterr ( * * old_settings ) # reset to default return ret / ret . sum ( )
305	def plot_monthly_returns_timeseries ( returns , ax = None , * * kwargs ) : def cumulate_returns ( x ) : return ep . cum_returns ( x ) [ - 1 ] if ax is None : ax = plt . gca ( ) monthly_rets = returns . resample ( 'M' ) . apply ( lambda x : cumulate_returns ( x ) ) monthly_rets = monthly_rets . to_period ( ) sns . barplot ( x = monthly_rets . index , y = monthly_rets . values , color = 'steelblue' ) locs , labels = plt . xticks ( ) plt . setp ( labels , rotation = 90 ) # only show x-labels on year boundary xticks_coord = [ ] xticks_label = [ ] count = 0 for i in monthly_rets . index : if i . month == 1 : xticks_label . append ( i ) xticks_coord . append ( count ) # plot yearly boundary line ax . axvline ( count , color = 'gray' , ls = '--' , alpha = 0.3 ) count += 1 ax . axhline ( 0.0 , color = 'darkgray' , ls = '-' ) ax . set_xticks ( xticks_coord ) ax . set_xticklabels ( xticks_label ) return ax
142	def from_shapely ( polygon_shapely , label = None ) : # load shapely lazily, which makes the dependency more optional import shapely . geometry ia . do_assert ( isinstance ( polygon_shapely , shapely . geometry . Polygon ) ) # polygon_shapely.exterior can be None if the polygon was instantiated without points if polygon_shapely . exterior is None or len ( polygon_shapely . exterior . coords ) == 0 : return Polygon ( [ ] , label = label ) exterior = np . float32 ( [ [ x , y ] for ( x , y ) in polygon_shapely . exterior . coords ] ) return Polygon ( exterior , label = label )
270	def check_intraday ( estimate , returns , positions , transactions ) : if estimate == 'infer' : if positions is not None and transactions is not None : if detect_intraday ( positions , transactions ) : warnings . warn ( 'Detected intraday strategy; inferring positi' + 'ons from transactions. Set estimate_intraday' + '=False to disable.' ) return estimate_intraday ( returns , positions , transactions ) else : return positions else : return positions elif estimate : if positions is not None and transactions is not None : return estimate_intraday ( returns , positions , transactions ) else : raise ValueError ( 'Positions and txns needed to estimate intraday' ) else : return positions
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
10593	def get_date ( date ) : if type ( date ) is str : return datetime . strptime ( date , '%Y-%m-%d' ) . date ( ) else : return date
13843	def close ( self ) : try : self . conn . close ( ) self . logger . debug ( "Close connect succeed." ) except pymssql . Error as e : self . unknown ( "Close connect error: %s" % e )
1734	def remove_objects ( code , count = 1 ) : replacements = { } #replacement dict br = bracket_split ( code , [ '{}' , '[]' ] ) res = '' last = '' for e in br : #test whether e is an object if e [ 0 ] == '{' : n , temp_rep , cand_count = remove_objects ( e [ 1 : - 1 ] , count ) # if e was not an object then n should not contain any : if is_object ( n , last ) : #e was an object res += ' ' + OBJECT_LVAL % count replacements [ OBJECT_LVAL % count ] = e count += 1 else : # e was just a code block but could contain objects inside res += '{%s}' % n count = cand_count replacements . update ( temp_rep ) elif e [ 0 ] == '[' : if is_array ( last ) : res += e # will be translated later else : # prop get n , rep , count = remove_objects ( e [ 1 : - 1 ] , count ) res += '[%s]' % n replacements . update ( rep ) else : # e does not contain any objects res += e last = e #needed to test for this stipid empty object return res , replacements , count
11501	def list_communities ( self , token = None ) : parameters = dict ( ) if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.list' , parameters ) return response
158	def InColorspace ( to_colorspace , from_colorspace = "RGB" , children = None , name = None , deterministic = False , random_state = None ) : return WithColorspace ( to_colorspace , from_colorspace , children , name , deterministic , random_state )
774	def main ( ) : initLogging ( verbose = True ) # Initialize PRNGs initExperimentPrng ( ) # Mock out the creation of the SDRClassifier. @ staticmethod def _mockCreate ( * args , * * kwargs ) : kwargs . pop ( 'implementation' , None ) return SDRClassifierDiff ( * args , * * kwargs ) SDRClassifierFactory . create = _mockCreate # Run it! runExperiment ( sys . argv [ 1 : ] )
11127	def update_file ( self , value , relativePath , name = None , description = False , klass = False , dump = False , pull = False , ACID = None , verbose = False ) : # check ACID if ACID is None : ACID = self . __ACID assert isinstance ( ACID , bool ) , "ACID must be boolean" # get relative path normalized relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' is not allowed as file name in main repository directory" assert name != '.pyrepstate' , "'.pyrepstate' is not allowed as file name in main repository directory" assert name != '.pyreplock' , "'.pyreplock' is not allowed as file name in main repository directory" if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) # get file info dict fileInfoDict , errorMessage = self . get_file_info ( relativePath , name ) assert fileInfoDict is not None , errorMessage # get real path realPath = os . path . join ( self . __path , relativePath ) # check if file exists if verbose : if not os . path . isfile ( os . path . join ( realPath , name ) ) : warnings . warn ( "file '%s' is in repository but does not exist in the system. It is therefore being recreated." % os . path . join ( realPath , name ) ) # convert dump and pull methods to strings if not dump : dump = fileInfoDict [ "dump" ] if not pull : pull = fileInfoDict [ "pull" ] # get savePath if ACID : savePath = os . path . join ( tempfile . gettempdir ( ) , name ) else : savePath = os . path . join ( realPath , name ) # dump file try : exec ( dump . replace ( "$FILE_PATH" , str ( savePath ) ) ) except Exception as e : message = "unable to dump the file (%s)" % e if 'pickle.dump(' in dump : message += '\nmore info: %s' % str ( get_pickling_errors ( value ) ) raise Exception ( message ) # copy if ACID if ACID : try : shutil . copyfile ( savePath , os . path . join ( realPath , name ) ) except Exception as e : os . remove ( savePath ) if verbose : warnings . warn ( e ) return os . remove ( savePath ) # update timestamp fileInfoDict [ "timestamp" ] = datetime . utcnow ( ) if description is not False : fileInfoDict [ "description" ] = description if klass is not False : assert inspect . isclass ( klass ) , "klass must be a class definition" fileInfoDict [ "class" ] = klass # save repository self . save ( )
11480	def _upload_as_item ( local_file , parent_folder_id , file_path , reuse_existing = False ) : current_item_id = _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing ) _create_bitstream ( file_path , local_file , current_item_id ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , current_item_id )
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
5559	def _unflatten_tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) # we are at the end of a branch if len ( path ) == 1 : tree [ key ] = value # there are more branches else : # create new dict if not path [ 0 ] in tree : tree [ path [ 0 ] ] = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) # add keys to existing dict else : branch = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
4025	def _ip_for_mac_from_ip_addr_show ( ip_addr_show , target_mac ) : return_next_ip = False for line in ip_addr_show . splitlines ( ) : line = line . strip ( ) if line . startswith ( 'link/ether' ) : line_mac = line . split ( ' ' ) [ 1 ] . replace ( ':' , '' ) if line_mac == target_mac : return_next_ip = True elif return_next_ip and line . startswith ( 'inet' ) and not line . startswith ( 'inet6' ) : ip = line . split ( ' ' ) [ 1 ] . split ( '/' ) [ 0 ] return ip
5276	def _terminalSymbolsGenerator ( self ) : py2 = sys . version [ 0 ] < '3' UPPAs = list ( list ( range ( 0xE000 , 0xF8FF + 1 ) ) + list ( range ( 0xF0000 , 0xFFFFD + 1 ) ) + list ( range ( 0x100000 , 0x10FFFD + 1 ) ) ) for i in UPPAs : if py2 : yield ( unichr ( i ) ) else : yield ( chr ( i ) ) raise ValueError ( "To many input strings." )
7780	def _process_rfc2425_record ( self , data ) : label , value = data . split ( ":" , 1 ) value = value . replace ( "\\n" , "\n" ) . replace ( "\\N" , "\n" ) psplit = label . lower ( ) . split ( ";" ) name = psplit [ 0 ] params = psplit [ 1 : ] if u"." in name : name = name . split ( "." , 1 ) [ 1 ] name = name . upper ( ) if name in ( u"X-DESC" , u"X-JABBERID" ) : name = name [ 2 : ] if not self . components . has_key ( name ) : return if params : params = dict ( [ p . split ( "=" , 1 ) for p in params ] ) cl , tp = self . components [ name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( name ) : raise ValueError ( "Duplicate %s" % ( name , ) ) try : self . content [ name ] = cl ( name , value , params ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( name ) : self . content [ name ] = [ ] try : self . content [ name ] . append ( cl ( name , value , params ) ) except Empty : pass else : return
9677	def calculate_bin_boundary ( self , bb ) : return min ( enumerate ( OPC_LOOKUP ) , key = lambda x : abs ( x [ 1 ] - bb ) ) [ 0 ]
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
8466	def run ( self ) : options = { } if bool ( self . config [ 'use_proxy' ] ) : options [ 'proxies' ] = { "http" : self . config [ 'proxy' ] , "https" : self . config [ 'proxy' ] } options [ "url" ] = self . config [ 'url' ] options [ "data" ] = { "issues" : json . dumps ( map ( lambda x : x . __todict__ ( ) , self . issues ) ) } if 'get' == self . config [ 'method' ] . lower ( ) : requests . get ( * * options ) else : requests . post ( * * options )
5949	def filename ( self , filename = None , ext = None , set_default = False , use_my_ext = False ) : if filename is None : if not hasattr ( self , '_filename' ) : self . _filename = None # add attribute to class if self . _filename : filename = self . _filename else : raise ValueError ( "A file name is required because no default file name was defined." ) my_ext = None else : filename , my_ext = os . path . splitext ( filename ) if set_default : # replaces existing default file name self . _filename = filename if my_ext and use_my_ext : ext = my_ext if ext is not None : if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] # strip a dot to avoid annoying mistakes if ext != "" : filename = filename + os . extsep + ext return filename
5229	def _load_yaml_ ( file_name ) : if not os . path . exists ( file_name ) : return dict ( ) with open ( file_name , 'r' , encoding = 'utf-8' ) as fp : return YAML ( ) . load ( stream = fp )
2584	def get_tasks ( self , count ) : tasks = [ ] for i in range ( 0 , count ) : try : x = self . pending_task_queue . get ( block = False ) except queue . Empty : break else : tasks . append ( x ) return tasks
4652	def set_fee_asset ( self , fee_asset ) : if isinstance ( fee_asset , self . amount_class ) : self . fee_asset_id = fee_asset [ "id" ] elif isinstance ( fee_asset , self . asset_class ) : self . fee_asset_id = fee_asset [ "id" ] elif fee_asset : self . fee_asset_id = fee_asset else : self . fee_asset_id = "1.3.0"
2763	def get_snapshot ( self , snapshot_id ) : return Snapshot . get_object ( api_token = self . token , snapshot_id = snapshot_id )
5100	def _dict2dict ( adj_dict ) : item = adj_dict . popitem ( ) adj_dict [ item [ 0 ] ] = item [ 1 ] if not isinstance ( item [ 1 ] , dict ) : new_dict = { } for key , value in adj_dict . items ( ) : new_dict [ key ] = { v : { } for v in value } adj_dict = new_dict return adj_dict
1234	def atomic_observe ( self , states , actions , internals , reward , terminal ) : # TODO probably unnecessary here. self . current_terminal = terminal self . current_reward = reward # print('action = {}'.format(actions)) if self . unique_state : states = dict ( state = states ) if self . unique_action : actions = dict ( action = actions ) self . episode = self . model . atomic_observe ( states = states , actions = actions , internals = internals , terminal = self . current_terminal , reward = self . current_reward )
6722	def list_instances ( show = 1 , name = None , group = None , release = None , except_release = None ) : from burlap . common import shelf , OrderedDict , get_verbose verbose = get_verbose ( ) require ( 'vm_type' , 'vm_group' ) assert env . vm_type , 'No VM type specified.' env . vm_type = ( env . vm_type or '' ) . lower ( ) _name = name _group = group _release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( _name , _group , _release ) ) env . vm_elastic_ip_mappings = shelf . get ( 'vm_elastic_ip_mappings' ) data = type ( env ) ( ) if env . vm_type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get_all_running_ec2_instances ( ) : name = instance . tags . get ( env . vm_name_tag ) group = instance . tags . get ( env . vm_group_tag ) release = instance . tags . get ( env . vm_release_tag ) if env . vm_group and env . vm_group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match env.vm_group "%s".' ) % ( instance . public_dns_name , group , env . vm_group ) ) continue if _group and group != _group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match local group "%s".' ) % ( instance . public_dns_name , group , _group ) ) continue if _name and name != _name : if verbose : print ( ( 'Skipping instance %s because its name "%s" ' 'does not match name "%s".' ) % ( instance . public_dns_name , name , _name ) ) continue if _release and release != _release : if verbose : print ( ( 'Skipping instance %s because its release "%s" ' 'does not match release "%s".' ) % ( instance . public_dns_name , release , _release ) ) continue if except_release and release == except_release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public_dns_name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public_dns_name' ] = instance . public_dns_name if verbose : print ( 'Public DNS: %s' % instance . public_dns_name ) if env . vm_elastic_ip_mappings and name in env . vm_elastic_ip_mappings : data [ name ] [ 'ip' ] = env . vm_elastic_ip_mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public_dns_name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm_type == KVM : #virsh list pass else : raise NotImplementedError
4561	def simpixel ( new = 0 , autoraise = True ) : simpixel_driver . open_browser ( new = new , autoraise = autoraise )
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) # This is a cleverer way of doing # # for (u, v) in matches.items(): # P[u, v] = 1 # P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
11302	def provider_for_url ( self , url ) : for provider , regex in self . get_registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise OEmbedMissingEndpoint ( 'No endpoint matches URL: %s' % url )
10061	def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) # Flag unapproved C++14 headers. if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
7624	def pattern_to_mireval ( ann ) : # It's easier to work with dictionaries, since we can't assume # sequential pattern or occurrence identifiers patterns = defaultdict ( lambda : defaultdict ( list ) ) # Iterate over the data in interval-value format for time , observation in zip ( * ann . to_event_values ( ) ) : pattern_id = observation [ 'pattern_id' ] occurrence_id = observation [ 'occurrence_id' ] obs = ( time , observation [ 'midi_pitch' ] ) # Push this note observation into the correct pattern/occurrence patterns [ pattern_id ] [ occurrence_id ] . append ( obs ) # Convert to list-list-tuple format for mir_eval return [ list ( _ . values ( ) ) for _ in six . itervalues ( patterns ) ]
5345	def compose_git ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'source_repo' ] ) > 0 ] : repos = [ ] for url in data [ p ] [ 'source_repo' ] : if len ( url [ 'url' ] . split ( ) ) > 1 : # Error at upstream the project 'tools.corrosion' repo = url [ 'url' ] . split ( ) [ 1 ] . replace ( '/c/' , '/gitroot/' ) else : repo = url [ 'url' ] . replace ( '/c/' , '/gitroot/' ) if repo not in repos : repos . append ( repo ) projects [ p ] [ 'git' ] = repos return projects
5430	def _local_uri_rewriter ( raw_uri ) : # The path is split into components so that the filename is not rewritten. raw_path , filename = os . path . split ( raw_uri ) # Generate the local path that can be resolved by filesystem operations, # this removes special shell characters, condenses indirects and replaces # any unnecessary prefix. prefix_replacements = [ ( 'file:///' , '/' ) , ( '~/' , os . getenv ( 'HOME' ) ) , ( './' , '' ) , ( 'file:/' , '/' ) ] normed_path = raw_path for prefix , replacement in prefix_replacements : if normed_path . startswith ( prefix ) : normed_path = os . path . join ( replacement , normed_path [ len ( prefix ) : ] ) # Because abspath strips the trailing '/' from bare directory references # other than root, this ensures that all directory references end with '/'. normed_uri = directory_fmt ( os . path . abspath ( normed_path ) ) normed_uri = os . path . join ( normed_uri , filename ) # Generate the path used inside the docker image; # 1) Get rid of extra indirects: /this/./that -> /this/that # 2) Rewrite required indirects as synthetic characters. # 3) Strip relative or absolute path leading character. # 4) Add 'file/' prefix. docker_rewrites = [ ( r'/\.\.' , '/_dotdot_' ) , ( r'^\.\.' , '_dotdot_' ) , ( r'^~/' , '_home_/' ) , ( r'^file:/' , '' ) ] docker_path = os . path . normpath ( raw_path ) for pattern , replacement in docker_rewrites : docker_path = re . sub ( pattern , replacement , docker_path ) docker_path = docker_path . lstrip ( './' ) # Strips any of '.' './' '/'. docker_path = directory_fmt ( 'file/' + docker_path ) + filename return normed_uri , docker_path
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
13335	def cache_resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise ResolveError
11148	def create_package ( self , path = None , name = None , mode = None ) : # check mode assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : #mode = 'w:bz2' mode = 'w:' # get root if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path # get name if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext # create tar file tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) # walk directory and create empty directories for dpath in sorted ( list ( self . walk_directories_path ( recursive = True ) ) ) : t = tarfile . TarInfo ( dpath ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) tarHandler . add ( os . path . join ( self . __path , dpath , self . __dirInfo ) , arcname = self . __dirInfo ) # walk files and add to tar for fpath in self . walk_files_path ( recursive = True ) : relaPath , fname = os . path . split ( fpath ) tarHandler . add ( os . path . join ( self . __path , fpath ) , arcname = fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) , arcname = self . __fileInfo % fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileClass % fname ) , arcname = self . __fileClass % fname ) # save repository .pyrepinfo tarHandler . add ( os . path . join ( self . __path , self . __repoFile ) , arcname = ".pyrepinfo" ) # close tar file tarHandler . close ( )
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
13399	def resourcePath ( self , relative_path ) : from os import path import sys try : # PyInstaller creates a temp folder and stores path in _MEIPASS base_path = sys . _MEIPASS except Exception : base_path = path . dirname ( path . abspath ( __file__ ) ) return path . join ( base_path , relative_path )
238	def create_full_tear_sheet ( returns , positions = None , transactions = None , market_data = None , benchmark_rets = None , slippage = None , live_start_date = None , sector_mappings = None , bayesian = False , round_trips = False , estimate_intraday = 'infer' , hide_positions = False , cone_std = ( 1.0 , 1.5 , 2.0 ) , bootstrap = False , unadjusted_returns = None , style_factor_panel = None , sectors = None , caps = None , shares_held = None , volumes = None , percentile = None , turnover_denom = 'AGB' , set_context = True , factor_returns = None , factor_loadings = None , pos_in_dollars = True , header_rows = None , factor_partitions = FACTOR_PARTITIONS ) : if ( unadjusted_returns is None ) and ( slippage is not None ) and ( transactions is not None ) : unadjusted_returns = returns . copy ( ) returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , slippage ) positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) create_returns_tear_sheet ( returns , positions = positions , transactions = transactions , live_start_date = live_start_date , cone_std = cone_std , benchmark_rets = benchmark_rets , bootstrap = bootstrap , turnover_denom = turnover_denom , header_rows = header_rows , set_context = set_context ) create_interesting_times_tear_sheet ( returns , benchmark_rets = benchmark_rets , set_context = set_context ) if positions is not None : create_position_tear_sheet ( returns , positions , hide_positions = hide_positions , set_context = set_context , sector_mappings = sector_mappings , estimate_intraday = False ) if transactions is not None : create_txn_tear_sheet ( returns , positions , transactions , unadjusted_returns = unadjusted_returns , estimate_intraday = False , set_context = set_context ) if round_trips : create_round_trip_tear_sheet ( returns = returns , positions = positions , transactions = transactions , sector_mappings = sector_mappings , estimate_intraday = False ) if market_data is not None : create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , last_n_days = 125 , estimate_intraday = False ) if style_factor_panel is not None : create_risk_tear_sheet ( positions , style_factor_panel , sectors , caps , shares_held , volumes , percentile ) if factor_returns is not None and factor_loadings is not None : create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , factor_partitions = factor_partitions ) if bayesian : create_bayesian_tear_sheet ( returns , live_start_date = live_start_date , benchmark_rets = benchmark_rets , set_context = set_context )
13611	def add_arguments ( cls ) : return [ ( ( '--yes' , ) , dict ( action = 'store_true' , help = 'clean .git repo' ) ) , ( ( '--variable' , '-s' ) , dict ( nargs = '+' , help = 'set extra variable,format is name:value' ) ) , ( ( '--skip-builtin' , ) , dict ( action = 'store_true' , help = 'skip replace builtin variable' ) ) , ]
1061	def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
3927	def set_tab ( self , widget , switch = False , title = None ) : if widget not in self . _widgets : self . _widgets . append ( widget ) self . _widget_title [ widget ] = '' if switch : self . _tab_index = self . _widgets . index ( widget ) if title : self . _widget_title [ widget ] = title self . _update_tabs ( )
11185	def publish ( quiet , dataset_uri ) : access_uri = http_publish ( dataset_uri ) if not quiet : click . secho ( "Dataset accessible at " , nl = False , fg = "green" ) click . secho ( access_uri )
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
12709	def world_to_body ( self , position ) : return np . array ( self . ode_body . getPosRelPoint ( tuple ( position ) ) )
7786	def timeout ( self ) : if not self . active : return if not self . _try_backup_item ( ) : if self . _timeout_handler : self . _timeout_handler ( self . address ) else : self . _error_handler ( self . address , None ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
7837	def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
3074	def authorize_view ( self ) : args = request . args . to_dict ( ) # Scopes will be passed as mutliple args, and to_dict() will only # return one. So, we use getlist() to get all of the scopes. args [ 'scopes' ] = request . args . getlist ( 'scopes' ) return_url = args . pop ( 'return_url' , None ) if return_url is None : return_url = request . referrer or '/' flow = self . _make_flow ( return_url = return_url , * * args ) auth_url = flow . step1_get_authorize_url ( ) return redirect ( auth_url )
9374	def download_file ( url ) : try : ( local_file , headers ) = urllib . urlretrieve ( url ) except : sys . exit ( "ERROR: Problem downloading config file. Please check the URL (" + url + "). Exiting..." ) return local_file
90	def copy_random_state ( random_state , force_copy = False ) : if random_state == np . random and not force_copy : return random_state else : rs_copy = dummy_random_state ( ) orig_state = random_state . get_state ( ) rs_copy . set_state ( orig_state ) return rs_copy
2851	def _mpsse_sync ( self , max_retries = 10 ) : # Send a bad/unknown command (0xAB), then read buffer until bad command # response is found. self . _write ( '\xAB' ) # Keep reading until bad command response (0xFA 0xAB) is returned. # Fail if too many read attempts are made to prevent sticking in a loop. tries = 0 sync = False while not sync : data = self . _poll_read ( 2 ) if data == '\xFA\xAB' : sync = True tries += 1 if tries >= max_retries : raise RuntimeError ( 'Could not synchronize with FT232H!' )
5756	def _strip_version_suffix ( version ) : global version_regex if not version : return version match = version_regex . search ( version ) return match . group ( 0 ) if match else version
1676	def FindHeader ( self , header ) : for section_list in self . include_list : for f in section_list : if f [ 0 ] == header : return f [ 1 ] return - 1
3520	def snapengage ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SnapEngageNode ( )
8604	def update_user ( self , user_id , * * kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps ( data ) ) return response
9788	def bookmark ( ctx , username ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
12408	def _merge ( options , name , bases , default = None ) : result = None for base in bases : if base is None : continue value = getattr ( base , name , None ) if value is None : continue result = utils . cons ( result , value ) value = options . get ( name ) if value is not None : result = utils . cons ( result , value ) return result or default
1934	def get_constructor_arguments ( self ) -> str : item = self . _constructor_abi_item return '()' if item is None else self . tuple_signature_for_components ( item [ 'inputs' ] )
4498	def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : # Awkward, but makes modifier-key-only combinations possible # (since sendKeyWithModifiers() calls this) if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) # Press the key keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) # Release the key keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) # Set modflags on keyDown (default None): Quartz . CGEventSetFlags ( keyDown , modFlags ) # Set modflags on keyUp: Quartz . CGEventSetFlags ( keyUp , modFlags ) # Post the event to the given app if not globally : # To direct output to the correct application need the PSN (macOS <=10.10) or PID(macOS > 10.10): macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
6167	def to_bin ( data , width ) : data_str = bin ( data & ( 2 ** width - 1 ) ) [ 2 : ] . zfill ( width ) return [ int ( x ) for x in tuple ( data_str ) ]
2527	def get_annotation_comment ( self , r_term ) : comment_list = list ( self . graph . triples ( ( r_term , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . error = True msg = 'Annotation can have at most one comment.' self . logger . log ( msg ) return else : return six . text_type ( comment_list [ 0 ] [ 2 ] )
1308	def IsProcess64Bit ( processId : int ) -> bool : try : func = ctypes . windll . ntdll . ZwWow64ReadVirtualMemory64 #only 64 bit OS has this function except Exception as ex : return False try : IsWow64Process = ctypes . windll . kernel32 . IsWow64Process IsWow64Process . argtypes = ( ctypes . c_void_p , ctypes . POINTER ( ctypes . c_int ) ) except Exception as ex : return False hProcess = ctypes . windll . kernel32 . OpenProcess ( 0x1000 , 0 , processId ) #PROCESS_QUERY_INFORMATION=0x0400,PROCESS_QUERY_LIMITED_INFORMATION=0x1000 if hProcess : is64Bit = ctypes . c_int32 ( ) if IsWow64Process ( hProcess , ctypes . byref ( is64Bit ) ) : ctypes . windll . kernel32 . CloseHandle ( ctypes . c_void_p ( hProcess ) ) return False if is64Bit . value else True else : ctypes . windll . kernel32 . CloseHandle ( ctypes . c_void_p ( hProcess ) )
3270	def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
8567	def add_loadbalanced_nics ( self , datacenter_id , loadbalancer_id , nic_id ) : data = '{ "id": "' + nic_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics' % ( datacenter_id , loadbalancer_id ) , method = 'POST' , data = data ) return response
5274	def find ( self , y ) : node = self . root while True : edge = self . _edgeLabel ( node , node . parent ) if edge . startswith ( y ) : return node . idx i = 0 while ( i < len ( edge ) and edge [ i ] == y [ 0 ] ) : y = y [ 1 : ] i += 1 if i != 0 : if i == len ( edge ) and y != '' : pass else : return - 1 node = node . _get_transition_link ( y [ 0 ] ) if not node : return - 1
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
6590	def put ( self , package ) : pkgidx = self . workingArea . put_package ( package ) logger = logging . getLogger ( __name__ ) logger . info ( 'submitting {}' . format ( self . workingArea . package_relpath ( pkgidx ) ) ) runid = self . dispatcher . run ( self . workingArea , pkgidx ) self . runid_pkgidx_map [ runid ] = pkgidx return pkgidx
7326	def with_continuations ( * * c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = tuple ( [ ] ) , tuple ( [ ] ) def d ( f ) : return C ( lambda kself , * conts : lambda * args : f ( * args , self = kself , * * dict ( zip ( keys , conts ) ) ) ) ( * k ) return d
4738	def warn ( txt ) : print ( "%s# %s%s%s" % ( PR_WARN_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
9774	def resources ( ctx , gpu ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . job . resources ( user , project_name , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
5835	def __convert_response_to_configuration ( self , result_blob , dataset_ids ) : builder = DataViewBuilder ( ) builder . dataset_ids ( dataset_ids ) for i , ( k , v ) in enumerate ( result_blob [ 'descriptors' ] . items ( ) ) : try : descriptor = self . __snake_case ( v [ 0 ] ) print ( json . dumps ( descriptor ) ) descriptor [ 'descriptor_key' ] = k builder . add_raw_descriptor ( descriptor ) except IndexError : pass for i , ( k , v ) in enumerate ( result_blob [ 'types' ] . items ( ) ) : builder . set_role ( k , v . lower ( ) ) return builder . build ( )
5785	def _raw_read ( self ) : data = self . _raw_bytes try : data += self . _socket . recv ( 8192 ) except ( socket_ . error ) : pass output = data written = libssl . BIO_write ( self . _rbio , data , len ( data ) ) self . _raw_bytes = data [ written : ] return output
1364	def get_argument_length ( self ) : try : length = self . get_argument ( constants . PARAM_LENGTH ) return length except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
4197	def HERMTOEP ( T0 , T , Z ) : assert len ( T ) > 0 M = len ( T ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save = T [ k ] beta = X [ 0 ] * T [ k ] if k == 0 : temp = - save / P else : for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] beta = beta + X [ j + 1 ] * T [ k - j - 1 ] temp = - save / P P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : #print 'skipping code for k=0' X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) continue khalf = ( k + 1 ) // 2 for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) return X
245	def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]
12121	def get_data_around ( self , timePoints , thisSweep = False , padding = 0.02 , msDeriv = 0 ) : if not np . array ( timePoints ) . shape : timePoints = [ float ( timePoints ) ] data = None for timePoint in timePoints : if thisSweep : sweep = self . currentSweep else : sweep = int ( timePoint / self . sweepInterval ) timePoint = timePoint - sweep * self . sweepInterval self . setSweep ( sweep ) if msDeriv : dx = int ( msDeriv * self . rate / 1000 ) #points per ms newData = ( self . dataY [ dx : ] - self . dataY [ : - dx ] ) * self . rate / 1000 / dx else : newData = self . dataY padPoints = int ( padding * self . rate ) pad = np . empty ( padPoints ) * np . nan Ic = timePoint * self . rate #center point (I) newData = np . concatenate ( ( pad , pad , newData , pad , pad ) ) Ic += padPoints * 2 newData = newData [ Ic - padPoints : Ic + padPoints ] newData = newData [ : int ( padPoints * 2 ) ] #TODO: omg so much trouble with this! if data is None : data = [ newData ] else : data = np . vstack ( ( data , newData ) ) #TODO: omg so much trouble with this! return data
5669	def combined_stop_to_stop_transit_network ( gtfs , start_time_ut = None , end_time_ut = None ) : multi_di_graph = networkx . MultiDiGraph ( ) for route_type in route_types . TRANSIT_ROUTE_TYPES : graph = stop_to_stop_network_for_route_type ( gtfs , route_type , start_time_ut = start_time_ut , end_time_ut = end_time_ut ) for from_node , to_node , data in graph . edges ( data = True ) : data [ 'route_type' ] = route_type multi_di_graph . add_edges_from ( graph . edges ( data = True ) ) multi_di_graph . add_nodes_from ( graph . nodes ( data = True ) ) return multi_di_graph
3299	def xml_to_bytes ( element , pretty_print = False ) : if use_lxml : xml = etree . tostring ( element , encoding = "UTF-8" , xml_declaration = True , pretty_print = pretty_print ) else : xml = etree . tostring ( element , encoding = "UTF-8" ) if not xml . startswith ( b"<?xml " ) : xml = b'<?xml version="1.0" encoding="utf-8" ?>\n' + xml assert xml . startswith ( b"<?xml " ) # ET should prepend an encoding header return xml
1603	def to_table ( metrics ) : all_queries = tracker_access . metric_queries ( ) m = tracker_access . queries_map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all_queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except KeyError : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all_queries if k in metrics . keys ( ) ] return stats , header
2919	def Serializable ( o ) : if isinstance ( o , ( str , dict , int ) ) : return o else : try : json . dumps ( o ) return o except Exception : LOG . debug ( "Got a non-serilizeable object: %s" % o ) return o . __repr__ ( )
9116	def reset_cleansers ( confirm = True ) : if value_asbool ( confirm ) and not yesno ( """\nObacht! This will destroy any existing and or currently running cleanser jails. Are you sure that you want to continue?""" ) : exit ( "Glad I asked..." ) get_vars ( ) cleanser_count = AV [ 'ploy_cleanser_count' ] # make sure no workers interfere: fab . run ( 'ezjail-admin stop worker' ) # stop and nuke the cleanser slaves for cleanser_index in range ( cleanser_count ) : cindex = '{:02d}' . format ( cleanser_index + 1 ) fab . run ( 'ezjail-admin stop cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy tank/jails/cleanser_{cindex}@jdispatch_rollback' . format ( cindex = cindex ) ) fab . run ( 'ezjail-admin delete -fw cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'umount -f /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'rm -rf /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : # remove master snapshot fab . run ( 'zfs destroy -R tank/jails/cleanser@clonesource' ) # restart worker and cleanser to prepare for subsequent ansible configuration runs fab . run ( 'ezjail-admin start worker' ) fab . run ( 'ezjail-admin stop cleanser' ) fab . run ( 'ezjail-admin start cleanser' )
699	def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , matured = None , lastDescendent = False ) : # The indexes of all the models in this swarm. This list excludes hidden # (orphaned) models. if swarmId is not None : entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) else : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) # Get the particles of interest particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] # If this entry is hidden (i.e. it was an orphaned model), it should # not be in this list if swarmId is not None : assert ( not entry [ 'hidden' ] ) # Get info on this model modelParams = entry [ 'modelParams' ] isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue if completed is not None and ( completed != isCompleted ) : continue if matured is not None and ( matured != isMatured ) : continue if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : continue # Incorporate into return values particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
3261	def get_style ( self , name , workspace = None ) : styles = self . get_styles ( names = name , workspaces = workspace ) return self . _return_first_item ( styles )
2945	def refresh_waiting_tasks ( self ) : assert not self . read_only for my_task in self . get_tasks ( Task . WAITING ) : my_task . task_spec . _update ( my_task )
10343	def overlay_type_data ( graph : BELGraph , data : Mapping [ str , float ] , func : str , namespace : str , label : Optional [ str ] = None , overwrite : bool = False , impute : Optional [ float ] = None , ) -> None : new_data = { node : data . get ( node [ NAME ] , impute ) for node in filter_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) ) } overlay_data ( graph , new_data , label = label , overwrite = overwrite )
13302	def rmse ( a , b ) : return np . sqrt ( np . square ( a - b ) . mean ( ) )
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : # The user is already enrolled in the program, so redirect to the program's dashboard. return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) # Check to see if access to any of the course runs in the program are restricted for this user. course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
2097	def cancel ( self , pk = None , fail_if_not_running = False , * * kwargs ) : # Search for the record if pk not given if not pk : existing_data = self . get ( * * kwargs ) pk = existing_data [ 'id' ] cancel_endpoint = '%s%s/cancel/' % ( self . endpoint , pk ) # Attempt to cancel the job. try : client . post ( cancel_endpoint ) changed = True except exc . MethodNotAllowed : changed = False if fail_if_not_running : raise exc . TowerCLIError ( 'Job not running.' ) # Return a success. return { 'status' : 'canceled' , 'changed' : changed }
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
7541	def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ "mindepth_majrule" ] and sum ( reps ) <= data . paramsdict [ "maxdepth" ] : return 1 else : return 0
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
5251	def _init_services ( self ) : logger = _get_logger ( self . debug ) # flush event queue in defensive way opened = self . _session . openService ( '//blp/refdata' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise ConnectionError ( 'Could not open a //blp/refdata service' ) self . refDataService = self . _session . getService ( '//blp/refdata' ) opened = self . _session . openService ( '//blp/exrsvc' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise ConnectionError ( 'Could not open a //blp/exrsvc service' ) self . exrService = self . _session . getService ( '//blp/exrsvc' ) return self
13660	def subroute ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , subroute ( * components ) ) return f return _factory
9356	def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
2489	def create_extracted_license ( self , lic ) : licenses = list ( self . graph . triples ( ( None , self . spdx_namespace . licenseId , lic . identifier ) ) ) if len ( licenses ) != 0 : return licenses [ 0 ] [ 0 ] # return subject in first triple else : license_node = BNode ( ) type_triple = ( license_node , RDF . type , self . spdx_namespace . ExtractedLicensingInfo ) self . graph . add ( type_triple ) ident_triple = ( license_node , self . spdx_namespace . licenseId , Literal ( lic . identifier ) ) self . graph . add ( ident_triple ) text_triple = ( license_node , self . spdx_namespace . extractedText , Literal ( lic . text ) ) self . graph . add ( text_triple ) if lic . full_name is not None : name_triple = ( license_node , self . spdx_namespace . licenseName , self . to_special_value ( lic . full_name ) ) self . graph . add ( name_triple ) for ref in lic . cross_ref : triple = ( license_node , RDFS . seeAlso , URIRef ( ref ) ) self . graph . add ( triple ) if lic . comment is not None : comment_triple = ( license_node , RDFS . comment , Literal ( lic . comment ) ) self . graph . add ( comment_triple ) return license_node
5377	def _build_pipeline_input_file_param ( cls , var_name , docker_path ) : # If the filename contains a wildcard, then the target Docker path must # be a directory in order to ensure consistency whether the source pattern # contains 1 or multiple files. # # In that case, we set the docker_path to explicitly have a trailing slash # (for the Pipelines API "gsutil cp" handling, and then override the # associated var_name environment variable in the generated Docker command. path , filename = os . path . split ( docker_path ) if '*' in filename : return cls . _build_pipeline_file_param ( var_name , path + '/' ) else : return cls . _build_pipeline_file_param ( var_name , docker_path )
740	def coordinateForPosition ( self , longitude , latitude , altitude = None ) : coords = PROJ ( longitude , latitude ) if altitude is not None : coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) coordinate = numpy . array ( coords ) coordinate = coordinate / self . scale return coordinate . astype ( int )
8957	def included ( self , path , is_dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is_dir == is_dir and pattern . matches ( path ) : inclusive = pattern . inclusive #print('+++' if inclusive else '---', path, pattern) return inclusive
11055	def rm_back_refs ( obj ) : for ref in _collect_refs ( obj ) : ref [ 'value' ] . _remove_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , strict = False )
4037	def error_handler ( req ) : error_codes = { 400 : ze . UnsupportedParams , 401 : ze . UserNotAuthorised , 403 : ze . UserNotAuthorised , 404 : ze . ResourceNotFound , 409 : ze . Conflict , 412 : ze . PreConditionFailed , 413 : ze . RequestEntityTooLarge , 428 : ze . PreConditionRequired , 429 : ze . TooManyRequests , } def err_msg ( req ) : """ Return a nicely-formatted error message """ return "\nCode: %s\nURL: %s\nMethod: %s\nResponse: %s" % ( req . status_code , # error.msg, req . url , req . request . method , req . text , ) if error_codes . get ( req . status_code ) : # check to see whether its 429 if req . status_code == 429 : # call our back-off function delay = backoff . delay if delay > 32 : # we've waited a total of 62 seconds (2 + 4  + 32), so give up backoff . reset ( ) raise ze . TooManyRetries ( "Continuing to receive HTTP 429 \ responses after 62 seconds. You are being rate-limited, try again later" ) time . sleep ( delay ) sess = requests . Session ( ) new_req = sess . send ( req . request ) try : new_req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( new_req ) else : raise error_codes . get ( req . status_code ) ( err_msg ( req ) ) else : raise ze . HTTPError ( err_msg ( req ) )
5484	def execute ( api ) : try : return api . execute ( ) except Exception as exception : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) _print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) # Re-raise exception to be handled by retry logic raise exception
7432	def _write_nex ( self , mdict , nlocus ) : ## create matrix as a string max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = "{:<" + str ( max_name_len + 1 ) + "} {}\n" matrix = "" for i in mdict . items ( ) : matrix += namestring . format ( i [ 0 ] , i [ 1 ] ) ## ensure dir minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) if not os . path . exists ( minidir ) : os . makedirs ( minidir ) ## write nexus block handle = os . path . join ( minidir , "{}.nex" . format ( nlocus ) ) with open ( handle , 'w' ) as outnex : outnex . write ( NEXBLOCK . format ( * * { "ntax" : len ( mdict ) , "nchar" : len ( mdict . values ( ) [ 0 ] ) , "matrix" : matrix , "ngen" : self . params . mb_mcmc_ngen , "sfreq" : self . params . mb_mcmc_sample_freq , "burnin" : self . params . mb_mcmc_burnin , } ) )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
5849	def list_files ( self , dataset_id , glob = "." , is_dir = False ) : data = { "list" : { "glob" : glob , "isDir" : is_dir } } return self . _get_success_json ( self . _post_json ( routes . list_files ( dataset_id ) , data , failure_message = "Failed to list files for dataset {}" . format ( dataset_id ) ) ) [ 'files' ]
2987	def get_app_kwarg_dict ( appInstance = None ) : app = ( appInstance or current_app ) # In order to support blueprints which do not have a config attribute app_config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors_' , '' ) : app_config . get ( k ) for k in CONFIG_OPTIONS if app_config . get ( k ) is not None }
5390	def _get_task_from_task_dir ( self , job_id , user_id , task_id , task_attempt ) : # We need to be very careful about how we read and interpret the contents # of the task directory. The directory could be changing because a new # task is being created. The directory could be changing because a task # is ending. # # If the meta.yaml does not exist, the task does not yet exist. # If the meta.yaml exists, it means the task is scheduled. It does not mean # it is yet running. # If the task.pid file exists, it means that the runner.sh was started. task_dir = self . _task_directory ( job_id , task_id , task_attempt ) job_descriptor = self . _read_task_metadata ( task_dir ) if not job_descriptor : return None # If we read up an old task, the user-id will not be in the job_descriptor. if not job_descriptor . job_metadata . get ( 'user-id' ) : job_descriptor . job_metadata [ 'user-id' ] = user_id # Get the pid of the runner pid = - 1 try : with open ( os . path . join ( task_dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IOError , OSError ) : pass # Get the script contents script = None script_name = job_descriptor . job_metadata . get ( 'script-name' ) if script_name : script = self . _read_script ( task_dir , script_name ) # Read the files written by the runner.sh. # For new tasks, these may not have been written yet. end_time = self . _get_end_time_from_task_dir ( task_dir ) last_update = self . _get_last_update_time_from_task_dir ( task_dir ) events = self . _get_events_from_task_dir ( task_dir ) status = self . _get_status_from_task_dir ( task_dir ) log_detail = self . _get_log_detail_from_task_dir ( task_dir ) # If the status file is not yet written, then mark the task as pending if not status : status = 'RUNNING' log_detail = [ 'Pending' ] return LocalTask ( task_status = status , events = events , log_detail = log_detail , job_descriptor = job_descriptor , end_time = end_time , last_update = last_update , pid = pid , script = script )
1435	def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
11963	def _dec_to_dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
10815	def invite_by_emails ( self , emails ) : assert emails is None or isinstance ( emails , list ) results = [ ] for email in emails : try : user = User . query . filter_by ( email = email ) . one ( ) results . append ( self . invite ( user ) ) except NoResultFound : results . append ( None ) return results
7207	def execute ( self ) : # if not self.tasks: # raise WorkflowError('Workflow contains no tasks, and cannot be executed.') # for task in self.tasks: # self.definition['tasks'].append( task.generate_task_workflow_json() ) self . generate_workflow_description ( ) # hit batch workflow endpoint if batch values if self . batch_values : self . id = self . workflow . launch_batch_workflow ( self . definition ) # use regular workflow endpoint if no batch values else : self . id = self . workflow . launch ( self . definition ) return self . id
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
4959	def get_course_runs_from_program ( program ) : course_runs = set ( ) for course in program . get ( "courses" , [ ] ) : for run in course . get ( "course_runs" , [ ] ) : if "key" in run and run [ "key" ] : course_runs . add ( run [ "key" ] ) return course_runs
4184	def window_bohman ( N ) : x = linspace ( - 1 , 1 , N ) w = ( 1. - abs ( x ) ) * cos ( pi * abs ( x ) ) + 1. / pi * sin ( pi * abs ( x ) ) return w
1484	def start_state_manager_watches ( self ) : Log . info ( "Start state manager watches" ) statemgr_config = StateMgrConfig ( ) statemgr_config . set_state_locations ( configloader . load_state_manager_locations ( self . cluster , state_manager_config_file = self . state_manager_config_file , overrides = { "heron.statemgr.connection.string" : self . state_manager_connection } ) ) try : self . state_managers = statemanagerfactory . get_all_state_managers ( statemgr_config ) for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) # pylint: disable=unused-argument def on_packing_plan_watch ( state_manager , new_packing_plan ) : Log . debug ( "State watch triggered for PackingPlan update on shard %s. Existing: %s, New: %s" % ( self . shard , str ( self . packing_plan ) , str ( new_packing_plan ) ) ) if self . packing_plan != new_packing_plan : Log . info ( "PackingPlan change detected on shard %s, relaunching effected processes." % self . shard ) self . update_packing_plan ( new_packing_plan ) Log . info ( "Updating executor processes" ) self . launch ( ) else : Log . info ( "State watch triggered for PackingPlan update but plan not changed so not relaunching." ) for state_manager in self . state_managers : # The callback function with the bound # state_manager as first variable. onPackingPlanWatch = functools . partial ( on_packing_plan_watch , state_manager ) state_manager . get_packing_plan ( self . topology_name , onPackingPlanWatch ) Log . info ( "Registered state watch for packing plan changes with state manager %s." % str ( state_manager ) )
1089	def indexOf ( a , b ) : for i , j in enumerate ( a ) : if j == b : return i else : raise ValueError ( 'sequence.index(x): x not in sequence' )
7762	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) msg = Message ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond , subject = self . _subject , body = self . _body , thread = self . _thread ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : msg . add_payload ( payload . copy ( ) ) return msg
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
6117	def circular ( cls , shape , pixel_scale , radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
11224	def dump_deque ( self , obj , class_name = "collections.deque" ) : return { "$" + class_name : [ self . _json_convert ( item ) for item in obj ] }
7127	def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
4196	def TOEPLITZ ( T0 , TC , TR , Z ) : assert len ( TC ) > 0 assert len ( TC ) == len ( TR ) M = len ( TC ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) B = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save1 = TC [ k ] save2 = TR [ k ] beta = X [ 0 ] * TC [ k ] if k == 0 : temp1 = - save1 / P temp2 = - save2 / P else : for j in range ( 0 , k ) : save1 = save1 + A [ j ] * TC [ k - j - 1 ] save2 = save2 + B [ j ] * TR [ k - j - 1 ] beta = beta + X [ j + 1 ] * TC [ k - j - 1 ] temp1 = - save1 / P temp2 = - save2 / P P = P * ( 1. - ( temp1 * temp2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp1 B [ k ] = temp2 alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] continue for j in range ( 0 , k ) : kj = k - j - 1 save1 = A [ j ] A [ j ] = save1 + temp1 * B [ kj ] B [ kj ] = B [ kj ] + temp2 * save1 X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] return X
3778	def T_dependent_property_integral ( self , T1 , T2 ) : Tavg = 0.5 * ( T1 + T2 ) if self . method : # retest within range if self . test_method_validity ( Tavg , self . method ) : try : return self . calculate_integral ( T1 , T2 , self . method ) except : # pragma: no cover pass sorted_valid_methods = self . select_valid_methods ( Tavg ) for method in sorted_valid_methods : try : return self . calculate_integral ( T1 , T2 , method ) except : pass return None
3334	def string_repr ( s ) : if compat . is_bytes ( s ) : res = "{!r}: " . format ( s ) for b in s : if type ( b ) is str : # Py2 b = ord ( b ) res += "%02x " % b return res return "{}" . format ( s )
3168	def replicate ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/replicate' ) )
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
12114	def save ( self , filename , imdata , * * data ) : if isinstance ( imdata , numpy . ndarray ) : imdata = Image . fromarray ( numpy . uint8 ( imdata ) ) elif isinstance ( imdata , Image . Image ) : imdata . save ( self . _savepath ( filename ) )
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
7304	def set_permissions_in_context ( self , context = { } ) : context [ 'has_view_permission' ] = self . mongoadmin . has_view_permission ( self . request ) context [ 'has_edit_permission' ] = self . mongoadmin . has_edit_permission ( self . request ) context [ 'has_add_permission' ] = self . mongoadmin . has_add_permission ( self . request ) context [ 'has_delete_permission' ] = self . mongoadmin . has_delete_permission ( self . request ) return context
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : # TODO: Use sys.stderr when that's implemented. file = open ( '/dev/stderr' , 'w' ) #file = sys.stderr if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( * * _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( * * _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) # TODO: Test that re-encryption does not occur on similar # passphrases keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
10938	def update_eig_J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc_residuals ( ) for a in range ( min ( [ self . num_eig_dirs , vls . size ] ) ) : #1. Finding stiff directions stif_dir = vcs [ - ( a + 1 ) ] #already normalized #2. Evaluating derivative along that direction, we'll use dl=5e-4: dl = self . eig_dl #1e-5 _ = self . update_function ( self . param_vals + dl * stif_dir ) res1 = self . calc_residuals ( ) #3. Updating grad_stif = ( res1 - res0 ) / dl self . _rank_1_J_update ( stif_dir , grad_stif ) self . JTJ = np . dot ( self . J , self . J . T ) #Putting the parameters back: _ = self . update_function ( self . param_vals )
5219	def ref_file ( ticker : str , fld : str , has_date = False , cache = False , ext = 'parq' , * * kwargs ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if ( not data_path ) or ( not cache ) : return '' proper_ticker = ticker . replace ( '/' , '_' ) cache_days = kwargs . pop ( 'cache_days' , 10 ) root = f'{data_path}/{ticker.split()[-1]}/{proper_ticker}/{fld}' if len ( kwargs ) > 0 : info = utils . to_str ( kwargs ) [ 1 : - 1 ] . replace ( '|' , '_' ) else : info = 'ovrd=None' # Check date info if has_date : cur_dt = utils . cur_time ( ) missing = f'{root}/asof={cur_dt}, {info}.{ext}' to_find = re . compile ( rf'{root}/asof=(.*), {info}\.pkl' ) cur_files = list ( filter ( to_find . match , sorted ( files . all_files ( path_name = root , keyword = info , ext = ext ) ) ) ) if len ( cur_files ) > 0 : upd_dt = to_find . match ( cur_files [ - 1 ] ) . group ( 1 ) diff = pd . Timestamp ( 'today' ) - pd . Timestamp ( upd_dt ) if diff >= pd . Timedelta ( days = cache_days ) : return missing return sorted ( cur_files ) [ - 1 ] else : return missing else : return f'{root}/{info}.{ext}'
10084	def discard ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) _ , record = self . fetch_published ( ) self . model . json = deepcopy ( record . model . json ) self . model . json [ '$schema' ] = self . build_deposit_schema ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
6303	def get_package ( self , name ) -> 'EffectPackage' : name , cls_name = parse_package_string ( name ) try : return self . package_map [ name ] except KeyError : raise EffectError ( "No package '{}' registered" . format ( name ) )
12099	def get_root_directory ( self , timestamp = None ) : if timestamp is None : timestamp = self . timestamp if self . timestamp_format is not None : root_name = ( time . strftime ( self . timestamp_format , timestamp ) + '-' + self . batch_name ) else : root_name = self . batch_name path = os . path . join ( self . output_directory , * ( self . subdir + [ root_name ] ) ) return os . path . abspath ( path )
3863	def _get_event_request_header ( self ) : otr_status = ( hangouts_pb2 . OFF_THE_RECORD_STATUS_OFF_THE_RECORD if self . is_off_the_record else hangouts_pb2 . OFF_THE_RECORD_STATUS_ON_THE_RECORD ) return hangouts_pb2 . EventRequestHeader ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , client_generated_id = self . _client . get_client_generated_id ( ) , expected_otr = otr_status , delivery_medium = self . _get_default_delivery_medium ( ) , )
1176	def copy ( self ) : if 0 : # set this to 1 to make the flow space crash return copy . deepcopy ( self ) clone = self . __class__ ( ) clone . length = self . length clone . count = [ ] + self . count [ : ] clone . input = [ ] + self . input clone . A = self . A clone . B = self . B clone . C = self . C clone . D = self . D return clone
3260	def get_layergroup ( self , name , workspace = None ) : layergroups = self . get_layergroups ( names = name , workspaces = workspace ) return self . _return_first_item ( layergroups )
7850	def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnode . newChild ( None , "feature" , None ) n . setProp ( "var" , to_utf8 ( var ) )
12323	def api_call_action ( func ) : def _inner ( * args , * * kwargs ) : return func ( * args , * * kwargs ) _inner . __name__ = func . __name__ _inner . __doc__ = func . __doc__ return _inner
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
5431	def _get_filtered_mounts ( mounts , mount_param_type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount_param_type ) ] )
4403	def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
10404	def canonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'float64' ) , ( 'moments' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
6416	def var ( nums , mean_func = amean , ddof = 0 ) : x_bar = mean_func ( nums ) return sum ( ( x - x_bar ) ** 2 for x in nums ) / ( len ( nums ) - ddof )
13304	def mfbe ( a , b ) : return 2 * bias ( a , b ) / ( a . mean ( ) + b . mean ( ) )
4815	def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
10463	def menuitemenabled ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) if menu_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
6316	def reload_programs ( self ) : print ( "Reloading programs:" ) for name , program in self . _programs . items ( ) : if getattr ( program , 'program' , None ) : print ( " - {}" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )
6889	def _parallel_bls_worker ( task ) : try : times , mags , errs = task [ : 3 ] magsarefluxes = task [ 3 ] minfreq , nfreq , stepsize = task [ 4 : 7 ] ndurations , mintransitduration , maxtransitduration = task [ 7 : 10 ] blsobjective , blsmethod , blsoversample = task [ 10 : ] frequencies = minfreq + nparange ( nfreq ) * stepsize periods = 1.0 / frequencies # astropy's BLS requires durations in units of time durations = nplinspace ( mintransitduration * periods . min ( ) , maxtransitduration * periods . min ( ) , ndurations ) # set up the correct units for the BLS model if magsarefluxes : blsmodel = BoxLeastSquares ( times * u . day , mags * u . dimensionless_unscaled , dy = errs * u . dimensionless_unscaled ) else : blsmodel = BoxLeastSquares ( times * u . day , mags * u . mag , dy = errs * u . mag ) blsresult = blsmodel . power ( periods * u . day , durations * u . day , objective = blsobjective , method = blsmethod , oversample = blsoversample ) return { 'blsresult' : blsresult , 'blsmodel' : blsmodel , 'durations' : durations , 'power' : nparray ( blsresult . power ) } except Exception as e : LOGEXCEPTION ( 'BLS for frequency chunk: (%.6f, %.6f) failed.' % ( frequencies [ 0 ] , frequencies [ - 1 ] ) ) return { 'blsresult' : None , 'blsmodel' : None , 'durations' : durations , 'power' : nparray ( [ npnan for x in range ( nfreq ) ] ) , }
12298	def discover_all_plugins ( self ) : for v in pkg_resources . iter_entry_points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
460	def evaluation ( y_test = None , y_predict = None , n_classes = None ) : c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) acc = accuracy_score ( y_test , y_predict ) tl . logging . info ( 'confusion matrix: \n%s' % c_mat ) tl . logging . info ( 'f1-score : %s' % f1 ) tl . logging . info ( 'f1-score(macro) : %f' % f1_macro ) # same output with > f1_score(y_true, y_pred, average='macro') tl . logging . info ( 'accuracy-score : %f' % acc ) return c_mat , f1 , acc , f1_macro
10753	def open_archive ( fs_url , archive ) : it = pkg_resources . iter_entry_points ( 'fs.archive.open_archive' ) entry_point = next ( ( ep for ep in it if archive . endswith ( ep . name ) ) , None ) if entry_point is None : raise UnsupportedProtocol ( 'unknown archive extension: {}' . format ( archive ) ) try : archive_opener = entry_point . load ( ) except pkg_resources . DistributionNotFound as df : # pragma: no cover six . raise_from ( UnsupportedProtocol ( 'extension {} requires {}' . format ( entry_point . name , df . req ) ) , None ) try : binfile = None archive_fs = None fs = open_fs ( fs_url ) if issubclass ( archive_opener , base . ArchiveFS ) : try : binfile = fs . openbin ( archive , 'r+' ) except errors . ResourceNotFound : binfile = fs . openbin ( archive , 'w' ) except errors . ResourceReadOnly : binfile = fs . openbin ( archive , 'r' ) archive_opener = archive_opener . _read_fs_cls elif issubclass ( archive_opener , base . ArchiveReadFS ) : binfile = fs . openbin ( archive , 'r' ) if not hasattr ( binfile , 'name' ) : binfile . name = basename ( archive ) archive_fs = archive_opener ( binfile ) except Exception : getattr ( archive_fs , 'close' , lambda : None ) ( ) getattr ( binfile , 'close' , lambda : None ) ( ) raise else : return archive_fs
5210	def bdp ( tickers , flds , * * kwargs ) : logger = logs . get_logger ( bdp , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( * * kwargs ) logger . info ( f'loading reference data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . ref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for r , snap in data . iterrows ( ) : subset = [ r ] data_file = storage . ref_file ( ticker = snap . ticker , fld = snap . field , ext = 'pkl' , * * kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( data . iloc [ subset ] ) files . create_folder ( data_file , is_file = True ) data . iloc [ subset ] . to_pickle ( data_file ) return qry_data
13815	def _StructMessageToJsonObject ( message , unused_including_default = False ) : fields = message . fields ret = { } for key in fields : ret [ key ] = _ValueMessageToJsonObject ( fields [ key ] ) return ret
13749	def many_to_one ( clsname , * * kw ) : @ declared_attr def m2o ( cls ) : cls . _references ( ( cls . __name__ , clsname ) ) return relationship ( clsname , * * kw ) return m2o
6620	def wait ( self ) : finished_pids = [ ] while self . running_procs : finished_pids . extend ( self . poll ( ) ) return finished_pids
4191	def window_cauchy ( N , alpha = 3 ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = 1. / ( 1. + ( alpha * n / ( N / 2. ) ) ** 2 ) return w
6424	def sim ( self , src , tar , qval = 2 ) : return super ( self . __class__ , self ) . sim ( src , tar , qval , 1 , 1 )
11097	def select_by_pattern_in_abspath ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . abspath else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . abspath . lower ( ) return self . select_file ( filters , recursive )
7352	def to_dataframe ( self , columns = BindingPrediction . fields + ( "length" , ) ) : return pd . DataFrame . from_records ( [ tuple ( [ getattr ( x , name ) for name in columns ] ) for x in self ] , columns = columns )
7812	def _decode_validity ( self , validity ) : not_after = validity . getComponentByName ( 'notAfter' ) not_after = str ( not_after . getComponent ( ) ) if isinstance ( not_after , GeneralizedTime ) : self . not_after = datetime . strptime ( not_after , "%Y%m%d%H%M%SZ" ) else : self . not_after = datetime . strptime ( not_after , "%y%m%d%H%M%SZ" ) self . alt_names = defaultdict ( list )
1256	def setup_hooks ( self ) : hooks = list ( ) # Checkpoint saver hook if self . saver_spec is not None and ( self . execution_type == 'single' or self . distributed_spec [ 'task_index' ] == 0 ) : self . saver_directory = self . saver_spec [ 'directory' ] hooks . append ( tf . train . CheckpointSaverHook ( checkpoint_dir = self . saver_directory , save_secs = self . saver_spec . get ( 'seconds' , None if 'steps' in self . saver_spec else 600 ) , save_steps = self . saver_spec . get ( 'steps' ) , # Either one or the other has to be set. saver = None , # None since given via 'scaffold' argument. checkpoint_basename = self . saver_spec . get ( 'basename' , 'model.ckpt' ) , scaffold = self . scaffold , listeners = None ) ) else : self . saver_directory = None # Stop at step hook # hooks.append(tf.train.StopAtStepHook( # num_steps=???, # This makes more sense, if load and continue training. # last_step=None # Either one or the other has to be set. # )) # # Step counter hook # hooks.append(tf.train.StepCounterHook( # every_n_steps=counter_config.get('steps', 100), # Either one or the other has to be set. # every_n_secs=counter_config.get('secs'), # Either one or the other has to be set. # output_dir=None, # None since given via 'summary_writer' argument. # summary_writer=summary_writer # )) # Other available hooks: # tf.train.FinalOpsHook(final_ops, final_ops_feed_dict=None) # tf.train.GlobalStepWaiterHook(wait_until_step) # tf.train.LoggingTensorHook(tensors, every_n_iter=None, every_n_secs=None) # tf.train.NanTensorHook(loss_tensor, fail_on_nan_loss=True) # tf.train.ProfilerHook(save_steps=None, save_secs=None, output_dir='', show_dataflow=True, show_memory=False) return hooks
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
1365	def get_required_arguments_metricnames ( self ) : try : metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) if not metricnames : raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) return metricnames except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
11752	def compute_precedence ( terminals , productions , precedence_levels ) : precedence = collections . OrderedDict ( ) for terminal in terminals : precedence [ terminal ] = DEFAULT_PREC level_precs = range ( len ( precedence_levels ) , 0 , - 1 ) for i , level in zip ( level_precs , precedence_levels ) : assoc = level [ 0 ] for symbol in level [ 1 : ] : precedence [ symbol ] = ( assoc , i ) for production , prec_symbol in productions : if prec_symbol is None : prod_terminals = [ symbol for symbol in production . rhs if symbol in terminals ] or [ None ] precedence [ production ] = precedence . get ( prod_terminals [ - 1 ] , DEFAULT_PREC ) else : precedence [ production ] = precedence . get ( prec_symbol , DEFAULT_PREC ) return precedence
6345	def stem ( self , word ) : terminate = False intact = True while not terminate : for n in range ( 6 , 0 , - 1 ) : if word [ - n : ] in self . _rule_table [ n ] : accept = False if len ( self . _rule_table [ n ] [ word [ - n : ] ] ) < 4 : for rule in self . _rule_table [ n ] [ word [ - n : ] ] : ( word , accept , intact , terminate , ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : rule = self . _rule_table [ n ] [ word [ - n : ] ] ( word , accept , intact , terminate ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : break return word
1833	def JCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC )
9656	def get_the_node_dict ( G , name ) : for node in G . nodes ( data = True ) : if node [ 0 ] == name : return node [ 1 ]
2258	def argsort ( indexable , key = None , reverse = False ) : # Create an iterator of value/key pairs if isinstance ( indexable , collections_abc . Mapping ) : vk_iter = ( ( v , k ) for k , v in indexable . items ( ) ) else : vk_iter = ( ( v , k ) for k , v in enumerate ( indexable ) ) # Sort by values and extract the indices if key is None : indices = [ k for v , k in sorted ( vk_iter , reverse = reverse ) ] else : # If key is provided, call it using the value as input indices = [ k for v , k in sorted ( vk_iter , key = lambda vk : key ( vk [ 0 ] ) , reverse = reverse ) ] return indices
954	def getCallerInfo ( depth = 2 ) : f = sys . _getframe ( depth ) method_name = f . f_code . co_name filename = f . f_code . co_filename arg_class = None args = inspect . getargvalues ( f ) if len ( args [ 0 ] ) > 0 : arg_name = args [ 0 ] [ 0 ] # potentially the 'self' arg if its a method arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ return ( method_name , filename , arg_class )
2573	def _create_task_log_info ( self , task_id , fail_mode = None ) : info_to_monitor = [ 'func_name' , 'fn_hash' , 'memoize' , 'checkpoint' , 'fail_count' , 'fail_history' , 'status' , 'id' , 'time_submitted' , 'time_returned' , 'executor' ] task_log_info = { "task_" + k : self . tasks [ task_id ] [ k ] for k in info_to_monitor } task_log_info [ 'run_id' ] = self . run_id task_log_info [ 'timestamp' ] = datetime . datetime . now ( ) task_log_info [ 'task_status_name' ] = self . tasks [ task_id ] [ 'status' ] . name task_log_info [ 'tasks_failed_count' ] = self . tasks_failed_count task_log_info [ 'tasks_completed_count' ] = self . tasks_completed_count task_log_info [ 'task_inputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task_log_info [ 'task_outputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task_log_info [ 'task_stdin' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdin' , None ) task_log_info [ 'task_stdout' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdout' , None ) task_log_info [ 'task_depends' ] = None if self . tasks [ task_id ] [ 'depends' ] is not None : task_log_info [ 'task_depends' ] = "," . join ( [ str ( t . _tid ) for t in self . tasks [ task_id ] [ 'depends' ] ] ) task_log_info [ 'task_elapsed_time' ] = None if self . tasks [ task_id ] [ 'time_returned' ] is not None : task_log_info [ 'task_elapsed_time' ] = ( self . tasks [ task_id ] [ 'time_returned' ] - self . tasks [ task_id ] [ 'time_submitted' ] ) . total_seconds ( ) if fail_mode is not None : task_log_info [ 'task_fail_mode' ] = fail_mode return task_log_info
12602	def col_values ( df , col_name ) : _check_cols ( df , [ col_name ] ) if 'O' in df [ col_name ] or pd . np . issubdtype ( df [ col_name ] . dtype , str ) : # if the column is of strings return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ]
8007	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) stanza = Presence ( stanza_type = "error" , from_jid = self . from_jid , to_jid = self . to_jid , stanza_id = self . stanza_id , status = self . _status , show = self . _show , priority = self . _priority , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : stanza . add_payload ( payload ) return stanza
11632	def codepointsInNamelist ( namFilename , unique_glyphs = False , cache = None ) : key = 'charset' if not unique_glyphs else 'ownCharset' internals_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) target = os . path . join ( internals_dir , namFilename ) result = readNamelist ( target , unique_glyphs , cache ) return result [ key ]
4140	def execute_script ( code_block , example_globals , image_path , fig_count , src_file , gallery_conf ) : time_elapsed = 0 stdout = '' # We need to execute the code print ( 'plotting code blocks in %s' % src_file ) plt . close ( 'all' ) cwd = os . getcwd ( ) # Redirect output to stdout and orig_stdout = sys . stdout try : # First cd in the original example dir, so that any file # created by the example get created in this directory os . chdir ( os . path . dirname ( src_file ) ) my_buffer = StringIO ( ) my_stdout = Tee ( sys . stdout , my_buffer ) sys . stdout = my_stdout t_start = time ( ) exec ( code_block , example_globals ) time_elapsed = time ( ) - t_start sys . stdout = orig_stdout my_stdout = my_buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my_stdout : stdout = CODE_OUTPUT . format ( indent ( my_stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure_list = save_figures ( image_path , fig_count , gallery_conf ) # Depending on whether we have one or more figures, we're using a # horizontal list or a single rst call to 'image'. image_list = "" if len ( figure_list ) == 1 : figure_name = figure_list [ 0 ] image_list = SINGLE_IMAGE % figure_name . lstrip ( '/' ) elif len ( figure_list ) > 1 : image_list = HLIST_HEADER for figure_name in figure_list : image_list += HLIST_IMAGE_TEMPLATE % figure_name . lstrip ( '/' ) except Exception : formatted_exception = traceback . format_exc ( ) print ( 80 * '_' ) print ( '%s is not compiling:' % src_file ) print ( formatted_exception ) print ( 80 * '_' ) figure_list = [ ] image_list = codestr2rst ( formatted_exception , lang = 'pytb' ) # Overrides the output thumbnail in the gallery for easy identification broken_img = os . path . join ( glr_path_static ( ) , 'broken_example.png' ) shutil . copyfile ( broken_img , os . path . join ( cwd , image_path . format ( 1 ) ) ) fig_count += 1 # raise count to avoid overwriting image # Breaks build on first example error if gallery_conf [ 'abort_on_example_error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig_stdout print ( " - time elapsed : %.2g sec" % time_elapsed ) code_output = "\n{0}\n\n{1}\n\n" . format ( image_list , stdout ) return code_output , time_elapsed , fig_count + len ( figure_list )
1216	def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
12094	def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) #do membrane test on every sweep swhlab . memtest . checkSweep ( abf ) #see all MT values swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) #generate IV clamp values abf . saveThing ( [ Xs , av1 ] , '01_iv' )
11924	def render_to ( path , template , * * data ) : try : renderer . render_to ( path , template , * * data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
2528	def get_annotation_date ( self , r_term ) : annotation_date_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationDate' ] , None ) ) ) if len ( annotation_date_list ) != 1 : self . error = True msg = 'Annotation must have exactly one annotation date.' self . logger . log ( msg ) return return six . text_type ( annotation_date_list [ 0 ] [ 2 ] )
10569	def filter_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : matched_songs = [ ] filtered_songs = [ ] for filepath in filepaths : try : song = _get_mutagen_metadata ( filepath ) except mutagen . MutagenError : filtered_songs . append ( filepath ) else : if include_filters or exclude_filters : if _check_filters ( song , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) : matched_songs . append ( filepath ) else : filtered_songs . append ( filepath ) else : matched_songs . append ( filepath ) return matched_songs , filtered_songs
10514	def windowuptime ( self , window_name ) : tmp_time = self . _remote_windowuptime ( window_name ) if tmp_time : tmp_time = tmp_time . split ( '-' ) start_time = tmp_time [ 0 ] . split ( ' ' ) end_time = tmp_time [ 1 ] . split ( ' ' ) _start_time = datetime . datetime ( int ( start_time [ 0 ] ) , int ( start_time [ 1 ] ) , int ( start_time [ 2 ] ) , int ( start_time [ 3 ] ) , int ( start_time [ 4 ] ) , int ( start_time [ 5 ] ) ) _end_time = datetime . datetime ( int ( end_time [ 0 ] ) , int ( end_time [ 1 ] ) , int ( end_time [ 2 ] ) , int ( end_time [ 3 ] ) , int ( end_time [ 4 ] ) , int ( end_time [ 5 ] ) ) return _start_time , _end_time return None
9830	def write ( self , filename ) : # comments (VMD chokes on lines of len > 80, so truncate) maxcol = 80 with open ( filename , 'w' ) as outfile : for line in self . comments : comment = '# ' + str ( line ) outfile . write ( comment [ : maxcol ] + '\n' ) # each individual object for component , object in self . sorted_components ( ) : object . write ( outfile ) # the field object itself DXclass . write ( self , outfile , quote = True ) for component , object in self . sorted_components ( ) : outfile . write ( 'component "%s" value %s\n' % ( component , str ( object . id ) ) )
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
949	def _createPeriodicActivities ( self ) : # Initialize periodic activities periodicActivities = [ ] # Metrics reporting class MetricsReportCb ( object ) : def __init__ ( self , taskRunner ) : self . __taskRunner = taskRunner return def __call__ ( self ) : self . __taskRunner . _getAndEmitExperimentMetrics ( ) reportMetrics = PeriodicActivityRequest ( repeating = True , period = 1000 , cb = MetricsReportCb ( self ) ) periodicActivities . append ( reportMetrics ) # Iteration progress class IterationProgressCb ( object ) : PROGRESS_UPDATE_PERIOD_TICKS = 1000 def __init__ ( self , taskLabel , requestedIterationCount , logger ) : self . __taskLabel = taskLabel self . __requestedIterationCount = requestedIterationCount self . __logger = logger self . __numIterationsSoFar = 0 def __call__ ( self ) : self . __numIterationsSoFar += self . PROGRESS_UPDATE_PERIOD_TICKS self . __logger . debug ( "%s: ITERATION PROGRESS: %s of %s" % ( self . __taskLabel , self . __numIterationsSoFar , self . __requestedIterationCount ) ) iterationProgressCb = IterationProgressCb ( taskLabel = self . __task [ 'taskLabel' ] , requestedIterationCount = self . __task [ 'iterationCount' ] , logger = self . __logger ) iterationProgressReporter = PeriodicActivityRequest ( repeating = True , period = IterationProgressCb . PROGRESS_UPDATE_PERIOD_TICKS , cb = iterationProgressCb ) periodicActivities . append ( iterationProgressReporter ) return periodicActivities
3355	def union ( self , iterable ) : _dict = self . _dict append = self . append for i in iterable : if i . id not in _dict : append ( i )
6976	def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr if kic_sdssgr < 0.8 : kepsdssr = ( keplermag - 0.2 * kic_sdssg ) / 0.8 else : kepsdssr = ( keplermag - 0.1 * kic_sdssg ) / 0.9 return kepsdssr
8493	def _handle_module ( args ) : module = _get_module_filename ( args . module ) if not module : _error ( "Could not load module or package: %r" , args . module ) elif isinstance ( module , Unparseable ) : _error ( "Could not determine module source: %r" , args . module ) _parse_and_output ( module , args )
12246	def get_bucket ( self , bucket_name , validate = True , headers = None , force = None ) : if force : bucket = super ( S3Connection , self ) . get_bucket ( bucket_name , validate , headers ) mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket if mimicdb . backend . sismember ( tpl . connection , bucket_name ) : return Bucket ( self , bucket_name ) else : if validate : raise S3ResponseError ( 404 , 'NoSuchBucket' ) else : return Bucket ( self , bucket_name )
9375	def get_run_time_period ( run_steps ) : init_ts_start = get_standardized_timestamp ( 'now' , None ) ts_start = init_ts_start ts_end = '0' for run_step in run_steps : if run_step . ts_start and run_step . ts_end : if run_step . ts_start < ts_start : ts_start = run_step . ts_start if run_step . ts_end > ts_end : ts_end = run_step . ts_end if ts_end == '0' : ts_end = None if ts_start == init_ts_start : ts_start = None logger . info ( 'get_run_time_period range returned ' + str ( ts_start ) + ' to ' + str ( ts_end ) ) return ts_start , ts_end
13516	def dimension ( self , length , draught , beam , speed , slenderness_coefficient , prismatic_coefficient ) : self . length = length self . draught = draught self . beam = beam self . speed = speed self . slenderness_coefficient = slenderness_coefficient self . prismatic_coefficient = prismatic_coefficient self . displacement = ( self . length / self . slenderness_coefficient ) ** 3 self . surface_area = 1.025 * ( 1.7 * self . length * self . draught + self . displacement / self . draught )
2544	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True self . file ( doc ) . comment = text return True else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
8356	def _toUnicode ( self , data , encoding ) : # strip Byte Order Mark (if present) if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
8253	def context_to_rgb ( self , str ) : matches = [ ] for clr in context : tags = context [ clr ] for tag in tags : if tag . startswith ( str ) or str . startswith ( tag ) : matches . append ( clr ) break matches = [ color ( name ) for name in matches ] return matches
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
10828	def delete ( cls , group , user ) : with db . session . begin_nested ( ) : cls . query . filter_by ( group = group , user_id = user . get_id ( ) ) . delete ( )
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
1627	def CheckForCopyright ( filename , lines , error ) : # We'll say it should occur by line 10. Don't forget there's a # dummy line at the front. for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : # means no copyright line was found error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found. ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
13405	def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
23	def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( "data" ) as f : return pickle . load ( f ) else : with open ( path , "rb" ) as f : return pickle . load ( f )
2439	def reset_annotations ( self ) : # FIXME: this state does not make sense self . annotation_date_set = False self . annotation_comment_set = False self . annotation_type_set = False self . annotation_spdx_id_set = False
12307	def auto_get_repo ( autooptions , debug = False ) : # plugin manager pluginmgr = plugins_get_mgr ( ) # get the repo manager repomgr = pluginmgr . get ( what = 'repomanager' , name = 'git' ) repo = None try : if debug : print ( "Looking repo" ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) except : # Clone the repo try : print ( "Checking and cloning if the dataset exists on backend" ) url = autooptions [ 'remoteurl' ] if debug : print ( "Doesnt exist. trying to clone: {}" . format ( url ) ) common_clone ( url ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) if debug : print ( "Cloning successful" ) except : # traceback.print_exc() yes = input ( "Repo doesnt exist. Should I create one? [yN]" ) if yes == 'y' : setup = "git" if autooptions [ 'remoteurl' ] . startswith ( 's3://' ) : setup = 'git+s3' repo = common_init ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] , setup = setup , force = True , options = autooptions ) if debug : print ( "Successfully inited repo" ) else : raise Exception ( "Cannot load repo" ) repo . options = autooptions return repo
2434	def set_lics_list_ver ( self , doc , value ) : if not self . lics_list_ver_set : self . lics_list_ver_set = True vers = version . Version . from_str ( value ) if vers is not None : doc . creation_info . license_list_version = vers return True else : raise SPDXValueError ( 'CreationInfo::LicenseListVersion' ) else : raise CardinalityError ( 'CreationInfo::LicenseListVersion' )
10951	def update ( self , params , values ) : return super ( State , self ) . update ( params , values )
11550	def setup ( self , configuration = "ModbusSerialClient(method='rtu',port='/dev/cu.usbmodem14101',baudrate=9600)" ) : from pymodbus3 . client . sync import ModbusSerialClient , ModbusUdpClient , ModbusTcpClient self . _client = eval ( configuration ) self . _client . connect ( )
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
5721	def _convert_schemas ( mapping , schemas ) : schemas = deepcopy ( schemas ) for schema in schemas : for fk in schema . get ( 'foreignKeys' , [ ] ) : resource = fk [ 'reference' ] [ 'resource' ] if resource != 'self' : if resource not in mapping : message = 'Not resource "%s" for foreign key "%s"' message = message % ( resource , fk ) raise ValueError ( message ) fk [ 'reference' ] [ 'resource' ] = mapping [ resource ] return schemas
11590	def _rc_smove ( self , src , dst , value ) : if self . type ( src ) != b ( "set" ) : return self . smove ( src + "{" + src + "}" , dst , value ) if self . type ( dst ) != b ( "set" ) : return self . smove ( dst + "{" + dst + "}" , src , value ) if self . srem ( src , value ) : return 1 if self . sadd ( dst , value ) else 0 return 0
8283	def _curvepoint ( self , t , x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , handles = False ) : # Originally from nodebox-gl mint = 1 - t x01 = x0 * mint + x1 * t y01 = y0 * mint + y1 * t x12 = x1 * mint + x2 * t y12 = y1 * mint + y2 * t x23 = x2 * mint + x3 * t y23 = y2 * mint + y3 * t out_c1x = x01 * mint + x12 * t out_c1y = y01 * mint + y12 * t out_c2x = x12 * mint + x23 * t out_c2y = y12 * mint + y23 * t out_x = out_c1x * mint + out_c2x * t out_y = out_c1y * mint + out_c2y * t if not handles : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y ) else : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y , x01 , y01 , x23 , y23 )
10843	def pending ( self ) : pending_updates = [ ] url = PATHS [ 'GET_PENDING' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : pending_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __pending = pending_updates return self . __pending
12671	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) : return self . __do_aggregate ( clazz , new_col , * args )
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
3784	def TP_dependent_property ( self , T , P ) : # Optimistic track, with the already set method if self . method_P : # retest within range if self . test_method_validity_P ( T , P , self . method_P ) : try : prop = self . calculate_P ( T , P , self . method_P ) if self . test_property_validity ( prop ) : return prop except : # pragma: no cover pass # get valid methods at T, and try them until one yields a valid # property; store the method_P and return the answer self . sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method_P in self . sorted_valid_methods_P : try : prop = self . calculate_P ( T , P , method_P ) if self . test_property_validity ( prop ) : self . method_P = method_P return prop except : # pragma: no cover pass # Function returns None if it does not work. return None
12481	def get_rcfile_variable_value ( var_name , app_name , section_name = None ) : cfg = get_rcfile_section ( app_name , section_name ) if var_name in cfg : raise KeyError ( 'Option {} not found in {} ' 'section.' . format ( var_name , section_name ) ) return cfg [ var_name ]
1149	def warnpy3k ( message , category = None , stacklevel = 1 ) : if sys . py3kwarning : if category is None : category = DeprecationWarning warn ( message , category , stacklevel + 1 )
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) # TODO: this process could use a timeout object like the disconnect # timeout thing, and ONLY send packets when none are sent! # We would do that by calling timeout.set() for a "sending" # timeout. If we're sending 100 messages a second, there is # no need to push some heartbeats in there also. self . put_client_msg ( "2::" )
6281	def keyboard_event ( self , key , action , modifier ) : # The well-known standard key for quick exit if key == self . keys . ESCAPE : self . close ( ) return # Toggle pause time if key == self . keys . SPACE and action == self . keys . ACTION_PRESS : self . timer . toggle_pause ( ) # Camera movement # Right if key == self . keys . D : if action == self . keys . ACTION_PRESS : self . sys_camera . move_right ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_right ( False ) # Left elif key == self . keys . A : if action == self . keys . ACTION_PRESS : self . sys_camera . move_left ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_left ( False ) # Forward elif key == self . keys . W : if action == self . keys . ACTION_PRESS : self . sys_camera . move_forward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_forward ( False ) # Backwards elif key == self . keys . S : if action == self . keys . ACTION_PRESS : self . sys_camera . move_backward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_backward ( False ) # UP elif key == self . keys . Q : if action == self . keys . ACTION_PRESS : self . sys_camera . move_down ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_down ( False ) # Down elif key == self . keys . E : if action == self . keys . ACTION_PRESS : self . sys_camera . move_up ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_up ( False ) # Screenshots if key == self . keys . X and action == self . keys . ACTION_PRESS : screenshot . create ( ) if key == self . keys . R and action == self . keys . ACTION_PRESS : project . instance . reload_programs ( ) if key == self . keys . RIGHT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) + 10.0 ) if key == self . keys . LEFT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) - 10.0 ) # Forward the event to the timeline self . timeline . key_event ( key , action , modifier )
3019	def from_json ( cls , json_data ) : if not isinstance ( json_data , dict ) : json_data = json . loads ( _helpers . _from_bytes ( json_data ) ) private_key_pkcs8_pem = None pkcs12_val = json_data . get ( _PKCS12_KEY ) password = None if pkcs12_val is None : private_key_pkcs8_pem = json_data [ '_private_key_pkcs8_pem' ] signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) else : # NOTE: This assumes that private_key_pkcs8_pem is not also # in the serialized data. This would be very incorrect # state. pkcs12_val = base64 . b64decode ( pkcs12_val ) password = json_data [ '_private_key_password' ] signer = crypt . Signer . from_string ( pkcs12_val , password ) credentials = cls ( json_data [ '_service_account_email' ] , signer , scopes = json_data [ '_scopes' ] , private_key_id = json_data [ '_private_key_id' ] , client_id = json_data [ 'client_id' ] , user_agent = json_data [ '_user_agent' ] , * * json_data [ '_kwargs' ] ) if private_key_pkcs8_pem is not None : credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem if pkcs12_val is not None : credentials . _private_key_pkcs12 = pkcs12_val if password is not None : credentials . _private_key_password = password credentials . invalid = json_data [ 'invalid' ] credentials . access_token = json_data [ 'access_token' ] credentials . token_uri = json_data [ 'token_uri' ] credentials . revoke_uri = json_data [ 'revoke_uri' ] token_expiry = json_data . get ( 'token_expiry' , None ) if token_expiry is not None : credentials . token_expiry = datetime . datetime . strptime ( token_expiry , client . EXPIRY_FORMAT ) return credentials
10107	def get_context_data ( self , * * kwargs ) : context = super ( TabView , self ) . get_context_data ( * * kwargs ) # Update the context with kwargs, TemplateView doesn't do this. context . update ( kwargs ) # Add tabs and "current" references to context process_tabs_kwargs = { 'tabs' : self . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : self , } context [ 'tabs' ] = self . _process_tabs ( * * process_tabs_kwargs ) context [ 'current_tab_id' ] = self . tab_id # Handle parent tabs if self . tab_parent is not None : # Verify that tab parent is valid if self . tab_parent not in self . _registry : msg = '%s has no attribute _is_tab' % self . tab_parent . __class__ . __name__ raise ImproperlyConfigured ( msg ) # Get parent tab instance parent = self . tab_parent ( ) # Add parent tabs to context process_parents_kwargs = { 'tabs' : parent . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : parent , } context [ 'parent_tabs' ] = self . _process_tabs ( * * process_parents_kwargs ) context [ 'parent_tab_id' ] = parent . tab_id # Handle child tabs if self . tab_id in self . _children : process_children_kwargs = { 'tabs' : [ t ( ) for t in self . _children [ self . tab_id ] ] , 'current_tab' : self , 'group_current_tab' : None , } context [ 'child_tabs' ] = self . _process_tabs ( * * process_children_kwargs ) return context
1546	def get_component_metrics ( component , cluster , env , topology , role ) : all_queries = metric_queries ( ) try : result = get_topology_metrics ( cluster , env , topology , component , [ ] , all_queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format_exc ( ) ) raise
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current # self.sched() next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
6848	def find_working_password ( self , usernames = None , host_strings = None ) : r = self . local_renderer if host_strings is None : host_strings = [ ] if not host_strings : host_strings . append ( self . genv . host_string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host_string in host_strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user_default_passwords [ username ] ) passwords . append ( self . genv . user_passwords [ username ] ) passwords . append ( self . env . default_password ) for password in passwords : with settings ( warn_only = True ) : r . env . host_string = host_string r . env . password = password r . env . user = username ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #print('ret.return_code:', ret.return_code) # print('ret000:[%s]' % ret) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. return host_string , username , password raise Exception ( 'No working login found.' )
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
1998	def visit ( self , node , use_fixed_point = False ) : cache = self . _cache visited = set ( ) stack = [ ] stack . append ( node ) while stack : node = stack . pop ( ) if node in cache : self . push ( cache [ node ] ) elif isinstance ( node , Operation ) : if node in visited : operands = [ self . pop ( ) for _ in range ( len ( node . operands ) ) ] value = self . _method ( node , * operands ) visited . remove ( node ) self . push ( value ) cache [ node ] = value else : visited . add ( node ) stack . append ( node ) stack . extend ( node . operands ) else : self . push ( self . _method ( node ) ) if use_fixed_point : old_value = None new_value = self . pop ( ) while old_value is not new_value : self . visit ( new_value ) old_value = new_value new_value = self . pop ( ) self . push ( new_value )
10851	def local_max_featuring ( im , radius = 2.5 , noise_size = 1. , bkg_size = None , minmass = 1. , trim_edge = False ) : if radius <= 0 : raise ValueError ( '`radius` must be > 0' ) #1. Remove noise filtered = nd . gaussian_filter ( im , noise_size , mode = 'mirror' ) #2. Remove long-wavelength background: if bkg_size is None : bkg_size = 2 * radius filtered -= nd . gaussian_filter ( filtered , bkg_size , mode = 'mirror' ) #3. Local max feature footprint = generate_sphere ( radius ) e = nd . maximum_filter ( filtered , footprint = footprint ) mass_im = nd . convolve ( filtered , footprint , mode = 'mirror' ) good_im = ( e == filtered ) * ( mass_im > minmass ) pos = np . transpose ( np . nonzero ( good_im ) ) if trim_edge : good = np . all ( pos > 0 , axis = 1 ) & np . all ( pos + 1 < im . shape , axis = 1 ) pos = pos [ good , : ] . copy ( ) masses = mass_im [ pos [ : , 0 ] , pos [ : , 1 ] , pos [ : , 2 ] ] . copy ( ) return pos , masses
12570	def _fill_missing_values ( df , range_values , fill_value = 0 , fill_method = None ) : idx_colnames = df . index . names idx_colranges = [ range_values [ x ] for x in idx_colnames ] fullindex = pd . Index ( [ p for p in product ( * idx_colranges ) ] , name = tuple ( idx_colnames ) ) fulldf = df . reindex ( index = fullindex , fill_value = fill_value , method = fill_method ) fulldf . index . names = idx_colnames return fulldf , idx_colranges
7280	def quit ( self ) : if self . _process is None : logger . debug ( 'Quit was called after self._process had already been released' ) return try : logger . debug ( 'Quitting OMXPlayer' ) process_group_id = os . getpgid ( self . _process . pid ) os . killpg ( process_group_id , signal . SIGTERM ) logger . debug ( 'SIGTERM Sent to pid: %s' % process_group_id ) self . _process_monitor . join ( ) except OSError : logger . error ( 'Could not find the process to kill' ) self . _process = None
8183	def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None
12890	def handle_text ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return doc . value . c8_array . text or None
6373	def accuracy_gain ( self ) : if self . population ( ) == 0 : return float ( 'NaN' ) random_accuracy = ( self . cond_pos_pop ( ) / self . population ( ) ) ** 2 + ( self . cond_neg_pop ( ) / self . population ( ) ) ** 2 return self . accuracy ( ) / random_accuracy
5163	def __intermediate_address ( self , address ) : for key in self . _address_keys : if key in address : del address [ key ] return address
10535	def get_categories ( limit = 20 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) try : res = _pybossa_req ( 'get' , 'category' , params = params ) if type ( res ) . __name__ == 'list' : return [ Category ( category ) for category in res ] else : raise TypeError except : raise
9372	def read_stream ( schema , stream , * , buffer_size = io . DEFAULT_BUFFER_SIZE ) : reader = _lancaster . Reader ( schema ) buf = stream . read ( buffer_size ) remainder = b'' while len ( buf ) > 0 : values , n = reader . read_seq ( buf ) yield from values remainder = buf [ n : ] buf = stream . read ( buffer_size ) if len ( buf ) > 0 and len ( remainder ) > 0 : ba = bytearray ( ) ba . extend ( remainder ) ba . extend ( buf ) buf = memoryview ( ba ) . tobytes ( ) if len ( remainder ) > 0 : raise EOFError ( '{} bytes remaining but could not continue reading ' 'from stream' . format ( len ( remainder ) ) )
8670	def get_key ( key_name , value_name , jsonify , no_decrypt , stash , passphrase , backend ) : if value_name and no_decrypt : sys . exit ( 'VALUE_NAME cannot be used in conjuction with --no-decrypt' ) stash = _get_stash ( backend , stash , passphrase , quiet = jsonify or value_name ) try : key = stash . get ( key_name = key_name , decrypt = not no_decrypt ) except GhostError as ex : sys . exit ( ex ) if not key : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) if value_name : key = key [ 'value' ] . get ( value_name ) if not key : sys . exit ( 'Value name `{0}` could not be found under key `{1}`' . format ( value_name , key_name ) ) if jsonify or value_name : click . echo ( json . dumps ( key , indent = 4 , sort_keys = False ) . strip ( '"' ) , nl = True ) else : click . echo ( 'Retrieving key...' ) click . echo ( '\n' + _prettify_dict ( key ) )
8687	def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0 ]
759	def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break
4343	def silence ( self , location = 0 , silence_threshold = 0.1 , min_silence_duration = 0.1 , buffer_around_silence = False ) : if location not in [ - 1 , 0 , 1 ] : raise ValueError ( "location must be one of -1, 0, 1." ) if not is_number ( silence_threshold ) or silence_threshold < 0 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) elif silence_threshold >= 100 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) if not is_number ( min_silence_duration ) or min_silence_duration <= 0 : raise ValueError ( "min_silence_duration must be a positive number." ) if not isinstance ( buffer_around_silence , bool ) : raise ValueError ( "buffer_around_silence must be a boolean." ) effect_args = [ ] if location == - 1 : effect_args . append ( 'reverse' ) if buffer_around_silence : effect_args . extend ( [ 'silence' , '-l' ] ) else : effect_args . append ( 'silence' ) effect_args . extend ( [ '1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == 0 : effect_args . extend ( [ '-1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'silence' ) return self
5224	def market_timing ( ticker , dt , timing = 'EOD' , tz = 'local' ) -> str : logger = logs . get_logger ( market_timing ) exch = pd . Series ( exch_info ( ticker = ticker ) ) if any ( req not in exch . index for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return '' mkt_time = { 'BOD' : exch . day [ 0 ] , 'FINISHED' : exch . allday [ - 1 ] } . get ( timing , exch . day [ - 1 ] ) cur_dt = pd . Timestamp ( str ( dt ) ) . strftime ( '%Y-%m-%d' ) if tz == 'local' : return f'{cur_dt} {mkt_time}' return timezone . tz_convert ( f'{cur_dt} {mkt_time}' , to_tz = tz , from_tz = exch . tz )
9796	def get ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : response = PolyaxonClient ( ) . experiment_group . get_experiment_group ( user , project_name , _group ) cache . cache ( config_manager = GroupManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_group_details ( response )
858	def _getStartRow ( self , bookmark ) : bookMarkDict = json . loads ( bookmark ) realpath = os . path . realpath ( self . _filename ) bookMarkFile = bookMarkDict . get ( 'filepath' , None ) if bookMarkFile != realpath : print ( "Ignoring bookmark due to mismatch between File's " "filename realpath vs. bookmark; realpath: %r; bookmark: %r" ) % ( realpath , bookMarkDict ) return 0 else : return bookMarkDict [ 'currentRow' ]
13011	def get_own_ip ( ) : own_ip = None interfaces = psutil . net_if_addrs ( ) for _ , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF_INET : ip_address = ipaddress . ip_address ( detail . address ) if not ( ip_address . is_link_local or ip_address . is_loopback ) : own_ip = str ( ip_address ) break return own_ip
4820	def refresh_token ( func ) : @ wraps ( func ) def inner ( self , * args , * * kwargs ) : """ Before calling the wrapped function, we check if the JWT token is expired, and if so, re-connect. """ if self . token_expired ( ) : self . connect ( ) return func ( self , * args , * * kwargs ) return inner
3869	async def update_read_timestamp ( self , read_timestamp = None ) : if read_timestamp is None : read_timestamp = ( self . events [ - 1 ] . timestamp if self . events else datetime . datetime . now ( datetime . timezone . utc ) ) if read_timestamp > self . latest_read_timestamp : logger . info ( 'Setting {} latest_read_timestamp from {} to {}' . format ( self . id_ , self . latest_read_timestamp , read_timestamp ) ) # Prevent duplicate requests by updating the conversation now. state = self . _conversation . self_conversation_state state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( read_timestamp ) ) try : await self . _client . update_watermark ( hangouts_pb2 . UpdateWatermarkRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , last_read_timestamp = parsers . to_timestamp ( read_timestamp ) , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to update read timestamp: {}' . format ( e ) ) raise
5628	def hook ( self , event_type = 'push' ) : def decorator ( func ) : self . _hooks [ event_type ] . append ( func ) return func return decorator
12565	def get_3D_from_4D ( image , vol_idx = 0 ) : img = check_img ( image ) hdr , aff = get_img_info ( img ) if len ( img . shape ) != 4 : raise AttributeError ( 'Volume in {} does not have 4 dimensions.' . format ( repr_imgs ( img ) ) ) if not 0 <= vol_idx < img . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, ' 'not {}.' . format ( repr_imgs ( img ) , img . shape [ 3 ] , vol_idx ) ) img_data = img . get_data ( ) new_vol = img_data [ : , : , : , vol_idx ] . copy ( ) hdr . set_data_shape ( hdr . get_data_shape ( ) [ : 3 ] ) return new_vol , hdr , aff
7323	def sendmail ( message , sender , recipients , config_filename ) : # Read config file from disk to get SMTP server host, port, username if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) # Prompt for password if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) # Connect to SMTP server if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) # Send credentials if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) # Send message. Note that we can't use the elegant # "smtp.send_message(message)" because that's python3 only smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
7115	def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False , build = False ) : sources = [ # Machine-specific ( configs_dirs , 'hostname' ) , ( configs_dirs , 'hostname-local' ) , ( configs_dirs , 'hostname-build' ) , # Global ( configs_dirs , 'common' ) , # Environment + Cluster ( configs_dirs , 'common-%s' % environment ) , ( configs_dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs_dirs , 'common-local' ) , ( configs_dirs , 'common-build' ) , # Machine-specific overrides ( configs_dirs , 'common-overrides' ) , # Application-specific ( [ app_dir ] , '%s-default' % app ) , ( [ app_dir ] , '%s-%s' % ( app , environment ) ) , ( [ app_dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs_dirs , app ) , ( configs_dirs , '%s-%s' % ( app , environment ) ) , ( configs_dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app_dir ] , '%s-local' % app ) , ( [ app_dir ] , '%s-build' % app ) , ( configs_dirs , '%s-local' % app ) , ( configs_dirs , '%s-build' % app ) , # Machine-specific application override ( configs_dirs , '%s-overrides' % app ) , ] # Filter out build sources if not requested if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] # Filter out local sources if not build and not local if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available_sources ( sources )
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) # Don't lock down if django-lockdown is disabled altogether. if settings . ENABLED is False : return None # Don't lock down if the client REMOTE_ADDR matched and is part of the # exception list. if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : # If forwarding proxies are used they must be listed as trusted trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : # If REMOTE_ADDR is a trusted proxy check x-forwarded-for x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None # Don't lock down if the URL matches an exception pattern. if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None # Don't lock down if the URL resolves to a whitelisted view. try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None # Don't lock down if outside of the lockdown dates. if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , * * self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) # Don't lock down if the user is already authorized for previewing. if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
12185	def extract ( self , document , selector , debug_offset = '' ) : selected = self . select ( document , selector ) if selected is not None : if isinstance ( selected , ( list , tuple ) ) : # FIXME: return None or return empty list? if not len ( selected ) : return return [ self . _extract_single ( m ) for m in selected ] else : return self . _extract_single ( selected ) # selector did not match anything else : if self . DEBUG : print ( debug_offset , "selector did not match anything; return None" ) return None
12061	def processArgs ( ) : if len ( sys . argv ) < 2 : print ( "\n\nERROR:" ) print ( "this script requires arguments!" ) print ( 'try "python command.py info"' ) return if sys . argv [ 1 ] == 'info' : print ( "import paths:\n " , "\n " . join ( sys . path ) ) print ( ) print ( "python version:" , sys . version ) print ( "SWHLab path:" , __file__ ) print ( "SWHLab version:" , swhlab . __version__ ) return if sys . argv [ 1 ] == 'glanceFolder' : abfFolder = swhlab . common . gui_getFolder ( ) if not abfFolder or not os . path . isdir ( abfFolder ) : print ( "bad path" ) return fnames = sorted ( glob . glob ( abfFolder + "/*.abf" ) ) outFolder = tempfile . gettempdir ( ) + "/swhlab/" if os . path . exists ( outFolder ) : shutil . rmtree ( outFolder ) os . mkdir ( outFolder ) outFile = outFolder + "/index.html" out = '<html><body>' out += '<h2>%s</h2>' % abfFolder for i , fname in enumerate ( fnames ) : print ( "\n\n### PROCESSING %d of %d" % ( i , len ( fnames ) ) ) saveAs = os . path . join ( os . path . dirname ( outFolder ) , os . path . basename ( fname ) ) + ".png" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href="%s"><img src="%s"></a><br>' % ( saveAs , saveAs ) swhlab . analysis . glance . processAbf ( fname , saveAs ) out += '</body></html>' with open ( outFile , 'w' ) as f : f . write ( out ) webbrowser . open_new_tab ( outFile ) return print ( "\n\nERROR:\nI'm not sure how to process these arguments!" ) print ( sys . argv )
10597	def h_L ( self , L , theta , Ts , * * statef ) : Nu_L = self . Nu_L ( L , theta , Ts , * * statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_L * k / L
8523	def add_float ( self , name , min , max , warp = None ) : min , max = map ( float , ( min , max ) ) if not min < max : raise ValueError ( 'variable %s: min >= max error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = FloatVariable ( name , min , max , warp )
3290	def get_preferred_path ( self ) : if self . path in ( "" , "/" ) : return "/" # Append '/' for collections if self . is_collection and not self . path . endswith ( "/" ) : return self . path + "/" # TODO: handle case-sensitivity, depending on OS # (FileSystemProvider could do this with os.path: # (?) on unix we can assume that the path already matches exactly the case of filepath # on windows we could use path.lower() or get the real case from the # file system return self . path
5082	def save_statement ( self , statement ) : response = self . lrs . save_statement ( statement ) if not response : raise ClientError ( 'EnterpriseXAPIClient request failed.' )
397	def cross_entropy ( output , target , name = None ) : if name is None : raise Exception ( "Please give a unique name to tl.cost.cross_entropy for TF1.0+" ) return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) # List file to backup files = self . file_list ( ) # then download each of then self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
11072	def _to_primary_key ( self , value ) : if value is None : return None if isinstance ( value , self . base_class ) : if not value . _is_loaded : raise exceptions . DatabaseError ( 'Record must be loaded.' ) return value . _primary_key return self . base_class . _to_primary_key ( value )
5401	def _get_logging_env ( self , logging_uri , user_project ) : if not logging_uri . endswith ( '.log' ) : raise ValueError ( 'Logging URI must end in ".log": {}' . format ( logging_uri ) ) logging_prefix = logging_uri [ : - len ( '.log' ) ] return { 'LOGGING_PATH' : '{}.log' . format ( logging_prefix ) , 'STDOUT_PATH' : '{}-stdout.log' . format ( logging_prefix ) , 'STDERR_PATH' : '{}-stderr.log' . format ( logging_prefix ) , 'USER_PROJECT' : user_project , }
4691	def decode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Encryption " raw = bytes ( message , "ascii" ) cleartext = aes . decrypt ( unhexlify ( raw ) ) " Checksum " checksum = cleartext [ 0 : 4 ] message = cleartext [ 4 : ] message = _unpad ( message , 16 ) " Verify checksum " check = hashlib . sha256 ( message ) . digest ( ) [ 0 : 4 ] if check != checksum : # pragma: no cover raise ValueError ( "checksum verification failure" ) return message . decode ( "utf8" )
6374	def pr_lmean ( self ) : precision = self . precision ( ) recall = self . recall ( ) if not precision or not recall : return 0.0 elif precision == recall : return precision return ( precision - recall ) / ( math . log ( precision ) - math . log ( recall ) )
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
7577	def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : ## concat results for k=x reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) ## create CLUMPP args string outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] ## call clumpp proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) ## cleanup for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ## parse clumpp results file ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] ## apply names to cols and rows table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
8858	def on_goto_out_of_doc ( self , assignment ) : editor = self . open_file ( assignment . module_path ) if editor : TextHelper ( editor ) . goto_line ( assignment . line , assignment . column )
6675	def umask ( self , use_sudo = False ) : func = use_sudo and run_as_root or self . run return func ( 'umask' )
10731	def lower ( option , value ) : if type ( option ) is str : option = option . lower ( ) if type ( value ) is str : value = value . lower ( ) return ( option , value )
6215	def load_glb ( self ) : with open ( self . path , 'rb' ) as fd : # Check header magic = fd . read ( 4 ) if magic != GLTF_MAGIC_HEADER : raise ValueError ( "{} has incorrect header {} != {}" . format ( self . path , magic , GLTF_MAGIC_HEADER ) ) version = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] if version != 2 : raise ValueError ( "{} has unsupported version {}" . format ( self . path , version ) ) # Total file size including headers _ = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] # noqa # Chunk 0 - json chunk_0_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_0_type = fd . read ( 4 ) if chunk_0_type != b'JSON' : raise ValueError ( "Expected JSON chunk, not {} in file {}" . format ( chunk_0_type , self . path ) ) json_meta = fd . read ( chunk_0_length ) . decode ( ) # chunk 1 - binary buffer chunk_1_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_1_type = fd . read ( 4 ) if chunk_1_type != b'BIN\x00' : raise ValueError ( "Expected BIN chunk, not {} in file {}" . format ( chunk_1_type , self . path ) ) self . meta = GLTFMeta ( self . path , json . loads ( json_meta ) , binary_buffer = fd . read ( chunk_1_length ) )
10104	def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
12623	def dir_match ( regex , wd = os . curdir ) : ls = os . listdir ( wd ) filt = re . compile ( regex ) . match return filter_list ( ls , filt )
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : """Fail for edges with PubMed citations matching the contained PubMed identifier. :return: If the edge has a PubMed citation with the contained PubMed identifier """ return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : """Pass for edges with PubMed citations matching one of the contained PubMed identifiers. :return: If the edge has a PubMed citation with one of the contained PubMed identifiers """ return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
606	def nupicBindingsPrereleaseInstalled ( ) : try : nupicDistribution = pkg_resources . get_distribution ( "nupic.bindings" ) if pkg_resources . parse_version ( nupicDistribution . version ) . is_prerelease : # A pre-release dev version of nupic.bindings is installed. return True except pkg_resources . DistributionNotFound : pass # Silently ignore. The absence of nupic.bindings will be handled by # setuptools by default return False
3970	def _get_build_path ( app_spec ) : if os . path . isabs ( app_spec [ 'build' ] ) : return app_spec [ 'build' ] return os . path . join ( Repo ( app_spec [ 'repo' ] ) . local_path , app_spec [ 'build' ] )
5732	def parse_response ( gdb_mi_text ) : stream = StringStream ( gdb_mi_text , debug = _DEBUG ) if _GDB_MI_NOTIFY_RE . match ( gdb_mi_text ) : token , message , payload = _get_notify_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "notify" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_RESULT_RE . match ( gdb_mi_text ) : token , message , payload = _get_result_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "result" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) : return { "type" : "console" , "message" : None , "payload" : _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_LOG_RE . match ( gdb_mi_text ) : return { "type" : "log" , "message" : None , "payload" : _GDB_MI_LOG_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) : return { "type" : "target" , "message" : None , "payload" : _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif response_is_finished ( gdb_mi_text ) : return { "type" : "done" , "message" : None , "payload" : None } else : # This was not gdb mi output, so it must have just been printed by # the inferior program that's being debugged return { "type" : "output" , "message" : None , "payload" : gdb_mi_text }
7095	def init_map ( self ) : d = self . declaration if d . show_location : self . set_show_location ( d . show_location ) if d . show_traffic : self . set_show_traffic ( d . show_traffic ) if d . show_indoors : self . set_show_indoors ( d . show_indoors ) if d . show_buildings : self . set_show_buildings ( d . show_buildings ) #: Local ref access is faster mapview = self . map mid = mapview . getId ( ) #: Connect signals #: Camera mapview . onCameraChange . connect ( self . on_camera_changed ) mapview . onCameraMoveStarted . connect ( self . on_camera_move_started ) mapview . onCameraMoveCanceled . connect ( self . on_camera_move_stopped ) mapview . onCameraIdle . connect ( self . on_camera_move_stopped ) mapview . setOnCameraChangeListener ( mid ) mapview . setOnCameraMoveStartedListener ( mid ) mapview . setOnCameraMoveCanceledListener ( mid ) mapview . setOnCameraIdleListener ( mid ) #: Clicks mapview . onMapClick . connect ( self . on_map_clicked ) mapview . setOnMapClickListener ( mid ) mapview . onMapLongClick . connect ( self . on_map_long_clicked ) mapview . setOnMapLongClickListener ( mid ) #: Markers mapview . onMarkerClick . connect ( self . on_marker_clicked ) mapview . setOnMarkerClickListener ( self . map . getId ( ) ) mapview . onMarkerDragStart . connect ( self . on_marker_drag_start ) mapview . onMarkerDrag . connect ( self . on_marker_drag ) mapview . onMarkerDragEnd . connect ( self . on_marker_drag_end ) mapview . setOnMarkerDragListener ( mid ) #: Info window mapview . onInfoWindowClick . connect ( self . on_info_window_clicked ) mapview . onInfoWindowLongClick . connect ( self . on_info_window_long_clicked ) mapview . onInfoWindowClose . connect ( self . on_info_window_closed ) mapview . setOnInfoWindowClickListener ( mid ) mapview . setOnInfoWindowCloseListener ( mid ) mapview . setOnInfoWindowLongClickListener ( mid ) #: Polys mapview . onPolygonClick . connect ( self . on_poly_clicked ) mapview . onPolylineClick . connect ( self . on_poly_clicked ) mapview . setOnPolygonClickListener ( mid ) mapview . setOnPolylineClickListener ( mid ) #: Circle mapview . onCircleClick . connect ( self . on_circle_clicked ) mapview . setOnCircleClickListener ( mid )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
269	def detect_intraday ( positions , transactions , threshold = 0.25 ) : daily_txn = transactions . copy ( ) daily_txn . index = daily_txn . index . date txn_count = daily_txn . groupby ( level = 0 ) . symbol . nunique ( ) . sum ( ) daily_pos = positions . drop ( 'cash' , axis = 1 ) . replace ( 0 , np . nan ) return daily_pos . count ( axis = 1 ) . sum ( ) / txn_count < threshold
10203	def register_queries ( ) : return [ dict ( query_name = 'bucket-file-download-histogram' , query_class = ESDateHistogramQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) , required_filters = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) ) ) , dict ( query_name = 'bucket-file-download-total' , query_class = ESTermsQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( # bucket_id='bucket_id', ) , required_filters = dict ( bucket_id = 'bucket_id' , ) , aggregated_fields = [ 'file_key' ] ) ) , ]
6735	def write_temp_file_or_dryrun ( content , * args , * * kwargs ) : dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp_fn = tempfile . mkstemp ( ) os . remove ( tmp_fn ) cmd_run = 'local' cmd = 'cat <<EOT >> %s\n%s\nEOT' % ( tmp_fn , content ) if BURLAP_COMMAND_PREFIX : print ( '%s %s: %s' % ( render_command_prefix ( ) , cmd_run , cmd ) ) else : print ( cmd ) else : fd , tmp_fn = tempfile . mkstemp ( ) fout = open ( tmp_fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp_fn
8301	def dispatch ( self , message , source = None ) : msgtype = "" try : if type ( message [ 0 ] ) == str : # got a single message address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except KeyError , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except IndexError , e : print '%s: %s' % ( e , message ) pass except None , e : print "Exception in" , address , "callback :" , e return
5658	def _validate_no_null_values ( self ) : for table in DB_TABLE_NAMES : null_not_ok_warning = "Null values in must-have columns in table {table}" . format ( table = table ) null_warn_warning = "Null values in good-to-have columns in table {table}" . format ( table = table ) null_not_ok_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_NOT_OK [ table ] null_warn_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_OK_BUT_WARN [ table ] # CW, TODO: make this validation source by source df = self . gtfs . get_table ( table ) for warning , fields in zip ( [ null_not_ok_warning , null_warn_warning ] , [ null_not_ok_fields , null_warn_fields ] ) : null_unwanted_df = df [ fields ] rows_having_null = null_unwanted_df . isnull ( ) . any ( 1 ) if sum ( rows_having_null ) > 0 : rows_having_unwanted_null = df [ rows_having_null . values ] self . warnings_container . add_warning ( warning , rows_having_unwanted_null , len ( rows_having_unwanted_null ) )
9477	def parse_dom ( dom ) : root = dom . getElementsByTagName ( "graphml" ) [ 0 ] graph = root . getElementsByTagName ( "graph" ) [ 0 ] name = graph . getAttribute ( 'id' ) g = Graph ( name ) # # Get attributes # attributes = [] # for attr in root.getElementsByTagName("key"): # attributes.append(attr) # Get nodes for node in graph . getElementsByTagName ( "node" ) : n = g . add_node ( id = node . getAttribute ( 'id' ) ) for attr in node . getElementsByTagName ( "data" ) : if attr . firstChild : n [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : n [ attr . getAttribute ( "key" ) ] = "" # Get edges for edge in graph . getElementsByTagName ( "edge" ) : source = edge . getAttribute ( 'source' ) dest = edge . getAttribute ( 'target' ) # source/target attributes refer to IDs: http://graphml.graphdrawing.org/xmlns/1.1/graphml-structure.xsd e = g . add_edge_by_id ( source , dest ) for attr in edge . getElementsByTagName ( "data" ) : if attr . firstChild : e [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : e [ attr . getAttribute ( "key" ) ] = "" return g
5990	def constant_regularization_matrix_from_pixel_neighbors ( coefficients , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( pixel_neighbors ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_coefficient = coefficients [ 0 ] ** 2.0 for i in range ( pixels ) : regularization_matrix [ i , i ] += 1e-8 for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_coefficient regularization_matrix [ i , neighbor_index ] -= regularization_coefficient return regularization_matrix
4619	def unlocked ( self ) : if self . password is not None : return bool ( self . password ) else : if ( "UNLOCK" in os . environ and os . environ [ "UNLOCK" ] and self . config_key in self . config and self . config [ self . config_key ] ) : log . debug ( "Trying to use environmental " "variable to unlock wallet" ) self . unlock ( os . environ . get ( "UNLOCK" ) ) return bool ( self . password ) return False
7184	def copy_arguments_to_annotations ( args , type_comment , * , is_method = False ) : if isinstance ( type_comment , ast3 . Ellipsis ) : return expected = len ( args . args ) if args . vararg : expected += 1 expected += len ( args . kwonlyargs ) if args . kwarg : expected += 1 actual = len ( type_comment ) if isinstance ( type_comment , list ) else 1 if expected != actual : if is_method and expected - actual == 1 : pass # fine, we're just skipping `self`, `cls`, etc. else : raise ValueError ( f"number of arguments in type comment doesn't match; " + f"expected {expected}, found {actual}" ) if isinstance ( type_comment , list ) : next_value = type_comment . pop else : # If there's just one value, only one of the loops and ifs below will # be populated. We ensure this with the expected/actual length check # above. _tc = type_comment def next_value ( index : int = 0 ) -> ast3 . expr : return _tc for arg in args . args [ expected - actual : ] : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . vararg : ensure_no_annotation ( args . vararg . annotation ) args . vararg . annotation = next_value ( 0 ) for arg in args . kwonlyargs : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . kwarg : ensure_no_annotation ( args . kwarg . annotation ) args . kwarg . annotation = next_value ( 0 )
8952	def get_project_root ( ) : try : tasks_py = sys . modules [ 'tasks' ] except KeyError : return None else : return os . path . abspath ( os . path . dirname ( tasks_py . __file__ ) )
1995	def _named_stream ( self , name , binary = False ) : with self . _store . save_stream ( self . _named_key ( name ) , binary = binary ) as s : yield s
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == '___' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) # fname=os.path.join(path,fname) # fname2=os.path.join(path,fname2) # if not os.path.exists(fname2): # os.rename(fname,fname2) return
1318	def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
13059	def get_inventory ( self ) : if self . _inventory is not None : return self . _inventory self . _inventory = self . resolver . getMetadata ( ) return self . _inventory
2893	def get_outgoing_sequence_names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing_sequence_flows_by_id . values ( ) ) ] )
4116	def lsf2poly ( lsf ) : # Reference: A.M. Kondoz, "Digital Speech: Coding for Low Bit Rate Communications # Systems" John Wiley & Sons 1994 ,Chapter 4 # Line spectral frequencies must be real. lsf = numpy . array ( lsf ) if max ( lsf ) > numpy . pi or min ( lsf ) < 0 : raise ValueError ( 'Line spectral frequencies must be between 0 and pi.' ) p = len ( lsf ) # model order # Form zeros using the LSFs and unit amplitudes z = numpy . exp ( 1.j * lsf ) # Separate the zeros to those belonging to P and Q rQ = z [ 0 : : 2 ] rP = z [ 1 : : 2 ] # Include the conjugates as well rQ = numpy . concatenate ( ( rQ , rQ . conjugate ( ) ) ) rP = numpy . concatenate ( ( rP , rP . conjugate ( ) ) ) # Form the polynomials P and Q, note that these should be real Q = numpy . poly ( rQ ) P = numpy . poly ( rP ) # Form the sum and difference filters by including known roots at z = 1 and # z = -1 if p % 2 : # Odd order: z = +1 and z = -1 are roots of the difference filter, P1(z) P1 = numpy . convolve ( P , [ 1 , 0 , - 1 ] ) Q1 = Q else : # Even order: z = -1 is a root of the sum filter, Q1(z) and z = 1 is a # root of the difference filter, P1(z) P1 = numpy . convolve ( P , [ 1 , - 1 ] ) Q1 = numpy . convolve ( Q , [ 1 , 1 ] ) # Prediction polynomial is formed by averaging P1 and Q1 a = .5 * ( P1 + Q1 ) return a [ 0 : - 1 : 1 ]
7769	def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
13339	def transpose ( a , axes = None ) : if isinstance ( a , np . ndarray ) : return np . transpose ( a , axes ) elif isinstance ( a , RemoteArray ) : return a . transpose ( * axes ) elif isinstance ( a , Remote ) : return _remote_to_array ( a ) . transpose ( * axes ) elif isinstance ( a , DistArray ) : if axes is None : axes = range ( a . ndim - 1 , - 1 , - 1 ) axes = list ( axes ) if len ( set ( axes ) ) < len ( axes ) : raise ValueError ( "repeated axis in transpose" ) if sorted ( axes ) != list ( range ( a . ndim ) ) : raise ValueError ( "axes don't match array" ) distaxis = a . _distaxis new_distaxis = axes . index ( distaxis ) new_subarrays = [ ra . transpose ( * axes ) for ra in a . _subarrays ] return DistArray ( new_subarrays , new_distaxis ) else : return np . transpose ( a , axes )
11484	def _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing = False ) : item_id = _create_or_reuse_item ( local_folder , parent_folder_id , reuse_existing ) subdir_contents = sorted ( os . listdir ( local_folder ) ) # for each file in the subdir, add it to the item filecount = len ( subdir_contents ) for ( ind , current_file ) in enumerate ( subdir_contents ) : file_path = os . path . join ( local_folder , current_file ) log_ind = '({0} of {1})' . format ( ind + 1 , filecount ) _create_bitstream ( file_path , current_file , item_id , log_ind ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , item_id )
4735	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.pci.env: invalid SSH environment" ) return 1 pci = cij . env_to_dict ( PREFIX , REQUIRED ) pci [ "BUS_PATH" ] = "/sys/bus/pci" pci [ "DEV_PATH" ] = os . sep . join ( [ pci [ "BUS_PATH" ] , "devices" , pci [ "DEV_NAME" ] ] ) cij . env_export ( PREFIX , EXPORTED , pci ) return 0
13778	def FindFileContainingSymbol ( self , symbol ) : symbol = _NormalizeFullyQualifiedName ( symbol ) try : return self . _descriptors [ symbol ] . file except KeyError : pass try : return self . _enum_descriptors [ symbol ] . file except KeyError : pass try : file_proto = self . _internal_db . FindFileContainingSymbol ( symbol ) except KeyError as error : if self . _descriptor_db : file_proto = self . _descriptor_db . FindFileContainingSymbol ( symbol ) else : raise error if not file_proto : raise KeyError ( 'Cannot find a file containing %s' % symbol ) return self . _ConvertFileProtoToFileDescriptor ( file_proto )
3657	def remove_cti_file ( self , file_path : str ) : if file_path in self . _cti_files : self . _cti_files . remove ( file_path ) self . _logger . info ( 'Removed {0} from the CTI file list.' . format ( file_path ) )
6329	def gng_importer ( self , corpus_file ) : with c_open ( corpus_file , 'r' , encoding = 'utf-8' ) as gng : for line in gng : line = line . rstrip ( ) . split ( '\t' ) words = line [ 0 ] . split ( ) self . _add_to_ngcorpus ( self . ngcorpus , words , int ( line [ 2 ] ) )
3720	def ionic_strength ( mis , zis ) : return 0.5 * sum ( [ mi * zi * zi for mi , zi in zip ( mis , zis ) ] )
8012	def check_paypal_api_key ( app_configs = None , * * kwargs ) : messages = [ ] mode = getattr ( djpaypal_settings , "PAYPAL_MODE" , None ) if mode not in VALID_MODES : msg = "Invalid PAYPAL_MODE specified: {}." . format ( repr ( mode ) ) hint = "PAYPAL_MODE must be one of {}" . format ( ", " . join ( repr ( k ) for k in VALID_MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C001" ) ) for setting in "PAYPAL_CLIENT_ID" , "PAYPAL_CLIENT_SECRET" : if not getattr ( djpaypal_settings , setting , None ) : msg = "Invalid value specified for {}" . format ( setting ) hint = "Add PAYPAL_CLIENT_ID and PAYPAL_CLIENT_SECRET to your settings." messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C002" ) ) return messages
2836	def read ( self , length , assert_ss = True , deassert_ss = True ) : if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( length ) for i in range ( length ) : for j in range ( 8 ) : # Flip clock off base. self . _gpio . output ( self . _sclk , not self . _clock_base ) # Handle read on leading edge of clock. if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : # Set bit to 1 at appropriate location. result [ i ] |= self . _read_shift ( self . _mask , j ) else : # Set bit to 0 at appropriate location. result [ i ] &= ~ self . _read_shift ( self . _mask , j ) # Return clock to base. self . _gpio . output ( self . _sclk , self . _clock_base ) # Handle read on trailing edge of clock. if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : # Set bit to 1 at appropriate location. result [ i ] |= self . _read_shift ( self . _mask , j ) else : # Set bit to 0 at appropriate location. result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
6518	def is_excluded ( self , path ) : relpath = path . relative_to ( self . base_path ) . as_posix ( ) return matches_masks ( relpath , self . excludes )
4169	def zpk2tf ( z , p , k ) : import scipy . signal b , a = scipy . signal . zpk2tf ( z , p , k ) return b , a
4792	def is_unicode ( self ) : if type ( self . val ) is not unicode : self . _err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . __name__ ) ) return self
3394	def find_gene_knockout_reactions ( cobra_model , gene_list , compiled_gene_reaction_rules = None ) : potential_reactions = set ( ) for gene in gene_list : if isinstance ( gene , string_types ) : gene = cobra_model . genes . get_by_id ( gene ) potential_reactions . update ( gene . _reaction ) gene_set = { str ( i ) for i in gene_list } if compiled_gene_reaction_rules is None : compiled_gene_reaction_rules = { r : parse_gpr ( r . gene_reaction_rule ) [ 0 ] for r in potential_reactions } return [ r for r in potential_reactions if not eval_gpr ( compiled_gene_reaction_rules [ r ] , gene_set ) ]
2297	def fit ( self , x , y ) : train = np . vstack ( ( np . array ( [ self . featurize_row ( row . iloc [ 0 ] , row . iloc [ 1 ] ) for idx , row in x . iterrows ( ) ] ) , np . array ( [ self . featurize_row ( row . iloc [ 1 ] , row . iloc [ 0 ] ) for idx , row in x . iterrows ( ) ] ) ) ) labels = np . vstack ( ( y , - y ) ) . ravel ( ) verbose = 1 if self . verbose else 0 self . clf = CLF ( verbose = verbose , min_samples_leaf = self . L , n_estimators = self . E , max_depth = self . max_depth , n_jobs = self . n_jobs ) . fit ( train , labels )
1381	def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return exception_request = tmaster_pb2 . ExceptionLogRequest ( ) exception_request . component_name = component_name if len ( instances ) > 0 : exception_request . instances . extend ( instances ) request_str = exception_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) Log . debug ( "Creating request object." ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch exceptionsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) # Check the response code - error if it is in 400s or 500s responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) # Parse the response from tmaster. exception_response = tmaster_pb2 . ExceptionLogResponse ( ) exception_response . ParseFromString ( result . body ) if exception_response . status . status == common_pb2 . NOTOK : if exception_response . status . HasField ( "message" ) : raise tornado . gen . Return ( { "message" : exception_response . status . message } ) # Send response ret = [ ] for exception_log in exception_response . exceptions : ret . append ( { 'class_name' : exception_log . stacktrace , 'lasttime' : exception_log . lasttime , 'firsttime' : exception_log . firsttime , 'count' : str ( exception_log . count ) } ) raise tornado . gen . Return ( ret )
4584	def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
11438	def _create_record_lxml ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : parser = etree . XMLParser ( dtd_validation = correct , recover = ( verbose <= 3 ) ) if correct : marcxml = '<?xml version="1.0" encoding="UTF-8"?>\n' '<collection>\n%s\n</collection>' % ( marcxml , ) try : tree = etree . parse ( StringIO ( marcxml ) , parser ) # parser errors are located in parser.error_log # if 1 <= verbose <=3 then show them to the user? # if verbose == 0 then continue # if verbose >3 then an exception will be thrown except Exception as e : raise InvenioBibRecordParserError ( str ( e ) ) record = { } field_position_global = 0 controlfield_iterator = tree . iter ( tag = '{*}controlfield' ) for controlfield in controlfield_iterator : tag = controlfield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = ' ' ind2 = ' ' text = controlfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) subfields = [ ] if text or keep_singletons : field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) datafield_iterator = tree . iter ( tag = '{*}datafield' ) for datafield in datafield_iterator : tag = datafield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = datafield . attrib . get ( 'ind1' , '!' ) . encode ( "UTF-8" ) ind2 = datafield . attrib . get ( 'ind2' , '!' ) . encode ( "UTF-8" ) if ind1 in ( '' , '_' ) : ind1 = ' ' if ind2 in ( '' , '_' ) : ind2 = ' ' subfields = [ ] subfield_iterator = datafield . iter ( tag = '{*}subfield' ) for subfield in subfield_iterator : code = subfield . attrib . get ( 'code' , '!' ) . encode ( "UTF-8" ) text = subfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) if text or keep_singletons : subfields . append ( ( code , text ) ) if subfields or keep_singletons : text = '' field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) return record
9153	def get_exif_info ( self ) : _dict = { } for tag in _EXIF_TAGS : ret = self . img . attribute ( "EXIF:%s" % tag ) if ret and ret != 'unknown' : _dict [ tag ] = ret return _dict
6444	def _cond_s ( self , word , suffix_len ) : return word [ - suffix_len - 2 : - suffix_len ] == 'dr' or ( word [ - suffix_len - 1 ] == 't' and word [ - suffix_len - 2 : - suffix_len ] != 'tt' )
247	def map_transaction ( txn ) : if isinstance ( txn [ 'sid' ] , dict ) : sid = txn [ 'sid' ] [ 'sid' ] symbol = txn [ 'sid' ] [ 'symbol' ] else : sid = txn [ 'sid' ] symbol = txn [ 'sid' ] return { 'sid' : sid , 'symbol' : symbol , 'price' : txn [ 'price' ] , 'order_id' : txn [ 'order_id' ] , 'amount' : txn [ 'amount' ] , 'commission' : txn [ 'commission' ] , 'dt' : txn [ 'dt' ] }
10234	def _get_catalysts_in_reaction ( reaction : Reaction ) -> Set [ BaseAbundance ] : return { reactant for reactant in reaction . reactants if reactant in reaction . products }
3690	def solve_T ( self , P , V , quick = True ) : a , b = self . a , self . b if quick : x1 = - 1.j * 1.7320508075688772 + 1. x2 = V - b x3 = x2 / R x4 = V + b x5 = ( 1.7320508075688772 * ( x2 * x2 * ( - 4. * P * P * P * x3 + 27. * a * a / ( V * V * x4 * x4 ) ) / ( R * R ) ) ** 0.5 - 9. * a * x3 / ( V * x4 ) + 0j ) ** ( 1. / 3. ) return ( 3.3019272488946263 * ( 11.537996562459266 * P * x3 / ( x1 * x5 ) + 1.2599210498948732 * x1 * x5 ) ** 2 / 144.0 ) . real else : return ( ( - ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) / 3 + ( - P * V + P * b ) / ( R * ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) ) ) ** 2 ) . real
6306	def load_package ( self ) : try : self . package = importlib . import_module ( self . name ) except ModuleNotFoundError : raise ModuleNotFoundError ( "Effect package '{}' not found." . format ( self . name ) )
10528	def _pybossa_req ( method , domain , id = None , payload = None , params = { } , headers = { 'content-type' : 'application/json' } , files = None ) : url = _opts [ 'endpoint' ] + '/api/' + domain if id is not None : url += '/' + str ( id ) if 'api_key' in _opts : params [ 'api_key' ] = _opts [ 'api_key' ] if method == 'get' : r = requests . get ( url , params = params ) elif method == 'post' : if files is None and headers [ 'content-type' ] == 'application/json' : r = requests . post ( url , params = params , headers = headers , data = json . dumps ( payload ) ) else : r = requests . post ( url , params = params , files = files , data = payload ) elif method == 'put' : r = requests . put ( url , params = params , headers = headers , data = json . dumps ( payload ) ) elif method == 'delete' : r = requests . delete ( url , params = params , headers = headers , data = json . dumps ( payload ) ) if r . status_code // 100 == 2 : if r . text and r . text != '""' : return json . loads ( r . text ) else : return True else : return json . loads ( r . text )
10184	def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
13750	def one_to_many ( clsname , * * kw ) : @ declared_attr def o2m ( cls ) : cls . _references ( ( clsname , cls . __name__ ) ) return relationship ( clsname , * * kw ) return o2m
12040	def checkOut ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not "__" in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
4301	def setup_database ( config_data ) : with chdir ( config_data . project_directory ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) commands = [ ] commands . append ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'migrate' ] , ) if config_data . verbose : sys . stdout . write ( 'Database setup commands: {0}\n' . format ( ', ' . join ( [ ' ' . join ( cmd ) for cmd in commands ] ) ) ) for command in commands : try : output = subprocess . check_output ( command , env = env , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : # pragma: no cover if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise if not config_data . no_user : sys . stdout . write ( 'Creating admin user\n' ) if config_data . noinput : create_user ( config_data ) else : subprocess . check_call ( ' ' . join ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'createsuperuser' ] ) , shell = True , stderr = subprocess . STDOUT )
3573	def peripheral_didUpdateValueForCharacteristic_error_ ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral_didUpdateValueForCharacteristic_error called' ) # Stop if there was some kind of error. if error is not None : return # Notify the device about the updated characteristic value. device = device_list ( ) . get ( peripheral ) if device is not None : device . _characteristic_changed ( characteristic )
5542	def contours ( self , elevation , interval = 100 , field = 'elev' , base = 0 ) : return commons_contours . extract_contours ( elevation , self . tile , interval = interval , field = field , base = base )
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) # How many new bits are introduced in each successive pattern? numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
9536	def enumeration ( * args ) : assert len ( args ) > 0 , 'at least one argument is required' if len ( args ) == 1 : # assume the first argument defines the membership members = args [ 0 ] else : # assume the arguments are the members members = args def checker ( value ) : if value not in members : raise ValueError ( value ) return checker
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
1301	def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
10744	def print_runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'tot' ) . print_stats ( 20 ) return output return wrapper
11493	def list_user_folders ( self , token ) : parameters = dict ( ) parameters [ 'token' ] = token response = self . request ( 'midas.user.folders' , parameters ) return response
11557	def extended_analog ( self , pin , data ) : analog_data = [ pin , data & 0x7f , ( data >> 7 ) & 0x7f , ( data >> 14 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . EXTENDED_ANALOG , analog_data )
785	def jobCountCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT COUNT(job_id) ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows [ 0 ] [ 0 ]
9332	def empty_like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
3840	async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
12919	def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
3997	def copy_between_containers ( source_name , source_path , dest_name , dest_path ) : if not container_path_exists ( source_name , source_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( source_path , source_name ) ) temp_path = os . path . join ( tempfile . mkdtemp ( ) , str ( uuid . uuid1 ( ) ) ) with _cleanup_path ( temp_path ) : copy_to_local ( temp_path , source_name , source_path , demote = False ) copy_from_local ( temp_path , dest_name , dest_path , demote = False )
1173	def insort_right ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if x < a [ mid ] : hi = mid else : lo = mid + 1 a . insert ( lo , x )
11151	def md5file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . md5 , nbytes = nbytes , chunk_size = chunk_size )
7036	def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , samplespec = None , limitspec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : # turn the input into a param dict coords = '%.5f %.5f %.1f' % ( center_ra , center_decl , radiusarcmin ) params = { 'coords' : coords } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done # we won't wait for the LC ZIP to complete if email_when_done = True if email_when_done : download_data = False # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # hit the server api_url = '%s/api/conesearch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) # check the status of the search status = searchresult [ 0 ] # now we'll check if we want to download the data if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
1410	def filter_bolts ( table , header ) : bolts_info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts_info . append ( row ) return bolts_info , header
154	def max_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . right is not None : node = node . right return node . key , node . value
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
3474	def check_mass_balance ( self ) : reaction_element_dict = defaultdict ( int ) for metabolite , coefficient in iteritems ( self . _metabolites ) : if metabolite . charge is not None : reaction_element_dict [ "charge" ] += coefficient * metabolite . charge if metabolite . elements is None : raise ValueError ( "No elements found in metabolite %s" % metabolite . id ) for element , amount in iteritems ( metabolite . elements ) : reaction_element_dict [ element ] += coefficient * amount # filter out 0 values return { k : v for k , v in iteritems ( reaction_element_dict ) if v != 0 }
6034	def padded_grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_padded_grid = PaddedRegularGrid . padded_grid_from_shape_psf_shape_and_pixel_scale ( shape = mask . shape , psf_shape = psf_shape , pixel_scale = mask . pixel_scale ) sub_padded_grid = PaddedSubGrid . padded_grid_from_mask_sub_grid_size_and_psf_shape ( mask = mask , sub_grid_size = sub_grid_size , psf_shape = psf_shape ) # TODO : The blurring grid is not used when the grid mapper is called, the 0.0 0.0 stops errors inr ayT_racing # TODO : implement a more explicit solution return GridStack ( regular = regular_padded_grid , sub = sub_padded_grid , blurring = np . array ( [ [ 0.0 , 0.0 ] ] ) )
2626	def cancel ( self , job_ids ) : if self . linger is True : logger . debug ( "Ignoring cancel requests due to linger mode" ) return [ False for x in job_ids ] try : self . client . terminate_instances ( InstanceIds = list ( job_ids ) ) except Exception as e : logger . error ( "Caught error while attempting to remove instances: {0}" . format ( job_ids ) ) raise e else : logger . debug ( "Removed the instances: {0}" . format ( job_ids ) ) for job_id in job_ids : self . resources [ job_id ] [ "status" ] = "COMPLETED" for job_id in job_ids : self . instances . remove ( job_id ) return [ True for x in job_ids ]
8009	def execute ( self ) : # Save the execution time first. # If execute() fails, executed_at will be set, with no executed_agreement set. self . executed_at = now ( ) self . save ( ) with transaction . atomic ( ) : ret = BillingAgreement . execute ( self . id ) ret . user = self . user ret . save ( ) self . executed_agreement = ret self . save ( ) return ret
11320	def update_date_year ( self ) : dates = record_get_field_instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published_years = record_get_field_values ( self . record , "773" , code = "y" ) if published_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , published_years [ 0 ] [ : 4 ] ) ] ) else : other_years = record_get_field_values ( self . record , "269" , code = "c" ) if other_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , other_years [ 0 ] [ : 4 ] ) ] )
13661	def _tempfile ( filename ) : return tempfile . NamedTemporaryFile ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : # Find the indices of the requested fields if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None # turn key fields to key indices key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : # Select requested fields only if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp # Store processed record records . append ( r ) # Check memory available_memory = psutil . avail_phymem ( ) # If bellow the watermark create a new chunk, reset and keep going if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 # Sort and write the remainder if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 # Marge all the files _mergeFiles ( key , chunk , outputFile , fields )
5240	def market_close ( self , session , mins ) -> Session : if session not in self . exch : return SessNA end_time = self . exch [ session ] [ - 1 ] return Session ( shift_time ( end_time , - int ( mins ) + 1 ) , end_time )
5326	def __create_arthur_json ( self , repo , backend_args ) : backend_args = self . _compose_arthur_params ( self . backend_section , repo ) if self . backend_section == 'git' : backend_args [ 'gitpath' ] = os . path . join ( self . REPOSITORY_DIR , repo ) backend_args [ 'tag' ] = self . backend_tag ( repo ) ajson = { "tasks" : [ { } ] } # This is the perceval tag ajson [ "tasks" ] [ 0 ] [ 'task_id' ] = self . backend_tag ( repo ) ajson [ "tasks" ] [ 0 ] [ 'backend' ] = self . backend_section . split ( ":" ) [ 0 ] ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] = backend_args ajson [ "tasks" ] [ 0 ] [ 'category' ] = backend_args [ 'category' ] ajson [ "tasks" ] [ 0 ] [ 'archive' ] = { } ajson [ "tasks" ] [ 0 ] [ 'scheduler' ] = { "delay" : self . ARTHUR_TASK_DELAY } # from-date or offset param must be added es_col_url = self . _get_collection_url ( ) es_index = self . conf [ self . backend_section ] [ 'raw_index' ] # Get the last activity for the data source es = ElasticSearch ( es_col_url , es_index ) connector = get_connector_from_name ( self . backend_section ) klass = connector [ 0 ] # Backend for the connector signature = inspect . signature ( klass . fetch ) last_activity = None filter_ = { "name" : "tag" , "value" : backend_args [ 'tag' ] } if 'from_date' in signature . parameters : last_activity = es . get_last_item_field ( 'metadata__updated_on' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'from_date' ] = last_activity . isoformat ( ) elif 'offset' in signature . parameters : last_activity = es . get_last_item_field ( 'offset' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'offset' ] = last_activity if last_activity : logging . info ( "Getting raw item with arthur since %s" , last_activity ) return ( ajson )
6188	def print_summary ( string = 'Repository' , git_path = None ) : if git_path is None : git_path = GIT_PATH # If git is available, check fretbursts version if not git_path_valid ( ) : print ( '\n%s revision unknown (git not found).' % string ) else : last_commit = get_last_commit_line ( ) print ( '\n{} revision:\n {}\n' . format ( string , last_commit ) ) if not check_clean_status ( ) : print ( '\nWARNING -> Uncommitted changes:' ) print ( get_status ( ) )
697	def bestModelIdAndErrScore ( self , swarmId = None , genIdx = None ) : if swarmId is None : return ( self . _bestModelID , self . _bestResult ) else : if swarmId not in self . _swarmBestOverall : return ( None , numpy . inf ) # Get the best score, considering the appropriate generations genScores = self . _swarmBestOverall [ swarmId ] bestModelId = None bestScore = numpy . inf for ( i , ( modelId , errScore ) ) in enumerate ( genScores ) : if genIdx is not None and i > genIdx : break if errScore < bestScore : bestScore = errScore bestModelId = modelId return ( bestModelId , bestScore )
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
10894	def filtered_image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
11979	def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise ValueError ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , IPv4Address ) : self . _ip = ip else : self . _ip = IPv4Address ( ip ) if isinstance ( netmask , IPv4NetMask ) : self . _nm = netmask else : self . _nm = IPv4NetMask ( netmask ) ipl = int ( self . _ip ) nml = int ( self . _nm ) base_add = ipl & nml self . _ip_num = 0xFFFFFFFF - 1 - nml # NOTE: quite a mess. # This's here to handle /32 (-1) and /31 (0) netmasks. if self . _ip_num in ( - 1 , 0 ) : if self . _ip_num == - 1 : self . _ip_num = 1 else : self . _ip_num = 2 self . _net_ip = None self . _bc_ip = None self . _first_ip_dec = base_add self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) if self . _ip_num == 1 : last_ip_dec = self . _first_ip_dec else : last_ip_dec = self . _first_ip_dec + 1 self . _last_ip = IPv4Address ( last_ip_dec , notation = IP_DEC ) return self . _net_ip = IPv4Address ( base_add , notation = IP_DEC ) self . _bc_ip = IPv4Address ( base_add + self . _ip_num + 1 , notation = IP_DEC ) self . _first_ip_dec = base_add + 1 self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) self . _last_ip = IPv4Address ( base_add + self . _ip_num , notation = IP_DEC )
5862	def validate ( self , ml_template ) : data = { "ml_template" : ml_template } failure_message = "ML template validation invoke failed" res = self . _get_success_json ( self . _post_json ( 'ml_templates/validate' , data , failure_message = failure_message ) ) [ 'data' ] if res [ 'valid' ] : return 'OK' return res [ 'reason' ]
12360	def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
10769	def matlab_formatter ( level , vertices , codes = None ) : vertices = numpy_formatter ( level , vertices , codes ) if codes is not None : level = level [ 0 ] headers = np . vstack ( ( [ v . shape [ 0 ] for v in vertices ] , [ level ] * len ( vertices ) ) ) . T vertices = np . vstack ( list ( it . __next__ ( ) for it in itertools . cycle ( ( iter ( headers ) , iter ( vertices ) ) ) ) ) return vertices
1527	def is_host_port_reachable ( self ) : for hostport in self . hostportlist : try : socket . create_connection ( hostport , StateManager . TIMEOUT_SECONDS ) return True except : LOG . info ( "StateManager %s Unable to connect to host: %s port %i" % ( self . name , hostport [ 0 ] , hostport [ 1 ] ) ) continue return False
4041	def _retrieve_data ( self , request = None ) : full_url = "%s%s" % ( self . endpoint , request ) # The API doesn't return this any more, so we have to cheat self . self_link = request self . request = requests . get ( url = full_url , headers = self . default_headers ( ) ) self . request . encoding = "utf-8" try : self . request . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( self . request ) return self . request
6453	def dist ( self , src , tar ) : if tar == src : return 0.0 if not src or not tar : return 1.0 max_length = max ( len ( src ) , len ( tar ) ) return self . dist_abs ( src , tar ) / max_length
2898	def get_task ( self , id ) : tasks = [ task for task in self . get_tasks ( ) if task . id == id ] return tasks [ 0 ] if len ( tasks ) == 1 else None
3599	def delivery ( self , packageName , versionCode = None , offerType = 1 , downloadToken = None , expansion_files = False ) : if versionCode is None : # pick up latest version versionCode = self . details ( packageName ) . get ( 'versionCode' ) params = { 'ot' : str ( offerType ) , 'doc' : packageName , 'vc' : str ( versionCode ) } headers = self . getHeaders ( ) if downloadToken is not None : params [ 'dtok' ] = downloadToken response = requests . get ( DELIVERY_URL , headers = headers , params = params , verify = ssl_verify , timeout = 60 , proxies = self . proxies_config ) response = googleplay_pb2 . ResponseWrapper . FromString ( response . content ) if response . commands . displayErrorMessage != "" : raise RequestError ( response . commands . displayErrorMessage ) elif response . payload . deliveryResponse . appDeliveryData . downloadUrl == "" : raise RequestError ( 'App not purchased' ) else : result = { } result [ 'docId' ] = packageName result [ 'additionalData' ] = [ ] downloadUrl = response . payload . deliveryResponse . appDeliveryData . downloadUrl cookie = response . payload . deliveryResponse . appDeliveryData . downloadAuthCookie [ 0 ] cookies = { str ( cookie . name ) : str ( cookie . value ) } result [ 'file' ] = self . _deliver_data ( downloadUrl , cookies ) if not expansion_files : return result for obb in response . payload . deliveryResponse . appDeliveryData . additionalFile : a = { } # fileType == 0 -> main # fileType == 1 -> patch if obb . fileType == 0 : obbType = 'main' else : obbType = 'patch' a [ 'type' ] = obbType a [ 'versionCode' ] = obb . versionCode a [ 'file' ] = self . _deliver_data ( obb . downloadUrl , None ) result [ 'additionalData' ] . append ( a ) return result
9027	def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
11228	def before ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self last = None if inc : for i in gen : if i > dt : break last = i else : for i in gen : if i >= dt : break last = i return last
3363	def save_yaml_model ( model , filename , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ "version" ] = YAML_SPEC if isinstance ( filename , string_types ) : with io . open ( filename , "w" ) as file_handle : yaml . dump ( obj , file_handle , * * kwargs ) else : yaml . dump ( obj , filename , * * kwargs )
4079	def get_version ( ) : version = _get_attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"LanguageTool-?.*?(\S+)$" , get_directory ( ) ) if match : version = match . group ( 1 ) return version
4947	def send_course_completion_statement ( lrs_configuration , user , course_overview , course_grade ) : user_details = LearnerInfoSerializer ( user ) course_details = CourseInfoSerializer ( course_overview ) statement = LearnerCourseCompletionStatement ( user , course_overview , user_details . data , course_details . data , course_grade , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
6506	def process_result ( cls , dictionary , match_phrase , user ) : result_processor = _load_class ( getattr ( settings , "SEARCH_RESULT_PROCESSOR" , None ) , cls ) srp = result_processor ( dictionary , match_phrase ) if srp . should_remove ( user ) : return None try : srp . add_properties ( ) # protect around any problems introduced by subclasses within their properties except Exception as ex : # pylint: disable=broad-except log . exception ( "error processing properties for %s - %s: will remove from results" , json . dumps ( dictionary , cls = DjangoJSONEncoder ) , str ( ex ) ) return None return dictionary
7010	def skyview_stamp ( ra , decl , survey = 'DSS2 Red' , scaling = 'Linear' , flip = True , convolvewith = None , forcefetch = False , cachedir = '~/.astrobase/stamp-cache' , timeout = 10.0 , retry_failed = False , savewcsheader = True , verbose = False ) : stampdict = get_stamp ( ra , decl , survey = survey , scaling = scaling , forcefetch = forcefetch , cachedir = cachedir , timeout = timeout , retry_failed = retry_failed , verbose = verbose ) # # DONE WITH FETCHING STUFF # if stampdict : # open the frame stampfits = pyfits . open ( stampdict [ 'fitsfile' ] ) header = stampfits [ 0 ] . header frame = stampfits [ 0 ] . data stampfits . close ( ) # finally, we can process the frame if flip : frame = np . flipud ( frame ) if verbose : LOGINFO ( 'fetched stamp successfully for (%.3f, %.3f)' % ( ra , decl ) ) if convolvewith : convolved = aconv . convolve ( frame , convolvewith ) if savewcsheader : return convolved , header else : return convolved else : if savewcsheader : return frame , header else : return frame else : LOGERROR ( 'could not fetch the requested stamp for ' 'coords: (%.3f, %.3f) from survey: %s and scaling: %s' % ( ra , decl , survey , scaling ) ) return None
8390	def validate ( self ) : with open ( self . filename , "rb" ) as f : text = f . read ( ) start_last_line = text . rfind ( b"\n" , 0 , - 1 ) if start_last_line == - 1 : return False original_text = text [ : start_last_line + 1 ] last_line = text [ start_last_line + 1 : ] expected_hash = hashlib . sha1 ( original_text ) . hexdigest ( ) . encode ( 'utf8' ) match = re . search ( b"[0-9a-f]{40}" , last_line ) if not match : return False actual_hash = match . group ( 0 ) return actual_hash == expected_hash
7678	def pitch_contour ( annotation , * * kwargs ) : ax = kwargs . pop ( 'ax' , None ) # If the annotation is empty, we need to construct a new axes ax = mir_eval . display . __get_axes ( ax = ax ) [ 0 ] times , values = annotation . to_interval_values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir_eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , * * kwargs ) return ax
630	def binSearch ( arr , val ) : i = bisect_left ( arr , val ) if i != len ( arr ) and arr [ i ] == val : return i return - 1
6895	def pwd_phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_phases = phases [ thisbin_inds ] thisbin_mags = mags [ thisbin_inds ] if thisbin_inds . size > minbin : binnedphases . append ( npmedian ( thisbin_phases ) ) binnedmags . append ( npmedian ( thisbin_mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
8742	def update_floatingip ( context , id , content ) : LOG . info ( 'update_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) if 'port_id' not in content : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'port_id is required.' ) requested_ports = [ ] if content . get ( 'port_id' ) : requested_ports = [ { 'port_id' : content . get ( 'port_id' ) } ] flip = _update_flip ( context , id , ip_types . FLOATING , requested_ports ) return v . _make_floating_ip_dict ( flip )
13808	def run ( self ) : config = config_creator ( ) debug = config . debug branch_thread_sleep = config . branch_thread_sleep while 1 : url = self . branch_queue . get ( ) if debug : print ( 'branch thread-{} start' . format ( url ) ) branch_spider = self . branch_spider ( url ) sleep ( random . randrange ( * branch_thread_sleep ) ) branch_spider . request_page ( ) if debug : print ( 'branch thread-{} end' . format ( url ) ) self . branch_queue . task_done ( )
1980	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
135	def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot reorder polygon points, because it contains no points." ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( "Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )
8585	def get_attached_cdroms ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
12781	def set_name ( self , name ) : if not self . _campfire . get_user ( ) . admin : return False result = self . _connection . put ( "room/%s" % self . id , { "room" : { "name" : name } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
2261	def dict_hist ( item_list , weight_list = None , ordered = False , labels = None ) : if labels is None : hist_ = defaultdict ( lambda : 0 ) else : hist_ = { k : 0 for k in labels } if weight_list is None : weight_list = it . repeat ( 1 ) # Accumulate frequency for item , weight in zip ( item_list , weight_list ) : hist_ [ item ] += weight if ordered : # Order by value getval = op . itemgetter ( 1 ) hist = OrderedDict ( [ ( key , value ) for ( key , value ) in sorted ( hist_ . items ( ) , key = getval ) ] ) else : # Cast to a normal dictionary hist = dict ( hist_ ) return hist
11852	def parse ( self , words , S = 'S' ) : self . chart = [ [ ] for i in range ( len ( words ) + 1 ) ] self . add_edge ( [ 0 , 0 , 'S_' , [ ] , [ S ] ] ) for i in range ( len ( words ) ) : self . scanner ( i , words [ i ] ) return self . chart
8898	def _deserialize_from_store ( profile ) : # we first serialize to avoid deserialization merge conflicts _serialize_into_store ( profile ) fk_cache = { } with transaction . atomic ( ) : syncable_dict = _profile_models [ profile ] excluded_list = [ ] # iterate through classes which are in foreign key dependency order for model_name , klass_model in six . iteritems ( syncable_dict ) : # handle cases where a class has a single FK reference to itself self_ref_fk = _self_referential_fk ( klass_model ) query = Q ( model_name = klass_model . morango_model_name ) for klass in klass_model . morango_model_dependencies : query |= Q ( model_name = klass . morango_model_name ) if self_ref_fk : clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile ) . filter ( Q ( _self_ref_fk__in = clean_parents ) | Q ( _self_ref_fk = '' ) ) . filter ( query ) # keep iterating until size of dirty_children is 0 while len ( dirty_children ) > 0 : for store_model in dirty_children : try : app_model = store_model . _deserialize_store_model ( fk_cache ) if app_model : with mute_signals ( signals . pre_save , signals . post_save ) : app_model . save ( update_dirty_bit_to = False ) # we update a store model after we have deserialized it to be able to mark it as a clean parent store_model . dirty_bit = False store_model . save ( update_fields = [ 'dirty_bit' ] ) except exceptions . ValidationError : # if the app model did not validate, we leave the store dirty bit set excluded_list . append ( store_model . id ) # update lists with new clean parents and dirty children clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile , _self_ref_fk__in = clean_parents ) . filter ( query ) else : # array for holding db values from the fields of each model for this class db_values = [ ] fields = klass_model . _meta . fields for store_model in Store . objects . filter ( model_name = model_name , profile = profile , dirty_bit = True ) : try : app_model = store_model . _deserialize_store_model ( fk_cache ) # if the model was not deleted add its field values to the list if app_model : for f in fields : value = getattr ( app_model , f . attname ) db_value = f . get_db_prep_value ( value , connection ) db_values . append ( db_value ) except exceptions . ValidationError : # if the app model did not validate, we leave the store dirty bit set excluded_list . append ( store_model . id ) if db_values : # number of rows to update num_of_rows = len ( db_values ) // len ( fields ) # create '%s' placeholders for a single row placeholder_tuple = tuple ( [ '%s' for _ in range ( len ( fields ) ) ] ) # create list of the '%s' tuple placeholders based on number of rows to update placeholder_list = [ str ( placeholder_tuple ) for _ in range ( num_of_rows ) ] with connection . cursor ( ) as cursor : DBBackend . _bulk_insert_into_app_models ( cursor , klass_model . _meta . db_table , fields , db_values , placeholder_list ) # clear dirty bit for all store models for this profile except for models that did not validate Store . objects . exclude ( id__in = excluded_list ) . filter ( profile = profile , dirty_bit = True ) . update ( dirty_bit = False )
8853	def on_open ( self ) : filename , filter = QtWidgets . QFileDialog . getOpenFileName ( self , 'Open' ) if filename : self . open_file ( filename ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True )
10892	def translate ( self , dr ) : tile = self . copy ( ) tile . l += dr tile . r += dr return tile
13554	def create_shift ( self , params = { } ) : url = "/2/shifts/" body = params data = self . _post_resource ( url , body ) shift = self . shift_from_json ( data [ "shift" ] ) return shift
11412	def record_get_field ( rec , tag , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) for field in rec [ tag ] : if field [ 4 ] == field_position_global : return field raise InvenioBibRecordFieldError ( "No field has the tag '%s' and the " "global field position '%d'." % ( tag , field_position_global ) ) else : try : return rec [ tag ] [ field_position_local ] except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
13608	def unpickle ( filepath ) : arr = [ ] with open ( filepath , 'rb' ) as f : carr = f . read ( blosc . MAX_BUFFERSIZE ) while len ( carr ) > 0 : arr . append ( blosc . decompress ( carr ) ) carr = f . read ( blosc . MAX_BUFFERSIZE ) return pkl . loads ( b"" . join ( arr ) )
5486	def jsonify_status_code ( status_code , * args , * * kw ) : is_batch = kw . pop ( 'is_batch' , False ) if is_batch : response = flask_make_response ( json . dumps ( * args , * * kw ) ) response . mimetype = 'application/json' response . status_code = status_code return response response = jsonify ( * args , * * kw ) response . status_code = status_code return response
13708	def check_ip ( self , ip ) : self . _last_result = None if is_valid_ipv4 ( ip ) : key = None if self . _use_cache : key = self . _make_cache_key ( ip ) self . _last_result = self . _cache . get ( key , version = self . _cache_version ) if self . _last_result is None : # request httpBL API error , age , threat , type = self . _request_httpbl ( ip ) if error == 127 or error == 0 : self . _last_result = { 'error' : error , 'age' : age , 'threat' : threat , 'type' : type } if self . _use_cache : self . _cache . set ( key , self . _last_result , timeout = self . _api_timeout , version = self . _cache_version ) if self . _last_result is not None and settings . CACHED_HTTPBL_USE_LOGGING : logger . info ( 'httpBL check ip: {0}; ' 'httpBL result: error: {1}, age: {2}, threat: {3}, type: {4}' . format ( ip , self . _last_result [ 'error' ] , self . _last_result [ 'age' ] , self . _last_result [ 'threat' ] , self . _last_result [ 'type' ] ) ) return self . _last_result
7167	def load_intent ( self , name , file_name , reload_cache = False ) : self . intents . load ( name , file_name , reload_cache ) with open ( file_name ) as f : self . padaos . add_intent ( name , f . read ( ) . split ( '\n' ) ) self . must_train = True
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) # stop all jobs if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
6648	def _loadConfig ( self ) : config_dicts = [ self . additional_config , self . app_config ] + [ t . getConfig ( ) for t in self . hierarchy ] # create an identical set of dictionaries, but with the names of the # sources in place of the values. When these are merged they will show # where each merged property came from: config_blame = [ _mirrorStructure ( self . additional_config , 'command-line config' ) , _mirrorStructure ( self . app_config , 'application\'s config.json' ) , ] + [ _mirrorStructure ( t . getConfig ( ) , t . getName ( ) ) for t in self . hierarchy ] self . config = _mergeDictionaries ( * config_dicts ) self . config_blame = _mergeDictionaries ( * config_blame )
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
8514	def dict_merge ( base , top ) : out = dict ( top ) for key in base : if key in top : if isinstance ( base [ key ] , dict ) and isinstance ( top [ key ] , dict ) : out [ key ] = dict_merge ( base [ key ] , top [ key ] ) else : out [ key ] = base [ key ] return out
11603	def parse_byteranges ( cls , environ ) : r = [ ] s = environ . get ( cls . header_range , '' ) . replace ( ' ' , '' ) . lower ( ) if s : l = s . split ( '=' ) if len ( l ) == 2 : unit , vals = tuple ( l ) if unit == 'bytes' and vals : gen_rng = ( tuple ( rng . split ( '-' ) ) for rng in vals . split ( ',' ) if '-' in rng ) for start , end in gen_rng : if start or end : r . append ( ( int ( start ) if start else None , int ( end ) if end else None ) ) return r
8143	def rotate ( self , angle ) : #When a layer rotates, its corners will fall outside #of its defined width and height. #Thus, its bounding box needs to be expanded. #Calculate the diagonal width, and angle from the layer center. #This way we can use the layers's corners #to calculate the bounding box. from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) #The rotation box's background color #is the mean pixel value of the rotating image. #This is the best option to avoid borders around #the rotated image. bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box #Since rotate changes the bounding box size, #update the layers' width, height, and position, #so it rotates from the center. self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
6031	def unmasked_blurred_image_from_psf_and_unmasked_image ( self , psf , unmasked_image_1d ) : blurred_image_1d = self . regular . convolve_array_1d_with_psf ( padded_array_1d = unmasked_image_1d , psf = psf ) return self . regular . scaled_array_2d_from_array_1d ( array_1d = blurred_image_1d )
4044	def publications ( self ) : if self . library_type != "users" : raise ze . CallDoesNotExist ( "This API call does not exist for group libraries" ) query_string = "/{t}/{u}/publications/items" return self . _build_query ( query_string )
8977	def _file ( self , file ) : if not self . __text_is_expected : file = BytesWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
3695	def Clapeyron ( T , Tc , Pc , dZ = 1 , Psat = 101325 ) : Tr = T / Tc return R * T * dZ * log ( Pc / Psat ) / ( 1. - Tr )
4604	def history ( self , first = 0 , last = 0 , limit = - 1 , only_ops = [ ] , exclude_ops = [ ] ) : _limit = 100 cnt = 0 if first < 0 : first = 0 while True : # RPC call txs = self . blockchain . rpc . get_account_history ( self [ "id" ] , "1.11.{}" . format ( last ) , _limit , "1.11.{}" . format ( first - 1 ) , api = "history" , ) for i in txs : if ( exclude_ops and self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in exclude_ops ) : continue if ( not only_ops or self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in only_ops ) : cnt += 1 yield i if limit >= 0 and cnt >= limit : # pragma: no cover return if not txs : log . info ( "No more history returned from API node" ) break if len ( txs ) < _limit : log . info ( "Less than {} have been returned." . format ( _limit ) ) break first = int ( txs [ - 1 ] [ "id" ] . split ( "." ) [ 2 ] )
517	def _bumpUpWeakColumns ( self ) : weakColumns = numpy . where ( self . _overlapDutyCycles < self . _minOverlapDutyCycles ) [ 0 ] for columnIndex in weakColumns : perm = self . _permanences [ columnIndex ] . astype ( realDType ) maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] perm [ maskPotential ] += self . _synPermBelowStimulusInc self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False )
5077	def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) # course runs with no start date should be considered last. never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
13908	def create_subparsers ( self , parser ) : subparsers = parser . add_subparsers ( ) for name in self . config [ 'subparsers' ] : subparser = subparsers . add_parser ( name ) self . create_commands ( self . config [ 'subparsers' ] [ name ] , subparser )
10802	def _c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
285	def plot_drawdown_underwater ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , * * kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
9663	def get_tied_targets ( original_targets , the_ties ) : my_ties = [ ] for original_target in original_targets : for item in the_ties : if original_target in item : for thing in item : my_ties . append ( thing ) my_ties = list ( set ( my_ties ) ) if my_ties : ties_message = "" ties_message += "The following targets share dependencies and must be run together:" for item in sorted ( my_ties ) : ties_message += "\n - {}" . format ( item ) return list ( set ( my_ties + original_targets ) ) , ties_message return original_targets , ""
9271	def filter_since_tag ( self , all_tags ) : tag = self . detect_since_tag ( ) if not tag or tag == REPO_CREATED_TAG_NAME : return copy . deepcopy ( all_tags ) filtered_tags = [ ] tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "since-tag" ) return copy . deepcopy ( all_tags ) since_tag = all_tags [ idx ] since_date = self . get_time_of_tag ( since_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if since_date <= tag_date : filtered_tags . append ( t ) return filtered_tags
10352	def make_pubmed_gene_group ( entrez_ids : Iterable [ Union [ str , int ] ] ) -> Iterable [ str ] : url = PUBMED_GENE_QUERY_URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez_ids ) ) response = requests . get ( url ) tree = ElementTree . fromstring ( response . content ) for x in tree . findall ( './DocumentSummarySet/DocumentSummary' ) : yield '\n# {}' . format ( x . find ( 'Description' ) . text ) yield 'SET Citation = {{"Other", "PubMed Gene", "{}"}}' . format ( x . attrib [ 'uid' ] ) yield 'SET Evidence = "{}"' . format ( x . find ( 'Summary' ) . text . strip ( ) . replace ( '\n' , '' ) ) yield '\nUNSET Evidence\nUNSET Citation'
428	def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
2636	def update_parent ( self , fut ) : self . parent = fut try : fut . add_done_callback ( self . parent_callback ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) )
2676	def get_callable_handler_function ( src , handler ) : # "cd" into `src` directory. os . chdir ( src ) module_name , function_name = handler . split ( '.' ) filename = get_handler_filename ( handler ) path_to_module_file = os . path . join ( src , filename ) module = load_source ( module_name , path_to_module_file ) return getattr ( module , function_name )
7286	def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
10657	def amounts ( masses ) : return { compound : amount ( compound , masses [ compound ] ) for compound in masses . keys ( ) }
11783	def add_example ( self , example ) : self . check_example ( example ) self . examples . append ( example )
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
13598	def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0
13622	def many ( func ) : def _many ( result ) : if _isSequenceTypeNotText ( result ) : return map ( func , result ) return [ ] return maybe ( _many , default = [ ] )
242	def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( "## Performance Relative to Common Risk Factors" ) ) # aggregate perf attrib stats and show summary table perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) # one section for the returns plot, and for each factor grouping # one section for factor returns, and one for risk exposures vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig
2033	def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 32 ) self . _store ( address , value , 32 )
9123	def belns ( keyword : str , file : TextIO , encoding : Optional [ str ] , use_names : bool ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belns ( graph , file = file , encoding = encoding , use_names = use_names , )
833	def createInput ( self ) : print "-" * 70 + "Creating a random input vector" + "-" * 70 #clear the inputArray to zero before creating a new input vector self . inputArray [ 0 : ] = 0 for i in range ( self . inputSize ) : #randrange returns 0 or 1 self . inputArray [ i ] = random . randrange ( 2 )
2777	def add_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = POST , params = { "forwarding_rules" : rules_dict } )
11029	def sse_content ( response , handler , * * sse_kwargs ) : # An SSE response must be 200/OK and have content-type 'text/event-stream' raise_for_not_ok_status ( response ) raise_for_header ( response , 'Content-Type' , 'text/event-stream' ) finished , _ = _sse_content_with_protocol ( response , handler , * * sse_kwargs ) return finished
11225	def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
13180	def _get_column_nums_from_args ( columns ) : nums = [ ] for c in columns : for p in c . split ( ',' ) : p = p . strip ( ) try : c = int ( p ) nums . append ( c ) except ( TypeError , ValueError ) : start , ignore , end = p . partition ( '-' ) try : start = int ( start ) end = int ( end ) except ( TypeError , ValueError ) : raise ValueError ( 'Did not understand %r, expected digit-digit' % c ) inc = 1 if start < end else - 1 nums . extend ( range ( start , end + inc , inc ) ) # The user will pass us 1-based indexes, but we need to use # 0-based indexing with the row. return [ n - 1 for n in nums ]
5618	def multipart_to_singleparts ( geom ) : if isinstance ( geom , base . BaseGeometry ) : if hasattr ( geom , "geoms" ) : for subgeom in geom : yield subgeom else : yield geom
2192	def renew ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = { 'timestamp' : util_time . timestamp ( ) , 'product' : products , } if products is not None : if not all ( map ( os . path . exists , products ) ) : raise IOError ( 'The stamped product must exist: {}' . format ( products ) ) certificate [ 'product_file_hash' ] = self . _product_file_hash ( products ) self . cacher . save ( certificate , cfgstr = cfgstr ) return certificate
3135	def create ( self , store_id , order_id , data ) : self . store_id = store_id self . order_id = order_id if 'id' not in data : raise KeyError ( 'The order line must have an id' ) if 'product_id' not in data : raise KeyError ( 'The order line must have a product_id' ) if 'product_variant_id' not in data : raise KeyError ( 'The order line must have a product_variant_id' ) if 'quantity' not in data : raise KeyError ( 'The order line must have a quantity' ) if 'price' not in data : raise KeyError ( 'The order line must have a price' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'orders' , order_id , 'lines' ) ) if response is not None : self . line_id = response [ 'id' ] else : self . line_id = None return response
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , * * kwargs ) : # Taken from Nodebox path = self . BezierPath ( * * kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
8673	def purge_stash ( force , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) # Maybe we should verify that the list is empty # afterwards? click . echo ( 'Purge complete!' ) except GhostError as ex : sys . exit ( ex )
2429	def reset_document ( self ) : # FIXME: this state does not make sense self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
3454	def add_SBO ( model ) : for r in model . reactions : # don't annotate already annotated reactions if r . annotation . get ( "sbo" ) : continue # only doing exchanges if len ( r . metabolites ) != 1 : continue met_id = list ( r . _metabolites ) [ 0 ] . id if r . id . startswith ( "EX_" ) and r . id == "EX_" + met_id : r . annotation [ "sbo" ] = "SBO:0000627" elif r . id . startswith ( "DM_" ) and r . id == "DM_" + met_id : r . annotation [ "sbo" ] = "SBO:0000628"
13099	def getAnnotations ( self , targets , wildcard = "." , include = None , exclude = None , limit = None , start = 1 , expand = False , * * kwargs ) : return 0 , [ ]
12852	def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
6947	def jhk_to_sdssu ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSU_JHK , SDSSU_JH , SDSSU_JK , SDSSU_HK , SDSSU_J , SDSSU_H , SDSSU_K )
6127	def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
5365	def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . _fmt = self . fmt [ record . levelname ] if sys . version_info > ( 3 , 2 ) : # Update self._style because we've changed self._fmt # (code based on stdlib's logging.Formatter.__init__()) if self . style not in logging . _STYLES : raise ValueError ( 'Style must be one of: %s' % ',' . join ( list ( logging . _STYLES . keys ( ) ) ) ) self . _style = logging . _STYLES [ self . style ] [ 0 ] ( self . _fmt ) if sys . version_info > ( 2 , 7 ) : message = super ( LevelFormatter , self ) . format ( record ) else : message = ColoredFormatter . format ( self , record ) return message
7654	def summary ( obj , indent = 0 ) : if hasattr ( obj , '__summary__' ) : rep = obj . __summary__ ( ) elif isinstance ( obj , SortedKeyList ) : rep = '<{:d} observations>' . format ( len ( obj ) ) else : rep = repr ( obj ) return rep . replace ( '\n' , '\n' + ' ' * indent )
1389	def get_machines ( self ) : if self . physical_plan : stmgrs = list ( self . physical_plan . stmgrs ) return map ( lambda s : s . host_name , stmgrs ) return [ ]
7178	def lib2to3_parse ( src_txt ) : grammar = pygram . python_grammar_no_print_statement drv = driver . Driver ( grammar , pytree . convert ) if src_txt [ - 1 ] != '\n' : nl = '\r\n' if '\r\n' in src_txt [ : 1024 ] else '\n' src_txt += nl try : result = drv . parse_string ( src_txt , True ) except ParseError as pe : lineno , column = pe . context [ 1 ] lines = src_txt . splitlines ( ) try : faulty_line = lines [ lineno - 1 ] except IndexError : faulty_line = "<line number missing in source>" raise ValueError ( f"Cannot parse: {lineno}:{column}: {faulty_line}" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file_input , [ result ] ) return result
11815	def decode ( self , ciphertext ) : self . ciphertext = ciphertext problem = PermutationDecoderProblem ( decoder = self ) return search . best_first_tree_search ( problem , lambda node : self . score ( node . state ) )
7335	async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , size_limit = None , * * params ) : if isinstance ( file_ , str ) : url = urlparse ( file_ ) if url . scheme . startswith ( 'http' ) : media = await self . _session . get ( file_ ) else : path = urlparse ( file_ ) . path . strip ( " \"'" ) media = await utils . execute ( open ( path , 'rb' ) ) elif hasattr ( file_ , 'read' ) or isinstance ( file_ , bytes ) : media = file_ else : raise TypeError ( "upload_media input must be a file object or a " "filename or binary data or an aiohttp request" ) media_size = await utils . get_size ( media ) if chunked is not None : size_test = False else : size_test = await self . _size_test ( media_size , size_limit ) if isinstance ( media , aiohttp . ClientResponse ) : # send the content of the response media = media . content if chunked or ( size_test and chunked is None ) : args = media , media_size , file_ , media_type , media_category response = await self . _chunked_upload ( * args , * * params ) else : response = await self . upload . media . upload . post ( media = media , * * params ) if not hasattr ( file_ , 'read' ) and not getattr ( media , 'closed' , True ) : media . close ( ) return response
3354	def append ( self , object ) : the_id = object . id self . _check ( the_id ) self . _dict [ the_id ] = len ( self ) list . append ( self , object )
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
6344	def idf ( self , term , transform = None ) : docs_with_term = 0 docs = self . docs_of_words ( ) for doc in docs : doc_set = set ( doc ) if transform : transformed_doc = [ ] for word in doc_set : transformed_doc . append ( transform ( word ) ) doc_set = set ( transformed_doc ) if term in doc_set : docs_with_term += 1 if docs_with_term == 0 : return float ( 'inf' ) return log10 ( len ( docs ) / docs_with_term )
500	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
3726	def Pc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Pc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Pc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) : methods . append ( PSRK ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) : methods . append ( PD ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Pc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Pc = float ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) elif Method == MATTHEWS : _Pc = float ( _crit_Matthews . at [ CASRN , 'Pc' ] ) elif Method == CRC : _Pc = float ( _crit_CRC . at [ CASRN , 'Pc' ] ) elif Method == PSRK : _Pc = float ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) elif Method == PD : _Pc = float ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) elif Method == YAWS : _Pc = float ( _crit_Yaws . at [ CASRN , 'Pc' ] ) elif Method == SURF : _Pc = third_property ( CASRN = CASRN , P = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Pc
6067	def convergence_from_grid ( self , grid ) : surface_density_grid = np . zeros ( grid . shape [ 0 ] ) grid_eta = self . grid_to_elliptical_radii ( grid ) for i in range ( grid . shape [ 0 ] ) : surface_density_grid [ i ] = self . convergence_func ( grid_eta [ i ] ) return surface_density_grid
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
5415	def parse_args ( parser , provider_required_args , argv ) : # Add the provider required arguments epilog message epilog = 'Provider-required arguments:\n' for provider in provider_required_args : epilog += ' %s: %s\n' % ( provider , provider_required_args [ provider ] ) parser . epilog = epilog # Parse arguments args = parser . parse_args ( argv ) # For the selected provider, check the required arguments for arg in provider_required_args [ args . provider ] : if not args . __getattribute__ ( arg ) : parser . error ( 'argument --%s is required' % arg ) return args
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : # Project Haystack 3.0 type. self . _assert_version ( VER_3_0 )
8040	def is_public ( self ) : # Check if we are a setter/deleter method, and mark as private if so. for decorator in self . decorators : # Given 'foo', match 'foo.bar' but not 'foobar' or 'sfoo' if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
10341	def main ( graph : BELGraph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia_matrices = bel_to_spia_matrices ( graph ) if xlsx : spia_matrices_to_excel ( spia_matrices , xlsx ) if tsvs : spia_matrices_to_tsvs ( spia_matrices , tsvs )
3002	def start ( self ) : if self . extra_args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . _toggle_value : nbextensions . install_nbextension_python ( _pkg_name , overwrite = True , symlink = False , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) else : nbextensions . uninstall_nbextension_python ( _pkg_name , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) self . toggle_nbextension_python ( _pkg_name ) self . toggle_server_extension_python ( _pkg_name )
4866	def save ( self ) : # pylint: disable=arguments-differ enterprise_customer = self . validated_data [ 'enterprise_customer' ] ecu = models . EnterpriseCustomerUser ( user_id = self . user . pk , enterprise_customer = enterprise_customer , ) ecu . save ( )
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
12631	def group_dicom_files ( dicom_file_paths , header_fields ) : dist = SimpleDicomFileDistance ( field_weights = header_fields ) path_list = dicom_file_paths . copy ( ) path_groups = DefaultOrderedDict ( DicomFileSet ) while len ( path_list ) > 0 : file_path1 = path_list . pop ( ) file_subgroup = [ file_path1 ] dist . set_dicom_file1 ( file_path1 ) j = len ( path_list ) - 1 while j >= 0 : file_path2 = path_list [ j ] dist . set_dicom_file2 ( file_path2 ) if dist . transform ( ) : file_subgroup . append ( file_path2 ) path_list . pop ( j ) j -= 1 path_groups [ file_path1 ] . from_set ( file_subgroup , check_if_dicoms = False ) return path_groups
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
10161	def py_hash ( key , num_buckets ) : b , j = - 1 , 0 if num_buckets < 1 : raise ValueError ( 'num_buckets must be a positive number' ) while j < num_buckets : b = int ( j ) key = ( ( key * long ( 2862933555777941757 ) ) + 1 ) & 0xffffffffffffffff j = float ( b + 1 ) * ( float ( 1 << 31 ) / float ( ( key >> 33 ) + 1 ) ) return int ( b )
5683	def tripI_takes_place_on_dsut ( self , trip_I , day_start_ut ) : query = "SELECT * FROM days WHERE trip_I=? AND day_start_ut=?" params = ( trip_I , day_start_ut ) cur = self . conn . cursor ( ) rows = list ( cur . execute ( query , params ) ) if len ( rows ) == 0 : return False else : assert len ( rows ) == 1 , 'On a day, a trip_I should be present at most once' return True
47	def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )
9230	def fetch_repo_creation_date ( self ) : gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . get ( ) if rc == 200 : return REPO_CREATED_TAG_NAME , data [ "created_at" ] else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) return None , None
5494	def expand_mentions ( text , embed_names = True ) : if embed_names : mention_format = "@<{name} {url}>" else : mention_format = "@<{url}>" def handle_mention ( match ) : source = get_source_by_name ( match . group ( 1 ) ) if source is None : return "@{0}" . format ( match . group ( 1 ) ) return mention_format . format ( name = source . nick , url = source . url ) return short_mention_re . sub ( handle_mention , text )
10010	def get_command_names ( ) : ret = [ ] for f in os . listdir ( COMMAND_MODULE_PATH ) : if os . path . isfile ( os . path . join ( COMMAND_MODULE_PATH , f ) ) and f . endswith ( COMMAND_MODULE_SUFFIX ) : ret . append ( f [ : - len ( COMMAND_MODULE_SUFFIX ) ] ) return ret
7604	def get_player_chests ( self , tag : crtag , timeout : int = None ) : url = self . api . PLAYER + '/' + tag + '/upcomingchests' return self . _get_model ( url , timeout = timeout )
6539	def matches_masks ( target , masks ) : for mask in masks : if mask . search ( target ) : return True return False
268	def print_table ( table , name = None , float_format = None , formatters = None , header_rows = None ) : if isinstance ( table , pd . Series ) : table = pd . DataFrame ( table ) if name is not None : table . columns . name = name html = table . to_html ( float_format = float_format , formatters = formatters ) if header_rows is not None : # Count the number of columns for the text to span n_cols = html . split ( '<thead>' ) [ 1 ] . split ( '</thead>' ) [ 0 ] . count ( '<th>' ) # Generate the HTML for the extra rows rows = '' for name , value in header_rows . items ( ) : rows += ( '\n <tr style="text-align: right;"><th>%s</th>' + '<td colspan=%d>%s</td></tr>' ) % ( name , n_cols , value ) # Inject the new HTML html = html . replace ( '<thead>' , '<thead>' + rows ) display ( HTML ( html ) )
10400	def done_chomping ( self ) -> bool : return self . tag in self . graph . nodes [ self . target_node ]
2837	def transfer ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( len ( data ) ) for i in range ( len ( data ) ) : for j in range ( 8 ) : # Write bit to MOSI. if self . _write_shift ( data [ i ] , j ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) # Flip clock off base. self . _gpio . output ( self . _sclk , not self . _clock_base ) # Handle read on leading edge of clock. if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : # Set bit to 1 at appropriate location. result [ i ] |= self . _read_shift ( self . _mask , j ) else : # Set bit to 0 at appropriate location. result [ i ] &= ~ self . _read_shift ( self . _mask , j ) # Return clock to base. self . _gpio . output ( self . _sclk , self . _clock_base ) # Handle read on trailing edge of clock. if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : # Set bit to 1 at appropriate location. result [ i ] |= self . _read_shift ( self . _mask , j ) else : # Set bit to 0 at appropriate location. result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
5745	def fsplit ( file_to_split ) : dirname = file_to_split + '_splitted' if not os . path . exists ( dirname ) : os . mkdir ( dirname ) part_file_size = os . path . getsize ( file_to_split ) / number_of_files + 1 splitted_files = [ ] with open ( file_to_split , "r" ) as f : number = 0 actual = 0 while 1 : prec = actual # Jump of "size" from the current place in the file f . seek ( part_file_size , os . SEEK_CUR ) # find the next separator or EOF s = f . readline ( ) if len ( s ) == 0 : s = f . readline ( ) while len ( s ) != 0 and s != separator : s = f . readline ( ) # Get the current place actual = f . tell ( ) new_file = os . path . join ( dirname , str ( number ) ) # Create the new file with open ( file_to_split , "r" ) as temp : temp . seek ( prec ) # Get the text we want to put in the new file copy = temp . read ( actual - prec ) # Write the new file open ( new_file , 'w' ) . write ( copy ) splitted_files . append ( new_file ) number += 1 # End of file if len ( s ) == 0 : break return splitted_files
10608	def _create_element_list ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
1796	def XADD ( cpu , dest , src ) : MASK = ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) temp = ( arg1 + arg0 ) & MASK src . write ( arg0 ) dest . write ( temp ) # Affected flags: oszapc tempCF = Operators . OR ( Operators . ULT ( temp , arg0 ) , Operators . ULT ( temp , arg1 ) ) cpu . CF = tempCF cpu . AF = ( ( arg0 ^ arg1 ) ^ temp ) & 0x10 != 0 cpu . ZF = temp == 0 cpu . SF = ( temp & SIGN_MASK ) != 0 cpu . OF = ( ( ( arg0 ^ arg1 ^ SIGN_MASK ) & ( temp ^ arg1 ) ) & SIGN_MASK ) != 0 cpu . PF = cpu . _calculate_parity_flag ( temp )
10170	def set_scheduled ( self ) : with self . _idle_lock : if self . _idle : self . _idle = False return True return False
2486	def licenses_from_tree ( self , tree ) : # FIXME: this is unordered! licenses = set ( ) self . licenses_from_tree_helper ( tree , licenses ) return licenses
4350	def vol ( self , gain , gain_type = 'amplitude' , limiter_gain = None ) : if not is_number ( gain ) : raise ValueError ( 'gain must be a number.' ) if limiter_gain is not None : if ( not is_number ( limiter_gain ) or limiter_gain <= 0 or limiter_gain >= 1 ) : raise ValueError ( 'limiter gain must be a positive number less than 1' ) if gain_type in [ 'amplitude' , 'power' ] and gain < 0 : raise ValueError ( "If gain_type = amplitude or power, gain must be positive." ) effect_args = [ 'vol' ] effect_args . append ( '{:f}' . format ( gain ) ) if gain_type == 'amplitude' : effect_args . append ( 'amplitude' ) elif gain_type == 'power' : effect_args . append ( 'power' ) elif gain_type == 'db' : effect_args . append ( 'dB' ) else : raise ValueError ( 'gain_type must be one of amplitude power or db' ) if limiter_gain is not None : if gain_type in [ 'amplitude' , 'power' ] and gain > 1 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) elif gain_type == 'db' and gain > 0 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vol' ) return self
8177	def copy ( self , graph ) : l = self . __class__ ( graph , self . n ) l . i = 0 return l
6347	def _redo_language ( self , term , name_mode , rules , final_rules1 , final_rules2 , concat ) : language_arg = self . _language ( term , name_mode ) return self . _phonetic ( term , name_mode , rules , final_rules1 , final_rules2 , language_arg , concat , )
13658	def _addRoute ( self , f , matcher ) : self . _routes . append ( ( f . func_name , f , matcher ) )
10531	def find_project ( * * kwargs ) : try : res = _pybossa_req ( 'get' , 'project' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : return res except : # pragma: no cover raise
5580	def extract_contours ( array , tile , interval = 100 , field = 'elev' , base = 0 ) : import matplotlib . pyplot as plt levels = _get_contour_values ( array . min ( ) , array . max ( ) , interval = interval , base = base ) if not levels : return [ ] contours = plt . contour ( array , levels ) index = 0 out_contours = [ ] for level in range ( len ( contours . collections ) ) : elevation = levels [ index ] index += 1 paths = contours . collections [ level ] . get_paths ( ) for path in paths : out_coords = [ ( tile . left + ( y * tile . pixel_x_size ) , tile . top - ( x * tile . pixel_y_size ) , ) for x , y in zip ( path . vertices [ : , 1 ] , path . vertices [ : , 0 ] ) ] if len ( out_coords ) >= 2 : out_contours . append ( dict ( properties = { field : elevation } , geometry = mapping ( LineString ( out_coords ) ) ) ) return out_contours
11536	def map_pin ( self , abstract_pin_id , physical_pin_id ) : if physical_pin_id : self . _pin_mapping [ abstract_pin_id ] = physical_pin_id else : self . _pin_mapping . pop ( abstract_pin_id , None )
3673	def draw_2d ( self , width = 300 , height = 300 , Hs = False ) : # pragma: no cover try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol return Draw . MolToImage ( mol , size = ( width , height ) ) except : return 'Rdkit is required for this feature.'
2137	def create ( self , fail_on_found = False , force_on_exists = False , * * kwargs ) : if kwargs . get ( 'parent' , None ) : parent_data = self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs [ 'inventory' ] = parent_data [ 'inventory' ] elif 'inventory' not in kwargs : raise exc . UsageError ( 'To create a group, you must provide a parent inventory or parent group.' ) return super ( Resource , self ) . create ( fail_on_found = fail_on_found , force_on_exists = force_on_exists , * * kwargs )
2803	def convert_slice ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting slice ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert slice by multiple dimensions' ) if params [ 'axes' ] [ 0 ] not in [ 0 , 1 , 2 , 3 ] : raise AssertionError ( 'Slice by dimension more than 3 or less than 0 is not supported' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) , start = int ( params [ 'starts' ] [ 0 ] ) , end = int ( params [ 'ends' ] [ 0 ] ) ) : if axis == 0 : return x [ start : end ] elif axis == 1 : return x [ : , start : end ] elif axis == 2 : return x [ : , : , start : end ] elif axis == 3 : return x [ : , : , : , start : end ] lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
5069	def ungettext_min_max ( singular , plural , range_text , min_val , max_val ) : if min_val is None and max_val is None : return None if min_val == max_val or min_val is None or max_val is None : # pylint: disable=translation-of-non-string return ungettext ( singular , plural , min_val or max_val ) . format ( min_val or max_val ) return range_text . format ( min_val , max_val )
11247	def triangle_area ( point1 , point2 , point3 ) : """Lengths of the three sides of the triangle""" a = point_distance ( point1 , point2 ) b = point_distance ( point1 , point3 ) c = point_distance ( point2 , point3 ) """Where s is the semiperimeter""" s = ( a + b + c ) / 2.0 """Return the area of the triangle (using Heron's formula)""" return math . sqrt ( s * ( s - a ) * ( s - b ) * ( s - c ) )
12711	def add_force ( self , force , relative = False , position = None , relative_position = None ) : b = self . ode_body if relative_position is not None : op = b . addRelForceAtRelPos if relative else b . addForceAtRelPos op ( force , relative_position ) elif position is not None : op = b . addRelForceAtPos if relative else b . addForceAtPos op ( force , position ) else : op = b . addRelForce if relative else b . addForce op ( force )
6133	def from_string ( cls , content , position = 1 , file_id = None ) : if file_id is None : file_id = 'inlined_input' return cls ( FileMetadata ( file_id , position ) , content )
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
10178	def list_bookmarks ( self , start_date = None , end_date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : query = query . filter ( 'range' , date = range_args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )
8610	def get_resource ( self , resource_type , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/resources/%s/%s?depth=%s' % ( resource_type , resource_id , str ( depth ) ) ) return response
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
7623	def melody ( ref , est , * * kwargs ) : namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , ref_p = ref . to_event_values ( ) est_times , est_p = est . to_event_values ( ) ref_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . melody . evaluate ( ref_times , ref_freq , est_times , est_freq , * * kwargs )
12008	def _generate_key ( pass_id , passphrases , salt , algorithm ) : if pass_id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass_id ) passphrase = passphrases [ pass_id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = EncryptedPickle . _get_hashlib ( algorithm [ 'pbkdf2_algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2_iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key_size' ] )
1776	def AND ( cpu , dest , src ) : # XXX bypass a capstone bug that incorrectly extends and computes operands sizes # the bug has been fixed since capstone 4.0.alpha2 (commit de8dd26) if src . size == 64 and src . type == 'immediate' and dest . size == 64 : arg1 = Operators . SEXTEND ( src . read ( ) , 32 , 64 ) else : arg1 = src . read ( ) res = dest . write ( dest . read ( ) & arg1 ) # Defined Flags: szp cpu . _calculate_logic_flags ( dest . size , res )
2685	def fate ( name ) : return cached_download ( 'http://fate.ffmpeg.org/fate-suite/' + name , os . path . join ( 'fate-suite' , name . replace ( '/' , os . path . sep ) ) )
2160	def _format_id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text_type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise MultipleRelatedError ( 'Could not serialize output with id format.' )
3022	def _detect_gce_environment ( ) : # NOTE: The explicit ``timeout`` is a workaround. The underlying # issue is that resolving an unknown host on some networks will take # 20-30 seconds; making this timeout short fixes the issue, but # could lead to false negatives in the event that we are on GCE, but # the metadata resolution was particularly slow. The latter case is # "unlikely". http = transport . get_http_object ( timeout = GCE_METADATA_TIMEOUT ) try : response , _ = transport . request ( http , _GCE_METADATA_URI , headers = _GCE_HEADERS ) return ( response . status == http_client . OK and response . get ( _METADATA_FLAVOR_HEADER ) == _DESIRED_METADATA_FLAVOR ) except socket . error : # socket.timeout or socket.error(64, 'Host is down') logger . info ( 'Timeout attempting to reach GCE metadata service.' ) return False
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
4288	def read_settings ( filename = None ) : logger = logging . getLogger ( __name__ ) logger . info ( "Reading settings ..." ) settings = _DEFAULT_CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings_path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ '__builtins__' ] ) # Make the paths relative to the settings file paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings_path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings_path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings_path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img_size' , 'thumb_size' , 'video_size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img_processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
12780	def get_users ( self , sort = True ) : self . _load ( ) if sort : self . users . sort ( key = operator . itemgetter ( "name" ) ) return self . users
45	def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1
7260	def get_data_location ( self , catalog_id ) : try : record = self . get ( catalog_id ) except : return None # Handle Landsat8 if 'Landsat8' in record [ 'type' ] and 'LandsatAcquisition' in record [ 'type' ] : bucket = record [ 'properties' ] [ 'bucketName' ] prefix = record [ 'properties' ] [ 'bucketPrefix' ] return 's3://' + bucket + '/' + prefix # Handle DG Acquisition if 'DigitalGlobeAcquisition' in record [ 'type' ] : o = Ordering ( ) res = o . location ( [ catalog_id ] ) return res [ 'acquisitions' ] [ 0 ] [ 'location' ] return None
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) # Constrain to a valid fd and one past the end of fds self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
10094	def get_template ( self , template_id , version = None , timeout = None ) : if ( version ) : return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version ) , self . HTTP_GET , timeout = timeout ) else : return self . _api_request ( self . TEMPLATES_SPECIFIC_ENDPOINT % template_id , self . HTTP_GET , timeout = timeout )
3890	def markdown ( tag ) : return ( MARKDOWN_START . format ( tag = tag ) , MARKDOWN_END . format ( tag = tag ) )
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
2889	def parse_node ( self ) : try : self . task = self . create_task ( ) self . task . documentation = self . parser . _parse_documentation ( self . node , xpath = self . xpath , task_parser = self ) boundary_event_nodes = self . process_xpath ( './/bpmn:boundaryEvent[@attachedToRef="%s"]' % self . get_id ( ) ) if boundary_event_nodes : parent_task = _BoundaryEventParent ( self . spec , '%s.BoundaryEventParent' % self . get_id ( ) , self . task , lane = self . task . lane ) self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = parent_task parent_task . connect_outgoing ( self . task , '%s.FromBoundaryEventParent' % self . get_id ( ) , None , None ) for boundary_event in boundary_event_nodes : b = self . process_parser . parse_node ( boundary_event ) parent_task . connect_outgoing ( b , '%s.FromBoundaryEventParent' % boundary_event . get ( 'id' ) , None , None ) else : self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process_xpath ( './/bpmn:sequenceFlow[@sourceRef="%s"]' % self . get_id ( ) ) if len ( outgoing ) > 1 and not self . handles_multiple_outgoing ( ) : raise ValidationException ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process_parser . filename ) for sequence_flow in outgoing : target_ref = sequence_flow . get ( 'targetRef' ) target_node = one ( self . process_xpath ( './/*[@id="%s"]' % target_ref ) ) c = self . process_parser . parse_node ( target_node ) children . append ( ( c , target_node , sequence_flow ) ) if children : default_outgoing = self . node . get ( 'default' ) if not default_outgoing : ( c , target_node , sequence_flow ) = children [ 0 ] default_outgoing = sequence_flow . get ( 'id' ) for ( c , target_node , sequence_flow ) in children : self . connect_outgoing ( c , target_node , sequence_flow , sequence_flow . get ( 'id' ) == default_outgoing ) return parent_task if boundary_event_nodes else self . task except ValidationException : raise except Exception as ex : exc_info = sys . exc_info ( ) tb = "" . join ( traceback . format_exception ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise ValidationException ( "%r" % ( ex ) , node = self . node , filename = self . process_parser . filename )
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
3318	def get ( self , token ) : self . _lock . acquire_read ( ) try : lock = self . _dict . get ( token ) if lock is None : # Lock not found: purge dangling URL2TOKEN entries _logger . debug ( "Lock purged dangling: {}" . format ( token ) ) self . delete ( token ) return None expire = float ( lock [ "expire" ] ) if expire >= 0 and expire < time . time ( ) : _logger . debug ( "Lock timed-out({}): {}" . format ( expire , lock_string ( lock ) ) ) self . delete ( token ) return None return lock finally : self . _lock . release ( )
12090	def proto_01_13_steps025dual ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A_' + feature ) f1 = swhlab . ap . getAvgBySweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . getAvgBySweep ( abf , 'freq' , 1 , None ) f1 = np . nan_to_num ( f1 ) f2 = np . nan_to_num ( f2 ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = "step 1" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = "step 2" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )
161	def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
6369	def recall ( self ) : if self . _tp + self . _fn == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fn )
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) # Try to set the random state from the last session to preserve # a single random stream when simulating timestamps multiple times if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
1087	def concat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) # Special MathML handling ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : # We are doing our own prettyfication as etree pretty_print is too insane. ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
8242	def outline ( path , colors , precision = 0.4 , continuous = True ) : # The count of points in a given path/contour. def _point_count ( path , precision ) : return max ( int ( path . length * precision * 0.5 ) , 10 ) # The total count of points in the path. n = sum ( [ _point_count ( contour , precision ) for contour in path . contours ] ) # For a continuous gradient, # we need to calculate a subrange in the list of colors # for each contour to draw colors from. contour_i = 0 contour_n = len ( path . contours ) - 1 if contour_n == 0 : continuous = False i = 0 for contour in path . contours : if not continuous : i = 0 # The number of points for each contour. j = _point_count ( contour , precision ) first = True for pt in contour . points ( j ) : if first : first = False else : if not continuous : # If we have a list of 100 colors and 50 points, # point i maps to color i*2. clr = float ( i ) / j * len ( colors ) else : # In a continuous gradient of 100 colors, # the 2nd contour in a path with 10 contours # draws colors between 10-20 clr = float ( i ) / n * len ( colors ) - 1 * contour_i / contour_n _ctx . stroke ( colors [ int ( clr ) ] ) _ctx . line ( x0 , y0 , pt . x , pt . y ) x0 = pt . x y0 = pt . y i += 1 pt = contour . point ( 0.9999999 ) # Fix in pathmatics! _ctx . line ( x0 , y0 , pt . x , pt . y ) contour_i += 1
11671	def _flann_args ( self , X = None ) : args = { 'cores' : self . _n_jobs } if self . flann_algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree_single' else : args [ 'algorithm' ] = self . flann_algorithm if self . flann_args : args . update ( self . flann_args ) # check that arguments are correct try : FLANNParameters ( ) . update ( args ) except AttributeError as e : msg = "flann_args contains an invalid argument:\n {}" raise TypeError ( msg . format ( e ) ) return args
2319	def autoset_settings ( set_var ) : try : devices = ast . literal_eval ( os . environ [ "CUDA_VISIBLE_DEVICES" ] ) if type ( devices ) != list and type ( devices ) != tuple : devices = [ devices ] if len ( devices ) != 0 : set_var . GPU = len ( devices ) set_var . NB_JOBS = len ( devices ) warnings . warn ( "Detecting CUDA devices : {}" . format ( devices ) ) except KeyError : set_var . GPU = check_cuda_devices ( ) set_var . NB_JOBS = set_var . GPU warnings . warn ( "Detecting {} CUDA devices." . format ( set_var . GPU ) ) if not set_var . GPU : warnings . warn ( "No GPU automatically detected. Setting SETTINGS.GPU to 0, " + "and SETTINGS.NB_JOBS to cpu_count." ) set_var . GPU = 0 set_var . NB_JOBS = multiprocessing . cpu_count ( ) return set_var
9500	def _get_instructions_bytes ( code , varnames = None , names = None , constants = None , cells = None , linestarts = None , line_offset = 0 ) : labels = dis . findlabels ( code ) extended_arg = 0 starts_line = None free = None # enumerate() is not an option, since we sometimes process # multiple elements on a single pass through the loop n = len ( code ) i = 0 while i < n : op = code [ i ] offset = i if linestarts is not None : starts_line = linestarts . get ( i , None ) if starts_line is not None : starts_line += line_offset is_jump_target = i in labels i = i + 1 arg = None argval = None argrepr = '' if op >= dis . HAVE_ARGUMENT : arg = code [ i ] + code [ i + 1 ] * 256 + extended_arg extended_arg = 0 i = i + 2 if op == dis . EXTENDED_ARG : extended_arg = arg * 65536 # Set argval to the dereferenced value of the argument when # availabe, and argrepr to the string representation of argval. # _disassemble_bytes needs the string repr of the # raw name index for LOAD_GLOBAL, LOAD_CONST, etc. argval = arg if op in dis . hasconst : argval , argrepr = dis . _get_const_info ( arg , constants ) elif op in dis . hasname : argval , argrepr = dis . _get_name_info ( arg , names ) elif op in dis . hasjrel : argval = i + arg argrepr = "to " + repr ( argval ) elif op in dis . haslocal : argval , argrepr = dis . _get_name_info ( arg , varnames ) elif op in dis . hascompare : argval = dis . cmp_op [ arg ] argrepr = argval elif op in dis . hasfree : argval , argrepr = dis . _get_name_info ( arg , cells ) elif op in dis . hasnargs : argrepr = "%d positional, %d keyword pair" % ( code [ i - 2 ] , code [ i - 1 ] ) yield dis . Instruction ( dis . opname [ op ] , op , arg , argval , argrepr , offset , starts_line , is_jump_target )
9184	def _node_to_model ( tree_or_item , metadata = None , parent = None , lucent_id = cnxepub . TRANSLUCENT_BINDER_ID ) : if 'contents' in tree_or_item : # It is a binder. tree = tree_or_item binder = cnxepub . TranslucentBinder ( metadata = tree ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder , lucent_id = lucent_id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) result = binder else : # It is an item pointing at a document. item = tree_or_item result = cnxepub . DocumentPointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result
4233	def autodetect_url ( ) : for url in [ "http://routerlogin.net:5000" , "https://routerlogin.net" , "http://routerlogin.net" ] : try : r = requests . get ( url + "/soap/server_sa/" , headers = _get_soap_headers ( "Test:1" , "test" ) , verify = False ) if r . status_code == 200 : return url except requests . exceptions . RequestException : pass return None
3351	def get_by_any ( self , iterable ) : def get_item ( item ) : if isinstance ( item , int ) : return self [ item ] elif isinstance ( item , string_types ) : return self . get_by_id ( item ) elif item in self : return item else : raise TypeError ( "item in iterable cannot be '%s'" % type ( item ) ) if not isinstance ( iterable , list ) : iterable = [ iterable ] return [ get_item ( item ) for item in iterable ]
5328	def __get_uuids_from_profile_name ( self , profile_name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile_name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids
6605	def result_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . result_relpath ( package_index ) ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009/result.p.gz' return ret
926	def generateDataset ( aggregationInfo , inputFilename , outputFilename = None ) : # Create the input stream inputFullPath = resource_filename ( "nupic.datafiles" , inputFilename ) inputObj = FileRecordStream ( inputFullPath ) # Instantiate the aggregator aggregator = Aggregator ( aggregationInfo = aggregationInfo , inputFields = inputObj . getFields ( ) ) # Is it a null aggregation? If so, just return the input file unmodified if aggregator . isNullAggregation ( ) : return inputFullPath # ------------------------------------------------------------------------ # If we were not given an output filename, create one based on the # aggregation settings if outputFilename is None : outputFilename = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFullPath ) ) [ 0 ] timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if aggregationInfo . get ( k , 0 ) > 0 : outputFilename += '_%s_%d' % ( k , aggregationInfo [ k ] ) outputFilename += '.csv' outputFilename = os . path . join ( os . path . dirname ( inputFullPath ) , outputFilename ) # ------------------------------------------------------------------------ # If some other process already started creating this file, simply # wait for it to finish and return without doing anything lockFilePath = outputFilename + '.please_wait' if os . path . isfile ( outputFilename ) or os . path . isfile ( lockFilePath ) : while os . path . isfile ( lockFilePath ) : print 'Waiting for %s to be fully written by another process' % lockFilePath time . sleep ( 1 ) return outputFilename # Create the lock file lockFD = open ( lockFilePath , 'w' ) # ------------------------------------------------------------------------- # Create the output stream outputObj = FileRecordStream ( streamID = outputFilename , write = True , fields = inputObj . getFields ( ) ) # ------------------------------------------------------------------------- # Write all aggregated records to the output while True : inRecord = inputObj . getNextRecord ( ) ( aggRecord , aggBookmark ) = aggregator . next ( inRecord , None ) if aggRecord is None and inRecord is None : break if aggRecord is not None : outputObj . appendRecord ( aggRecord ) return outputFilename
12486	def get_possible_paths ( base_path , path_regex ) : if not path_regex : return [ ] if len ( path_regex ) < 1 : return [ ] if path_regex [ 0 ] == os . sep : path_regex = path_regex [ 1 : ] rest_files = '' if os . sep in path_regex : #split by os.sep node_names = path_regex . partition ( os . sep ) first_node = node_names [ 0 ] rest_nodes = node_names [ 2 ] folder_names = filter_list ( os . listdir ( base_path ) , first_node ) for nom in folder_names : new_base = op . join ( base_path , nom ) if op . isdir ( new_base ) : rest_files = get_possible_paths ( new_base , rest_nodes ) else : rest_files = filter_list ( os . listdir ( base_path ) , path_regex ) files = [ ] if rest_files : files = [ op . join ( base_path , f ) for f in rest_files ] return files
13880	def MoveFile ( source_filename , target_filename ) : _AssertIsLocal ( source_filename ) _AssertIsLocal ( target_filename ) import shutil shutil . move ( source_filename , target_filename )
12480	def get_rcfile_section ( app_name , section_name ) : try : settings = rcfile ( app_name , section_name ) except IOError : raise except : raise KeyError ( 'Error looking for section {} in {} ' ' rcfiles.' . format ( section_name , app_name ) ) else : return settings
10901	def check_inputs ( self , comps ) : error = False compcats = [ c . category for c in comps ] # Check that the components are all provided, given the categories for k , v in iteritems ( self . varmap ) : if k not in self . modelstr [ 'full' ] : log . warn ( 'Component (%s : %s) not used in model.' % ( k , v ) ) if v not in compcats : log . error ( 'Map component (%s : %s) not found in list of components.' % ( k , v ) ) error = True if error : raise ModelError ( 'Component list incomplete or incorrect' )
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) # call get service with headers and params with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
873	def getState ( self ) : varStates = dict ( ) for varName , var in self . permuteVars . iteritems ( ) : varStates [ varName ] = var . getState ( ) return dict ( id = self . particleId , genIdx = self . genIdx , swarmId = self . swarmId , varStates = varStates )
9107	def sanitize_filename ( filename ) : # TODO: fix broken splitext (it reveals everything of the filename after the first `.` - doh!) token = generate_drop_id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token
7324	def create_sample_input_files ( template_filename , database_filename , config_filename ) : print ( "Creating sample template email {}" . format ( template_filename ) ) if os . path . exists ( template_filename ) : print ( "Error: file exists: " + template_filename ) sys . exit ( 1 ) with io . open ( template_filename , "w" ) as template_file : template_file . write ( u"TO: {{email}}\n" u"SUBJECT: Testing mailmerge\n" u"FROM: My Self <myself@mydomain.com>\n" u"\n" u"Hi, {{name}},\n" u"\n" u"Your number is {{number}}.\n" ) print ( "Creating sample database {}" . format ( database_filename ) ) if os . path . exists ( database_filename ) : print ( "Error: file exists: " + database_filename ) sys . exit ( 1 ) with io . open ( database_filename , "w" ) as database_file : database_file . write ( u'email,name,number\n' u'myself@mydomain.com,"Myself",17\n' u'bob@bobdomain.com,"Bob",42\n' ) print ( "Creating sample config file {}" . format ( config_filename ) ) if os . path . exists ( config_filename ) : print ( "Error: file exists: " + config_filename ) sys . exit ( 1 ) with io . open ( config_filename , "w" ) as config_file : config_file . write ( u"# Example: GMail\n" u"[smtp_server]\n" u"host = smtp.gmail.com\n" u"port = 465\n" u"security = SSL/TLS\n" u"username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: Wide open\n" u"# [smtp_server]\n" u"# host = open-smtp.example.com\n" u"# port = 25\n" u"# security = Never\n" u"# username = None\n" u"#\n" u"# Example: University of Michigan\n" u"# [smtp_server]\n" u"# host = smtp.mail.umich.edu\n" u"# port = 465\n" u"# security = SSL/TLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with STARTTLS security\n" # noqa: E501 u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = STARTTLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with no encryption\n" # noqa: E501 u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = Never\n" u"# username = YOUR_USERNAME_HERE\n" ) print ( "Edit these files, and then run mailmerge again" )
5903	def glob_parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) # at least some rough sorting... return files
95	def quokka_segmentation_map ( size = None , extract = None ) : # TODO get rid of this deferred import from imgaug . augmentables . segmaps import SegmentationMapOnImage with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) xx = [ ] yy = [ ] for kp_dict in json_dict [ "polygons" ] [ 0 ] [ "keypoints" ] : x = kp_dict [ "x" ] y = kp_dict [ "y" ] xx . append ( x ) yy . append ( y ) img_seg = np . zeros ( ( 643 , 960 , 1 ) , dtype = np . float32 ) rr , cc = skimage . draw . polygon ( np . array ( yy ) , np . array ( xx ) , shape = img_seg . shape ) img_seg [ rr , cc ] = 1.0 if extract is not None : bb = _quokka_normalize_extract ( extract ) img_seg = bb . extract_from_image ( img_seg ) segmap = SegmentationMapOnImage ( img_seg , shape = img_seg . shape [ 0 : 2 ] + ( 3 , ) ) if size is not None : shape_resized = _compute_resized_shape ( img_seg . shape , size ) segmap = segmap . resize ( shape_resized [ 0 : 2 ] ) segmap . shape = tuple ( shape_resized [ 0 : 2 ] ) + ( 3 , ) return segmap
7180	def reapply_all ( ast_node , lib2to3_node ) : late_processing = reapply ( ast_node , lib2to3_node ) for lazy_func in reversed ( late_processing ) : lazy_func ( )
1622	def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : # Having // dummy comments makes the lines non-empty, so we will not get # unnecessary blank line warnings later in the code. for i in range ( begin , end ) : lines [ i ] = '/**/'
9504	def distance_to_point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
4474	def __serial_transform ( self , jam , steps ) : # This uses the round-robin itertools recipe if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
7006	def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None confmatrix = clfdict [ 'best_confmatrix' ] overall_feature_importances = clfdict [ 'best_classifier' ] . feature_importances_ feature_importances_per_tree = np . array ( [ tree . feature_importances_ for tree in clfdict [ 'best_classifier' ] . estimators_ ] ) stdev_feature_importances = np . std ( feature_importances_per_tree , axis = 0 ) feature_names = np . array ( clfdict [ 'feature_names' ] ) plt . figure ( figsize = ( 6.4 * 3.0 , 4.8 ) ) # confusion matrix plt . subplot ( 121 ) classes = np . array ( classlabels ) plt . imshow ( confmatrix , interpolation = 'nearest' , cmap = plt . cm . Blues ) tick_marks = np . arange ( len ( classes ) ) plt . xticks ( tick_marks , classes ) plt . yticks ( tick_marks , classes ) plt . title ( 'evaluation set confusion matrix' ) plt . ylabel ( 'predicted class' ) plt . xlabel ( 'actual class' ) thresh = confmatrix . max ( ) / 2. for i , j in itertools . product ( range ( confmatrix . shape [ 0 ] ) , range ( confmatrix . shape [ 1 ] ) ) : plt . text ( j , i , confmatrix [ i , j ] , horizontalalignment = "center" , color = "white" if confmatrix [ i , j ] > thresh else "black" ) # feature importances plt . subplot ( 122 ) features = np . array ( feature_names ) sorted_ind = np . argsort ( overall_feature_importances ) [ : : - 1 ] features = features [ sorted_ind ] feature_names = feature_names [ sorted_ind ] overall_feature_importances = overall_feature_importances [ sorted_ind ] stdev_feature_importances = stdev_feature_importances [ sorted_ind ] plt . bar ( np . arange ( 0 , features . size ) , overall_feature_importances , yerr = stdev_feature_importances , width = 0.8 , color = 'grey' ) plt . xticks ( np . arange ( 0 , features . size ) , features , rotation = 90 ) plt . yticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) plt . xlim ( - 0.75 , features . size - 1.0 + 0.75 ) plt . ylim ( 0.0 , 0.9 ) plt . ylabel ( 'relative importance' ) plt . title ( 'relative importance of features' ) plt . subplots_adjust ( wspace = 0.1 ) plt . savefig ( outfile , bbox_inches = 'tight' , dpi = 100 ) plt . close ( 'all' ) return outfile
3691	def solve_T ( self , P , V , quick = True ) : if self . S2 == 0 : self . m = self . S1 return SRK . solve_T ( self , P , V , quick = quick ) else : # Previously coded method is 63 microseconds vs 47 here # return super(SRK, self).solve_T(P, V, quick=quick) Tc , a , b , S1 , S2 = self . Tc , self . a , self . b , self . S1 , self . S2 if quick : x2 = R / ( V - b ) x3 = ( V * ( V + b ) ) def to_solve ( T ) : x0 = ( T / Tc ) ** 0.5 x1 = x0 - 1. return ( x2 * T - a * ( S1 * x1 + S2 * x1 / x0 - 1. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( S1 * ( - sqrt ( T / Tc ) + 1 ) + S2 * ( - sqrt ( T / Tc ) + 1 ) / sqrt ( T / Tc ) + 1 ) ** 2 / ( V * ( V + b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
12122	def filter_gaussian ( self , sigmaMs = 100 , applyFiltered = False , applyBaseline = False ) : if sigmaMs == 0 : return self . dataY filtered = cm . filter_gaussian ( self . dataY , sigmaMs ) if applyBaseline : self . dataY = self . dataY - filtered elif applyFiltered : self . dataY = filtered else : return filtered
6124	def gaussian_prior_model_for_arguments ( self , arguments ) : new_model = copy . deepcopy ( self ) for key , value in filter ( lambda t : isinstance ( t [ 1 ] , pm . PriorModel ) , self . __dict__ . items ( ) ) : setattr ( new_model , key , value . gaussian_prior_model_for_arguments ( arguments ) ) return new_model
5534	def execute ( self , process_tile , raise_nodata = False ) : if self . config . mode not in [ "memory" , "continue" , "overwrite" ] : raise ValueError ( "process mode must be memory, continue or overwrite" ) if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif isinstance ( process_tile , BufferedTile ) : pass else : raise TypeError ( "process_tile must be tuple or BufferedTile" ) if process_tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( process_tile ) return self . _execute ( process_tile , raise_nodata = raise_nodata )
7680	def beat_position ( annotation , * * kwargs ) : times , values = annotation . to_interval_values ( ) labels = [ _ [ 'position' ] for _ in values ] # TODO: plot time signature, measure number return mir_eval . display . events ( times , labels = labels , * * kwargs )
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
13324	def info ( ) : env = cpenv . get_active_env ( ) modules = [ ] if env : modules = env . get_modules ( ) active_modules = cpenv . get_active_modules ( ) if not env and not modules and not active_modules : click . echo ( '\nNo active modules...' ) return click . echo ( bold ( '\nActive modules' ) ) if env : click . echo ( format_objects ( [ env ] + active_modules ) ) available_modules = set ( modules ) - set ( active_modules ) if available_modules : click . echo ( bold ( '\nInactive modules in {}\n' ) . format ( cyan ( env . name ) ) ) click . echo ( format_objects ( available_modules , header = False ) ) else : click . echo ( format_objects ( active_modules ) ) available_shared_modules = set ( cpenv . get_modules ( ) ) - set ( active_modules ) if not available_shared_modules : return click . echo ( bold ( '\nInactive shared modules \n' ) ) click . echo ( format_objects ( available_shared_modules , header = False ) )
2266	def map_vals ( func , dict_ ) : if not hasattr ( func , '__call__' ) : func = func . __getitem__ keyval_list = [ ( key , func ( val ) ) for key , val in six . iteritems ( dict_ ) ] dictclass = OrderedDict if isinstance ( dict_ , OrderedDict ) else dict newdict = dictclass ( keyval_list ) # newdict = type(dict_)(keyval_list) return newdict
6965	def get ( self ) : # add the reviewed key to the current dict if it doesn't exist # this will hold all the reviewed objects for the frontend if 'reviewed' not in self . currentproject : self . currentproject [ 'reviewed' ] = { } # just returns the current project as JSON self . write ( self . currentproject )
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
7803	def display_name ( self ) : if self . subject_name : return u", " . join ( [ u", " . join ( [ u"{0}={1}" . format ( k , v ) for k , v in dn_tuple ] ) for dn_tuple in self . subject_name ] ) for name_type in ( "XmppAddr" , "DNS" , "SRV" ) : names = self . alt_names . get ( name_type ) if names : return names [ 0 ] return u"<unknown>"
12203	def _compile ( self , parselet_node , level = 0 ) : if self . DEBUG : debug_offset = "" . join ( [ " " for x in range ( level ) ] ) if self . DEBUG : print ( debug_offset , "%s::compile(%s)" % ( self . __class__ . __name__ , parselet_node ) ) if isinstance ( parselet_node , dict ) : parselet_tree = ParsleyNode ( ) for k , v in list ( parselet_node . items ( ) ) : # we parse the key raw elements but without much # interpretation (which is done by the SelectorHandler) try : m = self . REGEX_PARSELET_KEY . match ( k ) if not m : if self . DEBUG : print ( debug_offset , "could not parse key" , k ) raise InvalidKeySyntax ( k ) except : raise InvalidKeySyntax ( "Key %s is not valid" % k ) key = m . group ( 'key' ) # by default, fields are required key_required = True operator = m . group ( 'operator' ) if operator == '?' : key_required = False # FIXME: "!" operator not supported (complete array) scope = m . group ( 'scope' ) # example: get list of H3 tags # { "titles": ["h3"] } # FIXME: should we support multiple selectors in list? # e.g. { "titles": ["h1", "h2", "h3", "h4"] } if isinstance ( v , ( list , tuple ) ) : v = v [ 0 ] iterate = True else : iterate = False # keys in the abstract Parsley trees are of type `ParsleyContext` try : parsley_context = ParsleyContext ( key , operator = operator , required = key_required , scope = self . selector_handler . make ( scope ) if scope else None , iterate = iterate ) except SyntaxError : if self . DEBUG : print ( "Invalid scope:" , k , scope ) raise if self . DEBUG : print ( debug_offset , "current context:" , parsley_context ) # go deeper in the Parsley tree... try : child_tree = self . _compile ( v , level = level + 1 ) except SyntaxError : if self . DEBUG : print ( "Invalid value: " , v ) raise except : raise if self . DEBUG : print ( debug_offset , "child tree:" , child_tree ) parselet_tree [ parsley_context ] = child_tree return parselet_tree # a string leaf should match some kind of selector, # let the selector handler deal with it elif isstr ( parselet_node ) : return self . selector_handler . make ( parselet_node ) else : raise ValueError ( "Unsupported type(%s) for Parselet node <%s>" % ( type ( parselet_node ) , parselet_node ) )
11841	def TraceAgent ( agent ) : old_program = agent . program def new_program ( percept ) : action = old_program ( percept ) print '%s perceives %s and does %s' % ( agent , percept , action ) return action agent . program = new_program return agent
883	def activateDendrites ( self , learn = True ) : ( numActiveConnected , numActivePotential ) = self . connections . computeActivity ( self . activeCells , self . connectedPermanence ) activeSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActiveConnected ) ) if numActiveConnected [ i ] >= self . activationThreshold ) matchingSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActivePotential ) ) if numActivePotential [ i ] >= self . minThreshold ) self . activeSegments = sorted ( activeSegments , key = self . connections . segmentPositionSortKey ) self . matchingSegments = sorted ( matchingSegments , key = self . connections . segmentPositionSortKey ) self . numActiveConnectedSynapsesForSegment = numActiveConnected self . numActivePotentialSynapsesForSegment = numActivePotential if learn : for segment in self . activeSegments : self . lastUsedIterationForSegment [ segment . flatIdx ] = self . iteration self . iteration += 1
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) # if Froude number is out of interpolation range, nearest extrapolation is used return Cr
5568	def bounds_at_zoom ( self , zoom = None ) : return ( ) if self . area_at_zoom ( zoom ) . is_empty else Bounds ( * self . area_at_zoom ( zoom ) . bounds )
3550	def list_characteristics ( self ) : paths = self . _props . Get ( _SERVICE_INTERFACE , 'Characteristics' ) return map ( BluezGattCharacteristic , get_provider ( ) . _get_objects_by_path ( paths ) )
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
5317	def use_style ( self , style_name ) : try : style = getattr ( styles , style_name . upper ( ) ) except AttributeError : raise ColorfulError ( 'the style "{0}" is undefined' . format ( style_name ) ) else : self . colorpalette = style
1894	def _send ( self , cmd : str ) : logger . debug ( '>%s' , cmd ) try : self . _proc . stdout . flush ( ) self . _proc . stdin . write ( f'{cmd}\n' ) except IOError as e : raise SolverError ( str ( e ) )
6502	def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
9546	def add_record_check ( self , record_check , modulus = 1 ) : assert callable ( record_check ) , 'record check must be a callable function' t = record_check , modulus self . _record_checks . append ( t )
9396	def extract_metric_name ( self , metric_name ) : for metric_type in self . supported_sar_types : if metric_type in metric_name : return metric_type logger . error ( 'Section [%s] does not contain a valid metric type, using type: "SAR-generic". Naarad works better ' 'if it knows the metric type. Valid SAR metric names are: %s' , metric_name , self . supported_sar_types ) return 'SAR-generic'
13510	def pyflakes ( ) : # filter out subpackages packages = [ x for x in options . setup . packages if '.' not in x ] sh ( 'pyflakes {param} {files}' . format ( param = options . paved . pycheck . pyflakes . param , files = ' ' . join ( packages ) ) )
9199	def reversals ( series , left = False , right = False ) : series = iter ( series ) x_last , x = next ( series ) , next ( series ) d_last = ( x - x_last ) if left : yield x_last for x_next in series : if x_next == x : continue d_next = x_next - x if d_last * d_next < 0 : yield x x_last , x = x , x_next d_last = d_next if right : yield x_next
3408	def parse_gpr ( str_expr ) : str_expr = str_expr . strip ( ) if len ( str_expr ) == 0 : return None , set ( ) for char , escaped in replacements : if char in str_expr : str_expr = str_expr . replace ( char , escaped ) escaped_str = keyword_re . sub ( "__cobra_escape__" , str_expr ) escaped_str = number_start_re . sub ( "__cobra_escape__" , escaped_str ) tree = ast_parse ( escaped_str , "<string>" , "eval" ) cleaner = GPRCleaner ( ) cleaner . visit ( tree ) eval_gpr ( tree , set ( ) ) # ensure the rule can be evaluated return tree , cleaner . gene_set
4173	def window_visu ( N = 51 , name = 'hamming' , * * kargs ) : # get the default parameters mindB = kargs . pop ( 'mindB' , - 100 ) maxdB = kargs . pop ( 'maxdB' , None ) norm = kargs . pop ( 'norm' , True ) # create a window object w = Window ( N , name , * * kargs ) # plot the time and frequency windows w . plot_time_freq ( mindB = mindB , maxdB = maxdB , norm = norm )
10672	def write_compound_to_auxi_file ( directory , compound ) : file_name = "Compound_" + compound . formula + ".json" with open ( os . path . join ( directory , file_name ) , 'w' ) as f : f . write ( str ( compound ) )
10803	def tk ( self , k , x ) : weights = np . diag ( np . ones ( k + 1 ) ) [ k ] return np . polynomial . chebyshev . chebval ( self . _x2c ( x ) , weights )
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) # Set up our mat file paths. out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) # Save the request data to the output file. req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) # Set up the engine and evaluate the `_pyeval()` function. engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) # Read in the output. resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] # Check for sentinel value. if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
5828	def pif_multi_search ( self , multi_query ) : failure_message = "Error while making PIF multi search request" response_dict = self . _get_success_json ( self . _post ( routes . pif_multi_search , data = json . dumps ( multi_query , cls = QueryEncoder ) , failure_message = failure_message ) ) return PifMultiSearchResult ( * * keys_to_snake_case ( response_dict [ 'results' ] ) )
260	def create_perf_attrib_stats ( perf_attrib , risk_exposures ) : summary = OrderedDict ( ) total_returns = perf_attrib [ 'total_returns' ] specific_returns = perf_attrib [ 'specific_returns' ] common_returns = perf_attrib [ 'common_returns' ] summary [ 'Annualized Specific Return' ] = ep . annual_return ( specific_returns ) summary [ 'Annualized Common Return' ] = ep . annual_return ( common_returns ) summary [ 'Annualized Total Return' ] = ep . annual_return ( total_returns ) summary [ 'Specific Sharpe Ratio' ] = ep . sharpe_ratio ( specific_returns ) summary [ 'Cumulative Specific Return' ] = ep . cum_returns_final ( specific_returns ) summary [ 'Cumulative Common Return' ] = ep . cum_returns_final ( common_returns ) summary [ 'Total Returns' ] = ep . cum_returns_final ( total_returns ) summary = pd . Series ( summary , name = '' ) annualized_returns_by_factor = [ ep . annual_return ( perf_attrib [ c ] ) for c in risk_exposures . columns ] cumulative_returns_by_factor = [ ep . cum_returns_final ( perf_attrib [ c ] ) for c in risk_exposures . columns ] risk_exposure_summary = pd . DataFrame ( data = OrderedDict ( [ ( 'Average Risk Factor Exposure' , risk_exposures . mean ( axis = 'rows' ) ) , ( 'Annualized Return' , annualized_returns_by_factor ) , ( 'Cumulative Return' , cumulative_returns_by_factor ) , ] ) , index = risk_exposures . columns , ) return summary , risk_exposure_summary
8163	def _set_mode ( self , mode ) : if mode == CENTER : self . _call_transform_mode = self . _center_transform elif mode == CORNER : self . _call_transform_mode = self . _corner_transform else : raise ValueError ( 'mode must be CENTER or CORNER' )
4364	def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : # '1::' [path] [query] msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : # heartbeat msg += '::' elif msg in [ '3' , '4' , '5' ] : # '3:' [id ('+')] ':' [endpoint] ':' [data] # '4:' [id ('+')] ':' [endpoint] ':' [json] # '5:' [id ('+')] ':' [endpoint] ':' [json encoded event] # The message id is an incremental integer, required for ACKs. # If the message id is followed by a +, the ACK is not handled by # socket.io, but by the user instead. if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : # '6:::' [id] '+' [data] msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : # '7::' [endpoint] ':' [reason] '+' [advice] msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] # NoOp, used to close a poll after the polling duration time elif msg == '8' : msg += '::' return msg
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
3685	def set_from_PT ( self , Vs ) : # All roots will have some imaginary component; ignore them if > 1E-9 good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : # Even in the case of three real roots, it is still the min/max that make sense self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
11675	def bare ( self ) : if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
8433	def cubehelix_pal ( start = 0 , rot = .4 , gamma = 1.0 , hue = 0.8 , light = .85 , dark = .15 , reverse = False ) : cdict = mpl . _cm . cubehelix ( gamma , start , rot , hue ) cubehelix_cmap = mpl . colors . LinearSegmentedColormap ( 'cubehelix' , cdict ) def cubehelix_palette ( n ) : values = np . linspace ( light , dark , n ) return [ mcolors . rgb2hex ( cubehelix_cmap ( x ) ) for x in values ] return cubehelix_palette
12407	def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( * * value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
2193	def isatty ( self ) : # nocover return ( self . redirect is not None and hasattr ( self . redirect , 'isatty' ) and self . redirect . isatty ( ) )
6467	def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
8400	def rescale ( x , to = ( 0 , 1 ) , _from = None ) : if _from is None : _from = np . min ( x ) , np . max ( x ) return np . interp ( x , _from , to )
959	def aggregationDivide ( dividend , divisor ) : # Convert each into microseconds dividendMonthSec = aggregationToMonthsSeconds ( dividend ) divisorMonthSec = aggregationToMonthsSeconds ( divisor ) # It is a usage error to mix both months and seconds in the same operation if ( dividendMonthSec [ 'months' ] != 0 and divisorMonthSec [ 'seconds' ] != 0 ) or ( dividendMonthSec [ 'seconds' ] != 0 and divisorMonthSec [ 'months' ] != 0 ) : raise RuntimeError ( "Aggregation dicts with months/years can only be " "inter-operated with other aggregation dicts that contain " "months/years" ) if dividendMonthSec [ 'months' ] > 0 : return float ( dividendMonthSec [ 'months' ] ) / divisor [ 'months' ] else : return float ( dividendMonthSec [ 'seconds' ] ) / divisorMonthSec [ 'seconds' ]
2985	def try_match ( request_origin , maybe_regex ) : if isinstance ( maybe_regex , RegexObject ) : return re . match ( maybe_regex , request_origin ) elif probably_regex ( maybe_regex ) : return re . match ( maybe_regex , request_origin , flags = re . IGNORECASE ) else : try : return request_origin . lower ( ) == maybe_regex . lower ( ) except AttributeError : return request_origin == maybe_regex
12421	def dump ( obj , fp , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : if startindex < 0 : raise ValueError ( 'startindex must be non-negative, but was {}' . format ( startindex ) ) try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return if isinstance ( firstkey , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator for key , value in six . iteritems ( obj ) : if isinstance ( value , ( list , tuple , set ) ) : for index , item in enumerate ( value , start = startindex ) : fp . write ( key ) fp . write ( index_separator ) fp . write ( converter ( str ( index ) ) ) fp . write ( separator ) fp . write ( item ) fp . write ( newline ) else : fp . write ( key ) fp . write ( separator ) fp . write ( value ) fp . write ( newline )
11585	def _getnodenamefor ( self , name ) : return 'node_' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no_servers ) + 1 )
1550	def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) # Add output streams self . _add_out_streams ( spout ) return spout
10847	def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post_data = "text=%s&" % text post_data += "profile_ids[]=%s&" % self . profile_id if shorten : post_data += "shorten=%s&" % shorten if now : post_data += "now=%s&" % now if top : post_data += "top=%s&" % top if when : post_data += "scheduled_at=%s&" % str ( when ) if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) new_update = Update ( api = self . api , raw_response = response [ 'updates' ] [ 0 ] ) self . append ( new_update ) return new_update
7483	def parse_single_results ( data , sample , res1 ) : ## set default values #sample.stats_dfs.s2["reads_raw"] = 0 sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 ## parse new values from cutadapt results output lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value ## save to stats summary if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , 0 ) ] ## write the long form output to the log file. LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . _spacer , sample . name ) )
9712	def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : # if item >= heap[0], it will be popped immediately after pushed item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
13346	def run ( * args , * * kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , * * kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
4588	def show_image ( setter , width , height , image_path = '' , image_obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color_scale ( bgcolor , brightness ) img = image_obj if image_path and not img : from PIL import Image img = Image . open ( image_path ) elif not img : raise ValueError ( 'Must provide either image_path or image_obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color_scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color_scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
9998	def cellsiter_to_dataframe ( cellsiter , args , drop_allna = True ) : from modelx . core . cells import shareable_parameters if len ( args ) : indexes = shareable_parameters ( cellsiter ) else : indexes = get_all_params ( cellsiter . values ( ) ) result = None for cells in cellsiter . values ( ) : df = cells_to_dataframe ( cells , args ) if drop_allna and df . isnull ( ) . all ( ) . all ( ) : continue # Ignore all NA or empty if df . index . names != [ None ] : if isinstance ( df . index , pd . MultiIndex ) : if _pd_ver < ( 0 , 20 ) : df = _reset_naindex ( df ) df = df . reset_index ( ) missing_params = set ( indexes ) - set ( df ) for params in missing_params : df [ params ] = np . nan if result is None : result = df else : try : result = pd . merge ( result , df , how = "outer" ) except MergeError : # When no common column exists, i.e. all cells are scalars. result = pd . concat ( [ result , df ] , axis = 1 ) except ValueError : # When common columns are not coercible (numeric vs object), # Make the numeric column object type cols = set ( result . columns ) & set ( df . columns ) for col in cols : # When only either of them has object dtype if ( len ( [ str ( frame [ col ] . dtype ) for frame in ( result , df ) if str ( frame [ col ] . dtype ) == "object" ] ) == 1 ) : if str ( result [ col ] . dtype ) == "object" : frame = df else : frame = result frame [ [ col ] ] = frame [ col ] . astype ( "object" ) # Try again result = pd . merge ( result , df , how = "outer" ) if result is None : return pd . DataFrame ( ) else : return result . set_index ( indexes ) if indexes else result
4420	async def stop ( self ) : await self . _lavalink . ws . send ( op = 'stop' , guildId = self . guild_id ) self . current = None
11838	def result ( self , state , row ) : col = state . index ( None ) new = state [ : ] new [ col ] = row return new
7655	def update ( self , * * kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value )
5795	def cf_dictionary_to_dict ( dictionary ) : dict_length = CoreFoundation . CFDictionaryGetCount ( dictionary ) keys = ( CFTypeRef * dict_length ) ( ) values = ( CFTypeRef * dict_length ) ( ) CoreFoundation . CFDictionaryGetKeysAndValues ( dictionary , _cast_pointer_p ( keys ) , _cast_pointer_p ( values ) ) output = { } for index in range ( 0 , dict_length ) : output [ CFHelpers . native ( keys [ index ] ) ] = CFHelpers . native ( values [ index ] ) return output
489	def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
1167	def _dump_registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . __module__ , cls . __name__ ) print >> file , "Inv.counter: %s" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( "_abc_" ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
11512	def share_item ( self , token , item_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.share' , parameters ) return response
6460	def _ends_in_doubled_cons ( self , term ) : return ( len ( term ) > 1 and term [ - 1 ] not in self . _vowels and term [ - 2 ] == term [ - 1 ] )
2023	def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
9351	def number ( type = None , length = None , prefixes = None ) : # select credit card type if type and type in CARDS : card = type else : card = random . choice ( list ( CARDS . keys ( ) ) ) # select a credit card number's prefix if not prefixes : prefixes = CARDS [ card ] [ 'prefixes' ] prefix = random . choice ( prefixes ) # select length of the credit card number, if it's not set if not length : length = CARDS [ card ] [ 'length' ] # generate all digits but the last one result = str ( prefix ) for d in range ( length - len ( str ( prefix ) ) ) : result += str ( basic . number ( ) ) last_digit = check_digit ( int ( result ) ) return int ( result [ : - 1 ] + str ( last_digit ) )
7683	def display_multi ( annotations , fig_kw = None , meta = True , * * kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) # Filter down to coercable annotations first display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break # If there are no displayable annotations, fail here if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , * * fig_kw ) # MPL is stupid when making singleton subplots. # We catch this and make it always iterable. if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , * * kwargs ) return fig , axs
4998	def delete_enterprise_learner_role_assignment ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) try : SystemWideEnterpriseUserRoleAssignment . objects . get ( user = instance . user , role = enterprise_learner_role ) . delete ( ) except SystemWideEnterpriseUserRoleAssignment . DoesNotExist : # Do nothing if no role assignment is present for the enterprise customer user. pass
9042	def eigh ( self ) : from numpy . linalg import svd if self . _cache [ "eig" ] is not None : return self . _cache [ "eig" ] U , S = svd ( self . L ) [ : 2 ] S *= S S += self . _epsilon self . _cache [ "eig" ] = S , U return self . _cache [ "eig" ]
8080	def relmoveto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relmoveto ( x , y )
703	def _getStreamDef ( self , modelDescription ) : #-------------------------------------------------------------------------- # Generate the string containing the aggregation settings. aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } # Honor any overrides provided in the stream definition aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) # Do we have any aggregation at all? hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break # Convert the aggFunctionsDict to a list aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef
6574	def formatter ( self , api_client , data , newval ) : if newval is None : return None user_param = data [ '_paramAdditionalUrls' ] urls = { } if isinstance ( newval , str ) : urls [ user_param [ 0 ] ] = newval else : for key , url in zip ( user_param , newval ) : urls [ key ] = url return urls
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
4423	async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
7681	def piano_roll ( annotation , * * kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , * * kwargs )
11796	def min_conflicts_value ( csp , var , current ) : return argmin_random_tie ( csp . domains [ var ] , lambda val : csp . nconflicts ( var , val , current ) )
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
12470	def copy_w_plus ( src , dst ) : dst_ext = get_extension ( dst ) dst_pre = remove_ext ( dst ) while op . exists ( dst_pre + dst_ext ) : dst_pre += '+' shutil . copy ( src , dst_pre + dst_ext ) return dst_pre + dst_ext
2563	def pull_tasks ( self , kill_event ) : logger . info ( "[TASK PULL THREAD] starting" ) poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) # Send a registration message msg = self . create_reg_message ( ) logger . debug ( "Sending registration message: {}" . format ( msg ) ) self . task_incoming . send ( msg ) last_beat = time . time ( ) last_interchange_contact = time . time ( ) task_recv_counter = 0 poll_timer = 1 while not kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) ready_worker_count = self . ready_worker_queue . qsize ( ) pending_task_count = self . pending_task_queue . qsize ( ) logger . debug ( "[TASK_PULL_THREAD] ready workers:{}, pending tasks:{}" . format ( ready_worker_count , pending_task_count ) ) if time . time ( ) > last_beat + self . heartbeat_period : self . heartbeat ( ) last_beat = time . time ( ) if pending_task_count < self . max_queue_size and ready_worker_count > 0 : logger . debug ( "[TASK_PULL_THREAD] Requesting tasks: {}" . format ( ready_worker_count ) ) msg = ( ( ready_worker_count ) . to_bytes ( 4 , "little" ) ) self . task_incoming . send ( msg ) socks = dict ( poller . poll ( timeout = poll_timer ) ) if self . task_incoming in socks and socks [ self . task_incoming ] == zmq . POLLIN : _ , pkl_msg = self . task_incoming . recv_multipart ( ) tasks = pickle . loads ( pkl_msg ) last_interchange_contact = time . time ( ) if tasks == 'STOP' : logger . critical ( "[TASK_PULL_THREAD] Received stop request" ) kill_event . set ( ) break elif tasks == HEARTBEAT_CODE : logger . debug ( "Got heartbeat from interchange" ) else : # Reset timer on receiving message poll_timer = 1 task_recv_counter += len ( tasks ) logger . debug ( "[TASK_PULL_THREAD] Got tasks: {} of {}" . format ( [ t [ 'task_id' ] for t in tasks ] , task_recv_counter ) ) for task in tasks : self . pending_task_queue . put ( task ) else : logger . debug ( "[TASK_PULL_THREAD] No incoming tasks" ) # Limit poll duration to heartbeat_period # heartbeat_period is in s vs poll_timer in ms poll_timer = min ( self . heartbeat_period * 1000 , poll_timer * 2 ) # Only check if no messages were received. if time . time ( ) > last_interchange_contact + self . heartbeat_threshold : logger . critical ( "[TASK_PULL_THREAD] Missing contact with interchange beyond heartbeat_threshold" ) kill_event . set ( ) logger . critical ( "[TASK_PULL_THREAD] Exiting" ) break
4195	def plot_time_freq ( self , mindB = - 100 , maxdB = None , norm = True , yaxis_label_position = "right" ) : from pylab import subplot , gca subplot ( 1 , 2 , 1 ) self . plot_window ( ) subplot ( 1 , 2 , 2 ) self . plot_frequencies ( mindB = mindB , maxdB = maxdB , norm = norm ) if yaxis_label_position == "left" : try : tight_layout ( ) except : pass else : ax = gca ( ) ax . yaxis . set_label_position ( "right" )
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
1000	def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
13866	def fromtsms ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts / 1000 ) . replace ( microsecond = ts % 1000 * 1000 ) when = when . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
11470	def get_filesize ( self , filename ) : result = [ ] def dir_callback ( val ) : result . append ( val . split ( ) [ 4 ] ) self . _ftp . dir ( filename , dir_callback ) return result [ 0 ]
7626	def transcription ( ref , est , * * kwargs ) : namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , * * kwargs )
10080	def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
11970	def _wildcard_to_dec ( nm , check = False ) : if check and not is_wildcard_nm ( nm ) : raise ValueError ( '_wildcard_to_dec: invalid netmask: "%s"' % nm ) return 0xFFFFFFFF - _dot_to_dec ( nm , check = False )
4529	def set_device_brightness ( self , val ) : # bitshift to scale from 8 bit to 5 self . _chipset_brightness = ( val >> 3 ) self . _brightness_list = [ 0xE0 + self . _chipset_brightness ] * self . numLEDs self . _packet [ self . _start_frame : self . _pixel_stop : 4 ] = ( self . _brightness_list )
6382	def sim_hamming ( src , tar , diff_lens = True ) : return Hamming ( ) . sim ( src , tar , diff_lens )
8696	def set_timeout ( self , timeout ) : timeout = int ( timeout ) # will raise on Error self . _timeout = timeout == 0 and 999999 or timeout
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
2101	def log ( s , header = '' , file = sys . stderr , nl = 1 , * * kwargs ) : # Sanity check: If we are not in verbose mode, this is a no-op. if not settings . verbose : return # Construct multi-line string to stderr if header is provided. if header : word_arr = s . split ( ' ' ) multi = [ ] word_arr . insert ( 0 , '%s:' % header . upper ( ) ) i = 0 while i < len ( word_arr ) : to_add = [ '***' ] count = 3 while count <= 79 : count += len ( word_arr [ i ] ) + 1 if count <= 79 : to_add . append ( word_arr [ i ] ) i += 1 if i == len ( word_arr ) : break # Handle corner case of extra-long word longer than 75 characters. if len ( to_add ) == 1 : to_add . append ( word_arr [ i ] ) i += 1 if i != len ( word_arr ) : count -= len ( word_arr [ i ] ) + 1 to_add . append ( '*' * ( 78 - count ) ) multi . append ( ' ' . join ( to_add ) ) s = '\n' . join ( multi ) lines = len ( multi ) else : lines = 1 # If `nl` is an int greater than the number of rows of a message, # add the appropriate newlines to the output. if isinstance ( nl , int ) and nl > lines : s += '\n' * ( nl - lines ) # Output to stderr. return secho ( s , file = file , * * kwargs )
7657	def append ( self , time = None , duration = None , value = None , confidence = None ) : self . data . add ( Observation ( time = float ( time ) , duration = float ( duration ) , value = value , confidence = confidence ) )
4936	def strfdelta ( tdelta , fmt = '{D:02}d {H:02}h {M:02}m {S:02}s' , input_type = 'timedelta' ) : # Convert tdelta to integer seconds. if input_type == 'timedelta' : remainder = int ( tdelta . total_seconds ( ) ) elif input_type in [ 's' , 'seconds' ] : remainder = int ( tdelta ) elif input_type in [ 'm' , 'minutes' ] : remainder = int ( tdelta ) * 60 elif input_type in [ 'h' , 'hours' ] : remainder = int ( tdelta ) * 3600 elif input_type in [ 'd' , 'days' ] : remainder = int ( tdelta ) * 86400 elif input_type in [ 'w' , 'weeks' ] : remainder = int ( tdelta ) * 604800 else : raise ValueError ( 'input_type is not valid. Valid input_type strings are: "timedelta", "s", "m", "h", "d", "w"' ) f = Formatter ( ) desired_fields = [ field_tuple [ 1 ] for field_tuple in f . parse ( fmt ) ] possible_fields = ( 'W' , 'D' , 'H' , 'M' , 'S' ) constants = { 'W' : 604800 , 'D' : 86400 , 'H' : 3600 , 'M' : 60 , 'S' : 1 } values = { } for field in possible_fields : if field in desired_fields and field in constants : values [ field ] , remainder = divmod ( remainder , constants [ field ] ) return f . format ( fmt , * * values )
6829	def pull ( self , path , use_sudo = False , user = None , force = False ) : if path is None : raise ValueError ( "Path to the working copy is needed to pull from a remote repository." ) options = [ ] if force : options . append ( '--force' ) options = ' ' . join ( options ) cmd = 'git pull %s' % options with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
6126	def norm_and_check ( source_tree , requested ) : if os . path . isabs ( requested ) : raise ValueError ( "paths must be relative" ) abs_source = os . path . abspath ( source_tree ) abs_requested = os . path . normpath ( os . path . join ( abs_source , requested ) ) # We have to use commonprefix for Python 2.7 compatibility. So we # normalise case to avoid problems because commonprefix is a character # based comparison :-( norm_source = os . path . normcase ( abs_source ) norm_requested = os . path . normcase ( abs_requested ) if os . path . commonprefix ( [ norm_source , norm_requested ] ) != norm_source : raise ValueError ( "paths must be inside source tree" ) return abs_requested
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
5211	def bds ( tickers , flds , * * kwargs ) : logger = logs . get_logger ( bds , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( * * kwargs ) logger . info ( f'loading block data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . bulkref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for ( ticker , fld ) , grp in data . groupby ( [ 'ticker' , 'field' ] ) : data_file = storage . ref_file ( ticker = ticker , fld = fld , ext = 'pkl' , has_date = kwargs . get ( 'has_date' , True ) , * * kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( grp ) files . create_folder ( data_file , is_file = True ) grp . reset_index ( drop = True ) . to_pickle ( data_file ) return qry_data
1857	def BT ( cpu , dest , src ) : if dest . type == 'register' : cpu . CF = ( ( dest . read ( ) >> ( src . read ( ) % dest . size ) ) & 1 ) != 0 elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) value = cpu . read_int ( addr + base , 8 ) cpu . CF = Operators . EXTRACT ( value , pos , 1 ) == 1 else : raise NotImplementedError ( f"Unknown operand for BT: {dest.type}" )
7826	def payload_element_name ( element_name ) : def decorator ( klass ) : """The payload_element_name decorator.""" # pylint: disable-msg=W0212,W0404 from . stanzapayload import STANZA_PAYLOAD_CLASSES from . stanzapayload import STANZA_PAYLOAD_ELEMENTS if hasattr ( klass , "_pyxmpp_payload_element_name" ) : klass . _pyxmpp_payload_element_name . append ( element_name ) else : klass . _pyxmpp_payload_element_name = [ element_name ] if element_name in STANZA_PAYLOAD_CLASSES : logger = logging . getLogger ( 'pyxmpp.payload_element_name' ) logger . warning ( "Overriding payload class for {0!r}" . format ( element_name ) ) STANZA_PAYLOAD_CLASSES [ element_name ] = klass STANZA_PAYLOAD_ELEMENTS [ klass ] . append ( element_name ) return klass return decorator
10221	def preprocessing_br_projection_excel ( path : str ) -> pd . DataFrame : if not os . path . exists ( path ) : raise ValueError ( "Error: %s file not found" % path ) return pd . read_excel ( path , sheetname = 0 , header = 0 )
9296	def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
9466	def conference_record_stop ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStop/' method = 'POST' return self . request ( path , method , call_params )
3736	def Stockmayer ( Tm = None , Tb = None , Tc = None , Zc = None , omega = None , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and omega : methods . append ( TEEGOTOSTEWARD2 ) if Tc : methods . append ( FLYNN ) methods . append ( BSLC ) methods . append ( TEEGOTOSTEWARD1 ) if Tb : methods . append ( BSLB ) if Tm : methods . append ( BSLM ) if Tc and Zc : methods . append ( STIELTHODOS ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : epsilon = epsilon_Flynn ( Tc ) elif Method == BSLC : epsilon = epsilon_Bird_Stewart_Lightfoot_critical ( Tc ) elif Method == BSLB : epsilon = epsilon_Bird_Stewart_Lightfoot_boiling ( Tb ) elif Method == BSLM : epsilon = epsilon_Bird_Stewart_Lightfoot_melting ( Tm ) elif Method == STIELTHODOS : epsilon = epsilon_Stiel_Thodos ( Tc , Zc ) elif Method == TEEGOTOSTEWARD1 : epsilon = epsilon_Tee_Gotoh_Steward_1 ( Tc ) elif Method == TEEGOTOSTEWARD2 : epsilon = epsilon_Tee_Gotoh_Steward_2 ( Tc , omega ) elif Method == MAGALHAES : epsilon = float ( MagalhaesLJ_data . at [ CASRN , "epsilon" ] ) elif Method == NONE : epsilon = None else : raise Exception ( 'Failure in in function' ) return epsilon
5144	def _load ( self , config ) : if isinstance ( config , six . string_types ) : try : config = json . loads ( config ) except ValueError : pass if not isinstance ( config , dict ) : raise TypeError ( 'config block must be an istance ' 'of dict or a valid NetJSON string' ) return config
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
4646	def exists ( self ) : query = ( "SELECT name FROM sqlite_master " + "WHERE type='table' AND name=?" , ( self . __tablename__ , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
2320	def check_cuda_devices ( ) : import ctypes # Some constants taken from cuda.h CUDA_SUCCESS = 0 libnames = ( 'libcuda.so' , 'libcuda.dylib' , 'cuda.dll' ) for libname in libnames : try : cuda = ctypes . CDLL ( libname ) except OSError : continue else : break else : # raise OSError("could not load any of: " + ' '.join(libnames)) return 0 nGpus = ctypes . c_int ( ) error_str = ctypes . c_char_p ( ) result = cuda . cuInit ( 0 ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) # print("cuInit failed with error code %d: %s" % (result, error_str.value.decode())) return 0 result = cuda . cuDeviceGetCount ( ctypes . byref ( nGpus ) ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) # print("cuDeviceGetCount failed with error code %d: %s" % (result, error_str.value.decode())) return 0 # print("Found %d device(s)." % nGpus.value) return nGpus . value
1128	def SeqN ( n , * inner_rules , * * kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
4342	def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
3559	def find_service ( self , uuid ) : for service in self . list_services ( ) : if service . uuid == uuid : return service return None
11221	def get ( self , request , hash , filename ) : if _ws_download is True : return HttpResponseForbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return FileResponse ( upload . file , content_type = upload . type )
10359	def shuffle_relations ( graph : BELGraph , percentage : Optional [ str ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_edges ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) edges = result . edges ( keys = True ) for _ in range ( swaps ) : ( s1 , t1 , k1 ) , ( s2 , t2 , k2 ) = random . sample ( edges , 2 ) result [ s1 ] [ t1 ] [ k1 ] , result [ s2 ] [ t2 ] [ k2 ] = result [ s2 ] [ t2 ] [ k2 ] , result [ s1 ] [ t1 ] [ k1 ] return result
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
3271	def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
916	def debug ( self , msg , * args , * * kwargs ) : self . _baseLogger . debug ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
6707	def run_as_root ( command , * args , * * kwargs ) : from burlap . common import run_or_dryrun , sudo_or_dryrun if env . user == 'root' : func = run_or_dryrun else : func = sudo_or_dryrun return func ( command , * args , * * kwargs )
7992	def _send_stream_error ( self , condition ) : if self . _output_state is "closed" : return if self . _output_state in ( None , "restart" ) : self . _send_stream_start ( ) element = StreamErrorElement ( condition ) . as_xml ( ) self . transport . send_element ( element ) self . transport . disconnect ( ) self . _output_state = "closed"
7825	def feature_uri ( uri ) : def decorator ( class_ ) : """Returns a decorated class""" if "_pyxmpp_feature_uris" not in class_ . __dict__ : class_ . _pyxmpp_feature_uris = set ( ) class_ . _pyxmpp_feature_uris . add ( uri ) return class_ return decorator
12443	def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : # The specified method is not allowed for the resource # identified by the request URI. # RFC 2616  10.4.6  405 Method Not Allowed raise http . exceptions . MethodNotAllowed ( allowed )
9020	def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) # TODO: test all kinds of connections number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
6132	def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
13824	def ToJsonString ( self ) : if self . seconds < 0 or self . nanos < 0 : result = '-' seconds = - self . seconds + int ( ( 0 - self . nanos ) // 1e9 ) nanos = ( 0 - self . nanos ) % 1e9 else : result = '' seconds = self . seconds + int ( self . nanos // 1e9 ) nanos = self . nanos % 1e9 result += '%d' % seconds if ( nanos % 1e9 ) == 0 : # If there are 0 fractional digits, the fractional # point '.' should be omitted when serializing. return result + 's' if ( nanos % 1e6 ) == 0 : # Serialize 3 fractional digits. return result + '.%03ds' % ( nanos / 1e6 ) if ( nanos % 1e3 ) == 0 : # Serialize 6 fractional digits. return result + '.%06ds' % ( nanos / 1e3 ) # Serialize 9 fractional digits. return result + '.%09ds' % nanos
4118	def _swapsides ( data ) : N = len ( data ) return np . concatenate ( ( data [ N // 2 + 1 : ] , data [ 0 : N // 2 ] ) )
7281	def render_to_response ( self , context , * * response_kwargs ) : if self . request . is_ajax ( ) : template = self . page_template else : template = self . get_template_names ( ) return self . response_class ( request = self . request , template = template , context = context , * * response_kwargs )
2288	def forward ( self ) : self . noise . data . normal_ ( ) if not self . confounding : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . corr_noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i_adj_matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
6882	def describe_lcc_csv ( lcdict , returndesc = False ) : metadata_lines = [ ] coldef_lines = [ ] if 'lcformat' in lcdict and 'lcc-csv' in lcdict [ 'lcformat' ] . lower ( ) : metadata = lcdict [ 'metadata' ] metakeys = lcdict [ 'objectinfo' ] . keys ( ) coldefs = lcdict [ 'coldefs' ] for mk in metakeys : metadata_lines . append ( '%20s | %s' % ( mk , metadata [ mk ] [ 'desc' ] ) ) for ck in lcdict [ 'columns' ] : coldef_lines . append ( 'column %02d | %8s | numpy dtype: %3s | %s' % ( coldefs [ ck ] [ 'colnum' ] , ck , coldefs [ ck ] [ 'dtype' ] , coldefs [ ck ] [ 'desc' ] ) ) desc = LCC_CSVLC_DESCTEMPLATE . format ( objectid = lcdict [ 'objectid' ] , metadata_desc = '\n' . join ( metadata_lines ) , metadata = pformat ( lcdict [ 'objectinfo' ] ) , columndefs = '\n' . join ( coldef_lines ) ) print ( desc ) if returndesc : return desc else : LOGERROR ( "this lcdict is not from an LCC CSV, can't figure it out..." ) return None
505	def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector
2580	def checkpoint ( self , tasks = None ) : with self . checkpoint_lock : checkpoint_queue = None if tasks : checkpoint_queue = tasks else : checkpoint_queue = self . tasks checkpoint_dir = '{0}/checkpoint' . format ( self . run_dir ) checkpoint_dfk = checkpoint_dir + '/dfk.pkl' checkpoint_tasks = checkpoint_dir + '/tasks.pkl' if not os . path . exists ( checkpoint_dir ) : try : os . makedirs ( checkpoint_dir ) except FileExistsError : pass with open ( checkpoint_dfk , 'wb' ) as f : state = { 'rundir' : self . run_dir , 'task_count' : self . task_count } pickle . dump ( state , f ) count = 0 with open ( checkpoint_tasks , 'ab' ) as f : for task_id in checkpoint_queue : if not self . tasks [ task_id ] [ 'checkpoint' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None : hashsum = self . tasks [ task_id ] [ 'hashsum' ] if not hashsum : continue t = { 'hash' : hashsum , 'exception' : None , 'result' : None } try : # Asking for the result will raise an exception if # the app had failed. Should we even checkpoint these? # TODO : Resolve this question ? r = self . memoizer . hash_lookup ( hashsum ) . result ( ) except Exception as e : t [ 'exception' ] = e else : t [ 'result' ] = r # We are using pickle here since pickle dumps to a file in 'ab' # mode behave like a incremental log. pickle . dump ( t , f ) count += 1 self . tasks [ task_id ] [ 'checkpoint' ] = True logger . debug ( "Task {} checkpointed" . format ( task_id ) ) self . checkpointed_tasks += count if count == 0 : if self . checkpointed_tasks == 0 : logger . warn ( "No tasks checkpointed so far in this run. Please ensure caching is enabled" ) else : logger . debug ( "No tasks checkpointed in this pass." ) else : logger . info ( "Done checkpointing {} tasks" . format ( count ) ) return checkpoint_dir
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) # Update origData with the new data if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) # Update origData with the new data obj . _origData [ thisField ] = newValue
664	def makeCloneMap ( columnsShape , outputCloningWidth , outputCloningHeight = - 1 ) : if outputCloningHeight < 0 : outputCloningHeight = outputCloningWidth columnsHeight , columnsWidth = columnsShape numDistinctMasters = outputCloningWidth * outputCloningHeight a = numpy . empty ( ( columnsHeight , columnsWidth ) , 'uint32' ) for row in xrange ( columnsHeight ) : for col in xrange ( columnsWidth ) : a [ row , col ] = ( col % outputCloningWidth ) + ( row % outputCloningHeight ) * outputCloningWidth return a , numDistinctMasters
410	def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : input_shape = inputs . get_shape ( ) batch_size = tf . shape ( inputs ) [ 0 ] kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) input_h = input_shape [ 1 ] input_w = input_shape [ 2 ] channel = input_shape [ 3 ] # inputs (b, h, w, c) --> (b*c, h, w) inputs = self . _to_bc_h_w ( inputs , input_shape ) # offsets (b, h, w, 2*n) --> (b, h, w, n, 2) offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) # offsets (b, h, w, n, 2) --> (b*c, h, w, n, 2) # offsets = tf.tile(offsets, [channel, 1, 1, 1, 1]) coords = tf . expand_dims ( grid_offset , 0 ) # grid_offset --> (1, h, w, n, 2) coords = tf . tile ( coords , [ batch_size , 1 , 1 , 1 , 1 ] ) + offsets # grid_offset --> (b, h, w, n, 2) # clip out of bound coords = tf . stack ( [ tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) ] , axis = - 1 ) coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) # (b*c, h, w, n) --> (b, h, w, n, c) mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) return mapped_vals
7509	def _save ( self ) : ## save each attribute as dict fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . __dict__ fulldumps = json . dumps ( fulldict , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file, make dir if it wasn't made earlier assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
9252	def generate_unreleased_section ( self ) : if not self . filtered_tags : return "" now = datetime . datetime . utcnow ( ) now = now . replace ( tzinfo = dateutil . tz . tzutc ( ) ) head_tag = { "name" : self . options . unreleased_label } self . tag_times_dict [ head_tag [ "name" ] ] = now unreleased_log = self . generate_log_between_tags ( self . filtered_tags [ 0 ] , head_tag ) return unreleased_log
3747	def calculate ( self , T , P , zs , ws , method ) : if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
8666	def init_stash ( stash_path , passphrase , passphrase_size , backend ) : stash_path = stash_path or STORAGE_DEFAULT_PATH_MAPPING [ backend ] click . echo ( 'Stash: {0} at {1}' . format ( backend , stash_path ) ) storage = STORAGE_MAPPING [ backend ] ( * * _parse_path_string ( stash_path ) ) try : click . echo ( 'Initializing stash...' ) if os . path . isfile ( PASSPHRASE_FILENAME ) : raise GhostError ( '{0} already exists. Overwriting might prevent you ' 'from accessing the stash it was generated for. ' 'Please make sure to save and remove the file before ' 'initializing another stash.' . format ( PASSPHRASE_FILENAME ) ) stash = Stash ( storage , passphrase = passphrase , passphrase_size = passphrase_size ) passphrase = stash . init ( ) if not passphrase : click . echo ( 'Stash already initialized.' ) sys . exit ( 0 ) _write_passphrase_file ( passphrase ) except GhostError as ex : sys . exit ( ex ) except ( OSError , IOError ) as ex : click . echo ( "Seems like we've run into a problem." ) file_path = _parse_path_string ( stash_path ) [ 'db_path' ] click . echo ( 'Removing stale stash and passphrase: {0}. Note that any ' 'directories created are not removed for safety reasons and you ' 'might want to remove them manually.' . format ( file_path ) ) if os . path . isfile ( file_path ) : os . remove ( file_path ) sys . exit ( ex ) click . echo ( 'Initialized stash at: {0}' . format ( stash_path ) ) click . echo ( 'Your passphrase can be found under the `{0}` file in the ' 'current directory.' . format ( PASSPHRASE_FILENAME ) ) click . echo ( 'Make sure you save your passphrase somewhere safe. ' 'If lost, you will lose access to your stash.' )
5159	def _add_tc_script ( self ) : # fill context context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) # import pdb; pdb.set_trace() contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add tc_script.sh to list of included files self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
10928	def _run2 ( self ) : if self . check_update_J ( ) : self . update_J ( ) else : if self . check_Broyden_J ( ) : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) : self . update_eig_J ( ) #0. Find _last_residuals, _last_error, etc: _last_residuals = self . calc_residuals ( ) . copy ( ) _last_error = 1 * self . error _last_vals = self . param_vals . copy ( ) #1. Calculate 2 possible steps delta_params_1 = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False ) self . decrease_damping ( ) delta_params_2 = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False ) self . decrease_damping ( undo_decrease = True ) #2. Check which step is best: er1 = self . update_function ( self . param_vals + delta_params_1 ) er2 = self . update_function ( self . param_vals + delta_params_2 ) triplet = ( self . error , er1 , er2 ) best_step = find_best_step ( triplet ) if best_step == 0 : #Both bad steps, put back & increase damping: _ = self . update_function ( self . param_vals . copy ( ) ) grad = self . calc_grad ( ) CLOG . debug ( 'Bad step, increasing damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) for _try in range ( self . _max_inner_loop ) : self . increase_damping ( ) delta_vals = self . find_LM_updates ( grad ) er_new = self . update_function ( self . param_vals + delta_vals ) good_step = er_new < self . error if good_step : #Update params, error, break: self . update_param_vals ( delta_vals , incremental = True ) self . error = er_new CLOG . debug ( 'Sufficiently increased damping' ) CLOG . debug ( '%f\t%f' % ( triplet [ 0 ] , self . error ) ) break else : #for-break-else #Throw a warning, put back the parameters CLOG . warn ( 'Stuck!' ) self . error = self . update_function ( self . param_vals . copy ( ) ) elif best_step == 1 : #er1 <= er2: good_step = True CLOG . debug ( 'Good step, same damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) #Update to er1 params: er1_1 = self . update_function ( self . param_vals + delta_params_1 ) if np . abs ( er1_1 - er1 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . update_param_vals ( delta_params_1 , incremental = True ) self . error = er1 elif best_step == 2 : #er2 < er1: good_step = True self . error = er2 CLOG . debug ( 'Good step, decreasing damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) #-we're already at the correct parameters self . update_param_vals ( delta_params_2 , incremental = True ) self . decrease_damping ( ) #3. Run with current J, damping; update what we need to:: if good_step : self . _last_residuals = _last_residuals self . _last_error = _last_error self . _last_vals = _last_vals self . error self . do_internal_run ( initial_count = 1 )
12228	def register_admin_models ( admin_site ) : global __MODELS_REGISTRY prefs = get_prefs ( ) for app_label , prefs_items in prefs . items ( ) : model_class = get_pref_model_class ( app_label , prefs_items , get_app_prefs ) if model_class is not None : __MODELS_REGISTRY [ app_label ] = model_class admin_site . register ( model_class , get_pref_model_admin_class ( prefs_items ) )
6876	def _gzip_sqlitecurve ( sqlitecurve , force = False ) : # -k to keep the input file just in case something explodes if force : cmd = 'gzip -k -f %s' % sqlitecurve else : cmd = 'gzip -k %s' % sqlitecurve try : outfile = '%s.gz' % sqlitecurve if os . path . exists ( outfile ) and not force : # get rid of the .sqlite file only os . remove ( sqlitecurve ) return outfile else : subprocess . check_output ( cmd , shell = True ) # check if the output file was successfully created if os . path . exists ( outfile ) : return outfile else : return None except subprocess . CalledProcessError : return None
10874	def get_hsym_asym ( rho , z , get_hdet = False , include_K3_det = True , * * kwargs ) : K1 , Kprefactor = get_K ( rho , z , K = 1 , get_hdet = get_hdet , Kprefactor = None , return_Kprefactor = True , * * kwargs ) K2 = get_K ( rho , z , K = 2 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , * * kwargs ) if get_hdet and not include_K3_det : K3 = 0 * K1 else : K3 = get_K ( rho , z , K = 3 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , * * kwargs ) hsym = K1 * K1 . conj ( ) + K2 * K2 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) hasym = K1 * K2 . conj ( ) + K2 * K1 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) return hsym . real , hasym . real
11657	def fit ( self , X , y = None ) : X = check_array ( X , copy = self . copy , dtype = [ np . float64 , np . float32 , np . float16 , np . float128 ] ) feature_range = self . feature_range if feature_range [ 0 ] >= feature_range [ 1 ] : raise ValueError ( "Minimum of desired feature range must be smaller" " than maximum. Got %s." % str ( feature_range ) ) if self . fit_feature_range is not None : fit_feature_range = self . fit_feature_range if fit_feature_range [ 0 ] >= fit_feature_range [ 1 ] : raise ValueError ( "Minimum of desired (fit) feature range must " "be smaller than maximum. Got %s." % str ( feature_range ) ) if ( fit_feature_range [ 0 ] < feature_range [ 0 ] or fit_feature_range [ 1 ] > feature_range [ 1 ] ) : raise ValueError ( "fit_feature_range must be a subset of " "feature_range. Got %s, fit %s." % ( str ( feature_range ) , str ( fit_feature_range ) ) ) feature_range = fit_feature_range data_min = np . min ( X , axis = 0 ) data_range = np . max ( X , axis = 0 ) - data_min # Do not scale constant features data_range [ data_range == 0.0 ] = 1.0 self . scale_ = ( feature_range [ 1 ] - feature_range [ 0 ] ) / data_range self . min_ = feature_range [ 0 ] - data_min * self . scale_ self . data_range = data_range self . data_min = data_min return self
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
9151	def _convert_coordinatelist ( input_obj ) : cdl = pgmagick . CoordinateList ( ) for obj in input_obj : cdl . append ( pgmagick . Coordinate ( obj [ 0 ] , obj [ 1 ] ) ) return cdl
13045	def f_i18n_iso ( isocode , lang = "eng" ) : if lang not in flask_nemo . _data . AVAILABLE_TRANSLATIONS : lang = "eng" try : return flask_nemo . _data . ISOCODES [ isocode ] [ lang ] except KeyError : return "Unknown"
501	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } # Only use points before record to classify and after the wait period. classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
12679	def can_send ( self , user , notice_type ) : from notification . models import NoticeSetting return NoticeSetting . for_user ( user , notice_type , self . medium_id ) . send
9211	def cli ( url , user_agent ) : kwargs = { } if user_agent : kwargs [ 'user_agent' ] = user_agent archive_url = capture ( url , * * kwargs ) click . echo ( archive_url )
11457	def match ( self , query = None , * * kwargs ) : from invenio . search_engine import perform_request_search if not query : # We use default setup recid = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = "035:%s" % ( recid , ) , of = "id" ) else : if "recid" not in kwargs : kwargs [ "recid" ] = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = query % kwargs , of = "id" )
775	def _abbreviate ( text , threshold ) : if text is not None and len ( text ) > threshold : text = text [ : threshold ] + "..." return text
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
4720	def tcase_enter ( trun , tsuite , tcase ) : #pylint: disable=locally-disabled, unused-argument if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:enter" ) cij . emph ( "rnr:tcase:enter { fname: %r }" % tcase [ "fname" ] ) cij . emph ( "rnr:tcase:enter { log_fpath: %r }" % tcase [ "log_fpath" ] ) rcode = 0 for hook in tcase [ "hooks" ] [ "enter" ] : # tcase ENTER-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:exit: { rcode: %r }" % rcode , rcode ) return rcode
13729	def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
12456	def install ( env , requirements , args , ignore_activated = False , install_dev_requirements = False , quiet = False ) : if os . path . isfile ( requirements ) : args += ( '-r' , requirements ) label = 'project' else : args += ( '-U' , '-e' , '.' ) label = 'library' # Attempt to install development requirements if install_dev_requirements : dev_requirements = None dirname = os . path . dirname ( requirements ) basename , ext = os . path . splitext ( os . path . basename ( requirements ) ) # Possible dev requirements files: # # * <requirements>-dev.<ext> # * dev-<requirements>.<ext> # * <requirements>_dev.<ext> # * dev_<requirements>.<ext> # * <requirements>dev.<ext> # * dev<requirements>.<ext> # # Where <requirements> is basename of given requirements file to use # and <ext> is its extension. for delimiter in ( '-' , '_' , '' ) : filename = os . path . join ( dirname , '' . join ( ( basename , delimiter , 'dev' , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break filename = os . path . join ( dirname , '' . join ( ( 'dev' , delimiter , basename , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break # If at least one dev requirements file found, install dev requirements if dev_requirements : args += ( '-r' , dev_requirements ) if not quiet : print_message ( '== Step 2. Install {0} ==' . format ( label ) ) result = not pip_cmd ( env , ( 'install' , ) + args , ignore_activated , echo = not quiet ) if not quiet : print_message ( ) return result
9726	async def send_xml ( self , xml ) : return await asyncio . wait_for ( self . _protocol . send_command ( xml , command_type = QRTPacketType . PacketXML ) , timeout = self . _timeout , )
11917	def render ( template , * * data ) : try : return renderer . render ( template , * * data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
2057	def TBH ( cpu , dest ) : # Capstone merges the two registers values into one operand, so we need to extract them back # Specifies the base register. This contains the address of the table of branch lengths. This # register is allowed to be the PC. If it is, the table immediately follows this instruction. base_addr = dest . get_mem_base_addr ( ) if dest . mem . base in ( 'PC' , 'R15' ) : base_addr = cpu . PC # Specifies the index register. This contains an integer pointing to a halfword within the table. # The offset within the table is twice the value of the index. offset = cpu . read_int ( base_addr + dest . get_mem_offset ( ) , 16 ) offset = Operators . ZEXTEND ( offset , cpu . address_bit_size ) cpu . PC += ( offset << 1 )
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
3761	def draw_2d ( self , Hs = False ) : # pragma: no cover try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
4682	def getKeyType ( self , account , pub ) : for authority in [ "owner" , "active" ] : for key in account [ authority ] [ "key_auths" ] : if str ( pub ) == key [ 0 ] : return authority if str ( pub ) == account [ "options" ] [ "memo_key" ] : return "memo" return None
12827	def add_data ( self , data ) : if not self . _data : self . _data = { } self . _data . update ( data )
10112	def rewrite ( fname , visitor , * * kw ) : if not isinstance ( fname , pathlib . Path ) : assert isinstance ( fname , string_types ) fname = pathlib . Path ( fname ) assert fname . is_file ( ) with tempfile . NamedTemporaryFile ( delete = False ) as fp : tmp = pathlib . Path ( fp . name ) with UnicodeReader ( fname , * * kw ) as reader_ : with UnicodeWriter ( tmp , * * kw ) as writer : for i , row in enumerate ( reader_ ) : row = visitor ( i , row ) if row is not None : writer . writerow ( row ) shutil . move ( str ( tmp ) , str ( fname ) )
12096	def save ( self , * args , * * kwargs ) : current_activable_value = getattr ( self , self . ACTIVATABLE_FIELD_NAME ) is_active_changed = self . id is None or self . __original_activatable_value != current_activable_value self . __original_activatable_value = current_activable_value ret_val = super ( BaseActivatableModel , self ) . save ( * args , * * kwargs ) # Emit the signals for when the is_active flag is changed if is_active_changed : model_activations_changed . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) if self . activatable_field_updated : model_activations_updated . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) return ret_val
4636	def derive_from_seed ( self , offset ) : seed = int ( hexlify ( bytes ( self ) ) . decode ( "ascii" ) , 16 ) z = int ( hexlify ( offset ) . decode ( "ascii" ) , 16 ) order = ecdsa . SECP256k1 . order secexp = ( seed + z ) % order secret = "%0x" % secexp if len ( secret ) < 64 : # left-pad with zeroes secret = ( "0" * ( 64 - len ( secret ) ) ) + secret return PrivateKey ( secret , prefix = self . pubkey . prefix )
3988	def parallel_task_queue ( pool_size = multiprocessing . cpu_count ( ) ) : task_queue = TaskQueue ( pool_size ) yield task_queue task_queue . execute ( )
11008	def get_project_slug ( self , bet ) : if bet . get ( 'form_params' ) : params = json . loads ( bet [ 'form_params' ] ) return params . get ( 'project' ) return None
11882	def scanAllProcessesForMapping ( searchPortion , isExactMatch = False , ignoreCase = False ) : pids = getAllRunningPids ( ) # Since processes could disappear, we run the scan as fast as possible here with a list comprehension, then assemble the return dictionary later. mappingResults = [ scanProcessForMapping ( pid , searchPortion , isExactMatch , ignoreCase ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if mappingResults [ i ] is not None : ret [ pids [ i ] ] = mappingResults [ i ] return ret
6363	def dist ( self , src , tar ) : if src == tar : return 0.0 src_comp = self . _rle . encode ( self . _bwt . encode ( src ) ) tar_comp = self . _rle . encode ( self . _bwt . encode ( tar ) ) concat_comp = self . _rle . encode ( self . _bwt . encode ( src + tar ) ) concat_comp2 = self . _rle . encode ( self . _bwt . encode ( tar + src ) ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
4714	def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
11959	def _check_nm ( nm , notation ) : # Convert to decimal, and check if it's in the list of valid netmasks. _NM_CHECK_FUNCT = { NM_DOT : _dot_to_dec , NM_HEX : _hex_to_dec , NM_BIN : _bin_to_dec , NM_OCT : _oct_to_dec , NM_DEC : _dec_to_dec_long } try : dec = _NM_CHECK_FUNCT [ notation ] ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
4330	def gain ( self , gain_db = 0.0 , normalize = True , limiter = False , balance = None ) : if not is_number ( gain_db ) : raise ValueError ( "gain_db must be a number." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize must be a boolean." ) if not isinstance ( limiter , bool ) : raise ValueError ( "limiter must be a boolean." ) if balance not in [ None , 'e' , 'B' , 'b' ] : raise ValueError ( "balance must be one of None, 'e', 'B', or 'b'." ) effect_args = [ 'gain' ] if balance is not None : effect_args . append ( '-{}' . format ( balance ) ) if normalize : effect_args . append ( '-n' ) if limiter : effect_args . append ( '-l' ) effect_args . append ( '{:f}' . format ( gain_db ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'gain' ) return self
11870	def color_from_rgb ( red , green , blue ) : r = min ( red , 255 ) g = min ( green , 255 ) b = min ( blue , 255 ) if r > 1 or g > 1 or b > 1 : r = r / 255.0 g = g / 255.0 b = b / 255.0 return color_from_hls ( * rgb_to_hls ( r , g , b ) )
1581	def create_packet ( header , data ) : packet = IncomingPacket ( ) packet . header = header packet . data = data if len ( header ) == HeronProtocol . HEADER_SIZE : packet . is_header_read = True if len ( data ) == packet . get_datasize ( ) : packet . is_complete = True return packet
8031	def compareChunks ( handles , chunk_size = CHUNK_SIZE ) : chunks = [ ( path , fh , fh . read ( chunk_size ) ) for path , fh , _ in handles ] more , done = [ ] , [ ] # While there are combinations not yet tried... while chunks : # Compare the first chunk to all successive chunks matches , non_matches = [ chunks [ 0 ] ] , [ ] for chunk in chunks [ 1 : ] : if matches [ 0 ] [ 2 ] == chunk [ 2 ] : matches . append ( chunk ) else : non_matches . append ( chunk ) # Check for EOF or obviously unique files if len ( matches ) == 1 or matches [ 0 ] [ 2 ] == "" : for x in matches : x [ 1 ] . close ( ) done . append ( [ x [ 0 ] for x in matches ] ) else : more . append ( matches ) chunks = non_matches return more , done
10798	def _weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter_size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
267	def vectorize ( func ) : def wrapper ( df , * args , * * kwargs ) : if df . ndim == 1 : return func ( df , * args , * * kwargs ) elif df . ndim == 2 : return df . apply ( func , * args , * * kwargs ) return wrapper
3868	async def set_typing ( self , typing = hangouts_pb2 . TYPING_TYPE_STARTED ) : # TODO: Add rate-limiting to avoid unnecessary requests. try : await self . _client . set_typing ( hangouts_pb2 . SetTypingRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , type = typing , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set typing status: {}' . format ( e ) ) raise
1037	def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
6662	def generate_csr ( self , domain = '' , r = None ) : r = r or self . local_renderer r . env . domain = domain or r . env . domain role = self . genv . ROLE or ALL site = self . genv . SITE or self . genv . default_site print ( 'self.genv.default_site:' , self . genv . default_site , file = sys . stderr ) print ( 'site.csr0:' , site , file = sys . stderr ) ssl_dst = 'roles/%s/ssl' % ( role , ) print ( 'ssl_dst:' , ssl_dst ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) for site , site_data in self . iter_sites ( ) : print ( 'site.csr1:' , site , file = sys . stderr ) assert r . env . domain , 'No SSL domain defined.' r . env . ssl_base_dst = '%s/%s' % ( ssl_dst , r . env . domain . replace ( '*.' , '' ) ) r . env . ssl_csr_year = date . today ( ) . year r . local ( 'openssl req -nodes -newkey rsa:{ssl_length} ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.{ssl_csr_year}.key -out {ssl_base_dst}.{ssl_csr_year}.csr' )
10683	def S ( self , T ) : result = self . Sref for Tmax in sorted ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) : result += self . _Cp_records [ str ( Tmax ) ] . S ( T ) if T <= Tmax : return result + self . S_mag ( T ) # Extrapolate beyond the upper limit by using a constant heat capacity. Tmax = max ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) result += self . Cp ( Tmax ) * math . log ( T / Tmax ) return result + self . S_mag ( T )
10832	def delete ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls . query . filter ( cls . admin == admin , cls . group == group ) . one ( ) db . session . delete ( obj )
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
9435	def _read_a_packet ( file_h , hdrp , layers = 0 ) : raw_packet_header = file_h . read ( 16 ) if not raw_packet_header or len ( raw_packet_header ) != 16 : return None # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the packet header if hdrp [ 0 ] . byteorder == 'big' : packet_header = struct . unpack ( '>IIII' , raw_packet_header ) else : packet_header = struct . unpack ( '<IIII' , raw_packet_header ) ( timestamp , timestamp_us , capture_len , packet_len ) = packet_header raw_packet_data = file_h . read ( capture_len ) if not raw_packet_data or len ( raw_packet_data ) != capture_len : return None if layers > 0 : layers -= 1 raw_packet = linklayer . clookup ( hdrp [ 0 ] . ll_type ) ( raw_packet_data , layers = layers ) else : raw_packet = raw_packet_data packet = pcap_packet ( hdrp , timestamp , timestamp_us , capture_len , packet_len , raw_packet ) return packet
7082	def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphases = fourierparams # figure out the order from the length of the Fourier param list forder = len ( famps ) # phase the times with this period iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] # calculate all the individual terms of the series fseries = [ famps [ x ] * np . cos ( 2.0 * np . pi * x * phase + fphases [ x ] ) for x in range ( forder ) ] # this is the zeroth order coefficient - a constant equal to median mag modelmags = np . median ( mags ) # sum the series for fo in fseries : modelmags += fo return modelmags , phase , ptimes , pmags , perrs
13837	def ConsumeIdentifier ( self ) : result = self . token if not self . _IDENTIFIER . match ( result ) : raise self . _ParseError ( 'Expected identifier.' ) self . NextToken ( ) return result
11885	def connect ( self ) : try : self . _socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _socket . settimeout ( TIMEOUT_SECONDS ) self . _socket . connect ( ( self . _ip , self . _port ) ) _LOGGER . debug ( "Successfully created Hub at %s:%s :)" , self . _ip , self . _port ) except socket . error as error : _LOGGER . error ( "Error creating Hub: %s :(" , error ) self . _socket . close ( )
5446	def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
6853	def partitions ( device = "" ) : partitions_list = { } with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'sfdisk -d %(device)s' % locals ( ) ) spart = re . compile ( r'(?P<pname>^/.*) : .* Id=(?P<ptypeid>[0-9a-z]+)' ) for line in res . splitlines ( ) : m = spart . search ( line ) if m : partitions_list [ m . group ( 'pname' ) ] = int ( m . group ( 'ptypeid' ) , 16 ) return partitions_list
1195	def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue # For submodules, the parent packages are also deps. package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps
6656	def calibrateEB ( variances , sigma2 ) : if ( sigma2 <= 0 or min ( variances ) == max ( variances ) ) : return ( np . maximum ( variances , 0 ) ) sigma = np . sqrt ( sigma2 ) eb_prior = gfit ( variances , sigma ) # Set up a partial execution of the function part = functools . partial ( gbayes , g_est = eb_prior , sigma = sigma ) if len ( variances ) >= 200 : # Interpolate to speed up computations: calib_x = np . percentile ( variances , np . arange ( 0 , 102 , 2 ) ) calib_y = list ( map ( part , calib_x ) ) calib_all = np . interp ( variances , calib_x , calib_y ) else : calib_all = list ( map ( part , variances ) ) return np . asarray ( calib_all )
3466	def gene_name_reaction_rule ( self ) : names = { i . id : i . name for i in self . _genes } ast = parse_gpr ( self . _gene_reaction_rule ) [ 0 ] return ast2str ( ast , names = names )
3814	def _get_upload_session_status ( res ) : response = json . loads ( res . body . decode ( ) ) if 'sessionStatus' not in response : try : info = ( response [ 'errorMessage' ] [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) reason = '{} : {}' . format ( info [ 'status' ] , info [ 'message' ] ) except KeyError : reason = 'unknown reason' raise exceptions . NetworkError ( 'image upload failed: {}' . format ( reason ) ) return response [ 'sessionStatus' ]
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
6439	def euclidean ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Euclidean ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
2999	def sectorPerformanceDF ( token = '' , version = '' ) : df = pd . DataFrame ( sectorPerformance ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'name' ) return df
4976	def render_page_with_error_code_message ( request , context_data , error_code , log_message ) : LOGGER . error ( log_message ) messages . add_generic_error_message_with_code ( request , error_code ) return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , )
7766	def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
1042	def compare ( left , right , compare_locs = False ) : if type ( left ) != type ( right ) : return False if isinstance ( left , ast . AST ) : for field in left . _fields : if not compare ( getattr ( left , field ) , getattr ( right , field ) ) : return False if compare_locs : for loc in left . _locs : if getattr ( left , loc ) != getattr ( right , loc ) : return False return True elif isinstance ( left , list ) : if len ( left ) != len ( right ) : return False for left_elt , right_elt in zip ( left , right ) : if not compare ( left_elt , right_elt ) : return False return True else : return left == right
10063	def process_minter ( value ) : try : return current_pidstore . minters [ value ] except KeyError : raise click . BadParameter ( 'Unknown minter {0}. Please use one of {1}.' . format ( value , ', ' . join ( current_pidstore . minters . keys ( ) ) ) )
4031	def _decrypt ( self , value , encrypted_value ) : if sys . platform == 'win32' : return self . _decrypt_windows_chrome ( value , encrypted_value ) if value or ( encrypted_value [ : 3 ] != b'v10' ) : return value # Encrypted cookies should be prefixed with 'v10' according to the # Chromium code. Strip it off. encrypted_value = encrypted_value [ 3 : ] encrypted_value_half_len = int ( len ( encrypted_value ) / 2 ) cipher = pyaes . Decrypter ( pyaes . AESModeOfOperationCBC ( self . key , self . iv ) ) decrypted = cipher . feed ( encrypted_value [ : encrypted_value_half_len ] ) decrypted += cipher . feed ( encrypted_value [ encrypted_value_half_len : ] ) decrypted += cipher . feed ( ) return decrypted . decode ( "utf-8" )
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : # conditionally turn off autoescaping for .txt extensions in format if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
4945	def get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) : enterprise_customer = get_enterprise_customer ( enterprise_customer_uuid ) discovery_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) course_ids = discovery_client . get_program_course_keys ( program_uuid ) child_consents = ( get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = individual_course_id ) for individual_course_id in course_ids ) return ProxyDataSharingConsent . from_children ( program_uuid , * child_consents )
7526	def get_quick_depths ( data , sample ) : ## use existing sample cluster path if it exists, since this ## func can be used in step 4 and that can occur after merging ## assemblies after step3, and if we then referenced by data.dirs.clusts ## the path would be broken. ## ## If branching at step 3 to test different clust thresholds, the ## branched samples will retain the samples.files.clusters of the ## parent (which have the clust_threshold value of the parent), so ## it will look like nothing has changed. If we call this func ## from step 3 then it indicates we are in a branch and should ## reset the sample.files.clusters handle to point to the correct ## data.dirs.clusts directory. See issue #229. ## Easier to just always trust that samples.files.clusters is right, ## no matter what step? #if sample.files.clusters and not sample.stats.state == 3: # pass #else: # ## set cluster file handles sample . files . clusters = os . path . join ( data . dirs . clusts , sample . name + ".clustS.gz" ) ## get new clustered loci fclust = data . samples [ sample . name ] . files . clusters clusters = gzip . open ( fclust , 'r' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## storage depths = [ ] maxlen = [ ] ## start with cluster 0 tdepth = 0 tlen = 0 ## iterate until empty while 1 : ## grab next try : name , seq = pairdealer . next ( ) except StopIteration : break ## if not the end of a cluster #print name.strip(), seq.strip() if name . strip ( ) == seq . strip ( ) : depths . append ( tdepth ) maxlen . append ( tlen ) tlen = 0 tdepth = 0 else : tdepth += int ( name . split ( ";" ) [ - 2 ] [ 5 : ] ) tlen = len ( seq ) ## return clusters . close ( ) return np . array ( maxlen ) , np . array ( depths )
12107	def cross_check_launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root_directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root_directory = launcher . get_root_directory ( ) if os . path . isdir ( root_directory ) : raise Exception ( "Root directory already exists: %r" % root_directory ) if root_directory in root_directories : raise Exception ( "Each launcher requires a unique root directory" ) root_directories . append ( root_directory )
9037	def walk_connections ( self , mapping = identity ) : for start in self . walk_instructions ( ) : for stop_instruction in start . instruction . consuming_instructions : if stop_instruction is None : continue stop = self . _walk . instruction_in_grid ( stop_instruction ) connection = Connection ( start , stop ) if connection . is_visible ( ) : # print("connection:", # connection.start.instruction, # connection.stop.instruction) yield mapping ( connection )
1448	def parse ( version ) : match = _REGEX . match ( version ) if match is None : raise ValueError ( '%s is not valid SemVer string' % version ) verinfo = match . groupdict ( ) verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) return verinfo
10295	def get_undefined_namespaces ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) }
10140	def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
1511	def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
8289	def sbot_executable ( ) : gsettings = load_gsettings ( ) venv = gsettings . get_string ( 'current-virtualenv' ) if venv == 'Default' : sbot = which ( 'sbot' ) elif venv == 'System' : # find system python env_venv = os . environ . get ( 'VIRTUAL_ENV' ) if not env_venv : return which ( 'sbot' ) # First sbot in path that is not in current venv for p in os . environ [ 'PATH' ] . split ( os . path . pathsep ) : sbot = '%s/sbot' % p if not p . startswith ( env_venv ) and os . path . isfile ( sbot ) : return sbot else : sbot = os . path . join ( venv , 'bin/sbot' ) if not os . path . isfile ( sbot ) : print ( 'Shoebot not found, reverting to System shoebot' ) sbot = which ( 'sbot' ) return os . path . realpath ( sbot )
6386	def _sb_r1 ( self , term , r1_prefixes = None ) : vowel_found = False if hasattr ( r1_prefixes , '__iter__' ) : for prefix in r1_prefixes : if term [ : len ( prefix ) ] == prefix : return len ( prefix ) for i in range ( len ( term ) ) : if not vowel_found and term [ i ] in self . _vowels : vowel_found = True elif vowel_found and term [ i ] not in self . _vowels : return i + 1 return len ( term )
212	def from_uint8 ( arr_uint8 , shape , min_value = 0.0 , max_value = 1.0 ) : arr_0to1 = arr_uint8 . astype ( np . float32 ) / 255.0 return HeatmapsOnImage . from_0to1 ( arr_0to1 , shape , min_value = min_value , max_value = max_value )
1874	def MOVHPD ( cpu , dest , src ) : if src . size == 128 : assert dest . size == 64 dest . write ( Operators . EXTRACT ( src . read ( ) , 64 , 64 ) ) else : assert src . size == 64 and dest . size == 128 value = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) # low part dest . write ( Operators . CONCAT ( 128 , src . read ( ) , value ) )
10445	def getchild ( self , window_name , child_name = '' , role = '' , parent = '' ) : matches = [ ] if role : role = re . sub ( ' ' , '_' , role ) self . _windows = { } if parent and ( child_name or role ) : _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) obj = self . _get_object_map ( window_name , parent ) def _get_all_children_under_obj ( obj , child_list ) : if role and obj [ 'class' ] == role : child_list . append ( obj [ 'label' ] ) elif child_name and self . _match_name_to_appmap ( child_name , obj ) : child_list . append ( obj [ 'label' ] ) if obj : children = obj [ 'children' ] if not children : return child_list for child in children . split ( ) : return _get_all_children_under_obj ( appmap [ child ] , child_list ) matches = _get_all_children_under_obj ( obj , [ ] ) if not matches : if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) for name in appmap . keys ( ) : obj = appmap [ name ] # When only role arg is passed if role and not child_name and obj [ 'class' ] == role : matches . append ( name ) # When parent and child_name arg is passed if parent and child_name and not role and self . _match_name_to_appmap ( parent , obj ) : matches . append ( name ) # When only child_name arg is passed if child_name and not role and self . _match_name_to_appmap ( child_name , obj ) : return name matches . append ( name ) # When role and child_name args are passed if role and child_name and obj [ 'class' ] == role and self . _match_name_to_appmap ( child_name , obj ) : matches . append ( name ) if not matches : _name = '' _role = '' _parent = '' if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches
11620	def to_utf8 ( y ) : out = [ ] for x in y : if x < 0x080 : out . append ( x ) elif x < 0x0800 : out . append ( ( x >> 6 ) | 0xC0 ) out . append ( ( x & 0x3F ) | 0x80 ) elif x < 0x10000 : out . append ( ( x >> 12 ) | 0xE0 ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) else : out . append ( ( x >> 18 ) | 0xF0 ) out . append ( ( x >> 12 ) & 0x3F ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) return '' . join ( map ( chr , out ) )
6410	def lehmer_mean ( nums , exp = 2 ) : return sum ( x ** exp for x in nums ) / sum ( x ** ( exp - 1 ) for x in nums )
8597	def delete_group ( self , group_id ) : response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'DELETE' ) return response
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
11514	def search_item_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbyname' , parameters ) return response [ 'items' ]
585	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete . tolist ( ) ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
11154	def auto_complete_choices ( self , case_sensitive = False ) : self_basename = self . basename self_basename_lower = self . basename . lower ( ) if case_sensitive : # pragma: no cover def match ( basename ) : return basename . startswith ( self_basename ) else : def match ( basename ) : return basename . lower ( ) . startswith ( self_basename_lower ) choices = list ( ) if self . is_dir ( ) : choices . append ( self ) for p in self . sort_by_abspath ( self . select ( recursive = False ) ) : choices . append ( p ) else : p_parent = self . parent if p_parent . is_dir ( ) : for p in self . sort_by_abspath ( p_parent . select ( recursive = False ) ) : if match ( p . basename ) : choices . append ( p ) else : # pragma: no cover raise ValueError ( "'%s' directory does not exist!" % p_parent ) return choices
9059	def gradient ( self ) : L = self . L n = self . L . shape [ 0 ] grad = { "Lu" : zeros ( ( n , n , n * self . _L . shape [ 1 ] ) ) } for ii in range ( self . _L . shape [ 0 ] * self . _L . shape [ 1 ] ) : row = ii // self . _L . shape [ 1 ] col = ii % self . _L . shape [ 1 ] grad [ "Lu" ] [ row , : , ii ] = L [ : , col ] grad [ "Lu" ] [ : , row , ii ] += L [ : , col ] return grad
8367	def output_closure ( self , target , file_number = None ) : def output_context ( ctx ) : target_ctx = target target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return target_ctx def output_surface ( ctx ) : target_ctx = cairo . Context ( target ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return target_ctx def output_file ( ctx ) : root , extension = os . path . splitext ( target ) if file_number : filename = '%s_%04d%s' % ( root , file_number , extension ) else : filename = target extension = extension . lower ( ) if extension == '.png' : surface = ctx . get_target ( ) surface . write_to_png ( target ) elif extension == '.pdf' : target_ctx = cairo . Context ( cairo . PDFSurface ( filename , * self . size_or_default ( ) ) ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) elif extension in ( '.ps' , '.eps' ) : target_ctx = cairo . Context ( cairo . PSSurface ( filename , * self . size_or_default ( ) ) ) if extension == '.eps' : target_ctx . set_eps ( extension = '.eps' ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) elif extension == '.svg' : target_ctx = cairo . Context ( cairo . SVGSurface ( filename , * self . size_or_default ( ) ) ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return filename if isinstance ( target , cairo . Context ) : return output_context elif isinstance ( target , cairo . Surface ) : return output_surface else : return output_file
1121	def format ( self , o , context , maxlevels , level ) : return _safe_repr ( o , context , maxlevels , level )
9411	def _encode ( data , convert_to_float ) : ctf = convert_to_float # Handle variable pointer. if isinstance ( data , ( OctaveVariablePtr ) ) : return _encode ( data . value , ctf ) # Handle a user defined object. if isinstance ( data , OctaveUserClass ) : return _encode ( OctaveUserClass . to_value ( data ) , ctf ) # Handle a function pointer. if isinstance ( data , ( OctaveFunctionPtr , MatlabFunction ) ) : raise Oct2PyError ( 'Cannot write Octave functions' ) # Handle matlab objects. if isinstance ( data , MatlabObject ) : view = data . view ( np . ndarray ) out = MatlabObject ( data , data . classname ) for name in out . dtype . names : out [ name ] = _encode ( view [ name ] , ctf ) return out # Handle pandas series and dataframes if isinstance ( data , ( DataFrame , Series ) ) : return _encode ( data . values , ctf ) # Extract and encode values from dict-like objects. if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _encode ( value , ctf ) return out # Send None as nan. if data is None : return np . NaN # Sets are treated like lists. if isinstance ( data , set ) : return _encode ( list ( data ) , ctf ) # Lists can be interpreted as numeric arrays or cell arrays. if isinstance ( data , list ) : if _is_simple_numeric ( data ) : return _encode ( np . array ( data ) , ctf ) return _encode ( tuple ( data ) , ctf ) # Tuples are handled as cells. if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = _encode ( item , ctf ) return obj # Sparse data must be floating type. if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) # Return other data types unchanged. if not isinstance ( data , np . ndarray ) : return data # Extract and encode data from object-like arrays. if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = _encode ( item [ name ] , ctf ) else : out [ i ] = _encode ( item , ctf ) return out . reshape ( data . shape ) # Complex 128 is the highest supported by savemat. if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) # Convert to float if applicable. if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) # Return standard array. return data
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
13765	def insert ( self , index , value ) : self . _list . insert ( index , value ) self . _sync ( )
4290	def write ( self , album , media_group ) : from sigal import __url__ as sigal_link file_path = os . path . join ( album . dst_path , media_group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media_group [ 0 ] , 'previous_media' : media_group [ - 1 ] , 'next_media' : media_group [ 1 ] , 'index_title' : self . index_title , 'settings' : self . settings , 'sigal_link' : sigal_link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url_from_path ( os . path . relpath ( self . theme_path , album . dst_path ) ) } , } ) output_file = "%s.html" % file_path with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
11787	def add ( self , o ) : self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
1887	def emulate ( self , instruction ) : # The emulation might restart if Unicorn needs to bring in a memory map # or bring a value from Manticore state. while True : self . reset ( ) # Establish Manticore state, potentially from past emulation # attempts for base in self . _should_be_mapped : size , perms = self . _should_be_mapped [ base ] self . _emu . mem_map ( base , size , perms ) for address , values in self . _should_be_written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import ConcretizeMemory raise ConcretizeMemory ( self . _cpu . memory , offset , 8 , "Concretizing for emulation" ) self . _emu . mem_write ( address , b'' . join ( values ) ) # Try emulation self . _should_try_again = False self . _step ( instruction ) if not self . _should_try_again : break
4902	def course_modal ( context , course = None ) : if course : context . update ( { 'course_image_uri' : course . get ( 'course_image_uri' , '' ) , 'course_title' : course . get ( 'course_title' , '' ) , 'course_level_type' : course . get ( 'course_level_type' , '' ) , 'course_short_description' : course . get ( 'course_short_description' , '' ) , 'course_effort' : course . get ( 'course_effort' , '' ) , 'course_full_description' : course . get ( 'course_full_description' , '' ) , 'expected_learning_items' : course . get ( 'expected_learning_items' , [ ] ) , 'staff' : course . get ( 'staff' , [ ] ) , 'premium_modes' : course . get ( 'premium_modes' , [ ] ) , } ) return context
3395	def remove_genes ( cobra_model , gene_list , remove_reactions = True ) : gene_set = { cobra_model . genes . get_by_id ( str ( i ) ) for i in gene_list } gene_id_set = { i . id for i in gene_set } remover = _GeneRemover ( gene_id_set ) ast_rules = get_compiled_gene_reaction_rules ( cobra_model ) target_reactions = [ ] for reaction , rule in iteritems ( ast_rules ) : if reaction . gene_reaction_rule is None or len ( reaction . gene_reaction_rule ) == 0 : continue # reactions to remove if remove_reactions and not eval_gpr ( rule , gene_id_set ) : target_reactions . append ( reaction ) else : # if the reaction is not removed, remove the gene # from its gpr remover . visit ( rule ) new_rule = ast2str ( rule ) if new_rule != reaction . gene_reaction_rule : reaction . gene_reaction_rule = new_rule for gene in gene_set : cobra_model . genes . remove ( gene ) # remove reference to the gene in all groups associated_groups = cobra_model . get_associated_groups ( gene ) for group in associated_groups : group . remove_members ( gene ) cobra_model . remove_reactions ( target_reactions )
3249	def get_base ( managed_policy , * * conn ) : managed_policy [ '_version' ] = 1 arn = _get_name_from_structure ( managed_policy , 'Arn' ) policy = get_policy ( arn , * * conn ) document = get_managed_policy_document ( arn , policy_metadata = policy , * * conn ) managed_policy . update ( policy [ 'Policy' ] ) managed_policy [ 'Document' ] = document # Fix the dates: managed_policy [ 'CreateDate' ] = get_iso_string ( managed_policy [ 'CreateDate' ] ) managed_policy [ 'UpdateDate' ] = get_iso_string ( managed_policy [ 'UpdateDate' ] ) return managed_policy
10469	def launchAppByBundlePath ( bundlePath , arguments = None ) : if arguments is None : arguments = [ ] bundleUrl = NSURL . fileURLWithPath_ ( bundlePath ) workspace = AppKit . NSWorkspace . sharedWorkspace ( ) arguments_strings = list ( map ( lambda a : NSString . stringWithString_ ( str ( a ) ) , arguments ) ) arguments = NSDictionary . dictionaryWithDictionary_ ( { AppKit . NSWorkspaceLaunchConfigurationArguments : NSArray . arrayWithArray_ ( arguments_strings ) } ) return workspace . launchApplicationAtURL_options_configuration_error_ ( bundleUrl , AppKit . NSWorkspaceLaunchAllowingClassicStartup , arguments , None )
3034	def _oauth2_web_server_flow_params ( kwargs ) : params = { 'access_type' : 'offline' , 'response_type' : 'code' , } params . update ( kwargs ) # Check for the presence of the deprecated approval_prompt param and # warn appropriately. approval_prompt = params . get ( 'approval_prompt' ) if approval_prompt is not None : logger . warning ( 'The approval_prompt parameter for OAuth2WebServerFlow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval_prompt == 'force' : logger . warning ( 'approval_prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval_prompt' ] return params
9040	def add_instruction ( self , specification ) : instruction = self . as_instruction ( specification ) self . _type_to_instruction [ instruction . type ] = instruction
378	def samplewise_norm ( x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 ) : if rescale : x *= rescale if x . shape [ channel_index ] == 1 : # greyscale if samplewise_center : x = x - np . mean ( x ) if samplewise_std_normalization : x = x / np . std ( x ) return x elif x . shape [ channel_index ] == 3 : # rgb if samplewise_center : x = x - np . mean ( x , axis = channel_index , keepdims = True ) if samplewise_std_normalization : x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) return x else : raise Exception ( "Unsupported channels %d" % x . shape [ channel_index ] )
5113	def clear ( self ) : self . _t = 0 self . num_events = 0 self . num_agents = np . zeros ( self . nE , int ) self . _fancy_heap = PriorityQueue ( ) self . _prev_edge = None self . _initialized = False self . reset_colors ( ) for q in self . edge2queue : q . clear ( )
3924	def _on_return ( self , text ) : # Ignore if the user hasn't typed a message. if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : # Temporary UI for testing image uploads filename = text . split ( ' ' ) [ 1 ] image_file = open ( filename , 'rb' ) text = '' else : image_file = None text = replace_emoticons ( text ) segments = hangups . ChatMessageSegment . from_str ( text ) self . _coroutine_queue . put ( self . _handle_send_message ( self . _conversation . send_message ( segments , image_file = image_file ) ) )
13275	def update_desc_lsib_path ( desc ) : if ( desc [ 'sib_seq' ] > 0 ) : lsib_path = copy . deepcopy ( desc [ 'path' ] ) lsib_path [ - 1 ] = desc [ 'sib_seq' ] - 1 desc [ 'lsib_path' ] = lsib_path else : pass return ( desc )
4001	def get_port_spec_document ( expanded_active_specs , docker_vm_ip ) : forwarding_port = 65000 port_spec = { 'docker_compose' : { } , 'nginx' : [ ] , 'hosts_file' : [ ] } host_full_addresses , host_names , stream_host_ports = set ( ) , set ( ) , set ( ) # No matter the order of apps in expanded_active_specs, we want to produce a consistent # port_spec with respect to the apps and the ports they are outputted on for app_name in sorted ( expanded_active_specs [ 'apps' ] . keys ( ) ) : app_spec = expanded_active_specs [ 'apps' ] [ app_name ] if 'host_forwarding' not in app_spec : continue port_spec [ 'docker_compose' ] [ app_name ] = [ ] for host_forwarding_spec in app_spec [ 'host_forwarding' ] : # These functions are just used for validating the set of specs all works together _add_full_addresses ( host_forwarding_spec , host_full_addresses ) if host_forwarding_spec [ 'type' ] == 'stream' : _add_stream_host_port ( host_forwarding_spec , stream_host_ports ) port_spec [ 'docker_compose' ] [ app_name ] . append ( _docker_compose_port_spec ( host_forwarding_spec , forwarding_port ) ) port_spec [ 'nginx' ] . append ( _nginx_port_spec ( host_forwarding_spec , forwarding_port , docker_vm_ip ) ) _add_host_names ( host_forwarding_spec , docker_vm_ip , port_spec , host_names ) forwarding_port += 1 return port_spec
8307	def get_command_responses ( self ) : if not self . response_queue . empty ( ) : yield None while not self . response_queue . empty ( ) : line = self . response_queue . get ( ) if line is not None : yield line
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
6840	def distrib_family ( ) : distrib = ( distrib_id ( ) or '' ) . lower ( ) if distrib in [ 'debian' , 'ubuntu' , 'linuxmint' , 'elementary os' ] : return DEBIAN elif distrib in [ 'redhat' , 'rhel' , 'centos' , 'sles' , 'fedora' ] : return REDHAT elif distrib in [ 'sunos' ] : return SUN elif distrib in [ 'gentoo' ] : return GENTOO elif distrib in [ 'arch' , 'manjarolinux' ] : return ARCH return 'other'
12640	def get_unique_field_values_per_group ( self , field_name , field_to_use_as_key = None ) : unique_vals = DefaultOrderedDict ( set ) for dcmg in self . dicom_groups : for f in self . dicom_groups [ dcmg ] : field_val = DicomFile ( f ) . get_attributes ( field_name ) key_val = dcmg if field_to_use_as_key is not None : try : key_val = str ( DicomFile ( dcmg ) . get_attributes ( field_to_use_as_key ) ) except KeyError as ke : raise KeyError ( 'Error getting field {} from ' 'file {}' . format ( field_to_use_as_key , dcmg ) ) from ke unique_vals [ key_val ] . add ( field_val ) return unique_vals
7586	def _get_boots ( arr , nboots ) : ## hold results (nboots, [dstat, ]) boots = np . zeros ( ( nboots , ) ) ## iterate to fill boots for bidx in xrange ( nboots ) : ## sample with replacement lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst ## return bootarr return boots
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : # guard conditions assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
4481	def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
2595	def use_pickle ( ) : from . import serialize serialize . pickle = serialize . _stdlib_pickle # restore special function handling can_map [ FunctionType ] = _original_can_map [ FunctionType ]
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
4520	def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
4258	def url_from_path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )
7644	def can_convert ( annotation , target_namespace ) : # If we're already in the target namespace, do nothing if annotation . namespace == target_namespace : return True if target_namespace in __CONVERSION__ : # Look for a way to map this namespace to the target for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return True return False
3698	def Hsub ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : # pragma: no cover def list_methods ( ) : methods = [ ] # if Hfus(T=T, P=P, MW=MW, CASRN=CASRN) and Hvap(T=T, P=P, MW=MW, CASRN=CASRN): # methods.append('Hfus + Hvap') if CASRN in GharagheiziHsub_data . index : methods . append ( 'Ghazerati Appendix, at 298K' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section # if Method == 'Hfus + Hvap': # p1 = Hfus(T=T, P=P, MW=MW, CASRN=CASRN) # p2 = Hvap(T=T, P=P, MW=MW, CASRN=CASRN) # if p1 and p2: # _Hsub = p1 + p2 # else: # _Hsub = None if Method == 'Ghazerati Appendix, at 298K' : _Hsub = float ( GharagheiziHsub_data . at [ CASRN , 'Hsub' ] ) elif Method == 'None' or not _Hsub or not MW : return None else : raise Exception ( 'Failure in in function' ) _Hsub = property_molar_to_mass ( _Hsub , MW ) return _Hsub
12630	def compose_err_msg ( msg , * * kwargs ) : updated_msg = msg for k , v in sorted ( kwargs . items ( ) ) : if isinstance ( v , _basestring ) : # print only str-like arguments updated_msg += "\n" + k + ": " + v return updated_msg
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) # allow overriding htmode if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) # disables n return 'NONE'
12951	def _get_new_connection ( self ) : pool = getRedisPool ( self . mdl . REDIS_CONNECTION_PARAMS ) return redis . Redis ( connection_pool = pool )
3192	def create ( self , list_id , data ) : self . list_id = list_id if 'status' not in data : raise KeyError ( 'The list member must have a status' ) if data [ 'status' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
4673	def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
12158	def abfSort ( IDs ) : IDs = list ( IDs ) monO = [ ] monN = [ ] monD = [ ] good = [ ] for ID in IDs : if ID is None : continue if 'o' in ID : monO . append ( ID ) elif 'n' in ID : monN . append ( ID ) elif 'd' in ID : monD . append ( ID ) else : good . append ( ID ) return sorted ( good ) + sorted ( monO ) + sorted ( monN ) + sorted ( monD )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
10389	def calculate_average_scores_on_subgraphs ( subgraphs : Mapping [ H , BELGraph ] , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , tqdm_kwargs : Optional [ Mapping [ str , Any ] ] = None , ) -> Mapping [ H , Tuple [ float , float , float , float , int , int ] ] : results = { } log . info ( 'calculating results for %d candidate mechanisms using %d permutations' , len ( subgraphs ) , runs ) it = subgraphs . items ( ) if use_tqdm : _tqdm_kwargs = dict ( total = len ( subgraphs ) , desc = 'Candidate mechanisms' ) if tqdm_kwargs : _tqdm_kwargs . update ( tqdm_kwargs ) it = tqdm ( it , * * _tqdm_kwargs ) for node , subgraph in it : number_first_neighbors = subgraph . in_degree ( node ) number_first_neighbors = 0 if isinstance ( number_first_neighbors , dict ) else number_first_neighbors mechanism_size = subgraph . number_of_nodes ( ) runners = workflow ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if 0 == len ( scores ) : results [ node ] = ( None , None , None , None , number_first_neighbors , mechanism_size , ) continue scores = np . array ( scores ) average_score = np . average ( scores ) score_std = np . std ( scores ) med_score = np . median ( scores ) chi_2_stat , norm_p = stats . normaltest ( scores ) results [ node ] = ( average_score , score_std , norm_p , med_score , number_first_neighbors , mechanism_size , ) return results
10396	def unscored_nodes_iter ( self ) -> BaseEntity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node
11085	def shutdown ( self , msg , args ) : self . log . info ( "Received shutdown from %s" , msg . user . username ) self . _bot . runnable = False return "Shutting down..."
13671	def strip_codes ( s : Any ) -> str : return codepat . sub ( '' , str ( s ) if ( s or ( s == 0 ) ) else '' )
9580	def read_struct_array ( fd , endian , header ) : # read field name length (unused, as strings are null terminated) field_name_length = read_elements ( fd , endian , [ 'miINT32' ] ) if field_name_length > 32 : raise ParseError ( 'Unexpected field name length: {}' . format ( field_name_length ) ) # read field names fields = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) if isinstance ( fields , basestring ) : fields = [ fields ] # read rows and columns of each field empty = lambda : [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] array = { } for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : for field in fields : # read the matrix header and array vheader , next_pos , fd_var = read_var_header ( fd , endian ) data = read_var_array ( fd_var , endian , vheader ) if field not in array : array [ field ] = empty ( ) array [ field ] [ row ] . append ( data ) # move on to next field fd . seek ( next_pos ) # pack the nested arrays for field in fields : rows = array [ field ] for i in range ( header [ 'dims' ] [ 0 ] ) : rows [ i ] = squeeze ( rows [ i ] ) array [ field ] = squeeze ( array [ field ] ) return array
7422	def ref_build_and_muscle_chunk ( data , sample ) : ## get regions using bedtools regions = bedtools_merge ( data , sample ) . strip ( ) . split ( "\n" ) nregions = len ( regions ) chunksize = ( nregions / 10 ) + ( nregions % 10 ) LOGGER . debug ( "nregions {} chunksize {}" . format ( nregions , chunksize ) ) ## create an output file to write clusters to idx = 0 tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" ) ## remove old files if they exist to avoid append errors for i in range ( 11 ) : if os . path . exists ( tmpfile . format ( i ) ) : os . remove ( tmpfile . format ( i ) ) fopen = open ## If reference+denovo we drop the reads back into clust.gz ## and let the muscle_chunker do it's thing back in cluster_within if data . paramsdict [ "assembly_method" ] == "denovo+reference" : tmpfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) fopen = gzip . open ## build clusters for aligning with muscle from the sorted bam file samfile = pysam . AlignmentFile ( sample . files . mapped_reads , 'rb' ) #"./tortas_refmapping/PZ70-mapped-sorted.bam", "rb") ## fill clusts list and dump periodically clusts = [ ] nclusts = 0 for region in regions : chrom , pos1 , pos2 = region . split ( ) try : ## fetches pairs quickly but then goes slow to merge them. if "pair" in data . paramsdict [ "datatype" ] : clust = fetch_cluster_pairs ( data , samfile , chrom , int ( pos1 ) , int ( pos2 ) ) ## fetch but no need to merge else : clust = fetch_cluster_se ( data , samfile , chrom , int ( pos1 ) , int ( pos2 ) ) except IndexError as inst : LOGGER . error ( "Bad region chrom:start-end {}:{}-{}" . format ( chrom , pos1 , pos2 ) ) continue if clust : clusts . append ( "\n" . join ( clust ) ) nclusts += 1 if nclusts == chunksize : ## write to file tmphandle = tmpfile . format ( idx ) with fopen ( tmphandle , 'a' ) as tmp : #LOGGER.debug("Writing tmpfile - {}".format(tmpfile.format(idx))) #if data.paramsdict["assembly_method"] == "denovo+reference": # ## This is dumb, but for this method you need to prepend the # ## separator to maintain proper formatting of clust.gz tmp . write ( "\n//\n//\n" . join ( clusts ) + "\n//\n//\n" ) idx += 1 nclusts = 0 clusts = [ ] if clusts : ## write remaining to file with fopen ( tmpfile . format ( idx ) , 'a' ) as tmp : #tmp.write("\n//\n//\n" + ("\n//\n//\n".join(clusts))) tmp . write ( "\n//\n//\n" . join ( clusts ) + "\n//\n//\n" ) clusts = [ ] if not data . paramsdict [ "assembly_method" ] == "denovo+reference" : chunkfiles = glob . glob ( os . path . join ( data . tmpdir , sample . name + "_chunk_*.ali" ) ) LOGGER . info ( "created chunks %s" , chunkfiles ) ## cleanup samfile . close ( )
1878	def MOVSS ( cpu , dest , src ) : if dest . type == 'register' and src . type == 'register' : assert dest . size == 128 and src . size == 128 dest . write ( dest . read ( ) & ~ 0xffffffff | src . read ( ) & 0xffffffff ) elif dest . type == 'memory' : assert src . type == 'register' dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : assert src . type == 'memory' and dest . type == 'register' assert src . size == 32 and dest . size == 128 dest . write ( Operators . ZEXTEND ( src . read ( ) , 128 ) )
1152	def warn ( message , category = None , stacklevel = 1 ) : # Check if message is already a Warning object if isinstance ( message , Warning ) : category = message . __class__ # Check category argument if category is None : category = UserWarning assert issubclass ( category , Warning ) # Get context information try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : # embedded interpreters don't have sys.argv, see bug #839151 filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
333	def plot_stoch_vol ( data , trace = None , ax = None ) : if trace is None : trace = model_stoch_vol ( data ) if ax is None : fig , ax = plt . subplots ( figsize = ( 15 , 8 ) ) data . abs ( ) . plot ( ax = ax ) ax . plot ( data . index , np . exp ( trace [ 's' , : : 30 ] . T ) , 'r' , alpha = .03 ) ax . set ( title = 'Stochastic volatility' , xlabel = 'Time' , ylabel = 'Volatility' ) ax . legend ( [ 'Abs returns' , 'Stochastic volatility process' ] , frameon = True , framealpha = 0.5 ) return ax
3380	def add_lexicographic_constraints ( model , objectives , objective_direction = 'max' ) : if type ( objective_direction ) is not list : objective_direction = [ objective_direction ] * len ( objectives ) constraints = [ ] for rxn_id , obj_dir in zip ( objectives , objective_direction ) : model . objective = model . reactions . get_by_id ( rxn_id ) model . objective_direction = obj_dir constraints . append ( fix_objective_as_constraint ( model ) ) return pd . Series ( constraints , index = objectives )
2712	def pretty_print ( obj , indent = False ) : if indent : return json . dumps ( obj , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) else : return json . dumps ( obj , sort_keys = True )
5880	def get_siblings_content ( self , current_sibling , baselinescore_siblings_para ) : if current_sibling . tag == 'p' and self . parser . getText ( current_sibling ) : tmp = current_sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential_paragraphs = self . parser . getElementsByTag ( current_sibling , tag = 'p' ) if potential_paragraphs is None : return None paragraphs = list ( ) for first_paragraph in potential_paragraphs : text = self . parser . getText ( first_paragraph ) if text : # no len(text) > 0 word_stats = self . stopwords_class ( language = self . get_language ( ) ) . get_stopword_count ( text ) paragraph_score = word_stats . get_stopword_count ( ) sibling_baseline_score = float ( .30 ) high_link_density = self . is_highlink_density ( first_paragraph ) score = float ( baselinescore_siblings_para * sibling_baseline_score ) if score < paragraph_score and not high_link_density : para = self . parser . createElement ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
13164	def format_value ( value ) : value_id = id ( value ) if value_id in recursion_breaker . processed : return u'<recursion>' recursion_breaker . processed . add ( value_id ) try : if isinstance ( value , six . binary_type ) : # suppose, all byte strings are in unicode # don't know if everybody in the world uses anything else? return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text_type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : # long lists or lists with multiline items # will be shown vertically values = list ( map ( format_value , value ) ) result = serialize_list ( u'[' , values , delimiter = u',' ) + u']' return force_unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) # format each key/value pair as a text, # calling format_value recursively items = ( tuple ( map ( format_value , item ) ) for item in items ) items = list ( items ) # sort by keys for readability items . sort ( ) # for each item value items = [ serialize_text ( u'{0}: ' . format ( key ) , item_value ) for key , item_value in items ] # and serialize these pieces as a list, enclosing # them into a curve brackets result = serialize_list ( u'{' , items , delimiter = u',' ) + u'}' return force_unicode ( result ) return force_unicode ( repr ( value ) ) finally : recursion_breaker . processed . remove ( value_id )
10329	def get_path_effect ( graph , path , relationship_dict ) : causal_effect = [ ] for predecessor , successor in pairwise ( path ) : if pair_has_contradiction ( graph , predecessor , successor ) : return Effect . ambiguous edges = graph . get_edge_data ( predecessor , successor ) edge_key , edge_relation , _ = rank_edges ( edges ) relation = graph [ predecessor ] [ successor ] [ edge_key ] [ RELATION ] # Returns Effect.no_effect if there is a non causal edge in path if relation not in relationship_dict or relationship_dict [ relation ] == 0 : return Effect . no_effect causal_effect . append ( relationship_dict [ relation ] ) final_effect = reduce ( lambda x , y : x * y , causal_effect ) return Effect . activation if final_effect == 1 else Effect . inhibition
13394	def setting ( self , name_hyphen ) : if name_hyphen in self . _instance_settings : value = self . _instance_settings [ name_hyphen ] [ 1 ] else : msg = "No setting named '%s'" % name_hyphen raise UserFeedback ( msg ) if hasattr ( value , 'startswith' ) and value . startswith ( "$" ) : env_var = value . lstrip ( "$" ) if env_var in os . environ : return os . getenv ( env_var ) else : msg = "'%s' is not defined in your environment" % env_var raise UserFeedback ( msg ) elif hasattr ( value , 'startswith' ) and value . startswith ( "\$" ) : return value . replace ( "\$" , "$" ) else : return value
2694	def fix_hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == "HYPH" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev_tok = bar [ - 1 ] next_tok = foo [ i + 1 ] prev_tok [ 0 ] += "-" + next_tok [ 0 ] prev_tok [ 1 ] += "-" + next_tok [ 1 ] bar [ - 1 ] = prev_tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
1694	def flat_map ( self , flatmap_function ) : from heronpy . streamlet . impl . flatmapbolt import FlatMapStreamlet fm_streamlet = FlatMapStreamlet ( flatmap_function , self ) self . _add_child ( fm_streamlet ) return fm_streamlet
12769	def load_markers ( self , filename , attachments , max_frames = 1e100 ) : self . markers = Markers ( self ) fn = filename . lower ( ) if fn . endswith ( '.c3d' ) : self . markers . load_c3d ( filename , max_frames = max_frames ) elif fn . endswith ( '.csv' ) or fn . endswith ( '.csv.gz' ) : self . markers . load_csv ( filename , max_frames = max_frames ) else : logging . fatal ( '%s: not sure how to load markers!' , filename ) self . markers . load_attachments ( attachments , self . skeleton )
8469	def getOSName ( self ) : _system = platform . system ( ) if _system in [ self . __class__ . OS_WINDOWS , self . __class__ . OS_MAC , self . __class__ . OS_LINUX ] : if _system == self . __class__ . OS_LINUX : _dist = platform . linux_distribution ( ) [ 0 ] if _dist . lower ( ) == self . __class__ . OS_UBUNTU . lower ( ) : return self . __class__ . OS_UBUNTU elif _dist . lower ( ) == self . __class__ . OS_DEBIAN . lower ( ) : return self . __class__ . OS_DEBIAN elif _dist . lower ( ) == self . __class__ . OS_CENTOS . lower ( ) : return self . __class__ . OS_CENTOS elif _dist . lower ( ) == self . __class__ . OS_REDHAT . lower ( ) : return self . __class__ . OS_REDHAT elif _dist . lower ( ) == self . __class__ . OS_KALI . lower ( ) : return self . __class__ . OS_KALI return _system else : return None
6484	def _process_field_values ( request ) : return { field_key : request . POST [ field_key ] for field_key in request . POST if field_key in course_discovery_filter_fields ( ) }
1004	def _inferPhase2 ( self ) : # Init to zeros to start self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) self . colConfidence [ 't' ] . fill ( 0 ) # Phase 2 - Compute new predicted state and update cell and column # confidences for c in xrange ( self . numberOfCols ) : # For each cell in the column for i in xrange ( self . cellsPerColumn ) : # For each segment in the cell for s in self . cells [ c ] [ i ] : # See if it has the min number of active synapses numActiveSyns = self . _getSegmentActivityLevel ( s , self . infActiveState [ 't' ] , connectedSynapsesOnly = False ) if numActiveSyns < self . activationThreshold : continue # Incorporate the confidence into the owner cell and column if self . verbosity >= 6 : print "incorporating DC from cell[%d,%d]: " % ( c , i ) , s . debugPrint ( ) dc = s . dutyCycle ( ) self . cellConfidence [ 't' ] [ c , i ] += dc self . colConfidence [ 't' ] [ c ] += dc # If we reach threshold on the connected synapses, predict it # If not active, skip over it if self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) : self . infPredictedState [ 't' ] [ c , i ] = 1 # Normalize column and cell confidences sumConfidences = self . colConfidence [ 't' ] . sum ( ) if sumConfidences > 0 : self . colConfidence [ 't' ] /= sumConfidences self . cellConfidence [ 't' ] /= sumConfidences # Are we predicting the required minimum number of columns? numPredictedCols = self . infPredictedState [ 't' ] . max ( axis = 1 ) . sum ( ) if numPredictedCols >= 0.5 * self . avgInputDensity : return True else : return False
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , * * kwargs ) return wrapper return decorator
3770	def mixing_simple ( fracs , props ) : if not none_and_length_check ( [ fracs , props ] ) : return None result = sum ( frac * prop for frac , prop in zip ( fracs , props ) ) return result
12960	def _filter ( filterObj , * * kwargs ) : for key , value in kwargs . items ( ) : if key . endswith ( '__ne' ) : notFilter = True key = key [ : - 4 ] else : notFilter = False if key not in filterObj . indexedFields : raise ValueError ( 'Field "' + key + '" is not in INDEXED_FIELDS array. Filtering is only supported on indexed fields.' ) if notFilter is False : filterObj . filters . append ( ( key , value ) ) else : filterObj . notFilters . append ( ( key , value ) ) return filterObj
9360	def _to_lower_alpha_only ( s ) : s = re . sub ( r'\n' , ' ' , s . lower ( ) ) return re . sub ( r'[^a-z\s]' , '' , s )
4733	def generate_steady_rt_pic ( process_data , para_meter , scale , steady_time ) : pic_path_steady = para_meter [ 'filename' ] + '_steady.png' plt . figure ( figsize = ( 4 * scale , 2.5 * scale ) ) for key in process_data . keys ( ) : if len ( process_data [ key ] ) < steady_time : steady_time = len ( process_data [ key ] ) plt . scatter ( process_data [ key ] [ - 1 * steady_time : , 0 ] , process_data [ key ] [ - 1 * steady_time : , 1 ] , label = str ( key ) , s = 10 ) steady_value = np . mean ( process_data [ key ] [ - 1 * steady_time : , 1 ] ) steady_value_5 = steady_value * ( 1 + 0.05 ) steady_value_10 = steady_value * ( 1 + 0.1 ) steady_value_ng_5 = steady_value * ( 1 - 0.05 ) steady_value_ng_10 = steady_value * ( 1 - 0.1 ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value ] * steady_time , 'b' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_5 ] * steady_time , 'g' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_ng_5 ] * steady_time , 'g' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_10 ] * steady_time , 'r' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_ng_10 ] * steady_time , 'r' ) plt . title ( para_meter [ 'title' ] + '(steady)' ) plt . xlabel ( para_meter [ 'x_axis_name' ] + '(steady)' ) plt . ylabel ( para_meter [ 'y_axis_name' ] + '(steady)' ) plt . legend ( loc = 'upper left' ) plt . savefig ( pic_path_steady ) return pic_path_steady
8876	def compute_dosage ( expec , alt = None ) : if alt is None : return expec [ ... , - 1 ] try : return expec [ : , alt ] except NotImplementedError : alt = asarray ( alt , int ) return asarray ( expec , float ) [ : , alt ]
12672	def group ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . group ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . GROUP , * args )
1992	def _get_id ( self ) : id_ = self . _last_id . value self . _last_id . value += 1 return id_
252	def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out
3114	def _from_base_type ( self , value ) : if not value : return None try : # Uses the from_json method of the implied class of value credentials = client . Credentials . new_from_json ( value ) except ValueError : credentials = None return credentials
4708	def power_off ( self , interval = 200 ) : if self . __power_off_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_OFF" ) return 1 return self . __press ( self . __power_off_port , interval = interval )
2679	def get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : client = get_client ( 'sts' , profile_name , aws_access_key_id , aws_secret_access_key , region , ) return client . get_caller_identity ( ) . get ( 'Account' )
1212	def WorkerAgentGenerator ( agent_class ) : # Support special case where class is given as type-string (AgentsDictionary) or class-name-string. if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) # Last resort: Class name given as string? if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : """ Worker agent receiving a shared model to avoid creating multiple models. """ def __init__ ( self , model = None , * * kwargs ) : # Set our model externally. self . model = model # Be robust against `network` coming in from kwargs even though this agent doesn't have one if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) # Call super c'tor (which will call initialize_model and assign self.model to the return value). super ( WorkerAgent , self ) . __init__ ( * * kwargs ) def initialize_model ( self ) : # Return our model (already given and initialized). return self . model return WorkerAgent
7516	def init_arrays ( data ) : ## get stats from step6 h5 and create new h5 co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) ## get maxlen and chunk len maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] ## make array for snp string, 2 cols, - and * snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] ## array for filters that will be applied in step7 filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] ## array for edgetrimming edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] ## xfer data from clustdb to finaldb edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] ## close h5s io5 . close ( ) co5 . close ( )
12035	def sweepYfiltered ( self ) : assert self . kernel is not None return swhlab . common . convolve ( self . sweepY , self . kernel )
6245	def set_time ( self , value : float ) : if value < 0 : value = 0 self . controller . row = self . rps * value
10402	def calculate_score ( self , node : BaseEntity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default_score ) for predecessor , _ , d in self . graph . in_edges ( node , data = True ) : if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
1384	def trigger_watches ( self ) : to_remove = [ ] for uid , callback in self . watches . items ( ) : try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) to_remove . append ( uid ) for uid in to_remove : self . unregister_watch ( uid )
11473	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : block_parser = TextBlockParser ( ) lines = text . splitlines ( ) parsed = [ ] for line in lines : if STANDALONE_URL_RE . match ( line ) : user_url = line . strip ( ) try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) except OEmbedException : if urlize_all_links : line = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) line = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) else : line = block_parser . parse ( line , maxwidth , maxheight , 'inline' , context , urlize_all_links ) parsed . append ( line ) return mark_safe ( '\n' . join ( parsed ) )
5093	def refresh_maps ( self ) : for robot in self . robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _maps . update ( { robot . serial : resp2 . json ( ) } )
7050	def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate_type , sigclip ) = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # get the LC into a dict lcdict = readerfunc ( lcfile ) # this should handle lists/tuples being returned by readerfunc # we assume that the first element is the actual lcdict # FIXME: figure out how to not need this assumption if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] outdict = { } # dereference the columns and get them from the lcdict if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) # normalize here if not using special normalization if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs # # now we'll do: 1. sigclip, 2. reform to timebase, 3. renorm to zero # # 1. sigclip as requested stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip ) # 2. now, we'll renorm to the timebase mags_interpolator = spi . interp1d ( stimes , smags , kind = interpolate_type , fill_value = 'extrapolate' ) errs_interpolator = spi . interp1d ( stimes , serrs , kind = interpolate_type , fill_value = 'extrapolate' ) interpolated_mags = mags_interpolator ( timebase ) interpolated_errs = errs_interpolator ( timebase ) # 3. renorm to zero magmedian = np . median ( interpolated_mags ) renormed_mags = interpolated_mags - magmedian # update the dict outdict = { 'mags' : renormed_mags , 'errs' : interpolated_errs , 'origmags' : interpolated_mags } # # done with this magcol # return outdict except Exception as e : LOGEXCEPTION ( 'reform LC task failed: %s' % repr ( task ) ) return None
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
9856	def get_data ( self , * * kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) # If endDate is left blank (not passed in), the most recent results will be returned. if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , * * data )
8206	def overlap ( self , x1 , y1 , x2 , y2 , r = 5 ) : if abs ( x2 - x1 ) < r and abs ( y2 - y1 ) < r : return True else : return False
10515	def verifyscrollbarvertical ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXVerticalOrientation" : return 1 except : pass return 0
7266	def run_matcher ( self , subject , * expected , * * kw ) : # Update assertion expectation self . expected = expected _args = ( subject , ) if self . kind == OperatorTypes . MATCHER : _args += expected try : result = self . match ( * _args , * * kw ) except Exception as error : return self . _make_error ( error = error ) reasons = [ ] if isinstance ( result , tuple ) : result , reasons = result if result is False and self . ctx . negate : return True if result is True and not self . ctx . negate : return True return self . _make_error ( reasons = reasons )
927	def getFilename ( aggregationInfo , inputFile ) : # Find the actual file, with an absolute path inputFile = resource_filename ( "nupic.datafiles" , inputFile ) a = defaultdict ( lambda : 0 , aggregationInfo ) outputDir = os . path . dirname ( inputFile ) outputFile = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFile ) ) [ 0 ] noAggregation = True timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if a [ k ] > 0 : noAggregation = False outputFile += '_%s_%d' % ( k , a [ k ] ) if noAggregation : return inputFile outputFile += '.csv' outputFile = os . path . join ( outputDir , outputFile ) return outputFile
10903	def examine_unexplained_noise ( state , bins = 1000 , xlim = ( - 10 , 10 ) ) : r = state . residuals q = np . fft . fftn ( r ) #Get the expected values of `sigma`: calc_sig = lambda x : np . sqrt ( np . dot ( x , x ) / x . size ) rh , xr = np . histogram ( r . ravel ( ) / calc_sig ( r . ravel ( ) ) , bins = bins , density = True ) bigq = np . append ( q . real . ravel ( ) , q . imag . ravel ( ) ) qh , xq = np . histogram ( bigq / calc_sig ( q . real . ravel ( ) ) , bins = bins , density = True ) xr = 0.5 * ( xr [ 1 : ] + xr [ : - 1 ] ) xq = 0.5 * ( xq [ 1 : ] + xq [ : - 1 ] ) gauss = lambda t : np . exp ( - t * t * 0.5 ) / np . sqrt ( 2 * np . pi ) plt . figure ( figsize = [ 16 , 8 ] ) axes = [ ] for a , ( x , r , lbl ) in enumerate ( [ [ xr , rh , 'Real' ] , [ xq , qh , 'Fourier' ] ] ) : ax = plt . subplot ( 1 , 2 , a + 1 ) ax . semilogy ( x , r , label = 'Data' ) ax . plot ( x , gauss ( x ) , label = 'Gauss Fit' , scalex = False , scaley = False ) ax . set_xlabel ( 'Residuals value $r/\sigma$' ) ax . set_ylabel ( 'Probability $P(r/\sigma)$' ) ax . legend ( loc = 'upper right' ) ax . set_title ( '{}-Space' . format ( lbl ) ) ax . set_xlim ( xlim ) axes . append ( ax ) return axes
8368	def _parse ( self ) : p1 = "\[.*?\](.*?)\[\/.*?\]" p2 = "\[(.*?)\]" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , "\\1" , self . description ) self . description = self . description . strip ( )
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) # convert dates to datetimes. # when we change code to datetimes, we won't have to do this. start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
2899	def get_tasks_from_spec_name ( self , name ) : return [ task for task in self . get_tasks ( ) if task . task_spec . name == name ]
9715	async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
6415	def median ( nums ) : nums = sorted ( nums ) mag = len ( nums ) if mag % 2 : mag = int ( ( mag - 1 ) / 2 ) return nums [ mag ] mag = int ( mag / 2 ) med = ( nums [ mag - 1 ] + nums [ mag ] ) / 2 return med if not med . is_integer ( ) else int ( med )
5243	def tz_convert ( dt , to_tz , from_tz = None ) -> str : logger = logs . get_logger ( tz_convert , level = 'info' ) f_tz , t_tz = get_tz ( from_tz ) , get_tz ( to_tz ) from_dt = pd . Timestamp ( str ( dt ) , tz = f_tz ) logger . debug ( f'converting {str(from_dt)} from {f_tz} to {t_tz} ...' ) return str ( pd . Timestamp ( str ( from_dt ) , tz = t_tz ) )
6050	def run ( self , data , results = None , mask = None , positions = None ) : model_image = results . last . unmasked_model_image galaxy_tuples = results . last . constant . name_instance_tuples_for_class ( g . Galaxy ) results_copy = copy . copy ( results . last ) for name , galaxy in galaxy_tuples : optimizer = self . optimizer . copy_with_name_extension ( name ) optimizer . variable . hyper_galaxy = g . HyperGalaxy galaxy_image = results . last . unmasked_image_for_galaxy ( galaxy ) optimizer . fit ( self . __class__ . Analysis ( data , model_image , galaxy_image ) ) getattr ( results_copy . variable , name ) . hyper_galaxy = optimizer . variable . hyper_galaxy getattr ( results_copy . constant , name ) . hyper_galaxy = optimizer . constant . hyper_galaxy return results_copy
2597	def can ( obj ) : import_needed = False for cls , canner in iteritems ( can_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import_needed : # perform can_map imports, then try again # this will usually only happen once _import_mapping ( can_map , _original_can_map ) return can ( obj ) return obj
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
8826	def populate_subtasks ( self , context , sg , parent_job_id ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None ports = db_api . sg_gather_associated_ports ( context , db_sg ) if len ( ports ) == 0 : return { "ports" : 0 } for port in ports : job_body = dict ( action = "update port %s" % port [ 'id' ] , tenant_id = db_sg [ 'tenant_id' ] , resource_id = port [ 'id' ] , parent_id = parent_job_id ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_consumer = QuarkSGAsyncConsumerClient ( ) try : rpc_consumer . update_port ( context , port [ 'id' ] , job [ 'id' ] ) except om_exc . MessagingTimeout : # TODO(roaet): Not too sure what can be done here other than # updating the job as a failure? LOG . error ( "Failed to update port. Rabbit running?" ) return None
7553	def nworker ( data , chunk ) : ## set the thread limit on the remote engine oldlimit = set_mkl_thread_limit ( 1 ) ## open seqarray view, the modified arr is in bootstarr with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : , 0 ] smps = io5 [ "quartets" ] [ chunk : chunk + data . _chunksize ] ## create an N-mask array of all seq cols nall_mask = seqview [ : ] == 78 ## init arrays to fill with results rquartets = np . zeros ( ( smps . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rinvariants = np . zeros ( ( smps . shape [ 0 ] , 16 , 16 ) , dtype = np . uint16 ) ## fill arrays with results as we compute them. This iterates ## over all of the quartet sets in this sample chunk. It would ## be nice to have this all numbified. for idx in xrange ( smps . shape [ 0 ] ) : sidx = smps [ idx ] seqs = seqview [ sidx ] ## these axis calls cannot be numbafied, but I can't ## find a faster way that is JIT compiled, and I've ## really, really, really tried. Tried again now that ## numba supports axis args for np.sum. Still can't ## get speed improvements by numbifying this loop. nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqs == seqs [ 0 ] , axis = 0 ) ## here are the jitted funcs bidx , invar = calculate ( seqs , maparr , nmask , TESTS ) ## store results rquartets [ idx ] = smps [ idx ] [ bidx ] rinvariants [ idx ] = invar ## reset thread limit set_mkl_thread_limit ( oldlimit ) ## return results... return rquartets , rinvariants
2007	def _deserialize_uint ( data , nbytes = 32 , padding = 0 , offset = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True , offset = offset ) value = Operators . ZEXTEND ( value , ( nbytes + padding ) * 8 ) return value
13831	def _api_call ( self , method_name , * args , * * kwargs ) : params = kwargs . setdefault ( 'params' , { } ) params . update ( { 'key' : self . _apikey } ) if self . _token is not None : params . update ( { 'token' : self . _token } ) http_method = getattr ( requests , method_name ) return http_method ( TRELLO_URL + self . _url , * args , * * kwargs )
1961	def sys_fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except IndexError : ret = - errno . EBADF except FdError : ret = - errno . EINVAL return ret
10532	def create_project ( name , short_name , description ) : try : project = dict ( name = name , short_name = short_name , description = description ) res = _pybossa_req ( 'post' , 'project' , payload = project ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
6004	def setup_random_seed ( seed ) : if seed == - 1 : seed = np . random . randint ( 0 , int ( 1e9 ) ) # Use one seed, so all regions have identical column non-uniformity. np . random . seed ( seed )
600	def compute ( self , activeColumns , predictedColumns , inputValue = None , timestamp = None ) : # Start by computing the raw anomaly score. anomalyScore = computeRawAnomalyScore ( activeColumns , predictedColumns ) # Compute final anomaly based on selected mode. if self . _mode == Anomaly . MODE_PURE : score = anomalyScore elif self . _mode == Anomaly . MODE_LIKELIHOOD : if inputValue is None : raise ValueError ( "Selected anomaly mode 'Anomaly.MODE_LIKELIHOOD' " "requires 'inputValue' as parameter to compute() method. " ) probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) # low likelihood -> hi anomaly score = 1 - probability elif self . _mode == Anomaly . MODE_WEIGHTED : probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = anomalyScore * ( 1 - probability ) # Last, do moving-average if windowSize was specified. if self . _movingAverage is not None : score = self . _movingAverage . next ( score ) # apply binary discretization if required if self . _binaryThreshold is not None : if score >= self . _binaryThreshold : score = 1.0 else : score = 0.0 return score
9478	def parse_string ( self , string ) : dom = minidom . parseString ( string ) return self . parse_dom ( dom )
1677	def ResetSection ( self , directive ) : # The name of the current section. self . _section = self . _INITIAL_SECTION # The path of last found header. self . _last_header = '' # Update list of includes. Note that we never pop from the # include list. if directive in ( 'if' , 'ifdef' , 'ifndef' ) : self . include_list . append ( [ ] ) elif directive in ( 'else' , 'elif' ) : self . include_list [ - 1 ] = [ ]
11540	def pin_type ( self , pin ) : if type ( pin ) is list : return [ self . pin_type ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_type ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
12885	def python_value ( self , value ) : if self . field_type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
2402	def gen_bag_feats ( self , e_set ) : if ( hasattr ( self , '_stem_dict' ) ) : sfeats = self . _stem_dict . transform ( e_set . _clean_stem_text ) nfeats = self . _normal_dict . transform ( e_set . _text ) bag_feats = numpy . concatenate ( ( sfeats . toarray ( ) , nfeats . toarray ( ) ) , axis = 1 ) else : raise util_functions . InputError ( self , "Dictionaries must be initialized prior to generating bag features." ) return bag_feats . copy ( )
10152	def _extract_path_from_service ( self , service ) : path_obj = { } path = service . path route_name = getattr ( service , 'pyramid_route' , None ) # handle services that don't create fresh routes, # we still need the paths so we need to grab pyramid introspector to # extract that information if route_name : # avoid failure if someone forgets to pass registry registry = self . pyramid_registry or get_current_registry ( ) route_intr = registry . introspector . get ( 'routes' , route_name ) if route_intr : path = route_intr [ 'pattern' ] else : msg = 'Route `{}` is not found by ' 'pyramid introspector' . format ( route_name ) raise ValueError ( msg ) # handle traverse and subpath as regular parameters # docs.pylonsproject.org/projects/pyramid/en/latest/narr/hybrid.html for subpath_marker in ( '*subpath' , '*traverse' ) : path = path . replace ( subpath_marker , '{subpath}' ) # Extract path parameters parameters = self . parameters . from_path ( path ) if parameters : path_obj [ 'parameters' ] = parameters return path , path_obj
3116	def oauth2_callback ( request ) : if 'error' in request . GET : reason = request . GET . get ( 'error_description' , request . GET . get ( 'error' , '' ) ) reason = html . escape ( reason ) return http . HttpResponseBadRequest ( 'Authorization failed {0}' . format ( reason ) ) try : encoded_state = request . GET [ 'state' ] code = request . GET [ 'code' ] except KeyError : return http . HttpResponseBadRequest ( 'Request missing state or authorization code' ) try : server_csrf = request . session [ _CSRF_KEY ] except KeyError : return http . HttpResponseBadRequest ( 'No existing session for this flow.' ) try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return http . HttpResponseBadRequest ( 'Invalid state parameter.' ) if client_csrf != server_csrf : return http . HttpResponseBadRequest ( 'Invalid CSRF token.' ) flow = _get_flow_for_token ( client_csrf , request ) if not flow : return http . HttpResponseBadRequest ( 'Missing Oauth2 flow.' ) try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : return http . HttpResponseBadRequest ( 'An error has occurred: {0}' . format ( exchange_error ) ) get_storage ( request ) . put ( credentials ) signals . oauth2_authorized . send ( sender = signals . oauth2_authorized , request = request , credentials = credentials ) return shortcuts . redirect ( return_url )
10603	def calculate ( self , * * state ) : T = state [ 'T' ] y_C = state [ 'y_C' ] y_H = state [ 'y_H' ] y_O = state [ 'y_O' ] y_N = state [ 'y_N' ] y_S = state [ 'y_S' ] a = self . _calc_a ( y_C , y_H , y_O , y_N , y_S ) / 1000 # kg/mol result = ( R / a ) * ( 380 * self . _calc_g0 ( 380 / T ) + 3600 * self . _calc_g0 ( 1800 / T ) ) return result
11864	def add ( self , node_spec ) : node = BayesNode ( * node_spec ) assert node . variable not in self . vars assert every ( lambda parent : parent in self . vars , node . parents ) self . nodes . append ( node ) self . vars . append ( node . variable ) for parent in node . parents : self . variable_node ( parent ) . children . append ( node )
9143	def _iterate_managers ( connection , skip ) : for idx , name , manager_cls in _iterate_manage_classes ( skip ) : if name in skip : continue try : manager = manager_cls ( connection = connection ) except TypeError as e : click . secho ( f'Could not instantiate {name}: {e}' , fg = 'red' ) else : yield idx , name , manager
2511	def parse_package ( self , p_term ) : # Check there is a pacakge name if not ( p_term , self . spdx_namespace [ 'name' ] , None ) in self . graph : self . error = True self . logger . log ( 'Package must have a name.' ) # Create dummy package so that we may continue parsing the rest of # the package fields. self . builder . create_package ( self . doc , 'dummy_package' ) else : for _s , _p , o in self . graph . triples ( ( p_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . create_package ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Package name' ) break self . p_pkg_vinfo ( p_term , self . spdx_namespace [ 'versionInfo' ] ) self . p_pkg_fname ( p_term , self . spdx_namespace [ 'packageFileName' ] ) self . p_pkg_suppl ( p_term , self . spdx_namespace [ 'supplier' ] ) self . p_pkg_originator ( p_term , self . spdx_namespace [ 'originator' ] ) self . p_pkg_down_loc ( p_term , self . spdx_namespace [ 'downloadLocation' ] ) self . p_pkg_homepg ( p_term , self . doap_namespace [ 'homepage' ] ) self . p_pkg_chk_sum ( p_term , self . spdx_namespace [ 'checksum' ] ) self . p_pkg_src_info ( p_term , self . spdx_namespace [ 'sourceInfo' ] ) self . p_pkg_verif_code ( p_term , self . spdx_namespace [ 'packageVerificationCode' ] ) self . p_pkg_lic_conc ( p_term , self . spdx_namespace [ 'licenseConcluded' ] ) self . p_pkg_lic_decl ( p_term , self . spdx_namespace [ 'licenseDeclared' ] ) self . p_pkg_lics_info_from_files ( p_term , self . spdx_namespace [ 'licenseInfoFromFiles' ] ) self . p_pkg_comments_on_lics ( p_term , self . spdx_namespace [ 'licenseComments' ] ) self . p_pkg_cr_text ( p_term , self . spdx_namespace [ 'copyrightText' ] ) self . p_pkg_summary ( p_term , self . spdx_namespace [ 'summary' ] ) self . p_pkg_descr ( p_term , self . spdx_namespace [ 'description' ] )
4968	def _validate_program ( self ) : program = self . cleaned_data . get ( self . Fields . PROGRAM ) if not program : return course_runs = get_course_runs_from_program ( program ) try : client = CourseCatalogApiClient ( self . _user , self . _enterprise_customer . site ) available_modes = client . get_common_course_modes ( course_runs ) course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . FAILED_TO_OBTAIN_COURSE_MODES . format ( program_title = program . get ( "title" ) ) ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) if course_mode not in available_modes : raise ValidationError ( ValidationMessages . COURSE_MODE_NOT_AVAILABLE . format ( mode = course_mode , program_title = program . get ( "title" ) , modes = ", " . join ( available_modes ) ) )
13584	def _obj_display ( obj , display = '' ) : result = '' if not display : result = str ( obj ) else : template = Template ( display ) context = Context ( { 'obj' : obj } ) result = template . render ( context ) return result
183	def to_segmentation_map ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : from . segmaps import SegmentationMapOnImage return SegmentationMapOnImage ( self . draw_mask ( image_shape , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
13787	def flush ( self ) : queue = self . queue size = queue . qsize ( ) queue . join ( ) self . log . debug ( 'successfully flushed %s items.' , size )
1732	def is_empty_object ( n , last ) : if n . strip ( ) : return False # seems to be but can be empty code last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
6312	def from_separate ( cls , meta : ProgramDescription , vertex_source , geometry_source = None , fragment_source = None , tess_control_source = None , tess_evaluation_source = None ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , vertex_source , ) if geometry_source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , geometry_source , ) if fragment_source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , fragment_source , ) if tess_control_source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , tess_control_source , ) if tess_evaluation_source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_control_shader , tess_evaluation_source , ) return instance
10379	def calculate_concordance_by_annotation ( graph , annotation , key , cutoff = None ) : return { value : calculate_concordance ( subgraph , key , cutoff = cutoff ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) }
3913	def _on_event ( self , conv_event ) : if isinstance ( conv_event , hangups . ChatMessageEvent ) : self . _typing_statuses [ conv_event . user_id ] = ( hangups . TYPING_TYPE_STOPPED ) self . _update ( )
7609	def get_all_locations ( self , timeout : int = None ) : url = self . api . LOCATIONS return self . _get_model ( url , timeout = timeout )
6583	def _post_start ( self ) : flags = fcntl . fcntl ( self . _process . stdout , fcntl . F_GETFL ) fcntl . fcntl ( self . _process . stdout , fcntl . F_SETFL , flags | os . O_NONBLOCK )
8097	def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
12200	def from_jsonfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . _from_jsonlines ( fp , selector_handler = selector_handler , strict = strict , debug = debug )
8897	def _fsic_queuing_calc ( fsic1 , fsic2 ) : return { instance : fsic2 . get ( instance , 0 ) for instance , counter in six . iteritems ( fsic1 ) if fsic2 . get ( instance , 0 ) < counter }
3162	def all ( self , campaign_id , get_all = False , * * queryparams ) : self . campaign_id = campaign_id self . subscriber_hash = None if get_all : return self . _iterate ( url = self . _build_path ( campaign_id , 'unsubscribed' ) , * * queryparams ) else : return self . _mc_client . _get ( url = self . _build_path ( campaign_id , 'unsubscribed' ) , * * queryparams )
9049	def B ( self ) : return unvec ( self . _vecB . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )
7356	def create_input_peptides_files ( peptides , max_peptides_per_file = None , group_by_length = False ) : if group_by_length : peptide_lengths = { len ( p ) for p in peptides } peptide_groups = { l : [ ] for l in peptide_lengths } for p in peptides : peptide_groups [ len ( p ) ] . append ( p ) else : peptide_groups = { "" : peptides } file_names = [ ] for key , group in peptide_groups . items ( ) : n_peptides = len ( group ) if not max_peptides_per_file : max_peptides_per_file = n_peptides input_file = None for i , p in enumerate ( group ) : if i % max_peptides_per_file == 0 : if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) input_file = make_writable_tempfile ( prefix_number = i // max_peptides_per_file , prefix_name = key , suffix = ".txt" ) input_file . write ( "%s\n" % p ) if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) return file_names
13079	def render ( self , template , * * kwargs ) : kwargs [ "cache_key" ] = "%s" % kwargs [ "url" ] . values ( ) kwargs [ "lang" ] = self . get_locale ( ) kwargs [ "assets" ] = self . assets kwargs [ "main_collections" ] = self . main_collections ( kwargs [ "lang" ] ) kwargs [ "cache_active" ] = self . cache is not None kwargs [ "cache_time" ] = 0 kwargs [ "cache_key" ] , kwargs [ "cache_key_i18n" ] = self . make_cache_keys ( request . endpoint , kwargs [ "url" ] ) kwargs [ "template" ] = template for plugin in self . __plugins_render_views__ : kwargs . update ( plugin . render ( * * kwargs ) ) return render_template ( kwargs [ "template" ] , * * kwargs )
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' # calculate the totla size of the packet incl. header typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) # first write out how much data is there as the header packet += HeronProtocol . pack_int ( datasize ) # next write the type string packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename # reqid packet += reqid . pack ( ) # add the proto packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : # pylint: disable=missing-docstring e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
3043	def _expires_in ( self ) : if self . token_expiry : now = _UTCNOW ( ) if self . token_expiry > now : time_delta = self . token_expiry - now # TODO(orestica): return time_delta.total_seconds() # once dropping support for Python 2.6 return time_delta . days * 86400 + time_delta . seconds else : return 0
139	def to_bounding_box ( self ) : # TODO get rid of this deferred import from imgaug . augmentables . bbs import BoundingBox xx = self . xx yy = self . yy return BoundingBox ( x1 = min ( xx ) , x2 = max ( xx ) , y1 = min ( yy ) , y2 = max ( yy ) , label = self . label )
1076	def _ymd2ord ( year , month , day ) : assert 1 <= month <= 12 , 'month must be in 1..12' dim = _days_in_month ( year , month ) assert 1 <= day <= dim , ( 'day must be in 1..%d' % dim ) return ( _days_before_year ( year ) + _days_before_month ( year , month ) + day )
3204	def create ( self , data ) : if 'operations' not in data : raise KeyError ( 'The batch must have operations' ) for op in data [ 'operations' ] : if 'method' not in op : raise KeyError ( 'The batch operation must have a method' ) if op [ 'method' ] not in [ 'GET' , 'POST' , 'PUT' , 'PATCH' , 'DELETE' ] : raise ValueError ( 'The batch operation method must be one of "GET", "POST", "PUT", "PATCH", ' 'or "DELETE", not {0}' . format ( op [ 'method' ] ) ) if 'path' not in op : raise KeyError ( 'The batch operation must have a path' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
4661	def broadcast ( self , tx = None ) : if tx : # If tx is provided, we broadcast the tx return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
6924	def open ( self , database , user , password , host ) : try : self . connection = pg . connect ( user = user , password = password , database = database , host = host ) LOGINFO ( 'postgres connection successfully ' 'created, using DB %s, user %s' % ( database , user ) ) self . database = database self . user = user except Exception as e : LOGEXCEPTION ( 'postgres connection failed, ' 'using DB %s, user %s' % ( database , user ) ) self . database = None self . user = None
1633	def CheckForNewlineAtEOF ( filename , lines , error ) : # The array lines() was created by adding two newlines to the # original file (go figure), then splitting on \n. # To verify that the file ends in \n, we just have to make sure the # last-but-two element of lines() exists and is empty. if len ( lines ) < 3 or lines [ - 2 ] : error ( filename , len ( lines ) - 2 , 'whitespace/ending_newline' , 5 , 'Could not find a newline character at the end of the file.' )
13649	def get_fuel_prices_within_radius ( self , latitude : float , longitude : float , radius : int , fuel_type : str , brands : Optional [ List [ str ] ] = None ) -> List [ StationPrice ] : if brands is None : brands = [ ] response = requests . post ( '{}/prices/nearby' . format ( API_URL_BASE ) , json = { 'fueltype' : fuel_type , 'latitude' : latitude , 'longitude' : longitude , 'radius' : radius , 'brand' : brands , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) stations = { station [ 'code' ] : Station . deserialize ( station ) for station in data [ 'stations' ] } station_prices = [ ] # type: List[StationPrice] for serialized_price in data [ 'prices' ] : price = Price . deserialize ( serialized_price ) station_prices . append ( StationPrice ( price = price , station = stations [ price . station_code ] ) ) return station_prices
9912	def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
4541	def _on_index ( self , old_index ) : if self . animation : log . debug ( '%s: %s' , self . __class__ . __name__ , self . current_animation . title ) self . frames = self . animation . generate_frames ( False )
5252	def bdib ( self , ticker , start_datetime , end_datetime , event_type , interval , elms = None ) : elms = [ ] if not elms else elms # flush event queue in case previous call errored out logger = _get_logger ( self . debug ) while ( self . _session . tryNextEvent ( ) ) : pass # Create and fill the request for the historical data request = self . refDataService . createRequest ( 'IntradayBarRequest' ) request . set ( 'security' , ticker ) request . set ( 'eventType' , event_type ) request . set ( 'interval' , interval ) # bar interval in minutes request . set ( 'startDateTime' , start_datetime ) request . set ( 'endDateTime' , end_datetime ) for name , val in elms : request . set ( name , val ) logger . info ( 'Sending Request:\n{}' . format ( request ) ) # Send the request self . _session . sendRequest ( request , identity = self . _identity ) # Process received events data = [ ] flds = [ 'open' , 'high' , 'low' , 'close' , 'volume' , 'numEvents' ] for msg in self . _receive_events ( ) : d = msg [ 'element' ] [ 'IntradayBarResponse' ] for bar in d [ 'barData' ] [ 'barTickData' ] : data . append ( bar [ 'barTickData' ] ) data = pd . DataFrame ( data ) . set_index ( 'time' ) . sort_index ( ) . loc [ : , flds ] return data
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
4459	def between ( a , b , inclusive_min = True , inclusive_max = True ) : return RangeValue ( a , b , inclusive_min = inclusive_min , inclusive_max = inclusive_max )
4890	def update_course_runs ( self , course_runs , enterprise_customer , enterprise_context ) : updated_course_runs = [ ] for course_run in course_runs : track_selection_url = utils . get_course_track_selection_url ( course_run = course_run , query_parameters = dict ( enterprise_context , * * utils . get_enterprise_utm_context ( enterprise_customer ) ) , ) enrollment_url = enterprise_customer . get_course_run_enrollment_url ( course_run . get ( 'key' ) ) course_run . update ( { 'enrollment_url' : enrollment_url , 'track_selection_url' : track_selection_url , } ) # Update marketing urls in course metadata to include enterprise related info. marketing_url = course_run . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , * * utils . get_enterprise_utm_context ( enterprise_customer ) ) course_run . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) # Add updated course run to the list. updated_course_runs . append ( course_run ) return updated_course_runs
11023	def gen_key ( self , key ) : b_key = self . _hash_digest ( key ) return self . _hash_val ( b_key , lambda x : x )
1824	def SETZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
8564	def create_loadbalancer ( self , datacenter_id , loadbalancer ) : data = json . dumps ( self . _create_loadbalancer_dict ( loadbalancer ) ) response = self . _perform_request ( url = '/datacenters/%s/loadbalancers' % datacenter_id , method = 'POST' , data = data ) return response
3093	def _create_flow ( self , request_handler ) : if self . flow is None : redirect_uri = request_handler . request . relative_url ( self . _callback_path ) # Usually /oauth2callback self . flow = client . OAuth2WebServerFlow ( self . _client_id , self . _client_secret , self . _scope , redirect_uri = redirect_uri , user_agent = self . _user_agent , auth_uri = self . _auth_uri , token_uri = self . _token_uri , revoke_uri = self . _revoke_uri , * * self . _kwargs )
6420	def readfile ( fn ) : with open ( path . join ( HERE , fn ) , 'r' , encoding = 'utf-8' ) as f : return f . read ( )
7579	def result_files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-*_f" ) repfiles = glob . glob ( reps ) return repfiles
2916	def get_dump ( self , indent = 0 , recursive = True ) : dbg = ( ' ' * indent * 2 ) dbg += '%s/' % self . id dbg += '%s:' % self . thread_id dbg += ' Task of %s' % self . get_name ( ) if self . task_spec . description : dbg += ' (%s)' % self . get_description ( ) dbg += ' State: %s' % self . get_state_name ( ) dbg += ' Children: %s' % len ( self . children ) if recursive : for child in self . children : dbg += '\n' + child . get_dump ( indent + 1 ) return dbg
10274	def prune_mechanism_by_data ( graph , key : Optional [ str ] = None ) -> None : remove_unweighted_leaves ( graph , key = key ) remove_unweighted_sources ( graph , key = key )
2260	def group_items ( items , groupids ) : if callable ( groupids ) : keyfunc = groupids pair_list = ( ( keyfunc ( item ) , item ) for item in items ) else : pair_list = zip ( groupids , items ) # Initialize a dict of lists groupid_to_items = defaultdict ( list ) # Insert each item into the correct group for key , item in pair_list : groupid_to_items [ key ] . append ( item ) return groupid_to_items
12991	def create_hierarchy ( hierarchy , level ) : if level not in hierarchy : hierarchy [ level ] = OrderedDict ( ) return hierarchy [ level ]
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
7674	def slice ( self , start_time , end_time , strict = False ) : # Make sure duration is set in file metadata if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) # Make sure start and end times are within the file start/end times if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) # Create a new jams jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) # trim annotations jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) # adjust dutation jam_sliced . file_metadata . duration = end_time - start_time # Document jam-level trim in top level sandbox if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
1397	def extract_execution_state ( self , topology ) : execution_state = topology . execution_state executionState = { "cluster" : execution_state . cluster , "environ" : execution_state . environ , "role" : execution_state . role , "jobname" : topology . name , "submission_time" : execution_state . submission_time , "submission_user" : execution_state . submission_user , "release_username" : execution_state . release_state . release_username , "release_tag" : execution_state . release_state . release_tag , "release_version" : execution_state . release_state . release_version , "has_physical_plan" : None , "has_tmaster_location" : None , "has_scheduler_location" : None , "extra_links" : [ ] , } for extra_link in self . config . extra_links : link = extra_link . copy ( ) link [ "url" ] = self . config . get_formatted_url ( executionState , link [ EXTRA_LINK_FORMATTER_KEY ] ) executionState [ "extra_links" ] . append ( link ) return executionState
5451	def convert_to_label_chars ( s ) : # We want the results to be user-friendly, not just functional. # So we can't base-64 encode it. # * If upper-case: lower-case it # * If the char is not a standard letter or digit. make it a dash # March 2019 note: underscores are now allowed in labels. # However, removing the conversion of underscores to dashes here would # create inconsistencies between old jobs and new jobs. # With existing code, $USER "jane_doe" has a user-id label of "jane-doe". # If we remove the conversion, the user-id label for new jobs is "jane_doe". # This makes looking up old jobs more complicated. accepted_characters = string . ascii_lowercase + string . digits + '-' def label_char_transform ( char ) : if char in accepted_characters : return char if char in string . ascii_uppercase : return char . lower ( ) return '-' return '' . join ( label_char_transform ( c ) for c in s )
11560	def i2c_stop_reading ( self , address ) : data = [ address , self . I2C_STOP_READING ] self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
1820	def SETP ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
4659	def as_quote ( self , quote ) : if quote == self [ "quote" ] [ "symbol" ] : return self . copy ( ) elif quote == self [ "base" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
2675	def build ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Get the absolute path to the output directory and create it if it doesn't # already exist. dist_directory = cfg . get ( 'dist_directory' , 'dist' ) path_to_dist = os . path . join ( src , dist_directory ) mkdir ( path_to_dist ) # Combine the name of the Lambda function with the current timestamp to use # for the output filename. function_name = cfg . get ( 'function_name' ) output_filename = '{0}-{1}.zip' . format ( timestamp ( ) , function_name ) path_to_temp = mkdtemp ( prefix = 'aws-lambda' ) pip_install_to_target ( path_to_temp , requirements = requirements , local_package = local_package , ) # Hack for Zope. if 'zope' in os . listdir ( path_to_temp ) : print ( 'Zope packages detected; fixing Zope package paths to ' 'make them importable.' , ) # Touch. with open ( os . path . join ( path_to_temp , 'zope/__init__.py' ) , 'wb' ) : pass # Gracefully handle whether ".zip" was included in the filename or not. output_filename = ( '{0}.zip' . format ( output_filename ) if not output_filename . endswith ( '.zip' ) else output_filename ) # Allow definition of source code directories we want to build into our # zipped package. build_config = defaultdict ( * * cfg . get ( 'build' , { } ) ) build_source_directories = build_config . get ( 'source_directories' , '' ) build_source_directories = ( build_source_directories if build_source_directories is not None else '' ) source_directories = [ d . strip ( ) for d in build_source_directories . split ( ',' ) ] files = [ ] for filename in os . listdir ( src ) : if os . path . isfile ( filename ) : if filename == '.DS_Store' : continue if filename == config_file : continue print ( 'Bundling: %r' % filename ) files . append ( os . path . join ( src , filename ) ) elif os . path . isdir ( filename ) and filename in source_directories : print ( 'Bundling directory: %r' % filename ) files . append ( os . path . join ( src , filename ) ) # "cd" into `temp_path` directory. os . chdir ( path_to_temp ) for f in files : if os . path . isfile ( f ) : _ , filename = os . path . split ( f ) # Copy handler file into root of the packages folder. copyfile ( f , os . path . join ( path_to_temp , filename ) ) copystat ( f , os . path . join ( path_to_temp , filename ) ) elif os . path . isdir ( f ) : destination_folder = os . path . join ( path_to_temp , f [ len ( src ) + 1 : ] ) copytree ( f , destination_folder ) # Zip them together into a single file. # TODO: Delete temp directory created once the archive has been compiled. path_to_zip_file = archive ( './' , path_to_dist , output_filename ) return path_to_zip_file
4683	def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : # Filter those keys not for our network if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
12792	def post ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None , listener = None ) : return self . _fetch ( "POST" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , listener = listener , full_return = True )
3217	def get_subnets ( vpc , * * conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
2856	def write ( self , data ) : #check for hardware limit of FT232H and similar MPSSE chips if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) # Build command to write SPI data. command = 0x10 | ( self . lsbfirst << 3 ) | self . write_clock_ve logger . debug ( 'SPI write with command {0:2X}.' . format ( command ) ) # Compute length low and high bytes. # NOTE: Must actually send length minus one because the MPSSE engine # considers 0 a length of 1 and FFFF a length of 65536 # splitting into two lists for two commands to prevent buffer errors data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) # Send command and length, then data, split into two commands, handle for length 1 if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) self . _deassert_cs ( )
4008	def _increase_file_handle_limit ( ) : logging . info ( 'Increasing file handle limit to {}' . format ( constants . FILE_HANDLE_LIMIT ) ) resource . setrlimit ( resource . RLIMIT_NOFILE , ( constants . FILE_HANDLE_LIMIT , resource . RLIM_INFINITY ) )
8187	def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove_node ( n . id )
9418	def document_func_view ( serializer_class = None , response_serializer_class = None , filter_backends = None , permission_classes = None , authentication_classes = None , doc_format_args = list ( ) , doc_format_kwargs = dict ( ) ) : def decorator ( func ) : if serializer_class : func . cls . serializer_class = func . view_class . serializer_class = serializer_class if response_serializer_class : func . cls . response_serializer_class = func . view_class . response_serializer_class = response_serializer_class if filter_backends : func . cls . filter_backends = func . view_class . filter_backends = filter_backends if permission_classes : func . cls . permission_classes = func . view_class . permission_classes = permission_classes if authentication_classes : func . cls . authentication_classes = func . view_class . authentication_classes = authentication_classes if doc_format_args or doc_format_kwargs : func . cls . __doc__ = func . view_class . __doc__ = getdoc ( func ) . format ( * doc_format_args , * * doc_format_kwargs ) return func return decorator
5846	def load_file_as_yaml ( path ) : with open ( path , "r" ) as f : raw_yaml = f . read ( ) parsed_dict = yaml . load ( raw_yaml ) return parsed_dict
5050	def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
1144	def init ( self ) : self . length = 0 self . input = [ ] # Initial 160 bit message digest (5 times 32 bit). self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0
5201	def Operate ( self , command , index , op_type ) : OutstationApplication . process_point_value ( 'Operate' , command , index , op_type ) return opendnp3 . CommandStatus . SUCCESS
9637	def format ( self , record ) : data = record . _raw . copy ( ) # serialize the datetime date as utc string data [ 'time' ] = data [ 'time' ] . isoformat ( ) # stringify exception data if data . get ( 'traceback' ) : data [ 'traceback' ] = self . formatException ( data [ 'traceback' ] ) return json . dumps ( data )
12870	def migrate ( migrator , database , * * kwargs ) : @ migrator . create_table class DataItem ( pw . Model ) : created = pw . DateTimeField ( default = dt . datetime . utcnow ) content = pw . CharField ( )
11495	def list_users ( self , limit = 20 ) : parameters = dict ( ) parameters [ 'limit' ] = limit response = self . request ( 'midas.user.list' , parameters ) return response
13357	def moyennes_glissantes ( df , sur = 8 , rep = 0.75 ) : return pd . rolling_mean ( df , window = sur , min_periods = rep * sur )
718	def emit ( self , modelInfo ) : # Open/init csv file, if needed if self . __csvFileObj is None : # sets up self.__sortedVariableNames and self.__csvFileObj self . __openAndInitCSVFile ( modelInfo ) csv = self . __csvFileObj # Emit model info row to report.csv print >> csv , "%s, " % ( self . __searchJobID ) , print >> csv , "%s, " % ( modelInfo . getModelID ( ) ) , print >> csv , "%s, " % ( modelInfo . statusAsString ( ) ) , if modelInfo . isFinished ( ) : print >> csv , "%s, " % ( modelInfo . getCompletionReason ( ) ) , else : print >> csv , "NA, " , if not modelInfo . isWaitingToStart ( ) : print >> csv , "%s, " % ( modelInfo . getStartTime ( ) ) , else : print >> csv , "NA, " , if modelInfo . isFinished ( ) : dateFormat = "%Y-%m-%d %H:%M:%S" startTime = modelInfo . getStartTime ( ) endTime = modelInfo . getEndTime ( ) print >> csv , "%s, " % endTime , st = datetime . strptime ( startTime , dateFormat ) et = datetime . strptime ( endTime , dateFormat ) print >> csv , "%s, " % ( str ( ( et - st ) . seconds ) ) , else : print >> csv , "NA, " , print >> csv , "NA, " , print >> csv , "%s, " % str ( modelInfo . getModelDescription ( ) ) , print >> csv , "%s, " % str ( modelInfo . getNumRecords ( ) ) , paramLabelsDict = modelInfo . getParamLabels ( ) for key in self . __sortedVariableNames : # Some values are complex structures,.. which need to be represented as # strings if key in paramLabelsDict : print >> csv , "%s, " % ( paramLabelsDict [ key ] ) , else : print >> csv , "None, " , metrics = modelInfo . getReportMetrics ( ) for key in self . __sortedMetricsKeys : value = metrics . get ( key , "NA" ) value = str ( value ) value = value . replace ( "\n" , " " ) print >> csv , "%s, " % ( value ) , print >> csv
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
4035	def cleanwrap ( func ) : def enc ( self , * args , * * kwargs ) : """ Send each item to _cleanup() """ return ( func ( self , item , * * kwargs ) for item in args ) return enc
11912	def fail ( message = None , exit_status = None ) : print ( 'Error:' , message , file = sys . stderr ) sys . exit ( exit_status or 1 )
5282	def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( * * self . get_formset_kwargs ( ) )
7810	def from_der_data ( cls , data ) : # pylint: disable=W0212 logger . debug ( "Decoding DER certificate: {0!r}" . format ( data ) ) if cls . _cert_asn1_type is None : cls . _cert_asn1_type = Certificate ( ) cert = der_decoder . decode ( data , asn1Spec = cls . _cert_asn1_type ) [ 0 ] result = cls ( ) tbs_cert = cert . getComponentByName ( 'tbsCertificate' ) subject = tbs_cert . getComponentByName ( 'subject' ) logger . debug ( "Subject: {0!r}" . format ( subject ) ) result . _decode_subject ( subject ) validity = tbs_cert . getComponentByName ( 'validity' ) result . _decode_validity ( validity ) extensions = tbs_cert . getComponentByName ( 'extensions' ) if extensions : for extension in extensions : logger . debug ( "Extension: {0!r}" . format ( extension ) ) oid = extension . getComponentByName ( 'extnID' ) logger . debug ( "OID: {0!r}" . format ( oid ) ) if oid != SUBJECT_ALT_NAME_OID : continue value = extension . getComponentByName ( 'extnValue' ) logger . debug ( "Value: {0!r}" . format ( value ) ) if isinstance ( value , Any ) : # should be OctetString, but is Any # in pyasn1_modules-0.0.1a value = der_decoder . decode ( value , asn1Spec = OctetString ( ) ) [ 0 ] alt_names = der_decoder . decode ( value , asn1Spec = GeneralNames ( ) ) [ 0 ] logger . debug ( "SubjectAltName: {0!r}" . format ( alt_names ) ) result . _decode_alt_names ( alt_names ) return result
8706	def verify_file ( self , path , destination , verify = 'none' ) : content = from_file ( path ) log . info ( 'Verifying using %s...' % verify ) if verify == 'raw' : data = self . download_file ( destination ) if content != data : log . error ( 'Raw verification failed.' ) raise VerificationError ( 'Verification failed.' ) else : log . info ( 'Verification successful. Contents are identical.' ) elif verify == 'sha1' : #Calculate SHA1 on remote file. Extract just hash from result data = self . __exchange ( 'shafile("' + destination + '")' ) . splitlines ( ) [ 1 ] log . info ( 'Remote SHA1: %s' , data ) #Calculate hash of local data filehashhex = hashlib . sha1 ( content . encode ( ENCODING ) ) . hexdigest ( ) log . info ( 'Local SHA1: %s' , filehashhex ) if data != filehashhex : log . error ( 'SHA1 verification failed.' ) raise VerificationError ( 'SHA1 Verification failed.' ) else : log . info ( 'Verification successful. Checksums match' ) elif verify != 'none' : raise Exception ( verify + ' is not a valid verification method.' )
10233	def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : # Skip create increases edges between enzymes if reactant in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
5473	def trim_display_field ( self , value , max_length ) : if not value : return '' if len ( value ) > max_length : return value [ : max_length - 3 ] + '...' return value
5338	def __upload_title ( self , kibiter_major ) : if kibiter_major == "6" : resource = ".kibana/doc/projectname" data = { "projectname" : { "name" : self . project_name } } mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for dashboard title" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for dashboard title." ) logger . error ( res . json ( ) ) logger . debug ( "Uploading dashboard title" ) res = self . grimoire_con . post ( url , data = json . dumps ( data ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create dashboard title." ) logger . error ( res . json ( ) )
966	def percentOverlap ( x1 , x2 , size ) : nonZeroX1 = np . count_nonzero ( x1 ) nonZeroX2 = np . count_nonzero ( x2 ) minX1X2 = min ( nonZeroX1 , nonZeroX2 ) percentOverlap = 0 if minX1X2 > 0 : percentOverlap = float ( np . dot ( x1 , x2 ) ) / float ( minX1X2 ) return percentOverlap
13021	def process_columns ( self , columns ) : if type ( columns ) == list : self . columns = columns elif type ( columns ) == str : self . columns = [ c . strip ( ) for c in columns . split ( ) ] elif type ( columns ) == IntEnum : self . columns = [ str ( c ) for c in columns ] else : raise RawlException ( "Unknown format for columns" )
7376	async def prepare_request ( self , method , url , headers = None , skip_params = False , proxy = None , * * kwargs ) : if method . lower ( ) == "post" : key = 'data' else : key = 'params' if key in kwargs and not skip_params : request_params = { key : kwargs . pop ( key ) } else : request_params = { } request_params . update ( dict ( method = method . upper ( ) , url = url ) ) coro = self . sign ( * * request_params , skip_params = skip_params , headers = headers ) request_params [ 'headers' ] = await utils . execute ( coro ) request_params [ 'proxy' ] = proxy kwargs . update ( request_params ) return kwargs
6832	def ssh_config ( self , name = '' ) : r = self . local_renderer with self . settings ( hide ( 'running' ) ) : output = r . local ( 'vagrant ssh-config %s' % name , capture = True ) config = { } for line in output . splitlines ( ) [ 1 : ] : key , value = line . strip ( ) . split ( ' ' , 2 ) config [ key ] = value return config
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
398	def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
13557	def get_all_images ( self ) : self_imgs = self . image_set . all ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) return list ( chain ( self_imgs , u_images ) )
11069	def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
9743	async def reboot ( ip_address ) : _ , protocol = await asyncio . get_event_loop ( ) . create_datagram_endpoint ( QRebootProtocol , local_addr = ( ip_address , 0 ) , allow_broadcast = True , reuse_address = True , ) LOG . info ( "Sending reboot on %s" , ip_address ) protocol . send_reboot ( )
11237	def sendreturn ( gen , value ) : try : gen . send ( value ) except StopIteration as e : return stopiter_value ( e ) else : raise RuntimeError ( 'generator did not return as expected' )
10132	def parse_grid ( grid_data ) : try : # Split the grid up. grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) # Grid and column metadata are the first two lines. grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) # First element is the grid metadata ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) # Now parse the rest of the grid accordingly try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
8855	def setup_mnu_style ( self , editor ) : menu = QtWidgets . QMenu ( 'Styles' , self . menuEdit ) group = QtWidgets . QActionGroup ( self ) self . styles_group = group current_style = editor . syntax_highlighter . color_scheme . name group . triggered . connect ( self . on_style_changed ) for s in sorted ( PYGMENTS_STYLES ) : a = QtWidgets . QAction ( menu ) a . setText ( s ) a . setCheckable ( True ) if s == current_style : a . setChecked ( True ) group . addAction ( a ) menu . addAction ( a ) self . menuEdit . addMenu ( menu )
1964	def sys_chroot ( self , path ) : if path not in self . current . memory : return - errno . EFAULT path_s = self . current . read_string ( path ) if not os . path . exists ( path_s ) : return - errno . ENOENT if not os . path . isdir ( path_s ) : return - errno . ENOTDIR return - errno . EPERM
7969	def _remove_io_handler ( self , handler ) : if handler not in self . io_handlers : return self . io_handlers . remove ( handler ) for thread in self . io_threads : if thread . io_handler is handler : thread . stop ( )
13062	def get_siblings ( self , objectId , subreference , passage ) : reffs = [ reff for reff , _ in self . get_reffs ( objectId ) ] if subreference in reffs : index = reffs . index ( subreference ) # Not the first item and not the last one if 0 < index < len ( reffs ) - 1 : return reffs [ index - 1 ] , reffs [ index + 1 ] elif index == 0 and index < len ( reffs ) - 1 : return None , reffs [ 1 ] elif index > 0 and index == len ( reffs ) - 1 : return reffs [ index - 1 ] , None else : return None , None else : return passage . siblingsId
8570	def get_location ( self , location_id , depth = 0 ) : response = self . _perform_request ( '/locations/%s?depth=%s' % ( location_id , depth ) ) return response
2965	def _sm_start ( self , * args , * * kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
4325	def dcshift ( self , shift = 0.0 ) : if not is_number ( shift ) or shift < - 2 or shift > 2 : raise ValueError ( 'shift must be a number between -2 and 2.' ) effect_args = [ 'dcshift' , '{:f}' . format ( shift ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'dcshift' ) return self
11792	def lcv ( var , assignment , csp ) : return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )
11508	def download_item ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision method_url = self . full_url + 'midas.item.download' request = requests . get ( method_url , params = parameters , stream = True , verify = self . _verify_ssl_certificate ) filename = request . headers [ 'content-disposition' ] [ 21 : ] . strip ( '"' ) return filename , request . iter_content ( chunk_size = 10 * 1024 )
3739	def Hf_g ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in ATcT_g . index : methods . append ( ATCT_G ) if CASRN in TRC_gas_data . index and not np . isnan ( TRC_gas_data . at [ CASRN , 'Hf' ] ) : methods . append ( TRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ATCT_G : _Hfg = float ( ATcT_g . at [ CASRN , 'Hf_298K' ] ) elif Method == TRC : _Hfg = float ( TRC_gas_data . at [ CASRN , 'Hf' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Hfg
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
12768	def load_skeleton ( self , filename , pid_params = None ) : self . skeleton = skeleton . Skeleton ( self ) self . skeleton . load ( filename , color = ( 0.3 , 0.5 , 0.9 , 0.8 ) ) if pid_params : self . skeleton . set_pid_params ( * * pid_params ) self . skeleton . erp = 0.1 self . skeleton . cfm = 0
9414	def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
8479	def potential ( self , value ) : if value : self . _potential = True else : self . _potential = False
11303	def invalidate_stored_oembeds ( self , sender , instance , created , * * kwargs ) : ctype = ContentType . objects . get_for_model ( instance ) StoredOEmbed . objects . filter ( object_id = instance . pk , content_type = ctype ) . delete ( )
10082	def _prepare_edit ( self , record ) : data = record . dumps ( ) # Keep current record revision for merging. data [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] = record . revision_id data [ '_deposit' ] [ 'status' ] = 'draft' data [ '$schema' ] = self . build_deposit_schema ( record ) return data
1135	def isfile ( path ) : try : st = os . stat ( path ) except os . error : return False return stat . S_ISREG ( st . st_mode )
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
1443	def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
176	def extract_from_image ( self , image , size = 1 , pad = True , pad_max = None , antialiased = True , prevent_zero_size = True ) : from . bbs import BoundingBox assert image . ndim in [ 2 , 3 ] , ( "Expected image of shape (H,W,[C]), " "got shape %s." % ( image . shape , ) ) if len ( self . coords ) == 0 or size <= 0 : if prevent_zero_size : return np . zeros ( ( 1 , 1 ) + image . shape [ 2 : ] , dtype = image . dtype ) return np . zeros ( ( 0 , 0 ) + image . shape [ 2 : ] , dtype = image . dtype ) xx = self . xx_int yy = self . yy_int # this would probably work if drawing was subpixel-accurate # x1 = np.min(self.coords[:, 0]) - (size / 2) # y1 = np.min(self.coords[:, 1]) - (size / 2) # x2 = np.max(self.coords[:, 0]) + (size / 2) # y2 = np.max(self.coords[:, 1]) + (size / 2) # this works currently with non-subpixel-accurate drawing sizeh = ( size - 1 ) / 2 x1 = np . min ( xx ) - sizeh y1 = np . min ( yy ) - sizeh x2 = np . max ( xx ) + 1 + sizeh y2 = np . max ( yy ) + 1 + sizeh bb = BoundingBox ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 ) if len ( self . coords ) == 1 : return bb . extract_from_image ( image , pad = pad , pad_max = pad_max , prevent_zero_size = prevent_zero_size ) heatmap = self . draw_lines_heatmap_array ( image . shape [ 0 : 2 ] , alpha = 1.0 , size = size , antialiased = antialiased ) if image . ndim == 3 : heatmap = np . atleast_3d ( heatmap ) image_masked = image . astype ( np . float32 ) * heatmap extract = bb . extract_from_image ( image_masked , pad = pad , pad_max = pad_max , prevent_zero_size = prevent_zero_size ) return np . clip ( np . round ( extract ) , 0 , 255 ) . astype ( np . uint8 )
4064	def add_tags ( self , item , * tags ) : # Make sure there's a tags field, or add one try : assert item [ "data" ] [ "tags" ] except AssertionError : item [ "data" ] [ "tags" ] = list ( ) for tag in tags : item [ "data" ] [ "tags" ] . append ( { "tag" : "%s" % tag } ) # make sure everything's OK assert self . check_items ( [ item ] ) return self . update_item ( item )
5809	def parse_handshake_messages ( data ) : pointer = 0 data_len = len ( data ) while pointer < data_len : length = int_from_bytes ( data [ pointer + 1 : pointer + 4 ] ) yield ( data [ pointer : pointer + 1 ] , data [ pointer + 4 : pointer + 4 + length ] ) pointer += 4 + length
9166	def task ( * * kwargs ) : def wrapper ( wrapped ) : def callback ( scanner , name , obj ) : celery_app = scanner . config . registry . celery_app celery_app . task ( * * kwargs ) ( obj ) venusian . attach ( wrapped , callback ) return wrapped return wrapper
356	def save_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] ckpt_file = os . path . join ( save_dir , mode_name ) if var_list == [ ] : var_list = tf . global_variables ( ) logging . info ( "[*] save %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) saver = tf . train . Saver ( var_list ) saver . save ( sess , ckpt_file , global_step = global_step )
5102	def get_edge_type ( self , edge_type ) : edges = [ ] for e in self . edges ( ) : if self . adj [ e [ 0 ] ] [ e [ 1 ] ] . get ( 'edge_type' ) == edge_type : edges . append ( e ) return edges
8789	def _pop ( self , model ) : tags = [ ] # collect any exsiting tags with matching prefix for tag in model . tags : if self . is_tag ( tag ) : tags . append ( tag ) # remove collected tags from model if tags : for tag in tags : model . tags . remove ( tag ) return tags
12219	def _make_all_matchers ( cls , parameters ) : for name , param in parameters : annotation = param . annotation if annotation is not Parameter . empty : yield name , cls . _make_param_matcher ( annotation , param . kind )
1189	def _randbelow ( self , n ) : # TODO # change once int.bit_length is implemented. # k = n.bit_length() k = _int_bit_length ( n ) r = self . getrandbits ( k ) while r >= n : r = self . getrandbits ( k ) return r
1159	def wait ( self , timeout = None ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot wait on un-acquired lock" ) waiter = _allocate_lock ( ) waiter . acquire ( ) self . __waiters . append ( waiter ) saved_state = self . _release_save ( ) try : # restore state no matter what (e.g., KeyboardInterrupt) if timeout is None : waiter . acquire ( ) if __debug__ : self . _note ( "%s.wait(): got it" , self ) else : # Balancing act: We can't afford a pure busy loop, so we # have to sleep; but if we sleep the whole timeout time, # we'll be unresponsive. The scheme here sleeps very # little at first, longer as time goes on, but never longer # than 20 times per second (or the timeout time remaining). endtime = _time ( ) + timeout delay = 0.0005 # 500 us -> initial delay of 1 ms while True : gotit = waiter . acquire ( 0 ) if gotit : break remaining = endtime - _time ( ) if remaining <= 0 : break delay = min ( delay * 2 , remaining , .05 ) _sleep ( delay ) if not gotit : if __debug__ : self . _note ( "%s.wait(%s): timed out" , self , timeout ) try : self . __waiters . remove ( waiter ) except ValueError : pass else : if __debug__ : self . _note ( "%s.wait(%s): got it" , self , timeout ) finally : self . _acquire_restore ( saved_state )
3963	def start_local_env ( recreate_containers ) : assembled_spec = spec_assembler . get_assembled_specs ( ) required_absent_assets = virtualbox . required_absent_assets ( assembled_spec ) if required_absent_assets : raise RuntimeError ( 'Assets {} are specified as required but are not set. Set them with `dusty assets set`' . format ( required_absent_assets ) ) docker_ip = virtualbox . get_docker_vm_ip ( ) # Stop will fail if we've never written a Composefile before if os . path . exists ( constants . COMPOSEFILE_PATH ) : try : stop_apps_or_services ( rm_containers = recreate_containers ) except CalledProcessError as e : log_to_client ( "WARNING: docker-compose stop failed" ) log_to_client ( str ( e ) ) daemon_warnings . clear_namespace ( 'disk' ) df_info = virtualbox . get_docker_vm_disk_info ( as_dict = True ) if 'M' in df_info [ 'free' ] or 'K' in df_info [ 'free' ] : warning_msg = 'VM is low on disk. Available disk: {}' . format ( df_info [ 'free' ] ) daemon_warnings . warn ( 'disk' , warning_msg ) log_to_client ( warning_msg ) log_to_client ( "Compiling together the assembled specs" ) active_repos = spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) log_to_client ( "Compiling the port specs" ) port_spec = port_spec_compiler . get_port_spec_document ( assembled_spec , docker_ip ) log_to_client ( "Compiling the nginx config" ) docker_bridge_ip = virtualbox . get_docker_bridge_ip ( ) nginx_config = nginx_compiler . get_nginx_configuration_spec ( port_spec , docker_bridge_ip ) log_to_client ( "Creating setup and script bash files" ) make_up_command_files ( assembled_spec , port_spec ) log_to_client ( "Compiling docker-compose config" ) compose_config = compose_compiler . get_compose_dict ( assembled_spec , port_spec ) log_to_client ( "Saving port forwarding to hosts file" ) hosts . update_hosts_file_from_port_spec ( port_spec ) log_to_client ( "Configuring NFS" ) nfs . configure_nfs ( ) log_to_client ( "Saving updated nginx config to the VM" ) nginx . update_nginx_from_config ( nginx_config ) log_to_client ( "Saving Docker Compose config and starting all containers" ) compose . update_running_containers_from_spec ( compose_config , recreate_containers = recreate_containers ) log_to_client ( "Your local environment is now started!" )
407	def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )
3669	def Rachford_Rice_flash_error ( V_over_F , zs , Ks ) : return sum ( [ zi * ( Ki - 1. ) / ( 1. + V_over_F * ( Ki - 1. ) ) for Ki , zi in zip ( Ks , zs ) ] )
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
10557	def login ( self , oauth_filename = "oauth" , uploader_id = None ) : cls_name = type ( self ) . __name__ oauth_cred = os . path . join ( os . path . dirname ( OAUTH_FILEPATH ) , oauth_filename + '.cred' ) try : if not self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) : try : self . api . perform_oauth ( storage_filepath = oauth_cred ) except OSError : logger . exception ( "\nUnable to login with specified oauth code." ) self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) except ( OSError , ValueError ) : logger . exception ( "{} authentication failed." . format ( cls_name ) ) return False if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
6950	def jhk_to_sdssi ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSI_JHK , SDSSI_JH , SDSSI_JK , SDSSI_HK , SDSSI_J , SDSSI_H , SDSSI_K )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : """ Allow the passing of parameters to require_at_least_one_query_parameter. """ @ wraps ( view ) def wrapper ( request , * args , * * kwargs ) : """ Checks for the existence of the specified query parameters, raises a ValidationError if none of them were included in the request. """ requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , * * kwargs ) return wrapper return outer_wrapper
4370	def emit ( self , event , * args , * * kwargs ) : callback = kwargs . pop ( 'callback' , None ) if kwargs : raise ValueError ( "emit() only supports positional argument, to stay " "compatible with the Socket.IO protocol. You can " "however pass in a dictionary as the first argument" ) pkt = dict ( type = "event" , name = event , args = args , endpoint = self . ns_name ) if callback : # By passing 'data', we indicate that we *want* an explicit ack # by the client code, not an automatic as with send(). pkt [ 'ack' ] = 'data' pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
7120	def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDict ( obj ) elif isinstance ( obj , list ) : # must mutate and not just reassign, otherwise it will # just use original object mutable/immutable for i , item in enumerate ( obj ) : if isinstance ( item , dict ) and not isinstance ( item , DotDict ) : obj [ i ] = DotDict ( item ) return obj
11868	def strip_minidom_whitespace ( node ) : for child in node . childNodes : if child . nodeType == Node . TEXT_NODE : if child . nodeValue : child . nodeValue = child . nodeValue . strip ( ) elif child . nodeType == Node . ELEMENT_NODE : strip_minidom_whitespace ( child )
9398	def exit ( self ) : if self . _engine : self . _engine . repl . terminate ( ) self . _engine = None
5110	def simulate ( self , n = 1 , t = None , nA = None , nD = None ) : if t is None and nD is None and nA is None : for dummy in range ( n ) : self . next_event ( ) elif t is not None : then = self . _current_t + t while self . _current_t < then and self . _time < infty : self . next_event ( ) elif nD is not None : num_departures = self . num_departures + nD while self . num_departures < num_departures and self . _time < infty : self . next_event ( ) elif nA is not None : num_arrivals = self . _oArrivals + nA while self . _oArrivals < num_arrivals and self . _time < infty : self . next_event ( )
131	def is_partly_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = False )
2480	def datetime_from_iso_format ( string ) : match = DATE_ISO_REGEX . match ( string ) if match : date = datetime . datetime ( year = int ( match . group ( DATE_ISO_YEAR_GRP ) ) , month = int ( match . group ( DATE_ISO_MONTH_GRP ) ) , day = int ( match . group ( DATE_ISO_DAY_GRP ) ) , hour = int ( match . group ( DATE_ISO_HOUR_GRP ) ) , second = int ( match . group ( DATE_ISO_SEC_GRP ) ) , minute = int ( match . group ( DATE_ISO_MIN_GRP ) ) ) return date else : return None
2991	def symbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( symbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
3390	def validate ( self , samples ) : samples = np . atleast_2d ( samples ) prob = self . problem if samples . shape [ 1 ] == len ( self . model . reactions ) : S = create_stoichiometric_matrix ( self . model ) b = np . array ( [ self . model . constraints [ m . id ] . lb for m in self . model . metabolites ] ) bounds = np . array ( [ r . bounds for r in self . model . reactions ] ) . T elif samples . shape [ 1 ] == len ( self . model . variables ) : S = prob . equalities b = prob . b bounds = prob . variable_bounds else : raise ValueError ( "Wrong number of columns. samples must have a " "column for each flux or variable defined in the " "model!" ) feasibility = np . abs ( S . dot ( samples . T ) . T - b ) . max ( axis = 1 ) lb_error = ( samples - bounds [ 0 , ] ) . min ( axis = 1 ) ub_error = ( bounds [ 1 , ] - samples ) . min ( axis = 1 ) if ( samples . shape [ 1 ] == len ( self . model . variables ) and prob . inequalities . shape [ 0 ] ) : consts = prob . inequalities . dot ( samples . T ) lb_error = np . minimum ( lb_error , ( consts - prob . bounds [ 0 , ] ) . min ( axis = 1 ) ) ub_error = np . minimum ( ub_error , ( prob . bounds [ 1 , ] - consts ) . min ( axis = 1 ) ) valid = ( ( feasibility < self . feasibility_tol ) & ( lb_error > - self . bounds_tol ) & ( ub_error > - self . bounds_tol ) ) codes = np . repeat ( "" , valid . shape [ 0 ] ) . astype ( np . dtype ( ( str , 3 ) ) ) codes [ valid ] = "v" codes [ lb_error <= - self . bounds_tol ] = np . char . add ( codes [ lb_error <= - self . bounds_tol ] , "l" ) codes [ ub_error <= - self . bounds_tol ] = np . char . add ( codes [ ub_error <= - self . bounds_tol ] , "u" ) codes [ feasibility > self . feasibility_tol ] = np . char . add ( codes [ feasibility > self . feasibility_tol ] , "e" ) return codes
11003	def psffunc ( self , * args , * * kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_linescan_psf else : func = psfcalc . calculate_linescan_psf return func ( * args , * * kwargs )
3170	def send ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/send' ) )
4863	def to_representation ( self , instance ) : request = self . context [ 'request' ] enterprise_customer = instance . enterprise_customer representation = super ( EnterpriseCustomerCatalogDetailSerializer , self ) . to_representation ( instance ) # Retrieve the EnterpriseCustomerCatalog search results from the discovery service. paginated_content = instance . get_paginated_content ( request . GET ) count = paginated_content [ 'count' ] search_results = paginated_content [ 'results' ] for item in search_results : content_type = item [ 'content_type' ] marketing_url = item . get ( 'marketing_url' ) if marketing_url : item [ 'marketing_url' ] = utils . update_query_parameters ( marketing_url , utils . get_enterprise_utm_context ( enterprise_customer ) ) # Add the Enterprise enrollment URL to each content item returned from the discovery service. if content_type == 'course' : item [ 'enrollment_url' ] = instance . get_course_enrollment_url ( item [ 'key' ] ) if content_type == 'courserun' : item [ 'enrollment_url' ] = instance . get_course_run_enrollment_url ( item [ 'key' ] ) if content_type == 'program' : item [ 'enrollment_url' ] = instance . get_program_enrollment_url ( item [ 'uuid' ] ) # Build pagination URLs previous_url = None next_url = None page = int ( request . GET . get ( 'page' , '1' ) ) request_uri = request . build_absolute_uri ( ) if paginated_content [ 'previous' ] : previous_url = utils . update_query_parameters ( request_uri , { 'page' : page - 1 } ) if paginated_content [ 'next' ] : next_url = utils . update_query_parameters ( request_uri , { 'page' : page + 1 } ) representation [ 'count' ] = count representation [ 'previous' ] = previous_url representation [ 'next' ] = next_url representation [ 'results' ] = search_results return representation
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
4732	def generate_rt_pic ( process_data , para_meter , scale ) : pic_path = para_meter [ 'filename' ] + '.png' plt . figure ( figsize = ( 5.6 * scale , 3.2 * scale ) ) for key in process_data . keys ( ) : plt . plot ( process_data [ key ] [ : , 0 ] , process_data [ key ] [ : , 1 ] , label = str ( key ) ) plt . title ( para_meter [ 'title' ] ) plt . xlabel ( para_meter [ 'x_axis_name' ] ) plt . ylabel ( para_meter [ 'y_axis_name' ] ) plt . legend ( loc = 'upper left' ) plt . savefig ( pic_path ) return pic_path
5972	def generate_submit_scripts ( templates , prefix = None , deffnm = 'md' , jobname = 'MD' , budget = None , mdrun_opts = None , walltime = 1.0 , jobarray_string = None , startdir = None , npme = None , * * kwargs ) : if not jobname [ 0 ] . isalpha ( ) : jobname = 'MD_' + jobname wmsg = "To make the jobname legal it must start with a letter: changed to {0!r}" . format ( jobname ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) if prefix is None : prefix = "" if mdrun_opts is not None : mdrun_opts = '"' + str ( mdrun_opts ) + '"' # TODO: could test if quotes already present dirname = kwargs . pop ( 'dirname' , os . path . curdir ) wt = Timedelta ( hours = walltime ) walltime = wt . strftime ( "%h:%M:%S" ) wall_hours = wt . ashours def write_script ( template ) : submitscript = os . path . join ( dirname , prefix + os . path . basename ( template ) ) logger . info ( "Setting up queuing system script {submitscript!r}..." . format ( * * vars ( ) ) ) # These substitution rules are documented for the user in the module doc string qsystem = detect_queuing_system ( template ) if qsystem is not None and ( qsystem . name == 'Slurm' ) : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-J)' , '((?<=-J\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-t)' , '(?<=-t\s)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , # only replace literal "" ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] else : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-N|job_name)' , '((?<=-N\s)|(?<=job_name\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-l walltime|wall_clock_limit)' , '(?<==)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , # only replace literal "" ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] if ext in ( '.sh' , '.csh' , '.bash' ) : os . chmod ( submitscript , 0o755 ) return submitscript return [ write_script ( template ) for template in config . get_templates ( templates ) ]
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
7662	def slice ( self , start_time , end_time , strict = False ) : # start by trimming the annotation sliced_ann = self . trim ( start_time , end_time , strict = strict ) raw_data = sliced_ann . pop_data ( ) # now adjust the start time of the annotation and the observations it # contains. for obs in raw_data : new_time = max ( 0 , obs . time - start_time ) # if obs.time > start_time, # duration doesn't change # if obs.time < start_time, # duration shrinks by start_time - obs.time sliced_ann . append ( time = new_time , duration = obs . duration , value = obs . value , confidence = obs . confidence ) ref_time = sliced_ann . time slice_start = ref_time slice_end = ref_time + sliced_ann . duration if 'slice' not in sliced_ann . sandbox . keys ( ) : sliced_ann . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ] ) else : sliced_ann . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ) # Update the timing for the sliced annotation sliced_ann . time = max ( 0 , ref_time - start_time ) return sliced_ann
5058	def get_notification_subject_line ( course_name , template_configuration = None ) : stock_subject_template = _ ( 'You\'ve been enrolled in {course_name}!' ) default_subject_template = getattr ( settings , 'ENTERPRISE_ENROLLMENT_EMAIL_DEFAULT_SUBJECT_LINE' , stock_subject_template , ) if template_configuration is not None and template_configuration . subject_line : final_subject_template = template_configuration . subject_line else : final_subject_template = default_subject_template try : return final_subject_template . format ( course_name = course_name ) except KeyError : pass try : return default_subject_template . format ( course_name = course_name ) except KeyError : return stock_subject_template . format ( course_name = course_name )
13796	def handle_rereduce ( self , reduce_function_names , values ) : # This gets a large list of reduction functions, given their names. reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , * * kwargs : None ) # This gets the list of results from those functions. results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
6591	def receive ( self ) : ret = [ ] # a list of (pkgid, result) while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) # TODO: we should check if Python is at least 3.7 and sort by kwargs # keys otherwise. Should we use hash_data for key generation if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : # nocover # We can sort because they keys are gaurenteed to be strings kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
481	def main_restore_embedding_layer ( ) : # Step 1: Build the embedding matrix and load the existing embedding matrix. vocabulary_size = 50000 embedding_size = 128 model_file_name = "model_word2vec_50k_128" batch_size = None print ( "Load existing embedding matrix and dictionaries" ) all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) data = all_var [ 'data' ] count = all_var [ 'count' ] dictionary = all_var [ 'dictionary' ] reverse_dictionary = all_var [ 'reverse_dictionary' ] tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) del all_var , data , count load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) # sess.run(tf.global_variables_initializer()) sess . run ( tf . global_variables_initializer ( ) ) tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) emb_net . print_params ( ) emb_net . print_layers ( ) # Step 2: Input word(s), output the word vector(s). word = b'hello' word_id = dictionary [ word ] print ( 'word_id:' , word_id ) words = [ b'i' , b'am' , b'tensor' , b'layer' ] word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) print ( 'word_ids:' , word_ids ) print ( 'context:' , context ) vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) print ( 'vector:' , vector . shape ) vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) print ( 'vectors:' , vectors . shape )
5038	def enroll_user ( cls , enterprise_customer , user , course_mode , * course_ids ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enrollment_client = EnrollmentApiClient ( ) succeeded = True for course_id in course_ids : try : enrollment_client . enroll_user_in_course ( user . username , course_id , course_mode ) except HttpClientError as exc : # Check if user is already enrolled then we should ignore exception if cls . is_user_enrolled ( user , course_id , course_mode ) : succeeded = True else : succeeded = False default_message = 'No error message provided' try : error_message = json . loads ( exc . content . decode ( ) ) . get ( 'message' , default_message ) except ValueError : error_message = default_message logging . error ( 'Error while enrolling user %(user)s: %(message)s' , dict ( user = user . username , message = error_message ) ) if succeeded : __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id ) if created : track_enrollment ( 'admin-enrollment' , user . id , course_id ) return succeeded
5987	def compute_deflections_at_next_plane ( plane_index , total_planes ) : if plane_index < total_planes - 1 : return True elif plane_index == total_planes - 1 : return False else : raise exc . RayTracingException ( 'A galaxy was not correctly allocated its previous / next redshifts' )
3705	def Townsend_Hales ( T , Tc , Vc , omega ) : Tr = T / Tc return Vc / ( 1 + 0.85 * ( 1 - Tr ) + ( 1.692 + 0.986 * omega ) * ( 1 - Tr ) ** ( 1 / 3. ) )
2153	def config_from_environment ( ) : kwargs = { } for k in CONFIG_OPTIONS : env = 'TOWER_' + k . upper ( ) v = os . getenv ( env , None ) if v is not None : kwargs [ k ] = v return kwargs
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
4256	def get_compressed_filename ( self , filename ) : if not os . path . splitext ( filename ) [ 1 ] [ 1 : ] in self . suffixes_to_compress : return False file_stats = None compressed_stats = None compressed_filename = '{}.{}' . format ( filename , self . suffix ) try : file_stats = os . stat ( filename ) compressed_stats = os . stat ( compressed_filename ) except OSError : # FileNotFoundError is for Python3 only pass if file_stats and compressed_stats : return ( compressed_filename if file_stats . st_mtime > compressed_stats . st_mtime else False ) else : return compressed_filename
10698	def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
5376	def outputs_are_present ( outputs ) : # outputs are OutputFileParam (see param_util.py) # If outputs contain a pattern, then there is no way for `dsub` to verify # that *all* output is present. The best that `dsub` can do is to verify # that *some* output was created for each such parameter. for o in outputs : if not o . value : continue if o . recursive : if not folder_exists ( o . value ) : return False else : if not simple_pattern_exists_in_gcs ( o . value ) : return False return True
4919	def course_run_detail ( self , request , pk , course_id ) : # pylint: disable=invalid-name,unused-argument enterprise_customer_catalog = self . get_object ( ) course_run = enterprise_customer_catalog . get_course_run ( course_id ) if not course_run : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseRunDetailSerializer ( course_run , context = context ) return Response ( serializer . data )
3342	def send_status_response ( environ , start_response , e , add_headers = None , is_head = False ) : status = get_http_status_string ( e ) headers = [ ] if add_headers : headers . extend ( add_headers ) # if 'keep-alive' in environ.get('HTTP_CONNECTION', '').lower(): # headers += [ # ('Connection', 'keep-alive'), # ] if e in ( HTTP_NOT_MODIFIED , HTTP_NO_CONTENT ) : # See paste.lint: these code don't have content start_response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get_rfc1123_time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP_OK , HTTP_CREATED ) : e = DAVError ( e ) assert isinstance ( e , DAVError ) content_type , body = e . get_response_page ( ) if is_head : body = compat . b_empty assert compat . is_bytes ( body ) , body # If not, Content-Length is wrong! start_response ( status , [ ( "Content-Type" , content_type ) , ( "Date" , get_rfc1123_time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
13792	def get_function ( function_name ) : module , basename = str ( function_name ) . rsplit ( '.' , 1 ) try : return getattr ( __import__ ( module , fromlist = [ basename ] ) , basename ) except ( ImportError , AttributeError ) : raise FunctionNotFound ( function_name )
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
6354	def _apply_rule_if_compat ( self , phonetic , target , language_arg ) : candidate = phonetic + target if '[' not in candidate : # no attributes so we need test no further return candidate # expand the result, converting incompatible attributes to [0] candidate = self . _expand_alternates ( candidate ) candidate_array = candidate . split ( '|' ) # drop each alternative that has incompatible attributes candidate = '' found = False for i in range ( len ( candidate_array ) ) : this_candidate = candidate_array [ i ] if language_arg != 1 : this_candidate = self . _normalize_lang_attrs ( this_candidate + '[' + str ( language_arg ) + ']' , False ) if this_candidate != '[0]' : found = True if candidate : candidate += '|' candidate += this_candidate # return false if no compatible alternatives remain if not found : return None # return the result of applying the rule if '|' in candidate : candidate = '(' + candidate + ')' return candidate
12969	def get ( self , pk , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hgetall ( key ) if type ( res ) != dict or not len ( res . keys ( ) ) : return None res [ '_id' ] = pk ret = self . _redisResultToObj ( res ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
12457	def iteritems ( data , * * kwargs ) : return iter ( data . items ( * * kwargs ) ) if IS_PY3 else data . iteritems ( * * kwargs )
13682	def get_json_tuples ( self , prettyprint = False , translate = True ) : j = self . get_json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
8150	def _frame_limit ( self , start_time ) : if self . _speed : completion_time = time ( ) exc_time = completion_time - start_time sleep_for = ( 1.0 / abs ( self . _speed ) ) - exc_time if sleep_for > 0 : sleep ( sleep_for )
9705	def run ( self ) : while self . isRunning . is_set ( ) : try : try : # self.checkTUN() self . monitorTUN ( ) except timeout_decorator . TimeoutError as error : # No data received so just move on pass self . checkSerial ( ) except KeyboardInterrupt : break
11710	def request ( self , path , data = None , headers = None , method = None ) : if isinstance ( data , str ) : data = data . encode ( 'utf-8' ) response = urlopen ( self . _request ( path , data = data , headers = headers , method = method ) ) self . _set_session_cookie ( response ) return response
11903	def static ( * * kwargs ) : def wrap ( fn ) : fn . func_globals [ 'static' ] = fn fn . __dict__ . update ( kwargs ) return fn return wrap
11289	def strip_xml_namespace ( root ) : try : root . tag = root . tag . split ( '}' ) [ 1 ] except IndexError : pass for element in root . getchildren ( ) : strip_xml_namespace ( element )
1623	def FindEndOfExpressionInLine ( line , startpos , stack ) : for i in xrange ( startpos , len ( line ) ) : char = line [ i ] if char in '([{' : # Found start of parenthesized expression, push to expression stack stack . append ( char ) elif char == '<' : # Found potential start of template argument list if i > 0 and line [ i - 1 ] == '<' : # Left shift operator if stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) elif i > 0 and Search ( r'\boperator\s*$' , line [ 0 : i ] ) : # operator<, don't add to stack continue else : # Tentative start of template argument list stack . append ( '<' ) elif char in ')]}' : # Found end of parenthesized expression. # # If we are currently expecting a matching '>', the pending '<' # must have been an operator. Remove them from expression stack. while stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) if ( ( stack [ - 1 ] == '(' and char == ')' ) or ( stack [ - 1 ] == '[' and char == ']' ) or ( stack [ - 1 ] == '{' and char == '}' ) ) : stack . pop ( ) if not stack : return ( i + 1 , None ) else : # Mismatched parentheses return ( - 1 , None ) elif char == '>' : # Found potential end of template argument list. # Ignore "->" and operator functions if ( i > 0 and ( line [ i - 1 ] == '-' or Search ( r'\boperator\s*$' , line [ 0 : i - 1 ] ) ) ) : continue # Pop the stack if there is a matching '<'. Otherwise, ignore # this '>' since it must be an operator. if stack : if stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( i + 1 , None ) elif char == ';' : # Found something that look like end of statements. If we are currently # expecting a '>', the matching '<' must have been an operator, since # template argument list should not contain statements. while stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) # Did not find end of expression or unbalanced parentheses on this line return ( - 1 , stack )
4812	def tokenize ( text , custom_dict = None ) : global TOKENIZER if not TOKENIZER : TOKENIZER = DeepcutTokenizer ( ) return TOKENIZER . tokenize ( text , custom_dict = custom_dict )
243	def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
2327	def orient_undirected_graph ( self , data , graph ) : # Building setup w/ arguments. self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) whitelist = DataFrame ( list ( nx . edges ( graph ) ) , columns = [ "from" , "to" ] ) blacklist = DataFrame ( list ( nx . edges ( nx . DiGraph ( DataFrame ( - nx . adj_matrix ( graph , weight = None ) . to_dense ( ) + 1 , columns = list ( graph . nodes ( ) ) , index = list ( graph . nodes ( ) ) ) ) ) ) , columns = [ "from" , "to" ] ) results = self . _run_bnlearn ( data , whitelist = whitelist , blacklist = blacklist , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
13010	def print_line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) except ValueError : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IOError : sys . exit ( 0 )
11156	def print_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . size ) for p in self . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
9363	def user_name ( with_num = False ) : result = first_name ( ) if with_num : result += str ( random . randint ( 63 , 94 ) ) return result . lower ( )
9179	def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
11434	def _tag_matches_pattern ( tag , pattern ) : for char1 , char2 in zip ( tag , pattern ) : if char2 not in ( '%' , char1 ) : return False return True
5512	def connect ( ) : ftp_class = ftplib . FTP if not SSL else ftplib . FTP_TLS ftp = ftp_class ( timeout = TIMEOUT ) ftp . connect ( HOST , PORT ) ftp . login ( USER , PASSWORD ) if SSL : ftp . prot_p ( ) # secure data connection return ftp
2219	def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
1589	def set_topology_context ( self , metrics_collector ) : Log . debug ( "Setting topology context" ) cluster_config = self . get_topology_config ( ) cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) task_to_component_map = self . _get_task_to_comp_map ( ) self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , self . my_task_id , metrics_collector , self . topology_pex_abs_path )
1651	def _ClassifyInclude ( fileinfo , include , is_system ) : # This is a list of all standard c++ header files, except # those already checked for above. is_cpp_h = include in _CPP_HEADERS # Headers with C++ extensions shouldn't be considered C system headers if is_system and os . path . splitext ( include ) [ 1 ] in [ '.hpp' , '.hxx' , '.h++' ] : is_system = False if is_system : if is_cpp_h : return _CPP_SYS_HEADER else : return _C_SYS_HEADER # If the target file and the include we're checking share a # basename when we drop common extensions, and the include # lives in . , then it's likely to be owned by the target file. target_dir , target_base = ( os . path . split ( _DropCommonSuffixes ( fileinfo . RepositoryName ( ) ) ) ) include_dir , include_base = os . path . split ( _DropCommonSuffixes ( include ) ) target_dir_pub = os . path . normpath ( target_dir + '/../public' ) target_dir_pub = target_dir_pub . replace ( '\\' , '/' ) if target_base == include_base and ( include_dir == target_dir or include_dir == target_dir_pub ) : return _LIKELY_MY_HEADER # If the target and include share some initial basename # component, it's possible the target is implementing the # include, so it's allowed to be first, but we'll never # complain if it's not there. target_first_component = _RE_FIRST_COMPONENT . match ( target_base ) include_first_component = _RE_FIRST_COMPONENT . match ( include_base ) if ( target_first_component and include_first_component and target_first_component . group ( 0 ) == include_first_component . group ( 0 ) ) : return _POSSIBLE_MY_HEADER return _OTHER_HEADER
8551	def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response
6251	def create_transformation ( self , rotation = None , translation = None ) : mat = None if rotation is not None : mat = Matrix44 . from_eulers ( Vector3 ( rotation ) ) if translation is not None : trans = matrix44 . create_from_translation ( Vector3 ( translation ) ) if mat is None : mat = trans else : mat = matrix44 . multiply ( mat , trans ) return mat
4797	def is_before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise TypeError ( 'val must be datetime, but was type <%s>' % type ( self . val ) . __name__ ) if type ( other ) is not datetime . datetime : raise TypeError ( 'given arg must be datetime, but was type <%s>' % type ( other ) . __name__ ) if self . val >= other : self . _err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self
13168	def children ( self , name = None , reverse = False ) : elems = self . _children if reverse : elems = reversed ( elems ) for elem in elems : if name is None or elem . tagname == name : yield elem
7155	def raw ( prompt , * args , * * kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'type' , str ) default = kwargs . get ( 'default' , '' ) with stdout_redirected ( sys . stderr ) : while True : try : if kwargs . get ( 'secret' , False ) : answer = getpass . getpass ( prompt ) elif sys . version_info < ( 3 , 0 ) : answer = raw_input ( prompt ) else : answer = input ( prompt ) if not answer : answer = default if answer == go_back : raise QuestionnaireGoBack return type_ ( answer ) except ValueError : eprint ( '\n`{}` is not a valid `{}`\n' . format ( answer , type_ ) )
123	def terminate ( self ) : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) self . nb_workers_finished = len ( self . workers ) if not self . queue_result . _closed : self . queue_result . close ( ) time . sleep ( 0.01 )
8425	def grey_pal ( start = 0.2 , end = 0.8 ) : gamma = 2.2 ends = ( ( 0.0 , start , start ) , ( 1.0 , end , end ) ) cdict = { 'red' : ends , 'green' : ends , 'blue' : ends } grey_cmap = mcolors . LinearSegmentedColormap ( 'grey' , cdict ) def continuous_grey_palette ( n ) : colors = [ ] # The grey scale points are linearly separated in # gamma encoded space for x in np . linspace ( start ** gamma , end ** gamma , n ) : # Map points onto the [0, 1] palette domain x = ( x ** ( 1. / gamma ) - start ) / ( end - start ) colors . append ( mcolors . rgb2hex ( grey_cmap ( x ) ) ) return colors return continuous_grey_palette
11187	def create ( quiet , name , base_uri , symlink_path ) : _validate_name ( name ) admin_metadata = dtoolcore . generate_admin_metadata ( name ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) if parsed_base_uri . scheme == "symlink" : if symlink_path is None : raise click . UsageError ( "Need to specify symlink path using the -s/--symlink-path option" ) # NOQA if symlink_path : base_uri = dtoolcore . utils . sanitise_uri ( "symlink:" + parsed_base_uri . path ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) # Create the dataset. proto_dataset = dtoolcore . generate_proto_dataset ( admin_metadata = admin_metadata , base_uri = dtoolcore . utils . urlunparse ( parsed_base_uri ) , config_path = CONFIG_PATH ) # If we are creating a symlink dataset we need to set the symlink_path # attribute on the storage broker. if symlink_path : symlink_abspath = os . path . abspath ( symlink_path ) proto_dataset . _storage_broker . symlink_path = symlink_abspath try : proto_dataset . create ( ) except dtoolcore . storagebroker . StorageBrokerOSError as err : raise click . UsageError ( str ( err ) ) proto_dataset . put_readme ( "" ) if quiet : click . secho ( proto_dataset . uri ) else : # Give the user some feedback and hints on what to do next. click . secho ( "Created proto dataset " , nl = False , fg = "green" ) click . secho ( proto_dataset . uri ) click . secho ( "Next steps: " ) step = 1 if parsed_base_uri . scheme != "symlink" : click . secho ( "{}. Add raw data, eg:" . format ( step ) ) click . secho ( " dtool add item my_file.txt {}" . format ( proto_dataset . uri ) , fg = "cyan" ) if parsed_base_uri . scheme == "file" : # Find the abspath of the data directory for user feedback. data_path = proto_dataset . _storage_broker . _data_abspath click . secho ( " Or use your system commands, e.g: " ) click . secho ( " mv my_data_directory {}/" . format ( data_path ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Add descriptive metadata, e.g: " . format ( step ) ) click . secho ( " dtool readme interactive {}" . format ( proto_dataset . uri ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Convert the proto dataset into a dataset: " . format ( step ) ) click . secho ( " dtool freeze {}" . format ( proto_dataset . uri ) , fg = "cyan" )
3008	def _credentials_from_request ( request ) : # ORM storage requires a logged in user if ( oauth2_settings . storage_model is None or request . user . is_authenticated ( ) ) : return get_storage ( request ) . get ( ) else : return None
8990	def first_consumed_mesh ( self ) : for instruction in self . instructions : if instruction . consumes_meshes ( ) : return instruction . first_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
12643	def set_config_value ( name , value ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) cli_config . set_value ( 'servicefabric' , name , value )
2756	def get_all_tags ( self ) : data = self . get_data ( "tags" ) return [ Tag ( token = self . token , * * tag ) for tag in data [ 'tags' ] ]
10567	def _check_filters ( song , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : include = True if include_filters : if all_includes : if not all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False else : if not any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False if exclude_filters : if all_excludes : if all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False else : if any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False return include
4246	def id_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . id_by_addr ( addr )
2337	def remove_indirect_links ( g , alg = "aracne" , * * kwargs ) : alg = { "aracne" : aracne , "nd" : network_deconvolution , "clr" : clr } [ alg ] mat = np . array ( nx . adjacency_matrix ( g ) . todense ( ) ) return nx . relabel_nodes ( nx . DiGraph ( alg ( mat , * * kwargs ) ) , { idx : i for idx , i in enumerate ( list ( g . nodes ( ) ) ) } )
9421	def _read_header ( self , handle ) : header_data = unrarlib . RARHeaderDataEx ( ) try : res = unrarlib . RARReadHeaderEx ( handle , ctypes . byref ( header_data ) ) rarinfo = RarInfo ( header = header_data ) except unrarlib . ArchiveEnd : return None except unrarlib . MissingPassword : raise RuntimeError ( "Archive is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for Archive" ) except unrarlib . UnrarException as e : raise BadRarFile ( str ( e ) ) return rarinfo
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
1445	def register_metric ( self , name , metric , time_bucket_in_sec ) : if name in self . metrics_map : raise RuntimeError ( "Another metric has already been registered with name: %s" % name ) Log . debug ( "Register metric: %s, with interval: %s" , name , str ( time_bucket_in_sec ) ) self . metrics_map [ name ] = metric if time_bucket_in_sec in self . time_bucket_in_sec_to_metrics_name : self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] . append ( name ) else : self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] = [ name ] self . _register_timer_task ( time_bucket_in_sec )
13167	def insert ( self , before , name , attrs = None , data = None ) : if isinstance ( before , self . __class__ ) : if before . parent != self : raise ValueError ( 'Cannot insert before an element with a different parent.' ) before = before . index # Make sure 0 <= before <= len(_children). before = min ( max ( 0 , before ) , len ( self . _children ) ) elem = self . __class__ ( name , attrs , data , parent = self , index = before ) self . _children . insert ( before , elem ) # Re-index all the children. for idx , c in enumerate ( self . _children ) : c . index = idx return elem
7269	def attribute ( * args , * * kw ) : return operator ( kind = Operator . Type . ATTRIBUTE , * args , * * kw )
3896	def generate_enum_doc ( enum_descriptor , locations , path , name_prefix = '' ) : print ( make_subsection ( name_prefix + enum_descriptor . name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for value_index , value in enumerate ( enum_descriptor . value ) : field_location = locations [ path + ( 2 , value_index ) ] row_tuples . append ( ( make_code ( value . name ) , value . number , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Name' , 'Number' , 'Description' ) , row_tuples )
8579	def delete_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'DELETE' ) return response
4889	def update_course ( self , course , enterprise_customer , enterprise_context ) : course [ 'course_runs' ] = self . update_course_runs ( course_runs = course . get ( 'course_runs' ) or [ ] , enterprise_customer = enterprise_customer , enterprise_context = enterprise_context , ) # Update marketing urls in course metadata to include enterprise related info (i.e. our global context). marketing_url = course . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , * * utils . get_enterprise_utm_context ( enterprise_customer ) ) course . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) # Finally, add context to the course as a whole. course . update ( enterprise_context ) return course
9966	def convert_args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs
8315	def parse_links ( self , markup ) : links = [ ] m = re . findall ( self . re [ "link" ] , markup ) for link in m : # We don't like [[{{{1|Universe (disambiguation)}}}]] if link . find ( "{" ) >= 0 : link = re . sub ( "\{{1,3}[0-9]{0,2}\|" , "" , link ) link = link . replace ( "{" , "" ) link = link . replace ( "}" , "" ) link = link . split ( "|" ) link [ 0 ] = link [ 0 ] . split ( "#" ) page = link [ 0 ] [ 0 ] . strip ( ) #anchor = u"" #display = u"" #if len(link[0]) > 1: # anchor = link[0][1].strip() #if len(link) > 1: # display = link[1].strip() if not page in links : links . append ( page ) #links[page] = WikipediaLink(page, anchor, display) links . sort ( ) return links
1340	def binarize ( x , values , threshold = None , included_in = 'upper' ) : lower , upper = values if threshold is None : threshold = ( lower + upper ) / 2. x = x . copy ( ) if included_in == 'lower' : x [ x <= threshold ] = lower x [ x > threshold ] = upper elif included_in == 'upper' : x [ x < threshold ] = lower x [ x >= threshold ] = upper else : raise ValueError ( 'included_in must be "lower" or "upper"' ) return x
825	def expValue ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] return sum ( [ x * p for x , p in pred . items ( ) ] )
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : # pragma: no cover raise
8701	def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( ) except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )
12304	def post ( self , repo ) : datapackage = repo . package url = self . url token = self . token headers = { 'Authorization' : 'Token {}' . format ( token ) , 'Content-Type' : 'application/json' } try : r = requests . post ( url , data = json . dumps ( datapackage ) , headers = headers ) return r except Exception as e : #print(e) #traceback.print_exc() raise NetworkError ( ) return ""
8381	def hover ( self , node ) : if self . popup == False : return if self . popup == True or self . popup . node != node : if self . popup_text . has_key ( node . id ) : texts = self . popup_text [ node . id ] else : texts = None self . popup = popup ( self . _ctx , node , texts ) self . popup . draw ( )
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) # pylint: disable=invalid-name try : return EnterpriseCustomer . objects . get ( uuid = uuid ) # pylint: disable=no-member except EnterpriseCustomer . DoesNotExist : return None
3723	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : sigmas = [ i ( T ) for i in self . SurfaceTensions ] return mixing_simple ( zs , sigmas ) elif method == DIGUILIOTEJA : return Diguilio_Teja ( T = T , xs = zs , sigmas_Tb = self . sigmas_Tb , Tbs = self . Tbs , Tcs = self . Tcs ) elif method == WINTERFELDSCRIVENDAVIS : sigmas = [ i ( T ) for i in self . SurfaceTensions ] rhoms = [ 1. / i ( T , P ) for i in self . VolumeLiquids ] return Winterfeld_Scriven_Davis ( zs , sigmas , rhoms ) else : raise Exception ( 'Method not valid' )
4168	def ss2zpk ( a , b , c , d , input = 0 ) : import scipy . signal z , p , k = scipy . signal . ss2zpk ( a , b , c , d , input = input ) return z , p , k
4560	def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
6097	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( radius , dim . Length ) : radius = dim . Length ( value = radius , unit_length = 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
12248	def sync ( self , * buckets ) : if buckets : for _bucket in buckets : for key in mimicdb . backend . smembers ( tpl . bucket % _bucket ) : mimicdb . backend . delete ( tpl . key % ( _bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % _bucket ) bucket = self . get_bucket ( _bucket , force = True ) for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) else : for bucket in mimicdb . backend . smembers ( tpl . connection ) : for key in mimicdb . backend . smembers ( tpl . bucket % bucket ) : mimicdb . backend . delete ( tpl . key % ( bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % bucket ) for bucket in self . get_all_buckets ( force = True ) : for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) )
9684	def sn ( self ) : string = [ ] # Send the command byte and sleep for 9 ms self . cnxn . xfer ( [ 0x10 ] ) sleep ( 9e-3 ) # Read the info string by sending 60 empty bytes for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] string . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( string )
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
1228	def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : # Mean loss per instance loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) # Returns no-op. updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) # Loss without regularization summary. if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) # Regularization losses. losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) # Total loss summary. if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss
22	def get_wrapper_by_name ( env , classname ) : currentenv = env while True : if classname == currentenv . class_name ( ) : return currentenv elif isinstance ( currentenv , gym . Wrapper ) : currentenv = currentenv . env else : raise ValueError ( "Couldn't find wrapper named %s" % classname )
7333	async def close ( self ) : tasks = self . _get_close_tasks ( ) if tasks : await asyncio . wait ( tasks ) self . _session = None
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
10871	def f_theta ( cos_theta , zint , z , n2n1 = 0.95 , sph6_ab = None , * * kwargs ) : wvfront = ( np . outer ( np . ones_like ( z ) * zint , cos_theta ) - np . outer ( zint + z , csqrt ( n2n1 ** 2 - 1 + cos_theta ** 2 ) ) ) if ( sph6_ab is not None ) and ( not np . isnan ( sph6_ab ) ) : sec2_theta = 1.0 / ( cos_theta * cos_theta ) wvfront += sph6_ab * ( sec2_theta - 1 ) * ( sec2_theta - 2 ) * cos_theta #Ensuring evanescent waves are always suppressed: if wvfront . dtype == np . dtype ( 'complex128' ) : wvfront . imag = - np . abs ( wvfront . imag ) return wvfront
8530	def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff ( ) return t_diff
12193	def _respond ( self , channel , text ) : result = self . _format_message ( channel , text ) if result is not None : logger . info ( 'Sending message: %r' , truncate ( result , max_len = 50 ) , ) self . socket . send_str ( result )
3626	def pad_to ( unpadded , target_len ) : under = target_len - len ( unpadded ) if under <= 0 : return unpadded return unpadded + ( ' ' * under )
8155	def create_table ( self , name , fields = [ ] , key = "id" ) : for f in fields : if f == key : fields . remove ( key ) sql = "create table " + name + " " sql += "(" + key + " integer primary key" for f in fields : sql += ", " + f + " varchar(255)" sql += ")" self . _cur . execute ( sql ) self . _con . commit ( ) self . index ( name , key , unique = True ) self . connect ( self . _name )
13031	def poll_once ( self , timeout = 0.0 ) : if self . _map : self . _poll_func ( timeout , self . _map )
2652	def execute_no_wait ( self , cmd , walltime = 2 , envs = { } ) : # Execute the command stdin , stdout , stderr = self . ssh_client . exec_command ( self . prepend_envs ( cmd , envs ) , bufsize = - 1 , timeout = walltime ) return None , stdout , stderr
3186	def create ( self , conversation_id , data ) : self . conversation_id = conversation_id if 'from_email' not in data : raise KeyError ( 'The conversation message must have a from_email' ) check_email ( data [ 'from_email' ] ) if 'read' not in data : raise KeyError ( 'The conversation message must have a read' ) if data [ 'read' ] not in [ True , False ] : raise TypeError ( 'The conversation message read must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( conversation_id , 'messages' ) , data = data ) if response is not None : self . message_id = response [ 'id' ] else : self . message_id = None return response
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
925	def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 # Get the most common one if nonNone == 0 : return None # Sort by counts sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
12177	def show_variances ( Y , variances , varianceX , logScale = False ) : plt . figure ( 1 , figsize = ( 10 , 7 ) ) plt . figure ( 2 , figsize = ( 10 , 7 ) ) varSorted = sorted ( variances ) plt . figure ( 1 ) plt . subplot ( 211 ) plt . grid ( ) plt . title ( "chronological variance" ) plt . ylabel ( "original data" ) plot_shaded_data ( X , Y , variances , varianceX ) plt . margins ( 0 , .1 ) plt . subplot ( 212 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "time in sweep (sec)" ) plt . plot ( varianceX , variances , 'k-' , lw = 2 ) plt . figure ( 2 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "chunk number" ) plt . title ( "sorted variance" ) plt . plot ( varSorted , 'k-' , lw = 2 ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) label = "%2d-%d percentile" % ( i , i + + PERCENT_STEP ) color = COLORMAP ( i / 100 ) print ( "%s: variance = %.02f - %.02f" % ( label , varLimitLow , varLimitHigh ) ) plt . figure ( 1 ) plt . axhspan ( varLimitLow , varLimitHigh , alpha = .5 , lw = 0 , color = color , label = label ) plt . figure ( 2 ) chunkLow = np . where ( varSorted >= varLimitLow ) [ 0 ] [ 0 ] chunkHigh = np . where ( varSorted >= varLimitHigh ) [ 0 ] [ 0 ] plt . axvspan ( chunkLow , chunkHigh , alpha = .5 , lw = 0 , color = color , label = label ) for fignum in [ 1 , 2 ] : plt . figure ( fignum ) if logScale : plt . semilogy ( ) plt . margins ( 0 , 0 ) plt . grid ( ) if fignum is 2 : plt . legend ( fontsize = 10 , loc = 'upper left' , shadow = True ) plt . tight_layout ( ) plt . savefig ( '2016-12-15-variance-%d-log%s.png' % ( fignum , str ( logScale ) ) ) plt . show ( )
11364	def run_shell_command ( commands , * * kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , * * kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
3037	def new_from_json ( cls , json_data ) : json_data_as_unicode = _helpers . _from_bytes ( json_data ) data = json . loads ( json_data_as_unicode ) # Find and call the right classmethod from_json() to restore # the object. module_name = data [ '_module' ] try : module_obj = __import__ ( module_name ) except ImportError : # In case there's an object from the old package structure, # update it module_name = module_name . replace ( '.googleapiclient' , '' ) module_obj = __import__ ( module_name ) module_obj = __import__ ( module_name , fromlist = module_name . split ( '.' ) [ : - 1 ] ) kls = getattr ( module_obj , data [ '_class' ] ) return kls . from_json ( json_data_as_unicode )
7307	def set_embedded_doc ( self , document , form_key , current_key , remaining_key ) : embedded_doc = getattr ( document , current_key , False ) if not embedded_doc : embedded_doc = document . _fields [ current_key ] . document_type_obj ( ) new_key , new_remaining_key_array = trim_field_key ( embedded_doc , remaining_key ) self . process_document ( embedded_doc , form_key , make_key ( new_key , new_remaining_key_array ) ) setattr ( document , current_key , embedded_doc )
8306	def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
9550	def ivalidate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , context = None , report_unexpected_exceptions = True ) : unique_sets = self . _init_unique_sets ( ) # used for unique checks for i , r in enumerate ( data ) : if expect_header_row and i == ignore_lines : # r is the header row for p in self . _apply_header_checks ( i , r , summarize , context ) : yield p elif i >= ignore_lines : # r is a data row skip = False for p in self . _apply_skips ( i , r , summarize , report_unexpected_exceptions , context ) : if p is True : skip = True else : yield p if not skip : for p in self . _apply_each_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p # may yield a problem if an exception is raised for p in self . _apply_value_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_length_checks ( i , r , summarize , context ) : yield p for p in self . _apply_value_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_unique_checks ( i , r , unique_sets , summarize ) : yield p for p in self . _apply_check_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_assert_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_finally_assert_methods ( summarize , report_unexpected_exceptions , context ) : yield p
2128	def set_display_columns ( self , set_true = [ ] , set_false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set_true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set_false : self . fields [ i ] . display = False
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
8726	def _localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except AttributeError : return dt
12196	def get_tasks ( ) : task_classes = [ ] for task_path in TASKS : try : module , classname = task_path . rsplit ( '.' , 1 ) except ValueError : raise ImproperlyConfigured ( '%s isn\'t a task module' % task_path ) try : mod = import_module ( module ) except ImportError as e : raise ImproperlyConfigured ( 'Error importing task %s: "%s"' % ( module , e ) ) try : task_class = getattr ( mod , classname ) except AttributeError : raise ImproperlyConfigured ( 'Task module "%s" does not define a ' '"%s" class' % ( module , classname ) ) task_classes . append ( task_class ) return task_classes
13881	def MoveDirectory ( source_dir , target_dir ) : if not IsDir ( source_dir ) : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( source_dir ) if Exists ( target_dir ) : from . _exceptions import DirectoryAlreadyExistsError raise DirectoryAlreadyExistsError ( target_dir ) from six . moves . urllib . parse import urlparse source_url = urlparse ( source_dir ) target_url = urlparse ( target_dir ) # Local to local if _UrlIsLocal ( source_url ) and _UrlIsLocal ( target_url ) : import shutil shutil . move ( source_dir , target_dir ) # FTP to FTP elif source_url . scheme == 'ftp' and target_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( target_url . scheme ) else : raise NotImplementedError ( 'Can only move directories local->local or ftp->ftp' )
11586	def getnodefor ( self , name ) : node = self . _getnodenamefor ( name ) return { node : self . cluster [ 'nodes' ] [ node ] }
6151	def fir_remez_hpf ( f_stop , f_pass , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : # Transform HPF critical frequencies to lowpass equivalent f_pass_eq = fs / 2. - f_pass f_stop_eq = fs / 2. - f_stop # Design LPF equivalent n , ff , aa , wts = lowpass_order ( f_pass_eq , f_stop_eq , d_pass , d_stop , fsamp = fs ) # Bump up the order by N_bump to bring down the final d_pass & d_stop N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) # Transform LPF equivalent to HPF n = np . arange ( len ( b ) ) b *= ( - 1 ) ** n print ( 'Remez filter taps = %d.' % N_taps ) return b
10521	def oneright ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
6753	def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
513	def _updateDutyCycles ( self , overlaps , activeColumns ) : overlapArray = numpy . zeros ( self . _numColumns , dtype = realDType ) activeArray = numpy . zeros ( self . _numColumns , dtype = realDType ) overlapArray [ overlaps > 0 ] = 1 activeArray [ activeColumns ] = 1 period = self . _dutyCyclePeriod if ( period > self . _iterationNum ) : period = self . _iterationNum self . _overlapDutyCycles = self . _updateDutyCyclesHelper ( self . _overlapDutyCycles , overlapArray , period ) self . _activeDutyCycles = self . _updateDutyCyclesHelper ( self . _activeDutyCycles , activeArray , period )
3089	def locked_get ( self ) : credentials = None if self . _cache : json = self . _cache . get ( self . _key_name ) if json : credentials = client . Credentials . new_from_json ( json ) if credentials is None : entity = self . _get_entity ( ) if entity is not None : credentials = getattr ( entity , self . _property_name ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) ) if credentials and hasattr ( credentials , 'set_store' ) : credentials . set_store ( self ) return credentials
2973	def from_dict ( values ) : try : containers = values [ 'containers' ] parsed_containers = { } for name , container_dict in containers . items ( ) : try : # one config entry might result in many container # instances (indicated by the 'count' config value) for cnt in BlockadeContainerConfig . from_dict ( name , container_dict ) : # check for duplicate 'container_name' definitions if cnt . container_name : cname = cnt . container_name existing = [ c for c in parsed_containers . values ( ) if c . container_name == cname ] if existing : raise BlockadeConfigError ( "Duplicate 'container_name' definition: %s" % ( cname ) ) parsed_containers [ cnt . name ] = cnt except Exception as err : raise BlockadeConfigError ( "Container '%s' config problem: %s" % ( name , err ) ) network = values . get ( 'network' ) if network : defaults = _DEFAULT_NETWORK_CONFIG . copy ( ) defaults . update ( network ) network = defaults else : network = _DEFAULT_NETWORK_CONFIG . copy ( ) return BlockadeConfig ( parsed_containers , network = network ) except KeyError as err : raise BlockadeConfigError ( "Config missing value: " + str ( err ) ) except Exception as err : # TODO log this to some debug stream? raise BlockadeConfigError ( "Failed to load config: " + str ( err ) )
13420	def validate ( cls , definition ) : schema_path = os . path . join ( os . path . dirname ( __file__ ) , '../../schema/mapper_definition_schema.json' ) with open ( schema_path , 'r' ) as jsonfp : schema = json . load ( jsonfp ) # Validation of JSON schema jsonschema . validate ( definition , schema ) # Validation of JSON properties relations assert definition [ 'main_key' ] in definition [ 'supported_keys' ] , '\'main_key\' must be contained in \'supported_keys\'' assert set ( definition . get ( 'list_valued_keys' , [ ] ) ) <= set ( definition [ 'supported_keys' ] ) , '\'list_valued_keys\' must be a subset of \'supported_keys\'' assert set ( definition . get ( 'disjoint' , [ ] ) ) <= set ( definition . get ( 'list_valued_keys' , [ ] ) ) , '\'disjoint\' must be a subset of \'list_valued_keys\'' assert set ( definition . get ( 'key_synonyms' , { } ) . values ( ) ) <= set ( definition [ 'supported_keys' ] ) , '\'The values of the \'key_synonyms\' mapping must be in \'supported_keys\''
5266	def sentencecase ( string ) : joiner = ' ' string = re . sub ( r"[\-_\.\s]" , joiner , str ( string ) ) if not string : return string return capitalcase ( trimcase ( re . sub ( r"[A-Z]" , lambda matched : joiner + lowercase ( matched . group ( 0 ) ) , string ) ) )
8135	def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
12234	def pref ( preference , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : try : bound = bind_proxy ( ( preference , ) , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) return bound [ 0 ] except IndexError : return
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) # Create a search template from dataset ids search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
10020	def environment_exists ( self , env_name ) : response = self . ebs . describe_environments ( application_name = self . app_name , environment_names = [ env_name ] , include_deleted = False ) return len ( response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] ) > 0 and response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
5608	def bounds_to_ranges ( out_bounds = None , in_affine = None , in_shape = None ) : return itertools . chain ( * from_bounds ( * out_bounds , transform = in_affine , height = in_shape [ - 2 ] , width = in_shape [ - 1 ] ) . round_lengths ( pixel_precision = 0 ) . round_offsets ( pixel_precision = 0 ) . toranges ( ) )
8690	def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
8629	def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) # Hire Me job, required project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } # POST /api/projects/0.1/projects/ response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
7653	def serialize_obj ( obj ) : if isinstance ( obj , np . integer ) : return int ( obj ) elif isinstance ( obj , np . floating ) : return float ( obj ) elif isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , list ) : return [ serialize_obj ( x ) for x in obj ] elif isinstance ( obj , Observation ) : return { k : serialize_obj ( v ) for k , v in six . iteritems ( obj . _asdict ( ) ) } return obj
6928	def close_cursor ( self , handle ) : if handle in self . cursors : self . cursors [ handle ] . close ( ) else : raise KeyError ( 'cursor with handle %s was not found' % handle )
6552	def check ( self , solution ) : return self . func ( * ( solution [ v ] for v in self . variables ) )
8227	def _makeColorableInstance ( self , clazz , args , kwargs ) : kwargs = dict ( kwargs ) fill = kwargs . get ( 'fill' , self . _canvas . fillcolor ) if not isinstance ( fill , Color ) : fill = Color ( fill , mode = 'rgb' , color_range = 1 ) kwargs [ 'fill' ] = fill stroke = kwargs . get ( 'stroke' , self . _canvas . strokecolor ) if not isinstance ( stroke , Color ) : stroke = Color ( stroke , mode = 'rgb' , color_range = 1 ) kwargs [ 'stroke' ] = stroke kwargs [ 'strokewidth' ] = kwargs . get ( 'strokewidth' , self . _canvas . strokewidth ) inst = clazz ( self , * args , * * kwargs ) return inst
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
3718	def estimate ( self ) : # Pre-generate the coefficients or they will not be returned self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
3855	async def _sync_all_conversations ( client ) : conv_states = [ ] sync_timestamp = None request = hangouts_pb2 . SyncRecentConversationsRequest ( request_header = client . get_request_header ( ) , max_conversations = CONVERSATIONS_PER_REQUEST , max_events_per_conversation = 1 , sync_filter = [ hangouts_pb2 . SYNC_FILTER_INBOX , hangouts_pb2 . SYNC_FILTER_ARCHIVED , ] ) for _ in range ( MAX_CONVERSATION_PAGES ) : logger . info ( 'Requesting conversations page %s' , request . last_event_timestamp ) response = await client . sync_recent_conversations ( request ) conv_states = list ( response . conversation_state ) + conv_states sync_timestamp = parsers . from_timestamp ( # SyncRecentConversations seems to return a sync_timestamp 4 # minutes before the present. To prevent SyncAllNewEvents later # breaking requesting events older than what we already have, use # current_server_time instead. response . response_header . current_server_time ) if response . continuation_end_timestamp == 0 : logger . info ( 'Reached final conversations page' ) break else : request . last_event_timestamp = response . continuation_end_timestamp else : logger . warning ( 'Exceeded maximum number of conversation pages' ) logger . info ( 'Synced %s total conversations' , len ( conv_states ) ) return conv_states , sync_timestamp
3517	def spring_metrics ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SpringMetricsNode ( )
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
2110	def send ( source = None , prevent = None , exclude = None , secret_management = 'default' , no_color = False ) : from tower_cli . cli . transfer . send import Sender sender = Sender ( no_color ) sender . send ( source , prevent , exclude , secret_management )
9524	def sort_by_size ( infile , outfile , smallest_first = False ) : seqs = { } file_to_dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest_first ) fout = utils . open_file_write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
4177	def window_cosine ( N ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) win = sin ( pi * n / ( N - 1. ) ) return win
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : ## iterate across k-vals kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : ## concat results for k=x reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) ## report if some results were excluded if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" ## all we really need is the lnlik replnliks . append ( [ i . est_lnlik for i in reps ] ) ## compare lnlik and var of results if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) ## calculate Evanno's for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) ## return table return tab
13417	def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call_manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call_manage ( "loaddata %s" % fixture )
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
8605	def delete_user ( self , user_id ) : response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'DELETE' ) return response
6230	def draw_bbox ( self , projection_matrix = None , camera_matrix = None , all = True ) : projection_matrix = projection_matrix . astype ( 'f4' ) . tobytes ( ) camera_matrix = camera_matrix . astype ( 'f4' ) . tobytes ( ) # Scene bounding box self . bbox_program [ "m_proj" ] . write ( projection_matrix ) self . bbox_program [ "m_view" ] . write ( self . _view_matrix . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "m_cam" ] . write ( camera_matrix ) self . bbox_program [ "bb_min" ] . write ( self . bbox_min . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "bb_max" ] . write ( self . bbox_max . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "color" ] . value = ( 1.0 , 0.0 , 0.0 ) self . bbox_vao . render ( self . bbox_program ) if not all : return # Draw bounding box for children for node in self . root_nodes : node . draw_bbox ( projection_matrix , camera_matrix , self . bbox_program , self . bbox_vao )
2415	def write_review ( review , out ) : out . write ( '# Review\n\n' ) write_value ( 'Reviewer' , review . reviewer , out ) write_value ( 'ReviewDate' , review . review_date_iso_format , out ) if review . has_comment : write_text_value ( 'ReviewComment' , review . comment , out )
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
4027	def create_local_copy ( cookie_file ) : # if type of cookie_file is a list, use the first element in the list if isinstance ( cookie_file , list ) : cookie_file = cookie_file [ 0 ] # check if cookie file exists if os . path . exists ( cookie_file ) : # copy to random name in tmp folder tmp_cookie_file = tempfile . NamedTemporaryFile ( suffix = '.sqlite' ) . name open ( tmp_cookie_file , 'wb' ) . write ( open ( cookie_file , 'rb' ) . read ( ) ) return tmp_cookie_file else : raise BrowserCookieError ( 'Can not find cookie file at: ' + cookie_file )
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
6135	def _fix_docs ( this_abc , child_class ) : # After python 3.5, this is basically handled automatically if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) # This method is modified from solution given in # https://stackoverflow.com/a/8101598/8863865 for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
5440	def get_variable_name ( self , name ) : if not name : name = '%s%s' % ( self . _auto_prefix , self . _auto_index ) self . _auto_index += 1 return name
13760	def _format_iso_time ( self , time ) : if isinstance ( time , str ) : return time elif isinstance ( time , datetime ) : return time . strftime ( '%Y-%m-%dT%H:%M:%S.%fZ' ) else : return None
4537	def wheel_helper ( pos , length , cycle_step ) : return wheel_color ( ( pos * len ( _WHEEL ) / length ) + cycle_step )
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) # not to deadlock with the stream try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) # not to deadlock with the stream try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
8962	def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( "# Requirements frozen by 'pip freeze' on {}\n" . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( "Frozen {} requirements." . format ( len ( frozen . splitlines ( ) ) , ) )
12747	def load ( self , source , * * kwargs ) : if hasattr ( source , 'endswith' ) and source . lower ( ) . endswith ( '.asf' ) : self . load_asf ( source , * * kwargs ) else : self . load_skel ( source , * * kwargs )
1714	def emit ( self , op_code , * args ) : self . tape . append ( OP_CODES [ op_code ] ( * args ) )
10679	def S ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : # Create a modified exponent to analytically integrate Cp(T)/T # instead of Cp(T). e_modified = e - 1.0 if e_modified == - 1.0 : result += c * math . log ( lT / Tref ) else : e_mod = e_modified + 1.0 result += c * ( lT ** e_mod - Tref ** e_mod ) / e_mod return result
11989	def on_message ( self , websocket , message ) : waiter = self . _waiter self . _waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER_ERROR : raise PusherError ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER_CONNECTION : self . socket_id = data . get ( 'socket_id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket_id ) waiter . set_result ( self . socket_id ) elif event == PUSHER_SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . _event ( event , data ) except Exception as exc : if waiter : waiter . set_exception ( exc ) else : self . logger . exception ( 'pusher error' )
4057	def _csljson_processor ( self , retrieved ) : items = [ ] json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , * * json_kwargs ) ) self . url_params = None return items
3672	def bubble_at_P ( P , zs , vapor_pressure_eqns , fugacities = None , gammas = None ) : def bubble_P_error ( T ) : Psats = [ VP ( T ) for VP in vapor_pressure_eqns ] Pcalc = bubble_at_T ( zs , Psats , fugacities , gammas ) return P - Pcalc T_bubble = newton ( bubble_P_error , 300 ) return T_bubble
655	def _fillInOnTimes ( vector , durations ) : # Find where the nonzeros are nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] # Nothing to do if vector is empty if len ( nonzeros ) == 0 : return # Special case of only 1 on bit if len ( nonzeros ) == 1 : durations [ nonzeros [ 0 ] ] = 1 return # Count the consecutive non-zeros prev = nonzeros [ 0 ] onTime = 1 onStartIdx = prev endIdx = nonzeros [ - 1 ] for idx in nonzeros [ 1 : ] : if idx != prev + 1 : # Fill in the durations durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) onTime = 1 onStartIdx = idx else : onTime += 1 prev = idx # Fill in the last one durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 )
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
2168	def list_misc_commands ( self ) : answer = set ( [ ] ) for cmd_name in misc . __all__ : answer . add ( cmd_name ) return sorted ( answer )
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
1810	def SETGE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == cpu . OF , 1 , 0 ) )
3791	def refractive_index ( CASRN , T = None , AvailableMethods = False , Method = None , full_info = True ) : def list_methods ( ) : methods = [ ] if CASRN in CRC_RI_organic . index : methods . append ( CRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : _RI = float ( CRC_RI_organic . at [ CASRN , 'RI' ] ) if full_info : _T = float ( CRC_RI_organic . at [ CASRN , 'RIT' ] ) elif Method == NONE : _RI , _T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return _RI , _T else : return _RI
4586	def parse ( s ) : parts = s . replace ( ',' , ' ' ) . split ( ) if not parts : raise ValueError ( 'Cannot parse empty string' ) pieces = [ ] for part in parts : m = PART_MATCH ( part ) pieces . extend ( m . groups ( ) if m else [ part ] ) if len ( pieces ) == 1 : pieces . append ( 's' ) if len ( pieces ) % 2 : raise ValueError ( 'Malformed duration %s: %s: %s' % ( s , parts , pieces ) ) result = 0 for number , units in zip ( * [ iter ( pieces ) ] * 2 ) : number = float ( number ) if number < 0 : raise ValueError ( 'Durations cannot have negative components' ) result += number * _get_units ( units ) return result
10041	def deposit_minter ( record_uuid , data ) : provider = DepositProvider . create ( object_type = 'rec' , object_uuid = record_uuid , pid_value = uuid . uuid4 ( ) . hex , ) data [ '_deposit' ] = { 'id' : provider . pid . pid_value , 'status' : 'draft' , } return provider . pid
10728	def _handle_struct ( toks ) : subtrees = toks [ 1 : - 1 ] signature = '' . join ( s for ( _ , s ) in subtrees ) funcs = [ f for ( f , _ ) in subtrees ] def the_func ( a_list , variant = 0 ) : """ Function for generating a Struct from a list. :param a_list: the list to transform :type a_list: list or tuple :param int variant: variant index :returns: a dbus Struct of transformed values and variant level :rtype: Struct * int :raises IntoDPValueError: """ if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "must be a simple sequence, is a dict" ) if len ( a_list ) != len ( funcs ) : raise IntoDPValueError ( a_list , "a_list" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( a_list ) ) ) elements = [ f ( x ) for ( f , x ) in zip ( funcs , a_list ) ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Struct ( ( x for ( x , _ ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_func , '(' + signature + ')' )
381	def zca_whitening ( x , principal_components ) : flatx = np . reshape ( x , ( x . size ) ) # tl.logging.info(principal_components.shape, x.shape) # ((28160, 28160), (160, 176, 1)) # flatx = np.reshape(x, (x.shape)) # flatx = np.reshape(x, (x.shape[0], )) # tl.logging.info(flatx.shape) # (160, 176, 1) whitex = np . dot ( flatx , principal_components ) x = np . reshape ( whitex , ( x . shape [ 0 ] , x . shape [ 1 ] , x . shape [ 2 ] ) ) return x
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
11678	def run ( self ) : logger . info ( u'Started listening' ) while not self . _stop : xml = self . _readxml ( ) # Exit on invalid XML if xml is None : break # Raw xml only if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue # Model objects + raw xml as fallback if xml . tag == 'RECOGOUT' : sentence = Sentence . from_shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )
8299	def decodeOSC ( data ) : table = { "i" : readInt , "f" : readFloat , "s" : readString , "b" : readBlob } decoded = [ ] address , rest = readString ( data ) typetags = "" if address == "#bundle" : time , rest = readLong ( rest ) # decoded.append(address) # decoded.append(time) while len ( rest ) > 0 : length , rest = readInt ( rest ) decoded . append ( decodeOSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = readString ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
122	def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : # wait for a new batch in the source queue and load it try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True # put it back in so that other workers know that the loading queue is finished queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) # send augmented batch to output queue batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )
7477	def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ "cut" , "-f" , "2" ] cmd2 = [ "uniq" ] cmd3 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close_fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close_fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close_fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds
11849	def percept ( self , agent ) : return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
10276	def get_neurommsig_scores ( graph : BELGraph , genes : List [ Gene ] , annotation : str = 'Subgraph' , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , preprocess : bool = False ) -> Optional [ Mapping [ str , float ] ] : if preprocess : graph = neurommsig_graph_preprocessor . run ( graph ) if not any ( gene in graph for gene in genes ) : logger . debug ( 'no genes mapping to graph' ) return subgraphs = get_subgraphs_by_annotation ( graph , annotation = annotation ) return get_neurommsig_scores_prestratified ( subgraphs = subgraphs , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , )
13527	def clean ( options , info ) : info ( "Cleaning patterns %s" , options . paved . clean . patterns ) for wd in options . paved . clean . dirs : info ( "Cleaning in %s" , wd ) for p in options . paved . clean . patterns : for f in wd . walkfiles ( p ) : f . remove ( )
2734	def get_object ( cls , api_token , ip ) : floating_ip = cls ( token = api_token , ip = ip ) floating_ip . load ( ) return floating_ip
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
663	def getCentreAndSpreadOffsets ( spaceShape , spreadShape , stepSize = 1 ) : from nupic . math . cross import cross # ===================================================================== # Init data structures # What is the range on the X and Y offsets of the center points? shape = spaceShape # If the shape is (1,1), special case of just 1 center point if shape [ 0 ] == 1 and shape [ 1 ] == 1 : centerOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) centerOffsets = list ( cross ( yPositions , xPositions ) ) numCenterOffsets = len ( centerOffsets ) print "centerOffsets:" , centerOffsets # What is the range on the X and Y offsets of the spread points? shape = spreadShape # If the shape is (1,1), special case of no spreading around each center # point if shape [ 0 ] == 1 and shape [ 1 ] == 1 : spreadOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) spreadOffsets = list ( cross ( yPositions , xPositions ) ) # Put the (0,0) entry first spreadOffsets . remove ( ( 0 , 0 ) ) spreadOffsets . insert ( 0 , ( 0 , 0 ) ) numSpreadOffsets = len ( spreadOffsets ) print "spreadOffsets:" , spreadOffsets return centerOffsets , spreadOffsets
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
7017	def parallel_concat_worker ( task ) : lcbasedir , objectid , kwargs = task try : return concat_write_pklc ( lcbasedir , objectid , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed LC concatenation for %s in %s' % ( objectid , lcbasedir ) ) return None
7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) # get cache path to put the file cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path # make HEAD request to check ETag response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : # dropbox return code 301, so we ignore this error pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) # add ETag to filename if it exists # etag = response.headers.get("ETag") if not cache_path . exists ( ) : # Download to temporary file, then copy to cache dir once finished. # Otherwise you get corrupt cache entries if the download gets interrupted. fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) # GET file object req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : # filter out keep-alive new chunks progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
249	def get_txn_vol ( transactions ) : txn_norm = transactions . copy ( ) txn_norm . index = txn_norm . index . normalize ( ) amounts = txn_norm . amount . abs ( ) prices = txn_norm . price values = amounts * prices daily_amounts = amounts . groupby ( amounts . index ) . sum ( ) daily_values = values . groupby ( values . index ) . sum ( ) daily_amounts . name = "txn_shares" daily_values . name = "txn_volume" return pd . concat ( [ daily_values , daily_amounts ] , axis = 1 )
543	def _getFieldStats ( self ) : fieldStats = dict ( ) fieldNames = self . _inputSource . getFieldNames ( ) for field in fieldNames : curStats = dict ( ) curStats [ 'min' ] = self . _inputSource . getFieldMin ( field ) curStats [ 'max' ] = self . _inputSource . getFieldMax ( field ) fieldStats [ field ] = curStats return fieldStats
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
5199	def process_point_value ( cls , command_type , command , index , op_type ) : _log . debug ( 'Processing received point value for index {}: {}' . format ( index , command ) )
2000	def visit_BitVecAdd ( self , expression , * operands ) : left = expression . operands [ 0 ] right = expression . operands [ 1 ] if isinstance ( right , BitVecConstant ) : if right . value == 0 : return left if isinstance ( left , BitVecConstant ) : if left . value == 0 : return right
7519	def write_snps_map ( data ) : ## grab map data from tmparr start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ "maparr" ] [ : ] ## get last data end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] ## write to map file (this is too slow...) outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : ## build to list line = maparr [ idx , : ] #print(line) outchunk . append ( "{}\trad{}_snp{}\t{}\t{}\n" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) ## clear list if not idx % 10000 : out . write ( "" . join ( outchunk ) ) outchunk = [ ] ## write remaining out . write ( "" . join ( outchunk ) ) LOGGER . debug ( "finished writing snps_map in: %s" , time . time ( ) - start )
4608	def nolist ( self , account ) : # pragma: no cover assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ ] , account = self )
3571	def centralManager_didDisconnectPeripheral_error_ ( self , manager , peripheral , error ) : logger . debug ( 'centralManager_didDisconnectPeripheral called' ) # Get the device and remove it from the device list, then fire its # disconnected event. device = device_list ( ) . get ( peripheral ) if device is not None : # Fire disconnected event and remove device from device list. device . _set_disconnected ( ) device_list ( ) . remove ( peripheral )
2121	def disassociate_success_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'success' ) , parent , child )
7585	def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : #if isinstance(inarr, str): # with open(inarr, 'r') as infile: # inarr = infile.read().strip().split("|\n") # ## get data as an array from loci file # ## if loci-list then parse arr from loci if isinstance ( inarr , list ) : arr , _ = _loci_to_arr ( inarr , taxdict , mindict ) # ## if it's an array already then go ahead # elif isinstance(inarr, np.ndarray): # arr = inarr # ## if it's a simulation object get freqs from array # elif isinstance(inarr, Sim): # arr = _msp_to_arr(inarr, taxdict) #elif isinstance(inarr, types.GeneratorType): # arr = _msp_to_arr(inarr, taxdict) #elif isinstance(inarr, list): # arr = _msp_to_arr(inarr, taxdict) ## get data from Sim object, do not digest the ms generator #else: # raise Exception("Must enter either a 'locifile' or 'arr'") ## run tests #if len(taxdict) == 4: if arr . shape [ 1 ] == 4 : ## get results res , boots = _get_signif_4 ( arr , nboots ) ## make res into a nice DataFrame res = pd . DataFrame ( res , columns = [ name ] , index = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) else : ## get results res , boots = _get_signif_5 ( arr , nboots ) ## make int a DataFrame res = pd . DataFrame ( res , index = [ "p3" , "p4" , "shared" ] , columns = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABxxA" , "BAxxA" , "nloci" ] ) return res . T , boots
5021	def get_enterprise_customer_from_catalog_id ( catalog_id ) : try : return str ( EnterpriseCustomerCatalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except EnterpriseCustomerCatalog . DoesNotExist : return None
5206	def format_earning ( data : pd . DataFrame , header : pd . DataFrame ) -> pd . DataFrame : if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) res = pd . concat ( [ grp . loc [ : , [ 'value' ] ] . set_index ( header . value ) for _ , grp in data . groupby ( data . position ) ] , axis = 1 ) res . index . name = None res . columns = res . iloc [ 0 ] res = res . iloc [ 1 : ] . transpose ( ) . reset_index ( ) . apply ( pd . to_numeric , downcast = 'float' , errors = 'ignore' ) res . rename ( columns = lambda vv : '_' . join ( vv . lower ( ) . split ( ) ) . replace ( 'fy_' , 'fy' ) , inplace = True , ) years = res . columns [ res . columns . str . startswith ( 'fy' ) ] lvl_1 = res . level == 1 for yr in years : res . loc [ : , yr ] = res . loc [ : , yr ] . round ( 1 ) pct = f'{yr}_pct' res . loc [ : , pct ] = 0. res . loc [ lvl_1 , pct ] = res . loc [ lvl_1 , pct ] . astype ( float ) . round ( 1 ) res . loc [ lvl_1 , pct ] = res . loc [ lvl_1 , yr ] / res . loc [ lvl_1 , yr ] . sum ( ) * 100 sub_pct = [ ] for _ , snap in res [ : : - 1 ] . iterrows ( ) : if snap . level > 2 : continue if snap . level == 1 : if len ( sub_pct ) == 0 : continue sub = pd . concat ( sub_pct , axis = 1 ) . transpose ( ) res . loc [ sub . index , pct ] = res . loc [ sub . index , yr ] / res . loc [ sub . index , yr ] . sum ( ) * 100 sub_pct = [ ] if snap . level == 2 : sub_pct . append ( snap ) res . set_index ( 'segment_name' , inplace = True ) res . index . name = None return res
12684	def query ( self , input = '' , params = { } ) : # Get and construct query parameters # Default parameters payload = { 'input' : input , 'appid' : self . appid } # Additional parameters (from params), formatted for url for key , value in params . items ( ) : # Check if value is list or tuple type (needs to be comma joined) if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value # Catch any issues with connecting to Wolfram Alpha API try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) # Raise Exception (to be returned as error) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
3305	def _run_gevent ( app , config , mode ) : import gevent import gevent . monkey gevent . monkey . patch_all ( ) from gevent . pywsgi import WSGIServer server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , # TODO: SSL support "keyfile" : None , "certfile" : None , } protocol = "http" # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) dav_server = WSGIServer ( server_args [ "bind_addr" ] , app ) _logger . info ( "Running {}" . format ( dav_server ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) try : gevent . spawn ( dav_server . serve_forever ( ) ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
1266	def make_game ( ) : return ascii_art . ascii_art_to_game ( GAME_ART , what_lies_beneath = ' ' , sprites = dict ( [ ( 'P' , PlayerSprite ) ] + [ ( c , UpwardLaserBoltSprite ) for c in UPWARD_BOLT_CHARS ] + [ ( c , DownwardLaserBoltSprite ) for c in DOWNWARD_BOLT_CHARS ] ) , drapes = dict ( X = MarauderDrape , B = BunkerDrape ) , update_schedule = [ 'P' , 'B' , 'X' ] + list ( _ALL_BOLT_CHARS ) )
9382	def aggregate_count_over_time ( self , metric_store , line_data , transaction_list , aggregate_timestamp ) : for transaction in transaction_list : if line_data . get ( 's' ) == 'true' : all_qps = metric_store [ 'qps' ] else : all_qps = metric_store [ 'eqps' ] qps = all_qps [ transaction ] if aggregate_timestamp in qps : qps [ aggregate_timestamp ] += 1 else : qps [ aggregate_timestamp ] = 1 return None
4930	def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
2871	def remove_event_detect ( self , pin ) : self . mraa_gpio . Gpio . isrExit ( self . mraa_gpio . Gpio ( pin ) )
7337	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : sequence_dict = check_sequence_dictionary ( sequence_dict ) peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) # take each mutated sequence in the dataframe # and general MHC binding scores for all k-mer substrings binding_predictions = [ ] expected_peptides = set ( [ ] ) normalized_alleles = [ ] for key , amino_acid_sequence in sequence_dict . items ( ) : for l in peptide_lengths : for i in range ( len ( amino_acid_sequence ) - l + 1 ) : expected_peptides . add ( amino_acid_sequence [ i : i + l ] ) self . _check_peptide_inputs ( expected_peptides ) for allele in self . alleles : # IEDB MHCII predictor expects DRA1 to be omitted. allele = normalize_allele_name ( allele , omit_dra1 = True ) normalized_alleles . append ( allele ) request = self . _get_iedb_request_params ( amino_acid_sequence , allele ) logger . info ( "Calling IEDB (%s) with request %s" , self . url , request ) response_df = _query_iedb ( request , self . url ) for _ , row in response_df . iterrows ( ) : binding_predictions . append ( BindingPrediction ( source_sequence_name = key , offset = row [ 'start' ] - 1 , allele = row [ 'allele' ] , peptide = row [ 'peptide' ] , affinity = row [ 'ic50' ] , percentile_rank = row [ 'rank' ] , prediction_method_name = "iedb-" + self . prediction_method ) ) self . _check_results ( binding_predictions , alleles = normalized_alleles , peptides = expected_peptides ) return BindingPredictionCollection ( binding_predictions )
11272	def register_default_types ( ) : register_type ( type , pipe . map ) register_type ( types . FunctionType , pipe . map ) register_type ( types . MethodType , pipe . map ) register_type ( tuple , seq ) register_type ( list , seq ) register_type ( types . GeneratorType , seq ) register_type ( string_type , sh ) register_type ( unicode_type , sh ) register_type ( file_type , fileobj ) if is_py3 : register_type ( range , seq ) register_type ( map , seq )
6208	def save_photon_hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge_da ( ) data = self . _make_photon_hdf5 ( identity = identity ) phc . hdf5 . save_photon_hdf5 ( data , h5_fname = str ( filepath ) , overwrite = overwrite )
11916	def render_to ( self , path , template , * * data ) : html = self . render ( template , * * data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
12868	def register ( self , model ) : self . models [ model . _meta . table_name ] = model model . _meta . database = self . database return model
8538	def run ( self , * args , * * kwargs ) : while True : try : timestamp , ip_p = self . _queue . popleft ( ) src_ip = get_ip ( ip_p , ip_p . src ) dst_ip = get_ip ( ip_p , ip_p . dst ) src = intern ( '%s:%s' % ( src_ip , ip_p . data . sport ) ) dst = intern ( '%s:%s' % ( dst_ip , ip_p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . _streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . _streams [ key ] = stream # HACK: save the timestamp setattr ( ip_p , 'timestamp' , timestamp ) pushed = stream . push ( ip_p ) if not pushed : continue # let listeners know about the updated stream for handler in self . _handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )
10085	def delete ( self , force = True , pid = None ) : pid = pid or self . pid if self [ '_deposit' ] . get ( 'pid' ) : raise PIDInvalidAction ( ) if pid : pid . delete ( ) return super ( Deposit , self ) . delete ( force = force )
538	def __runTaskMainLoop ( self , numIters , learningOffAt = None ) : ## Reset sequence states in the model, so it starts looking for a new ## sequence self . _model . resetSequenceStates ( ) self . _currentRecordIndex = - 1 while True : # If killed by a terminator, stop running if self . _isKilled : break # If job stops or hypersearch ends, stop running if self . _isCanceled : break # If the process is about to be killed, set as orphaned if self . _isInterrupted . isSet ( ) : self . __setAsOrphaned ( ) break # If model is mature, stop running ONLY IF we are not the best model # for the job. Otherwise, keep running so we can keep returning # predictions to the user if self . _isMature : if not self . _isBestModel : self . _cmpReason = self . _jobsDAO . CMPL_REASON_STOPPED break else : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF # Turn off learning? if learningOffAt is not None and self . _currentRecordIndex == learningOffAt : self . _model . disableLearning ( ) # Read input record. Note that any failure here is a critical JOB failure # and results in the job being immediately canceled and marked as # failed. The runModelXXX code in hypesearch.utils, if it sees an # exception of type utils.JobFailException, will cancel the job and # copy the error message into the job record. try : inputRecord = self . _inputSource . getNextRecordDict ( ) if self . _currentRecordIndex < 0 : self . _inputSource . setTimeout ( 10 ) except Exception , e : raise utils . JobFailException ( ErrorCodes . streamReading , str ( e . args ) , traceback . format_exc ( ) ) if inputRecord is None : # EOF self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF break if inputRecord : # Process input record self . _currentRecordIndex += 1 result = self . _model . run ( inputRecord = inputRecord ) # Compute metrics. result . metrics = self . __metricMgr . update ( result ) # If there are None, use defaults. see MetricsManager.getMetrics() # TODO remove this when JAVA API server is gone if not result . metrics : result . metrics = self . __metricMgr . getMetrics ( ) # Write the result to the output cache. Don't write encodings, if they # were computed if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) result . sensorInput . dataEncodings = None self . _writePrediction ( result ) # Run periodic activities self . _periodic . tick ( ) if numIters >= 0 and self . _currentRecordIndex >= numIters - 1 : break else : # Input source returned an empty record. # # NOTE: This is okay with Stream-based Source (when it times out # waiting for next record), but not okay with FileSource, which should # always return either with a valid record or None for EOF. raise ValueError ( "Got an empty record from FileSource: %r" % inputRecord )
3160	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id return self . _mc_client . _put ( url = self . _build_path ( campaign_id , 'content' ) , data = data )
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : # get the last working module name short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
1866	def PSHUFD ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) order = Operators . ZEXTEND ( op3 . read ( ) , size ) arg0 = arg0 & 0xffffffffffffffffffffffffffffffff00000000000000000000000000000000 arg0 |= ( ( arg1 >> ( ( ( order >> 0 ) & 3 ) * 32 ) ) & 0xffffffff ) arg0 |= ( ( arg1 >> ( ( ( order >> 2 ) & 3 ) * 32 ) ) & 0xffffffff ) << 32 arg0 |= ( ( arg1 >> ( ( ( order >> 4 ) & 3 ) * 32 ) ) & 0xffffffff ) << 64 arg0 |= ( ( arg1 >> ( ( ( order >> 6 ) & 3 ) * 32 ) ) & 0xffffffff ) << 96 op0 . write ( arg0 )
1126	def Expect ( inner_rule , loc = None ) : @ llrule ( loc , inner_rule . expected ) def rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : expected = reduce ( list . __add__ , [ rule . expected ( parser ) for rule in parser . _errrules ] ) expected = list ( sorted ( set ( expected ) ) ) if len ( expected ) > 1 : expected = " or " . join ( [ ", " . join ( expected [ 0 : - 1 ] ) , expected [ - 1 ] ] ) elif len ( expected ) == 1 : expected = expected [ 0 ] else : expected = "(impossible)" error_tok = parser . _tokens [ parser . _errindex ] error = diagnostic . Diagnostic ( "fatal" , "unexpected {actual}: expected {expected}" , { "actual" : error_tok . kind , "expected" : expected } , error_tok . loc ) parser . diagnostic_engine . process ( error ) return result return rule
9065	def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
13677	def prepare ( self ) : result_files = self . collect_files ( ) chain = self . prepare_handlers_chain if chain is None : # default handlers chain = [ LessCompilerPrepareHandler ( ) ] for prepare_handler in chain : result_files = prepare_handler . prepare ( result_files , self ) return result_files
8984	def to_svg ( self , instruction_or_id , i_promise_not_to_change_the_result = False ) : return self . _new_svg_dumper ( lambda : self . instruction_to_svg_dict ( instruction_or_id , not i_promise_not_to_change_the_result ) )
8354	def start_meta ( self , attrs ) : httpEquiv = None contentType = None contentTypeIndex = None tagNeedsEncodingSubstitution = False for i in range ( 0 , len ( attrs ) ) : key , value = attrs [ i ] key = key . lower ( ) if key == 'http-equiv' : httpEquiv = value elif key == 'content' : contentType = value contentTypeIndex = i if httpEquiv and contentType : # It's an interesting meta tag. match = self . CHARSET_RE . search ( contentType ) if match : if ( self . declaredHTMLEncoding is not None or self . originalEncoding == self . fromEncoding ) : # An HTML encoding was sniffed while converting # the document to Unicode, or an HTML encoding was # sniffed during a previous pass through the # document, or an encoding was specified # explicitly and it worked. Rewrite the meta tag. def rewrite ( match ) : return match . group ( 1 ) + "%SOUP-ENCODING%" newAttr = self . CHARSET_RE . sub ( rewrite , contentType ) attrs [ contentTypeIndex ] = ( attrs [ contentTypeIndex ] [ 0 ] , newAttr ) tagNeedsEncodingSubstitution = True else : # This is our first pass through the document. # Go through it again with the encoding information. newCharset = match . group ( 3 ) if newCharset and newCharset != self . originalEncoding : self . declaredHTMLEncoding = newCharset self . _feed ( self . declaredHTMLEncoding ) raise StopParsing pass tag = self . unknown_starttag ( "meta" , attrs ) if tag and tagNeedsEncodingSubstitution : tag . containsSubstitutions = True
13889	def ListMappedNetworkDrives ( ) : if sys . platform != 'win32' : raise NotImplementedError drives_list = [ ] netuse = _CallWindowsNetCommand ( [ 'use' ] ) for line in netuse . split ( EOL_STYLE_WINDOWS ) : match = re . match ( "(\w*)\s+(\w:)\s+(.+)" , line . rstrip ( ) ) if match : drives_list . append ( ( match . group ( 2 ) , match . group ( 3 ) , match . group ( 1 ) == 'OK' ) ) return drives_list
1287	def process_docstring ( app , what , name , obj , options , lines ) : markdown = "\n" . join ( lines ) # ast = cm_parser.parse(markdown) # html = cm_renderer.render(ast) rest = m2r ( markdown ) rest . replace ( "\r\n" , "\n" ) del lines [ : ] lines . extend ( rest . split ( "\n" ) )
11432	def _check_field_validity ( field ) : if type ( field ) not in ( list , tuple ) : raise InvenioBibRecordFieldError ( "Field of type '%s' should be either " "a list or a tuple." % type ( field ) ) if len ( field ) != 5 : raise InvenioBibRecordFieldError ( "Field of length '%d' should have 5 " "elements." % len ( field ) ) if type ( field [ 0 ] ) not in ( list , tuple ) : raise InvenioBibRecordFieldError ( "Subfields of type '%s' should be " "either a list or a tuple." % type ( field [ 0 ] ) ) if type ( field [ 1 ] ) is not str : raise InvenioBibRecordFieldError ( "Indicator 1 of type '%s' should be " "a string." % type ( field [ 1 ] ) ) if type ( field [ 2 ] ) is not str : raise InvenioBibRecordFieldError ( "Indicator 2 of type '%s' should be " "a string." % type ( field [ 2 ] ) ) if type ( field [ 3 ] ) is not str : raise InvenioBibRecordFieldError ( "Controlfield value of type '%s' " "should be a string." % type ( field [ 3 ] ) ) if type ( field [ 4 ] ) is not int : raise InvenioBibRecordFieldError ( "Global position of type '%s' should " "be an int." % type ( field [ 4 ] ) ) for subfield in field [ 0 ] : if ( type ( subfield ) not in ( list , tuple ) or len ( subfield ) != 2 or type ( subfield [ 0 ] ) is not str or type ( subfield [ 1 ] ) is not str ) : raise InvenioBibRecordFieldError ( "Subfields are malformed. " "Should a list of tuples of 2 strings." )
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
6053	def unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask , unmasked_sparse_grid_pixel_centres , total_sparse_pixels ) : total_unmasked_sparse_pixels = unmasked_sparse_grid_pixel_centres . shape [ 0 ] unmasked_sparse_to_sparse = np . zeros ( total_unmasked_sparse_pixels ) pixel_index = 0 for unmasked_sparse_pixel_index in range ( total_unmasked_sparse_pixels ) : y = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 1 ] unmasked_sparse_to_sparse [ unmasked_sparse_pixel_index ] = pixel_index if not mask [ y , x ] : if pixel_index < total_sparse_pixels - 1 : pixel_index += 1 return unmasked_sparse_to_sparse
6942	def invgauss_eclipses_func ( ebparams , times , mags , errs ) : ( period , epoch , pdepth , pduration , depthratio , secondaryphase ) = ebparams # generate the phases iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) primaryecl_amp = - pdepth secondaryecl_amp = - pdepth * depthratio primaryecl_std = pduration / 5.0 # we use 5-sigma as full-width -> duration secondaryecl_std = pduration / 5.0 # secondary eclipse has the same duration halfduration = pduration / 2.0 # phase indices primary_eclipse_ingress = ( ( phase >= ( 1.0 - halfduration ) ) & ( phase <= 1.0 ) ) primary_eclipse_egress = ( ( phase >= 0.0 ) & ( phase <= halfduration ) ) secondary_eclipse_phase = ( ( phase >= ( secondaryphase - halfduration ) ) & ( phase <= ( secondaryphase + halfduration ) ) ) # put in the eclipses modelmags [ primary_eclipse_ingress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_ingress ] , primaryecl_amp , 1.0 , primaryecl_std ) ) modelmags [ primary_eclipse_egress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_egress ] , primaryecl_amp , 0.0 , primaryecl_std ) ) modelmags [ secondary_eclipse_phase ] = ( zerolevel + _gaussian ( phase [ secondary_eclipse_phase ] , secondaryecl_amp , secondaryphase , secondaryecl_std ) ) return modelmags , phase , ptimes , pmags , perrs
8346	def findAll ( self , name = None , attrs = { } , recursive = True , text = None , limit = None , * * kwargs ) : generator = self . recursiveChildGenerator if not recursive : generator = self . childGenerator return self . _findAll ( name , attrs , text , limit , generator , * * kwargs )
3507	def create_stoichiometric_matrix ( model , array_type = 'dense' , dtype = None ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) if dtype is None : dtype = np . float64 array_constructor = { 'dense' : np . zeros , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : np . zeros , } n_metabolites = len ( model . metabolites ) n_reactions = len ( model . reactions ) array = array_constructor [ array_type ] ( ( n_metabolites , n_reactions ) , dtype = dtype ) m_ind = model . metabolites . index r_ind = model . reactions . index for reaction in model . reactions : for metabolite , stoich in iteritems ( reaction . metabolites ) : array [ m_ind ( metabolite ) , r_ind ( reaction ) ] = stoich if array_type == 'DataFrame' : metabolite_ids = [ met . id for met in model . metabolites ] reaction_ids = [ rxn . id for rxn in model . reactions ] return pd . DataFrame ( array , index = metabolite_ids , columns = reaction_ids ) else : return array
12370	def get ( self , id , * * kwargs ) : return super ( DomainRecords , self ) . get ( id , * * kwargs )
6371	def fallout ( self ) : if self . _fp + self . _tn == 0 : return float ( 'NaN' ) return self . _fp / ( self . _fp + self . _tn )
3708	def COSTALD_mixture ( xs , T , Tcs , Vcs , omegas ) : cmps = range ( len ( xs ) ) if not none_and_length_check ( [ xs , Tcs , Vcs , omegas ] ) : raise Exception ( 'Function inputs are incorrect format' ) sum1 = sum ( [ xi * Vci for xi , Vci in zip ( xs , Vcs ) ] ) sum2 = sum ( [ xi * Vci ** ( 2 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) sum3 = sum ( [ xi * Vci ** ( 1 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) Vm = 0.25 * ( sum1 + 3. * sum2 * sum3 ) VijTcij = [ [ ( Tcs [ i ] * Tcs [ j ] * Vcs [ i ] * Vcs [ j ] ) ** 0.5 for j in cmps ] for i in cmps ] omega = mixing_simple ( xs , omegas ) Tcm = sum ( [ xs [ i ] * xs [ j ] * VijTcij [ i ] [ j ] / Vm for j in cmps for i in cmps ] ) return COSTALD ( T , Tcm , Vm , omega )
10945	def _do_run ( self , mode = '1' ) : for a in range ( len ( self . particle_groups ) ) : group = self . particle_groups [ a ] lp = LMParticles ( self . state , group , * * self . _kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . _dif_tile = self . _load_j_diftile ( a ) if mode == '1' : lp . do_run_1 ( ) if mode == '2' : lp . do_run_2 ( ) if mode == 'internal' : lp . do_internal_run ( ) self . stats . append ( lp . get_termination_stats ( get_cos = self . get_cos ) ) if self . save_J and ( mode != 'internal' ) : self . _dump_j_diftile ( a , lp . J , lp . _dif_tile ) self . _has_saved_J [ a ] = True
2435	def reset_creation_info ( self ) : # FIXME: this state does not make sense self . created_date_set = False self . creation_comment_set = False self . lics_list_ver_set = False
9776	def outputs ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : PolyaxonClient ( ) . job . download_outputs ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download outputs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) # Defined Flags: szp cpu . _calculate_logic_flags ( dest . size , res )
1702	def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
7314	def search ( self ) : try : filters = json . loads ( self . query ) except ValueError : return False result = self . model_query if 'filter' in filters . keys ( ) : result = self . parse_filter ( filters [ 'filter' ] ) if 'sort' in filters . keys ( ) : result = result . order_by ( * self . sort ( filters [ 'sort' ] ) ) return result
12015	def define_spotsignal ( self ) : client = kplr . API ( ) star = client . star ( self . kic ) lcs = star . get_light_curves ( short_cadence = False ) time , flux , ferr , qual = [ ] , [ ] , [ ] , [ ] for lc in lcs : with lc . open ( ) as f : hdu_data = f [ 1 ] . data time . append ( hdu_data [ "time" ] ) flux . append ( hdu_data [ "pdcsap_flux" ] ) ferr . append ( hdu_data [ "pdcsap_flux_err" ] ) qual . append ( hdu_data [ "sap_quality" ] ) tout = np . array ( [ ] ) fout = np . array ( [ ] ) eout = np . array ( [ ] ) for i in range ( len ( flux ) ) : t = time [ i ] [ qual [ i ] == 0 ] f = flux [ i ] [ qual [ i ] == 0 ] e = ferr [ i ] [ qual [ i ] == 0 ] t = t [ np . isfinite ( f ) ] e = e [ np . isfinite ( f ) ] f = f [ np . isfinite ( f ) ] e /= np . median ( f ) f /= np . median ( f ) tout = np . append ( tout , t [ 50 : ] + 54833 ) fout = np . append ( fout , f [ 50 : ] ) eout = np . append ( eout , e [ 50 : ] ) self . spot_signal = np . zeros ( 52 ) for i in range ( len ( self . times ) ) : if self . times [ i ] < 55000 : self . spot_signal [ i ] = 1.0 else : self . spot_signal [ i ] = fout [ np . abs ( self . times [ i ] - tout ) == np . min ( np . abs ( self . times [ i ] - tout ) ) ]
8566	def get_loadbalancer_members ( self , datacenter_id , loadbalancer_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics?depth=%s' % ( datacenter_id , loadbalancer_id , str ( depth ) ) ) return response
11519	def perform_upload ( self , upload_token , filename , * * kwargs ) : parameters = dict ( ) parameters [ 'uploadtoken' ] = upload_token parameters [ 'filename' ] = filename try : create_additional_revision = kwargs [ 'create_additional_revision' ] except KeyError : create_additional_revision = False if not create_additional_revision : parameters [ 'revision' ] = 'head' optional_keys = [ 'mode' , 'folderid' , 'item_id' , 'itemid' , 'revision' ] for key in optional_keys : if key in kwargs : if key == 'item_id' : parameters [ 'itemid' ] = kwargs [ key ] continue if key == 'folder_id' : parameters [ 'folderid' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] # We may want a different name than path file_payload = open ( kwargs . get ( 'filepath' , filename ) , 'rb' ) # Arcane getting of the file size using fstat. More details can be # found in the python library docs parameters [ 'length' ] = os . fstat ( file_payload . fileno ( ) ) . st_size response = self . request ( 'midas.upload.perform' , parameters , file_payload ) return response
8171	def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
7493	def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) )
6975	def stellingwerf_pdm_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binvariances = [ ] binndets = [ ] goodbins = 0 for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_variance = npvar ( thisbin_mags , ddof = 1 ) binvariances . append ( thisbin_variance ) binndets . append ( thisbin_mags . size ) goodbins = goodbins + 1 # now calculate theta binvariances = nparray ( binvariances ) binndets = nparray ( binndets ) theta_top = npsum ( binvariances * ( binndets - 1 ) ) / ( npsum ( binndets ) - goodbins ) theta_bot = npvar ( pmags , ddof = 1 ) theta = theta_top / theta_bot return theta
8116	def line_line_intersection ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 , infinite = False ) : # Based on: P. Bourke, http://local.wasp.uwa.edu.au/~pbourke/geometry/lineline2d/ ua = ( x4 - x3 ) * ( y1 - y3 ) - ( y4 - y3 ) * ( x1 - x3 ) ub = ( x2 - x1 ) * ( y1 - y3 ) - ( y2 - y1 ) * ( x1 - x3 ) d = ( y4 - y3 ) * ( x2 - x1 ) - ( x4 - x3 ) * ( y2 - y1 ) if d == 0 : if ua == ub == 0 : # The lines are coincident return [ ] else : # The lines are parallel. return [ ] ua /= float ( d ) ub /= float ( d ) if not infinite and not ( 0 <= ua <= 1 and 0 <= ub <= 1 ) : # Intersection point is not within both line segments. return None , None return [ ( x1 + ua * ( x2 - x1 ) , y1 + ua * ( y2 - y1 ) ) ]
4085	def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
1015	def _getCellForNewSegment ( self , colIdx ) : # Not fixed size CLA, just choose a cell randomly if self . maxSegmentsPerCell < 0 : if self . cellsPerColumn > 1 : # Don't ever choose the start cell (cell # 0) in each column i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 else : i = 0 return i # Fixed size CLA, choose from among the cells that are below the maximum # number of segments. # NOTE: It is important NOT to always pick the cell with the fewest number # of segments. The reason is that if we always do that, we are more likely # to run into situations where we choose the same set of cell indices to # represent an 'A' in both context 1 and context 2. This is because the # cell indices we choose in each column of a pattern will advance in # lockstep (i.e. we pick cell indices of 1, then cell indices of 2, etc.). candidateCellIdxs = [ ] if self . cellsPerColumn == 1 : minIdx = 0 maxIdx = 0 else : minIdx = 1 # Don't include startCell in the mix maxIdx = self . cellsPerColumn - 1 for i in xrange ( minIdx , maxIdx + 1 ) : numSegs = len ( self . cells [ colIdx ] [ i ] ) if numSegs < self . maxSegmentsPerCell : candidateCellIdxs . append ( i ) # If we found one, return with it. Note we need to use _random to maintain # correspondence with CPP code. if len ( candidateCellIdxs ) > 0 : #candidateCellIdx = random.choice(candidateCellIdxs) candidateCellIdx = ( candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) if self . verbosity >= 5 : print "Cell [%d,%d] chosen for new segment, # of segs is %d" % ( colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) return candidateCellIdx # All cells in the column are full, find a segment to free up candidateSegment = None candidateSegmentDC = 1.0 # For each cell in this column for i in xrange ( minIdx , maxIdx + 1 ) : # For each segment in this cell for s in self . cells [ colIdx ] [ i ] : dc = s . dutyCycle ( ) if dc < candidateSegmentDC : candidateCellIdx = i candidateSegmentDC = dc candidateSegment = s # Free up the least used segment if self . verbosity >= 5 : print ( "Deleting segment #%d for cell[%d,%d] to make room for new " "segment" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) candidateSegment . debugPrint ( ) self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) return candidateCellIdx
12429	def create_virtualenv ( self ) : if check_command ( 'virtualenv' ) : ve_dir = os . path . join ( self . _ve_dir , self . _project_name ) if os . path . exists ( ve_dir ) : if self . _force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve_dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve_dir ) , shell = True ) os . waitpid ( p . pid , 0 ) # install modules for m in self . _modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . _ve_dir , self . _project_name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
1374	def parse_override_config_and_write_file ( namespace ) : overrides = parse_override_config ( namespace ) try : tmp_dir = tempfile . mkdtemp ( ) override_config_file = os . path . join ( tmp_dir , OVERRIDE_YAML ) with open ( override_config_file , 'w' ) as f : f . write ( yaml . dump ( overrides ) ) return override_config_file except Exception as e : raise Exception ( "Failed to parse override config: %s" % str ( e ) )
4445	def create_index ( self , fields , no_term_offsets = False , no_field_flags = False , stopwords = None ) : args = [ self . CREATE_CMD , self . index_name ] if no_term_offsets : args . append ( self . NOOFFSETS ) if no_field_flags : args . append ( self . NOFIELDS ) if stopwords is not None and isinstance ( stopwords , ( list , tuple , set ) ) : args += [ self . STOPWORDS , len ( stopwords ) ] if len ( stopwords ) > 0 : args += list ( stopwords ) args . append ( 'SCHEMA' ) args += list ( itertools . chain ( * ( f . redis_args ( ) for f in fields ) ) ) return self . redis . execute_command ( * args )
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
3812	async def set_active ( self ) : is_active = ( self . _active_client_state == hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) timed_out = ( time . time ( ) - self . _last_active_secs > SETACTIVECLIENT_LIMIT_SECS ) if not is_active or timed_out : # Update these immediately so if the function is called again # before the API request finishes, we don't start extra requests. self . _active_client_state = ( hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) self . _last_active_secs = time . time ( ) # The first time this is called, we need to retrieve the user's # email address. if self . _email is None : try : get_self_info_request = hangouts_pb2 . GetSelfInfoRequest ( request_header = self . get_request_header ( ) , ) get_self_info_response = await self . get_self_info ( get_self_info_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to find email address: {}' . format ( e ) ) return self . _email = ( get_self_info_response . self_entity . properties . email [ 0 ] ) # If the client_id hasn't been received yet, we can't set the # active client. if self . _client_id is None : logger . info ( 'Cannot set active client until client_id is received' ) return try : set_active_request = hangouts_pb2 . SetActiveClientRequest ( request_header = self . get_request_header ( ) , is_active = True , full_jid = "{}/{}" . format ( self . _email , self . _client_id ) , timeout_secs = ACTIVE_TIMEOUT_SECS , ) await self . set_active_client ( set_active_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set active client: {}' . format ( e ) ) else : logger . info ( 'Set active client for {} seconds' . format ( ACTIVE_TIMEOUT_SECS ) )
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
9379	def detect_timestamp_format ( timestamp ) : time_formats = { 'epoch' : re . compile ( r'^[0-9]{10}$' ) , 'epoch_ms' : re . compile ( r'^[0-9]{13}$' ) , 'epoch_fraction' : re . compile ( r'^[0-9]{10}\.[0-9]{3,9}$' ) , '%Y-%m-%d %H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d %H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%H:%M:%S' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%H:%M:%S.%f' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f%z' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+[+-][0-9]{4}$' ) } for time_format in time_formats : if re . match ( time_formats [ time_format ] , timestamp ) : return time_format return 'unknown'
3567	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_started . clear ( ) self . _adapter . StartDiscovery ( ) if not self . _scan_started . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to start scanning!' )
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
746	def anomalyAddLabel ( self , start , end , labelName ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . addLabel ( start , end , labelName )
4473	def __recursive_transform ( self , jam , steps ) : if len ( steps ) > 0 : head_transformer = steps [ 0 ] [ 1 ] for t_jam in head_transformer . transform ( jam ) : for q in self . __recursive_transform ( t_jam , steps [ 1 : ] ) : yield q else : yield jam
13530	def add_child ( self , * * kwargs ) : data_class = self . graph . data_content_type . model_class ( ) node = Node . objects . create ( graph = self . graph ) data_class . objects . create ( node = node , * * kwargs ) node . parents . add ( self ) self . children . add ( node ) return node
6509	def set_search_enviroment ( cls , * * kwargs ) : initializer = _load_class ( getattr ( settings , "SEARCH_INITIALIZER" , None ) , cls ) ( ) return initializer . initialize ( * * kwargs )
12474	def join_path_to_filelist ( path , filelist ) : return [ op . join ( path , str ( item ) ) for item in filelist ]
3894	async def _async_main ( example_coroutine , client , args ) : # Spawn a task for hangups to run in parallel with the example coroutine. task = asyncio . ensure_future ( client . connect ( ) ) # Wait for hangups to either finish connecting or raise an exception. on_connect = asyncio . Future ( ) client . on_connect . add_observer ( lambda : on_connect . set_result ( None ) ) done , _ = await asyncio . wait ( ( on_connect , task ) , return_when = asyncio . FIRST_COMPLETED ) await asyncio . gather ( * done ) # Run the example coroutine. Afterwards, disconnect hangups gracefully and # yield the hangups task to handle any exceptions. try : await example_coroutine ( client , args ) except asyncio . CancelledError : pass finally : await client . disconnect ( ) await task
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
3196	def delete_permanent ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'actions' , 'delete-permanent' ) )
10708	def delete_vacation ( _id ) : arequest = requests . delete ( VACATIONS_URL + "/" + _id , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "Failed to delete vacation. " + status_code ) return False return True
13327	def remove ( name_or_path ) : click . echo ( ) try : r = cpenv . resolve ( name_or_path ) except cpenv . ResolveError as e : click . echo ( e ) return obj = r . resolved [ 0 ] if not isinstance ( obj , cpenv . VirtualEnvironment ) : click . echo ( '{} is a module. Use `cpenv module remove` instead.' ) return click . echo ( format_objects ( [ obj ] ) ) click . echo ( ) user_confirmed = click . confirm ( red ( 'Are you sure you want to remove this environment?' ) ) if user_confirmed : click . echo ( 'Attempting to remove...' , nl = False ) try : obj . remove ( ) except Exception as e : click . echo ( bold_red ( 'FAIL' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'OK!' ) )
11267	def walk ( prev , inital_path , * args , * * kw ) : for dir_path , dir_names , filenames in os . walk ( inital_path ) : for filename in filenames : yield os . path . join ( dir_path , filename )
10773	def add_node ( self , node , offset ) : # calculate x,y from offset considering axis start and end points width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
13562	def save ( self , * args , * * kwargs ) : rerank = kwargs . pop ( 'rerank' , True ) if rerank : if not self . id : self . _process_new_rank_obj ( ) elif self . rank == self . _rank_at_load : # nothing changed pass else : self . _process_moved_rank_obj ( ) super ( RankedModel , self ) . save ( * args , * * kwargs )
6184	def get_git_version ( git_path = None ) : if git_path is None : git_path = GIT_PATH git_version = check_output ( [ git_path , "--version" ] ) . split ( ) [ 2 ] return git_version
12325	def untokenize ( tokens ) : text = '' previous_line = '' last_row = 0 last_column = - 1 last_non_whitespace_token_type = None for ( token_type , token_string , start , end , line ) in tokens : if TOKENIZE_HAS_ENCODING and token_type == tokenize . ENCODING : continue ( start_row , start_column ) = start ( end_row , end_column ) = end # Preserve escaped newlines. if ( last_non_whitespace_token_type != tokenize . COMMENT and start_row > last_row and previous_line . endswith ( ( '\\\n' , '\\\r\n' , '\\\r' ) ) ) : text += previous_line [ len ( previous_line . rstrip ( ' \t\n\r\\' ) ) : ] # Preserve spacing. if start_row > last_row : last_column = 0 if start_column > last_column : text += line [ last_column : start_column ] text += token_string previous_line = line last_row = end_row last_column = end_column if token_type not in WHITESPACE_TOKENS : last_non_whitespace_token_type = token_type return text
11795	def min_conflicts ( csp , max_steps = 100000 ) : # Generate a complete assignment for all vars (probably with conflicts) csp . current = current = { } for var in csp . vars : val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) # Now repeatedly choose a random conflicted variable and change it for i in range ( max_steps ) : conflicted = csp . conflicted_vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) return None
547	def __flushPredictionCache ( self ) : if not self . __predictionCache : return # Create an output store, if one doesn't exist already if self . _predictionLogger is None : self . _createPredictionLogger ( ) startTime = time . time ( ) self . _predictionLogger . writeRecords ( self . __predictionCache , progressCB = self . __writeRecordsCallback ) self . _logger . info ( "Flushed prediction cache; numrows=%s; elapsed=%s sec." , len ( self . __predictionCache ) , time . time ( ) - startTime ) self . __predictionCache . clear ( )
2940	def deserialize_condition ( self , workflow , start_node ) : # Collect all information. condition = None spec_name = None for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'successor' : if spec_name is not None : _exc ( 'Duplicate task name %s' % spec_name ) if node . firstChild is None : _exc ( 'Successor tag without a task name' ) spec_name = node . firstChild . nodeValue elif node . nodeName . lower ( ) in _op_map : if condition is not None : _exc ( 'Multiple conditions are not yet supported' ) condition = self . deserialize_logical ( node ) else : _exc ( 'Unknown node: %s' % node . nodeName ) if condition is None : _exc ( 'Missing condition in conditional statement' ) if spec_name is None : _exc ( 'A %s has no task specified' % start_node . nodeName ) return condition , spec_name
13509	def sloccount ( ) : # filter out subpackages setup = options . get ( 'setup' ) packages = options . get ( 'packages' ) if setup else None if packages : dirs = [ x for x in packages if '.' not in x ] else : dirs = [ '.' ] # sloccount has strange behaviour with directories, # can cause exception in hudson sloccount plugin. # Better to call it with file list ls = [ ] for d in dirs : ls += list ( path ( d ) . walkfiles ( ) ) #ls=list(set(ls)) files = ' ' . join ( ls ) param = options . paved . pycheck . sloccount . param sh ( 'sloccount {param} {files} | tee sloccount.sc' . format ( param = param , files = files ) )
10551	def update_result ( result ) : try : result_id = result . id result = _forbidden_attributes ( result ) res = _pybossa_req ( 'put' , 'result' , result_id , payload = result . data ) if res . get ( 'id' ) : return Result ( res ) else : return res except : # pragma: no cover raise
11118	def get_parent_directory_info ( self , relativePath ) : relativePath = os . path . normpath ( relativePath ) # if root directory if relativePath in ( '' , '.' ) : return self , "relativePath is empty pointing to the repostitory itself." # split path parentDirPath , _ = os . path . split ( relativePath ) # get parent directory info return self . get_directory_info ( parentDirPath )
6617	def expand_path_cfg ( path_cfg , alias_dict = { } , overriding_kargs = { } ) : if isinstance ( path_cfg , str ) : return _expand_str ( path_cfg , alias_dict , overriding_kargs ) if isinstance ( path_cfg , dict ) : return _expand_dict ( path_cfg , alias_dict ) # assume tuple or list return _expand_tuple ( path_cfg , alias_dict , overriding_kargs )
10561	def _mutagen_fields_to_single_value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
4599	def _clean_animation ( desc , parent ) : desc = load . load_if_filename ( desc ) or desc if isinstance ( desc , str ) : animation = { 'typename' : desc } elif not isinstance ( desc , dict ) : raise TypeError ( 'Unexpected type %s in collection' % type ( desc ) ) elif 'typename' in desc or 'animation' not in desc : animation = desc else : animation = desc . pop ( 'animation' , { } ) if isinstance ( animation , str ) : animation = { 'typename' : animation } animation [ 'run' ] = desc . pop ( 'run' , { } ) if desc : raise ValueError ( 'Extra animation fields: ' + ', ' . join ( desc ) ) animation . setdefault ( 'typename' , DEFAULT_ANIMATION ) animation = construct . to_type_constructor ( animation , ANIMATION_PATH ) datatype = animation . setdefault ( 'datatype' , failed . Failed ) animation . setdefault ( 'name' , datatype . __name__ ) # Children without fps or sleep_time get it from their parents. # TODO: We shouldn't have to rewrite our descriptions here! The # animation engine should be smart enough to figure out the right # speed to run a subanimation without a run: section. run = animation . setdefault ( 'run' , { } ) run_parent = parent . setdefault ( 'run' , { } ) if not ( 'fps' in run or 'sleep_time' in run ) : if 'fps' in run_parent : run . update ( fps = run_parent [ 'fps' ] ) elif 'sleep_time' in run_parent : run . update ( sleep_time = run_parent [ 'sleep_time' ] ) return animation
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) # Based on what Hangouts for Chrome does over 2 requests, this is # trimmed down to 1 request that includes the bare minimum to make # things work. services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
8850	def setup_editor ( self , editor ) : editor . cursorPositionChanged . connect ( self . on_cursor_pos_changed ) try : m = editor . modes . get ( modes . GoToAssignmentsMode ) except KeyError : pass else : assert isinstance ( m , modes . GoToAssignmentsMode ) m . out_of_doc . connect ( self . on_goto_out_of_doc )
6361	def sim_matrix ( src , tar , mat = None , mismatch_cost = 0 , match_cost = 1 , symmetric = True , alphabet = None , ) : if alphabet : alphabet = tuple ( alphabet ) for i in src : if i not in alphabet : raise ValueError ( 'src value not in alphabet' ) for i in tar : if i not in alphabet : raise ValueError ( 'tar value not in alphabet' ) if src == tar : if mat and ( src , src ) in mat : return mat [ ( src , src ) ] return match_cost if mat and ( src , tar ) in mat : return mat [ ( src , tar ) ] elif symmetric and mat and ( tar , src ) in mat : return mat [ ( tar , src ) ] return mismatch_cost
9762	def unbookmark ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : PolyaxonClient ( ) . experiment . unbookmark ( user , project_name , _experiment ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not unbookmark experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment is unbookmarked." )
2478	def reset ( self ) : # FIXME: this state does not make sense self . reset_creation_info ( ) self . reset_document ( ) self . reset_package ( ) self . reset_file_stat ( ) self . reset_reviews ( ) self . reset_annotations ( ) self . reset_extr_lics ( )
4000	def _mount_repo ( repo , wait_for_server = False ) : check_call_on_vm ( 'sudo mkdir -p {}' . format ( repo . vm_path ) ) if wait_for_server : for i in range ( 0 , 10 ) : try : _run_mount_command ( repo ) return except CalledProcessError as e : if 'Connection refused' in e . output : logging . info ( 'Failed to mount repo; waiting for nfsd to restart' ) time . sleep ( 1 ) else : logging . info ( e . output ) raise e log_to_client ( 'Failed to mount repo {}' . format ( repo . short_name ) ) raise RuntimeError ( 'Unable to mount repo with NFS' ) else : _run_mount_command ( repo )
8337	def findParent ( self , name = None , attrs = { } , * * kwargs ) : # NOTE: We can't use _findOne because findParents takes a different # set of arguments. r = None l = self . findParents ( name , attrs , 1 ) if l : r = l [ 0 ] return r
6827	def add_remote ( self , path , name , remote_url , use_sudo = False , user = None , fetch = True ) : if path is None : raise ValueError ( "Path to the working copy is needed to add a remote" ) if fetch : cmd = 'git remote add -f %s %s' % ( name , remote_url ) else : cmd = 'git remote add %s %s' % ( name , remote_url ) with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
7254	def heartbeat ( self ) : url = '%s/heartbeat' % self . base_url # Auth is not required to hit the heartbeat r = requests . get ( url ) try : return r . json ( ) == "ok" except : return False
13362	def prompt ( text , default = None , hide_input = False , confirmation_prompt = False , type = None , value_proc = None , prompt_suffix = ': ' , show_default = True , err = False ) : result = None def prompt_func ( text ) : f = hide_input and hidden_prompt_func or visible_prompt_func try : # Write the prompt separately so that we get nice # coloring through colorama on Windows echo ( text , nl = False , err = err ) return f ( '' ) except ( KeyboardInterrupt , EOFError ) : # getpass doesn't print a newline if the user aborts input with ^C. # Allegedly this behavior is inherited from getpass(3). # A doc bug has been filed at https://bugs.python.org/issue24711 if hide_input : echo ( None , err = err ) raise Abort ( ) if value_proc is None : value_proc = convert_type ( type , default ) prompt = _build_prompt ( text , prompt_suffix , show_default , default ) while 1 : while 1 : value = prompt_func ( prompt ) if value : break # If a default is set and used, then the confirmation # prompt is always skipped because that's the only thing # that really makes sense. elif default is not None : return default try : result = value_proc ( value ) except UsageError as e : echo ( 'Error: %s' % e . message , err = err ) continue if not confirmation_prompt : return result while 1 : value2 = prompt_func ( 'Repeat for confirmation: ' ) if value2 : break if value == value2 : return result echo ( 'Error: the two entered values do not match' , err = err )
1297	def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
8569	def remove_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics/%s' % ( datacenter_id , loadbalancer_id , nic_id ) , method = 'DELETE' ) return response
8951	def get_devpi_url ( ctx ) : cmd = 'devpi use --urls' lines = ctx . run ( cmd , hide = 'out' , echo = False ) . stdout . splitlines ( ) for line in lines : try : line , base_url = line . split ( ':' , 1 ) except ValueError : notify . warning ( 'Ignoring "{}"!' . format ( line ) ) else : if line . split ( ) [ - 1 ] . strip ( ) == 'simpleindex' : return base_url . split ( '\x1b' ) [ 0 ] . strip ( ) . rstrip ( '/' ) raise LookupError ( "Cannot find simpleindex URL in '{}' output:\n {}" . format ( cmd , '\n ' . join ( lines ) , ) )
12917	def deref ( self , ctx ) : if self in ctx . call_nodes : raise CyclicReferenceError ( ctx , self ) if self in ctx . cached_results : return ctx . cached_results [ self ] try : ctx . call_nodes . add ( self ) ctx . call_stack . append ( self ) result = self . evaluate ( ctx ) ctx . cached_results [ self ] = result return result except : if ctx . exception_call_stack is None : ctx . exception_call_stack = list ( ctx . call_stack ) raise finally : ctx . call_stack . pop ( ) ctx . call_nodes . remove ( self )
8101	def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
5499	def add_tweets ( self , url , last_modified , tweets ) : try : self . cache [ url ] = { "last_modified" : last_modified , "tweets" : tweets } self . mark_updated ( ) return True except TypeError : return False
5951	def strftime ( self , fmt = "%d:%H:%M:%S" ) : substitutions = { "%d" : str ( self . days ) , "%H" : "{0:02d}" . format ( self . dhours ) , "%h" : str ( 24 * self . days + self . dhours ) , "%M" : "{0:02d}" . format ( self . dminutes ) , "%S" : "{0:02d}" . format ( self . dseconds ) , } s = fmt for search , replacement in substitutions . items ( ) : s = s . replace ( search , replacement ) return s
5250	def start ( self ) : # flush event queue in defensive way logger = _get_logger ( self . debug ) started = self . _session . start ( ) if started : ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) else : ev = self . _session . nextEvent ( self . timeout ) if ev . eventType ( ) == blpapi . Event . SESSION_STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise ConnectionError ( 'Could not start blpapi.Session' ) self . _init_services ( ) return self
11935	def reuse ( context , block_list , * * kwargs ) : try : block_context = context . render_context [ BLOCK_CONTEXT_KEY ] except KeyError : block_context = BlockContext ( ) if not isinstance ( block_list , ( list , tuple ) ) : block_list = [ block_list ] for block in block_list : block = block_context . get_block ( block ) if block : break else : return '' with context . push ( kwargs ) : return block . render ( context )
2345	def forward ( self , x ) : features = self . conv ( x ) . mean ( dim = 2 ) return self . dense ( features )
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
7784	def got_it ( self , value , state = "new" ) : if not self . active : return item = CacheItem ( self . address , value , self . _item_freshness_period , self . _item_expiration_period , self . _item_purge_period , state ) self . _object_handler ( item . address , item . value , item . state ) self . cache . add_item ( item ) self . _deactivate ( )
9190	def admin_content_status_single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except ValueError : raise httpexceptions . HTTPBadRequest ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql_args = get_baking_statuses_sql ( { 'uuid' : uuid } ) with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( statement , sql_args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( uuid ) ) states = [ ] collection_info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : # pragma: no cover if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest_recipe = row [ 'latest_recipe_id' ] current_recipe = row [ 'recipe_id' ] if ( latest_recipe is not None and current_recipe != latest_recipe ) : state += ' stale_recipe' states . append ( { 'version' : row [ 'current_version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state_message' : message , } ) return { 'uuid' : str ( collection_info [ 'uuid' ] ) , 'title' : collection_info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format_authors ( collection_info [ 'authors' ] ) , 'print_style' : collection_info [ 'print_style' ] , 'current_recipe' : collection_info [ 'recipe_id' ] , 'current_ident' : collection_info [ 'module_ident' ] , 'current_state' : states [ 0 ] [ 'state' ] , 'states' : states }
355	def save_npz_dict ( save_list = None , name = 'model.npz' , sess = None ) : if sess is None : raise ValueError ( "session is None." ) if save_list is None : save_list = [ ] save_list_names = [ tensor . name for tensor in save_list ] save_list_var = sess . run ( save_list ) save_var_dict = { save_list_names [ idx ] : val for idx , val in enumerate ( save_list_var ) } np . savez ( name , * * save_var_dict ) save_list_var = None save_var_dict = None del save_list_var del save_var_dict logging . info ( "[*] Model saved in npz_dict %s" % name )
1609	def make_tick_tuple ( ) : return HeronTuple ( id = TupleHelper . TICK_TUPLE_ID , component = TupleHelper . TICK_SOURCE_COMPONENT , stream = TupleHelper . TICK_TUPLE_ID , task = None , values = None , creation_time = time . time ( ) , roots = None )
8785	def update_port ( self , context , port_id , * * kwargs ) : LOG . info ( "update_port %s %s" % ( context . tenant_id , port_id ) ) # TODO(morgabra): Change this when we enable security groups. if kwargs . get ( "security_groups" ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) return { "uuid" : port_id }
13291	def get_variables_by_attributes ( self , * * kwargs ) : vs = [ ] has_value_flag = False for vname in self . variables : var = self . variables [ vname ] for k , v in kwargs . items ( ) : if callable ( v ) : has_value_flag = v ( getattr ( var , k , None ) ) if has_value_flag is False : break elif hasattr ( var , k ) and getattr ( var , k ) == v : has_value_flag = True else : has_value_flag = False break if has_value_flag is True : vs . append ( self . variables [ vname ] ) return vs
11755	def tt_check_all ( kb , alpha , symbols , model ) : if not symbols : if pl_true ( kb , model ) : result = pl_true ( alpha , model ) assert result in ( True , False ) return result else : return True else : P , rest = symbols [ 0 ] , symbols [ 1 : ] return ( tt_check_all ( kb , alpha , rest , extend ( model , P , True ) ) and tt_check_all ( kb , alpha , rest , extend ( model , P , False ) ) )
8416	def precision ( x ) : from . bounds import zero_range rng = min_max ( x , na_rm = True ) if zero_range ( rng ) : span = np . abs ( rng [ 0 ] ) else : span = np . diff ( rng ) [ 0 ] if span == 0 : return 1 else : return 10 ** int ( np . floor ( np . log10 ( span ) ) )
6180	def merge_DA_ph_times ( ph_times_d , ph_times_a ) : ph_times = np . hstack ( [ ph_times_d , ph_times_a ] ) a_em = np . hstack ( [ np . zeros ( ph_times_d . size , dtype = np . bool ) , np . ones ( ph_times_a . size , dtype = np . bool ) ] ) index_sort = ph_times . argsort ( ) return ph_times [ index_sort ] , a_em [ index_sort ]
2432	def add_creator ( self , doc , creator ) : if validations . validate_creator ( creator ) : doc . creation_info . add_creator ( creator ) return True else : raise SPDXValueError ( 'CreationInfo::Creator' )
12367	def create ( self , name , ip_address ) : return ( self . post ( name = name , ip_address = ip_address ) . get ( self . singular , None ) )
6356	def dist_strcmp95 ( src , tar , long_strings = False ) : return Strcmp95 ( ) . dist ( src , tar , long_strings )
1764	def push_int ( self , value , force = False ) : self . STACK -= self . address_bit_size // 8 self . write_int ( self . STACK , value , force = force ) return self . STACK
2471	def set_file_atrificat_of_project ( self , doc , symbol , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_artifact ( symbol , value ) else : raise OrderError ( 'File::Artificat' )
6063	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = major_axis , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
3804	def calculate ( self , T , method ) : if method == GHARAGHEIZI_G : kg = Gharagheizi_gas ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == DIPPR_9B : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = DIPPR9B ( T , self . MW , Cvgm , mug , self . Tc ) elif method == CHUNG : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Chung ( T , self . MW , self . Tc , self . omega , Cvgm , mug ) elif method == ELI_HANLEY : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm ) elif method == EUCKEN_MOD : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken_modified ( self . MW , Cvgm , mug ) elif method == EUCKEN : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken ( self . MW , Cvgm , mug ) elif method == DIPPR_PERRY_8E : kg = EQ102 ( T , * self . Perrys2_314_coeffs ) elif method == VDI_PPDS : kg = horner ( self . VDI_PPDS_coeffs , T ) elif method == BAHADORI_G : kg = Bahadori_gas ( T , self . MW ) elif method == COOLPROP : kg = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'g' ) elif method in self . tabular_data : kg = self . interpolate ( T , method ) return kg
12510	def get_img_data ( image , copy = True ) : try : img = check_img ( image ) if copy : return get_data ( img ) else : return img . get_data ( ) except Exception as exc : raise Exception ( 'Error when reading file {0}.' . format ( repr_imgs ( image ) ) ) from exc
6262	def resize ( self , width , height ) : self . width = width self . height = height self . buffer_width , self . buffer_height = glfw . get_framebuffer_size ( self . window ) self . set_default_viewport ( )
4228	def _config_root_Linux ( ) : _check_old_config_root ( ) fallback = os . path . expanduser ( '~/.local/share' ) key = 'XDG_CONFIG_HOME' root = os . environ . get ( key , None ) or fallback return os . path . join ( root , 'python_keyring' )
5275	def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
2908	def _find_child_of ( self , parent_task_spec ) : if self . parent is None : return self if self . parent . task_spec == parent_task_spec : return self return self . parent . _find_child_of ( parent_task_spec )
7844	def set_type ( self , item_type ) : if not item_type : raise ValueError ( "Type is required in DiscoIdentity" ) item_type = unicode ( item_type ) self . xmlnode . setProp ( "type" , item_type . encode ( "utf-8" ) )
6284	def start ( self ) : self . music . start ( ) if not self . start_paused : self . rocket . start ( )
13036	def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) # pass the total entry number for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
10704	def get_device ( _id ) : url = DEVICE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
8223	def main_iteration ( self ) : if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( ) for snapshot_f in self . scheduled_snapshots : fn = snapshot_f ( self . last_draw_ctx ) print ( "Saved snapshot: %s" % fn ) else : self . scheduled_snapshots = deque ( ) while Gtk . events_pending ( ) : Gtk . main_iteration ( )
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : # Under an existing coefficient set - assume Tmax will come from another set self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return # Must be appended to end instead self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
5551	def snap_bounds ( bounds = None , pyramid = None , zoom = None ) : if not isinstance ( bounds , ( tuple , list ) ) : raise TypeError ( "bounds must be either a tuple or a list" ) if len ( bounds ) != 4 : raise ValueError ( "bounds has to have exactly four values" ) if not isinstance ( pyramid , BufferedTilePyramid ) : raise TypeError ( "pyramid has to be a BufferedTilePyramid" ) bounds = Bounds ( * bounds ) lb = pyramid . tile_from_xy ( bounds . left , bounds . bottom , zoom , on_edge_use = "rt" ) . bounds rt = pyramid . tile_from_xy ( bounds . right , bounds . top , zoom , on_edge_use = "lb" ) . bounds return Bounds ( lb . left , lb . bottom , rt . right , rt . top )
9653	def write_shas_to_shastore ( sha_dict ) : if sys . version_info [ 0 ] < 3 : fn_open = open else : fn_open = io . open with fn_open ( ".shastore" , "w" ) as fh : fh . write ( "---\n" ) fh . write ( 'sake version: {}\n' . format ( constants . VERSION ) ) if sha_dict : fh . write ( yaml . dump ( sha_dict ) ) fh . write ( "..." )
6997	def runcp_worker ( task ) : pfpickle , outdir , lcbasedir , kwargs = task try : return runcp ( pfpickle , outdir , lcbasedir , * * kwargs ) except Exception as e : LOGEXCEPTION ( ' could not make checkplots for %s: %s' % ( pfpickle , e ) ) return None
2550	def system ( cmd , data = None ) : import subprocess s = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) out , err = s . communicate ( data ) return out . decode ( 'utf8' )
13246	async def _download_lsst_bibtex ( bibtex_names ) : blob_url_template = ( 'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/' 'bibtex/bib/{name}.bib' ) urls = [ blob_url_template . format ( name = name ) for name in bibtex_names ] tasks = [ ] async with ClientSession ( ) as session : for url in urls : task = asyncio . ensure_future ( _download_text ( url , session ) ) tasks . append ( task ) return await asyncio . gather ( * tasks )
6454	def stem ( self , word ) : # lowercase, normalize, and compose word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) # remove umlauts word = word . translate ( self . _accents ) # Step 1 wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] # Step 2 wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
5957	def merge_ndx ( * args ) : ndxs = [ ] struct = None for fname in args : if fname . endswith ( '.ndx' ) : ndxs . append ( fname ) else : if struct is not None : raise ValueError ( "only one structure file supported" ) struct = fname fd , multi_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'multi_' ) os . close ( fd ) atexit . register ( os . unlink , multi_ndx ) if struct : make_ndx = registry [ 'Make_ndx' ] ( f = struct , n = ndxs , o = multi_ndx ) else : make_ndx = registry [ 'Make_ndx' ] ( n = ndxs , o = multi_ndx ) _ , _ , _ = make_ndx ( input = [ 'q' ] , stdout = False , stderr = False ) return multi_ndx
10075	def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] # ignore _deposit and $schema field args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
12348	def stitch_coordinates ( self , well_row = 0 , well_column = 0 ) : well = [ w for w in self . wells if attribute ( w , 'u' ) == well_column and attribute ( w , 'v' ) == well_row ] if len ( well ) == 1 : well = well [ 0 ] tile = os . path . join ( well , 'TileConfiguration.registered.txt' ) with open ( tile ) as f : data = [ x . strip ( ) for l in f . readlines ( ) if l [ 0 : 7 ] == 'image--' for x in l . split ( ';' ) ] # flat list coordinates = ( ast . literal_eval ( x ) for x in data [ 2 : : 3 ] ) # flatten coordinates = sum ( coordinates , ( ) ) attr = tuple ( attributes ( x ) for x in data [ 0 : : 3 ] ) return coordinates [ 0 : : 2 ] , coordinates [ 1 : : 2 ] , attr else : print ( 'leicaexperiment stitch_coordinates' '({}, {}) Well not found' . format ( well_row , well_column ) )
848	def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : encoders = self . encoder . getEncoderList ( ) types = self . encoder . getDecoderOutputFieldTypes ( ) for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : spatialData = spatialOutput [ i ] temporalData = temporalOutput [ i ] if type != FieldMetaType . integer and type != FieldMetaType . float : # TODO: Make sure that this doesn't modify any state spatialData = encoder . getScalars ( spatialData ) [ 0 ] temporalData = encoder . getScalars ( temporalData ) [ 0 ] assert isinstance ( spatialData , ( float , int ) ) assert isinstance ( temporalData , ( float , int ) ) output [ 'spatialTopDownOut' ] [ i ] = spatialData output [ 'temporalTopDownOut' ] [ i ] = temporalData
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
8265	def _interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in _range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
971	def _copyAllocatedStates ( self ) : # Get learn states if we need to print them out if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )
1271	def setup_components_and_tf_funcs ( self , custom_getter = None ) : # Create network before super-call, since non-empty internals_spec attribute (for RNN) is required subsequently. self . network = Network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) # Now that we have the network component: We can create the internals placeholders. assert len ( self . internals_spec ) == 0 self . internals_spec = self . network . internals_spec ( ) for name in sorted ( self . internals_spec ) : internal = self . internals_spec [ name ] self . internals_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ 'type' ] ) , shape = ( None , ) + tuple ( internal [ 'shape' ] ) , name = ( 'internal-' + name ) ) if internal [ 'initialization' ] == 'zeros' : self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) else : raise TensorForceError ( "Invalid internal initialization value." ) # And only then call super. custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) # Distributions self . distributions = self . create_distributions ( ) # KL divergence function self . fn_kl_divergence = tf . make_template ( name_ = 'kl-divergence' , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter
11616	def _roman ( data , scheme_map , * * kw ) : vowels = scheme_map . vowels marks = scheme_map . marks virama = scheme_map . virama consonants = scheme_map . consonants non_marks_viraama = scheme_map . non_marks_viraama max_key_length_from_scheme = scheme_map . max_key_length_from_scheme to_roman = scheme_map . to_scheme . is_roman togglers = kw . pop ( 'togglers' , set ( ) ) suspend_on = kw . pop ( 'suspend_on' , set ( ) ) suspend_off = kw . pop ( 'suspend_off' , set ( ) ) if kw : raise TypeError ( 'Unexpected keyword argument %s' % list ( kw . keys ( ) ) [ 0 ] ) buf = [ ] i = 0 had_consonant = found = False len_data = len ( data ) append = buf . append # If true, don't transliterate. The toggle token is discarded. toggled = False # If true, don't transliterate. The suspend token is retained. # `suspended` overrides `toggled`. suspended = False while i <= len_data : # The longest token in the source scheme has length `max_key_length_from_scheme`. Iterate # over `data` while taking `max_key_length_from_scheme` characters at a time. If we don`t # find the character group in our scheme map, lop off a character and # try again. # # If we've finished reading through `data`, then `token` will be empty # and the loop below will be skipped. token = data [ i : i + max_key_length_from_scheme ] while token : if token in togglers : toggled = not toggled i += 2 # skip over the token found = True # force the token to fill up again break if token in suspend_on : suspended = True elif token in suspend_off : suspended = False if toggled or suspended : token = token [ : - 1 ] continue # Catch the pattern CV, where C is a consonant and V is a vowel. # V should be rendered as a vowel mark, a.k.a. a "dependent" # vowel. But due to the nature of Brahmic scripts, 'a' is implicit # and has no vowel mark. If we see 'a', add nothing. if had_consonant and token in vowels : mark = marks . get ( token , '' ) if mark : append ( mark ) elif to_roman : append ( vowels [ token ] ) found = True # Catch any non_marks_viraama character, including consonants, punctuation, # and regular vowels. Due to the implicit 'a', we must explicitly # end any lingering consonants before we can handle the current # token. elif token in non_marks_viraama : if had_consonant : append ( virama [ '' ] ) append ( non_marks_viraama [ token ] ) found = True if found : had_consonant = token in consonants i += len ( token ) break else : token = token [ : - 1 ] # We've exhausted the token; this must be some other character. Due to # the implicit 'a', we must explicitly end any lingering consonants # before we can handle the current token. if not found : if had_consonant : append ( virama [ '' ] ) if i < len_data : append ( data [ i ] ) had_consonant = False i += 1 found = False return '' . join ( buf )
12610	def _concat_queries ( queries , operators = '__and__' ) : # checks first if not queries : raise ValueError ( 'Expected some `queries`, got {}.' . format ( queries ) ) if len ( queries ) == 1 : return queries [ 0 ] if isinstance ( operators , str ) : operators = [ operators ] * ( len ( queries ) - 1 ) if len ( queries ) - 1 != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( queries ) , operators ) ) # recursively build the query first , rest , end = queries [ 0 ] , queries [ 1 : - 1 ] , queries [ - 1 : ] [ 0 ] bigop = getattr ( first , operators [ 0 ] ) for i , q in enumerate ( rest ) : bigop = getattr ( bigop ( q ) , operators [ i ] ) return bigop ( end )
12287	def shellcmd ( repo , args ) : with cd ( repo . rootdir ) : result = run ( args ) return result
13257	def save ( self , entry , with_location = True , debug = False ) : entry_dict = { } if isinstance ( entry , DayOneEntry ) : # Get a dict of the DayOneEntry entry_dict = entry . as_dict ( ) else : entry_dict = entry # Set the UUID entry_dict [ 'UUID' ] = uuid . uuid4 ( ) . get_hex ( ) if with_location and not entry_dict [ 'Location' ] : entry_dict [ 'Location' ] = self . get_location ( ) # Do we have everything needed? if not all ( ( entry_dict [ 'UUID' ] , entry_dict [ 'Time Zone' ] , entry_dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file_path = self . _file_path ( entry_dict [ 'UUID' ] ) plistlib . writePlist ( entry_dict , file_path ) else : plist = plistlib . writePlistToString ( entry_dict ) print plist return True
12394	def try_delegation ( method ) : @ functools . wraps ( method ) def delegator ( self , * args , * * kwargs ) : if self . try_delegation : # Try to dispatch to the instance's implementation. inst = getattr ( self , 'inst' , None ) if inst is not None : method_name = ( self . delegator_prefix or '' ) + method . __name__ func = getattr ( inst , method_name , None ) if func is not None : return func ( * args , * * kwargs ) # Otherwise run the decorated func. return method ( self , * args , * * kwargs ) return delegator
10465	def _getRunningApps ( cls ) : def runLoopAndExit ( ) : AppHelper . stopEventLoop ( ) AppHelper . callLater ( 1 , runLoopAndExit ) AppHelper . runConsoleEventLoop ( ) # Get a list of running applications ws = AppKit . NSWorkspace . sharedWorkspace ( ) apps = ws . runningApplications ( ) return apps
4907	def _sync_content_metadata ( self , serialized_data , http_method ) : try : status_code , response_body = getattr ( self , '_' + http_method ) ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . course_api_path ) , serialized_data , self . CONTENT_PROVIDER_SCOPE ) except requests . exceptions . RequestException as exc : raise ClientError ( 'DegreedAPIClient request failed: {error} {message}' . format ( error = exc . __class__ . __name__ , message = str ( exc ) ) ) if status_code >= 400 : raise ClientError ( 'DegreedAPIClient request failed with status {status_code}: {message}' . format ( status_code = status_code , message = response_body ) )
11538	def pin_direction ( self , pin ) : if type ( pin ) is list : return [ self . pin_direction ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_direction ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7720	def set_history ( self , parameters ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : child . unlinkNode ( ) child . freeNode ( ) break if parameters . maxchars and parameters . maxchars < 0 : raise ValueError ( "History parameter maxchars must be positive" ) if parameters . maxstanzas and parameters . maxstanzas < 0 : raise ValueError ( "History parameter maxstanzas must be positive" ) if parameters . maxseconds and parameters . maxseconds < 0 : raise ValueError ( "History parameter maxseconds must be positive" ) hnode = self . xmlnode . newChild ( self . xmlnode . ns ( ) , "history" , None ) if parameters . maxchars is not None : hnode . setProp ( "maxchars" , str ( parameters . maxchars ) ) if parameters . maxstanzas is not None : hnode . setProp ( "maxstanzas" , str ( parameters . maxstanzas ) ) if parameters . maxseconds is not None : hnode . setProp ( "maxseconds" , str ( parameters . maxseconds ) ) if parameters . since is not None : hnode . setProp ( "since" , parameters . since . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) )
1143	def _bytelist2longBigEndian ( list ) : imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) # TODO: ensure unicode data works correctly for python2 args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) # Do vertical padding arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] # Initialize output all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : # Concatenate the new string for lx , line in enumerate ( lines ) : all_lines [ lx ] += line # Find the new maximum horizontal width width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : # Horizontal padding on all but last iter for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) # Clean up trailing whitespace all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
12952	def _get_connection ( self ) : if self . _connection is None : self . _connection = self . _get_new_connection ( ) return self . _connection
7236	def read ( self , bands = None , * * kwargs ) : arr = self if bands is not None : arr = self [ bands , ... ] return arr . compute ( scheduler = threaded_get )
4145	def speriodogram ( x , NFFT = None , detrend = True , sampling = 1. , scale_by_freq = True , window = 'hamming' , axis = 0 ) : x = np . array ( x ) # array with 1 dimension case if x . ndim == 1 : axis = 0 r = x . shape [ 0 ] w = Window ( r , window ) #same size as input data w = w . data # matrix case elif x . ndim == 2 : logging . debug ( '2D array. each row is a 1D array' ) [ r , c ] = x . shape w = np . array ( [ Window ( r , window ) . data for this in range ( c ) ] ) . reshape ( r , c ) if NFFT is None : NFFT = len ( x ) isreal = np . isrealobj ( x ) if detrend == True : m = np . mean ( x , axis = axis ) else : m = 0 if isreal == True : if x . ndim == 2 : res = ( abs ( rfft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( rfft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r else : if x . ndim == 2 : res = ( abs ( fft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( fft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r if scale_by_freq is True : df = sampling / float ( NFFT ) res *= 2 * np . pi / df if x . ndim == 1 : return res . transpose ( ) else : return res
2163	def list ( self , group = None , host_filter = None , * * kwargs ) : if group : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'groups__in' , group ) , ) if host_filter : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'host_filter' , host_filter ) , ) return super ( Resource , self ) . list ( * * kwargs )
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : # noinspection PyTypeChecker galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
2970	def _sm_cleanup ( self , * args , * * kwargs ) : if self . _done_notification_func is not None : self . _done_notification_func ( ) self . _timer . cancel ( )
5369	def _retry_storage_check ( exception ) : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) return isinstance ( exception , oauth2client . client . AccessTokenRefreshError )
13832	def _SkipFieldValue ( tokenizer ) : # String/bytes tokens can come in multiple adjacent string literals. # If we can consume one, consume as many as we can. if tokenizer . TryConsumeByteString ( ) : while tokenizer . TryConsumeByteString ( ) : pass return if ( not tokenizer . TryConsumeIdentifier ( ) and not tokenizer . TryConsumeInt64 ( ) and not tokenizer . TryConsumeUint64 ( ) and not tokenizer . TryConsumeFloat ( ) ) : raise ParseError ( 'Invalid field value: ' + tokenizer . token )
5398	def get_dsub_version ( ) : filename = os . path . join ( os . path . dirname ( __file__ ) , 'dsub/_dsub_version.py' ) with open ( filename , 'r' ) as versionfile : for line in versionfile : if line . startswith ( 'DSUB_VERSION =' ) : # Get the version then strip whitespace and quote characters. version = line . partition ( '=' ) [ 2 ] return version . strip ( ) . strip ( '\'"' ) raise ValueError ( 'Could not find version.' )
3978	def _get_referenced_libs ( specs ) : active_libs = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for lib in app_spec [ 'depends' ] [ 'libs' ] : active_libs . add ( lib ) return active_libs
3325	def _generate_lock ( self , principal , lock_type , lock_scope , lock_depth , lock_owner , path , timeout ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT elif timeout < 0 : timeout = - 1 lock_dict = { "root" : path , "type" : lock_type , "scope" : lock_scope , "depth" : lock_depth , "owner" : lock_owner , "timeout" : timeout , "principal" : principal , } # self . storage . create ( path , lock_dict ) return lock_dict
2442	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True if validations . validate_annotation_comment ( comment ) : doc . annotations [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'AnnotationComment::Comment' ) else : raise CardinalityError ( 'AnnotationComment::Comment' ) else : raise OrderError ( 'AnnotationComment::Comment' )
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( * * jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
2960	def __base_state ( self , containers ) : return dict ( blockade_id = self . _blockade_id , containers = containers , version = self . _state_version )
63	def is_partly_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] eps = np . finfo ( np . float32 ) . eps img_bb = BoundingBox ( x1 = 0 , x2 = width - eps , y1 = 0 , y2 = height - eps ) return self . intersection ( img_bb ) is not None
10412	def summarize_node_filter ( graph : BELGraph , node_filters : NodePredicates ) -> None : passed = count_passed_node_filter ( graph , node_filters ) print ( '{}/{} nodes passed' . format ( passed , graph . number_of_nodes ( ) ) )
12499	def fwhm2sigma ( fwhm ) : fwhm = np . asarray ( fwhm ) return fwhm / np . sqrt ( 8 * np . log ( 2 ) )
12752	def indices_for_joint ( self , name ) : j = 0 for joint in self . joints : if joint . name == name : return list ( range ( j , j + joint . ADOF ) ) j += joint . ADOF return [ ]
7843	def get_type ( self ) : item_type = self . xmlnode . prop ( "type" ) if not item_type : item_type = "?" return item_type . decode ( "utf-8" )
433	def draw_boxes_and_labels_to_image ( image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None ) : if len ( coords ) != len ( classes ) : raise AssertionError ( "number of coordinates and classes are equal" ) if len ( scores ) > 0 and len ( scores ) != len ( classes ) : raise AssertionError ( "number of scores and classes are equal" ) # don't change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) for i , _v in enumerate ( coords ) : if is_center : x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) else : x , y , x2 , y2 = coords [ i ] if is_rescale : # scale back to pixel unit if the coords are the portion of width and high x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) cv2 . rectangle ( image , ( int ( x ) , int ( y ) ) , ( int ( x2 ) , int ( y2 ) ) , # up-left and botton-right [ 0 , 255 , 0 ] , thick ) cv2 . putText ( image , classes_list [ classes [ i ] ] + ( ( " %.2f" % ( scores [ i ] ) ) if ( len ( scores ) != 0 ) else " " ) , ( int ( x ) , int ( y ) ) , # button left 0 , 1.5e-3 * imh , # bigger = larger font [ 0 , 0 , 256 ] , # self.meta['colors'][max_indx], int ( thick / 2 ) + 1 ) # bold if save_name is not None : # cv2.imwrite('_my.png', image) save_image ( image , save_name ) # if len(coords) == 0: # tl.logging.info("draw_boxes_and_labels_to_image: no bboxes exist, cannot draw !") return image
3864	async def send_message ( self , segments , image_file = None , image_id = None , image_user_id = None ) : async with self . _send_message_lock : if image_file : try : uploaded_image = await self . _client . upload_image ( image_file , return_uploaded_image = True ) except exceptions . NetworkError as e : logger . warning ( 'Failed to upload image: {}' . format ( e ) ) raise image_id = uploaded_image . image_id try : request = hangouts_pb2 . SendChatMessageRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , message_content = hangouts_pb2 . MessageContent ( segment = [ seg . serialize ( ) for seg in segments ] , ) , ) if image_id is not None : request . existing_media . photo . photo_id = image_id if image_user_id is not None : request . existing_media . photo . user_id = image_user_id request . existing_media . photo . is_custom_user_id = True await self . _client . send_chat_message ( request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to send message: {}' . format ( e ) ) raise
10685	def G_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : g = 1 - ( self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 6 + tau ** 9 / 135 + tau ** 15 / 600 ) ) / self . _D_mag else : g = - ( tau ** - 5 / 10 + tau ** - 15 / 315 + tau ** - 25 / 1500 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * g
11888	def get_data ( self ) : response = self . send_command ( GET_LIGHTS_COMMAND ) _LOGGER . debug ( "get_data response: %s" , repr ( response ) ) if not response : _LOGGER . debug ( "Empty response: %s" , response ) return { } response = response . strip ( ) # Check string before splitting (avoid IndexError if malformed) if not ( response . startswith ( "GLB" ) and response . endswith ( ";" ) ) : _LOGGER . debug ( "Invalid response: %s" , repr ( response ) ) return { } # deconstruct response string into light data. Example data: # GLB 143E,1,1,25,255,255,255,0,0;287B,1,1,22,255,255,255,0,0;\r\n response = response [ 4 : - 3 ] # strip start (GLB) and end (;\r\n) light_strings = response . split ( ';' ) light_data_by_id = { } for light_string in light_strings : values = light_string . split ( ',' ) try : light_data_by_id [ values [ 0 ] ] = [ int ( values [ 2 ] ) , int ( values [ 4 ] ) , int ( values [ 5 ] ) , int ( values [ 6 ] ) , int ( values [ 7 ] ) ] except ValueError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) except IndexError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) return light_data_by_id
12907	def assert_json_type ( value : JsonValue , expected_type : JsonCheckType ) -> None : def type_name ( t : Union [ JsonCheckType , Type [ None ] ] ) -> str : if t is None : return "None" if isinstance ( t , JList ) : return "list" return t . __name__ if expected_type is None : if value is None : return elif expected_type == float : if isinstance ( value , float ) or isinstance ( value , int ) : return elif expected_type in [ str , int , bool , list , dict ] : if isinstance ( value , expected_type ) : # type: ignore return elif isinstance ( expected_type , JList ) : if isinstance ( value , list ) : for v in value : assert_json_type ( v , expected_type . value_type ) return else : raise TypeError ( "unsupported type" ) raise TypeError ( "wrong JSON type {} != {}" . format ( type_name ( expected_type ) , type_name ( type ( value ) ) ) )
9921	def validate_key ( self , key ) : if not models . PasswordResetToken . valid_tokens . filter ( key = key ) . exists ( ) : raise serializers . ValidationError ( _ ( "The provided reset token does not exist, or is expired." ) ) return key
474	def save_vocab ( count = None , name = 'vocab.txt' ) : if count is None : count = [ ] pwd = os . getcwd ( ) vocabulary_size = len ( count ) with open ( os . path . join ( pwd , name ) , "w" ) as f : for i in xrange ( vocabulary_size ) : f . write ( "%s %d\n" % ( tf . compat . as_text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) tl . logging . info ( "%d vocab saved to %s in %s" % ( vocabulary_size , name , pwd ) )
8886	def finalize ( self ) : if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. finalize unusable' ) elif not self . __head_generate : warn ( f'{self.__class__.__name__} already finalized or fitted' ) elif not self . __head_dict : raise NotFittedError ( f'{self.__class__.__name__} instance is not fitted yet' ) else : if self . remove_rare_ratio : self . __clean_head ( * self . __head_rare ) self . __prepare_header ( ) self . __head_rare = None self . __head_generate = False
5644	def can_infect ( self , event ) : if event . from_stop_I != self . stop_I : return False if not self . has_been_visited ( ) : return False else : time_sep = event . dep_time_ut - self . get_min_visit_time ( ) # if the gap between the earliest visit_time and current time is # smaller than the min. transfer time, the stop can pass the spreading # forward if ( time_sep >= self . min_transfer_time ) or ( event . trip_I == - 1 and time_sep >= 0 ) : return True else : for visit in self . visit_events : # if no transfer, please hop-on if ( event . trip_I == visit . trip_I ) and ( time_sep >= 0 ) : return True return False
7176	def retype_path ( src , pyi_dir , targets , * , src_explicitly_given = False , quiet = False , hg = False ) : if src . is_dir ( ) : for child in src . iterdir ( ) : if child == pyi_dir or child == targets : continue yield from retype_path ( child , pyi_dir / src . name , targets / src . name , quiet = quiet , hg = hg , ) elif src . suffix == '.py' or src_explicitly_given : try : retype_file ( src , pyi_dir , targets , quiet = quiet , hg = hg ) except Exception as e : yield ( src , str ( e ) , type ( e ) , traceback . format_tb ( e . __traceback__ ) , )
3373	def add_cons_vars_to_problem ( model , what , * * kwargs ) : context = get_context ( model ) model . solver . add ( what , * * kwargs ) if context : context ( partial ( model . solver . remove , what ) )
7042	def stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = False ) : ndet = len ( fmags ) if ndet > 9 : # get the median and ndet medmag = npmedian ( fmags ) # get the stetson index elements delta_prefactor = ( ndet / ( ndet - 1 ) ) sigma_i = delta_prefactor * ( fmags - medmag ) / ferrs # Nicole's clever trick to advance indices by 1 and do x_i*x_(i+1) sigma_j = nproll ( sigma_i , 1 ) if weightbytimediff : difft = npdiff ( ftimes ) deltat = npmedian ( difft ) weights_i = npexp ( - difft / deltat ) products = ( weights_i * sigma_i [ 1 : ] * sigma_j [ 1 : ] ) else : # ignore first elem since it's actually x_0*x_n products = ( sigma_i * sigma_j ) [ 1 : ] stetsonj = ( npsum ( npsign ( products ) * npsqrt ( npabs ( products ) ) ) ) / ndet return stetsonj else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate stetson J index' ) return npnan
451	def flatten_reshape ( variable , name = 'flatten' ) : dim = 1 for d in variable . get_shape ( ) [ 1 : ] . as_list ( ) : dim *= d return tf . reshape ( variable , shape = [ - 1 , dim ] , name = name )
8656	def get_threads ( session , query ) : # GET /api/messages/0.1/threads response = make_get_request ( session , 'threads' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ThreadsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2397	def histogram ( ratings , min_rating = None , max_rating = None ) : ratings = [ int ( r ) for r in ratings ] if min_rating is None : min_rating = min ( ratings ) if max_rating is None : max_rating = max ( ratings ) num_ratings = int ( max_rating - min_rating + 1 ) hist_ratings = [ 0 for x in range ( num_ratings ) ] for r in ratings : hist_ratings [ r - min_rating ] += 1 return hist_ratings
11472	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : # create a dictionary of user urls -> rendered responses replacements = { } user_urls = set ( re . findall ( URL_RE , text ) ) for user_url in user_urls : try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) except OEmbedException : if urlize_all_links : replacements [ user_url ] = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) replacements [ user_url ] = replacement . strip ( ) # go through the text recording URLs that can be replaced # taking note of their start & end indexes user_urls = re . finditer ( URL_RE , text ) matches = [ ] for match in user_urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) # replace the URLs in order, offsetting the indices each go for indx , ( start , end , user_url ) in enumerate ( matches ) : replacement = replacements [ user_url ] difference = len ( replacement ) - len ( user_url ) # insert the replacement between two slices of text surrounding the # original url text = text [ : start ] + replacement + text [ end : ] # iterate through the rest of the matches offsetting their indices # based on the difference between replacement/original for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark_safe ( text )
7542	def storealleles ( consens , hidx , alleles ) : ## find the first hetero site and choose the priority base ## example, if W: then priority base in A and not T. PRIORITY=(order: CATG) bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] ## find which allele has priority based on bigbase bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] ## uplow other bases relative to this one and the priority list ## e.g., if there are two hetero sites (WY) and the two alleles are ## AT and TC, then since bigbase of (W) is A second hetero site should ## be stored as y, since the ordering is swapped in this case; the priority ## base (C versus T) is C, but C goes with the minor base at h site 1. #consens = list(consens) for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) ## return consens return consens
1248	def is_action_available ( self , action ) : temp_state = np . rot90 ( self . _state , action ) return self . _is_action_available_left ( temp_state )
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
9974	def get_mro ( self , space ) : seqs = [ self . get_mro ( base ) for base in self . get_bases ( space ) ] + [ list ( self . get_bases ( space ) ) ] res = [ ] while True : non_empty = list ( filter ( None , seqs ) ) if not non_empty : # Nothing left to process, we're done. res . insert ( 0 , space ) return res for seq in non_empty : # Find merge candidates among seq heads. candidate = seq [ 0 ] not_head = [ s for s in non_empty if candidate in s [ 1 : ] ] if not_head : # Reject the candidate. candidate = None else : break if not candidate : raise TypeError ( "inconsistent hierarchy, no C3 MRO is possible" ) res . append ( candidate ) for seq in non_empty : # Remove candidate. if seq [ 0 ] == candidate : del seq [ 0 ]
3440	def escape_ID ( cobra_model ) : for x in chain ( [ cobra_model ] , cobra_model . metabolites , cobra_model . reactions , cobra_model . genes ) : x . id = _escape_str_id ( x . id ) cobra_model . repair ( ) gene_renamer = _GeneEscaper ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) )
2227	def _digest_hasher ( hasher , hashlen , base ) : # Get a 128 character hex string hex_text = hasher . hexdigest ( ) # Shorten length of string (by increasing base) base_text = _convert_hexstr_base ( hex_text , base ) # Truncate text = base_text [ : hashlen ] return text
12017	def _dump_field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type_name if len ( fd . type_name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default_value ) if len ( fd . default_value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( * * v ) f = ' ' . join ( f . split ( ) ) self . _print ( f ) if len ( fd . type_name ) > 0 : self . uses . append ( fd . type_name )
4573	def hsv2rgb_spectrum ( hsv ) : h , s , v = hsv return hsv2rgb_raw ( ( ( h * 192 ) >> 8 , s , v ) )
13882	def GetFileContents ( filename , binary = False , encoding = None , newline = None ) : source_file = OpenFile ( filename , binary = binary , encoding = encoding , newline = newline ) try : contents = source_file . read ( ) finally : source_file . close ( ) return contents
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
6936	def cp_objectinfo_worker ( task ) : cpf , cpkwargs = task try : newcpf = update_checkplot_objectinfo ( cpf , * * cpkwargs ) return newcpf except Exception as e : LOGEXCEPTION ( 'failed to update objectinfo for %s' % cpf ) return None
9773	def statuses ( ctx , page ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) page = page or 1 try : response = PolyaxonClient ( ) . job . get_statuses ( user , project_name , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True )
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
5237	def get_interval ( ticker , session ) -> Session : if '_' not in session : session = f'{session}_normal_0_0' interval = Intervals ( ticker = ticker ) ss_info = session . split ( '_' ) return getattr ( interval , f'market_{ss_info.pop(1)}' ) ( * ss_info )
7428	def _subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp
7175	def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , traceback ) : Config . incremental = incremental Config . replace_any = replace_any returncode = 0 for src_entry in src : for file , error , exc_type , tb in retype_path ( Path ( src_entry ) , pyi_dir = Path ( pyi_dir ) , targets = Path ( target_dir ) , src_explicitly_given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc_type.__name__}: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) # According to http://tldp.org/LDP/abs/html/index.html starting with 126 # we have special returncodes. sys . exit ( min ( returncode , 125 ) )
13761	def _handle_response ( self , response ) : if not str ( response . status_code ) . startswith ( '2' ) : raise get_api_error ( response ) return response
7550	def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
6287	def get ( self , name ) -> Track : name = name . lower ( ) track = self . track_map . get ( name ) if not track : track = Track ( name ) self . tacks . append ( track ) self . track_map [ name ] = track return track
4111	def rc2poly ( kr , r0 = None ) : # Initialize the recursion from . levinson import levup p = len ( kr ) #% p is the order of the prediction polynomial. a = numpy . array ( [ 1 , kr [ 0 ] ] ) #% a is a true polynomial. e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) # Continue the recursion for k=2,3,...,p, where p is the order of the # prediction polynomial. for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
1785	def CMPXCHG ( cpu , dest , src ) : size = dest . size reg_name = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] accumulator = cpu . read_register ( reg_name ) sval = src . read ( ) dval = dest . read ( ) cpu . write_register ( reg_name , dval ) dest . write ( Operators . ITEBV ( size , accumulator == dval , sval , dval ) ) # Affected Flags o..szapc cpu . _calculate_CMP_flags ( size , accumulator - dval , accumulator , dval )
13462	def video_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) return render ( request , 'video/video_list.html' , { 'event' : event , 'video_list' : event . eventvideo_set . all ( ) } )
6069	def intensity_at_radius ( self , radius ) : return self . intensity * np . exp ( - self . sersic_constant * ( ( ( radius / self . effective_radius ) ** ( 1. / self . sersic_index ) ) - 1 ) )
11370	def convert_date_from_iso_to_human ( value ) : try : year , month , day = value . split ( "-" ) except ValueError : # Not separated by "-". Space? try : year , month , day = value . split ( " " ) except ValueError : # What gives? OK, lets just return as is return value try : date_object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except TypeError : return value return date_object . strftime ( "%d %b %Y" )
7978	def _post_auth ( self ) : ClientStream . _post_auth ( self ) if not self . initiator : self . unset_iq_get_handler ( "query" , "jabber:iq:auth" ) self . unset_iq_set_handler ( "query" , "jabber:iq:auth" )
2869	def setup ( self , pin , mode , pull_up_down = PUD_OFF ) : self . rpi_gpio . setup ( pin , self . _dir_mapping [ mode ] , pull_up_down = self . _pud_mapping [ pull_up_down ] )
6680	def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
1430	def run ( command , parser , cl_args , unknown_args ) : Log . debug ( "Update Args: %s" , cl_args ) # Build jar list extra_lib_jars = jars . packing_jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl_args [ "dry_run" ] else '' ) # Build extra args dict_extra_args = { } try : dict_extra_args = build_extra_args_dict ( cl_args ) except Exception as err : return SimpleResult ( Status . InvocationError , err . message ) # Execute if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) else : # Convert extra argument to commandline format and then execute list_extra_args = convert_args_dict_to_list ( dict_extra_args ) return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars )
11060	def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test_mode : self . plugins . save_state ( )
4965	def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
9952	def get_object ( name : str ) : # TODO: Duplicate of system.get_object elms = name . split ( "." ) parent = get_models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent
8203	def set_size ( self , size ) : if self . size is None : self . size = size return size else : return self . size
13754	def read_from_file ( file_path , encoding = "utf-8" ) : with codecs . open ( file_path , "r" , encoding ) as f : return f . read ( )
3748	def calculate ( self , T , method ) : if method == GHARAGHEIZI : mu = Gharagheizi_gas_viscosity ( T , self . Tc , self . Pc , self . MW ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'g' ) elif method == DIPPR_PERRY_8E : mu = EQ102 ( T , * self . Perrys2_312_coeffs ) elif method == VDI_PPDS : mu = horner ( self . VDI_PPDS_coeffs , T ) elif method == YOON_THODOS : mu = Yoon_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == STIEL_THODOS : mu = Stiel_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == LUCAS_GAS : mu = lucas_gas ( T , self . Tc , self . Pc , self . Zc , self . MW , self . dipole , CASRN = self . CASRN ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
2088	def delete ( self , pk = None , fail_on_missing = False , * * kwargs ) : # If we weren't given a primary key, determine which record we're deleting. if not pk : existing_data = self . _lookup ( fail_on_missing = fail_on_missing , * * kwargs ) if not existing_data : return { 'changed' : False } pk = existing_data [ 'id' ] # Attempt to delete the record. If it turns out the record doesn't exist, handle the 404 appropriately # (this is an okay response if `fail_on_missing` is False). url = '%s%s/' % ( self . endpoint , pk ) debug . log ( 'DELETE %s' % url , fg = 'blue' , bold = True ) try : client . delete ( url ) return { 'changed' : True } except exc . NotFound : if fail_on_missing : raise return { 'changed' : False }
12143	async def _push ( self , * args , * * kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
6764	def drop_database ( self , name ) : with settings ( warn_only = True ) : self . sudo ( 'dropdb %s' % ( name , ) , user = 'postgres' )
10814	def add_member ( self , user , state = MembershipState . ACTIVE ) : return Membership . create ( self , user , state )
7288	def get_field_value ( self , field_key ) : def get_value ( document , field_key ) : # Short circuit the function if we do not have a document if document is None : return None current_key , new_key_array = trim_field_key ( document , field_key ) key_array_digit = int ( new_key_array [ - 1 ] ) if new_key_array and has_digit ( new_key_array ) else None new_key = make_key ( new_key_array ) if key_array_digit is not None and len ( new_key_array ) > 0 : # Handleing list fields if len ( new_key_array ) == 1 : return_data = document . _data . get ( current_key , [ ] ) elif isinstance ( document , BaseList ) : return_list = [ ] if len ( document ) > 0 : return_list = [ get_value ( doc , new_key ) for doc in document ] return_data = return_list else : return_data = get_value ( getattr ( document , current_key ) , new_key ) elif len ( new_key_array ) > 0 : return_data = get_value ( document . _data . get ( current_key ) , new_key ) else : # Handeling all other fields and id try : # Added try except otherwise we get "TypeError: getattr(): attribute name must be string" error from mongoengine/base/datastructures.py return_data = ( document . _data . get ( None , None ) if current_key == "id" else document . _data . get ( current_key , None ) ) except : return_data = document . _data . get ( current_key , None ) return return_data if self . is_initialized : return get_value ( self . model_instance , field_key ) else : return None
3132	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . list_id = response [ 'id' ] else : self . list_id = None return response
10189	def consume ( self , event_type , no_ack = True , payload = True ) : assert event_type in self . events return current_queues . queues [ 'stats-{}' . format ( event_type ) ] . consume ( payload = payload )
922	def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False # None of the field filters triggered, accept the record as a good one return True
5156	def _get_install_context ( self ) : config = self . config # layer2 VPN list l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) # bridge list bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) # crontabs present? cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break # return context return dict ( hostname = config [ 'general' ] [ 'hostname' ] , # hostname is required l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , # radios might be empty cron = cron )
7234	def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , image = None , image_bounds = None , index = "vector-user-provided" , name = "GBDX_Task_Output" , * * kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX_API_KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorTileLayer ( url , source_name = name , styles = styles , * * kwargs ) image_layer = self . _build_image_layer ( image , image_bounds ) template = BaseTemplate ( map_id , * * { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : self . gbdx_connection . access_token } ) template . inject ( )
12168	def _dispatch_function ( self , event , listener , * args , * * kwargs ) : try : return listener ( * args , * * kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc )
11741	def _compute_first ( self ) : for terminal in self . terminals : self . _first [ terminal ] . add ( terminal ) self . _first [ END_OF_INPUT ] . add ( END_OF_INPUT ) while True : changed = False for nonterminal , productions in self . nonterminals . items ( ) : for production in productions : new_first = self . first ( production . rhs ) if new_first - self . _first [ nonterminal ] : self . _first [ nonterminal ] |= new_first changed = True if not changed : break
10806	def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
777	def connect ( self , deleteOldVersions = False , recreate = False ) : # Initialize tables, if needed with ConnectionFactory . get ( ) as conn : # Initialize tables self . _initTables ( cursor = conn . cursor , deleteOldVersions = deleteOldVersions , recreate = recreate ) # Save our connection id conn . cursor . execute ( 'SELECT CONNECTION_ID()' ) self . _connectionID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] self . _logger . info ( "clientJobsConnectionID=%r" , self . _connectionID ) return
12211	def invalidate_cache ( user , size = None ) : sizes = set ( AUTO_GENERATE_AVATAR_SIZES ) if size is not None : sizes . add ( size ) for prefix in cached_funcs : for size in sizes : cache . delete ( get_cache_key ( user , size , prefix ) )
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
694	def loadExperimentDescriptionScriptFromDir ( experimentDir ) : descriptionScriptPath = os . path . join ( experimentDir , "description.py" ) module = _loadDescriptionFile ( descriptionScriptPath ) return module
4600	def detach ( self , overlay ) : # See #868 for i , a in enumerate ( self . animations ) : a . layout = a . layout . clone ( ) if overlay and i : a . preclear = False
3442	def to_json ( model , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , * * kwargs )
12992	def line_chunker ( text , getreffs , lines = 30 ) : level = len ( text . citation ) source_reffs = [ reff . split ( ":" ) [ - 1 ] for reff in getreffs ( level = level ) ] reffs = [ ] i = 0 while i + lines - 1 < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ i + lines - 1 ] , source_reffs [ i ] ] ) ) i += lines if i < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ len ( source_reffs ) - 1 ] , source_reffs [ i ] ] ) ) return reffs
276	def customize ( func ) : @ wraps ( func ) def call_w_context ( * args , * * kwargs ) : set_context = kwargs . pop ( 'set_context' , True ) if set_context : with plotting_context ( ) , axes_style ( ) : return func ( * args , * * kwargs ) else : return func ( * args , * * kwargs ) return call_w_context
1628	def GetIndentLevel ( line ) : indent = Match ( r'^( *)\S' , line ) if indent : return len ( indent . group ( 1 ) ) else : return 0
5142	def make_index ( self ) : for prev , block in zip ( self . blocks [ : - 1 ] , self . blocks [ 1 : ] ) : if not block . is_comment : self . index [ block . start_lineno ] = prev
13007	def _from_parts ( cls , args , init = True ) : if args : args = list ( args ) if isinstance ( args [ 0 ] , WindowsPath2 ) : args [ 0 ] = args [ 0 ] . path elif args [ 0 ] . startswith ( "\\\\?\\" ) : args [ 0 ] = args [ 0 ] [ 4 : ] args = tuple ( args ) return super ( WindowsPath2 , cls ) . _from_parts ( args , init )
6391	def encode ( self , word , max_length = 8 ) : # Lowercase input & filter unknown characters word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '' # Perform initial eudex coding of each character values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] # Right-shift by one to determine if second instance should be skipped shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) # Add padding after first character & trim beyond max_length values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) # Combine individual character values into eudex hash hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
8300	def handle ( self , data , source = None ) : decoded = decodeOSC ( data ) self . dispatch ( decoded , source )
4726	def get_chunk_meta_item ( self , chunk_meta , grp , pug , chk ) : num_chk = self . envs [ "NUM_CHK" ] num_pu = self . envs [ "NUM_PU" ] index = grp * num_pu * num_chk + pug * num_chk + chk return chunk_meta [ index ]
9848	def _load_dx ( self , filename ) : dx = OpenDX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
12887	def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
12256	def lbfgs ( x , rho , f_df , maxiter = 20 ) : def f_df_augmented ( theta ) : f , df = f_df ( theta ) obj = f + ( rho / 2. ) * np . linalg . norm ( theta - x ) ** 2 grad = df + rho * ( theta - x ) return obj , grad res = scipy_minimize ( f_df_augmented , x , jac = True , method = 'L-BFGS-B' , options = { 'maxiter' : maxiter , 'disp' : False } ) return res . x
6483	def _process_pagination_values ( request ) : size = 20 page = 0 from_ = 0 if "page_size" in request . POST : size = int ( request . POST [ "page_size" ] ) max_page_size = getattr ( settings , "SEARCH_MAX_PAGE_SIZE" , 100 ) # The parens below are superfluous, but make it much clearer to the reader what is going on if not ( 0 < size <= max_page_size ) : # pylint: disable=superfluous-parens raise ValueError ( _ ( 'Invalid page size of {page_size}' ) . format ( page_size = size ) ) if "page_index" in request . POST : page = int ( request . POST [ "page_index" ] ) from_ = page * size return size , from_ , page
7208	def task_ids ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get task IDs.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for task IDs." ) wf = self . workflow . get ( self . id ) return [ task [ 'id' ] for task in wf [ 'tasks' ] ]
5598	def execute ( mp , resampling = "nearest" , scale_method = None , scales_minmax = None ) : with mp . open ( "raster" , resampling = resampling ) as raster_file : # exit if input tile is empty if raster_file . is_empty ( ) : return "empty" # actually read data and iterate through bands scaled = ( ) mask = ( ) raster_data = raster_file . read ( ) if raster_data . ndim == 2 : raster_data = ma . expand_dims ( raster_data , axis = 0 ) if not scale_method : scales_minmax = [ ( i , i ) for i in range ( len ( raster_data ) ) ] for band , ( scale_min , scale_max ) in zip ( raster_data , scales_minmax ) : if scale_method in [ "dtype_scale" , "minmax_scale" ] : scaled += ( _stretch_array ( band , scale_min , scale_max ) , ) elif scale_method == "crop" : scaled += ( np . clip ( band , scale_min , scale_max ) , ) else : scaled += ( band , ) mask += ( band . mask , ) return ma . masked_array ( np . stack ( scaled ) , np . stack ( mask ) )
2591	def stage_out ( self , file , executor ) : if file . scheme == 'http' or file . scheme == 'https' : raise Exception ( 'HTTP/HTTPS file staging out is not supported' ) elif file . scheme == 'ftp' : raise Exception ( 'FTP file staging out is not supported' ) elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_out_app = self . _globus_stage_out_app ( ) return stage_out_app ( globus_ep , inputs = [ file ] ) else : raise Exception ( 'Staging out with unknown file scheme {} is not supported' . format ( file . scheme ) )
2154	def _read ( self , fp , fpname ) : # Attempt to read the file using the superclass implementation. # # Check the permissions of the file we are considering reading # if the file exists and the permissions expose it to reads from # other users, raise a warning if os . path . isfile ( fpname ) : file_permission = os . stat ( fpname ) if fpname != os . path . join ( tower_dir , 'tower_cli.cfg' ) and ( ( file_permission . st_mode & stat . S_IRGRP ) or ( file_permission . st_mode & stat . S_IROTH ) ) : warnings . warn ( 'File {0} readable by group or others.' . format ( fpname ) , RuntimeWarning ) # If it doesn't work because there's no section header, then # create a section header and call the superclass implementation # again. try : return configparser . ConfigParser . _read ( self , fp , fpname ) except configparser . MissingSectionHeaderError : fp . seek ( 0 ) string = '[general]\n%s' % fp . read ( ) flo = StringIO ( string ) # flo == file-like object return configparser . ConfigParser . _read ( self , flo , fpname )
9086	def _sort ( self , concepts , sort = None , language = 'any' , reverse = False ) : sorted = copy . copy ( concepts ) if sort : sorted . sort ( key = methodcaller ( '_sortkey' , sort , language ) , reverse = reverse ) return sorted
12723	def max_forces ( self , max_forces ) : _set_params ( self . ode_obj , 'FMax' , max_forces , self . ADOF + self . LDOF )
9364	def domain_name ( ) : result = random . choice ( get_dictionary ( 'company_names' ) ) . strip ( ) result += '.' + top_level_domain ( ) return result . lower ( )
6931	def xmatch_cpdir_external_catalogs ( cpdir , xmatchpkl , cpfileglob = 'checkplot-*.pkl*' , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = xmatchradiusarcsec , updateexisting = updateexisting , resultstodir = resultstodir )
10787	def add_subtract_misfeatured_tile ( st , tile , rad = 'calc' , max_iter = 3 , invert = 'guess' , max_allowed_remove = 20 , minmass = None , use_tp = False , * * kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) if invert == 'guess' : invert = guess_invert ( st ) # 1. Remove all possibly bad particles within the tile. initial_error = np . copy ( st . error ) rinds = np . nonzero ( tile . contains ( st . obj_get_positions ( ) ) ) [ 0 ] if rinds . size >= max_allowed_remove : CLOG . fatal ( 'Misfeatured region too large!' ) raise RuntimeError elif rinds . size >= max_allowed_remove / 2 : CLOG . warn ( 'Large misfeatured regions.' ) elif rinds . size > 0 : rpos , rrad = st . obj_remove_particle ( rinds ) # 2-4. Feature & add particles to the tile, optimize, run until none added n_added = - rinds . size added_poses = [ ] for _ in range ( max_iter ) : if invert : im = 1 - st . residuals [ tile . slicer ] else : im = st . residuals [ tile . slicer ] guess , _ = _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp ) accepts , poses = check_add_particles ( st , guess + tile . l , rad = rad , do_opt = True , * * kwargs ) added_poses . extend ( poses ) n_added += accepts if accepts == 0 : break else : # for-break-else CLOG . warn ( 'Runaway adds or insufficient max_iter' ) # 5. Optimize added pos + rad: ainds = [ ] for p in added_poses : ainds . append ( st . obj_closest_particle ( p ) ) if len ( ainds ) > max_allowed_remove : for i in range ( 0 , len ( ainds ) , max_allowed_remove ) : opt . do_levmarq_particles ( st , np . array ( ainds [ i : i + max_allowed_remove ] ) , include_rad = True , max_iter = 3 ) elif len ( ainds ) > 0 : opt . do_levmarq_particles ( st , ainds , include_rad = True , max_iter = 3 ) # 6. Ensure that current error after add-subtracting is lower than initial did_something = ( rinds . size > 0 ) or ( len ( ainds ) > 0 ) if did_something & ( st . error > initial_error ) : CLOG . info ( 'Failed addsub, Tile {} -> {}' . format ( tile . l . tolist ( ) , tile . r . tolist ( ) ) ) if len ( ainds ) > 0 : _ = st . obj_remove_particle ( ainds ) if rinds . size > 0 : for p , r in zip ( rpos . reshape ( - 1 , 3 ) , rrad . reshape ( - 1 ) ) : _ = st . obj_add_particle ( p , r ) n_added = 0 ainds = [ ] return n_added , ainds
6185	def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
742	def readFromFile ( cls , f , packed = True ) : # Get capnproto schema from instance schema = cls . getSchema ( ) # Read from file if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) # Return first-class instance initialized from proto obj return cls . read ( proto )
11362	def convert_html_subscripts_to_latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$_{\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
8758	def get_subnet ( context , id , fields = None ) : LOG . info ( "get_subnet %s for tenant %s with fields %s" % ( id , context . tenant_id , fields ) ) subnet = db_api . subnet_find ( context = context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker_obj = None , fields = None , id = id , join_dns = True , join_routes = True , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) cache = subnet . get ( "_allocation_pool_cache" ) if not cache : new_cache = subnet . allocation_pools db_api . subnet_update_set_alloc_pool_cache ( context , subnet , new_cache ) return v . _make_subnet_dict ( subnet )
9072	def build_engine_session ( connection , echo = False , autoflush = None , autocommit = None , expire_on_commit = None , scopefunc = None ) : if connection is None : raise ValueError ( 'can not build engine when connection is None' ) engine = create_engine ( connection , echo = echo ) autoflush = autoflush if autoflush is not None else False autocommit = autocommit if autocommit is not None else False expire_on_commit = expire_on_commit if expire_on_commit is not None else True log . debug ( 'auto flush: %s, auto commit: %s, expire on commmit: %s' , autoflush , autocommit , expire_on_commit ) #: A SQLAlchemy session maker session_maker = sessionmaker ( bind = engine , autoflush = autoflush , autocommit = autocommit , expire_on_commit = expire_on_commit , ) #: A SQLAlchemy session object session = scoped_session ( session_maker , scopefunc = scopefunc ) return engine , session
12159	def abfGroupFiles ( groups , folder ) : assert os . path . exists ( folder ) files = os . listdir ( folder ) group2 = { } for parent in groups . keys ( ) : if not parent in group2 . keys ( ) : group2 [ parent ] = [ ] for ID in groups [ parent ] : for fname in [ x . lower ( ) for x in files if ID in x . lower ( ) ] : group2 [ parent ] . extend ( [ fname ] ) return group2
11367	def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
5673	def from_directory_as_inmemory_db ( cls , gtfs_directory ) : # this import is here to avoid circular imports (which turned out to be a problem) from gtfspy . import_gtfs import import_gtfs conn = sqlite3 . connect ( ":memory:" ) import_gtfs ( gtfs_directory , conn , preserve_connection = True , print_progress = False ) return cls ( conn )
5277	def query ( self , i , j ) : if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
7048	def bls_stats_singleperiod ( times , mags , errs , period , magsarefluxes = False , sigclip = 10.0 , perioddeltapercent = 10 , nphasebins = 200 , mintransitduration = 0.01 , maxtransitduration = 0.4 , ingressdurationfraction = 0.1 , verbose = True ) : # get rid of nans first and sigclip stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) # make sure there are enough points to calculate a spectrum if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : # get the period interval startp = period - perioddeltapercent * period / 100.0 if startp < 0 : startp = period endp = period + perioddeltapercent * period / 100.0 # rerun BLS in serial mode around the specified period to get the # transit depth, duration, ingress and egress bins blsres = bls_serial_pfind ( stimes , smags , serrs , verbose = verbose , startp = startp , endp = endp , nphasebins = nphasebins , mintransitduration = mintransitduration , maxtransitduration = maxtransitduration , magsarefluxes = magsarefluxes , get_stats = False , sigclip = None ) if ( not blsres or 'blsresult' not in blsres or blsres [ 'blsresult' ] is None ) : LOGERROR ( "BLS failed during a period-search " "performed around the input best period: %.6f. " "Can't continue. " % period ) return None thistransdepth = blsres [ 'blsresult' ] [ 'transdepth' ] thistransduration = blsres [ 'blsresult' ] [ 'transduration' ] thisbestperiod = blsres [ 'bestperiod' ] thistransingressbin = blsres [ 'blsresult' ] [ 'transingressbin' ] thistransegressbin = blsres [ 'blsresult' ] [ 'transegressbin' ] thisnphasebins = nphasebins stats = _get_bls_stats ( stimes , smags , serrs , thistransdepth , thistransduration , ingressdurationfraction , nphasebins , thistransingressbin , thistransegressbin , thisbestperiod , thisnphasebins , magsarefluxes = magsarefluxes , verbose = verbose ) return stats # if there aren't enough points in the mag series, bail out else : LOGERROR ( 'no good detections for these times and mags, skipping...' ) return None
11133	def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
3433	def remove_groups ( self , group_list ) : if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] for group in group_list : # make sure the group is in the model if group . id not in self . groups : LOGGER . warning ( "%r not in %r. Ignored." , group , self ) else : self . groups . remove ( group ) group . _model = None
4802	def is_child_of ( self , parent ) : self . is_file ( ) if not isinstance ( parent , str_types ) : raise TypeError ( 'given parent directory arg must be a path' ) val_abspath = os . path . abspath ( self . val ) parent_abspath = os . path . abspath ( parent ) if not val_abspath . startswith ( parent_abspath ) : self . _err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val_abspath , parent_abspath ) ) return self
13561	def launch ( title , items , selected = None ) : resp = { "code" : - 1 , "done" : False } curses . wrapper ( Menu , title , items , selected , resp ) return resp
11926	def parse_filename ( self , filepath ) : name = os . path . basename ( filepath ) [ : - src_ext_len ] try : dt = datetime . strptime ( name , "%Y-%m-%d-%H-%M" ) except ValueError : raise PostNameInvalid return { 'name' : name , 'datetime' : dt , 'filepath' : filepath }
3745	def calculate ( self , T , method ) : if method == DUTT_PRASAD : A , B , C = self . DUTT_PRASAD_coeffs mu = ViswanathNatarajan3 ( T , A , B , C , ) elif method == VISWANATH_NATARAJAN_3 : A , B , C = self . VISWANATH_NATARAJAN_3_coeffs mu = ViswanathNatarajan3 ( T , A , B , C ) elif method == VISWANATH_NATARAJAN_2 : A , B = self . VISWANATH_NATARAJAN_2_coeffs mu = ViswanathNatarajan2 ( T , self . VISWANATH_NATARAJAN_2_coeffs [ 0 ] , self . VISWANATH_NATARAJAN_2_coeffs [ 1 ] ) elif method == VISWANATH_NATARAJAN_2E : C , D = self . VISWANATH_NATARAJAN_2E_coeffs mu = ViswanathNatarajan2Exponential ( T , C , D ) elif method == DIPPR_PERRY_8E : mu = EQ101 ( T , * self . Perrys2_313_coeffs ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'l' ) elif method == LETSOU_STIEL : mu = Letsou_Stiel ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == PRZEDZIECKI_SRIDHAR : Vml = self . Vml ( T ) if hasattr ( self . Vml , '__call__' ) else self . Vml mu = Przedziecki_Sridhar ( T , self . Tm , self . Tc , self . Pc , self . Vc , Vml , self . omega , self . MW ) elif method == VDI_PPDS : A , B , C , D , E = self . VDI_PPDS_coeffs term = ( C - T ) / ( T - D ) if term < 0 : term1 = - ( ( T - C ) / ( T - D ) ) ** ( 1 / 3. ) else : term1 = term ** ( 1 / 3. ) term2 = term * term1 mu = E * exp ( A * term1 + B * term2 ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
4687	def decrypt ( self , message ) : if not message : return None # We first try to decode assuming we received the memo try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "to" ] ) pubkey = message [ "from" ] except KeyNotFound : try : # if that failed, we assume that we have sent the memo memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "from" ] ) pubkey = message [ "to" ] except KeyNotFound : # if all fails, raise exception raise MissingKeyError ( "None of the required memo keys are installed!" "Need any of {}" . format ( [ message [ "to" ] , message [ "from" ] ] ) ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix return memo . decode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( pubkey , prefix = self . chain_prefix ) , message . get ( "nonce" ) , message . get ( "message" ) , )
7400	def up ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__lt = self . order ) . order_by ( '-order' ) )
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
2326	def orient_graph ( self , df_data , graph , nb_runs = 6 , printout = None , * * kwargs ) : if type ( graph ) == nx . DiGraph : edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) ] oriented_edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) not in list ( graph . edges ( ) ) ] for a in edges : if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) : edges . remove ( a ) output = nx . DiGraph ( ) for i in oriented_edges : output . add_edge ( * i ) elif type ( graph ) == nx . Graph : edges = list ( graph . edges ( ) ) output = nx . DiGraph ( ) else : raise TypeError ( "Data type not understood." ) res = [ ] for idx , ( a , b ) in enumerate ( edges ) : weight = self . predict_proba ( df_data [ a ] . values . reshape ( ( - 1 , 1 ) ) , df_data [ b ] . values . reshape ( ( - 1 , 1 ) ) , idx = idx , nb_runs = nb_runs , * * kwargs ) if weight > 0 : # a causes b output . add_edge ( a , b , weight = weight ) else : output . add_edge ( b , a , weight = abs ( weight ) ) if printout is not None : res . append ( [ str ( a ) + '-' + str ( b ) , weight ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) for node in list ( df_data . columns . values ) : if node not in output . nodes ( ) : output . add_node ( node ) return output
5152	def merge_list ( list1 , list2 , identifiers = None ) : identifiers = identifiers or [ ] dict_map = { 'list1' : OrderedDict ( ) , 'list2' : OrderedDict ( ) } counter = 1 for list_ in [ list1 , list2 ] : container = dict_map [ 'list{0}' . format ( counter ) ] for el in list_ : # merge by internal python id by default key = id ( el ) # if el is a dict, merge by keys specified in ``identifiers`` if isinstance ( el , dict ) : for id_key in identifiers : if id_key in el : key = el [ id_key ] break container [ key ] = deepcopy ( el ) counter += 1 merged = merge_config ( dict_map [ 'list1' ] , dict_map [ 'list2' ] ) return list ( merged . values ( ) )
10177	def run ( self , start_date = None , end_date = None , update_bookmark = True ) : # If no events have been indexed there is nothing to aggregate if not Index ( self . event_index , using = self . client ) . exists ( ) : return lower_limit = start_date or self . get_bookmark ( ) # Stop here if no bookmark could be estimated. if lower_limit is None : return upper_limit = min ( end_date or datetime . datetime . max , # ignore if `None` datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , datetime . datetime . combine ( lower_limit + datetime . timedelta ( self . batch_size ) , datetime . datetime . min . time ( ) ) ) while upper_limit <= datetime . datetime . utcnow ( ) : self . indices = set ( ) self . new_bookmark = upper_limit . strftime ( self . doc_id_suffix ) bulk ( self . client , self . agg_iter ( lower_limit , upper_limit ) , stats_only = True , chunk_size = 50 ) # Flush all indices which have been modified current_search_client . indices . flush ( index = ',' . join ( self . indices ) , wait_if_ongoing = True ) if update_bookmark : self . set_bookmark ( ) self . indices = set ( ) lower_limit = lower_limit + datetime . timedelta ( self . batch_size ) upper_limit = min ( end_date or datetime . datetime . max , # ignore if `None`` datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , lower_limit + datetime . timedelta ( self . batch_size ) ) if lower_limit > upper_limit : break
6362	def encode ( self , word , max_length = - 1 ) : # uppercase, normalize, and decompose, filter to A-Z minus vowels & W word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _uc_set ) # merge repeated Ls & Rs word = word . replace ( 'LL' , 'L' ) word = word . replace ( 'R' , 'R' ) # apply the Soundex algorithm sdx = word . translate ( self . _trans ) if max_length > 0 : sdx = ( sdx + ( '0' * max_length ) ) [ : max_length ] return sdx
5081	def lrs ( self ) : return RemoteLRS ( version = self . lrs_configuration . version , endpoint = self . lrs_configuration . endpoint , auth = self . lrs_configuration . authorization_header , )
2211	def inject_method ( self , func , name = None ) : # TODO: if func is a bound method we should probably unbind it new_method = func . __get__ ( self , self . __class__ ) if name is None : name = func . __name__ setattr ( self , name , new_method )
11020	def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article_filename = find_last_article ( config [ 'CONTENT_DIR' ] ) if not article_filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article_filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find_images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\nAdd these images to the latest article' ) : abort ( config ) url_prefix = os . path . join ( '{filename}' , IMAGES_PATH ) images_dir = os . path . join ( config [ 'CONTENT_DIR' ] , IMAGES_PATH ) os . makedirs ( images_dir , exist_ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image_basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url_prefix , image_basename ) ) image_filename = os . path . join ( images_dir , image_basename ) print ( filename , image_filename ) import_image ( filename , image_filename ) content = '\n' for url in urls : url = url . replace ( '\\' , '/' ) content += '\n![image description]({})\n' . format ( url ) header ( 'Adding to article: {}' . format ( article_filename ) ) with click . open_file ( article_filename , 'a' ) as f : f . write ( content ) click . launch ( article_filename )
2741	def remove_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
4725	def get_chunk_meta ( self , meta_file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get_meta ( 0 , chunks * self . envs [ "CHUNK_META_SIZEOF" ] , meta_file ) : raise RuntimeError ( "cij.liblight.get_chunk_meta: fail" ) chunk_meta = cij . bin . Buffer ( types = self . envs [ "CHUNK_META_STRUCT" ] , length = chunks ) chunk_meta . read ( meta_file ) return chunk_meta
6608	def wait ( self ) : sleep = 5 while True : if self . clusterprocids_outstanding : self . poll ( ) if not self . clusterprocids_outstanding : break time . sleep ( sleep ) return self . clusterprocids_finished
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
1281	def block_code ( self , code , lang = None ) : code = code . rstrip ( '\n' ) if not lang : code = escape ( code , smart_amp = False ) return '<pre><code>%s\n</code></pre>\n' % code code = escape ( code , quote = True , smart_amp = False ) return '<pre><code class="lang-%s">%s\n</code></pre>\n' % ( lang , code )
195	def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : # TODO allow (1, None) and set to identity matrix if k == 1 k_param = iap . handle_discrete_param ( k , "k" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , "angle" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : # avoid cyclic import between blur and geometric from . import geometric as iaa_geometric # force discrete for k_sample via int() in case of stochastic parameter k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
2012	def instruction ( self ) : # FIXME check if pc points to invalid instruction # if self.pc >= len(self.bytecode): # return InvalidOpcode('Code out of range') # if self.pc in self.invalid: # raise InvalidOpcode('Opcode inside a PUSH immediate') try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
2433	def set_created_date ( self , doc , created ) : if not self . created_date_set : self . created_date_set = True date = utils . datetime_from_iso_format ( created ) if date is not None : doc . creation_info . created = date return True else : raise SPDXValueError ( 'CreationInfo::Date' ) else : raise CardinalityError ( 'CreationInfo::Created' )
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
1970	def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
3833	async def modify_otr_status ( self , modify_otr_status_request ) : response = hangouts_pb2 . ModifyOTRStatusResponse ( ) await self . _pb_request ( 'conversations/modifyotrstatus' , modify_otr_status_request , response ) return response
4164	def embed_code_links ( app , exception ) : if exception is not None : return # No need to waste time embedding hyperlinks when not running the examples # XXX: also at the time of writing this fixes make html-noplot # for some reason I don't fully understand if not app . builder . config . plot_gallery : return # XXX: Whitelist of builders for which it makes sense to embed # hyperlinks inside the example html. Note that the link embedding # require searchindex.js to exist for the links to the local doc # and there does not seem to be a good way of knowing which # builders creates a searchindex.js. if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery_conf = app . config . sphinx_gallery_conf gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] for gallery_dir in gallery_dirs : _embed_code_links ( app , gallery_conf , gallery_dir )
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
10715	def sendCommands ( comPort , commands ) : mutex . acquire ( ) try : try : port = serial . Serial ( port = comPort ) header = '11010101 10101010' footer = '10101101' for command in _translateCommands ( commands ) : _sendBinaryData ( port , header + command + footer ) except serial . SerialException : print ( 'Unable to open serial port %s' % comPort ) print ( '' ) raise finally : mutex . release ( )
8201	def settings ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
12585	def spatialimg_to_hdfpath ( file_path , spatial_img , h5path = None , append = True ) : if h5path is None : h5path = '/img' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' with h5py . File ( file_path , mode ) as f : try : h5img = f . create_group ( h5path ) spatialimg_to_hdfgroup ( h5img , spatial_img ) except ValueError as ve : raise Exception ( 'Error creating group ' + h5path ) from ve
11373	def return_letters_from_string ( text ) : out = "" for letter in text : if letter . isalpha ( ) : out += letter return out
10458	def clearContents ( cls ) : log_msg = 'Request to clear contents of pasteboard: general' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) pb . clearContents ( ) return True
10247	def enrich_pubmed_citations ( graph : BELGraph , manager : Manager ) -> Set [ str ] : pmids = get_pubmed_identifiers ( graph ) pmid_data , errors = get_citations_by_pmids ( manager = manager , pmids = pmids ) for u , v , k in filter_edges ( graph , has_pubmed ) : pmid = graph [ u ] [ v ] [ k ] [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) if pmid not in pmid_data : log . warning ( 'Missing data for PubMed identifier: %s' , pmid ) errors . add ( pmid ) continue graph [ u ] [ v ] [ k ] [ CITATION ] . update ( pmid_data [ pmid ] ) return errors
4367	def call_method_with_acl ( self , method_name , packet , * args ) : if not self . is_method_allowed ( method_name ) : self . error ( 'method_access_denied' , 'You do not have access to method "%s"' % method_name ) return return self . call_method ( method_name , packet , * args )
12441	def require_authentication ( self , request ) : request . user = user = None if request . method == 'OPTIONS' : # Authentication should not be checked on an OPTIONS request. return for auth in self . meta . authentication : user = auth . authenticate ( request ) if user is False : # Authentication protocol failed to authenticate; # pass the baton. continue if user is None and not auth . allow_anonymous : # Authentication protocol determined the user is # unauthenticated. auth . unauthenticated ( ) # Authentication protocol determined the user is indeed # authenticated (or not); Store the user for later reference. request . user = user return if not user and not auth . allow_anonymous : # No authenticated user found and protocol doesn't allow # anonymous users. auth . unauthenticated ( )
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
4975	def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) # pylint: disable=no-member return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
4093	def addBorrowers ( self , * borrowers ) : self . _borrowers . extend ( borrowers ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB borrower(s): %s' % ', ' . join ( [ str ( x ) for x in self . _borrowers ] ) ) return self
9515	def to_Fastq ( self , qual_scores ) : if len ( self ) != len ( qual_scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual_scores ] ) )
6283	def set_default_viewport ( self ) : # The expected height with the current viewport width expected_height = int ( self . buffer_width / self . aspect_ratio ) # How much positive or negative y padding blank_space = self . buffer_height - expected_height self . fbo . viewport = ( 0 , blank_space // 2 , self . buffer_width , expected_height )
11330	def get_record ( self , record ) : self . document = record rec = create_record ( ) language = self . _get_language ( ) if language and language != 'en' : record_add_field ( rec , '041' , subfields = [ ( 'a' , language ) ] ) publisher = self . _get_publisher ( ) date = self . _get_date ( ) if publisher and date : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) , ( 'c' , date ) ] ) elif publisher : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) ] ) elif date : record_add_field ( rec , '260' , subfields = [ ( 'c' , date ) ] ) title = self . _get_title ( ) if title : record_add_field ( rec , '245' , subfields = [ ( 'a' , title ) ] ) record_copyright = self . _get_copyright ( ) if record_copyright : record_add_field ( rec , '540' , subfields = [ ( 'a' , record_copyright ) ] ) subject = self . _get_subject ( ) if subject : record_add_field ( rec , '650' , ind1 = '1' , ind2 = '7' , subfields = [ ( 'a' , subject ) , ( '2' , 'PoS' ) ] ) authors = self . _get_authors ( ) first_author = True for author in authors : subfields = [ ( 'a' , author [ 0 ] ) ] for affiliation in author [ 1 ] : subfields . append ( ( 'v' , affiliation ) ) if first_author : record_add_field ( rec , '100' , subfields = subfields ) first_author = False else : record_add_field ( rec , '700' , subfields = subfields ) identifier = self . get_identifier ( ) conference = identifier . split ( ':' ) [ 2 ] conference = conference . split ( '/' ) [ 0 ] contribution = identifier . split ( ':' ) [ 2 ] contribution = contribution . split ( '/' ) [ 1 ] record_add_field ( rec , '773' , subfields = [ ( 'p' , 'PoS' ) , ( 'v' , conference . replace ( ' ' , '' ) ) , ( 'c' , contribution ) , ( 'y' , date [ : 4 ] ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'ConferencePaper' ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'HEP' ) ] ) return rec
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
9681	def config2 ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x3D ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 9 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "AMSamplingInterval" ] = self . _16bit_unsigned ( config [ 0 ] , config [ 1 ] ) data [ "AMIdleIntervalCount" ] = self . _16bit_unsigned ( config [ 2 ] , config [ 3 ] ) data [ 'AMFanOnIdle' ] = config [ 4 ] data [ 'AMLaserOnIdle' ] = config [ 5 ] data [ 'AMMaxDataArraysInFile' ] = self . _16bit_unsigned ( config [ 6 ] , config [ 7 ] ) data [ 'AMOnlySavePMData' ] = config [ 8 ] sleep ( 0.1 ) return data
10831	def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
6594	def run_multiple ( self , eventLoops ) : self . nruns += len ( eventLoops ) return self . communicationChannel . put_multiple ( eventLoops )
8449	def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
12690	def queue ( users , label , extra_context = None , sender = None ) : if extra_context is None : extra_context = { } if isinstance ( users , QuerySet ) : users = [ row [ "pk" ] for row in users . values ( "pk" ) ] else : users = [ user . pk for user in users ] notices = [ ] for user in users : notices . append ( ( user , label , extra_context , sender ) ) NoticeQueueBatch ( pickled_data = base64 . b64encode ( pickle . dumps ( notices ) ) ) . save ( )
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
8364	def _output_file ( self , frame ) : if self . buff : return self . buff elif self . multifile : return self . file_root + "_%03d" % frame + self . file_ext else : return self . filename
13720	def main ( ) : ep = requests . get ( TRELLO_API_DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, "reference internal")]/@href' ) pages = [ requests . get ( TRELLO_API_DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class="section"]/h2/..' ) for sec in sections : ep_html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep_text = html2text ( ep_html ) . splitlines ( ) match = EP_DESC_REGEX . match ( ep_text [ 0 ] ) if not match : continue ep_method , ep_url = match . groups ( ) ep_text [ 0 ] = ' ' . join ( [ ep_method , ep_url ] ) ep_doc = b64encode ( gzip . compress ( '\n' . join ( ep_text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep_method , ep_url , ep_doc ) ) print ( yaml . dump ( create_tree ( endpoints ) ) )
6172	def _select_manager ( backend_name ) : if backend_name == 'RedisBackend' : lock_manager = _LockManagerRedis elif backend_name == 'DatabaseBackend' : lock_manager = _LockManagerDB else : raise NotImplementedError return lock_manager
4762	def contents_of ( f , encoding = 'utf-8' ) : try : contents = f . read ( ) except AttributeError : try : with open ( f , 'r' ) as fp : contents = fp . read ( ) except TypeError : raise ValueError ( 'val must be file or path, but was type <%s>' % type ( f ) . __name__ ) except OSError : if not isinstance ( f , str_types ) : raise ValueError ( 'val must be file or path, but was type <%s>' % type ( f ) . __name__ ) raise if sys . version_info [ 0 ] == 3 and type ( contents ) is bytes : # in PY3 force decoding of bytes to target encoding return contents . decode ( encoding , 'replace' ) elif sys . version_info [ 0 ] == 2 and encoding == 'ascii' : # in PY2 force encoding back to ascii return contents . encode ( 'ascii' , 'replace' ) else : # in all other cases, try to decode to target encoding try : return contents . decode ( encoding , 'replace' ) except AttributeError : pass # if all else fails, just return the contents "as is" return contents
4847	def _load_data ( self , resource , detail_resource = None , resource_id = None , querystring = None , traverse_pagination = False , default = DEFAULT_VALUE_SAFEGUARD , ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } querystring = querystring if querystring else { } cache_key = utils . get_cache_key ( resource = resource , querystring = querystring , traverse_pagination = traverse_pagination , resource_id = resource_id ) response = cache . get ( cache_key ) if not response : # Response is not cached, so make a call. endpoint = getattr ( self . client , resource ) ( resource_id ) endpoint = getattr ( endpoint , detail_resource ) if detail_resource else endpoint response = endpoint . get ( * * querystring ) if traverse_pagination : results = utils . traverse_pagination ( response , endpoint ) response = { 'count' : len ( results ) , 'next' : 'None' , 'previous' : 'None' , 'results' : results , } if response : # Now that we've got a response, cache it. cache . set ( cache_key , response , settings . ENTERPRISE_API_CACHE_TIMEOUT ) return response or default_val
9647	def is_valid_in_template ( var , attr ) : # Remove private variables or methods if attr . startswith ( '_' ) : return False # Remove any attributes that raise an acception when read try : value = getattr ( var , attr ) except : return False if isroutine ( value ) : # Remove any routines that are flagged with 'alters_data' if getattr ( value , 'alters_data' , False ) : return False else : # Remove any routines that require arguments try : argspec = getargspec ( value ) num_args = len ( argspec . args ) if argspec . args else 0 num_defaults = len ( argspec . defaults ) if argspec . defaults else 0 if num_args - num_defaults > 1 : return False except TypeError : # C extension callables are routines, but getargspec fails with # a TypeError when these are passed. pass return True
7250	def launch_batch_workflow ( self , batch_workflow ) : # hit workflow api url = '%(base_url)s/batch_workflows' % { 'base_url' : self . base_url } try : r = self . gbdx_connection . post ( url , json = batch_workflow ) batch_workflow_id = r . json ( ) [ 'batch_workflow_id' ] return batch_workflow_id except TypeError as e : self . logger . debug ( 'Batch Workflow not launched, reason: {0}' . format ( e ) )
899	def prettyPrintSequence ( self , sequence , verbosity = 1 ) : text = "" for i in xrange ( len ( sequence ) ) : pattern = sequence [ i ] if pattern == None : text += "<reset>" if i < len ( sequence ) - 1 : text += "\n" else : text += self . patternMachine . prettyPrintPattern ( pattern , verbosity = verbosity ) return text
9317	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( * * response )
1856	def BSR ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , src . size - 1 , 1 ) == 1 res = 0 for pos in reversed ( range ( 0 , src . size ) ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , ( Operators . EXTRACT ( value , pos , 1 ) == 1 ) ) cpu . PF = cpu . _calculate_parity_flag ( res ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
3314	def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : # TODO: review this # XP and Vista MiniRedir submit PUT with Content-Length 0, # before LOCK and the real PUT. So we have to accept this. _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) # elif content_length < 0: # # TODO: review this # # If CONTENT_LENGTH is invalid, we may try to workaround this # # by reading until the end of the stream. This may block however! # # The iterator produced small chunks of varying size, but not # # sure, if we always get everything before it times out. # _logger.warning("PUT with invalid Content-Length (%s). " # "Trying to read all (this may timeout)..." # .format(environ.get("CONTENT_LENGTH"))) # nb = 0 # try: # for s in environ["wsgi.input"]: # environ["wsgidav.some_input_read"] = 1 # _logger.debug("PUT: read from wsgi.input.__iter__, len=%s" % len(s)) # yield s # nb += len (s) # except socket.timeout: # _logger.warning("PUT: input timed out after writing %s bytes" % nb) # hasErrors = True else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) # This happens with litmus expect-100 test: if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
12880	def _fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except StopIteration : self . buffer . append ( ( EndOfFile , EndOfFile ) ) self . len = len ( self . buffer )
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
1805	def SETA ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) == False , 1 , 0 ) )
9933	def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . __dict__ , self . obj , self . seen , self . _ignore ) # Ignore the calling frame, its builtins, globals and locals self . ignore_caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . _gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
11105	def sync_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if not self . _keepSynchronized : r = func ( self , * args , * * kwargs ) else : state = self . _load_state ( ) #print("-----------> ",state, self.state) if state is None : r = func ( self , * args , * * kwargs ) elif state == self . state : r = func ( self , * args , * * kwargs ) else : warnings . warn ( "Repository at '%s' is out of date. Need to load it again to avoid conflict." % self . path ) r = None return r return wrapper
13676	def add_prepare_handler ( self , prepare_handlers ) : if not isinstance ( prepare_handlers , static_bundle . BUNDLE_ITERABLE_TYPES ) : prepare_handlers = [ prepare_handlers ] if self . prepare_handlers_chain is None : self . prepare_handlers_chain = [ ] for handler in prepare_handlers : self . prepare_handlers_chain . append ( handler )
8070	def replace_entities ( ustring , placeholder = " " ) : def _repl_func ( match ) : try : if match . group ( 1 ) : # Numeric character reference return unichr ( int ( match . group ( 2 ) ) ) else : try : return cp1252 [ unichr ( int ( match . group ( 3 ) ) ) ] . strip ( ) except : return unichr ( name2codepoint [ match . group ( 3 ) ] ) except : return placeholder # Force to Unicode. if not isinstance ( ustring , unicode ) : ustring = UnicodeDammit ( ustring ) . unicode # Don't want some weird unicode character here # that truncate_spaces() doesn't know of: ustring = ustring . replace ( "&nbsp;" , " " ) # The ^> makes sure nothing inside a tag (i.e. href with query arguments) gets processed. _entity_re = re . compile ( r'&(?:(#)(\d+)|([^;^> ]+));' ) return _entity_re . sub ( _repl_func , ustring )
2025	def SGT ( self , a , b ) : # http://gavwood.com/paper.pdf s0 , s1 = to_signed ( a ) , to_signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
4303	def sox ( args ) : if args [ 0 ] . lower ( ) != "sox" : args . insert ( 0 , "sox" ) else : args [ 0 ] = "sox" try : logger . info ( "Executing: %s" , ' ' . join ( args ) ) process_handle = subprocess . Popen ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = process_handle . communicate ( ) out = out . decode ( "utf-8" ) err = err . decode ( "utf-8" ) status = process_handle . returncode return status , out , err except OSError as error_msg : logger . error ( "OSError: SoX failed! %s" , error_msg ) except TypeError as error_msg : logger . error ( "TypeError: %s" , error_msg ) return 1 , None , None
12431	def create_manage_scripts ( self ) : # create start script start = '# start script for {0}\n\n' . format ( self . _project_name ) # start uwsgi start += 'echo \'Starting uWSGI...\'\n' start += 'sh {0}.uwsgi\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' # start nginx start += 'echo \'Starting Nginx...\'\n' start += 'nginx -c {0}_nginx.conf\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' start += 'echo \'{0} started\'\n\n' . format ( self . _project_name ) # stop script stop = '# stop script for {0}\n\n' . format ( self . _project_name ) # stop nginx stop += 'if [ -e {0}_nginx.pid ]; then nginx -c {1}_nginx.conf -s stop ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) , os . path . join ( self . _conf_dir , self . _project_name ) ) # stop uwsgi stop += 'if [ -e {0}_uwsgi.pid ]; then kill -9 `cat {0}_uwsgi.pid` ; rm {0}_uwsgi.pid 2>&1 > /dev/null ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) stop += 'echo \'{0} stopped\'\n' . format ( self . _project_name ) # write scripts start_file = '{0}_start.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) stop_file = '{0}_stop.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) f = open ( start_file , 'w' ) f . write ( start ) f . close ( ) f = open ( stop_file , 'w' ) f . write ( stop ) f . close ( ) # make executable os . chmod ( start_file , 0754 ) os . chmod ( stop_file , 0754 )
5003	def handle ( self , * args , * * options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : # Assign admin role to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : # Assign operator role to staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : # Assign enterprise learner role to enterprise customer users. self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : # Assign enterprise enrollment api admin to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : # Assign enterprise catalog admin role to users with having credentials in catalog. self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
6289	def update ( self , * * kwargs ) : for name , value in kwargs . items ( ) : setattr ( self , name , value )
11823	def exp_schedule ( k = 20 , lam = 0.005 , limit = 100 ) : return lambda t : if_ ( t < limit , k * math . exp ( - lam * t ) , 0 )
5428	def _validate_job_and_task_arguments ( job_params , task_descriptors ) : if not task_descriptors : return task_params = task_descriptors [ 0 ] . task_params # The use case for specifying a label or env/input/output parameter on # the command-line and also including it in the --tasks file is not obvious. # Should the command-line override the --tasks file? Why? # Until this use is articulated, generate an error on overlapping names. # Check labels from_jobs = { label . name for label in job_params [ 'labels' ] } from_tasks = { label . name for label in task_params [ 'labels' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) # Check envs, inputs, and outputs, all of which must not overlap each other from_jobs = { item . name for item in job_params [ 'envs' ] | job_params [ 'inputs' ] | job_params [ 'outputs' ] } from_tasks = { item . name for item in task_params [ 'envs' ] | task_params [ 'inputs' ] | task_params [ 'outputs' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
6349	def _expand_alternates ( self , phonetic ) : alt_start = phonetic . find ( '(' ) if alt_start == - 1 : return self . _normalize_lang_attrs ( phonetic , False ) prefix = phonetic [ : alt_start ] alt_start += 1 # get past the ( alt_end = phonetic . find ( ')' , alt_start ) alt_string = phonetic [ alt_start : alt_end ] alt_end += 1 # get past the ) suffix = phonetic [ alt_end : ] alt_array = alt_string . split ( '|' ) result = '' for i in range ( len ( alt_array ) ) : alt = alt_array [ i ] alternate = self . _expand_alternates ( prefix + alt + suffix ) if alternate != '' and alternate != '[0]' : if result != '' : result += '|' result += alternate return result
956	def getArgumentDescriptions ( f ) : # Get the argument names and default values argspec = inspect . getargspec ( f ) # Scan through the docstring to extract documentation for each argument as # follows: # Check the first word of the line, stripping a colon if one is present. # If it matches an argument name: # Take the rest of the line, stripping leading whitespeace # Take each subsequent line if its indentation level is greater than the # initial indentation level # Once the indentation level is back to the original level, look for # another argument docstring = f . __doc__ descriptions = { } if docstring : lines = docstring . split ( '\n' ) i = 0 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : i += 1 continue # Indentation level is index of the first character indentLevel = lines [ i ] . index ( stripped [ 0 ] ) # Get the first word and remove the colon, if present firstWord = stripped . split ( ) [ 0 ] if firstWord . endswith ( ':' ) : firstWord = firstWord [ : - 1 ] if firstWord in argspec . args : # Found an argument argName = firstWord restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) argLines = [ restOfLine ] # Take the next lines as long as they are indented more i += 1 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : # Empty line - stop break if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : # No longer indented far enough - stop break # This line counts too argLines . append ( lines [ i ] . strip ( ) ) i += 1 # Store this description descriptions [ argName ] = ' ' . join ( argLines ) else : # Not an argument i += 1 # Build the list of (argName, description, defaultValue) args = [ ] if argspec . defaults : defaultCount = len ( argspec . defaults ) else : defaultCount = 0 nonDefaultArgCount = len ( argspec . args ) - defaultCount for i , argName in enumerate ( argspec . args ) : if i >= nonDefaultArgCount : defaultValue = argspec . defaults [ i - nonDefaultArgCount ] args . append ( ( argName , descriptions . get ( argName , "" ) , defaultValue ) ) else : args . append ( ( argName , descriptions . get ( argName , "" ) ) ) return args
12368	def records ( self , name ) : if self . get ( name ) : return DomainRecords ( self . api , name )
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
6012	def load_exposure_time_map ( exposure_time_map_path , exposure_time_map_hdu , pixel_scale , shape , exposure_time , exposure_time_map_from_inverse_noise_map , inverse_noise_map ) : exposure_time_map_options = sum ( [ exposure_time_map_from_inverse_noise_map ] ) if exposure_time is not None and exposure_time_map_path is not None : raise exc . DataException ( 'You have supplied both a exposure_time_map_path to an exposure time map and an exposure time. Only' 'one quantity should be supplied.' ) if exposure_time_map_options == 0 : if exposure_time is not None and exposure_time_map_path is None : return ExposureTimeMap . single_value ( value = exposure_time , pixel_scale = pixel_scale , shape = shape ) elif exposure_time is None and exposure_time_map_path is not None : return ExposureTimeMap . from_fits_with_pixel_scale ( file_path = exposure_time_map_path , hdu = exposure_time_map_hdu , pixel_scale = pixel_scale ) else : if exposure_time_map_from_inverse_noise_map : return ExposureTimeMap . from_exposure_time_and_inverse_noise_map ( pixel_scale = pixel_scale , exposure_time = exposure_time , inverse_noise_map = inverse_noise_map )
17	def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : def _write_obs ( maybe_dict_obs ) : flatdict = obs_to_dict ( maybe_dict_obs ) for k in keys : dst = obs_bufs [ k ] . get_obj ( ) dst_np = np . frombuffer ( dst , dtype = obs_dtypes [ k ] ) . reshape ( obs_shapes [ k ] ) # pylint: disable=W0212 np . copyto ( dst_np , flatdict [ k ] ) env = env_fn_wrapper . x ( ) parent_pipe . close ( ) try : while True : cmd , data = pipe . recv ( ) if cmd == 'reset' : pipe . send ( _write_obs ( env . reset ( ) ) ) elif cmd == 'step' : obs , reward , done , info = env . step ( data ) if done : obs = env . reset ( ) pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) elif cmd == 'render' : pipe . send ( env . render ( mode = 'rgb_array' ) ) elif cmd == 'close' : pipe . send ( None ) break else : raise RuntimeError ( 'Got unrecognized cmd %s' % cmd ) except KeyboardInterrupt : print ( 'ShmemVecEnv worker: got KeyboardInterrupt' ) finally : env . close ( )
3886	def _add_user_from_conv_part ( self , conv_part ) : user_ = User . from_conv_part_data ( conv_part , self . _self_user . id_ ) existing = self . _user_dict . get ( user_ . id_ ) if existing is None : logger . warning ( 'Adding fallback User with %s name "%s"' , user_ . name_type . name . lower ( ) , user_ . full_name ) self . _user_dict [ user_ . id_ ] = user_ return user_ else : existing . upgrade_name ( user_ ) return existing
5615	def read_vector_window ( input_files , tile , validity_check = True ) : if not isinstance ( input_files , list ) : input_files = [ input_files ] return [ feature for feature in chain . from_iterable ( [ _read_vector_window ( path , tile , validity_check = validity_check ) for path in input_files ] ) ]
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
13479	def _hyphens_to_dashes ( self ) : problematic_hyphens = [ ( r'-([.,!)])' , r'---\1' ) , ( r'(?<=\d)-(?=\d)' , '--' ) , ( r'(?<=\s)-(?=\s)' , '---' ) ] for problem_case in problematic_hyphens : self . _regex_replacement ( * problem_case )
278	def axes_style ( style = 'darkgrid' , rc = None ) : if rc is None : rc = { } rc_default = { } # Add defaults if they do not exist for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . axes_style ( style = style , rc = rc )
6323	def ac_encode ( text , probs ) : coder = Arithmetic ( ) coder . set_probs ( probs ) return coder . encode ( text )
4071	def split_multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\n' ) ) if element ]
7328	def _get_base_url ( base_url , api , version ) : format_args = { } if "{api}" in base_url : if api == "" : base_url = base_url . replace ( '{api}.' , '' ) else : format_args [ 'api' ] = api if "{version}" in base_url : if version == "" : base_url = base_url . replace ( '/{version}' , '' ) else : format_args [ 'version' ] = version return base_url . format ( api = api , version = version )
877	def agitate ( self ) : for ( varName , var ) in self . permuteVars . iteritems ( ) : var . agitate ( ) self . newPosition ( )
5302	def parse_rgb_txt_file ( path ) : #: Holds the generated color dict color_dict = { } with open ( path , 'r' ) as rgb_txt : for line in rgb_txt : line = line . strip ( ) if not line or line . startswith ( '!' ) : continue # skip comments parts = line . split ( ) color_dict [ " " . join ( parts [ 3 : ] ) ] = ( int ( parts [ 0 ] ) , int ( parts [ 1 ] ) , int ( parts [ 2 ] ) ) return color_dict
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
930	def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : # this field is not supposed to be aggregated. continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record
9339	def loadtxt2 ( fname , dtype = None , delimiter = ' ' , newline = '\n' , comment_character = '#' , skiplines = 0 ) : dtypert = [ None , None , None ] def preparedtype ( dtype ) : dtypert [ 0 ] = dtype flatten = flatten_dtype ( dtype ) dtypert [ 1 ] = flatten dtypert [ 2 ] = numpy . dtype ( [ ( 'a' , ( numpy . int8 , flatten . itemsize ) ) ] ) buf = numpy . empty ( ( ) , dtype = dtypert [ 1 ] ) converters = [ _default_conv [ flatten [ name ] . char ] for name in flatten . names ] return buf , converters , flatten . names def fileiter ( fh ) : converters = [ ] buf = None if dtype is not None : buf , converters , names = preparedtype ( dtype ) yield None for lineno , line in enumerate ( fh ) : if lineno < skiplines : continue if line [ 0 ] in comment_character : if buf is None and line [ 1 ] == '?' : ddtype = pickle . loads ( base64 . b64decode ( line [ 2 : ] ) ) buf , converters , names = preparedtype ( ddtype ) yield None continue for word , c , name in zip ( line . split ( ) , converters , names ) : buf [ name ] = c ( word ) buf2 = buf . copy ( ) . view ( dtype = dtypert [ 2 ] ) yield buf2 if isinstance ( fname , basestring ) : fh = file ( fh , 'r' ) cleanup = lambda : fh . close ( ) else : fh = iter ( fname ) cleanup = lambda : None try : i = fileiter ( fh ) i . next ( ) return numpy . fromiter ( i , dtype = dtypert [ 2 ] ) . view ( dtype = dtypert [ 0 ] ) finally : cleanup ( )
10541	def get_tasks ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'task' , params = params ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : # pragma: no cover raise
12148	def convertImages ( self ) : # copy over JPGs (and such) exts = [ '.jpg' , '.png' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_jpg_" + fname if not fname2 in self . files2 : self . log . info ( "copying over [%s]" % fname2 ) shutil . copy ( os . path . join ( self . folder1 , fname ) , os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname ) # convert TIFs (and such) to JPGs exts = [ '.tif' , '.tiff' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_tif_" + fname + ".jpg" if not fname2 in self . files2 : self . log . info ( "converting micrograph [%s]" % fname2 ) imaging . TIF_to_jpg ( os . path . join ( self . folder1 , fname ) , saveAs = os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname )
12174	def genIndex ( folder , forceIDs = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) #ABF folder files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( genPNGs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . getIDfileDict ( files ) #TODO: this is really slow print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . getABFgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abfID in groups [ ID ] : if abfID in forceIDs : overwrite = True try : htmlABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = expMenu ( groups , folder ) makeSplash ( menu , folder ) makeMenu ( menu , folder ) htmlFrames ( d , folder ) makeMenu ( menu , folder ) makeSplash ( menu , folder )
11867	def normalize ( self ) : assert len ( self . vars ) == 1 return ProbDist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )
13767	def add_bundle ( self , * args ) : for bundle in args : if not self . multitype and self . has_bundles ( ) : first_bundle = self . get_first_bundle ( ) if first_bundle . get_type ( ) != bundle . get_type ( ) : raise Exception ( 'Different bundle types for one Asset: %s[%s -> %s]' 'check types or set multitype parameter to True' % ( self . name , first_bundle . get_type ( ) , bundle . get_type ( ) ) ) self . bundles . append ( bundle ) return self
1412	def _get_topologies_with_watch ( self , callback , isWatching ) : path = self . get_topologies_path ( ) if isWatching : LOG . info ( "Adding children watch for path: " + path ) # pylint: disable=unused-variable @ self . client . ChildrenWatch ( path ) def watch_topologies ( topologies ) : """ callback to watch topologies """ callback ( topologies ) # Returning False will result in no future watches # being triggered. If isWatching is True, then # the future watches will be triggered. return isWatching
13738	def _real_time_thread ( self ) : while self . ws_client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws_client . receive ( ) if message is None : break message_type = message [ 'type' ] if message_type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message_type == 'open' : self . _handle_open ( message ) elif message_type == 'match' : self . _handle_match ( message ) elif message_type == 'done' : self . _handle_done ( message ) elif message_type == 'change' : self . _handle_change ( message ) else : continue self . ws_client . disconnect ( )
3456	def build_hugo_md ( filename , tag , bump ) : header = [ '+++\n' , 'date = "{}"\n' . format ( date . today ( ) . isoformat ( ) ) , 'title = "{}"\n' . format ( tag ) , 'author = "The COBRApy Team"\n' , 'release = "{}"\n' . format ( bump ) , '+++\n' , '\n' ] with open ( filename , "r" ) as file_h : content = insert_break ( file_h . readlines ( ) ) header . extend ( content ) with open ( filename , "w" ) as file_h : file_h . writelines ( header )
4977	def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
6941	def _double_inverted_gaussian ( x , amp1 , loc1 , std1 , amp2 , loc2 , std2 ) : gaussian1 = - _gaussian ( x , amp1 , loc1 , std1 ) gaussian2 = - _gaussian ( x , amp2 , loc2 , std2 ) return gaussian1 + gaussian2
375	def adjust_hue ( im , hout = 0.66 , is_offset = True , is_clip = True , is_random = False ) : hsv = rgb_to_hsv ( im ) if is_random : hout = np . random . uniform ( - hout , hout ) if is_offset : hsv [ ... , 0 ] += hout else : hsv [ ... , 0 ] = hout if is_clip : hsv [ ... , 0 ] = np . clip ( hsv [ ... , 0 ] , 0 , np . inf ) # Hao : can remove green dots rgb = hsv_to_rgb ( hsv ) return rgb
3859	def update_conversation ( self , conversation ) : # StateUpdate.conversation is actually a delta; fields that aren't # specified are assumed to be unchanged. Until this class is # refactored, hide this by saving and restoring previous values where # necessary. new_state = conversation . self_conversation_state old_state = self . _conversation . self_conversation_state self . _conversation = conversation # delivery_medium_option if not new_state . delivery_medium_option : new_state . delivery_medium_option . extend ( old_state . delivery_medium_option ) # latest_read_timestamp old_timestamp = old_state . self_read_state . latest_read_timestamp new_timestamp = new_state . self_read_state . latest_read_timestamp if new_timestamp == 0 : new_state . self_read_state . latest_read_timestamp = old_timestamp # user_read_state(s) for new_entry in conversation . read_state : tstamp = parsers . from_timestamp ( new_entry . latest_read_timestamp ) if tstamp == 0 : continue uid = parsers . from_participantid ( new_entry . participant_id ) if uid not in self . _watermarks or self . _watermarks [ uid ] < tstamp : self . _watermarks [ uid ] = tstamp
8072	def is_type ( url , types = [ ] , wait = 10 ) : # Types can also be a single string for convenience. if isinstance ( types , str ) : types = [ types ] try : connection = open ( url , wait ) except : return False type = connection . info ( ) [ "Content-Type" ] for t in types : if type . startswith ( t ) : return True return False
8109	def search_images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_IMAGES return GoogleSearch ( q , start , service , size , wait , asynchronous , cached )
152	def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
10718	def normalize_unitnumber ( unit_number ) : try : try : unit_number = int ( unit_number ) except ValueError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) except TypeError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) if not ( 1 <= unit_number <= 16 ) : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) return unit_number
5893	def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url # Response content type needs to be text/html here or else # IE will try to download the file. return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
12856	def merge_text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }
13172	def last ( self , name = None ) : for c in self . children ( name , reverse = True ) : return c
5270	def _get_word_start_index ( self , idx ) : i = 0 for _idx in self . word_starts [ 1 : ] : if idx < _idx : return i else : i += 1 return i
11251	def get_percentage ( a , b , i = False , r = False ) : # Round to the second decimal if i is False and r is True : percentage = round ( 100.0 * ( float ( a ) / b ) , 2 ) # Round to the nearest whole number elif ( i is True and r is True ) or ( i is True and r is False ) : percentage = int ( round ( 100 * ( float ( a ) / b ) ) ) # A rounded number and an integer were requested if r is False : warnings . warn ( "If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments." ) # A precise unrounded decimal else : percentage = 100.0 * ( float ( a ) / b ) return percentage
13734	def register_range_type ( pgrange , pyrange , conn ) : register_adapter ( pyrange , partial ( adapt_range , pgrange ) ) register_range_caster ( pgrange , pyrange , * query_range_oids ( pgrange , conn ) , scope = conn )
10706	def get_vacations ( ) : arequest = requests . get ( VACATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
2609	def _extract_buffers ( obj , threshold = MAX_BYTES ) : buffers = [ ] if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = _nbytes ( buf ) if nbytes > threshold : # buffer larger than threshold, prevent pickling obj . buffers [ i ] = None buffers . append ( buf ) # buffer too small for separate send, coerce to bytes # because pickling buffer objects just results in broken pointers elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers
7488	def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
4382	def is_allowed ( self , role , method , resource ) : return ( role , method , resource ) in self . _allowed
6042	def sparse_to_unmasked_sparse ( self ) : return mapping_util . sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels = self . total_sparse_pixels , mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres ) . astype ( 'int' )
4730	def start ( self ) : self . __thread = Thread ( target = self . __run , args = ( True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
1200	def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
6933	def colormagdiagram_cpdir ( cpdir , outpkl , cpfileglob = 'checkplot*.pkl*' , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return colormagdiagram_cplist ( cplist , outpkl , color_mag1 = color_mag1 , color_mag2 = color_mag2 , yaxis_mag = yaxis_mag )
9306	def encode_body ( req ) : if isinstance ( req . body , text_type ) : split = req . headers . get ( 'content-type' , 'text/plain' ) . split ( ';' ) if len ( split ) == 2 : ct , cs = split cs = cs . split ( '=' ) [ 1 ] req . body = req . body . encode ( cs ) else : ct = split [ 0 ] if ( ct == 'application/x-www-form-urlencoded' or 'x-amz-' in ct ) : req . body = req . body . encode ( ) else : req . body = req . body . encode ( 'utf-8' ) req . headers [ 'content-type' ] = ct + '; charset=utf-8'
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
7372	def _prepare_drb_allele_name ( self , parsed_beta_allele ) : if "DRB" not in parsed_beta_allele . gene : raise ValueError ( "Unexpected allele %s" % parsed_beta_allele ) return "%s_%s%s" % ( parsed_beta_allele . gene , parsed_beta_allele . allele_family , parsed_beta_allele . allele_code )
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
11234	def translate_array ( self , string , language , level = 3 , retdata = False ) : language = language . lower ( ) assert self . is_built_in ( language ) or language in self . outer_templates , "Sorry, " + language + " is not a supported language." # Serialized data converted to a python data structure (list of tuples) data = phpserialize . loads ( bytes ( string , 'utf-8' ) , array_hook = list , decode_strings = True ) # If language conversion is supported by python avoid recursion entirely # and use a built in library if self . is_built_in ( language ) : self . get_built_in ( language , level , data ) print ( self ) return self . data_structure if retdata else None # The language is not supported. Use recursion to build a data structure. def loop_print ( iterable , level = 3 ) : """ Loops over a python representation of a php array (list of tuples) and constructs a representation in another language. Translates a php array into another structure. Args: iterable: list or tuple to unpack. level: integer, number of spaces to use for indentation """ retval = '' indentation = ' ' * level # Base case - variable is not an iterable if not self . is_iterable ( iterable ) or isinstance ( iterable , str ) : non_iterable = str ( iterable ) return str ( non_iterable ) # Recursive case for item in iterable : # If item is a tuple it should be a key, value pair if isinstance ( item , tuple ) and len ( item ) == 2 : # Get the key value pair key = item [ 0 ] val = loop_print ( item [ 1 ] , level = level + 3 ) # Translate special values val = self . translate_val ( language , val ) if language in self . lang_specific_values and val in self . lang_specific_values [ language ] else val # Convert keys to their properly formatted strings # Integers are not quoted as array keys key = str ( key ) if isinstance ( key , int ) else '\'' + str ( key ) + '\'' # The first item is a key and the second item is an iterable, boolean needs_unpacking = hasattr ( item [ 0 ] , '__iter__' ) == False and hasattr ( item [ 1 ] , '__iter__' ) == True # The second item is an iterable if needs_unpacking : retval += self . get_inner_template ( language , 'iterable' , indentation , key , val ) # The second item is not an iterable else : # Convert values to their properly formatted strings # Integers and booleans are not quoted as array values val = str ( val ) if val . isdigit ( ) or val in self . lang_specific_values [ language ] . values ( ) else '\'' + str ( val ) + '\'' retval += self . get_inner_template ( language , 'singular' , indentation , key , val ) return retval # Execute the recursive call in language specific wrapper template self . data_structure = self . outer_templates [ language ] % ( loop_print ( data ) ) print ( self ) return self . data_structure if retdata else None
9102	def get_namespace_hash ( self , hash_fn = hashlib . md5 ) -> str : m = hash_fn ( ) if self . has_names : items = self . _get_namespace_name_to_encoding ( desc = 'getting hash' ) . items ( ) else : items = self . _get_namespace_identifier_to_encoding ( desc = 'getting hash' ) . items ( ) for name , encoding in items : m . update ( f'{name}:{encoding}' . encode ( 'utf8' ) ) return m . hexdigest ( )
6998	def parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = False , lcfnamelist = None , cprenorm = False , lclistpkl = None , gaia_max_timeout = 60.0 , gaia_mirror = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , makeneighborlcs = True , xmatchinfo = None , xmatchradiusarcsec = 3.0 , sigclip = 10.0 , minobservations = 99 , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , skipdone = False , done_callback = None , done_callback_args = None , done_callback_kwargs = None , liststartindex = None , maxobjects = None , nworkers = NCPUS , ) : # work around the Darwin segfault after fork if no network activity in # main thread bug: https://bugs.python.org/issue30385#msg293958 if sys . platform == 'darwin' : import requests requests . get ( 'http://captive.apple.com/hotspot-detect.html' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) # handle the start and end indices if ( liststartindex is not None ) and ( maxobjects is None ) : pfpicklelist = pfpicklelist [ liststartindex : ] if lcfnamelist is not None : lcfnamelist = lcfnamelist [ liststartindex : ] elif ( liststartindex is None ) and ( maxobjects is not None ) : pfpicklelist = pfpicklelist [ : maxobjects ] if lcfnamelist is not None : lcfnamelist = lcfnamelist [ : maxobjects ] elif ( liststartindex is not None ) and ( maxobjects is not None ) : pfpicklelist = ( pfpicklelist [ liststartindex : liststartindex + maxobjects ] ) if lcfnamelist is not None : lcfnamelist = lcfnamelist [ liststartindex : liststartindex + maxobjects ] # if the lcfnamelist is not provided, create a dummy if lcfnamelist is None : lcfnamelist = [ None ] * len ( pfpicklelist ) tasklist = [ ( x , outdir , lcbasedir , { 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'lcfname' : y , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lclistpkl' : lclistpkl , 'gaia_max_timeout' : gaia_max_timeout , 'gaia_mirror' : gaia_mirror , 'nbrradiusarcsec' : nbrradiusarcsec , 'maxnumneighbors' : maxnumneighbors , 'makeneighborlcs' : makeneighborlcs , 'xmatchinfo' : xmatchinfo , 'xmatchradiusarcsec' : xmatchradiusarcsec , 'sigclip' : sigclip , 'minobservations' : minobservations , 'skipdone' : skipdone , 'cprenorm' : cprenorm , 'fast_mode' : fast_mode , 'done_callback' : done_callback , 'done_callback_args' : done_callback_args , 'done_callback_kwargs' : done_callback_kwargs } ) for x , y in zip ( pfpicklelist , lcfnamelist ) ] resultfutures = [ ] results = [ ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( runcp_worker , tasklist ) results = [ x for x in resultfutures ] executor . shutdown ( ) return results
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
11481	def _create_folder ( local_folder , parent_folder_id ) : new_folder = session . communicator . create_folder ( session . token , os . path . basename ( local_folder ) , parent_folder_id ) return new_folder [ 'folder_id' ]
10726	def _handle_variant ( self ) : def the_func ( a_tuple , variant = 0 ) : """ Function for generating a variant value from a tuple. :param a_tuple: the parts of the variant :type a_tuple: (str * object) or list :param int variant: object's variant index :returns: a value of the correct type with correct variant level :rtype: object * int """ # pylint: disable=unused-argument ( signature , an_obj ) = a_tuple ( func , sig ) = self . COMPLETE . parseString ( signature ) [ 0 ] assert sig == signature ( xformed , _ ) = func ( an_obj , variant = variant + 1 ) return ( xformed , xformed . variant_level ) return ( the_func , 'v' )
8782	def select_ipam_strategy ( self , network_id , network_strategy , * * kwargs ) : LOG . info ( "Selecting IPAM strategy for network_id:%s " "network_strategy:%s" % ( network_id , network_strategy ) ) net_type = "tenant" if STRATEGY . is_provider_network ( network_id ) : net_type = "provider" strategy = self . _ipam_strategies . get ( net_type , { } ) default = strategy . get ( "default" ) overrides = strategy . get ( "overrides" , { } ) # If we override a particular strategy explicitly, we use it. if network_strategy in overrides : LOG . info ( "Selected overridden IPAM strategy: %s" % ( overrides [ network_strategy ] ) ) return overrides [ network_strategy ] # Otherwise, we are free to use an explicit default. if default : LOG . info ( "Selected default IPAM strategy for tenant " "network: %s" % ( default ) ) return default # Fallback to the network-specified IPAM strategy LOG . info ( "Selected network strategy for tenant " "network: %s" % ( network_strategy ) ) return network_strategy
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) # Filter commands if auth is enabled, hide_admin_commands is enabled, and user is not admin if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
10191	def get_anonymization_salt ( ts ) : salt_key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current_cache . get ( salt_key ) if not salt : salt_bytes = os . urandom ( 32 ) salt = b64encode ( salt_bytes ) . decode ( 'utf-8' ) current_cache . set ( salt_key , salt , timeout = 60 * 60 * 24 ) return salt
6801	def load_db_set ( self , name , r = None ) : r = r or self db_set = r . genv . db_sets . get ( name , { } ) r . genv . update ( db_set )
11909	def to_pattern_matrix ( D ) : result = np . zeros_like ( D ) # This is a cleverer way of doing # # for (u, v) in zip(*(D.nonzero())): # result[u, v] = 1 # result [ D . nonzero ( ) ] = 1 return result
10628	def T ( self , T ) : self . _T = T self . _Hfr = self . _calculate_Hfr ( T )
6458	def _m_degree ( self , term ) : mdeg = 0 last_was_vowel = False for letter in term : if letter in self . _vowels : last_was_vowel = True else : if last_was_vowel : mdeg += 1 last_was_vowel = False return mdeg
2289	def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , * * kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
2877	def one ( nodes , or_none = False ) : if not nodes and or_none : return None assert len ( nodes ) == 1 , 'Expected 1 result. Received %d results.' % ( len ( nodes ) ) return nodes [ 0 ]
8845	def _at_block_start ( tc , line ) : if tc . atBlockStart ( ) : return True column = tc . columnNumber ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
3835	async def set_active_client ( self , set_active_client_request ) : response = hangouts_pb2 . SetActiveClientResponse ( ) await self . _pb_request ( 'clients/setactiveclient' , set_active_client_request , response ) return response
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : # check if the process group is dead if not self . is_alive ( ) : # todo : can be graceful, in which # case the last item shall have been # flushed to Q. try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
3562	def find_characteristic ( self , uuid ) : for char in self . list_characteristics ( ) : if char . uuid == uuid : return char return None
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : # get set of tile columns tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) # if tile columns are an unbroken sequence, tiles are connected and are not # passing the Antimeridian if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : # look at column gaps and try to determine the smallest distance def gen_groups ( items ) : """Groups tile columns by sequence.""" j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : # item is next in expected sequence if i == j + 1 : group . append ( i ) # gap occured, so yield existing group and create new one else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) # in case there is only one group, don't shift if len ( groups ) == 1 : return False # distance between first column of first group and last column of last group normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] # distance between last column of first group and last column of first group # but crossing the antimeridian antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] # return whether distance over antimeridian is shorter return antimeridian_distance < normal_distance else : return False
5546	def raster2pyramid ( input_file , output_dir , options ) : pyramid_type = options [ "pyramid_type" ] scale_method = options [ "scale_method" ] output_format = options [ "output_format" ] resampling = options [ "resampling" ] zoom = options [ "zoom" ] bounds = options [ "bounds" ] mode = "overwrite" if options [ "overwrite" ] else "continue" # Prepare process parameters minzoom , maxzoom = _get_zoom ( zoom , input_file , pyramid_type ) with rasterio . open ( input_file , "r" ) as input_raster : output_bands = input_raster . count input_dtype = input_raster . dtypes [ 0 ] output_dtype = input_raster . dtypes [ 0 ] nodataval = input_raster . nodatavals [ 0 ] nodataval = nodataval if nodataval else 0 if output_format == "PNG" and output_bands > 3 : output_bands = 3 output_dtype = 'uint8' scales_minmax = ( ) if scale_method == "dtype_scale" : for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( DTYPE_RANGES [ input_dtype ] , ) elif scale_method == "minmax_scale" : for index in range ( 1 , output_bands + 1 ) : band = input_raster . read ( index ) scales_minmax += ( ( band . min ( ) , band . max ( ) ) , ) elif scale_method == "crop" : for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( ( 0 , 255 ) , ) if input_dtype == "uint8" : scale_method = None scales_minmax = ( ) for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( ( None , None ) , ) # Create configuration config = dict ( process = "mapchete.processes.pyramid.tilify" , output = { "path" : output_dir , "format" : output_format , "bands" : output_bands , "dtype" : output_dtype } , pyramid = dict ( pixelbuffer = 5 , grid = pyramid_type ) , scale_method = scale_method , scales_minmax = scales_minmax , input = { "raster" : input_file } , config_dir = os . getcwd ( ) , zoom_levels = dict ( min = minzoom , max = maxzoom ) , nodataval = nodataval , resampling = resampling , bounds = bounds , baselevel = { "zoom" : maxzoom , "resampling" : resampling } , mode = mode ) # create process with mapchete . open ( config , zoom = zoom , bounds = bounds ) as mp : # prepare output directory if not os . path . exists ( output_dir ) : os . makedirs ( output_dir ) # run process mp . batch_process ( zoom = [ minzoom , maxzoom ] )
11136	def create_client ( ) -> APIClient : global _client client = _client ( ) if client is None : # First try looking at the environment variables for specification of the daemon's location docker_environment = kwargs_from_env ( assert_hostname = False ) if "base_url" in docker_environment : client = _create_client ( docker_environment . get ( "base_url" ) , docker_environment . get ( "tls" ) ) if client is None : raise ConnectionError ( "Could not connect to the Docker daemon specified by the `DOCKER_X` environment variables: %s" % docker_environment ) else : logging . info ( "Connected to Docker daemon specified by the environment variables" ) else : # Let's see if the Docker daemon is accessible via the UNIX socket client = _create_client ( "unix://var/run/docker.sock" ) if client is not None : logging . info ( "Connected to Docker daemon running on UNIX socket" ) else : raise ConnectionError ( "Cannot connect to Docker - is the Docker daemon running? `$DOCKER_HOST` should be set or the " "daemon should be accessible via the standard UNIX socket." ) _client = weakref . ref ( client ) assert isinstance ( client , APIClient ) return client
6631	def set ( self , path , value = None , filename = None ) : if filename is None : config = self . _firstConfig ( ) [ 1 ] else : config = self . configs [ filename ] path = _splitPath ( path ) for el in path [ : - 1 ] : if el in config : config = config [ el ] else : config [ el ] = OrderedDict ( ) config = config [ el ] config [ path [ - 1 ] ] = value
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
10375	def calculate_concordance_helper ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , ) -> Tuple [ int , int , int , int ] : scores = defaultdict ( int ) for u , v , k , d in graph . edges ( keys = True , data = True ) : c = edge_concords ( graph , u , v , k , d , key , cutoff = cutoff ) scores [ c ] += 1 return ( scores [ Concordance . correct ] , scores [ Concordance . incorrect ] , scores [ Concordance . ambiguous ] , scores [ Concordance . unassigned ] , )
3722	def calculate ( self , T , method ) : if method == CRC : A , B , C , D = self . CRC_coeffs epsilon = A + B * T + C * T ** 2 + D * T ** 3 elif method == CRC_CONSTANT : epsilon = self . CRC_permittivity elif method in self . tabular_data : epsilon = self . interpolate ( T , method ) return epsilon
3680	def T_converter ( T , current , desired ) : def range_check ( T , Tmin , Tmax ) : if T < Tmin or T > Tmax : raise Exception ( 'Temperature conversion is outside one or both scales' ) try : if current == 'ITS-90' : pass elif current == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T68_to_T90 ( T ) elif current == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T76_to_T90 ( T ) elif current == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T48_to_T90 ( T ) elif current == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T27_to_T90 ( T ) else : raise Exception ( 'Current scale not supported' ) # T should be in ITS-90 now if desired == 'ITS-90' : pass elif desired == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T90_to_T68 ( T ) elif desired == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T90_to_T76 ( T ) elif desired == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T90_to_T48 ( T ) elif desired == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T90_to_T27 ( T ) else : raise Exception ( 'Desired scale not supported' ) except ValueError : raise Exception ( 'Temperature could not be converted to desired scale' ) return float ( T )
6754	def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
5582	def create ( mapchete_file , process_file , out_format , out_path = None , pyramid_type = None , force = False ) : if os . path . isfile ( process_file ) or os . path . isfile ( mapchete_file ) : if not force : raise IOError ( "file(s) already exists" ) out_path = out_path if out_path else os . path . join ( os . getcwd ( ) , "output" ) # copy file template to target directory process_template = pkg_resources . resource_filename ( "mapchete.static" , "process_template.py" ) process_file = os . path . join ( os . getcwd ( ) , process_file ) copyfile ( process_template , process_file ) # modify and copy mapchete file template to target directory mapchete_template = pkg_resources . resource_filename ( "mapchete.static" , "mapchete_template.mapchete" ) output_options = dict ( format = out_format , path = out_path , * * FORMAT_MANDATORY [ out_format ] ) pyramid_options = { 'grid' : pyramid_type } substitute_elements = { 'process_file' : process_file , 'output' : dump ( { 'output' : output_options } , default_flow_style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid_options } , default_flow_style = False ) } with open ( mapchete_template , 'r' ) as config_template : config = Template ( config_template . read ( ) ) customized_config = config . substitute ( substitute_elements ) with open ( mapchete_file , 'w' ) as target_config : target_config . write ( customized_config )
3476	def _associate_gene ( self , cobra_gene ) : self . _genes . add ( cobra_gene ) cobra_gene . _reaction . add ( self ) cobra_gene . _model = self . _model
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
1845	def JP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . PF , target . read ( ) , cpu . PC )
11709	def instance ( self , counter = None , pipeline_counter = None ) : pipeline_counter = pipeline_counter or self . pipeline_counter pipeline_instance = None if not pipeline_counter : pipeline_instance = self . server . pipeline ( self . pipeline_name ) . instance ( ) self . pipeline_counter = int ( pipeline_instance [ 'counter' ] ) if not counter : if pipeline_instance is None : pipeline_instance = ( self . server . pipeline ( self . pipeline_name ) . instance ( pipeline_counter ) ) for stages in pipeline_instance [ 'stages' ] : if stages [ 'name' ] == self . stage_name : return self . instance ( counter = int ( stages [ 'counter' ] ) , pipeline_counter = pipeline_counter ) return self . _get ( '/instance/{pipeline_counter:d}/{counter:d}' . format ( pipeline_counter = pipeline_counter , counter = counter ) )
4650	def appendSigner ( self , accounts , permission ) : assert permission in self . permission_types , "Invalid permission" if self . blockchain . wallet . locked ( ) : raise WalletLocked ( ) if not isinstance ( accounts , ( list , tuple , set ) ) : accounts = [ accounts ] for account in accounts : # Now let's actually deal with the accounts if account not in self . signing_accounts : # is the account an instance of public key? if isinstance ( account , self . publickey_class ) : self . appendWif ( self . blockchain . wallet . getPrivateKeyForPublicKey ( str ( account ) ) ) # ... or should we rather obtain the keys from an account name else : accountObj = self . account_class ( account , blockchain_instance = self . blockchain ) required_treshold = accountObj [ permission ] [ "weight_threshold" ] keys = self . _fetchkeys ( accountObj , permission , required_treshold = required_treshold ) # If we couldn't find an active key, let's try overwrite it # with an owner key if not keys and permission != "owner" : keys . extend ( self . _fetchkeys ( accountObj , "owner" , required_treshold = required_treshold ) ) for x in keys : self . appendWif ( x [ 0 ] ) self . signing_accounts . append ( account )
2952	def connect ( self , task_spec ) : assert self . default_task_spec is None self . outputs . append ( task_spec ) self . default_task_spec = task_spec . name task_spec . _connect_notify ( self )
384	def parse_darknet_ann_str_to_list ( annotations ) : annotations = annotations . split ( "\n" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann
13826	def FromJsonString ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
12824	def _exec ( self , globals_dict = None ) : globals_dict = globals_dict or { } globals_dict . setdefault ( '__builtins__' , { } ) exec ( self . _code , globals_dict ) return globals_dict
8162	def publish_event ( event_t , data = None , extra_channels = None , wait = None ) : event = Event ( event_t , data ) pubsub . publish ( "shoebot" , event ) for channel_name in extra_channels or [ ] : pubsub . publish ( channel_name , event ) if wait is not None : channel = pubsub . subscribe ( wait ) channel . listen ( wait )
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
8353	def parse_declaration ( self , i ) : j = None if self . rawdata [ i : i + 9 ] == '<![CDATA[' : k = self . rawdata . find ( ']]>' , i ) if k == - 1 : k = len ( self . rawdata ) data = self . rawdata [ i + 9 : k ] j = k + 3 self . _toStringSubclass ( data , CData ) else : try : j = SGMLParser . parse_declaration ( self , i ) except SGMLParseError : toHandle = self . rawdata [ i : ] self . handle_data ( toHandle ) j = i + len ( toHandle ) return j
9282	def set_filter ( self , filter_text ) : self . filter = filter_text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . _connected : self . _sendall ( "#filter %s\r\n" % self . filter )
3353	def _replace_on_id ( self , new_object ) : the_id = new_object . id the_index = self . _dict [ the_id ] list . __setitem__ ( self , the_index , new_object )
10902	def lbl ( axis , label , size = 22 ) : at = AnchoredText ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set_boxstyle ( "round,pad=0.,rounding_size=0.0" ) #bb = axis.get_yaxis_transform() #at = AnchoredText(label, # loc=3, prop=dict(size=18), frameon=True, # bbox_to_anchor=(-0.5,1),#(-.255, 0.90), # bbox_transform=bb,#axis.transAxes # ) axis . add_artist ( at )
5067	def get_cache_key ( * * kwargs ) : key = '__' . join ( [ '{}:{}' . format ( item , value ) for item , value in iteritems ( kwargs ) ] ) return hashlib . md5 ( key . encode ( 'utf-8' ) ) . hexdigest ( )
12414	def flush ( self ) : # Ensure we're not closed. self . require_not_closed ( ) # Pull out the accumulated chunk. chunk = self . _stream . getvalue ( ) self . _stream . truncate ( 0 ) self . _stream . seek ( 0 ) # Append the chunk to the body. self . body = chunk if ( self . _body is None ) else ( self . _body + chunk ) if self . asynchronous : # We are now streaming because we're asynchronous. self . streaming = True
5832	def get ( self , data_view_id ) : failure_message = "Dataview get failed" return self . _get_success_json ( self . _get ( 'v1/data_views/' + data_view_id , None , failure_message = failure_message ) ) [ 'data' ] [ 'data_view' ]
9331	def cpu_count ( ) : num = os . getenv ( "OMP_NUM_THREADS" ) if num is None : num = os . getenv ( "PBS_NUM_PPN" ) try : return int ( num ) except : return multiprocessing . cpu_count ( )
13386	def upstream_url ( self , uri ) : return self . application . options . upstream + self . request . uri
11954	def upload_gif ( gif ) : client_id = os . environ . get ( 'IMGUR_API_ID' ) client_secret = os . environ . get ( 'IMGUR_API_SECRET' ) if client_id is None or client_secret is None : click . echo ( 'Cannot upload - could not find IMGUR_API_ID or IMGUR_API_SECRET environment variables' ) return client = ImgurClient ( client_id , client_secret ) click . echo ( 'Uploading file {}' . format ( click . format_filename ( gif ) ) ) response = client . upload_from_path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )
3172	def get ( self , store_id , customer_id , * * queryparams ) : self . store_id = store_id self . customer_id = customer_id return self . _mc_client . _get ( url = self . _build_path ( store_id , 'customers' , customer_id ) , * * queryparams )
1346	def gradient ( self , image , label ) : _ , gradient = self . predictions_and_gradient ( image , label ) return gradient
8068	def refresh ( self ) : self . reset ( ) self . parse ( self . source ) return self . output ( )
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
13719	def create_tree ( endpoints ) : tree = { } for method , url , doc in endpoints : path = [ p for p in url . strip ( '/' ) . split ( '/' ) ] here = tree # First element (API Version). version = path [ 0 ] here . setdefault ( version , { } ) here = here [ version ] # The rest of elements of the URL. for p in path [ 1 : ] : part = _camelcase_to_underscore ( p ) here . setdefault ( part , { } ) here = here [ part ] # Allowed HTTP methods. if not 'METHODS' in here : here [ 'METHODS' ] = [ [ method , doc ] ] else : if not method in here [ 'METHODS' ] : here [ 'METHODS' ] . append ( [ method , doc ] ) return tree
7122	def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash_func . update ( token ) return hash_func . hexdigest ( )
5729	def main ( verbose = True ) : # Build C program find_executable ( MAKE_CMD ) if not find_executable ( MAKE_CMD ) : print ( 'Could not find executable "%s". Ensure it is installed and on your $PATH.' % MAKE_CMD ) exit ( 1 ) subprocess . check_output ( [ MAKE_CMD , "-C" , SAMPLE_C_CODE_DIR , "--quiet" ] ) # Initialize object that manages gdb subprocess gdbmi = GdbController ( verbose = verbose ) # Send gdb commands. Gdb machine interface commands are easier to script around, # hence the name "machine interface". # Responses are automatically printed as they are received if verbose is True. # Responses are returned after writing, by default. # Load the file responses = gdbmi . write ( "-file-exec-and-symbols %s" % SAMPLE_C_BINARY ) # Get list of source files used to compile the binary responses = gdbmi . write ( "-file-list-exec-source-files" ) # Add breakpoint responses = gdbmi . write ( "-break-insert main" ) # Run responses = gdbmi . write ( "-exec-run" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-continue" ) # noqa: F841 # gdbmi.gdb_process will be None because the gdb subprocess (and its inferior # program) will be terminated gdbmi . exit ( )
9631	def send ( self , extra_context = None , * * kwargs ) : message = self . render_to_message ( extra_context = extra_context , * * kwargs ) return message . send ( )
8878	def find_libname ( self , name ) : names = [ "{}.lib" , "lib{}.lib" , "{}lib.lib" ] names = [ n . format ( name ) for n in names ] dirs = self . get_library_dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = "Could not find the {} library." . format ( name ) raise ValueError ( msg )
5285	def get ( self , request , * args , * * kwargs ) : formset = self . construct_formset ( ) return self . render_to_response ( self . get_context_data ( formset = formset ) )
1493	def _get_next_timeout_interval ( self ) : if len ( self . timer_tasks ) == 0 : return sys . maxsize else : next_timeout_interval = self . timer_tasks [ 0 ] [ 0 ] - time . time ( ) return next_timeout_interval
3533	def gosquared ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return GoSquaredNode ( )
6649	def inheritsFrom ( self , target_name ) : for t in self . hierarchy : if t and t . getName ( ) == target_name or target_name in t . description . get ( 'inherits' , { } ) : return True return False
8467	def path ( self , value ) : if not value . endswith ( '/' ) : self . _path = '{v}/' . format ( v = value ) else : self . _path = value
11162	def size ( self ) : try : return self . _stat . st_size except : # pragma: no cover self . _stat = self . stat ( ) return self . size
13596	def get ( f , key , default = None ) : if key in f . keys ( ) : val = f [ key ] . value if default is None : return val else : if _np . shape ( val ) == _np . shape ( default ) : return val return default
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
11679	def connect ( self ) : try : logger . info ( u'Connecting %s:%d' % ( self . host , self . port ) ) self . sock . connect ( ( self . host , self . port ) ) except socket . error : raise ConnectionError ( ) self . state = CONNECTED
7221	def ingest_vectors ( self , output_port_value ) : # append two tasks to self['definition']['tasks'] ingest_task = Task ( 'IngestItemJsonToVectorServices' ) ingest_task . inputs . items = output_port_value ingest_task . impersonation_allowed = True stage_task = Task ( 'StageDataToS3' ) stage_task . inputs . destination = 's3://{vector_ingest_bucket}/{recipe_id}/{run_id}/{task_name}' stage_task . inputs . data = ingest_task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest_task . generate_task_workflow_json ( ) ) self . definition [ 'tasks' ] . append ( stage_task . generate_task_workflow_json ( ) )
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
12767	def forces ( self , dx_tm1 = None ) : cfm = self . cfms [ self . _frame_no ] [ : , None ] kp = self . erp / ( cfm * self . world . dt ) kd = ( 1 - self . erp ) / cfm dx = self . distances ( ) F = kp * dx if dx_tm1 is not None : bad = np . isnan ( dx ) | np . isnan ( dx_tm1 ) F [ ~ bad ] += ( kd * ( dx - dx_tm1 ) / self . world . dt ) [ ~ bad ] return F
7256	def get_strip_metadata ( self , catID ) : self . logger . debug ( 'Retrieving strip catalog metadata' ) url = '%(base_url)s/record/%(catID)s?includeRelationships=false' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) if r . status_code == 200 : return r . json ( ) [ 'properties' ] elif r . status_code == 404 : self . logger . debug ( 'Strip not found: %s' % catID ) r . raise_for_status ( ) else : self . logger . debug ( 'There was a problem retrieving catid: %s' % catID ) r . raise_for_status ( )
9552	def _apply_value_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , check , code , message , modulus in self . _value_checks : if i % modulus == 0 : # support sampling fi = self . _field_names . index ( field_name ) if fi < len ( r ) : # only apply checks if there is a value value = r [ fi ] try : check ( value ) except ValueError : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
4822	def get_course_details ( self , course_id ) : try : return self . client . course ( course_id ) . get ( ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to retrieve course enrollment details for course [%s] due to: [%s]' , course_id , str ( exc ) ) return { }
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
1614	def Match ( pattern , s ) : # The regexp compilation caching is inlined in both Match and Search for # performance reasons; factoring it out into a separate function turns out # to be noticeably expensive. if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . match ( s )
13421	def map ( self , ID_s , FROM = None , TO = None , target_as_set = False , no_match_sub = None ) : def io_mode ( ID_s ) : ''' Handles the input/output modalities of the mapping. ''' unlist_return = False list_of_lists = False if isinstance ( ID_s , str ) : ID_s = [ ID_s ] unlist_return = True elif isinstance ( ID_s , list ) : if len ( ID_s ) > 0 and isinstance ( ID_s [ 0 ] , list ) : # assuming ID_s is a list of lists of ID strings list_of_lists = True return ID_s , unlist_return , list_of_lists # interpret input if FROM == TO : return ID_s ID_s , unlist_return , list_of_lists = io_mode ( ID_s ) # map consistent with interpretation of input if list_of_lists : mapped_ids = [ self . map ( ID , FROM , TO , target_as_set , no_match_sub ) for ID in ID_s ] else : mapped_ids = self . _map ( ID_s , FROM , TO , target_as_set , no_match_sub ) # return consistent with interpretation of input if unlist_return : return mapped_ids [ 0 ] return Mapping ( ID_s , mapped_ids )
1930	def add ( self , name : str , default = None , description : str = None ) : if name in self . _vars : raise ConfigError ( f"{self.name}.{name} already defined." ) if name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default ) self . _vars [ name ] = v
11423	def print_recs ( listofrec , format = 1 , tags = None ) : if tags is None : tags = [ ] text = "" if type ( listofrec ) . __name__ != 'list' : return "" else : for rec in listofrec : text = "%s\n%s" % ( text , print_rec ( rec , format , tags ) ) return text
5281	def make_tpot_pmml_config ( config , user_classpath = [ ] ) : tpot_keys = set ( config . keys ( ) ) classes = _supported_classes ( user_classpath ) pmml_keys = ( set ( classes ) ) . union ( set ( [ _strip_module ( class_ ) for class_ in classes ] ) ) return { key : config [ key ] for key in ( tpot_keys ) . intersection ( pmml_keys ) }
1344	def _get_output ( self , a , image ) : sd = np . square ( self . _input_images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) # if we run into numerical problems with this approach, we might # need to add a very tiny threshold here if mses [ index ] > 0 : raise ValueError ( 'No precomputed output image for this image' ) return self . _output_images [ index ]
94	def quokka ( size = None , extract = None ) : img = imageio . imread ( QUOKKA_FP , pilmode = "RGB" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is not None : shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) return img
2100	def read ( self , * args , * * kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , * * kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
4629	def from_pubkey ( cls , pubkey , compressed = True , version = 56 , prefix = None ) : # Ensure this is a public key pubkey = PublicKey ( pubkey , prefix = prefix or Prefix . prefix ) if compressed : pubkey_plain = pubkey . compressed ( ) else : pubkey_plain = pubkey . uncompressed ( ) sha = hashlib . sha256 ( unhexlify ( pubkey_plain ) ) . hexdigest ( ) rep = hexlify ( ripemd160 ( sha ) ) . decode ( "ascii" ) s = ( "%.2x" % version ) + rep result = s + hexlify ( doublesha256 ( s ) [ : 4 ] ) . decode ( "ascii" ) result = hexlify ( ripemd160 ( result ) ) . decode ( "ascii" ) return cls ( result , prefix = pubkey . prefix )
2378	def report ( self , linenumber , filename , severity , message , rulename , char ) : if self . _print_filename is not None : # we print the filename only once. self._print_filename # will get reset each time a new file is processed. print ( "+ " + self . _print_filename ) self . _print_filename = None if severity in ( WARNING , ERROR ) : self . counts [ severity ] += 1 else : self . counts [ "other" ] += 1 print ( self . args . format . format ( linenumber = linenumber , filename = filename , severity = severity , message = message . encode ( 'utf-8' ) , rulename = rulename , char = char ) )
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
2786	def create_from_snapshot ( self , * args , * * kwargs ) : data = self . get_data ( 'volumes/' , type = POST , params = { 'name' : self . name , 'snapshot_id' : self . snapshot_id , 'region' : self . region , 'size_gigabytes' : self . size_gigabytes , 'description' : self . description , 'filesystem_type' : self . filesystem_type , 'filesystem_label' : self . filesystem_label } ) if data : self . id = data [ 'volume' ] [ 'id' ] self . created_at = data [ 'volume' ] [ 'created_at' ] return self
9427	def getinfo ( self , name ) : rarinfo = self . NameToInfo . get ( name ) if rarinfo is None : raise KeyError ( 'There is no item named %r in the archive' % name ) return rarinfo
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : # first, lowercase, then _squeeze to single spaces stringelems = _squeeze ( filterstring ) . lower ( ) # replace shady characters stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) # split into words stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] # get rid of all numbers stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) # get rid of everything within quotes stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] # check the filterstring words against the allowed words wordset = set ( stringwords2 ) # generate the allowed word set for these LC columns allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) # if there are words left over, then this filter string is suspicious if len ( validatecheck ) > 0 : # check if validatecheck contains an elem with % in it LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
11801	def restore ( self , removals ) : for B , b in removals : self . curr_domains [ B ] . append ( b )
1738	def unify_string_literals ( js_string ) : n = 0 res = '' limit = len ( js_string ) while n < limit : char = js_string [ n ] if char == '\\' : new , n = do_escape ( js_string , n ) res += new else : res += char n += 1 return res
7627	def add_namespace ( filename ) : with open ( filename , mode = 'r' ) as fileobj : __NAMESPACE__ . update ( json . load ( fileobj ) )
943	def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , checkpointLabel + g_defaultCheckpointExtension ) checkpointDir = os . path . abspath ( checkpointDir ) return checkpointDir
12358	def send_request ( self , kind , resource , url_components , * * kwargs ) : url = self . format_request_url ( resource , * url_components ) meth = getattr ( requests , kind ) headers = self . get_request_headers ( ) req_data = self . format_parameters ( * * kwargs ) response = meth ( url , headers = headers , data = req_data ) data = self . get_response ( response ) if response . status_code >= 300 : msg = data . pop ( 'message' , 'API request returned error' ) raise APIError ( msg , response . status_code , * * data ) return data
4259	def read_markdown ( filename ) : global MD # Use utf-8-sig codec to remove BOM if it is present. This is only possible # this way prior to feeding the text to the markdown parser (which would # also default to pure utf-8) with open ( filename , 'r' , encoding = 'utf-8-sig' ) as f : text = f . read ( ) if MD is None : MD = Markdown ( extensions = [ 'markdown.extensions.meta' , 'markdown.extensions.tables' ] , output_format = 'html5' ) else : MD . reset ( ) # When https://github.com/Python-Markdown/markdown/pull/672 # will be available, this can be removed. MD . Meta = { } # Mark HTML with Markup to prevent jinja2 autoescaping output = { 'description' : Markup ( MD . convert ( text ) ) } try : meta = MD . Meta . copy ( ) except AttributeError : pass else : output [ 'meta' ] = meta try : output [ 'title' ] = MD . Meta [ 'title' ] [ 0 ] except KeyError : pass return output
7953	def handle_write ( self ) : with self . lock : logger . debug ( "handle_write: queue: {0!r}" . format ( self . _write_queue ) ) try : job = self . _write_queue . popleft ( ) except IndexError : return if isinstance ( job , WriteData ) : self . _do_write ( job . data ) # pylint: disable=E1101 elif isinstance ( job , ContinueConnect ) : self . _continue_connect ( ) elif isinstance ( job , StartTLS ) : self . _initiate_starttls ( * * job . kwargs ) elif isinstance ( job , TLSHandshake ) : self . _continue_tls_handshake ( ) else : raise ValueError ( "Unrecognized job in the write queue: " "{0!r}" . format ( job ) )
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) # 1. Setup the forum. self . forum . on_start_game ( ) # 2. Setup the world. with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) # 3. Setup the actors. Because this is done after the forum and the # world have been setup, this signals to the actors that they can # send messages and query the game world as usual. num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
11104	def acquire_lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with self . locker as r : # get the result acquired , code , _ = r if acquired : try : r = func ( self , * args , * * kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . __name__ ) ) e = None r = None # raise error after exiting with statement and releasing the lock! if e is not None : traceback . print_stack ( ) raise Exception ( e ) return r return wrapper
9765	def check ( file , # pylint:disable=redefined-builtin version , definition ) : file = file or 'polyaxonfile.yaml' specification = check_polyaxonfile ( file ) . specification if version : Printer . decorate_format_value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job_condition = ( specification . is_job or specification . is_build or specification . is_notebook or specification . is_tensorboard ) if specification . is_experiment : Printer . decorate_format_value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job_condition : Printer . decorate_format_value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is_group : experiments_def = specification . experiments_def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get_group_experiments_info ( * * experiments_def ) return specification
2788	def resize ( self , size_gigabytes , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "resize" , "size_gigabytes" : size_gigabytes , "region" : region } )
10992	def _calc_ilm_order ( imshape ) : zorder = int ( imshape [ 0 ] / 6.25 ) + 1 l_npts = int ( imshape [ 1 ] / 42.5 ) + 1 npts = ( ) for a in range ( l_npts ) : if a < 5 : npts += ( int ( imshape [ 2 ] * [ 59 , 39 , 29 , 19 , 14 ] [ a ] / 512. ) + 1 , ) else : npts += ( int ( imshape [ 2 ] * 11 / 512. ) + 1 , ) return npts , zorder
1647	def CheckCheck ( filename , clean_lines , linenum , error ) : # Decide the set of replacement macros that should be suggested lines = clean_lines . elided ( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) if not check_macro : return # Find end of the boolean expression by matching parentheses ( last_line , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , start_pos ) if end_pos < 0 : return # If the check macro is followed by something other than a # semicolon, assume users will log their own custom error messages # and don't suggest any replacements. if not Match ( r'\s*;' , last_line [ end_pos : ] ) : return if linenum == end_line : expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] else : expression = lines [ linenum ] [ start_pos + 1 : ] for i in xrange ( linenum + 1 , end_line ) : expression += lines [ i ] expression += last_line [ 0 : end_pos - 1 ] # Parse expression so that we can take parentheses into account. # This avoids false positives for inputs like "CHECK((a < 4) == b)", # which is not replaceable by CHECK_LE. lhs = '' rhs = '' operator = None while expression : matched = Match ( r'^\s*(<<|<<=|>>|>>=|->\*|->|&&|\|\||' r'==|!=|>=|>|<=|<|\()(.*)$' , expression ) if matched : token = matched . group ( 1 ) if token == '(' : # Parenthesized operand expression = matched . group ( 2 ) ( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) if end < 0 : return # Unmatched parenthesis lhs += '(' + expression [ 0 : end ] expression = expression [ end : ] elif token in ( '&&' , '||' ) : # Logical and/or operators. This means the expression # contains more than one term, for example: # CHECK(42 < a && a < b); # # These are not replaceable with CHECK_LE, so bail out early. return elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : # Non-relational operator lhs += token expression = matched . group ( 2 ) else : # Relational operator operator = token rhs = matched . group ( 2 ) break else : # Unparenthesized operand. Instead of appending to lhs one character # at a time, we do another regular expression match to consume several # characters at once if possible. Trivial benchmark shows that this # is more efficient when the operands are longer than a single # character, which is generally the case. matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) if not matched : matched = Match ( r'^(\s*\S)(.*)$' , expression ) if not matched : break lhs += matched . group ( 1 ) expression = matched . group ( 2 ) # Only apply checks if we got all parts of the boolean expression if not ( lhs and operator and rhs ) : return # Check that rhs do not contain logical operators. We already know # that lhs is fine since the loop above parses out && and ||. if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : return # At least one of the operands must be a constant literal. This is # to avoid suggesting replacements for unprintable things like # CHECK(variable != iterator) # # The following pattern matches decimal, hex integers, strings, and # characters (in that order). lhs = lhs . strip ( ) rhs = rhs . strip ( ) match_constant = r'^([-+]?(\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|".*"|\'.*\')$' if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : # Note: since we know both lhs and rhs, we can provide a more # descriptive error message like: # Consider using CHECK_EQ(x, 42) instead of CHECK(x == 42) # Instead of: # Consider using CHECK_EQ instead of CHECK(a == b) # # We are still keeping the less descriptive message because if lhs # or rhs gets long, the error message might become unreadable. error ( filename , linenum , 'readability/check' , 2 , 'Consider using %s instead of %s(a %s b)' % ( _CHECK_REPLACEMENT [ check_macro ] [ operator ] , check_macro , operator ) )
1247	def recv ( self , socket_ , encoding = None ) : unpacker = msgpack . Unpacker ( encoding = encoding ) # Wait for an immediate response. response = socket_ . recv ( 8 ) # get the length of the message if response == b"" : raise TensorForceError ( "No data received by socket.recv in call to method `recv` " + "(listener possibly closed)!" ) orig_len = int ( response ) received_len = 0 while True : data = socket_ . recv ( min ( orig_len - received_len , self . max_msg_len ) ) # There must be a response. if not data : raise TensorForceError ( "No data of len {} received by socket.recv in call to method `recv`!" . format ( orig_len - received_len ) ) data_len = len ( data ) received_len += data_len unpacker . feed ( data ) if received_len == orig_len : break # Get the data. for message in unpacker : sts = message . get ( "status" , message . get ( b"status" ) ) if sts : if sts == "ok" or sts == b"ok" : return message else : raise TensorForceError ( "RemoteEnvironment server error: {}" . format ( message . get ( "message" , "not specified" ) ) ) else : raise TensorForceError ( "Message without field 'status' received!" ) raise TensorForceError ( "No message encoded in data stream (data stream had len={})" . format ( orig_len ) )
11210	def _set_tzdata ( self , tzobj ) : # Copy the relevant attributes over as private attributes for attr in _tzfile . attrs : setattr ( self , '_' + attr , getattr ( tzobj , attr ) )
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
5860	def default ( self , obj ) : if obj is None : return [ ] elif isinstance ( obj , list ) : return [ i . as_dictionary ( ) for i in obj ] elif isinstance ( obj , dict ) : return self . _keys_to_camel_case ( obj ) else : return obj . as_dictionary ( )
11230	def xafter ( self , dt , count = None , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self # Select the comparison function if inc : def comp ( dc , dtc ) : return dc >= dtc else : def comp ( dc , dtc ) : return dc > dtc # Generate dates n = 0 for d in gen : if comp ( d , dt ) : if count is not None : n += 1 if n > count : break yield d
6961	def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 ] , int ) : # python3 byte strings for x , y in zip ( a , b ) : result |= x ^ y else : # python2 for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0
11891	def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
5707	def get_lockdown_form ( form_path ) : if not form_path : raise ImproperlyConfigured ( 'No LOCKDOWN_FORM specified.' ) form_path_list = form_path . split ( "." ) new_module = "." . join ( form_path_list [ : - 1 ] ) attr = form_path_list [ - 1 ] try : mod = import_module ( new_module ) except ( ImportError , ValueError ) : raise ImproperlyConfigured ( 'Module configured in LOCKDOWN_FORM (%s) to' ' contain the form class couldn\'t be ' 'found.' % new_module ) try : form = getattr ( mod , attr ) except AttributeError : raise ImproperlyConfigured ( 'The module configured in LOCKDOWN_FORM ' ' (%s) doesn\'t define a "%s" form.' % ( new_module , attr ) ) return form
4101	def mdl_eigen ( s , N ) : import numpy as np kmdl = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kmdl . append ( - ( n - k ) * N * np . log ( gk / ak ) + 0.5 * k * ( 2. * n - k ) * np . log ( N ) ) return kmdl
7925	def is_ipv4_available ( ) : try : socket . socket ( socket . AF_INET ) . close ( ) except socket . error : return False return True
4121	def twosided_2_centerdc ( data ) : N = len ( data ) # could us int() or // in python 3 newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
13072	def r_passage ( self , objectId , subreference , lang = None ) : collection = self . get_collection ( objectId ) if isinstance ( collection , CtsWorkMetadata ) : editions = [ t for t in collection . children . values ( ) if isinstance ( t , CtsEditionMetadata ) ] if len ( editions ) == 0 : raise UnknownCollection ( "This work has no default edition" ) return redirect ( url_for ( ".r_passage" , objectId = str ( editions [ 0 ] . id ) , subreference = subreference ) ) text = self . get_passage ( objectId = objectId , subreference = subreference ) passage = self . transform ( text , text . export ( Mimetypes . PYTHON . ETREE ) , objectId ) prev , next = self . get_siblings ( objectId , subreference , text ) return { "template" : "main::text.html" , "objectId" : objectId , "subreference" : subreference , "collections" : { "current" : { "label" : collection . get_label ( lang ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , "author" : text . get_creator ( lang ) , "title" : text . get_title ( lang ) , "description" : text . get_description ( lang ) , "citation" : collection . citation , "coins" : self . make_coins ( collection , text , subreference , lang = lang ) } , "parents" : self . make_parents ( collection , lang = lang ) } , "text_passage" : Markup ( passage ) , "prev" : prev , "next" : next }
12595	def get_aad_token ( endpoint , no_verify ) : #pylint: disable-msg=too-many-locals from azure . servicefabric . service_fabric_client_ap_is import ( ServiceFabricClientAPIs ) from sfctl . auth import ClientCertAuthentication from sfctl . config import set_aad_metadata auth = ClientCertAuthentication ( None , None , no_verify ) client = ServiceFabricClientAPIs ( auth , base_url = endpoint ) aad_metadata = client . get_aad_metadata ( ) if aad_metadata . type != "aad" : raise CLIError ( "Not AAD cluster" ) aad_resource = aad_metadata . metadata tenant_id = aad_resource . tenant authority_uri = aad_resource . login + '/' + tenant_id context = adal . AuthenticationContext ( authority_uri , api_version = None ) cluster_id = aad_resource . cluster client_id = aad_resource . client set_aad_metadata ( authority_uri , cluster_id , client_id ) code = context . acquire_user_code ( cluster_id , client_id ) print ( code [ 'message' ] ) token = context . acquire_token_with_device_code ( cluster_id , code , client_id ) print ( "Succeed!" ) return token , context . cache
13570	def false_exit ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : ret = func ( * args , * * kwargs ) if ret is False : if "TMC_TESTING" in os . environ : raise TMCExit ( ) else : sys . exit ( - 1 ) return ret return inner
13388	def ttl ( self , response ) : if response . code != 200 : return 0 if not self . request . method in [ 'GET' , 'HEAD' , 'OPTIONS' ] : return 0 try : pragma = self . request . headers [ 'pragma' ] if pragma == 'no-cache' : return 0 except KeyError : pass try : cache_control = self . request . headers [ 'cache-control' ] # no caching options for option in [ 'private' , 'no-cache' , 'no-store' , 'must-revalidate' , 'proxy-revalidate' ] : if cache_control . find ( option ) : return 0 # further parsing to get a ttl options = parse_cache_control ( cache_control ) try : return int ( options [ 's-maxage' ] ) except KeyError : pass try : return int ( options [ 'max-age' ] ) except KeyError : pass if 's-maxage' in options : max_age = options [ 's-maxage' ] if max_age < ttl : ttl = max_age if 'max-age' in options : max_age = options [ 'max-age' ] if max_age < ttl : ttl = max_age return ttl except KeyError : pass try : expires = self . request . headers [ 'expires' ] return time . mktime ( time . strptime ( expires , '%a, %d %b %Y %H:%M:%S' ) ) - time . time ( ) except KeyError : pass
888	def _destroyMinPermanenceSynapses ( cls , connections , random , segment , nDestroy , excludeCells ) : destroyCandidates = sorted ( ( synapse for synapse in connections . synapsesForSegment ( segment ) if synapse . presynapticCell not in excludeCells ) , key = lambda s : s . _ordinal ) for _ in xrange ( nDestroy ) : if len ( destroyCandidates ) == 0 : break minSynapse = None minPermanence = float ( "inf" ) for synapse in destroyCandidates : if synapse . permanence < minPermanence - EPSILON : minSynapse = synapse minPermanence = synapse . permanence connections . destroySynapse ( minSynapse ) destroyCandidates . remove ( minSynapse )
8361	def create_rcontext ( self , size , frame ) : self . frame = frame width , height = size meta_surface = cairo . RecordingSurface ( cairo . CONTENT_COLOR_ALPHA , ( 0 , 0 , width , height ) ) ctx = cairo . Context ( meta_surface ) return ctx
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
5670	def temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = None ) : events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) events_df . drop ( 'to_seq' , 1 , inplace = True ) events_df . drop ( 'shape_id' , 1 , inplace = True ) events_df . drop ( 'duration' , 1 , inplace = True ) events_df . drop ( 'route_id' , 1 , inplace = True ) events_df . rename ( columns = { 'from_seq' : "seq" } , inplace = True ) return events_df
5336	def __kibiter_version ( self ) : version = None es_url = self . conf [ 'es_enrichment' ] [ 'url' ] config_url = '.kibana/config/_search' url = urijoin ( es_url , config_url ) version = None try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) version = res . json ( ) [ 'hits' ] [ 'hits' ] [ 0 ] [ '_id' ] logger . debug ( "Kibiter version: %s" , version ) except requests . exceptions . HTTPError : logger . warning ( "Can not find Kibiter version" ) return version
598	def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( "Temporal memory has not been initialized" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )
13773	def init_logs ( path = None , target = None , logger_name = 'root' , level = logging . DEBUG , maxBytes = 1 * 1024 * 1024 , backupCount = 5 , application_name = 'default' , server_hostname = None , fields = None ) : log_file = os . path . abspath ( os . path . join ( path , target ) ) logger = logging . getLogger ( logger_name ) logger . setLevel ( level ) handler = logging . handlers . RotatingFileHandler ( log_file , maxBytes = maxBytes , backupCount = backupCount ) handler . setLevel ( level ) handler . setFormatter ( JsonFormatter ( application_name = application_name , server_hostname = server_hostname , fields = fields ) ) logger . addHandler ( handler )
12295	def annotate_metadata_dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent_repos = options [ 'dependencies' ] for d in dependent_repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) # documentation in # https://cloud.google.com/storage/docs/json_api/v1/objects/list request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
13544	def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
12201	def from_yamlfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . from_yamlstring ( fp . read ( ) , selector_handler = selector_handler , strict = strict , debug = debug )
7730	def clear_muc_child ( self ) : if self . muc_child : self . muc_child . free_borrowed ( ) self . muc_child = None if not self . xmlnode . children : return n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ns_uri in ( MUC_NS , MUC_USER_NS , MUC_ADMIN_NS , MUC_OWNER_NS ) : n . unlinkNode ( ) n . freeNode ( ) n = n . next
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) # now that we have all the files, concatenate them # a single file will be returned as normalized if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
8881	def fit ( self , X , y = None ) : # Check that X have correct shape X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
3105	def code_verifier ( n_bytes = 64 ) : verifier = base64 . urlsafe_b64encode ( os . urandom ( n_bytes ) ) . rstrip ( b'=' ) # https://tools.ietf.org/html/rfc7636#section-4.1 # minimum length of 43 characters and a maximum length of 128 characters. if len ( verifier ) < 43 : raise ValueError ( "Verifier too short. n_bytes must be > 30." ) elif len ( verifier ) > 128 : raise ValueError ( "Verifier too long. n_bytes must be < 97." ) else : return verifier
11502	def folder_get ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.get' , parameters ) return response
7054	def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys . path . append ( os . path . dirname ( module ) ) importedok = importlib . import_module ( os . path . basename ( module . replace ( '.py' , '' ) ) ) else : importedok = importlib . import_module ( module ) except Exception as e : LOGEXCEPTION ( 'could not import the module: %s for LC format: %s. ' 'check the file path or fully qualified module name?' % ( module , formatkey ) ) importedok = False return importedok
3119	def get_prep_value ( self , value ) : if value is None : return None else : return encoding . smart_text ( base64 . b64encode ( jsonpickle . encode ( value ) . encode ( ) ) )
8547	def get_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) ) return response
13243	def weekdays ( self ) : if not self . root . xpath ( 'days' ) : return set ( range ( 7 ) ) return set ( int ( d ) - 1 for d in self . root . xpath ( 'days/day/text()' ) )
295	def plot_max_median_position_concentration ( positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) alloc_summary = pos . get_max_median_position_concentration ( positions ) colors = [ 'mediumblue' , 'steelblue' , 'tomato' , 'firebrick' ] alloc_summary . plot ( linewidth = 1 , color = colors , alpha = 0.6 , ax = ax ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Exposure' ) ax . set_title ( 'Long/short max and median position concentration' ) return ax
5441	def rewrite_uris ( self , raw_uri , file_provider ) : if file_provider == job_model . P_GCS : normalized , docker_path = _gcs_uri_rewriter ( raw_uri ) elif file_provider == job_model . P_LOCAL : normalized , docker_path = _local_uri_rewriter ( raw_uri ) else : raise ValueError ( 'File provider not supported: %r' % file_provider ) return normalized , os . path . join ( self . _relative_path , docker_path )
7123	def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = DetectMissingEncoder , separators = ( ',' , ': ' ) )
11335	def prompt ( question , choices = None ) : if not re . match ( "\s$" , question ) : question = "{}: " . format ( question ) while True : if sys . version_info [ 0 ] > 2 : answer = input ( question ) else : answer = raw_input ( question ) if not choices or answer in choices : break return answer
8791	def has_tag ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : return True return False
59	def intersection ( self , other , default = None ) : x1_i = max ( self . x1 , other . x1 ) y1_i = max ( self . y1 , other . y1 ) x2_i = min ( self . x2 , other . x2 ) y2_i = min ( self . y2 , other . y2 ) if x1_i > x2_i or y1_i > y2_i : return default else : return BoundingBox ( x1 = x1_i , y1 = y1_i , x2 = x2_i , y2 = y2_i )
7941	def _resolve_hostname ( self ) : self . _set_state ( "resolving-hostname" ) resolver = self . settings [ "dns_resolver" ] # pylint: disable=W0621 logger . debug ( "_dst_nameports: {0!r}" . format ( self . _dst_nameports ) ) name , port = self . _dst_nameports . pop ( 0 ) self . _dst_hostname = name resolver . resolve_address ( name , callback = partial ( self . _got_addresses , name , port ) , allow_cname = self . _dst_service is None ) self . event ( ResolvingAddressEvent ( name ) )
13413	def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
6207	def merge_da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts_d , ts_par_d = self . S . get_timestamps_part ( self . name_timestamps_d ) ts_a , ts_par_a = self . S . get_timestamps_part ( self . name_timestamps_a ) ts , a_ch , part = merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) assert a_ch . sum ( ) == ts_a . shape [ 0 ] assert ( ~ a_ch ) . sum ( ) == ts_d . shape [ 0 ] assert a_ch . size == ts_a . shape [ 0 ] + ts_d . shape [ 0 ] self . ts , self . a_ch , self . part = ts , a_ch , part self . clk_p = ts_d . attrs [ 'clk_p' ]
4034	def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
7805	def verify_jid_against_common_name ( self , jid ) : if not self . common_names : return False for name in self . common_names : try : cn_jid = JID ( name ) except ValueError : continue if jid == cn_jid : return True return False
3710	def calculate_P ( self , T , P , method ) : if method == COSTALD_COMPRESSED : Vm = self . T_dependent_property ( T ) Psat = self . Psat ( T ) if hasattr ( self . Psat , '__call__' ) else self . Psat Vm = COSTALD_compressed ( T , P , Psat , self . Tc , self . Pc , self . omega , Vm ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_l elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
274	def get_symbol_rets ( symbol , start = None , end = None ) : return SETTINGS [ 'returns_func' ] ( symbol , start = start , end = end )
7572	def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : ## try refreshing taker, else quit try : taker = itertools . takewhile ( lambda x : x [ 0 ] != "//\n" , pairdealer ) oneclust = [ "" . join ( taker . next ( ) ) ] except StopIteration : #LOGGER.debug('last chunk %s', chunk) return 1 , chunk ## load one cluster while 1 : try : oneclust . append ( "" . join ( taker . next ( ) ) ) except StopIteration : break chunk . append ( "" . join ( oneclust ) ) ccnt += 1 return 0 , chunk
11074	def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
7242	def pxbounds ( self , geom , clip = False ) : try : if isinstance ( geom , dict ) : if 'geometry' in geom : geom = shape ( geom [ 'geometry' ] ) else : geom = shape ( geom ) elif isinstance ( geom , BaseGeometry ) : geom = shape ( geom ) else : geom = wkt . loads ( geom ) except : raise TypeError ( "Invalid geometry object" ) # if geometry doesn't overlap the image, return an error if geom . disjoint ( shape ( self ) ) : raise ValueError ( "Geometry outside of image bounds" ) # clip to pixels within the image ( xmin , ymin , xmax , ymax ) = ops . transform ( self . __geo_transform__ . rev , geom ) . bounds _nbands , ysize , xsize = self . shape if clip : xmin = max ( xmin , 0 ) ymin = max ( ymin , 0 ) xmax = min ( xmax , xsize ) ymax = min ( ymax , ysize ) return ( xmin , ymin , xmax , ymax )
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : # pragma: no cover raise NotImplementedError ( 'Format not implemented: %s' % mode )
6022	def from_fits_with_scale ( cls , file_path , hdu , pixel_scale ) : return cls ( array = array_util . numpy_array_2d_from_fits ( file_path , hdu ) , pixel_scale = pixel_scale )
6402	def fingerprint ( self , phrase , joiner = ' ' ) : phrase = unicode_normalize ( 'NFKD' , text_type ( phrase . strip ( ) . lower ( ) ) ) phrase = '' . join ( [ c for c in phrase if c . isalnum ( ) or c . isspace ( ) ] ) phrase = joiner . join ( sorted ( list ( set ( phrase . split ( ) ) ) ) ) return phrase
8818	def get_networks ( context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , filters = None , fields = None ) : LOG . info ( "get_networks for tenant %s with filters %s, fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } nets = db_api . network_find ( context , limit , sorts , marker , page_reverse , join_subnets = True , * * filters ) or [ ] nets = [ v . _make_network_dict ( net , fields = fields ) for net in nets ] return nets
4898	def _remove_failed_items ( self , failed_items , items_to_create , items_to_update , items_to_delete ) : for item in failed_items : content_metadata_id = item [ 'courseID' ] items_to_create . pop ( content_metadata_id , None ) items_to_update . pop ( content_metadata_id , None ) items_to_delete . pop ( content_metadata_id , None )
7224	def save ( self , project ) : # test if this is a create vs. an update if 'id' in project and project [ 'id' ] is not None : # update -> use put op self . logger . debug ( 'Updating existing project: ' + json . dumps ( project ) ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project [ 'id' ] } r = self . gbdx_connection . put ( url , json = project ) try : r . raise_for_status ( ) except : print ( r . text ) raise # updates only get the Accepted response -> return the original project id return project [ 'id' ] else : self . logger . debug ( 'Creating new project: ' + json . dumps ( project ) ) # create -> use post op url = self . base_url r = self . gbdx_connection . post ( url , json = project ) try : r . raise_for_status ( ) except : print ( r . text ) raise project_json = r . json ( ) # create returns the saved project -> return the project id that's saved return project_json [ 'id' ]
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
3071	def _get_flow_for_token ( csrf_token ) : flow_pickle = session . pop ( _FLOW_KEY . format ( csrf_token ) , None ) if flow_pickle is None : return None else : return pickle . loads ( flow_pickle )
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( * * params )
9926	def handle ( self , * args , * * kwargs ) : cutoff = timezone . now ( ) cutoff -= app_settings . CONFIRMATION_EXPIRATION cutoff -= app_settings . CONFIRMATION_SAVE_PERIOD queryset = models . EmailConfirmation . objects . filter ( created_at__lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( "Removed {count} old email confirmation(s)" . format ( count = count ) ) ) else : self . stdout . write ( "No email confirmations to remove." )
12981	def string ( html , start_on = None , ignore = ( ) , use_short = True , * * queries ) : if use_short : html = grow_short ( html ) return _to_template ( fromstring ( html ) , start_on = start_on , ignore = ignore , * * queries )
5678	def get_stop_count_data ( self , start_ut , end_ut ) : # TODO! this function could perhaps be made a single sql query now with the new tables? trips_df = self . get_tripIs_active_in_range ( start_ut , end_ut ) # stop_I -> count, lat, lon, name stop_counts = Counter ( ) # loop over all trips: for row in trips_df . itertuples ( ) : # get stop_data and store it: stops_seq = self . get_trip_stop_time_data ( row . trip_I , row . day_start_ut ) for stop_time_row in stops_seq . itertuples ( index = False ) : if ( stop_time_row . dep_time_ut >= start_ut ) and ( stop_time_row . dep_time_ut <= end_ut ) : stop_counts [ stop_time_row . stop_I ] += 1 all_stop_data = self . stops ( ) counts = [ stop_counts [ stop_I ] for stop_I in all_stop_data [ "stop_I" ] . values ] all_stop_data . loc [ : , "count" ] = pd . Series ( counts , index = all_stop_data . index ) return all_stop_data
11203	def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6), # Because 7 % 7 = 0 weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
5037	def _handle_bulk_upload ( cls , enterprise_customer , manage_learners_form , request , email_list = None ) : errors = [ ] emails = set ( ) already_linked_emails = [ ] duplicate_emails = [ ] csv_file = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . BULK_UPLOAD ] if email_list : parsed_csv = [ { ManageLearnersForm . CsvColumns . EMAIL : email } for email in email_list ] else : parsed_csv = parse_csv ( csv_file , expected_columns = { ManageLearnersForm . CsvColumns . EMAIL } ) try : for index , row in enumerate ( parsed_csv ) : email = row [ ManageLearnersForm . CsvColumns . EMAIL ] try : already_linked = validate_email_to_link ( email , ignore_existing = True ) except ValidationError as exc : message = _ ( "Error at line {line}: {message}\n" ) . format ( line = index + 1 , message = exc ) errors . append ( message ) else : if already_linked : already_linked_emails . append ( ( email , already_linked . enterprise_customer ) ) elif email in emails : duplicate_emails . append ( email ) else : emails . add ( email ) except ValidationError as exc : errors . append ( exc ) if errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . GENERAL_ERRORS , ValidationMessages . BULK_LINK_FAILED ) for error in errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . BULK_UPLOAD , error ) return # There were no errors. Now do the actual linking: for email in emails : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) # Report what happened: count = len ( emails ) messages . success ( request , ungettext ( "{count} new learner was added to {enterprise_customer_name}." , "{count} new learners were added to {enterprise_customer_name}." , count ) . format ( count = count , enterprise_customer_name = enterprise_customer . name ) ) this_customer_linked_emails = [ email for email , customer in already_linked_emails if customer == enterprise_customer ] other_customer_linked_emails = [ email for email , __ in already_linked_emails if email not in this_customer_linked_emails ] if this_customer_linked_emails : messages . warning ( request , _ ( "The following learners were already associated with this Enterprise " "Customer: {list_of_emails}" ) . format ( list_of_emails = ", " . join ( this_customer_linked_emails ) ) ) if other_customer_linked_emails : messages . warning ( request , _ ( "The following learners are already associated with " "another Enterprise Customer. These learners were not " "added to {enterprise_customer_name}: {list_of_emails}" ) . format ( enterprise_customer_name = enterprise_customer . name , list_of_emails = ", " . join ( other_customer_linked_emails ) , ) ) if duplicate_emails : messages . warning ( request , _ ( "The following duplicate email addresses were not added: " "{list_of_emails}" ) . format ( list_of_emails = ", " . join ( duplicate_emails ) ) ) # Build a list of all the emails that we can act on further; that is, # emails that we either linked to this customer, or that were linked already. all_processable_emails = list ( emails ) + this_customer_linked_emails return all_processable_emails
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
180	def to_bounding_box ( self ) : from . bbs import BoundingBox # we don't have to mind the case of len(.) == 1 here, because # zero-sized BBs are considered valid if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )
8431	def desaturate_pal ( color , prop , reverse = False ) : if not 0 <= prop <= 1 : raise ValueError ( "prop must be between 0 and 1" ) # Get rgb tuple rep # Convert to hls # Desaturate the saturation channel # Convert back to rgb rgb = mcolors . colorConverter . to_rgb ( color ) h , l , s = colorsys . rgb_to_hls ( * rgb ) s *= prop desaturated_color = colorsys . hls_to_rgb ( h , l , s ) colors = [ color , desaturated_color ] if reverse : colors = colors [ : : - 1 ] return gradient_n_pal ( colors , name = 'desaturated' )
5324	def read_file_from_uri ( self , uri ) : logger . debug ( "Reading %s" % ( uri ) ) self . __check_looks_like_uri ( uri ) try : req = urllib . request . Request ( uri ) req . add_header ( 'Authorization' , 'token %s' % self . token ) r = urllib . request . urlopen ( req ) except urllib . error . HTTPError as err : if err . code == 404 : raise GithubFileNotFound ( 'File %s is not available. Check the URL to ensure it really exists' % uri ) else : raise return r . read ( ) . decode ( "utf-8" )
6473	def human ( self , size , base = 1000 , units = ' kMGTZ' ) : sign = '+' if size >= 0 else '-' size = abs ( size ) if size < 1000 : return '%s%d' % ( sign , size ) for i , suffix in enumerate ( units ) : unit = 1000 ** ( i + 1 ) if size < unit : return ( '%s%.01f%s' % ( sign , size / float ( unit ) * base , suffix , ) ) . strip ( ) raise OverflowError
2821	def convert_dropout ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting dropout ...' ) if names == 'short' : tf_name = 'DO' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) dropout = keras . layers . Dropout ( rate = params [ 'ratio' ] , name = tf_name ) layers [ scope_name ] = dropout ( layers [ inputs [ 0 ] ] )
2932	def package_for_editor_signavio ( self , spec , filename ) : signavio_file = filename [ : - len ( '.bpmn20.xml' ) ] + '.signavio.xml' if os . path . exists ( signavio_file ) : self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( signavio_file ) , signavio_file ) f = open ( signavio_file , 'r' ) try : signavio_tree = ET . parse ( f ) finally : f . close ( ) svg_node = one ( signavio_tree . findall ( './/svg-representation' ) ) self . write_to_package_zip ( "%s.svg" % spec . name , svg_node . text )
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
12227	def bind_proxy ( values , category = None , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : addrs = OrderedDict ( ) depth = 3 for local_name , locals_dict in traverse_local_prefs ( depth ) : addrs [ id ( locals_dict [ local_name ] ) ] = local_name proxies = [ ] locals_dict = get_frame_locals ( depth ) for value in values : # Try to preserve fields order. id_val = id ( value ) if id_val in addrs : local_name = addrs [ id_val ] local_val = locals_dict [ local_name ] if isinstance ( local_val , PatchedLocal ) and not isinstance ( local_val , PrefProxy ) : proxy = PrefProxy ( local_name , value . val , category = category , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) app_name = locals_dict [ '__name__' ] . split ( '.' ) [ - 2 ] # x.y.settings -> y prefs = get_prefs ( ) if app_name not in prefs : prefs [ app_name ] = OrderedDict ( ) prefs [ app_name ] [ local_name . lower ( ) ] = proxy # Replace original pref variable with a proxy. locals_dict [ local_name ] = proxy proxies . append ( proxy ) return proxies
8247	def str_to_rgb ( self , str ) : str = str . lower ( ) for ch in "_- " : str = str . replace ( ch , "" ) # if named_hues.has_key(str): # clr = color(named_hues[str], 1, 1, mode="hsb") # return clr.r, clr.g, clr.b if named_colors . has_key ( str ) : return named_colors [ str ] for suffix in [ "ish" , "ed" , "y" , "like" ] : str = re . sub ( "(.*?)" + suffix + "$" , "\\1" , str ) str = re . sub ( "(.*?)dd$" , "\\1d" , str ) matches = [ ] for name in named_colors : if name in str or str in name : matches . append ( named_colors [ name ] ) if len ( matches ) > 0 : return choice ( matches ) return named_colors [ "transparent" ]
13228	def get_installation_token ( installation_id , integration_jwt ) : api_root = 'https://api.github.com' url = '{root}/installations/{id_:d}/access_tokens' . format ( api_root = api_root , id_ = installation_id ) headers = { 'Authorization' : 'Bearer {0}' . format ( integration_jwt . decode ( 'utf-8' ) ) , 'Accept' : 'application/vnd.github.machine-man-preview+json' } resp = requests . post ( url , headers = headers ) resp . raise_for_status ( ) return resp . json ( )
12741	def _parse_persons ( self , datafield , subfield , roles = [ "aut" ] ) : # parse authors parsed_persons = [ ] raw_persons = self . get_subfields ( datafield , subfield ) for person in raw_persons : # check if person have at least one of the roles specified in # 'roles' parameter of function other_subfields = person . other_subfields if "4" in other_subfields and roles != [ "any" ] : person_roles = other_subfields [ "4" ] # list of role parameters relevant = any ( map ( lambda role : role in roles , person_roles ) ) # skip non-relevant persons if not relevant : continue # result of .strip() is string, so ind1/2 in MARCSubrecord are lost ind1 = person . i1 ind2 = person . i2 person = person . strip ( ) name = "" second_name = "" surname = "" title = "" # here it gets nasty - there is lot of options in ind1/ind2 # parameters if ind1 == "1" and ind2 == " " : if "," in person : surname , name = person . split ( "," , 1 ) elif " " in person : surname , name = person . split ( " " , 1 ) else : surname = person if "c" in other_subfields : title = "," . join ( other_subfields [ "c" ] ) elif ind1 == "0" and ind2 == " " : name = person . strip ( ) if "b" in other_subfields : second_name = "," . join ( other_subfields [ "b" ] ) if "c" in other_subfields : surname = "," . join ( other_subfields [ "c" ] ) elif ind1 == "1" and ind2 == "0" or ind1 == "0" and ind2 == "0" : name = person . strip ( ) if "c" in other_subfields : title = "," . join ( other_subfields [ "c" ] ) parsed_persons . append ( Person ( name . strip ( ) , second_name . strip ( ) , surname . strip ( ) , title . strip ( ) ) ) return parsed_persons
12509	def get_img_info ( image ) : try : img = check_img ( image ) except Exception as exc : raise Exception ( 'Error reading file {0}.' . format ( repr_imgs ( image ) ) ) from exc else : return img . get_header ( ) , img . get_affine ( )
5223	def ccy_pair ( local , base = 'USD' ) -> CurrencyPair : ccy_param = param . load_info ( cat = 'ccy' ) if f'{local}{base}' in ccy_param : info = ccy_param [ f'{local}{base}' ] elif f'{base}{local}' in ccy_param : info = ccy_param [ f'{base}{local}' ] info [ 'factor' ] = 1. / info . get ( 'factor' , 1. ) info [ 'power' ] = - info . get ( 'power' , 1 ) elif base . lower ( ) == local . lower ( ) : info = dict ( ticker = '' ) info [ 'factor' ] = 1. if base [ - 1 ] . lower ( ) == base [ - 1 ] : info [ 'factor' ] /= 100. if local [ - 1 ] . lower ( ) == local [ - 1 ] : info [ 'factor' ] *= 100. else : logger = logs . get_logger ( ccy_pair ) logger . error ( f'incorrect currency - local {local} / base {base}' ) return CurrencyPair ( ticker = '' , factor = 1. , power = 1 ) if 'factor' not in info : info [ 'factor' ] = 1. if 'power' not in info : info [ 'power' ] = 1 return CurrencyPair ( * * info )
96	def quokka_keypoints ( size = None , extract = None ) : # TODO get rid of this deferred import from imgaug . augmentables . kps import Keypoint , KeypointsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) keypoints = [ ] for kp_dict in json_dict [ "keypoints" ] : keypoints . append ( Keypoint ( x = kp_dict [ "x" ] - left , y = kp_dict [ "y" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) kpsoi = KeypointsOnImage ( keypoints , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) kpsoi = kpsoi . on ( shape_resized ) return kpsoi
5288	def forms_valid ( self , form , inlines ) : response = self . form_valid ( form ) for formset in inlines : formset . save ( ) return response
1017	def _adaptSegment ( self , segUpdate ) : # This will be set to True if detect that any syapses were decremented to # 0 trimSegment = False # segUpdate.segment is None when creating a new segment c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment # update.activeSynapses can be empty. # If not, it can contain either or both integers and tuples. # The integers are indices of synapses to update. # The tuples represent new synapses to create (src col, src cell in col). # We pre-process to separate these various element types. # synToCreate is not empty only if positiveReinforcement is True. # NOTE: the synapse indices start at *1* to skip the segment flags. activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) # Modify an existing segment if segment is not None : if self . verbosity >= 4 : print "Reinforcing segment #%d for cell[%d,%d]" % ( segment . segID , c , i ) print " before:" , segment . debugPrint ( ) # Mark it as recently useful segment . lastActiveIteration = self . lrnIterationIdx # Update frequency and positiveActivations segment . positiveActivations += 1 # positiveActivations += 1 segment . dutyCycle ( active = True ) # First, decrement synapses that are not active # s is a synapse *index*, with index 0 in the segment being the tuple # (segId, sequence segment flag). See below, creation of segments. lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) # Now, increment active synapses activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) # Finally, create new synapses if needed # syn is now a tuple (src col, src cell) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] # If we have fixed resources, get rid of some old syns if necessary if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print " after:" , segment . debugPrint ( ) # Create a new segment else : # (segID, sequenceSegment flag, frequency, positiveActivations, # totalActivations, lastActiveIteration) newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) # numpy.float32 important so that we can match with C++ for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print "New segment #%d for cell[%d,%d]" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment
9870	def build_environ ( self , sock_file , conn ) : # Grab the request line request = self . read_request_line ( sock_file ) # Copy the Base Environment environ = self . base_environ . copy ( ) # Grab the headers for k , v in self . read_headers ( sock_file ) . items ( ) : environ [ str ( 'HTTP_' + k ) ] = v # Add CGI Variables environ [ 'REQUEST_METHOD' ] = request [ 'method' ] environ [ 'PATH_INFO' ] = request [ 'path' ] environ [ 'SERVER_PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER_PORT' ] = str ( conn . server_port ) environ [ 'REMOTE_PORT' ] = str ( conn . client_port ) environ [ 'REMOTE_ADDR' ] = str ( conn . client_addr ) environ [ 'QUERY_STRING' ] = request [ 'query_string' ] if 'HTTP_CONTENT_LENGTH' in environ : environ [ 'CONTENT_LENGTH' ] = environ [ 'HTTP_CONTENT_LENGTH' ] if 'HTTP_CONTENT_TYPE' in environ : environ [ 'CONTENT_TYPE' ] = environ [ 'HTTP_CONTENT_TYPE' ] # Save the request method for later self . request_method = environ [ 'REQUEST_METHOD' ] # Add Dynamic WSGI Variables if conn . ssl : environ [ 'wsgi.url_scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url_scheme' ] = 'http' if environ . get ( 'HTTP_TRANSFER_ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = ChunkedReader ( sock_file ) else : environ [ 'wsgi.input' ] = sock_file return environ
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
6475	def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : # relative path target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : # source_file is a folder print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
7038	def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlparams = { 'strformat' : 1 if strformat else 0 , 'page' : page , 'json' : 1 } urlqs = urlencode ( urlparams ) dataset_url = '%s/set/%s?%s' % ( lcc_server , dataset_id , urlqs ) LOGINFO ( 'retrieving dataset %s from %s, using URL: %s ...' % ( lcc_server , dataset_id , dataset_url ) ) try : # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # if apikey is not None, add it in as an Authorization: Bearer [apikey] # header if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } # hit the server req = Request ( dataset_url , data = None , headers = headers ) resp = urlopen ( req ) dataset = json . loads ( resp . read ( ) ) return dataset except Exception as e : LOGEXCEPTION ( 'could not retrieve the dataset JSON!' ) return None
4011	def get_dusty_containers ( services , include_exited = False ) : client = get_docker_client ( ) if services : containers = [ get_container_for_app_or_service ( service , include_exited = include_exited ) for service in services ] return [ container for container in containers if container ] else : return [ container for container in client . containers ( all = include_exited ) if any ( name . startswith ( '/dusty' ) for name in container . get ( 'Names' , [ ] ) ) ]
11357	def format_arxiv_id ( arxiv_id ) : if arxiv_id and "/" not in arxiv_id and "arXiv" not in arxiv_id : return "arXiv:%s" % ( arxiv_id , ) elif arxiv_id and '.' not in arxiv_id and arxiv_id . lower ( ) . startswith ( 'arxiv:' ) : return arxiv_id [ 6 : ] # strip away arxiv: for old identifiers else : return arxiv_id
148	def remove_out_of_image ( self , fully = True , partly = False ) : polys_clean = [ poly for poly in self . polygons if not poly . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] # TODO use deepcopy() here return PolygonsOnImage ( polys_clean , shape = self . shape )
7517	def snpcount_numba ( superints , snpsarr ) : ## iterate over all loci for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : ## make new array catg = np . zeros ( 4 , dtype = np . int16 ) ## a list for only catgs ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : #C catg [ 0 ] += 1 elif ncol [ idx ] == 65 : #A catg [ 1 ] += 1 elif ncol [ idx ] == 84 : #T catg [ 2 ] += 1 elif ncol [ idx ] == 71 : #G catg [ 3 ] += 1 elif ncol [ idx ] == 82 : #R catg [ 1 ] += 1 #A catg [ 3 ] += 1 #G elif ncol [ idx ] == 75 : #K catg [ 2 ] += 1 #T catg [ 3 ] += 1 #G elif ncol [ idx ] == 83 : #S catg [ 0 ] += 1 #C catg [ 3 ] += 1 #G elif ncol [ idx ] == 89 : #Y catg [ 0 ] += 1 #C catg [ 2 ] += 1 #T elif ncol [ idx ] == 87 : #W catg [ 1 ] += 1 #A catg [ 2 ] += 1 #T elif ncol [ idx ] == 77 : #M catg [ 0 ] += 1 #C catg [ 1 ] += 1 #A ## get second most common site catg . sort ( ) ## if invariant e.g., [0, 0, 0, 9], then nothing (" ") if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : # f is the name of a static class method on the datatype. f = getattr ( datatype , f , None ) return f and f ( desc ) # Automatically load strings that look like JSON or Yaml filenames. desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' # This is because it's the "drivers" directory, whereas # the others are animation, control, layout, project # without the s. TODO: rename drivers/ to driver/ in v4 cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
3403	def find_boundary_types ( model , boundary_type , external_compartment = None ) : if not model . boundary : LOGGER . warning ( "There are no boundary reactions in this model. " "Therefore specific types of boundary reactions such " "as 'exchanges', 'demands' or 'sinks' cannot be " "identified." ) return [ ] if external_compartment is None : external_compartment = find_external_compartment ( model ) return model . reactions . query ( lambda r : is_boundary_type ( r , boundary_type , external_compartment ) )
221	async def check_config ( self ) -> None : if self . directory is None : return try : stat_result = await aio_stat ( self . directory ) except FileNotFoundError : raise RuntimeError ( f"StaticFiles directory '{self.directory}' does not exist." ) if not ( stat . S_ISDIR ( stat_result . st_mode ) or stat . S_ISLNK ( stat_result . st_mode ) ) : raise RuntimeError ( f"StaticFiles path '{self.directory}' is not a directory." )
3364	def load_yaml_model ( filename ) : if isinstance ( filename , string_types ) : with io . open ( filename , "r" ) as file_handle : return model_from_dict ( yaml . load ( file_handle ) ) else : return model_from_dict ( yaml . load ( filename ) )
8756	def delete_tenant_quota ( context , tenant_id ) : tenant_quotas = context . session . query ( Quota ) tenant_quotas = tenant_quotas . filter_by ( tenant_id = tenant_id ) tenant_quotas . delete ( )
4237	def get_attached_devices_2 ( self ) : _LOGGER . info ( "Get attached devices 2" ) success , response = self . _make_request ( SERVICE_DEVICE_INFO , "GetAttachDevice2" ) if not success : return None success , devices_node = _find_node ( response . text , ".//GetAttachDevice2Response/NewAttachDevice" ) if not success : return None xml_devices = devices_node . findall ( "Device" ) devices = [ ] for d in xml_devices : ip = _xml_get ( d , 'IP' ) name = _xml_get ( d , 'Name' ) mac = _xml_get ( d , 'MAC' ) signal = _convert ( _xml_get ( d , 'SignalStrength' ) , int ) link_type = _xml_get ( d , 'ConnectionType' ) link_rate = _xml_get ( d , 'Linkspeed' ) allow_or_block = _xml_get ( d , 'AllowOrBlock' ) device_type = _convert ( _xml_get ( d , 'DeviceType' ) , int ) device_model = _xml_get ( d , 'DeviceModel' ) ssid = _xml_get ( d , 'SSID' ) conn_ap_mac = _xml_get ( d , 'ConnAPMAC' ) devices . append ( Device ( name , ip , mac , link_type , signal , link_rate , allow_or_block , device_type , device_model , ssid , conn_ap_mac ) ) return devices
2225	def _update_hasher ( hasher , data , types = True ) : # Determine if the data should be hashed directly or iterated through if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : # Denote that we are hashing over an iterable # Multiple structure bytes makes it harder accidently make conflicts SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) # first, try to nest quickly without recursive calls # (this works if all data in the sequence is a non-iterable) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : # need to use recursive calls # Update based on current item _update_hasher ( hasher , item , types ) for item in iter_ : # Ensure the items have a spacer between them _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
9614	def element_or_none ( self , using , value ) : try : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } ) except : return None
13619	def get_current_branch ( self ) : return next ( ( self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) if branch . startswith ( '*' ) ) , None )
923	def _aggr_sum ( inList ) : aggrMean = _aggr_mean ( inList ) if aggrMean == None : return None aggrSum = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem else : aggrSum += aggrMean return aggrSum
8806	def calc_periods ( hour = 0 , minute = 0 ) : # Calculate the time intervals in a usable form period_end = datetime . datetime . utcnow ( ) . replace ( hour = hour , minute = minute , second = 0 , microsecond = 0 ) period_start = period_end - datetime . timedelta ( days = 1 ) # period end should be slightly before the midnight. # hence, we subtract a second # this will force period_end to store something like: # datetime.datetime(2016, 5, 19, 23, 59, 59, 999999) # instead of: # datetime.datetime(2016, 5, 20, 0, 0, 0, 0) period_end -= datetime . timedelta ( seconds = 1 ) return ( period_start , period_end )
938	def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , "model.pkl" ) path = os . path . abspath ( path ) return path
11066	def add_user_to_allow ( self , name , user ) : # Clear user from both allow and deny before adding if not self . remove_user_from_acl ( name , user ) : return False if name not in self . _acl : return False self . _acl [ name ] [ 'allow' ] . append ( user ) return True
11067	def create_acl ( self , name ) : if name in self . _acl : return False self . _acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True
4920	def program_detail ( self , request , pk , program_uuid ) : # pylint: disable=invalid-name,unused-argument enterprise_customer_catalog = self . get_object ( ) program = enterprise_customer_catalog . get_program ( program_uuid ) if not program : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . ProgramDetailSerializer ( program , context = context ) return Response ( serializer . data )
804	def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : @ g_retrySQL def findCandidateModelWithRetries ( ) : modelID = None with ConnectionFactory . get ( ) as conn : # TODO: may need a table index on job_id/status for speed query = 'SELECT model_id FROM %s ' ' WHERE status=%%s ' ' AND job_id=%%s ' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] numRows = conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) assert numRows <= 1 , "Unexpected numRows: %r" % numRows if numRows == 1 : ( modelID , ) = rows [ 0 ] return modelID @ g_retrySQL def adoptModelWithRetries ( modelID ) : adopted = False with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_worker_conn_id=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE model_id=%%s ' ' AND status=%%s' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , maxUpdateInterval ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , 'Unexpected numRowsAffected=%r' % ( numRowsAffected , ) if numRowsAffected == 1 : adopted = True else : # Discern between transient failure during update and someone else # claiming this model ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'model_id' : modelID } , [ 'status' , '_eng_worker_conn_id' ] ) adopted = ( status == self . STATUS_RUNNING and connectionID == self . _connectionID ) return adopted adoptedModelID = None while True : modelID = findCandidateModelWithRetries ( ) if modelID is None : break if adoptModelWithRetries ( modelID ) : adoptedModelID = modelID break return adoptedModelID
11441	def _correct_record ( record ) : errors = [ ] for tag in record . keys ( ) : upper_bound = '999' n = len ( tag ) if n > 3 : i = n - 3 while i > 0 : upper_bound = '%s%s' % ( '0' , upper_bound ) i -= 1 # Missing tag. Replace it with dummy tag '000'. if tag == '!' : errors . append ( ( 1 , '(field number(s): ' + str ( [ f [ 4 ] for f in record [ tag ] ] ) + ')' ) ) record [ '000' ] = record . pop ( tag ) tag = '000' elif not ( '001' <= tag <= upper_bound or tag in ( 'FMT' , 'FFT' , 'BDR' , 'BDM' ) ) : errors . append ( 2 ) record [ '000' ] = record . pop ( tag ) tag = '000' fields = [ ] for field in record [ tag ] : # Datafield without any subfield. if field [ 0 ] == [ ] and field [ 3 ] == '' : errors . append ( ( 8 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) subfields = [ ] for subfield in field [ 0 ] : if subfield [ 0 ] == '!' : errors . append ( ( 3 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) newsub = ( '' , subfield [ 1 ] ) else : newsub = subfield subfields . append ( newsub ) if field [ 1 ] == '!' : errors . append ( ( 4 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind1 = " " else : ind1 = field [ 1 ] if field [ 2 ] == '!' : errors . append ( ( 5 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind2 = " " else : ind2 = field [ 2 ] fields . append ( ( subfields , ind1 , ind2 , field [ 3 ] , field [ 4 ] ) ) record [ tag ] = fields return errors
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
4013	def _ensure_managed_repos_dir_exists ( ) : if not os . path . exists ( constants . REPOS_DIR ) : os . makedirs ( constants . REPOS_DIR )
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
2817	def convert_adaptive_max_pool2d ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting adaptive_avg_pool2d...' ) if names == 'short' : tf_name = 'APOL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) global_pool = keras . layers . GlobalMaxPooling2D ( data_format = 'channels_first' , name = tf_name ) layers [ scope_name ] = global_pool ( layers [ inputs [ 0 ] ] ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ scope_name ] ) # double expand dims layers [ scope_name ] = lambda_layer ( layers [ scope_name ] )
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
1716	def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
4202	def aryule ( X , order , norm = 'biased' , allow_singularity = True ) : assert norm in [ 'biased' , 'unbiased' ] r = CORRELATION ( X , maxlags = order , norm = norm ) A , P , k = LEVINSON ( r , allow_singularity = allow_singularity ) return A , P , k
5775	def _advapi32_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : algo = private_key . algorithm if algo == 'rsa' and hash_algorithm == 'raw' : padded_data = add_pkcs1v15_signature_padding ( private_key . byte_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) padded_data = add_pss_padding ( hash_algorithm , hash_length , private_key . bit_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if private_key . algorithm == 'dsa' and hash_algorithm == 'md5' : raise ValueError ( pretty_message ( ''' Windows does not support md5 signatures with DSA keys ''' ) ) hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( private_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) out_len = new ( advapi32 , 'DWORD *' ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , null ( ) , out_len ) handle_error ( res ) buffer_length = deref ( out_len ) buffer_ = buffer_from_bytes ( buffer_length ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , buffer_ , out_len ) handle_error ( res ) output = bytes_from_buffer ( buffer_ , deref ( out_len ) ) # CryptoAPI outputs the signature in little endian byte order, so we # must swap it for compatibility with other systems output = output [ : : - 1 ] if algo == 'dsa' : # Switch the two integers because the reversal just before switched # then half_len = len ( output ) // 2 output = output [ half_len : ] + output [ : half_len ] # Windows doesn't use the ASN.1 Sequence for DSA signatures, # so we have to convert it here for the verification to work output = algos . DSASignature . from_p1363 ( output ) . dump ( ) return output finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
9832	def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )
514	def _avgColumnsPerInput ( self ) : #TODO: extend to support different number of dimensions for inputs and # columns numDim = max ( self . _columnDimensions . size , self . _inputDimensions . size ) colDim = numpy . ones ( numDim ) colDim [ : self . _columnDimensions . size ] = self . _columnDimensions inputDim = numpy . ones ( numDim ) inputDim [ : self . _inputDimensions . size ] = self . _inputDimensions columnsPerInput = colDim . astype ( realDType ) / inputDim return numpy . average ( columnsPerInput )
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : # (False, "Domain lookup failed, defaulting to {0}".format(UDP_IP)) pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) # UDP sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
8397	def gettrans ( t ) : obj = t # Make sure trans object is instantiated if isinstance ( obj , str ) : name = '{}_trans' . format ( obj ) obj = globals ( ) [ name ] ( ) if callable ( obj ) : obj = obj ( ) if isinstance ( obj , type ) : obj = obj ( ) if not isinstance ( obj , trans ) : raise ValueError ( "Could not get transform object." ) return obj
797	def jobUpdateResults ( self , jobID , results ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_last_update_time=UTC_TIMESTAMP(), ' ' results=%%s ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ results , jobID ] )
5693	def evaluate ( self , dep_time , first_leg_can_be_walk = True , connection_arrival_time = None ) : walk_labels = list ( ) # walk label towards target if first_leg_can_be_walk and self . _walk_to_target_duration != float ( 'inf' ) : # add walk_label if connection_arrival_time is not None : walk_labels . append ( self . _get_label_to_target ( connection_arrival_time ) ) else : walk_labels . append ( self . _get_label_to_target ( dep_time ) ) # if dep time is larger than the largest dep time -> only walk labels are possible if dep_time in self . dep_times_to_index : assert ( dep_time != float ( 'inf' ) ) index = self . dep_times_to_index [ dep_time ] labels = self . _label_bags [ index ] pareto_optimal_labels = merge_pareto_frontiers ( labels , walk_labels ) else : pareto_optimal_labels = walk_labels if not first_leg_can_be_walk : pareto_optimal_labels = [ label for label in pareto_optimal_labels if not label . first_leg_is_walk ] return pareto_optimal_labels
5659	def _validate_danglers ( self ) : for query , warning in zip ( DANGLER_QUERIES , DANGLER_WARNINGS ) : dangler_count = self . gtfs . execute_custom_query ( query ) . fetchone ( ) [ 0 ] if dangler_count > 0 : if self . verbose : print ( str ( dangler_count ) + " " + warning ) self . warnings_container . add_warning ( warning , self . location , count = dangler_count )
13080	def register ( self ) : if self . app is not None : if not self . blueprint : self . blueprint = self . create_blueprint ( ) self . app . register_blueprint ( self . blueprint ) if self . cache is None : # We register a fake cache extension. setattr ( self . app . jinja_env , "_fake_cache_extension" , self ) self . app . jinja_env . add_extension ( FakeCacheExtension ) return self . blueprint return None
988	def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ "inputFilePath" ] scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] dateEncoderArgs = recordParams [ "dateEncoderArgs" ] scalarEncoder = ScalarEncoder ( * * scalarEncoderArgs ) dateEncoder = DateEncoder ( * * dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( { "verbosity" : verbosity } ) ) sensor = network . regions [ "sensor" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) # Create the spatial pooler region spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( spatialParams ) ) # Link the SP region to the sensor input network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "resetOut" , destInput = "resetIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) # Add the TPRegion on top of the SPRegion network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , json . dumps ( temporalParams ) ) network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "topDownIn" ) spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] # Make sure learning is enabled spatialPoolerRegion . setParameter ( "learningMode" , True ) # We want temporal anomalies so disable anomalyMode in the SP. This mode is # used for computing anomalies in a non-temporal model. spatialPoolerRegion . setParameter ( "anomalyMode" , False ) temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] # Enable topDownMode to get the predicted columns output temporalPoolerRegion . setParameter ( "topDownMode" , True ) # Make sure learning is enabled (this is the default) temporalPoolerRegion . setParameter ( "learningMode" , True ) # Enable inference mode so we get predictions temporalPoolerRegion . setParameter ( "inferenceMode" , True ) # Enable anomalyMode to compute the anomaly score. temporalPoolerRegion . setParameter ( "anomalyMode" , True ) return network
3482	def _get_doc_from_filename ( filename ) : if isinstance ( filename , string_types ) : if ( "win" in platform ) and ( len ( filename ) < 260 ) and os . path . exists ( filename ) : # path (win) doc = libsbml . readSBMLFromFile ( filename ) # noqa: E501 type: libsbml.SBMLDocument elif ( "win" not in platform ) and os . path . exists ( filename ) : # path other doc = libsbml . readSBMLFromFile ( filename ) # noqa: E501 type: libsbml.SBMLDocument else : # string representation if "<sbml" not in filename : raise IOError ( "The file with 'filename' does not exist, " "or is not an SBML string. Provide the path to " "an existing SBML file or a valid SBML string " "representation: \n%s" , filename ) doc = libsbml . readSBMLFromString ( filename ) # noqa: E501 type: libsbml.SBMLDocument elif hasattr ( filename , "read" ) : # file handle doc = libsbml . readSBMLFromString ( filename . read ( ) ) # noqa: E501 type: libsbml.SBMLDocument else : raise CobraSBMLError ( "Input type '%s' for 'filename' is not supported." " Provide a path, SBML str, " "or file handle." , type ( filename ) ) return doc
8442	def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - "{}"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp . raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories
3230	def service_list ( service = None , key_name = None , * * kwargs ) : resp_list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) if key_name and key_name in resp : resp_list . extend ( resp [ key_name ] ) else : resp_list . append ( resp ) # Not all list calls have a list_next if hasattr ( service , 'list_next' ) : req = service . list_next ( previous_request = req , previous_response = resp ) else : req = None return resp_list
9873	def CherryPyWSGIServer ( bind_addr , wsgi_app , numthreads = 10 , server_name = None , max = - 1 , request_queue_size = 5 , timeout = 10 , shutdown_timeout = 5 ) : max_threads = max if max_threads < 0 : max_threads = 0 return Rocket ( bind_addr , 'wsgi' , { 'wsgi_app' : wsgi_app } , min_threads = numthreads , max_threads = max_threads , queue_size = request_queue_size , timeout = timeout )
4809	def prepare_feature ( best_processed_path , option = 'train' ) : # padding for training and testing set n_pad = 21 n_pad_2 = int ( ( n_pad - 1 ) / 2 ) pad = [ { 'char' : ' ' , 'type' : 'p' , 'target' : True } ] df_pad = pd . DataFrame ( pad * n_pad_2 ) df = [ ] for article_type in article_types : df . append ( pd . read_csv ( os . path . join ( best_processed_path , option , 'df_best_{}_{}.csv' . format ( article_type , option ) ) ) ) df = pd . concat ( df ) df = pd . concat ( ( df_pad , df , df_pad ) ) # pad with empty string feature df [ 'char' ] = df [ 'char' ] . map ( lambda x : CHARS_MAP . get ( x , 80 ) ) df [ 'type' ] = df [ 'type' ] . map ( lambda x : CHAR_TYPES_MAP . get ( x , 4 ) ) df_pad = create_n_gram_df ( df , n_pad = n_pad ) char_row = [ 'char' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'char-' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'char' ] type_row = [ 'type' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'type-' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'type' ] x_char = df_pad [ char_row ] . as_matrix ( ) x_type = df_pad [ type_row ] . as_matrix ( ) y = df_pad [ 'target' ] . astype ( int ) . as_matrix ( ) return x_char , x_type , y
9543	def add_record_length_check ( self , code = RECORD_LENGTH_CHECK_FAILED , message = MESSAGES [ RECORD_LENGTH_CHECK_FAILED ] , modulus = 1 ) : t = code , message , modulus self . _record_length_checks . append ( t )
2649	def monitor ( pid , task_id , monitoring_hub_url , run_id , sleep_dur = 10 ) : import psutil radio = UDPRadio ( monitoring_hub_url , source_id = task_id ) # these values are simple to log. Other information is available in special formats such as memory below. simple = [ "cpu_num" , 'cpu_percent' , 'create_time' , 'cwd' , 'exe' , 'memory_percent' , 'nice' , 'name' , 'num_threads' , 'pid' , 'ppid' , 'status' , 'username' ] # values that can be summed up to see total resources used by task process and its children summable_values = [ 'cpu_percent' , 'memory_percent' , 'num_threads' ] pm = psutil . Process ( pid ) pm . cpu_percent ( ) first_msg = True while True : try : d = { "psutil_process_" + str ( k ) : v for k , v in pm . as_dict ( ) . items ( ) if k in simple } d [ "run_id" ] = run_id d [ "task_id" ] = task_id d [ 'resource_monitoring_interval' ] = sleep_dur d [ 'first_msg' ] = first_msg d [ 'timestamp' ] = datetime . datetime . now ( ) children = pm . children ( recursive = True ) d [ "psutil_cpu_count" ] = psutil . cpu_count ( ) d [ 'psutil_process_memory_virtual' ] = pm . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] = pm . memory_info ( ) . rss d [ 'psutil_process_time_user' ] = pm . cpu_times ( ) . user d [ 'psutil_process_time_system' ] = pm . cpu_times ( ) . system d [ 'psutil_process_children_count' ] = len ( children ) try : d [ 'psutil_process_disk_write' ] = pm . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] = pm . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : # occassionally pid temp files that hold this information are unvailable to be read so set to zero d [ 'psutil_process_disk_write' ] = 0 d [ 'psutil_process_disk_read' ] = 0 for child in children : for k , v in child . as_dict ( attrs = summable_values ) . items ( ) : d [ 'psutil_process_' + str ( k ) ] += v d [ 'psutil_process_time_user' ] += child . cpu_times ( ) . user d [ 'psutil_process_time_system' ] += child . cpu_times ( ) . system d [ 'psutil_process_memory_virtual' ] += child . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] += child . memory_info ( ) . rss try : d [ 'psutil_process_disk_write' ] += child . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] += child . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : # occassionally pid temp files that hold this information are unvailable to be read so add zero d [ 'psutil_process_disk_write' ] += 0 d [ 'psutil_process_disk_read' ] += 0 finally : radio . send ( MessageType . TASK_INFO , task_id , d ) time . sleep ( sleep_dur ) first_msg = False
5007	def get_user_from_social_auth ( tpa_provider , tpa_username ) : user_social_auth = UserSocialAuth . objects . select_related ( 'user' ) . filter ( user__username = tpa_username , provider = tpa_provider . backend_name ) . first ( ) return user_social_auth . user if user_social_auth else None
12950	def connectAlt ( cls , redisConnectionParams ) : if not isinstance ( redisConnectionParams , dict ) : raise ValueError ( 'redisConnectionParams must be a dictionary!' ) hashVal = hashDictOneLevel ( redisConnectionParams ) modelDictCopy = copy . deepcopy ( dict ( cls . __dict__ ) ) modelDictCopy [ 'REDIS_CONNECTION_PARAMS' ] = redisConnectionParams ConnectedIndexedRedisModel = type ( 'AltConnect' + cls . __name__ + str ( hashVal ) , cls . __bases__ , modelDictCopy ) return ConnectedIndexedRedisModel
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
5527	def backend_version ( backend , childprocess = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) if not childprocess : return _backend_version ( backend ) else : return run_in_childprocess ( _backend_version , None , backend )
5726	def get_gdb_response ( self , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 if USING_WINDOWS : retval = self . _get_responses_windows ( timeout_sec ) else : retval = self . _get_responses_unix ( timeout_sec ) if not retval and raise_error_on_timeout : raise GdbTimeoutError ( "Did not get response from gdb after %s seconds" % timeout_sec ) else : return retval
4755	def src_to_html ( fpath ) : if not os . path . exists ( fpath ) : return "COULD-NOT-FIND-TESTCASE-SRC-AT-FPATH:%r" % fpath # NOTE: Do SYNTAX highlight? return open ( fpath , "r" ) . read ( )
638	def getString ( cls , prop ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) # Allow configuration properties to be overridden via environment variables envValue = os . environ . get ( "%s%s" % ( cls . envPropPrefix , prop . replace ( '.' , '_' ) ) , None ) if envValue is not None : return envValue return cls . _properties [ prop ]
1005	def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : # How much input history have we accumulated? # The current input is always at the end of self._prevInfPatterns (at # index -1), but it is also evaluated as a potential starting point by # turning on it's start cells and seeing if it generates sufficient # predictions going forward. numPrevPatterns = len ( self . _prevLrnPatterns ) # This is an easy to use label for the current time step currentTimeStepsOffset = numPrevPatterns - 1 # Clear out any old segment updates. learnPhase2() adds to the segment # updates if we're not readOnly if not readOnly : self . segmentUpdates = { } # Status message if self . verbosity >= 3 : if readOnly : print ( "Trying to lock-on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( "Locking on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) # Play through up to the current time step inSequence = True for offset in range ( startOffset , numPrevPatterns ) : # Copy predicted and active states into t-1 self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] # Get the input pattern inputColumns = self . _prevLrnPatterns [ offset ] # Apply segment updates from the last set of predictions if not readOnly : self . _processSegmentUpdates ( inputColumns ) # Phase 1: # Compute activeState[t] given bottom-up and predictedState[t-1] if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : # Uses lrnActiveState['t-1'] and lrnPredictedState['t-1'] # computes lrnActiveState['t'] inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) # Break out immediately if we fell out of sequence or reached the current # time step if not inSequence or offset == currentTimeStepsOffset : break # Phase 2: # Computes predictedState['t'] given activeState['t'] and also queues # up active segments into self.segmentUpdates, unless this is readOnly if self . verbosity >= 3 : print " backtrack: computing predictions from " , inputColumns self . _learnPhase2 ( readOnly = readOnly ) # Return whether or not this starting point was valid return inSequence
12161	def userFolder ( ) : #path=os.path.abspath(tempfile.gettempdir()+"/swhlab/") #don't use tempdir! it will get deleted easily. path = os . path . expanduser ( "~" ) + "/.swhlab/" # works on windows or linux # for me, path=r"C:\Users\swharden\.swhlab" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
4103	def setup ( app ) : app . add_config_value ( 'plot_gallery' , True , 'html' ) app . add_config_value ( 'abort_on_example_error' , False , 'html' ) app . add_config_value ( 'sphinx_gallery_conf' , gallery_conf , 'html' ) app . add_stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate_gallery_rst ) app . connect ( 'build-finished' , embed_code_links )
2538	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True doc . package . verif_code = code else : raise CardinalityError ( 'Package::VerificationCode' )
13905	def match_part ( string , part ) : if not string or not re . match ( '^(' + PARTS [ part ] + ')$' , string ) : raise ValueError ( '{} should match {}' . format ( part , PARTS [ part ] ) )
354	def load_and_assign_npz ( sess = None , name = None , network = None ) : if network is None : raise ValueError ( "network is None." ) if sess is None : raise ValueError ( "session is None." ) if not os . path . exists ( name ) : logging . error ( "file {} doesn't exist." . format ( name ) ) return False else : params = load_npz ( name = name ) assign_params ( sess , params , network ) logging . info ( "[*] Load {} SUCCESS!" . format ( name ) ) return network
6211	def fit ( self , trX , trY , batch_size = 64 , n_epochs = 1 , len_filter = LenFilter ( ) , snapshot_freq = 1 , path = None ) : if len_filter is not None : trX , trY = len_filter . filter ( trX , trY ) trY = standardize_targets ( trY , cost = self . cost ) n = 0. t = time ( ) costs = [ ] for e in range ( n_epochs ) : epoch_costs = [ ] for xmb , ymb in self . iterator . iterXY ( trX , trY ) : c = self . _train ( xmb , ymb ) epoch_costs . append ( c ) n += len ( ymb ) if self . verbose >= 2 : n_per_sec = n / ( time ( ) - t ) n_left = len ( trY ) - n % len ( trY ) time_left = n_left / n_per_sec sys . stdout . write ( "\rEpoch %d Seen %d samples Avg cost %0.4f Time left %d seconds" % ( e , n , np . mean ( epoch_costs [ - 250 : ] ) , time_left ) ) sys . stdout . flush ( ) costs . extend ( epoch_costs ) status = "Epoch %d Seen %d samples Avg cost %0.4f Time elapsed %d seconds" % ( e , n , np . mean ( epoch_costs [ - 250 : ] ) , time ( ) - t ) if self . verbose >= 2 : sys . stdout . write ( "\r" + status ) sys . stdout . flush ( ) sys . stdout . write ( "\n" ) elif self . verbose == 1 : print ( status ) if path and e % snapshot_freq == 0 : save ( self , "{0}.{1}" . format ( path , e ) ) return costs
615	def _generateInferenceArgs ( options , tokenReplacements ) : inferenceType = options [ 'inferenceType' ] optionInferenceArgs = options . get ( 'inferenceArgs' , None ) resultInferenceArgs = { } predictedField = _getPredictedField ( options ) [ 0 ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ) : assert predictedField , "Inference Type '%s' needs a predictedField " "specified in the inferenceArgs dictionary" % inferenceType if optionInferenceArgs : # If we will be using a dynamically created predictionSteps, plug in that # variable name in place of the constant scalar value if options [ 'dynamicPredictionSteps' ] : altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) resultInferenceArgs = resultInferenceArgs . replace ( "'$REPLACE_ME'" , '[predictionSteps]' ) else : resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) tokenReplacements [ '\$INFERENCE_ARGS' ] = resultInferenceArgs tokenReplacements [ '\$PREDICTION_FIELD' ] = predictedField
2485	def to_special_value ( self , value ) : if isinstance ( value , utils . NoAssert ) : return self . spdx_namespace . noassertion elif isinstance ( value , utils . SPDXNone ) : return self . spdx_namespace . none else : return Literal ( value )
10236	def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
12224	def convertShpToExtend ( pathToShp ) : driver = ogr . GetDriverByName ( 'ESRI Shapefile' ) dataset = driver . Open ( pathToShp ) if dataset is not None : # from Layer layer = dataset . GetLayer ( ) spatialRef = layer . GetSpatialRef ( ) # from Geometry feature = layer . GetNextFeature ( ) geom = feature . GetGeometryRef ( ) spatialRef = geom . GetSpatialReference ( ) #WGS84 outSpatialRef = osr . SpatialReference ( ) outSpatialRef . ImportFromEPSG ( 4326 ) coordTrans = osr . CoordinateTransformation ( spatialRef , outSpatialRef ) env = geom . GetEnvelope ( ) pointMAX = ogr . Geometry ( ogr . wkbPoint ) pointMAX . AddPoint ( env [ 1 ] , env [ 3 ] ) pointMAX . Transform ( coordTrans ) pointMIN = ogr . Geometry ( ogr . wkbPoint ) pointMIN . AddPoint ( env [ 0 ] , env [ 2 ] ) pointMIN . Transform ( coordTrans ) return [ pointMAX . GetPoint ( ) [ 1 ] , pointMIN . GetPoint ( ) [ 0 ] , pointMIN . GetPoint ( ) [ 1 ] , pointMAX . GetPoint ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
2479	def datetime_iso_format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
5305	def detect_color_support ( env ) : # noqa if env . get ( 'COLORFUL_DISABLE' , '0' ) == '1' : return NO_COLORS if env . get ( 'COLORFUL_FORCE_8_COLORS' , '0' ) == '1' : return ANSI_8_COLORS if env . get ( 'COLORFUL_FORCE_16_COLORS' , '0' ) == '1' : return ANSI_16_COLORS if env . get ( 'COLORFUL_FORCE_256_COLORS' , '0' ) == '1' : return ANSI_256_COLORS if env . get ( 'COLORFUL_FORCE_TRUE_COLORS' , '0' ) == '1' : return TRUE_COLORS # if we are not a tty if not sys . stdout . isatty ( ) : return NO_COLORS colorterm_env = env . get ( 'COLORTERM' ) if colorterm_env : if colorterm_env in { 'truecolor' , '24bit' } : return TRUE_COLORS if colorterm_env in { '8bit' } : return ANSI_256_COLORS termprog_env = env . get ( 'TERM_PROGRAM' ) if termprog_env : if termprog_env in { 'iTerm.app' , 'Hyper' } : return TRUE_COLORS if termprog_env in { 'Apple_Terminal' } : return ANSI_256_COLORS term_env = env . get ( 'TERM' ) if term_env : if term_env in { 'screen-256' , 'screen-256color' , 'xterm-256' , 'xterm-256color' } : return ANSI_256_COLORS if term_env in { 'screen' , 'xterm' , 'vt100' , 'color' , 'ansi' , 'cygwin' , 'linux' } : return ANSI_16_COLORS if colorterm_env : # if there was no match with $TERM either but we # had one with $COLORTERM, we use it! return ANSI_16_COLORS return ANSI_8_COLORS
9585	def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) # write array data elements (size info) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) # write the compressed data fd . write ( data )
3103	def do_GET ( self ) : self . send_response ( http_client . OK ) self . send_header ( 'Content-type' , 'text/html' ) self . end_headers ( ) parts = urllib . parse . urlparse ( self . path ) query = _helpers . parse_unique_urlencoded ( parts . query ) self . server . query_params = query self . wfile . write ( b'<html><head><title>Authentication Status</title></head>' ) self . wfile . write ( b'<body><p>The authentication flow has completed.</p>' ) self . wfile . write ( b'</body></html>' )
1407	def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , direct_task = None , need_task_ids = False ) : # first check whether this tuple is sane self . pplan_helper . check_output_schema ( stream , tup ) # get custom grouping target task ids; get empty list if not custom grouping custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) data_tuple = tuple_pb2 . HeronDataTuple ( ) data_tuple . key = 0 if direct_task is not None : if not isinstance ( direct_task , int ) : raise TypeError ( "direct_task argument needs to be an integer, given: %s" % str ( type ( direct_task ) ) ) # performing emit-direct data_tuple . dest_task_ids . append ( direct_task ) elif custom_target_task_ids is not None : # for custom grouping for task_id in custom_target_task_ids : data_tuple . dest_task_ids . append ( task_id ) if tup_id is not None : tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) if self . acking_enabled : # this message is rooted root = data_tuple . roots . add ( ) root . taskid = self . pplan_helper . my_task_id root . key = tuple_info . key self . in_flight_tuples [ tuple_info . key ] = tuple_info else : self . immediate_acks . append ( tuple_info ) tuple_size_in_bytes = 0 start_time = time . time ( ) # Serialize for obj in tup : serialized = self . serializer . serialize ( obj ) data_tuple . values . append ( serialized ) tuple_size_in_bytes += len ( serialized ) serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , tuple_size_in_bytes = tuple_size_in_bytes ) self . total_tuples_emitted += 1 self . spout_metrics . update_emit_count ( stream ) if need_task_ids : sent_task_ids = custom_target_task_ids or [ ] if direct_task is not None : sent_task_ids . append ( direct_task ) return sent_task_ids
2087	def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
1971	def check_timers ( self ) : if self . _current is None : # Advance the clocks. Go to future!! advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f"Advancing the clock from {self.clocks} to {advance}" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )
70	def remove_out_of_image ( self , fully = True , partly = False ) : bbs_clean = [ bb for bb in self . bounding_boxes if not bb . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return BoundingBoxesOnImage ( bbs_clean , shape = self . shape )
12589	def treefall ( iterable ) : num_elems = len ( iterable ) for i in range ( num_elems , - 1 , - 1 ) : for c in combinations ( iterable , i ) : yield c
8882	def predict_proba ( self , X ) : # Check is fit had been called check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) # Check that X have correct shape X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix )
12350	def get ( self , id ) : info = self . _get_droplet_info ( id ) return DropletActions ( self . api , self , * * info )
5205	def proc_elms ( * * kwargs ) -> list : return [ ( ELEM_KEYS . get ( k , k ) , ELEM_VALS . get ( ELEM_KEYS . get ( k , k ) , dict ( ) ) . get ( v , v ) ) for k , v in kwargs . items ( ) if ( k in list ( ELEM_KEYS . keys ( ) ) + list ( ELEM_KEYS . values ( ) ) ) and ( k not in PRSV_COLS ) ]
7684	def mkclick ( freq , sr = 22050 , duration = 0.1 ) : times = np . arange ( int ( sr * duration ) ) click = np . sin ( 2 * np . pi * times * freq / float ( sr ) ) click *= np . exp ( - times / ( 1e-2 * sr ) ) return click
8941	def _to_webdav ( self , docs_base , release ) : try : git_path = subprocess . check_output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . CalledProcessError : git_path = '' else : git_path = git_path . decode ( 'ascii' ) . strip ( ) git_path = git_path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git_path = re . search ( r'[^:/]+?[:/](.+)' , git_path ) git_path = git_path . group ( 1 ) . replace ( '.git' , '' ) if git_path else '' url = None with self . _zipped ( docs_base ) as handle : url_ns = dict ( name = self . cfg . project . name , version = release , git_path = git_path ) reply = requests . put ( self . params [ 'url' ] . format ( * * url_ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( * * vars ( reply ) ) ) try : data = reply . json ( ) except ValueError as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'downloadUri' in data : # Artifactory url = data [ 'downloadUri' ] + '!/index.html' elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for PUT to {url}" . format ( * * data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
9132	def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
7099	def child_added ( self , child ) : if child . widget : # TODO: Should we keep count and remove the adapter if not all # markers request it? self . parent ( ) . init_info_window_adapter ( ) super ( AndroidMapMarker , self ) . child_added ( child )
10299	def group_errors ( graph : BELGraph ) -> Mapping [ str , List [ int ] ] : warning_summary = defaultdict ( list ) for _ , exc , _ in graph . warnings : warning_summary [ str ( exc ) ] . append ( exc . line_number ) return dict ( warning_summary )
4578	def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
1987	def save_state ( self , state , key ) : with self . save_stream ( key , binary = True ) as f : self . _serializer . serialize ( state , f )
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
7297	def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model_field , ObjectIdField ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
8421	def _format ( formatter , x ) : # For MPL to play nice formatter . create_dummy_axis ( ) # For sensible decimal places formatter . set_locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . orderOfMagnitude ) except AttributeError : oom = 0 labels = [ formatter ( tick ) for tick in x ] # Remove unnecessary decimals pattern = re . compile ( r'\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) # MPL does not add the exponential component if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels
9845	def ndmeshgrid ( * arrs ) : #arrs = tuple(reversed(arrs)) <-- wrong on stackoverflow.com arrs = tuple ( arrs ) lens = list ( map ( len , arrs ) ) dim = len ( arrs ) sz = 1 for s in lens : sz *= s ans = [ ] for i , arr in enumerate ( arrs ) : slc = [ 1 ] * dim slc [ i ] = lens [ i ] arr2 = numpy . asanyarray ( arr ) . reshape ( slc ) for j , sz in enumerate ( lens ) : if j != i : arr2 = arr2 . repeat ( sz , axis = j ) ans . append ( arr2 ) return tuple ( ans )
1254	def setup_saver ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] for c in self . get_savable_components ( ) : c . register_saver_ops ( ) # TensorFlow saver object # TODO potentially make other options configurable via saver spec. self . saver = tf . train . Saver ( var_list = global_variables , # should be given? reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True # filename=None )
5876	def check_link_tag ( self ) : node = self . article . raw_doc meta = self . parser . getElementsByTag ( node , tag = 'link' , attr = 'rel' , value = 'image_src' ) for item in meta : src = self . parser . getAttribute ( item , attr = 'href' ) if src : return self . get_image ( src , extraction_type = 'linktag' ) return None
5160	def render ( self ) : # get jinja2 template template_name = '{0}.jinja2' . format ( self . get_name ( ) ) template = self . template_env . get_template ( template_name ) # render template and cleanup context = getattr ( self . backend , 'intermediate_data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )
932	def run ( self , inputRecord ) : # 0-based prediction index for ModelResult predictionNumber = self . _numPredictions self . _numPredictions += 1 result = opf_utils . ModelResult ( predictionNumber = predictionNumber , rawInput = inputRecord ) return result
8372	def save_as ( self ) : chooser = ShoebotFileChooserDialog ( _ ( 'Save File' ) , None , Gtk . FileChooserAction . SAVE , ( Gtk . STOCK_SAVE , Gtk . ResponseType . ACCEPT , Gtk . STOCK_CANCEL , Gtk . ResponseType . CANCEL ) ) chooser . set_do_overwrite_confirmation ( True ) chooser . set_transient_for ( self ) saved = chooser . run ( ) == Gtk . ResponseType . ACCEPT if saved : old_filename = self . filename self . source_buffer . filename = chooser . get_filename ( ) if not self . save ( ) : self . filename = old_filename chooser . destroy ( ) return saved
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
1324	def threadFunc ( root ) : #print(root)# you cannot use root because it is root control created in main thread th = threading . currentThread ( ) auto . Logger . WriteLine ( '\nThis is running in a new thread. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan ) time . sleep ( 2 ) auto . InitializeUIAutomationInCurrentThread ( ) auto . GetConsoleWindow ( ) . CaptureToImage ( 'console_newthread.png' ) newRoot = auto . GetRootControl ( ) #ok, root control created in new thread auto . EnumAndLogControl ( newRoot , 1 ) auto . UninitializeUIAutomationInCurrentThread ( ) auto . Logger . WriteLine ( '\nThread exits. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan )
12098	def show ( self , args , file_handle = None , * * kwargs ) : full_string = '' info = { 'root_directory' : '<root_directory>' , 'batch_name' : '<batch_name>' , 'batch_tag' : '<batch_tag>' , 'batch_description' : '<batch_description>' , 'launcher' : '<launcher>' , 'timestamp_format' : '<timestamp_format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying_keys' : args . varying_keys , 'constant_keys' : args . constant_keys , 'constant_items' : args . constant_items } quoted_cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . _formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd_lines = [ '%d: %s\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted_cmds ) ] full_string += '' . join ( cmd_lines ) if file_handle : file_handle . write ( full_string ) file_handle . flush ( ) else : print ( full_string )
6907	def equatorial_to_galactic ( ra , decl , equinox = 'J2000' ) : # convert the ra/decl to gl, gb radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree , equinox = equinox ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return gl , gb
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
6452	def dist_abs ( self , src , tar ) : if tar == src : return 0 elif not src : return len ( tar ) elif not tar : return len ( src ) src_bag = Counter ( src ) tar_bag = Counter ( tar ) return max ( sum ( ( src_bag - tar_bag ) . values ( ) ) , sum ( ( tar_bag - src_bag ) . values ( ) ) , )
6970	def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , epdsmooth_sigclip = 3.0 , epdsmooth_func = smooth_magseries_signal_medfilt , epdsmooth_extraparams = None ) : # find all the finite values of the magsnitude finiteind = np . isfinite ( mags ) # calculate median and stdev mags_median = np . median ( mags [ finiteind ] ) mags_stdev = np . nanstd ( mags ) # if we're supposed to sigma clip, do so if epdsmooth_sigclip : excludeind = abs ( mags - mags_median ) < epdsmooth_sigclip * mags_stdev finalind = finiteind & excludeind else : finalind = finiteind final_mags = mags [ finalind ] final_len = len ( final_mags ) # smooth the signal if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize , * * epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize ) # make the linear equation matrix epdmatrix = np . c_ [ fsv [ finalind ] ** 2.0 , fsv [ finalind ] , fdv [ finalind ] ** 2.0 , fdv [ finalind ] , fkv [ finalind ] ** 2.0 , fkv [ finalind ] , np . ones ( final_len ) , fsv [ finalind ] * fdv [ finalind ] , fsv [ finalind ] * fkv [ finalind ] , fdv [ finalind ] * fkv [ finalind ] , np . sin ( 2 * np . pi * xcc [ finalind ] ) , np . cos ( 2 * np . pi * xcc [ finalind ] ) , np . sin ( 2 * np . pi * ycc [ finalind ] ) , np . cos ( 2 * np . pi * ycc [ finalind ] ) , np . sin ( 4 * np . pi * xcc [ finalind ] ) , np . cos ( 4 * np . pi * xcc [ finalind ] ) , np . sin ( 4 * np . pi * ycc [ finalind ] ) , np . cos ( 4 * np . pi * ycc [ finalind ] ) , bgv [ finalind ] , bge [ finalind ] ] # solve the matrix equation [epdmatrix] . [x] = [smoothedmags] # return the EPD differential magss if the solution succeeds try : coeffs , residuals , rank , singulars = lstsq ( epdmatrix , smoothedmags , rcond = None ) if DEBUG : print ( 'coeffs = %s, residuals = %s' % ( coeffs , residuals ) ) retdict = { 'times' : times , 'mags' : ( mags_median + _old_epd_diffmags ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , mags ) ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict # if the solution fails, return nothing except Exception as e : LOGEXCEPTION ( 'EPD solution did not converge' ) retdict = { 'times' : times , 'mags' : np . full_like ( mags , np . nan ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict
5420	def _get_job_resources ( args ) : logging = param_util . build_logging_param ( args . logging ) if args . logging else None timeout = param_util . timeout_in_seconds ( args . timeout ) log_interval = param_util . log_interval_in_seconds ( args . log_interval ) return job_model . Resources ( min_cores = args . min_cores , min_ram = args . min_ram , machine_type = args . machine_type , disk_size = args . disk_size , disk_type = args . disk_type , boot_disk_size = args . boot_disk_size , preemptible = args . preemptible , image = args . image , regions = args . regions , zones = args . zones , logging = logging , logging_path = None , service_account = args . service_account , scopes = args . scopes , keep_alive = args . keep_alive , cpu_platform = args . cpu_platform , network = args . network , subnetwork = args . subnetwork , use_private_address = args . use_private_address , accelerator_type = args . accelerator_type , accelerator_count = args . accelerator_count , nvidia_driver_version = args . nvidia_driver_version , timeout = timeout , log_interval = log_interval , ssh = args . ssh )
2451	def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )
13650	def get_fuel_price_trends ( self , latitude : float , longitude : float , fuel_types : List [ str ] ) -> PriceTrends : response = requests . post ( '{}/prices/trends/' . format ( API_URL_BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel_types ] , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return PriceTrends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average_prices = [ AveragePrice . deserialize ( avg_price ) for avg_price in data [ 'AveragePrices' ] ] )
1721	def limited ( func ) : def f ( standard = False , * * args ) : insert_pos = len ( inline_stack . names ) # in case line is longer than limit we will have to insert the lval at current position # this is because calling func will change inline_stack. # we cant use inline_stack.require here because we dont know whether line overflows yet res = func ( * * args ) if len ( res ) > LINE_LEN_LIMIT : name = inline_stack . require ( 'LONG' ) inline_stack . names . pop ( ) inline_stack . names . insert ( insert_pos , name ) res = 'def %s(var=var):\n return %s\n' % ( name , res ) inline_stack . define ( name , res ) return name + '()' else : return res f . __dict__ [ 'standard' ] = func return f
225	async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ "type" ] assert message_type in { "websocket.accept" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ "type" ] assert message_type in { "websocket.send" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call "send" once a close message has been sent.' )
2696	def get_tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word_id > 0 , graf ) ) keeps_len = len ( keeps ) for i in iter ( range ( 0 , keeps_len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps_len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
7449	def combinefiles ( filepath ) : ## unpack seq files in filepath fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] ## check names if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) ## get paired reads seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
4765	def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
3151	def update ( self , list_id , webhook_id , data ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) , data = data )
9673	def resolve ( self , context , quiet = True ) : try : obj = context for level in self . levels : if isinstance ( obj , dict ) : obj = obj [ level ] elif isinstance ( obj , list ) or isinstance ( obj , tuple ) : obj = obj [ int ( level ) ] else : if callable ( getattr ( obj , level ) ) : try : obj = getattr ( obj , level ) ( ) except KeyError : obj = getattr ( obj , level ) else : # for model field that has choice set # use get_xxx_display to access display = 'get_%s_display' % level obj = getattr ( obj , display ) ( ) if hasattr ( obj , display ) else getattr ( obj , level ) if not obj : break return obj except Exception as e : if quiet : return '' else : raise e
13182	def writerow ( self , observation_data ) : if isinstance ( observation_data , ( list , tuple ) ) : row = observation_data else : row = self . dict_to_row ( observation_data ) self . writer . writerow ( row )
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
9285	def close ( self ) : self . _connected = False self . buf = b'' if self . sock is not None : self . sock . close ( )
3018	def _generate_assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token_uri , 'scope' : self . _scopes , 'iat' : now , 'exp' : now + self . MAX_TOKEN_LIFETIME_SECS , 'iss' : self . _service_account_email , } payload . update ( self . _kwargs ) return crypt . make_signed_jwt ( self . _signer , payload , key_id = self . _private_key_id )
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , * * kwargs ) : """Check current deposit status.""" data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , * * kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
8711	def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
3389	def batch ( self , batch_size , batch_num , fluxes = True ) : for i in range ( batch_num ) : yield self . sample ( batch_size , fluxes = fluxes )
2416	def write_annotation ( annotation , out ) : out . write ( '# Annotation\n\n' ) write_value ( 'Annotator' , annotation . annotator , out ) write_value ( 'AnnotationDate' , annotation . annotation_date_iso_format , out ) if annotation . has_comment : write_text_value ( 'AnnotationComment' , annotation . comment , out ) write_value ( 'AnnotationType' , annotation . annotation_type , out ) write_value ( 'SPDXREF' , annotation . spdx_id , out )
10582	def set_parent_path ( self , value ) : self . _parent_path = value self . path = value + r'/' + self . name self . _update_childrens_parent_path ( )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] # a list of dictionaries for all the urls we can match endpoint = reverse ( 'oembed_json' ) # the public endpoint for our oembeds providers = oembed . site . get_providers ( ) for provider in providers : # first make sure this provider class is exposed at the public endpoint if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : # django providers define their regex_list by using urlreversing url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) # this regex replacement is set to be non-greedy, which results # in things like /news/*/*/*/*/ -- this is more explicit if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
5516	def append ( self , data , start ) : if self . _limit is not None and self . _limit > 0 : if self . _start is None : self . _start = start if start - self . _start > self . reset_rate : self . _sum -= round ( ( start - self . _start ) * self . _limit ) self . _start = start self . _sum += len ( data )
4581	def to_color ( c ) : if isinstance ( c , numbers . Number ) : return c , c , c if not c : raise ValueError ( 'Cannot create color from empty "%s"' % c ) if isinstance ( c , str ) : return name_to_color ( c ) if isinstance ( c , list ) : c = tuple ( c ) if isinstance ( c , tuple ) : if len ( c ) > 3 : return c [ : 3 ] while len ( c ) < 3 : c += ( c [ - 1 ] , ) return c raise ValueError ( 'Cannot create color from "%s"' % c )
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : # check by name to see if the folder already exists in the folder children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : # create the item for the subdir new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
10636	def get_element_mfrs ( self , elements = None ) : if elements is None : elements = self . material . elements result = numpy . zeros ( len ( elements ) ) for compound in self . material . compounds : result += self . get_compound_mfr ( compound ) * stoich . element_mass_fractions ( compound , elements ) return result
10054	def put ( self , pid , record ) : try : ids = [ data [ 'id' ] for data in json . loads ( request . data . decode ( 'utf-8' ) ) ] except KeyError : raise WrongFile ( ) record . files . sort_by ( * ids ) record . commit ( ) db . session . commit ( ) return self . make_response ( obj = record . files , pid = pid , record = record )
3461	def single_gene_deletion ( model , gene_list = None , method = "fba" , solution = None , processes = None , * * kwargs ) : return _multi_deletion ( model , 'gene' , element_lists = _element_lists ( model . genes , gene_list ) , method = method , solution = solution , processes = processes , * * kwargs )
365	def affine_transform_keypoints ( coords_list , transform_matrix ) : coords_result_list = [ ] for coords in coords_list : coords = np . asarray ( coords ) coords = coords . transpose ( [ 1 , 0 ] ) coords = np . insert ( coords , 2 , 1 , axis = 0 ) # print(coords) # print(transform_matrix) coords_result = np . matmul ( transform_matrix , coords ) coords_result = coords_result [ 0 : 2 , : ] . transpose ( [ 1 , 0 ] ) coords_result_list . append ( coords_result ) return coords_result_list
8880	def predict_proba ( self , X ) : # Check is fit had been called check_is_fitted ( self , [ 'tree' ] ) # Check data X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
10072	def record_schema ( self ) : schema_path = current_jsonschemas . url_to_path ( self [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path and schema_path . startswith ( schema_prefix ) : return current_jsonschemas . path_to_url ( schema_path [ len ( schema_prefix ) : ] )
6247	def get_program ( self , label : str ) -> moderngl . Program : return self . _project . get_program ( label )
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
6655	def sometimesPruneCache ( p ) : def decorator ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : r = fn ( * args , * * kwargs ) if random . random ( ) < p : pruneCache ( ) return r return wrapped return decorator
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
4858	def ignore_warning ( warning ) : def decorator ( func ) : """ Return a decorated function whose emitted warnings are ignored. """ @ wraps ( func ) def wrapper ( * args , * * kwargs ) : """ Wrap the function. """ warnings . simplefilter ( 'ignore' , warning ) return func ( * args , * * kwargs ) return wrapper return decorator
13349	def launch ( prompt_prefix = None ) : if prompt_prefix : os . environ [ 'PROMPT' ] = prompt ( prompt_prefix ) subprocess . call ( cmd ( ) , env = os . environ . data )
4024	def _get_host_only_mac_address ( ) : # Get the number of the host-only adapter vm_config = _get_vm_config ( ) for line in vm_config : if line . startswith ( 'hostonlyadapter' ) : adapter_number = int ( line [ 15 : 16 ] ) break else : raise ValueError ( 'No host-only adapter is defined for the Dusty VM' ) for line in vm_config : if line . startswith ( 'macaddress{}' . format ( adapter_number ) ) : return line . split ( '=' ) [ 1 ] . strip ( '"' ) . lower ( ) raise ValueError ( 'Could not find MAC address for adapter number {}' . format ( adapter_number ) )
10320	def _microcanonical_average_moments ( moments , alpha ) : ret = dict ( ) runs = moments . shape [ 0 ] sqrt_n = np . sqrt ( runs ) moments_sample_mean = moments . mean ( axis = 0 ) ret [ 'moments' ] = moments_sample_mean moments_sample_std = moments . std ( axis = 0 , ddof = 1 ) ret [ 'moments_ci' ] = np . empty ( ( 5 , 2 ) ) for k in range ( 5 ) : if moments_sample_std [ k ] : old_settings = np . seterr ( all = 'raise' ) ret [ 'moments_ci' ] [ k ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = moments_sample_mean [ k ] , scale = moments_sample_std [ k ] / sqrt_n ) np . seterr ( * * old_settings ) else : ret [ 'moments_ci' ] [ k ] = ( moments_sample_mean [ k ] * np . ones ( 2 ) ) return ret
12929	def as_dict ( self ) : self_as_dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref_allele' : self . ref_allele , 'alt_alleles' : self . alt_alleles , 'alleles' : [ x . as_dict ( ) for x in self . alleles ] } try : self_as_dict [ 'info' ] = self . info except AttributeError : pass return self_as_dict
2016	def _store ( self , offset , value , size = 1 ) : self . memory . write_BE ( offset , value , size ) for i in range ( size ) : self . _publish ( 'did_evm_write_memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
7797	def _register_server_authenticator ( klass , name ) : # pylint: disable-msg=W0212 SERVER_MECHANISMS_D [ name ] = klass items = sorted ( SERVER_MECHANISMS_D . items ( ) , key = _key_func , reverse = True ) SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items ] SECURE_SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items if v . _pyxmpp_sasl_secure ]
9799	def stop ( ctx , yes , pending ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not yes and not click . confirm ( "Are sure you want to stop experiments " "in group `{}`" . format ( _group ) ) : click . echo ( 'Existing without stopping experiments in group.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment_group . stop ( user , project_name , _group , pending = pending ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiments in group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments in group are being stopped." )
13261	def get_task_tree ( white_list = None ) : assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) if white_list is not None : white_list = set ( item if isinstance ( item , str ) else item . __qualname__ for item in white_list ) tree = dict ( ( task . qualified_name , task ) for task in _task_list . values ( ) if white_list is None or task . qualified_name in white_list ) plugins = get_plugin_list ( ) for plugin in [ plugin for plugin in plugins . values ( ) if white_list is None or plugin . __qualname__ in white_list ] : tasks = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . isfunction ( func ) and hasattr ( func , "yaz_task_config" ) ] if len ( tasks ) == 0 : continue node = tree for name in plugin . __qualname__ . split ( "." ) : if not name in node : node [ name ] = { } node = node [ name ] for func in tasks : logger . debug ( "Found task %s" , func ) node [ func . __name__ ] = Task ( plugin_class = plugin , func = func , config = func . yaz_task_config ) return tree
5218	def hist_file ( ticker : str , dt , typ = 'TRADE' ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return '' asset = ticker . split ( ) [ - 1 ] proper_ticker = ticker . replace ( '/' , '_' ) cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) return f'{data_path}/{asset}/{proper_ticker}/{typ}/{cur_dt}.parq'
7471	def build_tmp_h5 ( data , samples ) : ## get samples and names, sorted snames = [ i . name for i in samples ] snames . sort ( ) ## Build an array for quickly indexing consens reads from catg files. ## save as a npy int binary file. uhandle = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) ## send as first async1 job get_seeds_and_hits ( uhandle , bseeds , snames )
2341	def forward ( self , x ) : self . noise . normal_ ( ) return self . layers ( th . cat ( [ x , self . noise ] , 1 ) )
3805	def calculate_P ( self , T , P , method ) : if method == ELI_HANLEY_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley_dense ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm , Vmg ) elif method == CHUNG_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T , P ) if hasattr ( self . mug , '__call__' ) else self . mug kg = chung_dense ( T , self . MW , self . Tc , self . Vc , self . omega , Cvgm , Vmg , mug , self . dipole ) elif method == STIEL_THODOS_DENSE : kg = self . T_dependent_property ( T ) Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg kg = stiel_thodos_dense ( T , self . MW , self . Tc , self . Pc , self . Vc , self . Zc , Vmg , kg ) elif method == COOLPROP : kg = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kg = self . interpolate_P ( T , P , method ) return kg
11469	def rmdir ( self , foldername ) : current_folder = self . _ftp . pwd ( ) try : self . cd ( foldername ) except error_perm : print ( '550 Delete operation failed folder %s ' 'does not exist!' % ( foldername , ) ) else : self . cd ( current_folder ) try : self . _ftp . rmd ( foldername ) except error_perm : # folder not empty self . cd ( foldername ) contents = self . ls ( ) #delete the files map ( self . _ftp . delete , contents [ 0 ] ) #delete the subfolders map ( self . rmdir , contents [ 1 ] ) self . cd ( current_folder ) self . _ftp . rmd ( foldername )
10954	def model_to_data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set_image ( util . NullImage ( image = im ) )
4992	def transmit_content_metadata ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Transmitting content metadata to integrated channel using configuration: [%s]' , integrated_channel ) try : integrated_channel . transmit_content_metadata ( api_user ) except Exception : # pylint: disable=broad-except LOGGER . exception ( 'Transmission of content metadata failed for user [%s] and for integrated ' 'channel with code [%s] and id [%s].' , username , channel_code , channel_pk ) duration = time . time ( ) - start LOGGER . info ( 'Content metadata transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
12346	def compress ( self , delete_tif = False , folder = None ) : return compress ( self . images , delete_tif , folder )
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , # 4 = high confidence 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
1386	def set_packing_plan ( self , packing_plan ) : if not packing_plan : self . packing_plan = None self . id = None else : self . packing_plan = packing_plan self . id = packing_plan . id self . trigger_watches ( )
12152	def html_single_plot ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_plot.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDFF;">' html += '<span class="title">intrinsic properties for: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' for fname in filesByType [ 'plot' ] : html += self . htmlFor ( fname ) print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
7959	def handle_hup ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _hup = True
4048	def key_info ( self , * * kwargs ) : query_string = "/keys/{k}" . format ( k = self . api_key ) return self . _build_query ( query_string )
7883	def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qname ( name , is_element ) if namespace is None : prefix = None elif namespace in declared_prefixes : prefix = declared_prefixes [ namespace ] elif namespace in self . _prefixes : prefix = self . _prefixes [ namespace ] declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix else : if is_element : prefix = None else : prefix = self . _make_prefix ( declared_prefixes ) declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix if prefix : return prefix + u":" + name else : return name
3255	def list_granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
8098	def create ( self , stylename , * * kwargs ) : if stylename == "default" : self [ stylename ] = style ( stylename , self . _ctx , * * kwargs ) return self [ stylename ] k = kwargs . get ( "template" , "default" ) s = self [ stylename ] = self [ k ] . copy ( stylename ) for attr in kwargs : if s . __dict__ . has_key ( attr ) : s . __dict__ [ attr ] = kwargs [ attr ] return s
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
10243	def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
9982	def remove_decorator ( source : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break if node . decorator_list : deco_first = node . decorator_list [ 0 ] deco_last = node . decorator_list [ - 1 ] line_first = atok . tokens [ deco_first . first_token . index - 1 ] . start [ 0 ] line_last = atok . tokens [ deco_last . last_token . index + 1 ] . start [ 0 ] lines = lines [ : line_first - 1 ] + lines [ line_last : ] return "\n" . join ( lines ) + "\n"
348	def load_nietzsche_dataset ( path = 'data' ) : logging . info ( "Load or Download nietzsche dataset > {}" . format ( path ) ) path = os . path . join ( path , 'nietzsche' ) filename = "nietzsche.txt" url = 'https://s3.amazonaws.com/text-datasets/' filepath = maybe_download_and_extract ( filename , path , url ) with open ( filepath , "r" ) as f : words = f . read ( ) return words
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
1558	def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
5572	def is_valid_with_config ( self , config ) : validate_values ( config , [ ( "schema" , dict ) , ( "path" , str ) ] ) validate_values ( config [ "schema" ] , [ ( "properties" , dict ) , ( "geometry" , str ) ] ) if config [ "schema" ] [ "geometry" ] not in [ "Geometry" , "Point" , "MultiPoint" , "Line" , "MultiLine" , "Polygon" , "MultiPolygon" ] : raise TypeError ( "invalid geometry type" ) return True
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : #Gets equal-area sections, at sqrt(2/3) of the sides f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts # f = np.sqrt(2./3.) lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts #1. Get masks lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) #and the edge points are the x or y intercepts lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
8239	def triad ( clr , angle = 120 ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . rotate_ryb ( angle ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( - angle ) . lighten ( 0.1 ) ) return colors
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None # Input fields are not actually present on role model, and all have # to be managed as individual special-cases if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : # For teams, this is the best lookup we can do # without making the additional request for its member_role data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) # Add back fields unrelated to role lookup, such as all_pages for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
12330	def run ( cmd ) : cmd = [ pipes . quote ( c ) for c in cmd ] cmd = " " . join ( cmd ) cmd += "; exit 0" # print("Running {} in {}".format(cmd, os.getcwd())) try : output = subprocess . check_output ( cmd , stderr = subprocess . STDOUT , shell = True ) except subprocess . CalledProcessError as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) return output
11600	def get_queryset ( self , request ) : qs = super ( GalleryAdmin , self ) . get_queryset ( request ) return qs . annotate ( photo_count = Count ( 'photos' ) )
7924	def is_ipv6_available ( ) : try : socket . socket ( socket . AF_INET6 ) . close ( ) except ( socket . error , AttributeError ) : return False return True
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
1488	def save_module ( self , obj ) : self . modules . add ( obj ) self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj )
2678	def get_role_name ( region , account_id , role ) : prefix = ARN_PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account_id , role )
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
2198	def platform_data_dir ( ) : if LINUX : # nocover dpath_ = os . environ . get ( 'XDG_DATA_HOME' , '~/.local/share' ) elif DARWIN : # nocover dpath_ = '~/Library/Application Support' elif WIN32 : # nocover dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : # nocover raise '~/AppData/Local' dpath = normpath ( expanduser ( dpath_ ) ) return dpath
11971	def _detect ( ip , _isnm ) : ip = str ( ip ) if len ( ip ) > 1 : if ip [ 0 : 2 ] == '0x' : if _CHECK_FUNCT [ IP_HEX ] [ _isnm ] ( ip ) : return IP_HEX elif ip [ 0 ] == '0' : if _CHECK_FUNCT [ IP_OCT ] [ _isnm ] ( ip ) : return IP_OCT if _CHECK_FUNCT [ IP_DOT ] [ _isnm ] ( ip ) : return IP_DOT elif _isnm and _CHECK_FUNCT [ NM_BITS ] [ _isnm ] ( ip ) : return NM_BITS elif _CHECK_FUNCT [ IP_DEC ] [ _isnm ] ( ip ) : return IP_DEC elif _isnm and _CHECK_FUNCT [ NM_WILDCARD ] [ _isnm ] ( ip ) : return NM_WILDCARD elif _CHECK_FUNCT [ IP_BIN ] [ _isnm ] ( ip ) : return IP_BIN return IP_UNKNOWN
13049	def check_service ( service ) : # Try HTTP service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : # Try HTTPS try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
9106	def dropbox_editor_factory ( request ) : dropbox = dropbox_factory ( request ) if is_equal ( dropbox . editor_token , request . matchdict [ 'editor_token' ] . encode ( 'utf-8' ) ) : return dropbox else : raise HTTPNotFound ( 'invalid editor token' )
2611	def serialize_object ( obj , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : buffers = [ ] if istype ( obj , sequence_types ) and len ( obj ) < item_threshold : cobj = can_sequence ( obj ) for c in cobj : buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) elif istype ( obj , dict ) and len ( obj ) < item_threshold : cobj = { } for k in sorted ( obj ) : c = can ( obj [ k ] ) buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) cobj [ k ] = c else : cobj = can ( obj ) buffers . extend ( _extract_buffers ( cobj , buffer_threshold ) ) buffers . insert ( 0 , pickle . dumps ( cobj , PICKLE_PROTOCOL ) ) return buffers
7821	def challenge ( self , challenge ) : # pylint: disable=R0911 if not challenge : logger . debug ( "Empty challenge" ) return Failure ( "bad-challenge" ) if self . _server_first_message : return self . _final_challenge ( challenge ) match = SERVER_FIRST_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad challenge syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) self . _server_first_message = challenge mext = match . group ( "mext" ) if mext : logger . debug ( "Unsupported extension received: {0!r}" . format ( mext ) ) return Failure ( "bad-challenge" ) nonce = match . group ( "nonce" ) if not nonce . startswith ( self . _c_nonce ) : logger . debug ( "Nonce does not start with our nonce" ) return Failure ( "bad-challenge" ) salt = match . group ( "salt" ) try : salt = a2b_base64 ( salt ) except ValueError : logger . debug ( "Bad base64 encoding for salt: {0!r}" . format ( salt ) ) return Failure ( "bad-challenge" ) iteration_count = match . group ( "iteration_count" ) try : iteration_count = int ( iteration_count ) except ValueError : logger . debug ( "Bad iteration_count: {0!r}" . format ( iteration_count ) ) return Failure ( "bad-challenge" ) return self . _make_response ( nonce , salt , iteration_count )
946	def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
1645	def CheckTrailingSemicolon ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Block bodies should not be followed by a semicolon. Due to C++11 # brace initialization, there are more places where semicolons are # required than not, so we use a whitelist approach to check these # rather than a blacklist. These are the places where "};" should # be replaced by just "}": # 1. Some flavor of block following closing parenthesis: # for (;;) {}; # while (...) {}; # switch (...) {}; # Function(...) {}; # if (...) {}; # if (...) else if (...) {}; # # 2. else block: # if (...) else {}; # # 3. const member function: # Function(...) const {}; # # 4. Block following some statement: # x = 42; # {}; # # 5. Block at the beginning of a function: # Function(...) { # {}; # } # # Note that naively checking for the preceding "{" will also match # braces inside multi-dimensional arrays, but this is fine since # that expression will not contain semicolons. # # 6. Block following another block: # while (true) {} # {}; # # 7. End of namespaces: # namespace {}; # # These semicolons seems far more common than other kinds of # redundant semicolons, possibly due to people converting classes # to namespaces. For now we do not warn for this case. # # Try matching case 1 first. match = Match ( r'^(.*\)\s*)\{' , line ) if match : # Matched closing parenthesis (case 1). Check the token before the # matching opening parenthesis, and don't warn if it looks like a # macro. This avoids these false positives: # - macro that defines a base class # - multi-line macro that defines a base class # - macro that defines the whole class-head # # But we still issue warnings for macros that we know are safe to # warn, specifically: # - TEST, TEST_F, TEST_P, MATCHER, MATCHER_P # - TYPED_TEST # - INTERFACE_DEF # - EXCLUSIVE_LOCKS_REQUIRED, SHARED_LOCKS_REQUIRED, LOCKS_EXCLUDED: # # We implement a whitelist of safe macros instead of a blacklist of # unsafe macros, even though the latter appears less frequently in # google code and would have been easier to implement. This is because # the downside for getting the whitelist wrong means some extra # semicolons, while the downside for getting the blacklist wrong # would result in compile errors. # # In addition to macros, we also don't want to warn on # - Compound literals # - Lambdas # - alignas specifier with anonymous structs # - decltype closing_brace_pos = match . group ( 1 ) . rfind ( ')' ) opening_parenthesis = ReverseCloseExpression ( clean_lines , linenum , closing_brace_pos ) if opening_parenthesis [ 2 ] > - 1 : line_prefix = opening_parenthesis [ 0 ] [ 0 : opening_parenthesis [ 2 ] ] macro = Search ( r'\b([A-Z_][A-Z0-9_]*)\s*$' , line_prefix ) func = Match ( r'^(.*\])\s*$' , line_prefix ) if ( ( macro and macro . group ( 1 ) not in ( 'TEST' , 'TEST_F' , 'MATCHER' , 'MATCHER_P' , 'TYPED_TEST' , 'EXCLUSIVE_LOCKS_REQUIRED' , 'SHARED_LOCKS_REQUIRED' , 'LOCKS_EXCLUDED' , 'INTERFACE_DEF' ) ) or ( func and not Search ( r'\boperator\s*\[\s*\]' , func . group ( 1 ) ) ) or Search ( r'\b(?:struct|union)\s+alignas\s*$' , line_prefix ) or Search ( r'\bdecltype$' , line_prefix ) or Search ( r'\s+=\s*$' , line_prefix ) ) : match = None if ( match and opening_parenthesis [ 1 ] > 1 and Search ( r'\]\s*$' , clean_lines . elided [ opening_parenthesis [ 1 ] - 1 ] ) ) : # Multi-line lambda-expression match = None else : # Try matching cases 2-3. match = Match ( r'^(.*(?:else|\)\s*const)\s*)\{' , line ) if not match : # Try matching cases 4-6. These are always matched on separate lines. # # Note that we can't simply concatenate the previous line to the # current line and do a single match, otherwise we may output # duplicate warnings for the blank line case: # if (cond) { # // blank line # } prevline = GetPreviousNonBlankLine ( clean_lines , linenum ) [ 0 ] if prevline and Search ( r'[;{}]\s*$' , prevline ) : match = Match ( r'^(\s*)\{' , line ) # Check matching closing brace if match : ( endline , endlinenum , endpos ) = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if endpos > - 1 and Match ( r'^\s*;' , endline [ endpos : ] ) : # Current {} pair is eligible for semicolon check, and we have found # the redundant semicolon, output warning here. # # Note: because we are scanning forward for opening braces, and # outputting warnings for the matching closing brace, if there are # nested blocks with trailing semicolons, we will get the error # messages in reversed order. # We need to check the line forward for NOLINT raw_lines = clean_lines . raw_lines ParseNolintSuppressions ( filename , raw_lines [ endlinenum - 1 ] , endlinenum - 1 , error ) ParseNolintSuppressions ( filename , raw_lines [ endlinenum ] , endlinenum , error ) error ( filename , endlinenum , 'readability/braces' , 4 , "You don't need a ; after a }" )
8964	def _get_registered_executable ( exe_name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe_name ) [ 1 ] . lower ( ) != '.exe' : exe_name += '.exe' import _winreg # pylint: disable=import-error try : key = "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\" + exe_name value = _winreg . QueryValue ( _winreg . HKEY_LOCAL_MACHINE , key ) registered = ( value , "from HKLM\\" + key ) except _winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
9157	def stroke_linejoin ( self , linejoin ) : linejoin = getattr ( pgmagick . LineJoin , "%sJoin" % linejoin . title ( ) ) linejoin = pgmagick . DrawableStrokeLineJoin ( linejoin ) self . drawer . append ( linejoin )
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
11865	def pointwise_product ( self , other , bn ) : vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
6310	def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
6664	def list_expiration_dates ( self , base = 'roles/all/ssl' ) : max_fn_len = 0 max_date_len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration_date = self . get_expiration_date ( fqfn ) max_fn_len = max ( max_fn_len , len ( fn ) ) max_date_len = max ( max_date_len , len ( str ( expiration_date ) ) ) data . append ( ( fn , expiration_date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max_fn_len ) , 'Expiration Date' . ljust ( max_date_len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max_fn_len ) , str ( dt ) . ljust ( max_date_len ) , expired ) )
5217	def check_hours ( tickers , tz_exch , tz_loc = DEFAULT_TZ ) -> pd . DataFrame : cols = [ 'Trading_Day_Start_Time_EOD' , 'Trading_Day_End_Time_EOD' ] con , _ = create_connection ( ) hours = con . ref ( tickers = tickers , flds = cols ) cur_dt = pd . Timestamp ( 'today' ) . strftime ( '%Y-%m-%d ' ) hours . loc [ : , 'local' ] = hours . value . astype ( str ) . str [ : - 3 ] hours . loc [ : , 'exch' ] = pd . DatetimeIndex ( cur_dt + hours . value . astype ( str ) ) . tz_localize ( tz_loc ) . tz_convert ( tz_exch ) . strftime ( '%H:%M' ) hours = pd . concat ( [ hours . set_index ( [ 'ticker' , 'field' ] ) . exch . unstack ( ) . loc [ : , cols ] , hours . set_index ( [ 'ticker' , 'field' ] ) . local . unstack ( ) . loc [ : , cols ] , ] , axis = 1 ) hours . columns = [ 'Exch_Start' , 'Exch_End' , 'Local_Start' , 'Local_End' ] return hours
6966	def initialize ( self , executor , secret ) : self . executor = executor self . secret = secret
3880	async def _handle_watermark_notification ( self , watermark_notification ) : conv_id = watermark_notification . conversation_id . id res = parsers . parse_watermark_notification ( watermark_notification ) await self . on_watermark_notification . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for watermark notification: %s' , conv_id ) else : await conv . on_watermark_notification . fire ( res )
9742	def send_command ( self , command , callback = True , command_type = QRTPacketType . PacketCommand ) : if self . transport is not None : cmd_length = len ( command ) LOG . debug ( "S: %s" , command ) self . transport . write ( struct . pack ( RTCommand % cmd_length , RTheader . size + cmd_length + 1 , command_type . value , command . encode ( ) , b"\0" , ) ) future = self . loop . create_future ( ) if callback : self . request_queue . append ( future ) else : future . set_result ( None ) return future raise QRTCommandException ( "Not connected!" )
6311	def from_single ( cls , meta : ProgramDescription , source : str ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , source ) if GEOMETRY_SHADER in source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , source , ) if FRAGMENT_SHADER in source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , source , ) if TESS_CONTROL_SHADER in source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , source , ) if TESS_EVALUATION_SHADER in source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_evaluation_shader , source , ) return instance
2590	def stage_in ( self , file , executor ) : if file . scheme == 'ftp' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _ftp_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'http' or file . scheme == 'https' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _http_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_in_app = self . _globus_stage_in_app ( ) app_fut = stage_in_app ( globus_ep , outputs = [ file ] ) return app_fut . _outputs [ 0 ] else : raise Exception ( 'Staging in with unknown file scheme {} is not supported' . format ( file . scheme ) )
8664	def _build_dict_from_key_value ( keys_and_values ) : key_dict = { } for key_value in keys_and_values : if '=' not in key_value : raise GhostError ( 'Pair {0} is not of `key=value` format' . format ( key_value ) ) key , value = key_value . split ( '=' , 1 ) key_dict . update ( { str ( key ) : str ( value ) } ) return key_dict
5553	def _validate_zooms ( zooms ) : if isinstance ( zooms , dict ) : if any ( [ a not in zooms for a in [ "min" , "max" ] ] ) : raise MapcheteConfigError ( "min and max zoom required" ) zmin = _validate_zoom ( zooms [ "min" ] ) zmax = _validate_zoom ( zooms [ "max" ] ) if zmin > zmax : raise MapcheteConfigError ( "max zoom must not be smaller than min zoom" ) return list ( range ( zmin , zmax + 1 ) ) elif isinstance ( zooms , list ) : if len ( zooms ) == 1 : return zooms elif len ( zooms ) == 2 : zmin , zmax = sorted ( [ _validate_zoom ( z ) for z in zooms ] ) return list ( range ( zmin , zmax + 1 ) ) else : return zooms else : return [ _validate_zoom ( zooms ) ]
2857	def read ( self , length ) : #check for hardware limit of FT232H and similar MPSSE chips if ( 1 > length > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) # Build command to read SPI data. command = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) logger . debug ( 'SPI read with command {0:2X}.' . format ( command ) ) # Compute length low and high bytes. # NOTE: Must actually send length minus one because the MPSSE engine # considers 0 a length of 1 and FFFF a length of 65536 #force odd numbers to round up instead of down lengthR = length if length % 2 == 1 : lengthR += 1 lengthR = lengthR / 2 #when odd length requested, get the remainder instead of the same number lenremain = length - lengthR len_low = ( lengthR - 1 ) & 0xFF len_high = ( ( lengthR - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) # Send command and length. # Perform twice to prevent error from hardware defect/limits self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload1 = self . _ft232h . _poll_read ( lengthR ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) # Read response bytes return bytearray ( payload1 + payload2 )
7845	def get_items ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:item" ) if l is not None : for i in l : ret . append ( DiscoItem ( self , i ) ) return ret
5310	def translate_rgb_to_ansi_code ( red , green , blue , offset , colormode ) : if colormode == terminal . NO_COLORS : # colors are disabled, thus return empty string return '' , '' if colormode == terminal . ANSI_8_COLORS or colormode == terminal . ANSI_16_COLORS : color_code = ansi . rgb_to_ansi16 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = color_code + offset - ansi . FOREGROUND_COLOR_OFFSET ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . ANSI_256_COLORS : color_code = ansi . rgb_to_ansi256 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};5;{code}' . format ( base = 8 + offset , code = color_code ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . TRUE_COLORS : start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};2;{red};{green};{blue}' . format ( base = 8 + offset , red = red , green = green , blue = blue ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code raise ColorfulError ( 'invalid color mode "{0}"' . format ( colormode ) )
11941	def mark_all_read ( user ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_purge ( user )
12706	def state ( self , state ) : assert self . name == state . name , 'state name "{}" != body name "{}"' . format ( state . name , self . name ) self . position = state . position self . quaternion = state . quaternion self . linear_velocity = state . linear_velocity self . angular_velocity = state . angular_velocity
8161	def next_event ( block = False , timeout = None ) : try : return channel . listen ( block = block , timeout = timeout ) . next ( ) [ 'data' ] except StopIteration : return None
9359	def paragraphs ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : if html : wrap_start = '<p>' wrap_end = '</p>' separator = '\n\n' result = [ ] try : for _ in xrange ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) # Python 3 compatibility except NameError : for _ in range ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) if as_list : return result else : return separator . join ( result )
3166	def cancel ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/cancel-send' ) )
6263	def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
3733	def charge ( self ) : try : return self . _charge except AttributeError : self . _charge = charge_from_formula ( self . formula ) return self . _charge
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
12605	def _to_string ( data ) : sdata = data . copy ( ) for k , v in data . items ( ) : if isinstance ( v , datetime ) : sdata [ k ] = timestamp_to_date_str ( v ) elif not isinstance ( v , ( string_types , float , int ) ) : sdata [ k ] = str ( v ) return sdata
8198	def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
11271	def to_str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
973	def _slowIsSegmentActive ( self , seg , timeStep ) : numSyn = seg . size ( ) numActiveSyns = 0 for synIdx in xrange ( numSyn ) : if seg . getPermanence ( synIdx ) < self . connectedPerm : continue sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( synIdx ) ) if self . infActiveState [ timeStep ] [ sc , si ] : numActiveSyns += 1 if numActiveSyns >= self . activationThreshold : return True return numActiveSyns >= self . activationThreshold
5225	def flatten ( iterable , maps = None , unique = False ) -> list : if iterable is None : return [ ] if maps is None : maps = dict ( ) if isinstance ( iterable , ( str , int , float ) ) : return [ maps . get ( iterable , iterable ) ] else : x = [ maps . get ( item , item ) for item in _to_gen_ ( iterable ) ] return list ( set ( x ) ) if unique else x
7040	def list_recent_datasets ( lcc_server , nrecent = 25 ) : urlparams = { 'nsets' : nrecent } urlqs = urlencode ( urlparams ) url = '%s/api/datasets?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting list of recent publicly ' 'visible and owned datasets from %s' % ( lcc_server , ) ) # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # if apikey is not None, add it in as an Authorization: Bearer [apikey] # header if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } # hit the server req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) recent_datasets = json . loads ( resp . read ( ) ) [ 'result' ] return recent_datasets except HTTPError as e : LOGERROR ( 'could not retrieve recent datasets list, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
8111	def search_blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_BLOGS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
13251	async def process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) github_url = ltd_product_data [ 'doc_repo' ] github_url = normalize_repo_root_url ( github_url ) repo_slug = parse_repo_slug_from_url ( github_url ) try : metadata_yaml = await download_metadata_yaml ( session , github_url ) except aiohttp . ClientResponseError as err : # metadata.yaml not found; probably not a Sphinx technote logger . debug ( 'Tried to download %s\'s metadata.yaml, got status %d' , ltd_product_data [ 'slug' ] , err . code ) raise NotSphinxTechnoteError ( ) # Extract data from the GitHub API github_query = GitHubQuery . load ( 'technote_repo' ) github_variables = { "orgName" : repo_slug . owner , "repoName" : repo_slug . repo } github_data = await github_request ( session , github_api_token , query = github_query , variables = github_variables ) try : jsonld = reduce_technote_metadata ( github_url , metadata_yaml , github_data , ltd_product_data ) except Exception as exception : message = "Issue building JSON-LD for technote %s" logger . exception ( message , github_url , exception ) raise if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , jsonld ) logger . info ( 'Ingested technote %s into MongoDB' , github_url ) return jsonld
6142	def DSP_capture_add_samples ( self , new_data ) : self . capture_sample_count += len ( new_data ) if self . Tcapture > 0 : self . data_capture = np . hstack ( ( self . data_capture , new_data ) ) if ( self . Tcapture > 0 ) and ( len ( self . data_capture ) > self . Ncapture ) : self . data_capture = self . data_capture [ - self . Ncapture : ]
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
6692	def invalidate ( self , * paths ) : dj = self . get_satchel ( 'dj' ) if not paths : return # http://boto.readthedocs.org/en/latest/cloudfront_tut.html _settings = dj . get_settings ( ) if not _settings . AWS_STATIC_BUCKET_NAME : print ( 'No static media bucket set.' ) return if isinstance ( paths , six . string_types ) : paths = paths . split ( ',' ) all_paths = map ( str . strip , paths ) i = 0 while 1 : paths = all_paths [ i : i + 1000 ] if not paths : break c = boto . connect_cloudfront ( ) rs = c . get_all_distributions ( ) target_dist = None for dist in rs : print ( dist . domain_name , dir ( dist ) , dist . __dict__ ) bucket_name = dist . origin . dns_name . replace ( '.s3.amazonaws.com' , '' ) if bucket_name == _settings . AWS_STATIC_BUCKET_NAME : target_dist = dist break if not target_dist : raise Exception ( ( 'Target distribution %s could not be found in the AWS account.' ) % ( settings . AWS_STATIC_BUCKET_NAME , ) ) print ( 'Using distribution %s associated with origin %s.' % ( target_dist . id , _settings . AWS_STATIC_BUCKET_NAME ) ) inval_req = c . create_invalidation_request ( target_dist . id , paths ) print ( 'Issue invalidation request %s.' % ( inval_req , ) ) i += 1000
1751	def scan_mem ( self , data_to_find ) : # TODO: for the moment we just treat symbolic bytes as bytes that don't match. # for our simple test cases right now, the bytes we're interested in scanning # for will all just be there concretely # TODO: Can probably do something smarter here like Boyer-Moore, but unnecessary # if we're looking for short strings. # Querying mem with an index returns [bytes] if isinstance ( data_to_find , bytes ) : data_to_find = [ bytes ( [ c ] ) for c in data_to_find ] for mapping in sorted ( self . maps ) : for ptr in mapping : if ptr + len ( data_to_find ) >= mapping . end : break candidate = mapping [ ptr : ptr + len ( data_to_find ) ] # TODO: treat symbolic bytes as bytes that don't match. for our simple tests right now, the # bytes will be there concretely if issymbolic ( candidate [ 0 ] ) : break if candidate == data_to_find : yield ptr
13616	def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url )
4914	def course_enrollments ( self , request , pk ) : enterprise_customer = self . get_object ( ) serializer = serializers . EnterpriseCustomerCourseEnrollmentsSerializer ( data = request . data , many = True , context = { 'enterprise_customer' : enterprise_customer , 'request_user' : request . user , } ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP_200_OK ) return Response ( serializer . errors , status = HTTP_400_BAD_REQUEST )
5531	def get_process_tiles ( self , zoom = None ) : if zoom or zoom == 0 : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile else : for zoom in reversed ( self . config . zoom_levels ) : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile
8100	def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
10542	def find_tasks ( project_id , * * kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'task' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : # pragma: no cover raise
2179	def fetch_request_token ( self , url , realm = None , * * request_kwargs ) : self . _client . client . realm = " " . join ( realm ) if realm else None token = self . _fetch_token ( url , * * request_kwargs ) log . debug ( "Resetting callback_uri and realm (not needed in next phase)." ) self . _client . client . callback_uri = None self . _client . client . realm = None return token
5974	def isMine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix
12920	def reload ( self ) : if len ( self ) == 0 : return [ ] ret = [ ] for obj in self : res = None try : res = obj . reload ( ) except Exception as e : res = e ret . append ( res ) return ret
6959	def query_radecl ( ra , decl , filtersystem = 'sloan_2mass' , field_deg2 = 1.0 , usebinaries = True , extinction_sigma = 0.1 , magnitude_limit = 26.0 , maglim_filtercol = 4 , trilegal_version = 1.6 , extraparams = None , forcefetch = False , cachedir = '~/.astrobase/trilegal-cache' , verbose = True , timeout = 60.0 , refresh = 150.0 , maxtimeout = 700.0 ) : # convert the ra/decl to gl, gb radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return query_galcoords ( gl , gb , filtersystem = filtersystem , field_deg2 = field_deg2 , usebinaries = usebinaries , extinction_sigma = extinction_sigma , magnitude_limit = magnitude_limit , maglim_filtercol = maglim_filtercol , trilegal_version = trilegal_version , extraparams = extraparams , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout )
1768	def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
443	def _get_init_args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise ValueError ( "The length of the inspection stack is shorter than the requested start position." ) args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : # some args dont need to be saved into the graph. e.g. the input placeholder if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : val = values [ arg ] # change function (e.g. act) into dictionary of module path and function name if inspect . isfunction ( val ) : params [ arg ] = { "module_path" : val . __module__ , "func_name" : val . __name__ } # ignore more args e.g. TF class elif arg . endswith ( 'init' ) : continue # for other data type, save them directly else : params [ arg ] = val return params
13669	def check_readable ( self , timeout ) : rlist , wlist , xlist = select . select ( [ self . _stdout ] , [ ] , [ ] , timeout ) return bool ( len ( rlist ) )
5035	def get_pending_users_queryset ( self , search_keyword , customer_uuid ) : queryset = PendingEnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) if search_keyword is not None : queryset = queryset . filter ( user_email__icontains = search_keyword ) return queryset
1986	def load_stream ( self , key , binary = False ) : value = self . load_value ( key , binary = binary ) yield io . BytesIO ( value ) if binary else io . StringIO ( value )
13497	def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
6238	def add_point_light ( self , position , radius ) : self . point_lights . append ( PointLight ( position , radius ) )
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
2177	def authorized ( self ) : if self . _client . client . signature_method == SIGNATURE_RSA : # RSA only uses resource_owner_key return bool ( self . _client . client . resource_owner_key ) else : # other methods of authentication use all three pieces return ( bool ( self . _client . client . client_secret ) and bool ( self . _client . client . resource_owner_key ) and bool ( self . _client . client . resource_owner_secret ) )
3611	def delete_async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_delete_request , args = ( endpoint , params , headers ) , callback = callback )
7232	def get ( self , ID , index = 'vector-web-s' ) : url = self . get_url % index r = self . gbdx_connection . get ( url + ID ) r . raise_for_status ( ) return r . json ( )
13502	def extra_context ( request ) : host = os . environ . get ( 'DJANGO_LIVE_TEST_SERVER_ADDRESS' , None ) or request . get_host ( ) d = { 'request' : request , 'HOST' : host , 'IN_ADMIN' : request . path . startswith ( '/admin/' ) , } return d
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : if verbose is None : # nocover verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : # nocover dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : # nocover print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : # nocover os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : # nocover print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
11900	def _get_thumbnail_image_from_file ( dir_path , image_file ) : # Get image img = _get_image_from_file ( dir_path , image_file ) # If it's not supported, exit now if img is None : return None if img . format . lower ( ) == 'gif' : return None # Get image dimensions img_width , img_height = img . size # We need to perform a resize - first, work out the scale ratio to take the # image width to THUMBNAIL_WIDTH (THUMBNAIL_WIDTH:img_width ratio) scale_ratio = THUMBNAIL_WIDTH / float ( img_width ) # Work out target image height based on the scale ratio target_height = int ( scale_ratio * img_height ) # Perform the resize try : img . thumbnail ( ( THUMBNAIL_WIDTH , target_height ) , resample = RESAMPLE ) except IOError as exptn : print ( 'WARNING: IOError when thumbnailing %s/%s: %s' % ( dir_path , image_file , exptn ) ) return None # Return the resized image return img
6953	def bootstrap_falsealarmprob ( lspinfo , times , mags , errs , nbootstrap = 250 , magsarefluxes = False , sigclip = 10.0 , npeaks = None ) : # figure out how many periods to work on if ( npeaks and ( 0 < npeaks < len ( lspinfo [ 'nbestperiods' ] ) ) ) : nperiods = npeaks else : LOGWARNING ( 'npeaks not specified or invalid, ' 'getting FAP for all %s periodogram peaks' % len ( lspinfo [ 'nbestperiods' ] ) ) nperiods = len ( lspinfo [ 'nbestperiods' ] ) nbestperiods = lspinfo [ 'nbestperiods' ] [ : nperiods ] nbestpeaks = lspinfo [ 'nbestlspvals' ] [ : nperiods ] # get rid of nans first and sigclip stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) allpeaks = [ ] allperiods = [ ] allfaps = [ ] alltrialbestpeaks = [ ] # make sure there are enough points to calculate a spectrum if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : for ind , period , peak in zip ( range ( len ( nbestperiods ) ) , nbestperiods , nbestpeaks ) : LOGINFO ( 'peak %s: running %s trials...' % ( ind + 1 , nbootstrap ) ) trialbestpeaks = [ ] for _trial in range ( nbootstrap ) : # get a scrambled index tindex = np . random . randint ( 0 , high = mags . size , size = mags . size ) # get the kwargs dict out of the lspinfo if 'kwargs' in lspinfo : kwargs = lspinfo [ 'kwargs' ] # update the kwargs with some local stuff kwargs . update ( { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } ) else : kwargs = { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } # run the periodogram with scrambled mags and errs # and the appropriate keyword arguments lspres = LSPMETHODS [ lspinfo [ 'method' ] ] ( times , mags [ tindex ] , errs [ tindex ] , * * kwargs ) trialbestpeaks . append ( lspres [ 'bestlspval' ] ) trialbestpeaks = np . array ( trialbestpeaks ) alltrialbestpeaks . append ( trialbestpeaks ) # calculate the FAP for a trial peak j = FAP[j] = # (1.0 + sum(trialbestpeaks[i] > peak[j]))/(ntrialbestpeaks + 1) if lspinfo [ 'method' ] != 'pdm' : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks > peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) # for PDM, we're looking for a peak smaller than the best peak # because values closer to 0.0 are more significant else : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks < peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) LOGINFO ( 'FAP for peak %s, period: %.6f = %.3g' % ( ind + 1 , period , falsealarmprob ) ) allpeaks . append ( peak ) allperiods . append ( period ) allfaps . append ( falsealarmprob ) return { 'peaks' : allpeaks , 'periods' : allperiods , 'probabilities' : allfaps , 'alltrialbestpeaks' : alltrialbestpeaks } else : LOGERROR ( 'not enough mag series points to calculate periodogram' ) return None
7552	def _getbins ( ) : # Return error if system is 32-bit arch. # This is straight from the python docs: # https://docs.python.org/2/library/platform.html#cross-platform if not _sys . maxsize > 2 ** 32 : _sys . exit ( "ipyrad requires 64bit architecture" ) ## get platform mac or linux _platform = _sys . platform ## get current location if 'VIRTUAL_ENV' in _os . environ : ipyrad_path = _os . environ [ 'VIRTUAL_ENV' ] else : path = _os . path . abspath ( _os . path . dirname ( __file__ ) ) ipyrad_path = _os . path . dirname ( path ) ## find bin directory ipyrad_path = _os . path . dirname ( path ) bin_path = _os . path . join ( ipyrad_path , "bin" ) ## get the correct binaries if 'linux' in _platform : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-linux-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-linux-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-linux-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-linux-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-linux-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-linux-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-linux-x86_64" ) else : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-osx-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-osx-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-osx-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-osx-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-osx-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-osx-x86_64" ) ## only one compiled version available, works for all? qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-osx-x86_64" ) # Test for existence of binaries assert _cmd_exists ( muscle ) , "muscle not found here: " + muscle assert _cmd_exists ( vsearch ) , "vsearch not found here: " + vsearch assert _cmd_exists ( smalt ) , "smalt not found here: " + smalt assert _cmd_exists ( bwa ) , "bwa not found here: " + bwa assert _cmd_exists ( samtools ) , "samtools not found here: " + samtools assert _cmd_exists ( bedtools ) , "bedtools not found here: " + bedtools #assert _cmd_exists(qmc), "wQMC not found here: "+qmc return vsearch , muscle , smalt , bwa , samtools , bedtools , qmc
7362	async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . response = await self . _connect ( ) if self . response . status in range ( 200 , 300 ) : self . _error_timeout = 0 self . state = NORMAL elif self . response . status == 500 : self . state = DISCONNECTION elif self . response . status in range ( 501 , 600 ) : self . state = RECONNECTION elif self . response . status in ( 420 , 429 ) : self . state = ENHANCE_YOUR_CALM else : logger . debug ( "raising error during stream connection" ) raise await exceptions . throw ( self . response , loads = self . client . _loads , url = self . kwargs [ 'url' ] ) logger . debug ( "stream state: %d" % self . state )
13799	def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
13508	def create_position ( self , params = { } ) : url = "/2/positions/" body = params data = self . _post_resource ( url , body ) return self . position_from_json ( data [ "position" ] )
3658	def _destroy_image_acquirer ( self , ia ) : id_ = None if ia . device : # ia . stop_image_acquisition ( ) # ia . _release_data_streams ( ) # id_ = ia . _device . id_ # if ia . device . node_map : # if ia . _chunk_adapter : ia . _chunk_adapter . detach_buffer ( ) ia . _chunk_adapter = None self . _logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id_ ) ) ia . device . node_map . disconnect ( ) self . _logger . info ( 'Disconnected the port from the NodeMap of {0}.' . format ( id_ ) ) # if ia . _device . is_open ( ) : ia . _device . close ( ) self . _logger . info ( 'Closed Device module, {0}.' . format ( id_ ) ) ia . _device = None # if id_ : self . _logger . info ( 'Destroyed the ImageAcquirer object which {0} ' 'had belonged to.' . format ( id_ ) ) else : self . _logger . info ( 'Destroyed an ImageAcquirer.' ) if self . _profiler : self . _profiler . print_diff ( ) self . _ias . remove ( ia )
8777	def _chunks ( self , iterable , chunk_size ) : iterator = iter ( iterable ) chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) ) while chunk : yield chunk chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) )
11781	def compare ( algorithms = [ PluralityLearner , NaiveBayesLearner , NearestNeighborLearner , DecisionTreeLearner ] , datasets = [ iris , orings , zoo , restaurant , SyntheticRestaurant ( 20 ) , Majority ( 7 , 100 ) , Parity ( 7 , 100 ) , Xor ( 100 ) ] , k = 10 , trials = 1 ) : print_table ( [ [ a . __name__ . replace ( 'Learner' , '' ) ] + [ cross_validation ( a , d , k , trials ) for d in datasets ] for a in algorithms ] , header = [ '' ] + [ d . name [ 0 : 7 ] for d in datasets ] , numfmt = '%.2f' )
6202	def merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) : ts = np . hstack ( [ ts_d , ts_a ] ) ts_par = np . hstack ( [ ts_par_d , ts_par_a ] ) a_ch = np . hstack ( [ np . zeros ( ts_d . shape [ 0 ] , dtype = bool ) , np . ones ( ts_a . shape [ 0 ] , dtype = bool ) ] ) index_sort = ts . argsort ( ) return ts [ index_sort ] , a_ch [ index_sort ] , ts_par [ index_sort ]
1638	def CheckSpacing ( filename , clean_lines , linenum , nesting_state , error ) : # Don't use "elided" lines here, otherwise we can't check commented lines. # Don't want to use "raw" either, because we don't want to check inside C++11 # raw strings, raw = clean_lines . lines_without_raw_strings line = raw [ linenum ] # Before nixing comments, check if the line is blank for no good # reason. This includes the first line after a block is opened, and # blank lines at the end of a function (ie, right before a line like '}' # # Skip all the blank line checks if we are immediately inside a # namespace body. In other words, don't issue blank line warnings # for this block: # namespace { # # } # # A warning about missing end of namespace comments will be issued instead. # # Also skip blank line checks for 'extern "C"' blocks, which are formatted # like namespaces. if ( IsBlankLine ( line ) and not nesting_state . InNamespaceBody ( ) and not nesting_state . InExternC ( ) ) : elided = clean_lines . elided prev_line = elided [ linenum - 1 ] prevbrace = prev_line . rfind ( '{' ) # TODO(unknown): Don't complain if line before blank line, and line after, # both start with alnums and are indented the same amount. # This ignores whitespace at the start of a namespace block # because those are not usually indented. if prevbrace != - 1 and prev_line [ prevbrace : ] . find ( '}' ) == - 1 : # OK, we have a blank line at the start of a code block. Before we # complain, we check if it is an exception to the rule: The previous # non-empty line has the parameters of a function header that are indented # 4 spaces (because they did not fit in a 80 column line when placed on # the same line as the function name). We also check for the case where # the previous line is indented 6 spaces, which may happen when the # initializers of a constructor do not fit into a 80 column line. exception = False if Match ( r' {6}\w' , prev_line ) : # Initializer list? # We are looking for the opening column of initializer list, which # should be indented 4 spaces to cause 6 space indentation afterwards. search_position = linenum - 2 while ( search_position >= 0 and Match ( r' {6}\w' , elided [ search_position ] ) ) : search_position -= 1 exception = ( search_position >= 0 and elided [ search_position ] [ : 5 ] == ' :' ) else : # Search for the function arguments or an initializer list. We use a # simple heuristic here: If the line is indented 4 spaces; and we have a # closing paren, without the opening paren, followed by an opening brace # or colon (for initializer lists) we assume that it is the last line of # a function header. If we have a colon indented 4 spaces, it is an # initializer list. exception = ( Match ( r' {4}\w[^\(]*\)\s*(const\s*)?(\{\s*$|:)' , prev_line ) or Match ( r' {4}:' , prev_line ) ) if not exception : error ( filename , linenum , 'whitespace/blank_line' , 2 , 'Redundant blank line at the start of a code block ' 'should be deleted.' ) # Ignore blank lines at the end of a block in a long if-else # chain, like this: # if (condition1) { # // Something followed by a blank line # # } else if (condition2) { # // Something else # } if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] if ( next_line and Match ( r'\s*}' , next_line ) and next_line . find ( '} else ' ) == - 1 ) : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Redundant blank line at the end of a code block ' 'should be deleted.' ) matched = Match ( r'\s*(public|protected|private):' , prev_line ) if matched : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Do not leave a blank line after "%s:"' % matched . group ( 1 ) ) # Next, check comments next_line_start = 0 if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] next_line_start = len ( next_line ) - len ( next_line . lstrip ( ) ) CheckComment ( line , filename , linenum , next_line_start , error ) # get rid of comments and strings line = clean_lines . elided [ linenum ] # You shouldn't have spaces before your brackets, except maybe after # 'delete []' or 'return []() {};' if Search ( r'\w\s+\[' , line ) and not Search ( r'(?:delete|return)\s+\[' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Extra space before [' ) # In range-based for, we wanted spaces before and after the colon, but # not around "::" tokens that might appear. if ( Search ( r'for *\(.*[^:]:[^: ]' , line ) or Search ( r'for *\(.*[^: ]:[^:]' , line ) ) : error ( filename , linenum , 'whitespace/forcolon' , 2 , 'Missing space around colon in range-based for loop' )
6908	def galactic_to_equatorial ( gl , gb ) : gal = SkyCoord ( gl * u . degree , gl * u . degree , frame = 'galactic' ) transformed = gal . transform_to ( 'icrs' ) return transformed . ra . degree , transformed . dec . degree
8497	def _output ( calls , args ) : # Sort the keys appropriately if args . natural_sort or args . source : calls = sorted ( calls , key = lambda c : ( c . filename , c . lineno ) ) else : calls = sorted ( calls , key = lambda c : c . key ) out = [ ] # Handle displaying only the list of keys if args . only_keys : keys = set ( ) for call in calls : if call . key in keys : continue out . append ( _format_call ( call , args ) ) keys . add ( call . key ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' ) # We're done here return # Build a list of keys which have default values available, so that we can # toggle between displaying only those keys with defaults and all keys keys = set ( ) for call in calls : if call . default : keys . add ( call . key ) for call in calls : if not args . all and not call . default and call . key in keys : continue out . append ( _format_call ( call , args ) ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' )
10324	def microcanonical_averages_arrays ( microcanonical_averages ) : ret = dict ( ) for n , microcanonical_average in enumerate ( microcanonical_averages ) : assert n == microcanonical_average [ 'n' ] if n == 0 : num_edges = microcanonical_average [ 'M' ] num_sites = microcanonical_average [ 'N' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_average ) ret [ 'max_cluster_size' ] = np . empty ( num_edges + 1 ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( num_edges + 1 ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , num_edges + 1 ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , num_edges + 1 , 2 ) ) ret [ 'max_cluster_size' ] [ n ] = microcanonical_average [ 'max_cluster_size' ] ret [ 'max_cluster_size_ci' ] [ n ] = ( microcanonical_average [ 'max_cluster_size_ci' ] ) if spanning_cluster : ret [ 'spanning_cluster' ] [ n ] = ( microcanonical_average [ 'spanning_cluster' ] ) ret [ 'spanning_cluster_ci' ] [ n ] = ( microcanonical_average [ 'spanning_cluster_ci' ] ) ret [ 'moments' ] [ : , n ] = microcanonical_average [ 'moments' ] ret [ 'moments_ci' ] [ : , n ] = microcanonical_average [ 'moments_ci' ] # normalize by number of sites for key in ret : if 'spanning_cluster' in key : continue ret [ key ] /= num_sites ret [ 'M' ] = num_edges ret [ 'N' ] = num_sites return ret
2824	def convert_sigmoid ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting sigmoid ...' ) if names == 'short' : tf_name = 'SIGM' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sigmoid = keras . layers . Activation ( 'sigmoid' , name = tf_name ) layers [ scope_name ] = sigmoid ( layers [ inputs [ 0 ] ] )
6745	def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer #or render_remote_paths env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue # Only load site configurations that are allowed for this host. if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data # Revert modified keys. env . update ( env_default ) # Remove keys that were added, not simply updated. added_keys = set ( env ) . difference ( env_default ) for key in added_keys : # Don't remove internally maintained variables, because these are used to cache hostnames # used by iter_sites(). if key . startswith ( '_' ) : continue del env [ key ]
11698	def _unwrap_stream ( uri , timeout , scanner , requests_session ) : original_uri = uri seen_uris = set ( ) deadline = time . time ( ) + timeout while time . time ( ) < deadline : if uri in seen_uris : logger . info ( 'Unwrapping stream from URI (%s) failed: ' 'playlist referenced itself' , uri ) return None else : seen_uris . add ( uri ) logger . debug ( 'Unwrapping stream from URI: %s' , uri ) try : scan_timeout = deadline - time . time ( ) if scan_timeout < 0 : logger . info ( 'Unwrapping stream from URI (%s) failed: ' 'timed out in %sms' , uri , timeout ) return None scan_result = scanner . scan ( uri , timeout = scan_timeout ) except exceptions . ScannerError as exc : logger . debug ( 'GStreamer failed scanning URI (%s): %s' , uri , exc ) scan_result = None if scan_result is not None and not ( scan_result . mime . startswith ( 'text/' ) or scan_result . mime . startswith ( 'application/' ) ) : logger . debug ( 'Unwrapped potential %s stream: %s' , scan_result . mime , uri ) return uri download_timeout = deadline - time . time ( ) if download_timeout < 0 : logger . info ( 'Unwrapping stream from URI (%s) failed: timed out in %sms' , uri , timeout ) return None content = http . download ( requests_session , uri , timeout = download_timeout ) if content is None : logger . info ( 'Unwrapping stream from URI (%s) failed: ' 'error downloading URI %s' , original_uri , uri ) return None uris = playlists . parse ( content ) if not uris : logger . debug ( 'Failed parsing URI (%s) as playlist; found potential stream.' , uri ) return uri # TODO Test streams and return first that seems to be playable logger . debug ( 'Parsed playlist (%s) and found new URI: %s' , uri , uris [ 0 ] ) uri = uris [ 0 ]
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) # Compare the vlaue. if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical # Output self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
6927	def newcursor ( self , dictcursor = False ) : handle = hashlib . sha256 ( os . urandom ( 12 ) ) . hexdigest ( ) if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return ( self . cursors [ handle ] , handle )
13901	def get_db_from_db ( db_string ) : server = get_server_from_db ( db_string ) local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) # If this looks like a local specifier: if local_match : return server [ local_match . groupdict ( ) [ 'database' ] ] elif remote_match : return server [ remote_match . groupdict ( ) [ 'database' ] ] raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
9936	def list ( self , ignore_patterns ) : for prefix , root in self . locations : storage = self . storages [ root ] for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
12310	def pull_stream ( self , uri , * * kwargs ) : return self . protocol . execute ( 'pullStream' , uri = uri , * * kwargs )
10976	def delete ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_edit ( current_user ) : try : group . delete ( ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'Successfully removed group "%(group_name)s"' , group_name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'You cannot delete the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( ".index" ) )
12003	def _add_header ( self , data , options ) : # pylint: disable=W0142 version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( * * header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( * * options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
6203	def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
12302	def validate ( repo , validator_name = None , filename = None , rulesfiles = None , args = [ ] ) : mgr = plugins_get_mgr ( ) # Expand the specification. Now we have full file paths validator_specs = instantiate ( repo , validator_name , filename , rulesfiles ) # Run the validators with rules files... allresults = [ ] for v in validator_specs : keys = mgr . search ( what = 'validator' , name = v ) [ 'validator' ] for k in keys : validator = mgr . get_by_key ( 'validator' , k ) result = validator . evaluate ( repo , validator_specs [ v ] , args ) allresults . extend ( result ) return allresults
3540	def status_printer ( ) : last_len = [ 0 ] def p ( s ) : s = next ( spinner ) + ' ' + s len_s = len ( s ) output = '\r' + s + ( ' ' * max ( last_len [ 0 ] - len_s , 0 ) ) sys . stdout . write ( output ) sys . stdout . flush ( ) last_len [ 0 ] = len_s return p
8147	def hue ( self , img1 , img2 ) : import colorsys p1 = list ( img1 . getdata ( ) ) p2 = list ( img2 . getdata ( ) ) for i in range ( len ( p1 ) ) : r1 , g1 , b1 , a1 = p1 [ i ] r1 = r1 / 255.0 g1 = g1 / 255.0 b1 = b1 / 255.0 h1 , s1 , v1 = colorsys . rgb_to_hsv ( r1 , g1 , b1 ) r2 , g2 , b2 , a2 = p2 [ i ] r2 = r2 / 255.0 g2 = g2 / 255.0 b2 = b2 / 255.0 h2 , s2 , v2 = colorsys . rgb_to_hsv ( r2 , g2 , b2 ) r3 , g3 , b3 = colorsys . hsv_to_rgb ( h2 , s1 , v1 ) r3 = int ( r3 * 255 ) g3 = int ( g3 * 255 ) b3 = int ( b3 * 255 ) p1 [ i ] = ( r3 , g3 , b3 , a1 ) img = Image . new ( "RGBA" , img1 . size , 255 ) img . putdata ( p1 ) return img
9606	def check_unused_args ( self , used_args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used_args : self . _used_kwargs . update ( { k : v } ) else : self . _unused_kwargs . update ( { k : v } )
5977	def mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale r_arcsec = np . sqrt ( x_arcsec ** 2 + y_arcsec ** 2 ) if outer_radius_arcsec >= r_arcsec >= inner_radius_arcsec : mask [ y , x ] = False return mask
958	def aggregationToMonthsSeconds ( interval ) : seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 seconds += interval . get ( 'seconds' , 0 ) seconds += interval . get ( 'minutes' , 0 ) * 60 seconds += interval . get ( 'hours' , 0 ) * 60 * 60 seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 months = interval . get ( 'months' , 0 ) months += 12 * interval . get ( 'years' , 0 ) return { 'months' : months , 'seconds' : seconds }
11275	def check_pid ( pid , debug ) : try : # A Kill of 0 is to check if the PID is active. It won't kill the process os . kill ( pid , 0 ) if debug > 1 : print ( "Script has a PIDFILE where the process is still running" ) return True except OSError : if debug > 1 : print ( "Script does not appear to be running" ) return False
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) # else command is intended for the entire house code, not a single unit number # TODO normalize/validate state return self . _x10_command ( house_code , unit_number , state )
5637	def largest_finite_distance ( self ) : block_start_distances = [ block . distance_start for block in self . _profile_blocks if block . distance_start < float ( 'inf' ) ] block_end_distances = [ block . distance_end for block in self . _profile_blocks if block . distance_end < float ( 'inf' ) ] distances = block_start_distances + block_end_distances if len ( distances ) > 0 : return max ( distances ) else : return None
11270	def safe_substitute ( prev , * args , * * kw ) : template_obj = string . Template ( * args , * * kw ) for data in prev : yield template_obj . safe_substitute ( data )
3967	def _compose_dict_for_nginx ( port_specs ) : spec = { 'image' : constants . NGINX_IMAGE , 'volumes' : [ '{}:{}' . format ( constants . NGINX_CONFIG_DIR_IN_VM , constants . NGINX_CONFIG_DIR_IN_CONTAINER ) ] , 'command' : 'nginx -g "daemon off;" -c /etc/nginx/conf.d/nginx.primary' , 'container_name' : 'dusty_{}_1' . format ( constants . DUSTY_NGINX_NAME ) } all_host_ports = set ( [ nginx_spec [ 'host_port' ] for nginx_spec in port_specs [ 'nginx' ] ] ) if all_host_ports : spec [ 'ports' ] = [ ] for port in all_host_ports : spec [ 'ports' ] . append ( '{0}:{0}' . format ( port ) ) return { constants . DUSTY_NGINX_NAME : spec }
6087	def contribution_maps_1d_from_hyper_images_and_galaxies ( hyper_model_image_1d , hyper_galaxy_images_1d , hyper_galaxies , hyper_minimum_values ) : # noinspection PyArgumentList return list ( map ( lambda hyper_galaxy , hyper_galaxy_image_1d , hyper_minimum_value : hyper_galaxy . contributions_from_model_image_and_galaxy_image ( model_image = hyper_model_image_1d , galaxy_image = hyper_galaxy_image_1d , minimum_value = hyper_minimum_value ) , hyper_galaxies , hyper_galaxy_images_1d , hyper_minimum_values ) )
1656	def IsOutOfLineMethodDefinition ( clean_lines , linenum ) : # Scan back a few lines for start of current function for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : if Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) : return Match ( r'^[^()]*\w+::\w+\(' , clean_lines . elided [ i ] ) is not None return False
9975	def _alter_code ( code , * * attrs ) : PyCode_New = ctypes . pythonapi . PyCode_New PyCode_New . argtypes = ( ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . c_int , ctypes . py_object ) PyCode_New . restype = ctypes . py_object args = [ [ code . co_argcount , 'co_argcount' ] , [ code . co_kwonlyargcount , 'co_kwonlyargcount' ] , [ code . co_nlocals , 'co_nlocals' ] , [ code . co_stacksize , 'co_stacksize' ] , [ code . co_flags , 'co_flags' ] , [ code . co_code , 'co_code' ] , [ code . co_consts , 'co_consts' ] , [ code . co_names , 'co_names' ] , [ code . co_varnames , 'co_varnames' ] , [ code . co_freevars , 'co_freevars' ] , [ code . co_cellvars , 'co_cellvars' ] , [ code . co_filename , 'co_filename' ] , [ code . co_name , 'co_name' ] , [ code . co_firstlineno , 'co_firstlineno' ] , [ code . co_lnotab , 'co_lnotab' ] ] for arg in args : if arg [ 1 ] in attrs : arg [ 0 ] = attrs [ arg [ 1 ] ] return PyCode_New ( args [ 0 ] [ 0 ] , # code.co_argcount, args [ 1 ] [ 0 ] , # code.co_kwonlyargcount, args [ 2 ] [ 0 ] , # code.co_nlocals, args [ 3 ] [ 0 ] , # code.co_stacksize, args [ 4 ] [ 0 ] , # code.co_flags, args [ 5 ] [ 0 ] , # code.co_code, args [ 6 ] [ 0 ] , # code.co_consts, args [ 7 ] [ 0 ] , # code.co_names, args [ 8 ] [ 0 ] , # code.co_varnames, args [ 9 ] [ 0 ] , # code.co_freevars, args [ 10 ] [ 0 ] , # code.co_cellvars, args [ 11 ] [ 0 ] , # code.co_filename, args [ 12 ] [ 0 ] , # code.co_name, args [ 13 ] [ 0 ] , # code.co_firstlineno, args [ 14 ] [ 0 ] )
6058	def bin_up_array_2d_using_mean ( array_2d , bin_up_factor ) : padded_array_2d = pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = array_2d , bin_up_factor = bin_up_factor ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = 0.0 for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 value += padded_array_2d [ padded_y , padded_x ] binned_array_2d [ y , x ] = value / ( bin_up_factor ** 2.0 ) return binned_array_2d
10242	def get_evidences_by_pmid ( graph : BELGraph , pmids : Union [ str , Iterable [ str ] ] ) : result = defaultdict ( set ) for _ , _ , _ , data in filter_edges ( graph , build_pmid_inclusion_filter ( pmids ) ) : result [ data [ CITATION ] [ CITATION_REFERENCE ] ] . add ( data [ EVIDENCE ] ) return dict ( result )
12759	def load_csv ( self , filename , start_frame = 10 , max_frames = int ( 1e300 ) ) : import pandas as pd compression = None if filename . endswith ( '.gz' ) : compression = 'gzip' df = pd . read_csv ( filename , compression = compression ) . set_index ( 'time' ) . fillna ( - 1 ) # make sure the data frame's time index matches our world. assert self . world . dt == pd . Series ( df . index ) . diff ( ) . mean ( ) markers = [ ] for c in df . columns : m = re . match ( r'^marker\d\d-(.*)-c$' , c ) if m : markers . append ( m . group ( 1 ) ) self . channels = self . _map_labels_to_channels ( markers ) cols = [ c for c in df . columns if re . match ( r'^marker\d\d-.*-[xyzc]$' , c ) ] self . data = df [ cols ] . values . reshape ( ( len ( df ) , len ( markers ) , 4 ) ) [ start_frame : ] self . data [ : , : , [ 1 , 2 ] ] = self . data [ : , : , [ 2 , 1 ] ] logging . info ( '%s: loaded marker data %s' , filename , self . data . shape ) self . process_data ( ) self . create_bodies ( )
8652	def create_thread ( session , member_ids , context_type , context , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } thread_data = { 'members[]' : member_ids , 'context_type' : context_type , 'context' : context , 'message' : message , } # POST /api/messages/0.1/threads/ response = make_post_request ( session , 'threads' , headers , form_data = thread_data ) json_data = response . json ( ) if response . status_code == 200 : return Thread ( json_data [ 'result' ] ) else : raise ThreadNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1923	def binary_arch ( binary ) : with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) if elffile [ 'e_machine' ] == 'EM_X86_64' : return True else : return False
7128	def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( "#" ) if len ( path_tuple ) > 1 : path_str = "#" . join ( ( path_tuple [ 0 ] , path_tuple [ - 1 ] ) ) else : path_str = data [ 2 ] return path_str
10193	def get_user ( ) : return dict ( ip_address = request . remote_addr , user_agent = request . user_agent . string , user_id = ( current_user . get_id ( ) if current_user . is_authenticated else None ) , session_id = session . get ( 'sid_s' ) )
4885	def allow_request ( self , request , view ) : service_users = get_service_usernames ( ) # User service user throttling rates for service user. if request . user . username in service_users : self . update_throttle_scope ( ) return super ( ServiceUserThrottle , self ) . allow_request ( request , view )
2681	def create_function ( cfg , path_to_zip_file , use_s3 = False , s3_file = None ) : print ( 'Creating your new Lambda function' ) byte_stream = read ( path_to_zip_file , binary_file = True ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) account_id = get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' , ) , ) role = get_role_name ( cfg . get ( 'region' ) , account_id , cfg . get ( 'role' , 'lambda_basic_execution' ) , ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) # Do we prefer development variable over config? buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) print ( 'Creating lambda function with name: {}' . format ( func_name ) ) if use_s3 : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'S3Bucket' : '{}' . format ( buck_name ) , 'S3Key' : '{}' . format ( s3_file ) , } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } else : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'ZipFile' : byte_stream } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } if 'tags' in cfg : kwargs . update ( Tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } ) if 'environment_variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : get_environment_variable_value ( value ) for key , value in cfg . get ( 'environment_variables' ) . items ( ) } , } , ) client . create_function ( * * kwargs ) concurrency = get_concurrency ( cfg ) if concurrency > 0 : client . put_function_concurrency ( FunctionName = func_name , ReservedConcurrentExecutions = concurrency )
11683	def _readblock ( self ) : block = '' while not self . _stop : line = self . _readline ( ) if line == '.' : break block += line return block
3085	def service_account_email ( self ) : if self . _service_account_email is None : self . _service_account_email = ( app_identity . get_service_account_name ( ) ) return self . _service_account_email
8811	def filter_factory ( global_conf , * * local_conf ) : conf = global_conf . copy ( ) conf . update ( local_conf ) def wrapper ( app ) : return ResponseAsyncIdAdder ( app , conf ) return wrapper
3767	def zs_to_ws ( zs , MWs ) : Mavg = sum ( zi * MWi for zi , MWi in zip ( zs , MWs ) ) ws = [ zi * MWi / Mavg for zi , MWi in zip ( zs , MWs ) ] return ws
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
11111	def synchronize ( self , verbose = False ) : if self . __path is None : return # walk directories for dirPath in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , dirPath ) # if directory exist if os . path . isdir ( realPath ) : continue if verbose : warnings . warn ( "%s directory is missing" % realPath ) # loop to get dirInfoDict keys = dirPath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break # remove dirInfoDict directory if existing if dirInfoDict is not None : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is not None : dict . pop ( dirs , keys [ - 1 ] , None ) # walk files for filePath in sorted ( list ( self . walk_files_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , filePath ) # if file exists if os . path . isfile ( realPath ) : continue if verbose : warnings . warn ( "%s file is missing" % realPath ) # loop to get dirInfoDict keys = filePath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break # remove dirInfoDict file if existing if dirInfoDict is not None : files = dict . get ( dirInfoDict , 'files' , None ) if files is not None : dict . pop ( files , keys [ - 1 ] , None )
5092	def _login ( self , email , password ) : response = requests . post ( urljoin ( self . ENDPOINT , 'sessions' ) , json = { 'email' : email , 'password' : password , 'platform' : 'ios' , 'token' : binascii . hexlify ( os . urandom ( 64 ) ) . decode ( 'utf8' ) } , headers = self . _headers ) response . raise_for_status ( ) access_token = response . json ( ) [ 'access_token' ] self . _headers [ 'Authorization' ] = 'Token token=%s' % access_token
2731	def create ( self ) : # URL https://api.digitalocean.com/v2/domains data = { "name" : self . name , "ip_address" : self . ip_address , } domain = self . get_data ( "domains" , type = POST , params = data ) return domain
10470	def terminateAppByBundleId ( bundleID ) : ra = AppKit . NSRunningApplication if getattr ( ra , "runningApplicationsWithBundleIdentifier_" ) : appList = ra . runningApplicationsWithBundleIdentifier_ ( bundleID ) if appList and len ( appList ) > 0 : app = appList [ 0 ] return app and app . terminate ( ) and True or False return False
7896	def change_nick ( self , new_nick ) : new_room_jid = JID ( self . room_jid . node , self . room_jid . domain , new_nick ) p = Presence ( to_jid = new_room_jid ) self . manager . stream . send ( p )
8064	def precmd ( self , line ) : args = shlex . split ( line or "" ) if args and 'cookie=' in args [ - 1 ] : cookie_index = line . index ( 'cookie=' ) cookie = line [ cookie_index + 7 : ] line = line [ : cookie_index ] . strip ( ) self . cookie = cookie if line . startswith ( '#' ) : return '' elif '=' in line : # allow somevar=somevalue # first check if we really mean a command cmdname = line . partition ( " " ) [ 0 ] if hasattr ( self , "do_%s" % cmdname ) : return line if not line . startswith ( "set " ) : return "set " + line else : return line if len ( args ) and args [ 0 ] in self . shortcuts : return "%s %s" % ( self . shortcuts [ args [ 0 ] ] , " " . join ( args [ 1 : ] ) ) else : return line
3801	def calculate ( self , T , method ) : if method == SHEFFY_JOHNSON : kl = Sheffy_Johnson ( T , self . MW , self . Tm ) elif method == SATO_RIEDEL : kl = Sato_Riedel ( T , self . MW , self . Tb , self . Tc ) elif method == GHARAGHEIZI_L : kl = Gharagheizi_liquid ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == NICOLA : kl = Nicola ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == NICOLA_ORIGINAL : kl = Nicola_original ( T , self . MW , self . Tc , self . omega , self . Hfus ) elif method == LAKSHMI_PRASAD : kl = Lakshmi_Prasad ( T , self . MW ) elif method == BAHADORI_L : kl = Bahadori_liquid ( T , self . MW ) elif method == DIPPR_PERRY_8E : kl = EQ100 ( T , * self . Perrys2_315_coeffs ) elif method == VDI_PPDS : kl = horner ( self . VDI_PPDS_coeffs , T ) elif method == COOLPROP : kl = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'l' ) elif method in self . tabular_data : kl = self . interpolate ( T , method ) return kl
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
1278	def markdown ( text , escape = True , * * kwargs ) : return Markdown ( escape = escape , * * kwargs ) ( text )
2265	def dict_isect ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict common_keys = set . intersection ( * map ( set , args ) ) first_dict = args [ 0 ] return dictclass ( ( k , first_dict [ k ] ) for k in common_keys )
3612	def do_filter ( qs , keywords , exclude = False ) : and_q = Q ( ) for keyword , value in iteritems ( keywords ) : try : values = value . split ( "," ) if len ( values ) > 0 : or_q = Q ( ) for value in values : or_q |= Q ( * * { keyword : value } ) and_q &= or_q except AttributeError : # value can be a bool and_q &= Q ( * * { keyword : value } ) if exclude : qs = qs . exclude ( and_q ) else : qs = qs . filter ( and_q ) return qs
9078	def make_df_getter ( data_url : str , data_path : str , * * kwargs ) -> Callable [ [ Optional [ str ] , bool , bool ] , pd . DataFrame ] : download_function = make_downloader ( data_url , data_path ) def get_df ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> pd . DataFrame : """Get the data as a pandas DataFrame. :param url: The URL (or file path) to download. :param cache: If true, the data is downloaded to the file system, else it is loaded from the internet :param force_download: If true, overwrites a previously cached file """ if url is None and cache : url = download_function ( force_download = force_download ) return pd . read_csv ( url or data_url , * * kwargs ) return get_df
2632	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : block = self . provider . submit ( self . launch_cmd , 1 , self . workers_per_node ) logger . debug ( "Launched block {}:{}" . format ( i , block ) ) if not block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) self . engines . extend ( [ block ] ) r . extend ( [ block ] ) else : logger . error ( "No execution provider available" ) r = None return r
13766	def parse ( self , string ) : var , eq , values = string . strip ( ) . partition ( '=' ) assert var == 'runtimepath' assert eq == '=' return values . split ( ',' )
677	def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description
13912	def _AddPropertiesForExtensions ( descriptor , cls ) : extension_dict = descriptor . extensions_by_name for extension_name , extension_field in extension_dict . items ( ) : constant_name = extension_name . upper ( ) + "_FIELD_NUMBER" setattr ( cls , constant_name , extension_field . number )
13869	def _GetNativeEolStyle ( platform = sys . platform ) : _NATIVE_EOL_STYLE_MAP = { 'win32' : EOL_STYLE_WINDOWS , 'linux2' : EOL_STYLE_UNIX , 'linux' : EOL_STYLE_UNIX , 'darwin' : EOL_STYLE_MAC , } result = _NATIVE_EOL_STYLE_MAP . get ( platform ) if result is None : from . _exceptions import UnknownPlatformError raise UnknownPlatformError ( platform ) return result
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
2818	def convert_padding ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting padding...' ) if params [ 'mode' ] == 'constant' : # raise AssertionError('Cannot convert non-constant padding') if params [ 'value' ] != 0.0 : raise AssertionError ( 'Cannot convert non-zero padding' ) if names : tf_name = 'PADD' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) # Magic ordering padding_name = tf_name padding_layer = keras . layers . ZeroPadding2D ( padding = ( ( params [ 'pads' ] [ 2 ] , params [ 'pads' ] [ 6 ] ) , ( params [ 'pads' ] [ 3 ] , params [ 'pads' ] [ 7 ] ) ) , name = padding_name ) layers [ scope_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) elif params [ 'mode' ] == 'reflect' : def target_layer ( x , pads = params [ 'pads' ] ) : # x = tf.transpose(x, [0, 2, 3, 1]) layer = tf . pad ( x , [ [ 0 , 0 ] , [ 0 , 0 ] , [ pads [ 2 ] , pads [ 6 ] ] , [ pads [ 3 ] , pads [ 7 ] ] ] , 'REFLECT' ) # layer = tf.transpose(layer, [0, 3, 1, 2]) return layer lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
4484	def copyfileobj ( fsrc , fdst , total , length = 16 * 1024 ) : with tqdm ( unit = 'bytes' , total = total , unit_scale = True ) as pbar : while 1 : buf = fsrc . read ( length ) if not buf : break fdst . write ( buf ) pbar . update ( len ( buf ) )
3396	def gapfill ( model , universal = None , lower_bound = 0.05 , penalties = None , demand_reactions = True , exchange_reactions = False , iterations = 1 ) : gapfiller = GapFiller ( model , universal = universal , lower_bound = lower_bound , penalties = penalties , demand_reactions = demand_reactions , exchange_reactions = exchange_reactions ) return gapfiller . fill ( iterations = iterations )
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
10562	def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) # Remove "/<totaltracks>" from track number. metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) # Remove leading zero(s) from track number. metadata = re . sub ( r'^\d+\.+' , '' , metadata ) # Remove dots from track number. metadata = re . sub ( r'[^\w\s]' , '' , metadata ) # Remove any non-words. metadata = re . sub ( r'\s+' , ' ' , metadata ) # Reduce multiple spaces to a single space. metadata = re . sub ( r'^\s+' , '' , metadata ) # Remove leading space. metadata = re . sub ( r'\s+$' , '' , metadata ) # Remove trailing space. metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) # Remove leading "the". return metadata
5016	def transmit ( self , payload , * * kwargs ) : IntegratedChannelLearnerDataTransmissionAudit = apps . get_model ( # pylint: disable=invalid-name app_label = kwargs . get ( 'app_label' , 'integrated_channel' ) , model_name = kwargs . get ( 'model_name' , 'LearnerDataTransmissionAudit' ) , ) # Since we have started sending courses to integrated channels instead of course runs, # we need to attempt to send transmissions with course keys and course run ids in order to # ensure that we account for whether courses or course runs exist in the integrated channel. # The exporters have been changed to return multiple transmission records to attempt, # one by course key and one by course run id. # If the transmission with the course key succeeds, the next one will get skipped. # If it fails, the one with the course run id will be attempted and (presumably) succeed. for learner_data in payload . export ( ) : serialized_payload = learner_data . serialize ( enterprise_configuration = self . enterprise_configuration ) LOGGER . debug ( 'Attempting to transmit serialized payload: %s' , serialized_payload ) enterprise_enrollment_id = learner_data . enterprise_course_enrollment_id if learner_data . completed_timestamp is None : # The user has not completed the course, so we shouldn't send a completion status call LOGGER . info ( 'Skipping in-progress enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue previous_transmissions = IntegratedChannelLearnerDataTransmissionAudit . objects . filter ( enterprise_course_enrollment_id = enterprise_enrollment_id , error_message = '' ) if previous_transmissions . exists ( ) : # We've already sent a completion status call for this enrollment LOGGER . info ( 'Skipping previously sent enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue try : code , body = self . client . create_course_completion ( getattr ( learner_data , kwargs . get ( 'remote_user_id' ) ) , serialized_payload ) LOGGER . info ( 'Successfully sent completion status call for enterprise enrollment {}' . format ( enterprise_enrollment_id , ) ) except RequestException as request_exception : code = 500 body = str ( request_exception ) self . handle_transmission_error ( learner_data , request_exception ) learner_data . status = str ( code ) learner_data . error_message = body if code >= 400 else '' learner_data . save ( )
9753	def experiment ( ctx , project , experiment ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'experiment' ] = experiment
4904	def populate_data_sharing_consent ( apps , schema_editor ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) EnterpriseCourseEnrollment = apps . get_model ( 'enterprise' , 'EnterpriseCourseEnrollment' ) User = apps . get_model ( 'auth' , 'User' ) for enrollment in EnterpriseCourseEnrollment . objects . all ( ) : user = User . objects . get ( pk = enrollment . enterprise_customer_user . user_id ) data_sharing_consent , __ = DataSharingConsent . objects . get_or_create ( username = user . username , enterprise_customer = enrollment . enterprise_customer_user . enterprise_customer , course_id = enrollment . course_id , ) if enrollment . consent_granted is not None : data_sharing_consent . granted = enrollment . consent_granted else : # Check UDSCA instead. consent_state = enrollment . enterprise_customer_user . data_sharing_consent . first ( ) if consent_state is not None : data_sharing_consent . granted = consent_state . state in [ 'enabled' , 'external' ] else : data_sharing_consent . granted = False data_sharing_consent . save ( )
11053	def _issue_cert ( self , domain ) : def errback ( failure ) : # Don't fail on some of the errors we could get from the ACME # server, rather just log an error so that we can continue with # other domains. failure . trap ( txacme_ServerError ) acme_error = failure . value . message if acme_error . code in [ 'rateLimited' , 'serverInternal' , 'connection' , 'unknownHost' ] : # TODO: Fire off an error to Sentry or something? self . log . error ( 'Error ({code}) issuing certificate for "{domain}": ' '{detail}' , code = acme_error . code , domain = domain , detail = acme_error . detail ) else : # There are more error codes but if they happen then something # serious has gone wrong-- carry on error-ing. return failure d = self . txacme_service . issue_cert ( domain ) return d . addErrback ( errback )
13003	def modify_data ( data ) : with tempfile . NamedTemporaryFile ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to_dict ( include_meta = True ) , default = datetime_handler ) ) f . write ( '\n' ) f . flush ( ) print_success ( "Starting editor" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
10635	def afr ( self ) : result = 0.0 for compound in self . material . compounds : result += self . get_compound_afr ( compound ) return result
4214	def get_all_keyring ( ) : _load_plugins ( ) viable_classes = KeyringBackend . get_viable_backends ( ) rings = util . suppress_exceptions ( viable_classes , exceptions = TypeError ) return list ( rings )
4449	def load_document ( self , id ) : fields = self . redis . hgetall ( id ) if six . PY3 : f2 = { to_string ( k ) : to_string ( v ) for k , v in fields . items ( ) } fields = f2 try : del fields [ 'id' ] except KeyError : pass return Document ( id = id , * * fields )
7617	def coerce_annotation ( ann , namespace ) : ann = convert ( ann , namespace ) ann . validate ( strict = True ) return ann
1379	def print_build_info ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : release_map = yaml . load ( release_info ) release_items = sorted ( release_map . items ( ) , key = lambda tup : tup [ 0 ] ) for key , value in release_items : print ( "%s : %s" % ( key , value ) )
3614	def _get_available_choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained_model_field + "_id" ) filter = { self . chained_model_field : pk } except AttributeError : try : # maybe m2m? pks = getattr ( item , self . chained_model_field ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : try : # maybe a set? pks = getattr ( item , self . chained_model_field + "_set" ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : # give up filter = { } filtered = list ( get_model ( self . to_app_name , self . to_model_name ) . objects . filter ( * * filter ) . distinct ( ) ) if self . sort : sort_results ( filtered ) else : # invalid value for queryset filtered = [ ] return filtered
1548	def init_rotating_logger ( level , logfile , max_files , max_bytes ) : logging . basicConfig ( ) root_logger = logging . getLogger ( ) log_format = "[%(asctime)s] [%(levelname)s] %(filename)s: %(message)s" root_logger . setLevel ( level ) handler = RotatingFileHandler ( logfile , maxBytes = max_bytes , backupCount = max_files ) handler . setFormatter ( logging . Formatter ( fmt = log_format , datefmt = date_format ) ) root_logger . addHandler ( handler ) for handler in root_logger . handlers : root_logger . debug ( "Associated handlers - " + str ( handler ) ) if isinstance ( handler , logging . StreamHandler ) : root_logger . debug ( "Removing StreamHandler: " + str ( handler ) ) root_logger . handlers . remove ( handler )
11654	def fit ( self , X , y = None , * * params ) : X = as_features ( X , stack = True ) self . transformer . fit ( X . stacked_features , y , * * params ) return self
3796	def setup_a_alpha_and_derivatives ( self , i , T = None ) : if not hasattr ( self , 'kappas' ) : self . kappas = [ ] for Tc , kappa0 , kappa1 , kappa2 , kappa3 in zip ( self . Tcs , self . kappa0s , self . kappa1s , self . kappa2s , self . kappa3s ) : Tr = T / Tc kappa = kappa0 + ( ( kappa1 + kappa2 * ( kappa3 - Tr ) * ( 1. - Tr ** 0.5 ) ) * ( 1. + Tr ** 0.5 ) * ( 0.7 - Tr ) ) self . kappas . append ( kappa ) ( self . a , self . kappa , self . kappa0 , self . kappa1 , self . kappa2 , self . kappa3 , self . Tc ) = ( self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . kappa2s [ i ] , self . kappa3s [ i ] , self . Tcs [ i ] )
11588	def _rc_brpoplpush ( self , src , dst , timeout = 0 ) : rpop = self . brpop ( src , timeout ) if rpop is not None : self . lpush ( dst , rpop [ 1 ] ) return rpop [ 1 ] return None
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : # pragma: no cover load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
12935	def _parse_allele_data ( self ) : # Get allele frequencies if they exist. pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , * * cln_data ) # A few ClinVar variants are only reported as a combination with # other variants, and no single-variant effect is proposed. Skip these. if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
8402	def rescale_max ( x , to = ( 0 , 1 ) , _from = None ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) out = x / _from [ 1 ] * to [ 1 ] if not array_like : out = out [ 0 ] return out
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : #shorter than 5 minutes pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
7877	def _bind_success ( self , stanza ) : # pylint: disable-msg=R0201 payload = stanza . get_payload ( ResourceBindingPayload ) jid = payload . jid if not jid : raise BadRequestProtocolError ( u"<jid/> element mising in" " the bind response" ) self . stream . me = jid self . stream . event ( AuthorizedEvent ( self . stream . me ) )
6078	def convolve_image ( self , image_array , blurring_array ) : return self . convolve_jit ( image_array , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths , blurring_array , self . blurring_frame_indexes , self . blurring_frame_psfs , self . blurring_frame_lengths )
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
11006	def get_active_bets ( self , project_id = None ) : url = urljoin ( self . settings [ 'bets_url' ] , 'bets?state=fresh,active,accept_end&page=1&page_size=100' ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) bets = [ ] has_next_page = True while has_next_page : res = self . _req ( url ) bets . extend ( res [ 'bets' ] [ 'results' ] ) url = res [ 'bets' ] . get ( 'next' ) has_next_page = bool ( url ) return bets
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
11949	def configure_custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and isinstance ( c , types . ClassType ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) # Check for valid identifiers kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) result = c ( * * kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
6412	def agmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) : m_a , m_g = ( m_a + m_g ) / 2 , ( m_a * m_g ) ** ( 1 / 2 ) return m_a
3581	def _get_objects ( self , interface , parent_path = '/org/bluez' ) : # Iterate through all the objects in bluez's DBus hierarchy and return # any that implement the requested interface under the specified path. parent_path = parent_path . lower ( ) objects = [ ] for opath , interfaces in iteritems ( self . _bluez . GetManagedObjects ( ) ) : if interface in interfaces . keys ( ) and opath . lower ( ) . startswith ( parent_path ) : objects . append ( self . _bus . get_object ( 'org.bluez' , opath ) ) return objects
9730	def get_force ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlate , data , component_position ) force_list = [ ] for _ in range ( plate . force_count ) : component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) force_list . append ( force ) append_components ( ( plate , force_list ) ) return components
3596	def bulkDetails ( self , packageNames ) : params = { 'au' : '1' } req = googleplay_pb2 . BulkDetailsRequest ( ) req . docid . extend ( packageNames ) data = req . SerializeToString ( ) message = self . executeRequestApi2 ( BULK_URL , post_data = data . decode ( "utf-8" ) , content_type = CONTENT_TYPE_PROTO , params = params ) response = message . payload . bulkDetailsResponse return [ None if not utils . hasDoc ( entry ) else utils . parseProtobufObj ( entry . doc ) for entry in response . entry ]
13850	def ensure_dir_exists ( func ) : @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
13207	def _parse_author ( self ) : command = LatexCommand ( 'author' , { 'name' : 'authors' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return try : content = parsed [ 'authors' ] except KeyError : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return # Clean content content = content . replace ( '\n' , ' ' ) content = content . replace ( '~' , ' ' ) content = content . strip ( ) # Split content into list of individual authors authors = [ ] for part in content . split ( ',' ) : part = part . strip ( ) for split_part in part . split ( 'and ' ) : split_part = split_part . strip ( ) if len ( split_part ) > 0 : authors . append ( split_part ) self . _authors = authors
4499	def _json ( self , response , status_code ) : if isinstance ( status_code , numbers . Integral ) : status_code = ( status_code , ) if response . status_code in status_code : return response . json ( ) else : raise RuntimeError ( "Response has status " "code {} not {}" . format ( response . status_code , status_code ) )
8298	def readLong ( data ) : high , low = struct . unpack ( ">ll" , data [ 0 : 8 ] ) big = ( long ( high ) << 32 ) + low rest = data [ 8 : ] return ( big , rest )
8234	def complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) # A contrasting color: much darker or lighter than the original. c = clr . copy ( ) if clr . brightness > 0.4 : c . brightness = 0.1 + c . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) # A soft supporting color: lighter and less saturated. c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.3 colors . append ( c ) # A contrasting complement: very dark or very light. clr = clr . complement c = clr . copy ( ) if clr . brightness > 0.3 : c . brightness = 0.1 + clr . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) # The complement and a light supporting variant. colors . append ( clr ) c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.25 colors . append ( c ) return colors
12877	def many_until ( these , term ) : results = [ ] while True : stop , result = choice ( _tag ( True , term ) , _tag ( False , these ) ) if stop : return results , result else : results . append ( result )
5924	def setup ( filename = CONFIGNAME ) : # setup() must be separate and NOT run automatically when config # is loaded so that easy_install installations work # (otherwise we get a sandbox violation) # populate cfg with defaults (or existing data) get_configuration ( ) if not os . path . exists ( filename ) : with open ( filename , 'w' ) as configfile : cfg . write ( configfile ) # write the default file so that user can edit msg = "NOTE: GromacsWrapper created the configuration file \n\t%r\n" " for you. Edit the file to customize the package." % filename print ( msg ) # directories for d in config_directories : utilities . mkdir_p ( d )
5956	def load_v4_tools ( ) : logger . debug ( "Loading v4 tools..." ) names = config . get_tool_names ( ) if len ( names ) == 0 and 'GMXBIN' in os . environ : names = find_executables ( os . environ [ 'GMXBIN' ] ) if len ( names ) == 0 or len ( names ) > len ( V4TOOLS ) * 4 : names = list ( V4TOOLS ) names . extend ( config . get_extra_tool_names ( ) ) tools = { } for name in names : fancy = make_valid_identifier ( name ) tools [ fancy ] = tool_factory ( fancy , name , None ) if not tools : errmsg = "Failed to load v4 tools" logger . debug ( errmsg ) raise GromacsToolLoadingError ( errmsg ) logger . debug ( "Loaded {0} v4 tools successfully!" . format ( len ( tools ) ) ) return tools
13746	def create_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) now = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) attrs = { 'created_on' : now , 'modified_on' : now , 'count' : start , } if extra_attrs : attrs . update ( extra_attrs ) item = table . new_item ( hash_key = hash_key , attrs = attrs , ) return item
4760	def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout_backup = cij . ENV . get ( "SSH_CMD_TIMEOUT" ) try : time_start = time . time ( ) cij . ENV [ "SSH_CMD_TIMEOUT" ] = "3" while True : time_current = time . time ( ) if ( time_current - time_start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , _ , _ = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time_current - time_start ) ) finally : if timeout_backup is None : del cij . ENV [ "SSH_CMD_TIMEOUT" ] else : cij . ENV [ "SSH_CMD_TIMEOUT" ] = timeout_backup return 0
5632	def unindent ( lines ) : try : # Determine minimum indentation: indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
4379	def allow ( self , role , method , resource , with_children = True ) : if with_children : for r in role . get_children ( ) : permission = ( r . get_name ( ) , method , resource ) if permission not in self . _allowed : self . _allowed . append ( permission ) if role == 'anonymous' : permission = ( role , method , resource ) else : permission = ( role . get_name ( ) , method , resource ) if permission not in self . _allowed : self . _allowed . append ( permission )
6280	def clear_values ( self , red = 0.0 , green = 0.0 , blue = 0.0 , alpha = 0.0 , depth = 1.0 ) : self . clear_color = ( red , green , blue , alpha ) self . clear_depth = depth
12897	def get_mute ( self ) : mute = ( yield from self . handle_int ( self . API . get ( 'mute' ) ) ) return bool ( mute )
3360	def elements ( self ) : tmp_formula = self . formula if tmp_formula is None : return { } # necessary for some old pickles which use the deprecated # Formula class tmp_formula = str ( self . formula ) # commonly occurring characters in incorrectly constructed formulas if "*" in tmp_formula : warn ( "invalid character '*' found in formula '%s'" % self . formula ) tmp_formula = tmp_formula . replace ( "*" , "" ) if "(" in tmp_formula or ")" in tmp_formula : warn ( "invalid formula (has parenthesis) in '%s'" % self . formula ) return None composition = { } parsed = element_re . findall ( tmp_formula ) for ( element , count ) in parsed : if count == '' : count = 1 else : try : count = float ( count ) int_count = int ( count ) if count == int_count : count = int_count else : warn ( "%s is not an integer (in formula %s)" % ( count , self . formula ) ) except ValueError : warn ( "failed to parse %s (in formula %s)" % ( count , self . formula ) ) return None if element in composition : composition [ element ] += count else : composition [ element ] = count return composition
3781	def calculate ( self , T , method ) : if method == TEST_METHOD_1 : prop = self . TEST_METHOD_1_coeffs [ 0 ] + self . TEST_METHOD_1_coeffs [ 1 ] * T elif method == TEST_METHOD_2 : prop = self . TEST_METHOD_2_coeffs [ 0 ] + self . TEST_METHOD_2_coeffs [ 1 ] * T elif method in self . tabular_data : prop = self . interpolate ( T , method ) return prop
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
13876	def CopyFilesX ( file_mapping ) : # List files that match the mapping files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue # Do not copy dirs i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) # Copy files for i_source_filename , i_target_filename in files : # Create target dir if necessary target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
8317	def parse_balanced_image ( self , markup ) : opened = 0 closed = 0 for i in range ( len ( markup ) ) : if markup [ i ] == "[" : opened += 1 if markup [ i ] == "]" : closed += 1 if opened == closed : return markup [ : i + 1 ] return markup
2282	def launch_R_script ( template , arguments , output_function = None , verbose = True , debug = False ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_R_script_' + id + '/' ) try : scriptpath = '/tmp/cdt_R_script_' + id + '/instance_{}' . format ( os . path . basename ( template ) ) copy ( template , scriptpath ) with fileinput . FileInput ( scriptpath , inplace = True ) as file : for line in file : mline = line for elt in arguments : mline = mline . replace ( elt , arguments [ elt ] ) print ( mline , end = '' ) if output_function is None : output = subprocess . call ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) else : if verbose : process = subprocess . Popen ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True ) else : process = subprocess . Popen ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) process . wait ( ) output = output_function ( ) # Cleaning up except Exception as e : if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) raise e except KeyboardInterrupt : if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) raise KeyboardInterrupt if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) return output
7975	def loop_iteration ( self , timeout = 0.1 ) : try : exc_info = self . exc_queue . get ( True , timeout ) [ 1 ] except Queue . Empty : return exc_type , exc_value , ext_stack = exc_info raise exc_type , exc_value , ext_stack
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
6630	def get ( self , path ) : path = _splitPath ( path ) for config in self . configs . values ( ) : cur = config for el in path : if el in cur : cur = cur [ el ] else : cur = None break if cur is not None : return cur return None
5070	def format_price ( price , currency = '$' ) : if int ( price ) == price : return '{}{}' . format ( currency , int ( price ) ) return '{}{:0.2f}' . format ( currency , price )
10119	def circle ( cls , center , radius , n_vertices = 50 , * * kwargs ) : return cls . regular_polygon ( center , radius , n_vertices , * * kwargs )
4203	def LEVINSON ( r , order = None , allow_singularity = False ) : #from numpy import isrealobj T0 = numpy . real ( r [ 0 ] ) T = r [ 1 : ] M = len ( T ) if order is None : M = len ( T ) else : assert order <= M , 'order must be less than size of the input data' M = order realdata = numpy . isrealobj ( r ) if realdata is True : A = numpy . zeros ( M , dtype = float ) ref = numpy . zeros ( M , dtype = float ) else : A = numpy . zeros ( M , dtype = complex ) ref = numpy . zeros ( M , dtype = complex ) P = T0 for k in range ( 0 , M ) : save = T [ k ] if k == 0 : temp = - save / P else : #save += sum([A[j]*T[k-j-1] for j in range(0,k)]) for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] temp = - save / P if realdata : P = P * ( 1. - temp ** 2. ) else : P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 and allow_singularity == False : raise ValueError ( "singular matrix" ) A [ k ] = temp ref [ k ] = temp # save reflection coeff at each step if k == 0 : continue khalf = ( k + 1 ) // 2 if realdata is True : for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] if j != kj : A [ kj ] += temp * save else : for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) return A , P , ref
12311	def record ( self , localStreamName , pathToFile , * * kwargs ) : return self . protocol . execute ( 'record' , localStreamName = localStreamName , pathToFile = pathToFile , * * kwargs )
11082	def freeze ( value ) : if isinstance ( value , list ) : return FrozenList ( * value ) if isinstance ( value , dict ) : return FrozenDict ( * * value ) return value
2343	def predict_proba ( self , a , b , nb_runs = 6 , nb_jobs = None , gpu = None , idx = 0 , verbose = None , ttest_threshold = 0.01 , nb_max_runs = 16 , train_epochs = 1000 , test_epochs = 1000 ) : Nb_jobs , verbose , gpu = SETTINGS . get_default ( ( 'nb_jobs' , nb_jobs ) , ( 'verbose' , verbose ) , ( 'gpu' , gpu ) ) x = np . stack ( [ a . ravel ( ) , b . ravel ( ) ] , 1 ) ttest_criterion = TTestCriterion ( max_iter = nb_max_runs , runs_per_iter = nb_runs , threshold = ttest_threshold ) AB = [ ] BA = [ ] while ttest_criterion . loop ( AB , BA ) : if nb_jobs != 1 : result_pair = Parallel ( n_jobs = nb_jobs ) ( delayed ( GNN_instance ) ( x , idx = idx , device = 'cuda:{}' . format ( run % gpu ) if gpu else 'cpu' , verbose = verbose , train_epochs = train_epochs , test_epochs = test_epochs ) for run in range ( ttest_criterion . iter , ttest_criterion . iter + nb_runs ) ) else : result_pair = [ GNN_instance ( x , idx = idx , device = 'cuda:0' if gpu else 'cpu' , verbose = verbose , train_epochs = train_epochs , test_epochs = test_epochs ) for run in range ( ttest_criterion . iter , ttest_criterion . iter + nb_runs ) ] AB . extend ( [ runpair [ 0 ] for runpair in result_pair ] ) BA . extend ( [ runpair [ 1 ] for runpair in result_pair ] ) if verbose : print ( "P-value after {} runs : {}" . format ( ttest_criterion . iter , ttest_criterion . p_value ) ) score_AB = np . mean ( AB ) score_BA = np . mean ( BA ) return ( score_BA - score_AB ) / ( score_BA + score_AB )
6932	def colormagdiagram_cplist ( cplist , outpkl , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : # first, we'll collect all of the info cplist_objectids = [ ] cplist_mags = [ ] cplist_colors = [ ] for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) cplist_objectids . append ( cpd [ 'objectid' ] ) thiscp_mags = [ ] thiscp_colors = [ ] for cm1 , cm2 , ym in zip ( color_mag1 , color_mag2 , yaxis_mag ) : if ( ym in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ ym ] is not None ) : thiscp_mags . append ( cpd [ 'objectinfo' ] [ ym ] ) else : thiscp_mags . append ( np . nan ) if ( cm1 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm1 ] is not None and cm2 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm2 ] is not None ) : thiscp_colors . append ( cpd [ 'objectinfo' ] [ cm1 ] - cpd [ 'objectinfo' ] [ cm2 ] ) else : thiscp_colors . append ( np . nan ) cplist_mags . append ( thiscp_mags ) cplist_colors . append ( thiscp_colors ) # convert these to arrays cplist_objectids = np . array ( cplist_objectids ) cplist_mags = np . array ( cplist_mags ) cplist_colors = np . array ( cplist_colors ) # prepare the outdict cmddict = { 'objectids' : cplist_objectids , 'mags' : cplist_mags , 'colors' : cplist_colors , 'color_mag1' : color_mag1 , 'color_mag2' : color_mag2 , 'yaxis_mag' : yaxis_mag } # save the pickled figure and dict for fast retrieval later with open ( outpkl , 'wb' ) as outfd : pickle . dump ( cmddict , outfd , pickle . HIGHEST_PROTOCOL ) plt . close ( 'all' ) return cmddict
7935	def _auth ( self ) : if self . authenticated : self . __logger . debug ( "_auth: already authenticated" ) return self . __logger . debug ( "doing handshake..." ) hash_value = self . _compute_handshake ( ) n = common_root . newTextChild ( None , "handshake" , hash_value ) self . _write_node ( n ) n . unlinkNode ( ) n . freeNode ( ) self . __logger . debug ( "handshake hash sent." )
12157	def list_order_by ( l , firstItems ) : l = list ( l ) for item in firstItems [ : : - 1 ] : #backwards if item in l : l . remove ( item ) l . insert ( 0 , item ) return l
10319	def _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) : ret = dict ( ) runs = max_cluster_size . size sqrt_n = np . sqrt ( runs ) max_cluster_size_sample_mean = max_cluster_size . mean ( ) ret [ 'max_cluster_size' ] = max_cluster_size_sample_mean max_cluster_size_sample_std = max_cluster_size . std ( ddof = 1 ) if max_cluster_size_sample_std : old_settings = np . seterr ( all = 'raise' ) ret [ 'max_cluster_size_ci' ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = max_cluster_size_sample_mean , scale = max_cluster_size_sample_std / sqrt_n ) np . seterr ( * * old_settings ) else : ret [ 'max_cluster_size_ci' ] = ( max_cluster_size_sample_mean * np . ones ( 2 ) ) return ret
9694	def replace ( self , * * k ) : if self . date != 'infinity' : return Date ( self . date . replace ( * * k ) ) else : return Date ( 'infinity' )
9009	def next_instruction_in_row ( self ) : index = self . index_in_row + 1 if index >= len ( self . row_instructions ) : return None return self . row_instructions [ index ]
2548	def validate ( self , messages ) : messages = self . validate_version ( messages ) messages = self . validate_data_lics ( messages ) messages = self . validate_name ( messages ) messages = self . validate_spdx_id ( messages ) messages = self . validate_namespace ( messages ) messages = self . validate_ext_document_references ( messages ) messages = self . validate_creation_info ( messages ) messages = self . validate_package ( messages ) messages = self . validate_extracted_licenses ( messages ) messages = self . validate_reviews ( messages ) return messages
4799	def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
7543	def chunk_clusters ( data , sample ) : ## counter for split job submission num = 0 ## set optim size for chunks in N clusters. The first few chunks take longer ## because they contain larger clusters, so we create 4X as many chunks as ## processors so that they are split more evenly. optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) ## break up the file into smaller tmp files for each engine ## chunking by cluster is a bit trickier than chunking by N lines chunkslist = [ ] ## open to clusters with gzip . open ( sample . files . clusters , 'rb' ) as clusters : ## create iterator to sample 2 lines at a time pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## Use iterator to sample til end of cluster done = 0 while not done : ## grab optim clusters and write to file. done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
3051	def FromResponse ( cls , response ) : # device_code, user_code, and verification_url are required. kwargs = { 'device_code' : response [ 'device_code' ] , 'user_code' : response [ 'user_code' ] , } # The response may list the verification address as either # verification_url or verification_uri, so we check for both. verification_url = response . get ( 'verification_url' , response . get ( 'verification_uri' ) ) if verification_url is None : raise OAuth2DeviceCodeError ( 'No verification_url provided in server response' ) kwargs [ 'verification_url' ] = verification_url # expires_in and interval are optional. kwargs . update ( { 'interval' : response . get ( 'interval' ) , 'user_code_expiry' : None , } ) if 'expires_in' in response : kwargs [ 'user_code_expiry' ] = ( _UTCNOW ( ) + datetime . timedelta ( seconds = int ( response [ 'expires_in' ] ) ) ) return cls ( * * kwargs )
5181	def _url ( self , endpoint , path = None ) : log . debug ( '_url called with endpoint: {0} and path: {1}' . format ( endpoint , path ) ) try : endpoint = ENDPOINTS [ endpoint ] except KeyError : # If we reach this we're trying to query an endpoint that doesn't # exist. This shouldn't happen unless someone made a booboo. raise APIError url = '{base_url}/{endpoint}' . format ( base_url = self . base_url , endpoint = endpoint , ) if path is not None : url = '{0}/{1}' . format ( url , quote ( path ) ) return url
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) # Compute leave-one-out validation accuracy if # we actually received non-trivial partition info self . _accuracy = None
10015	def add_config_files_to_archive ( directory , filename , config = { } ) : with zipfile . ZipFile ( filename , 'a' ) as zip_file : for conf in config : for conf , tree in list ( conf . items ( ) ) : if 'yaml' in tree : content = yaml . dump ( tree [ 'yaml' ] , default_flow_style = False ) else : content = tree . get ( 'content' , '' ) out ( "Adding file " + str ( conf ) + " to archive " + str ( filename ) ) file_entry = zipfile . ZipInfo ( conf ) file_entry . external_attr = tree . get ( 'permissions' , 0o644 ) << 16 zip_file . writestr ( file_entry , content ) return filename
2441	def add_annotation_date ( self , doc , annotation_date ) : if len ( doc . annotations ) != 0 : if not self . annotation_date_set : self . annotation_date_set = True date = utils . datetime_from_iso_format ( annotation_date ) if date is not None : doc . annotations [ - 1 ] . annotation_date = date return True else : raise SPDXValueError ( 'Annotation::AnnotationDate' ) else : raise CardinalityError ( 'Annotation::AnnotationDate' ) else : raise OrderError ( 'Annotation::AnnotationDate' )
2115	def status ( self , pk = None , detail = False , * * kwargs ) : # Obtain the most recent project update job = self . last_job_data ( pk , * * kwargs ) # In most cases, we probably only want to know the status of the job # and the amount of time elapsed. However, if we were asked for # verbose information, provide it. if detail : return job # Print just the information we need. return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
10213	def calculate_subgraph_edge_overlap ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Tuple [ Mapping [ str , EdgeSet ] , Mapping [ str , Mapping [ str , EdgeSet ] ] , Mapping [ str , Mapping [ str , EdgeSet ] ] , Mapping [ str , Mapping [ str , float ] ] , ] : sg2edge = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue sg2edge [ d [ ANNOTATIONS ] [ annotation ] ] . add ( ( u , v ) ) subgraph_intersection = defaultdict ( dict ) subgraph_union = defaultdict ( dict ) result = defaultdict ( dict ) for sg1 , sg2 in itt . product ( sg2edge , repeat = 2 ) : subgraph_intersection [ sg1 ] [ sg2 ] = sg2edge [ sg1 ] & sg2edge [ sg2 ] subgraph_union [ sg1 ] [ sg2 ] = sg2edge [ sg1 ] | sg2edge [ sg2 ] result [ sg1 ] [ sg2 ] = len ( subgraph_intersection [ sg1 ] [ sg2 ] ) / len ( subgraph_union [ sg1 ] [ sg2 ] ) return sg2edge , subgraph_intersection , subgraph_union , result
4665	def id ( self ) : # Store signatures temporarily since they are not part of # transaction id sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) # Generage Hash of the seriliazed version h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) # recover signatures self . data [ "signatures" ] = sigs # Return properly truncated tx hash return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
3277	def handle_move ( self , dest_path ) : # path and destPath must be '/by_tag/<tag>/<resname>' if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
2713	def get_object ( cls , api_token , snapshot_id ) : snapshot = cls ( token = api_token , id = snapshot_id ) snapshot . load ( ) return snapshot
13100	def render ( self , * * kwargs ) : breadcrumbs = [ ] # this is the list of items we want to accumulate in the breadcrumb trail. # item[0] is the key into the kwargs["url"] object and item[1] is the name of the route # setting a route name to None means that it's needed to construct the route of the next item in the list # but shouldn't be included in the list itself (this is currently the case for work -- # at some point we probably should include work in the navigation) breadcrumbs = [ ] if "collections" in kwargs : breadcrumbs = [ { "title" : "Text Collections" , "link" : ".r_collections" , "args" : { } } ] if "parents" in kwargs [ "collections" ] : breadcrumbs += [ { "title" : parent [ "label" ] , "link" : ".r_collection_semantic" , "args" : { "objectId" : parent [ "id" ] , "semantic" : f_slugify ( parent [ "label" ] ) , } , } for parent in kwargs [ "collections" ] [ "parents" ] ] [ : : - 1 ] if "current" in kwargs [ "collections" ] : breadcrumbs . append ( { "title" : kwargs [ "collections" ] [ "current" ] [ "label" ] , "link" : None , "args" : { } } ) # don't link the last item in the trail if len ( breadcrumbs ) > 0 : breadcrumbs [ - 1 ] [ "link" ] = None return { "breadcrumbs" : breadcrumbs }
11110	def walk_directory_directories_relative_path ( self , relativePath = "" ) : # get directory info dict errorMessage = "" relativePath = os . path . normpath ( relativePath ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage for dname in dict . __getitem__ ( dirInfoDict , "directories" ) : yield os . path . join ( relativePath , dname )
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) # github doesn't exposes hashes of the archives being downloaded as far # as I can tell :( access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
1975	def sys_random ( self , cpu , buf , count , rnd_bytes ) : ret = 0 if count != 0 : if count > Decree . CGC_SSIZE_MAX or count < 0 : ret = Decree . CGC_EINVAL else : # TODO check count bytes from buf if buf not in cpu . memory or ( buf + count ) not in cpu . memory : logger . info ( "RANDOM: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT with open ( "/dev/urandom" , "rb" ) as f : data = f . read ( count ) self . syscall_trace . append ( ( "_random" , - 1 , data ) ) cpu . write_bytes ( buf , data ) # TODO check 4 bytes from rx_bytes if rnd_bytes : if rnd_bytes not in cpu . memory : logger . info ( "RANDOM: Not valid rnd_bytes. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rnd_bytes , len ( data ) , 32 ) logger . info ( "RANDOM(0x%08x, %d, 0x%08x) -> <%s>)" % ( buf , count , rnd_bytes , repr ( data [ : 10 ] ) ) ) return ret
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : # pragma: no cover parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
5332	def get_panels ( config ) : task = TaskPanels ( config ) task . execute ( ) task = TaskPanelsMenu ( config ) task . execute ( ) logging . info ( "Panels creation finished!" )
3050	def from_stream ( credential_filename ) : if credential_filename and os . path . isfile ( credential_filename ) : try : return _get_application_default_credential_from_file ( credential_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : extra_help = ( ' (provided as parameter to the ' 'from_stream() method)' ) _raise_exception_for_reading_json ( credential_filename , extra_help , error ) else : raise ApplicationDefaultCredentialsError ( 'The parameter passed to the from_stream() ' 'method should point to a file.' )
12568	def create_dataset ( self , ds_name , data , attrs = None , dtype = None ) : if ds_name in self . _datasets : ds = self . _datasets [ ds_name ] if ds . dtype != data . dtype : warnings . warn ( 'Dataset and data dtype are different!' ) else : if dtype is None : dtype = data . dtype ds = self . _group . create_dataset ( ds_name , data . shape , dtype = dtype ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) ds . read_direct ( data ) self . _datasets [ ds_name ] = ds return ds
4935	def chunks ( dictionary , chunk_size ) : iterable = iter ( dictionary ) for __ in range ( 0 , len ( dictionary ) , chunk_size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk_size ) }
4223	def init_backend ( limit = None ) : # save the limit for the chainer to honor backend . _limit = limit # get all keyrings passing the limit filter keyrings = filter ( limit , backend . get_all_keyring ( ) ) set_keyring ( load_env ( ) or load_config ( ) or max ( keyrings , default = fail . Keyring ( ) , key = backend . by_priority ) )
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
603	def add2DArray ( self , data , position = 111 , xlabel = None , ylabel = None , cmap = None , aspect = "auto" , interpolation = "nearest" , name = None ) : if cmap is None : # The default colormodel is an ugly blue-red model. cmap = cm . Greys ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . imshow ( data , cmap = cmap , aspect = aspect , interpolation = interpolation ) if self . _show : plt . draw ( ) if name is not None : if not os . path . exists ( "log" ) : os . mkdir ( "log" ) plt . savefig ( "log/{name}.png" . format ( name = name ) , bbox_inches = "tight" , figsize = ( 8 , 6 ) , dpi = 400 )
5979	def edge_pixels_from_mask ( mask ) : edge_pixel_total = total_edge_pixels_from_mask ( mask ) edge_pixels = np . zeros ( edge_pixel_total ) edge_index = 0 regular_index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : edge_pixels [ edge_index ] = regular_index edge_index += 1 regular_index += 1 return edge_pixels
10370	def build_edge_data_filter ( annotations : Mapping , partial_match : bool = True ) -> EdgePredicate : # noqa: D202 @ edge_predicate def annotation_dict_filter ( data : EdgeData ) -> bool : """A filter that matches edges with the given dictionary as a sub-dictionary.""" return subdict_matches ( data , annotations , partial_match = partial_match ) return annotation_dict_filter
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : if self . method : # retest within range if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : # pragma: no cover pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
6146	def IIR_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , Ripple_pass , Atten_stop , fs = 1.00 , ftype = 'butter' ) : b , a = signal . iirdesign ( [ 2 * float ( f_pass1 ) / fs , 2 * float ( f_pass2 ) / fs ] , [ 2 * float ( f_stop1 ) / fs , 2 * float ( f_stop2 ) / fs ] , Ripple_pass , Atten_stop , ftype = ftype , output = 'ba' ) sos = signal . iirdesign ( [ 2 * float ( f_pass1 ) / fs , 2 * float ( f_pass2 ) / fs ] , [ 2 * float ( f_stop1 ) / fs , 2 * float ( f_stop2 ) / fs ] , Ripple_pass , Atten_stop , ftype = ftype , output = 'sos' ) tag = 'IIR ' + ftype + ' order' print ( '%s = %d.' % ( tag , len ( a ) - 1 ) ) return b , a , sos
2481	def build ( self , * * kwargs ) : self . yacc = yacc . yacc ( module = self , * * kwargs )
8103	def load_profiles ( self ) : _profiles = { } for name , klass in inspect . getmembers ( profiles ) : if inspect . isclass ( klass ) and name . endswith ( 'Profile' ) and name != 'TuioProfile' : # Adding profile to the self.profiles dictionary profile = klass ( ) _profiles [ profile . address ] = profile # setting convenient variable to access objects of profile try : setattr ( self , profile . list_label , profile . objs ) except AttributeError : continue # Mapping callback method to every profile self . manager . add ( self . callback , profile . address ) return _profiles
3933	def _make_token_request ( session , token_request_data ) : try : r = session . post ( OAUTH2_TOKEN_REQUEST_URL , data = token_request_data ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Token request failed: {}' . format ( e ) ) else : res = r . json ( ) # If an error occurred, a key 'error' will contain an error code. if 'error' in res : raise GoogleAuthError ( 'Token request error: {!r}' . format ( res [ 'error' ] ) ) return res
5068	def traverse_pagination ( response , endpoint ) : results = response . get ( 'results' , [ ] ) next_page = response . get ( 'next' ) while next_page : querystring = parse_qs ( urlparse ( next_page ) . query , keep_blank_values = True ) response = endpoint . get ( * * querystring ) results += response . get ( 'results' , [ ] ) next_page = response . get ( 'next' ) return results
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
4754	def runlogs_to_html ( run_root ) : if not os . path . isdir ( run_root ) : return "CANNOT_LOCATE_LOGFILES" hook_enter = [ ] hook_exit = [ ] tcase = [ ] for fpath in glob . glob ( os . sep . join ( [ run_root , "*.log" ] ) ) : if "exit" in fpath : hook_exit . append ( fpath ) continue if "hook" in fpath : hook_enter . append ( fpath ) continue tcase . append ( fpath ) content = "" for fpath in hook_enter + tcase + hook_exit : content += "# BEGIN: run-log from log_fpath: %s\n" % fpath content += open ( fpath , "r" ) . read ( ) content += "# END: run-log from log_fpath: %s\n\n" % fpath return content
10298	def get_incorrect_names ( graph : BELGraph ) -> Mapping [ str , Set [ str ] ] : return { namespace : get_incorrect_names_by_namespace ( graph , namespace ) for namespace in get_namespaces ( graph ) }
9385	def convert_to_G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : # No unit value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
1115	def _make_prefix ( self ) : # Generate a unique anchor prefix so multiple tables # can exist on the same HTML page without conflicts. fromprefix = "from%d_" % HtmlDiff . _default_prefix toprefix = "to%d_" % HtmlDiff . _default_prefix HtmlDiff . _default_prefix += 1 # store prefixes so line format method has access self . _prefix = [ fromprefix , toprefix ]
7704	def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if replace : self . remove_item ( item . jid ) else : raise ValueError ( "JID already in the roster" ) index = len ( self . _items ) self . _items . append ( item ) self . _jids [ item . jid ] = index
6663	def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
12027	def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) #it should be in the first 30k of the file f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) # the whole protocol filename protocolID = protocol . split ( " " ) [ 0 ] # just the first number return protocolID
12129	def _build_specs ( self , specs , kwargs , fp_precision ) : if specs is None : overrides = param . ParamOverrides ( self , kwargs , allow_extra_keywords = True ) extra_kwargs = overrides . extra_keywords ( ) kwargs = dict ( [ ( k , v ) for ( k , v ) in kwargs . items ( ) if k not in extra_kwargs ] ) rounded_specs = list ( self . round_floats ( [ extra_kwargs ] , fp_precision ) ) if extra_kwargs == { } : return [ ] , kwargs , True else : return rounded_specs , kwargs , False return list ( self . round_floats ( specs , fp_precision ) ) , kwargs , True
9392	def calc_key_stats ( self , metric_store ) : stats_to_calculate = [ 'mean' , 'std' , 'min' , 'max' ] # TODO: get input from user percentiles_to_calculate = range ( 0 , 100 , 1 ) # TODO: get input from user for column , groups_store in metric_store . items ( ) : for group , time_store in groups_store . items ( ) : data = metric_store [ column ] [ group ] . values ( ) if self . groupby : column_name = group + '.' + column else : column_name = column if column . startswith ( 'qps' ) : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( data , stats_to_calculate , percentiles_to_calculate ) else : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( list ( heapq . merge ( * data ) ) , stats_to_calculate , percentiles_to_calculate ) self . update_summary_stats ( column_name )
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
13726	def set_connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
544	def _updateModelDBResults ( self ) : # ----------------------------------------------------------------------- # Get metrics metrics = self . _getMetrics ( ) # ----------------------------------------------------------------------- # Extract report metrics that match the requested report REs reportDict = dict ( [ ( k , metrics [ k ] ) for k in self . _reportMetricLabels ] ) # ----------------------------------------------------------------------- # Extract the report item that matches the optimize key RE # TODO cache optimizedMetricLabel sooner metrics = self . _getMetrics ( ) optimizeDict = dict ( ) if self . _optimizeKeyPattern is not None : optimizeDict [ self . _optimizedMetricLabel ] = metrics [ self . _optimizedMetricLabel ] # ----------------------------------------------------------------------- # Update model results results = json . dumps ( ( metrics , optimizeDict ) ) self . _jobsDAO . modelUpdateResults ( self . _modelID , results = results , metricValue = optimizeDict . values ( ) [ 0 ] , numRecords = ( self . _currentRecordIndex + 1 ) ) self . _logger . debug ( "Model Results: modelID=%s; numRecords=%s; results=%s" % ( self . _modelID , self . _currentRecordIndex + 1 , results ) ) return
8692	def init ( self ) : # ignore 400 (IndexAlreadyExistsException) when creating an index self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
9973	def _get_namedrange ( book , rangename , sheetname = None ) : def cond ( namedef ) : if namedef . type . upper ( ) == "RANGE" : if namedef . name . upper ( ) == rangename . upper ( ) : if sheetname is None : if not namedef . localSheetId : return True else : # sheet local name sheet_id = [ sht . upper ( ) for sht in book . sheetnames ] . index ( sheetname . upper ( ) ) if namedef . localSheetId == sheet_id : return True return False def get_destinations ( name_def ) : """Workaround for the bug in DefinedName.destinations""" from openpyxl . formula import Tokenizer from openpyxl . utils . cell import SHEETRANGE_RE if name_def . type == "RANGE" : tok = Tokenizer ( "=" + name_def . value ) for part in tok . items : if part . subtype == "RANGE" : m = SHEETRANGE_RE . match ( part . value ) if m . group ( "quoted" ) : sheet_name = m . group ( "quoted" ) else : sheet_name = m . group ( "notquoted" ) yield sheet_name , m . group ( "cells" ) namedef = next ( ( item for item in book . defined_names . definedName if cond ( item ) ) , None ) if namedef is None : return None dests = get_destinations ( namedef ) xlranges = [ ] sheetnames_upper = [ name . upper ( ) for name in book . sheetnames ] for sht , addr in dests : if sheetname : sht = sheetname index = sheetnames_upper . index ( sht . upper ( ) ) xlranges . append ( book . worksheets [ index ] [ addr ] ) if len ( xlranges ) == 1 : return xlranges [ 0 ] else : return xlranges
5766	def _setup_evp_encrypt_decrypt ( cipher , data ) : evp_cipher = { 'aes128' : libcrypto . EVP_aes_128_cbc , 'aes192' : libcrypto . EVP_aes_192_cbc , 'aes256' : libcrypto . EVP_aes_256_cbc , 'rc2' : libcrypto . EVP_rc2_cbc , 'rc4' : libcrypto . EVP_rc4 , 'des' : libcrypto . EVP_des_cbc , 'tripledes_2key' : libcrypto . EVP_des_ede_cbc , 'tripledes_3key' : libcrypto . EVP_des_ede3_cbc , } [ cipher ] ( ) if cipher == 'rc4' : buffer_size = len ( data ) else : block_size = { 'aes128' : 16 , 'aes192' : 16 , 'aes256' : 16 , 'rc2' : 8 , 'des' : 8 , 'tripledes_2key' : 8 , 'tripledes_3key' : 8 , } [ cipher ] buffer_size = block_size * int ( math . ceil ( len ( data ) / block_size ) ) return ( evp_cipher , buffer_size )
7162	def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
8698	def __expect ( self , exp = '> ' , timeout = None ) : timeout_before = self . _port . timeout timeout = timeout or self . _timeout #do NOT set timeout on Windows if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout # Finish as soon as either exp matches or we run out of time (work like dump, but faster on success) data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . _port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise CommunicationTimeout ( 'Timeout waiting for data' , data ) if not data . endswith ( exp ) and len ( exp ) > 0 : raise BadResponseException ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before return data
11981	def set_netmask ( self , netmask ) : self . set ( ip = self . _ip , netmask = netmask )
9667	def itertable ( table ) : for item in table : res = { k . lower ( ) : nfd ( v ) if isinstance ( v , text_type ) else v for k , v in item . items ( ) } for extra in res . pop ( 'extra' , [ ] ) : k , _ , v = extra . partition ( ':' ) res [ k . strip ( ) ] = v . strip ( ) yield res
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
8631	def get_project_by_id ( session , project_id , project_details = None , user_details = None ) : # GET /api/projects/0.1/projects/<int:project_id> query = { } if project_details : query . update ( project_details ) if user_details : query . update ( user_details ) response = make_get_request ( session , 'projects/{}' . format ( project_id ) , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1290	def tf_step ( self , time , variables , * * kwargs ) : fn_loss = kwargs [ "fn_loss" ] if variables is None : variables = tf . trainable_variables return tf . gradients ( fn_loss , variables )
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
5357	def _add_to_conf ( self , new_conf ) : for section in new_conf : if section not in self . conf : self . conf [ section ] = new_conf [ section ] else : for param in new_conf [ section ] : self . conf [ section ] [ param ] = new_conf [ section ] [ param ]
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
5125	def simulate ( self , n = 1 , t = None ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if t is None : for dummy in range ( n ) : self . _simulate_next_event ( slow = False ) else : now = self . _t while self . _t < now + t : self . _simulate_next_event ( slow = False )
11215	def compare_token ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) _ , expected_sig_seg = expected . rsplit ( b'.' , 1 ) _ , actual_sig_seg = actual . rsplit ( b'.' , 1 ) expected_sig = util . b64_decode ( expected_sig_seg ) actual_sig = util . b64_decode ( actual_sig_seg ) return compare_signature ( expected_sig , actual_sig )
5549	def get_hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
4806	def _fmt_args_kwargs ( self , * some_args , * * some_kwargs ) : if some_args : out_args = str ( some_args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some_kwargs : out_kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some_kwargs [ k ] ) for k in sorted ( some_kwargs . keys ( ) ) ] ] ) if some_args and some_kwargs : return out_args + ', ' + out_kwargs elif some_args : return out_args elif some_kwargs : return out_kwargs else : return ''
6926	def cursor ( self , handle , dictcursor = False ) : if handle in self . cursors : return self . cursors [ handle ] else : if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return self . cursors [ handle ]
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , * * kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , * * kwargs )
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : # Allow to use function interactively root = '.' else : root = dirname ( __file__ ) # The code below reads text file with unknown encoding in # in Python2/3 compatible way. Reading this text file # without specifying encoding will fail in Python 3 on some # systems (see http://goo.gl/5XmOH). Specifying encoding as # open() parameter is incompatible with Python 2 # cp437 is the encoding without missing points, safe against: # UnicodeDecodeError: 'charmap' codec can't decode byte... for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : # __version__ = "0.9" return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
1611	def ParseNolintSuppressions ( filename , raw_line , linenum , error ) : matched = Search ( r'\bNOLINT(NEXTLINE)?\b(\([^)]+\))?' , raw_line ) if matched : if matched . group ( 1 ) : suppressed_line = linenum + 1 else : suppressed_line = linenum category = matched . group ( 2 ) if category in ( None , '(*)' ) : # => "suppress all" _error_suppressions . setdefault ( None , set ( ) ) . add ( suppressed_line ) else : if category . startswith ( '(' ) and category . endswith ( ')' ) : category = category [ 1 : - 1 ] if category in _ERROR_CATEGORIES : _error_suppressions . setdefault ( category , set ( ) ) . add ( suppressed_line ) elif category not in _LEGACY_ERROR_CATEGORIES : error ( filename , linenum , 'readability/nolint' , 5 , 'Unknown NOLINT error category: %s' % category )
5074	def track_enrollment ( pathway , user_id , course_run_id , url_path = None ) : track_event ( user_id , 'edx.bi.user.enterprise.onboarding' , { 'pathway' : pathway , 'url_path' : url_path , 'course_run_id' : course_run_id , } )
7149	def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian_swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word_list [ w1 ] , cls . word_list [ w2 ] , cls . word_list [ w3 ] ] checksum = cls . get_checksum ( " " . join ( out ) ) out . append ( checksum ) return " " . join ( out )
7537	def branch_assembly ( args , parsedict ) : ## Get the current assembly data = getassembly ( args , parsedict ) ## get arguments to branch command bargs = args . branch ## get new name, trim off .txt if it was accidentally added newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] ## look for subsamples if len ( bargs ) > 1 : ## Branching and subsampling at step 6 is a bad idea, it messes up ## indexing into the hdf5 cluster file. Warn against this. if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass ## TODODODODODO #print("wat") ## are we removing or keeping listed samples? subsamples = bargs [ 1 : ] ## drop the matching samples if bargs [ 1 ] == "-" : ## check drop names fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) ## If the arg after the new param name is a file that exists if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) ## keeping all samples else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
12166	def remove_listener ( self , event , listener ) : with contextlib . suppress ( ValueError ) : self . _listeners [ event ] . remove ( listener ) return True with contextlib . suppress ( ValueError ) : self . _once [ event ] . remove ( listener ) return True return False
12005	def _remove_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) header_size = version_info [ 'header_size' ] if options [ 'flags' ] [ 'timestamp' ] : header_size += version_info [ 'timestamp_size' ] data = data [ header_size : ] return data
13732	def validate_is_boolean_true ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
10956	def get ( self , name ) : for c in self . comps : if c . category == name : return c return None
11635	def oauth2_access_parser ( self , raw_access ) : parsed_access = json . loads ( raw_access . content . decode ( 'utf-8' ) ) self . access_token = parsed_access [ 'access_token' ] self . token_type = parsed_access [ 'token_type' ] self . refresh_token = parsed_access [ 'refresh_token' ] self . guid = parsed_access [ 'xoauth_yahoo_guid' ] credentials = { 'access_token' : self . access_token , 'token_type' : self . token_type , 'refresh_token' : self . refresh_token , 'guid' : self . guid } return credentials
10241	def count_authors_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , typing . Counter [ str ] ] : authors = group_as_dict ( _iter_authors_by_annotation ( graph , annotation = annotation ) ) return count_defaultdict ( authors )
7970	def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
4546	def fill_circle ( setter , x0 , y0 , r , color = None ) : _draw_fast_vline ( setter , x0 , y0 - r , 2 * r + 1 , color ) _fill_circle_helper ( setter , x0 , y0 , r , 3 , 0 , color )
8770	def _lswitch_select_open ( self , context , switches = None , * * kwargs ) : if switches is not None : for res in switches [ "results" ] : count = res [ "_relations" ] [ "LogicalSwitchStatus" ] [ "lport_count" ] if ( self . limits [ 'max_ports_per_switch' ] == 0 or count < self . limits [ 'max_ports_per_switch' ] ) : return res [ "uuid" ] return None
5984	def image_psf_shape_tag_from_image_psf_shape ( image_psf_shape ) : if image_psf_shape is None : return '' else : y = str ( image_psf_shape [ 0 ] ) x = str ( image_psf_shape [ 1 ] ) return ( '_image_psf_' + y + 'x' + x )
3090	def locked_put ( self , credentials ) : entity = self . _model . get_or_insert ( self . _key_name ) setattr ( entity , self . _property_name , credentials ) entity . put ( ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) )
11791	def mrv ( assignment , csp ) : return argmin_random_tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num_legal_values ( csp , var , assignment ) )
1419	def get_scheduler_location ( self , topologyName , callback = None ) : isWatching = False # Temp dict used to return result # if callback is not provided. ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : """ Custom callback to get the scheduler location right now. """ ret [ "result" ] = data self . _get_scheduler_location_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
4991	def post ( self , request , * args , * * kwargs ) : # pylint: disable=unused-variable enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , * * kwargs )
4372	def get_socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None # you ask for a session that doesn't exist! if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr_hits ( ) return socket
2853	def mpsse_read_gpio ( self ) : # Send command to read low byte and high byte. self . _write ( '\x81\x83' ) # Wait for 2 byte response. data = self . _poll_read ( 2 ) # Assemble response into 16 bit value. low_byte = ord ( data [ 0 ] ) high_byte = ord ( data [ 1 ] ) logger . debug ( 'Read MPSSE GPIO low byte = {0:02X} and high byte = {1:02X}' . format ( low_byte , high_byte ) ) return ( high_byte << 8 ) | low_byte
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) # hex addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : # pragma: no cover key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : # pragma: no cover key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : # pragma: no cover raise ValueError ( "No scrypt module loaded" ) # pragma: no cover ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
8625	def set_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_put_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotSetException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3020	def create_with_claims ( self , claims ) : new_kwargs = dict ( self . _kwargs ) new_kwargs . update ( claims ) result = self . __class__ ( self . _service_account_email , self . _signer , scopes = self . _scopes , private_key_id = self . _private_key_id , client_id = self . client_id , user_agent = self . _user_agent , * * new_kwargs ) result . token_uri = self . token_uri result . revoke_uri = self . revoke_uri result . _private_key_pkcs8_pem = self . _private_key_pkcs8_pem result . _private_key_pkcs12 = self . _private_key_pkcs12 result . _private_key_password = self . _private_key_password return result
4674	def getPrivateKeyForPublicKey ( self , pub ) : if str ( pub ) not in self . store : raise KeyNotFound return self . store . getPrivateKeyForPublicKey ( str ( pub ) )
967	def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]
2370	def dump ( self ) : for table in self . tables : print ( "*** %s ***" % table . name ) table . dump ( )
6752	def get_tasks ( self ) : tasks = set ( self . tasks ) #DEPRECATED for _name in dir ( self ) : # Skip properties so we don't accidentally execute any methods. if isinstance ( getattr ( type ( self ) , _name , None ) , property ) : continue attr = getattr ( self , _name ) if hasattr ( attr , '__call__' ) and getattr ( attr , 'is_task' , False ) : tasks . add ( _name ) return sorted ( tasks )
1842	def JNP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . PF , target . read ( ) , cpu . PC )
4413	def add ( self , requester : int , track : dict ) : self . queue . append ( AudioTrack ( ) . build ( track , requester ) )
2570	def send_UDP_message ( self , message ) : x = 0 if self . tracking_enabled : try : proc = udp_messenger ( self . domain_name , self . UDP_IP , self . UDP_PORT , self . sock_timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( "Usage tracking failed: {}" . format ( e ) ) else : x = - 1 return x
9795	def group ( ctx , project , group ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group
11637	def get_data ( filename ) : name , ext = get_file_extension ( filename ) func = json_get_data if ext == '.json' else yaml_get_data return func ( filename )
1873	def MOVLPD ( cpu , dest , src ) : value = src . read ( ) if src . size == 64 and dest . size == 128 : value = ( dest . read ( ) & 0xffffffffffffffff0000000000000000 ) | Operators . ZEXTEND ( value , 128 ) dest . write ( value )
9808	def create_tarfile ( files , project_name ) : fd , filename = tempfile . mkstemp ( prefix = "polyaxon_{}" . format ( project_name ) , suffix = '.tar.gz' ) with tarfile . open ( filename , "w:gz" ) as tar : for f in files : tar . add ( f ) yield filename # clear os . close ( fd ) os . remove ( filename )
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
13136	def request ( key , features , query , timeout = 5 ) : data = { } data [ 'key' ] = key data [ 'features' ] = '/' . join ( [ f for f in features if f in FEATURES ] ) data [ 'query' ] = quote ( query ) data [ 'format' ] = 'json' r = requests . get ( API_URL . format ( * * data ) , timeout = timeout ) results = json . loads ( _unicode ( r . content ) ) return results
10361	def rewire_targets ( graph , rewiring_probability ) : if not all_edges_consistent ( graph ) : raise ValueError ( '{} is not consistent' . format ( graph ) ) result = graph . copy ( ) nodes = result . nodes ( ) for u , v in result . edges ( ) : if random . random ( ) < rewiring_probability : continue w = random . choice ( nodes ) while w == u or result . has_edge ( u , w ) : w = random . choice ( nodes ) result . add_edge ( w , v ) result . remove_edge ( u , v ) return result
494	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = SteadyDB . connect ( * * _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
7060	def sqs_create_queue ( queue_name , options = None , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : if isinstance ( options , dict ) : resp = client . create_queue ( QueueName = queue_name , Attributes = options ) else : resp = client . create_queue ( QueueName = queue_name ) if resp is not None : return { 'url' : resp [ 'QueueUrl' ] , 'name' : queue_name } else : LOGERROR ( 'could not create the specified queue: %s with options: %s' % ( queue_name , options ) ) return None except Exception as e : LOGEXCEPTION ( 'could not create the specified queue: %s with options: %s' % ( queue_name , options ) ) return None
2431	def build_tool ( self , doc , entity ) : match = self . tool_re . match ( entity ) if match and validations . validate_tool_name ( match . group ( self . TOOL_NAME_GROUP ) ) : name = match . group ( self . TOOL_NAME_GROUP ) return creationinfo . Tool ( name ) else : raise SPDXValueError ( 'Failed to extract tool name' )
9163	def lookup_api_key_info ( ) : info = { } with db_connect ( ) as conn : with conn . cursor ( ) as cursor : cursor . execute ( ALL_KEY_INFO_SQL_STMT ) for row in cursor . fetchall ( ) : id , key , name , groups = row user_id = "api_key:{}" . format ( id ) info [ key ] = dict ( id = id , user_id = user_id , name = name , groups = groups ) return info
13221	def breakfast ( self , message = "Breakfast is ready" , shout : bool = False ) : return self . helper . output ( message , shout )
12627	def iter_recursive_find ( folder_path , * regex ) : for root , dirs , files in os . walk ( folder_path ) : if len ( files ) > 0 : outlist = [ ] for f in files : for reg in regex : if re . search ( reg , f ) : outlist . append ( op . join ( root , f ) ) if len ( outlist ) == len ( regex ) : yield outlist
9121	def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) # set the message dropbox . message = data . get ( 'message' ) # recognize submission from watchdog if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) # a non-js client might have uploaded an attachment via the form's fileupload field: if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) # now we can call the process method dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
6569	def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , * * kwargs ) except TypeError : return False else : return True
2793	def create ( self ) : params = { "name" : self . name , "type" : self . type , "dns_names" : self . dns_names , "private_key" : self . private_key , "leaf_certificate" : self . leaf_certificate , "certificate_chain" : self . certificate_chain } data = self . get_data ( "certificates/" , type = POST , params = params ) if data : self . id = data [ 'certificate' ] [ 'id' ] self . not_after = data [ 'certificate' ] [ 'not_after' ] self . sha1_fingerprint = data [ 'certificate' ] [ 'sha1_fingerprint' ] self . created_at = data [ 'certificate' ] [ 'created_at' ] self . type = data [ 'certificate' ] [ 'type' ] self . dns_names = data [ 'certificate' ] [ 'dns_names' ] self . state = data [ 'certificate' ] [ 'state' ] return self
880	def create ( modelConfig , logLevel = logging . ERROR ) : logger = ModelFactory . __getLogger ( ) logger . setLevel ( logLevel ) logger . debug ( "ModelFactory returning Model from dict: %s" , modelConfig ) modelClass = None if modelConfig [ 'model' ] == "HTMPrediction" : modelClass = HTMPredictionModel elif modelConfig [ 'model' ] == "TwoGram" : modelClass = TwoGramModel elif modelConfig [ 'model' ] == "PreviousValue" : modelClass = PreviousValueModel else : raise Exception ( "ModelFactory received unsupported Model type: %s" % modelConfig [ 'model' ] ) return modelClass ( * * modelConfig [ 'modelParams' ] )
11934	def auto_widget ( field ) : # Auto-detect info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( * * info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
10834	def query_admins_by_group_ids ( cls , groups_ids = None ) : assert groups_ids is None or isinstance ( groups_ids , list ) query = db . session . query ( Group . id , func . count ( GroupAdmin . id ) ) . join ( GroupAdmin ) . group_by ( Group . id ) if groups_ids : query = query . filter ( Group . id . in_ ( groups_ids ) ) return query
13325	def activate ( paths , skip_local , skip_shared ) : if not paths : ctx = click . get_current_context ( ) if cpenv . get_active_env ( ) : ctx . invoke ( info ) return click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples: \n' ' cpenv activate my_env\n' ' cpenv activate ./relative/path/to/my_env\n' ' cpenv activate my_env my_module\n' ) click . echo ( examples ) return if skip_local : cpenv . module_resolvers . remove ( cpenv . resolver . module_resolver ) cpenv . module_resolvers . remove ( cpenv . resolver . active_env_module_resolver ) if skip_shared : cpenv . module_resolvers . remove ( cpenv . resolver . modules_path_resolver ) try : r = cpenv . resolve ( * paths ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) return resolved = set ( r . resolved ) active_modules = set ( ) env = cpenv . get_active_env ( ) if env : active_modules . add ( env ) active_modules . update ( cpenv . get_active_modules ( ) ) new_modules = resolved - active_modules old_modules = active_modules & resolved if old_modules and not new_modules : click . echo ( '\nModules already active: ' + bold ( ' ' . join ( [ obj . name for obj in old_modules ] ) ) ) return if env and contains_env ( new_modules ) : click . echo ( '\nUse bold(exit) to leave your active environment first.' ) return click . echo ( '\nResolved the following modules...' ) click . echo ( format_objects ( r . resolved ) ) r . activate ( ) click . echo ( blue ( '\nLaunching subshell...' ) ) modules = sorted ( resolved | active_modules , key = _type_and_name ) prompt = ':' . join ( [ obj . name for obj in modules ] ) shell . launch ( prompt )
3580	def disconnect_devices ( self , service_uuids = [ ] ) : service_uuids = set ( service_uuids ) for device in self . list_devices ( ) : # Skip devices that aren't connected. if not device . is_connected : continue device_uuids = set ( map ( lambda x : x . uuid , device . list_services ( ) ) ) if device_uuids >= service_uuids : # Found a device that has at least the requested services, now # disconnect from it. device . disconnect ( )
1782	def AAS ( cpu ) : if ( cpu . AL & 0x0F > 9 ) or cpu . AF == 1 : cpu . AX = cpu . AX - 6 cpu . AH = cpu . AH - 1 cpu . AF = True cpu . CF = True else : cpu . AF = False cpu . CF = False cpu . AL = cpu . AL & 0x0f
6838	def distrib_id ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : # lsb_release works on Ubuntu and Debian >= 6.0 # but is not always included in other distros if is_file ( '/usr/bin/lsb_release' ) : id_ = run ( 'lsb_release --id --short' ) . strip ( ) . lower ( ) if id in [ 'arch' , 'archlinux' ] : # old IDs used before lsb-release 1.4-14 id_ = ARCH return id_ else : if is_file ( '/etc/debian_version' ) : return DEBIAN elif is_file ( '/etc/fedora-release' ) : return FEDORA elif is_file ( '/etc/arch-release' ) : return ARCH elif is_file ( '/etc/redhat-release' ) : release = run ( 'cat /etc/redhat-release' ) if release . startswith ( 'Red Hat Enterprise Linux' ) : return REDHAT elif release . startswith ( 'CentOS' ) : return CENTOS elif release . startswith ( 'Scientific Linux' ) : return SLES elif is_file ( '/etc/gentoo-release' ) : return GENTOO elif kernel == SUNOS : return SUNOS
9030	def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
11548	def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 #if false, factor of 1, if true, factor of 2 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : #found no info from idx dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
2022	def SIGNEXTEND ( self , size , value ) : # FIXME maybe use Operators.SEXTEND testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
2656	def makedirs ( self , path , mode = 511 , exist_ok = False ) : if exist_ok is False and self . isdir ( path ) : raise OSError ( 'Target directory {} already exists' . format ( path ) ) self . execute_wait ( 'mkdir -p {}' . format ( path ) ) self . sftp_client . chmod ( path , mode )
12242	def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
10025	def get_versions ( self ) : response = self . ebs . describe_application_versions ( application_name = self . app_name ) return response [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ]
4468	def deserialize ( encoded , * * kwargs ) : params = jsonpickle . decode ( encoded , * * kwargs ) return __reconstruct ( params )
7467	def summarize_results ( self , individual_results = False ) : ## return results depending on algorithm ## algorithm 00 if ( not self . params . infer_delimit ) & ( not self . params . infer_sptree ) : if individual_results : ## return a list of parsed CSV results return [ _parse_00 ( i ) for i in self . files . outfiles ] else : ## concatenate each CSV and then get stats w/ describe return pd . concat ( [ pd . read_csv ( i , sep = '\t' , index_col = 0 ) for i in self . files . mcmcfiles ] ) . describe ( ) . T ## algorithm 01 if self . params . infer_delimit & ( not self . params . infer_sptree ) : return _parse_01 ( self . files . outfiles , individual = individual_results ) ## others else : return "summary function not yet ready for this type of result"
13460	def event_all_comments_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) comments = event . all_comments page = int ( request . GET . get ( 'page' , 99999 ) ) # feed empty page by default to push to last page is_paginated = False if comments : paginator = Paginator ( comments , 50 ) # Show 50 comments per page try : comments = paginator . page ( page ) except EmptyPage : # If page is out of range (e.g. 9999), deliver last page of results. comments = paginator . page ( paginator . num_pages ) is_paginated = comments . has_other_pages ( ) return render ( request , 'happenings/event_comments.html' , { "event" : event , "comment_list" : comments , "object_list" : comments , "page_obj" : comments , "page" : page , "is_paginated" : is_paginated , "key" : key } )
12970	def _doCascadeFetch ( obj ) : obj . validateModel ( ) if not obj . foreignFields : return # NOTE: Currently this fetches using one transaction per object. Implementation for actual resolution is in # IndexedRedisModel.__getattribute__ for foreignField in obj . foreignFields : subObjsData = object . __getattribute__ ( obj , foreignField ) if not subObjsData : setattr ( obj , str ( foreignField ) , irNull ) continue subObjs = subObjsData . getObjs ( ) for subObj in subObjs : if isIndexedRedisModel ( subObj ) : IndexedRedisQuery . _doCascadeFetch ( subObj )
10393	def workflow_all_aggregate ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) : results = { } bioprocess_nodes = list ( get_nodes_by_function ( graph , BIOPROCESS ) ) for bioprocess_node in tqdm ( bioprocess_nodes ) : subgraph = generate_mechanism ( graph , bioprocess_node , key = key ) try : results [ bioprocess_node ] = workflow_aggregate ( graph = subgraph , node = bioprocess_node , key = key , tag = tag , default_score = default_score , runs = runs , aggregator = aggregator ) except Exception : log . exception ( 'could not run on %' , bioprocess_node ) return results
4677	def getMemoKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) key = self . getPrivateKeyForPublicKey ( account [ "options" ] [ "memo_key" ] ) if key : return key return False
109	def show_grid ( images , rows = None , cols = None ) : grid = draw_grid ( images , rows = rows , cols = cols ) imshow ( grid )
6183	def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
4268	def generate_thumbnail ( source , outname , box , fit = True , options = None , thumb_fit_centering = ( 0.5 , 0.5 ) ) : logger = logging . getLogger ( __name__ ) img = _read_image ( source ) original_format = img . format if fit : img = ImageOps . fit ( img , box , PILImage . ANTIALIAS , centering = thumb_fit_centering ) else : img . thumbnail ( box , PILImage . ANTIALIAS ) outformat = img . format or original_format or 'JPEG' logger . debug ( 'Save thumnail image: %s (%s)' , outname , outformat ) save_image ( img , outname , outformat , options = options , autoconvert = True )
3556	def power_off ( self , timeout_sec = TIMEOUT_SEC ) : # Turn off bluetooth. self . _powered_off . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 0 ) if not self . _powered_off . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power off!' )
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
2903	def ref ( function , callback = None ) : try : function . __func__ except AttributeError : return _WeakMethodFree ( function , callback ) return _WeakMethodBound ( function , callback )
3332	def init_logging ( config ) : verbose = config . get ( "verbose" , 3 ) enable_loggers = config . get ( "enable_loggers" , [ ] ) if enable_loggers is None : enable_loggers = [ ] logger_date_format = config . get ( "logger_date_format" , "%Y-%m-%d %H:%M:%S" ) logger_format = config . get ( "logger_format" , "%(asctime)s.%(msecs)03d - <%(thread)d> %(name)-27s %(levelname)-8s: %(message)s" , ) formatter = logging . Formatter ( logger_format , logger_date_format ) # Define handlers consoleHandler = logging . StreamHandler ( sys . stdout ) # consoleHandler = logging.StreamHandler(sys.stderr) consoleHandler . setFormatter ( formatter ) # consoleHandler.setLevel(logging.DEBUG) # Add the handlers to the base logger logger = logging . getLogger ( BASE_LOGGER_NAME ) if verbose >= 4 : # --verbose logger . setLevel ( logging . DEBUG ) elif verbose == 3 : # default logger . setLevel ( logging . INFO ) elif verbose == 2 : # --quiet logger . setLevel ( logging . WARN ) # consoleHandler.setLevel(logging.WARN) elif verbose == 1 : # -qq logger . setLevel ( logging . ERROR ) # consoleHandler.setLevel(logging.WARN) else : # -qqq logger . setLevel ( logging . CRITICAL ) # consoleHandler.setLevel(logging.ERROR) # Don't call the root's handlers after our custom handlers logger . propagate = False # Remove previous handlers for hdlr in logger . handlers [ : ] : # Must iterate an array copy try : hdlr . flush ( ) hdlr . close ( ) except Exception : pass logger . removeHandler ( hdlr ) logger . addHandler ( consoleHandler ) if verbose >= 3 : for e in enable_loggers : if not e . startswith ( BASE_LOGGER_NAME + "." ) : e = BASE_LOGGER_NAME + "." + e lg = logging . getLogger ( e . strip ( ) ) lg . setLevel ( logging . DEBUG )
1681	def AddFilters ( self , filters ) : for filt in filters . split ( ',' ) : clean_filt = filt . strip ( ) if clean_filt : self . filters . append ( clean_filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise ValueError ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )
11173	def strsettings ( self , indent = 0 , maxindent = 25 , width = 0 ) : out = [ ] makelabel = lambda name : ' ' * indent + name + ': ' settingsindent = _autoindent ( [ makelabel ( s ) for s in self . options ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] label = makelabel ( name ) settingshelp = "%s(%s): %s" % ( option . formatname , option . strvalue , option . location ) wrapped = self . _wrap_labelled ( label , settingshelp , settingsindent , width ) out . extend ( wrapped ) return '\n' . join ( out )
10576	def _create_element_list_ ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , * * kw ) : return assertion . test ( subject , expected , * args , * * kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators # Register operator Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
3749	def calculate_P ( self , T , P , method ) : if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return # no need to warn if it's already outside. while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" # imaginary swhlab module path if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
12719	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
9701	def send ( self , msg ) : # Create a sliplib Driver slipDriver = sliplib . Driver ( ) # Package data in slip format slipData = slipDriver . send ( msg ) # Send data over serial port res = self . _serialPort . write ( slipData ) # Return number of bytes transmitted over serial port return res
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
8571	def get_nic ( self , datacenter_id , server_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s?depth=%s' % ( datacenter_id , server_id , nic_id , str ( depth ) ) ) return response
6455	def dist_mlipns ( src , tar , threshold = 0.25 , max_mismatches = 2 ) : return MLIPNS ( ) . dist ( src , tar , threshold , max_mismatches )
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( * * self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
11740	def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . _first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . _first [ symbol ] : break else : ret . add ( EPSILON ) return ret
7606	def search_clans ( self , * * params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , * * params )
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
4543	def compose_events ( events , condition = all ) : events = list ( events ) master_event = threading . Event ( ) def changed ( ) : if condition ( e . is_set ( ) for e in events ) : master_event . set ( ) else : master_event . clear ( ) def add_changed ( f ) : @ functools . wraps ( f ) def wrapped ( ) : f ( ) changed ( ) return wrapped for e in events : e . set = add_changed ( e . set ) e . clear = add_changed ( e . clear ) changed ( ) return master_event
1064	def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None
2875	def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
6445	def _cond_x ( self , word , suffix_len ) : return word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 : - suffix_len ] == 'u' and word [ - suffix_len - 1 ] == 'e' )
4055	def everything ( self , query ) : try : items = [ ] items . extend ( query ) while self . links . get ( "next" ) : items . extend ( self . follow ( ) ) except TypeError : # we have a bibliography object ughh items = copy . deepcopy ( query ) while self . links . get ( "next" ) : items . entries . extend ( self . follow ( ) . entries ) return items
289	def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , * * kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , * * kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , * * kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax
13851	def is_hidden ( path ) : full_path = os . path . abspath ( path ) name = os . path . basename ( full_path ) def no ( path ) : return False platform_hidden = globals ( ) . get ( 'is_hidden_' + platform . system ( ) , no ) return name . startswith ( '.' ) or platform_hidden ( full_path )
10897	def get_scale_from_raw ( raw , scaled ) : t0 , t1 = scaled . min ( ) , scaled . max ( ) r0 , r1 = float ( raw . min ( ) ) , float ( raw . max ( ) ) rmin = ( t1 * r0 - t0 * r1 ) / ( t1 - t0 ) rmax = ( r1 - r0 ) / ( t1 - t0 ) + rmin return ( rmin , rmax )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 # for numerical reasons we subtract the max logit # (mathematically it doesn't matter!) # otherwise exp(logits) might become too large or too small logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
444	def roi_pooling ( input , rois , pool_height , pool_width ) : # TODO(maciek): ops scope out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
9764	def cluster ( node ) : cluster_client = PolyaxonClient ( ) . cluster if node : try : node_config = cluster_client . get_node ( node ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load node `{}` info.' . format ( node ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_node_info ( node_config ) else : try : cluster_config = cluster_client . get_cluster ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load cluster info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_cluster_info ( cluster_config )
1233	def from_spec ( spec , kwargs = None ) : distribution = util . get_object ( obj = spec , predefined_objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution
4658	def as_base ( self , base ) : if base == self [ "base" ] [ "symbol" ] : return self . copy ( ) elif base == self [ "quote" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
5010	def _call_post_with_user_override ( self , sap_user_id , url , payload ) : SAPSuccessFactorsEnterpriseCustomerConfiguration = apps . get_model ( # pylint: disable=invalid-name 'sap_success_factors' , 'SAPSuccessFactorsEnterpriseCustomerConfiguration' ) oauth_access_token , _ = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , sap_user_id , SAPSuccessFactorsEnterpriseCustomerConfiguration . USER_TYPE_USER ) response = requests . post ( url , data = payload , headers = { 'Authorization' : 'Bearer {}' . format ( oauth_access_token ) , 'content-type' : 'application/json' } ) return response . status_code , response . text
4964	def clean_program ( self ) : program_id = self . cleaned_data [ self . Fields . PROGRAM ] . strip ( ) if not program_id : return None try : client = CourseCatalogApiClient ( self . _user , self . _enterprise_customer . site ) program = client . get_program_by_uuid ( program_id ) or client . get_program_by_title ( program_id ) except MultipleProgramMatchError as exc : raise ValidationError ( ValidationMessages . MULTIPLE_PROGRAM_MATCH . format ( program_count = exc . programs_matched ) ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_PROGRAM_ID . format ( program_id = program_id ) ) if not program : raise ValidationError ( ValidationMessages . INVALID_PROGRAM_ID . format ( program_id = program_id ) ) if program [ 'status' ] != ProgramStatuses . ACTIVE : raise ValidationError ( ValidationMessages . PROGRAM_IS_INACTIVE . format ( program_id = program_id , status = program [ 'status' ] ) ) return program
6140	def wait_for_simulation_stop ( self , timeout = None ) : start = datetime . now ( ) while self . get_is_sim_running ( ) : sleep ( 0.5 ) if timeout is not None : if ( datetime . now ( ) - start ) . seconds >= timeout : ret = None break else : ret = self . simulation_info ( ) return ret
1814	def SETNG ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , 1 , 0 ) )
1238	def put ( self , item , priority = None ) : if not self . _isfull ( ) : self . _memory . append ( None ) position = self . _next_position_then_increment ( ) old_priority = 0 if self . _memory [ position ] is None else ( self . _memory [ position ] . priority or 0 ) row = _SumRow ( item , priority ) self . _memory [ position ] = row self . _update_internal_nodes ( position , ( row . priority or 0 ) - old_priority )
542	def __getOptimizedMetricLabel ( self ) : matchingKeys = matchPatterns ( [ self . _optimizeKeyPattern ] , self . _getMetricLabels ( ) ) if len ( matchingKeys ) == 0 : raise Exception ( "None of the generated metrics match the specified " "optimization pattern: %s. Available metrics are %s" % ( self . _optimizeKeyPattern , self . _getMetricLabels ( ) ) ) elif len ( matchingKeys ) > 1 : raise Exception ( "The specified optimization pattern '%s' matches more " "than one metric: %s" % ( self . _optimizeKeyPattern , matchingKeys ) ) return matchingKeys [ 0 ]
9295	def disconnect ( self ) : for name , connection in self . items ( ) : if not connection . is_closed ( ) : connection . close ( )
8960	def clean ( _dummy_ctx , docs = False , backups = False , bytecode = False , dist = False , # pylint: disable=redefined-outer-name all = False , venv = False , tox = False , extra = '' ) : # pylint: disable=redefined-builtin cfg = config . load ( ) notify . banner ( "Cleaning up project files" ) # Add patterns based on given parameters venv_dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/_build/' , 'doc/_build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/__pycache__/' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project_root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv_dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) # Build fileset patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : # Do not scan venv dirs when not cleaning them patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv_dirs ] ) fileset = antglob . FileSet ( cfg . project_root , patterns ) # Iterate over matches and remove them for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project_root , name ) ) else : os . unlink ( os . path . join ( cfg . project_root , name ) )
8500	def _map_arg ( arg ) : # Grab the easy to parse values if isinstance ( arg , _ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , _ast . Num ) : return arg . n elif isinstance ( arg , _ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : # Everything else we don't bother with return Unparseable ( )
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
12971	def getMultiple ( self , pks , cascadeFetch = False ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : # Optimization to not pipeline on 1 id return IRQueryableList ( [ self . get ( pks [ 0 ] , cascadeFetch = cascadeFetch ) ] , mdl = self . mdl ) conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) for pk in pks : key = self . _get_key_for_id ( pk ) pipeline . hgetall ( key ) res = pipeline . execute ( ) ret = IRQueryableList ( mdl = self . mdl ) i = 0 pksLen = len ( pks ) while i < pksLen : if res [ i ] is None : ret . append ( None ) i += 1 continue res [ i ] [ '_id' ] = pks [ i ] obj = self . _redisResultToObj ( res [ i ] ) ret . append ( obj ) i += 1 if cascadeFetch is True : for obj in ret : if not obj : continue self . _doCascadeFetch ( obj ) return ret
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
4447	def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , language = None , * * fields ) : return self . _add_document ( doc_id , conn = None , nosave = nosave , score = score , payload = payload , replace = replace , partial = partial , language = language , * * fields )
2555	def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : # Convert to string so we fall into next if block obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom_tag ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( obj , '__iter__' ) : for subobj in obj : self . add ( subobj ) else : # wtf is it? raise ValueError ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
12549	def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , * * kwargs ) : # normal = direction / np.sqrt(np.dot(direction, direction)) normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , * * kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
13705	def iter_char_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\n' ) ) escapecodes = get_codes ( text ) if not escapecodes : # No escape codes, use simple method. yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : # Ignore escape codes when counting. blockwidth = 0 block = [ ] for i , s in enumerate ( get_indices_list ( text ) ) : block . append ( s ) if len ( s ) == 1 : # Normal char. blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )
10368	def find_activations ( graph : BELGraph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge_to_bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue # this was inferred, so need to investigate another way elif has_protein_modification_increases_activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find_related ( graph , v , data ) elif has_degradation_increases_activity ( data ) : print ( line , '- degradation changes -' , bel ) find_related ( graph , v , data ) elif has_translocation_increases_activity ( data ) : print ( line , '- translocation changes -' , bel ) find_related ( graph , v , data ) elif complex_increases_activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find_related ( graph , v , data ) elif has_same_subject_object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )
10526	def get_google_playlist_songs ( self , playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : logger . info ( "Loading Google Music playlist songs..." ) google_playlist = self . get_google_playlist ( playlist ) if not google_playlist : return [ ] , [ ] playlist_song_ids = [ track [ 'trackId' ] for track in google_playlist [ 'tracks' ] ] playlist_songs = [ song for song in self . api . get_all_songs ( ) if song [ 'id' ] in playlist_song_ids ] matched_songs , filtered_songs = filter_google_songs ( playlist_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Filtered {0} Google playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} Google playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs
9319	def _validate_status ( self ) : if not self . id : msg = "No 'id' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . status : msg = "No 'status' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . total_count is None : msg = "No 'total_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . success_count is None : msg = "No 'success_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . failure_count is None : msg = "No 'failure_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . pending_count is None : msg = "No 'pending_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if len ( self . successes ) != self . success_count : msg = "Found successes={}, but success_count={} in status '{}'" raise ValidationError ( msg . format ( self . successes , self . success_count , self . id ) ) if len ( self . pendings ) != self . pending_count : msg = "Found pendings={}, but pending_count={} in status '{}'" raise ValidationError ( msg . format ( self . pendings , self . pending_count , self . id ) ) if len ( self . failures ) != self . failure_count : msg = "Found failures={}, but failure_count={} in status '{}'" raise ValidationError ( msg . format ( self . failures , self . failure_count , self . id ) ) if ( self . success_count + self . pending_count + self . failure_count != self . total_count ) : msg = ( "(success_count={} + pending_count={} + " "failure_count={}) != total_count={} in status '{}'" ) raise ValidationError ( msg . format ( self . success_count , self . pending_count , self . failure_count , self . total_count , self . id ) )
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : # As a speed optimization for now (until we need online learning), skip # computing the inference output while learning if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) # Get the list of columns that have bottom-up activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns # Update segment duty cycles if we are crossing a "tier" # We determine if it's time to update the segment duty cycles. Since the # duty cycle calculation is a moving average based on a tiered alpha, it is # important that we update all segments on each tier boundary if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) # Update the average input density if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) # First, update the inference state # As a speed optimization for now (until we need online learning), skip # computing the inference output while learning if enableInference : self . _updateInferenceState ( activeColumns ) # Next, update the learning state if enableLearn : self . _updateLearningState ( activeColumns ) # Apply global decay, and remove synapses and/or segments. # Synapses are removed if their permanence value is <= 0. # Segments are removed when they don't have synapses anymore. # Removal of synapses can trigger removal of whole segments! # todo: isolate the synapse/segment retraction logic so that # it can be called in adaptSegments, in the case where we # do global decay only episodically. if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] # collect and remove outside the loop for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] # collect and remove outside the loop for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay # decrease permanence if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) # add to list to delete # 1 for sequenceSegment flag if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) # will remove the whole segment elif len ( synsToDel ) > 0 : for syn in synsToDel : # remove some synapses on segment segment . syns . remove ( syn ) for seg in segsToDel : # remove some segments of this cell self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) # Update the prediction score stats # Learning always includes inference if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) # Finally return the TM output output = self . _computeOutput ( ) # Print diagnostic information based on the current verbosity level self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
1661	def ExpectingFunctionArgs ( clean_lines , linenum ) : line = clean_lines . elided [ linenum ] return ( Match ( r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(' , line ) or ( linenum >= 2 and ( Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\((?:\S+,)?\s*$' , clean_lines . elided [ linenum - 1 ] ) or Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\(\s*$' , clean_lines . elided [ linenum - 2 ] ) or Search ( r'\bstd::m?function\s*\<\s*$' , clean_lines . elided [ linenum - 1 ] ) ) ) )
12818	def _build_chunk_headers ( self ) : if hasattr ( self , "_chunk_headers" ) and self . _chunk_headers : return self . _chunk_headers = { } for field in self . _files : self . _chunk_headers [ field ] = self . _headers ( field , True ) for field in self . _data : self . _chunk_headers [ field ] = self . _headers ( field )
4949	def _transform_item ( self , content_metadata_item ) : content_metadata_type = content_metadata_item [ 'content_type' ] transformed_item = { } for integrated_channel_schema_key , edx_data_schema_key in self . DATA_TRANSFORM_MAPPING . items ( ) : # Look for transformer functions defined on subclasses. # Favor content type-specific functions. transformer = ( getattr ( self , 'transform_{content_type}_{edx_data_schema_key}' . format ( content_type = content_metadata_type , edx_data_schema_key = edx_data_schema_key ) , None ) or getattr ( self , 'transform_{edx_data_schema_key}' . format ( edx_data_schema_key = edx_data_schema_key ) , None ) ) if transformer : transformed_item [ integrated_channel_schema_key ] = transformer ( content_metadata_item ) else : # The concrete subclass does not define an override for the given field, # so just use the data key to index the content metadata item dictionary. try : transformed_item [ integrated_channel_schema_key ] = content_metadata_item [ edx_data_schema_key ] except KeyError : # There may be a problem with the DATA_TRANSFORM_MAPPING on # the concrete subclass or the concrete subclass does not implement # the appropriate field tranformer function. LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx_data_schema_key , self . enterprise_customer . name , content_metadata_item , ) return transformed_item
5322	def get_humidity ( self , sensors = None ) : _sensors = sensors if _sensors is None : _sensors = list ( range ( 0 , self . _sensor_count ) ) if not set ( _sensors ) . issubset ( list ( range ( 0 , self . _sensor_count ) ) ) : raise ValueError ( 'Some or all of the sensors in the list %s are out of range ' 'given a sensor_count of %d. Valid range: %s' % ( _sensors , self . _sensor_count , list ( range ( 0 , self . _sensor_count ) ) , ) ) data = self . get_data ( ) data = data [ 'humidity_data' ] results = { } # Interpret device response for sensor in _sensors : offset = self . lookup_humidity_offset ( sensor ) if offset is None : continue humidity = ( struct . unpack_from ( '>H' , data , offset ) [ 0 ] * 32 ) / 1000.0 results [ sensor ] = { 'ports' : self . get_ports ( ) , 'bus' : self . get_bus ( ) , 'sensor' : sensor , 'humidity_pc' : humidity , } return results
10262	def _collapse_edge_passing_predicates ( graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : for u , v , _ in filter_edges ( graph , edge_predicates = edge_predicates ) : collapse_pair ( graph , survivor = u , victim = v )
9735	def get_3d_markers ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPosition , component_info , data , component_position )
9173	def _formatter_callback_factory ( ) : # pragma: no cover includes = [ ] exercise_url_template = '{baseUrl}/api/exercises?q={field}:"{{itemCode}}"' settings = get_current_registry ( ) . settings exercise_base_url = settings . get ( 'embeddables.exercise.base_url' , None ) exercise_matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise_token = settings . get ( 'embeddables.exercise.token' , None ) mathml_url = settings . get ( 'mathmlcloud.url' , None ) memcache_servers = settings . get ( 'memcache_servers' ) if memcache_servers : memcache_servers = memcache_servers . split ( ) else : memcache_servers = None if exercise_base_url and exercise_matches : mc_client = None if memcache_servers : mc_client = memcache . Client ( memcache_servers , debug = 0 ) for ( exercise_match , exercise_field ) in exercise_matches : template = exercise_url_template . format ( baseUrl = exercise_base_url , field = exercise_field ) includes . append ( exercise_callback_factory ( exercise_match , template , mc_client , exercise_token , mathml_url ) ) return includes
4176	def window_gaussian ( N , alpha = 2.5 ) : t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) #t = linspace(-(N)/2., (N)/2., N) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
9938	def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app_location = self . storages [ app ] . location if app_location not in searched_locations : searched_locations . append ( app_location ) match = self . find_in_app ( app , path ) if match : if not all : return match matches . append ( match ) return matches
389	def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break # <-- exit the for loop, prepcess next sequence return mask
12329	def compute_sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
11945	def _store ( self , messages , response , * args , * * kwargs ) : contrib_messages = [ ] if self . user . is_authenticated ( ) : if not messages : # erase inbox self . backend . inbox_purge ( self . user ) else : for m in messages : try : self . backend . inbox_store ( [ self . user ] , m ) except MessageTypeNotSupported : contrib_messages . append ( m ) super ( StorageMixin , self ) . _store ( contrib_messages , response , * args , * * kwargs )
10877	def calculate_linescan_psf ( x , y , z , normalize = False , kfki = 0.889 , zint = 100. , polar_angle = 0. , wrap = True , * * kwargs ) : #0. Set up vecs if wrap : xpts = vec_to_halfvec ( x ) ypts = vec_to_halfvec ( y ) x3 , y3 , z3 = np . meshgrid ( xpts , ypts , z , indexing = 'ij' ) else : x3 , y3 , z3 = np . meshgrid ( x , y , z , indexing = 'ij' ) rho3 = np . sqrt ( x3 * x3 + y3 * y3 ) #1. Hilm if wrap : y2 , z2 = np . meshgrid ( ypts , z , indexing = 'ij' ) hilm0 = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , polar_angle = polar_angle , * * kwargs ) if ypts [ 0 ] == 0 : hilm = np . append ( hilm0 [ - 1 : 0 : - 1 ] , hilm0 , axis = 0 ) else : hilm = np . append ( hilm0 [ : : - 1 ] , hilm0 , axis = 0 ) else : y2 , z2 = np . meshgrid ( y , z , indexing = 'ij' ) hilm = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , polar_angle = polar_angle , * * kwargs ) #2. Hdet if wrap : #Lambda function that ignores its args but still returns correct values func = lambda * args : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) [ 0 ] hdet = wrap_and_calc_psf ( xpts , ypts , z , func ) else : hdet , toss = get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) if normalize : hilm /= hilm . sum ( ) hdet /= hdet . sum ( ) for a in range ( x . size ) : hdet [ a ] *= hilm return hdet if normalize else hdet / hdet . sum ( )
1596	def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
11570	def set_brightness ( self , brightness ) : if brightness > 15 : brightness = 15 brightness |= 0xE0 self . brightness = brightness self . firmata . i2c_write ( 0x70 , brightness )
9088	async def _update_loop ( self ) -> None : await asyncio . sleep ( self . _update_interval ) while not self . _closed : await self . update ( ) await asyncio . sleep ( self . _update_interval )
7972	def _remove_timeout_handler ( self , handler ) : if handler not in self . timeout_handlers : return self . timeout_handlers . remove ( handler ) for thread in self . timeout_threads : if thread . method . im_self is handler : thread . stop ( )
7659	def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , duration = d , value = v , confidence = c ) for ( t , d , v , c ) in six . moves . zip ( columns [ 'time' ] , columns [ 'duration' ] , columns [ 'value' ] , columns [ 'confidence' ] ) ] )
13294	def convert_text ( content , from_fmt , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : logger = logging . getLogger ( __name__ ) if extra_args is not None : extra_args = list ( extra_args ) else : extra_args = [ ] if mathjax : extra_args . append ( '--mathjax' ) if smart : extra_args . append ( '--smart' ) if deparagraph : extra_args . append ( '--filter=lsstprojectmeta-deparagraph' ) extra_args . append ( '--wrap=none' ) # de-dupe extra args extra_args = set ( extra_args ) logger . debug ( 'Running pandoc from %s to %s with extra_args %s' , from_fmt , to_fmt , extra_args ) output = pypandoc . convert_text ( content , to_fmt , format = from_fmt , extra_args = extra_args ) return output
117	def imap_batches ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) # TODO change this to 'yield from' once switched to 3.3+ gen = self . pool . imap ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
7344	def get_data ( self , response ) : if self . _response_list : return response elif self . _response_key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , "__getitem__" ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . _response_key = key return data else : self . _response_list = True return response else : return response [ self . _response_key ] raise NoDataFound ( response = response , url = self . request . get_url ( ) )
9175	def db_connect ( connection_string = None , * * kwargs ) : if connection_string is None : connection_string = get_current_registry ( ) . settings [ CONNECTION_STRING ] db_conn = psycopg2 . connect ( connection_string , * * kwargs ) try : with db_conn : yield db_conn finally : db_conn . close ( )
1119	def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
8636	def get_milestone_by_id ( session , milestone_id , user_details = None ) : # GET /api/projects/0.1/milestones/{milestone_id}/ endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11543	def set_analog_reference ( self , reference , pin = None ) : if pin is None : self . _set_analog_reference ( reference , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_analog_reference ( reference , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
3358	def index ( self , id , * args ) : # because values are unique, start and stop are not relevant if isinstance ( id , string_types ) : try : return self . _dict [ id ] except KeyError : raise ValueError ( "%s not found" % id ) try : i = self . _dict [ id . id ] if self [ i ] is not id : raise ValueError ( "Another object with the identical id (%s) found" % id . id ) return i except KeyError : raise ValueError ( "%s not found" % str ( id ) )
311	def sortino_ratio ( returns , required_return = 0 , period = DAILY ) : return ep . sortino_ratio ( returns , required_return = required_return )
5902	def prehook ( self , * * kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
11929	def watch_files ( self ) : try : while 1 : sleep ( 1 ) # check every 1s try : files_stat = self . get_files_stat ( ) except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) if self . files_stat != files_stat : logger . info ( "Changes detected, start rebuilding.." ) try : generator . re_generate ( ) global _root _root = generator . root except SystemExit : # catch sys.exit, it means fatal error logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) self . files_stat = files_stat # update files' stat except KeyboardInterrupt : # I dont know why, but this exception won't be catched # because absolutly each KeyboardInterrupt is catched by # the server thread, which will terminate this thread the same time logger . info ( "^C received, shutting down watcher" ) self . shutdown_watcher ( )
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) # choose the epoch epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) # choose the period, fourierorder, and amplitude period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) # fix the amplitude if it needs to be flipped if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude # generate the amplitudes and phases of the Fourier components ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] # now that we have our amp and pha components, generate the light curve modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) # resort in original time order timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] # return a dict with everything modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , # these are standard keys that help with later characterization of # variability as a function period, variability amplitude, object mag, # ndet, etc. 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
6806	def init_raspbian_vm ( self ) : r = self . local_renderer r . comment ( 'Installing system packages.' ) r . sudo ( 'add-apt-repository ppa:linaro-maintainers/tools' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install libsdl-dev qemu-system' ) r . comment ( 'Download image.' ) r . local ( 'wget https://downloads.raspberrypi.org/raspbian_lite_latest' ) r . local ( 'unzip raspbian_lite_latest.zip' ) #TODO:fix name? #TODO:resize image? r . comment ( 'Find start of the Linux ext4 partition.' ) r . local ( "parted -s 2016-03-18-raspbian-jessie-lite.img unit B print | " "awk '/^Number/{{p=1;next}}; p{{gsub(/[^[:digit:]]/, " ", $2); print $2}}' | sed -n 2p" , assign_to = 'START' ) r . local ( 'mkdir -p {raspbian_mount_point}' ) r . sudo ( 'mount -v -o offset=$START -t ext4 {raspbian_image} $MNT' ) r . comment ( 'Comment out everything in ld.so.preload' ) r . local ( "sed -i 's/^/#/g' {raspbian_mount_point}/etc/ld.so.preload" ) r . comment ( 'Comment out entries containing /dev/mmcblk in fstab.' ) r . local ( "sed -i '/mmcblk/ s?^?#?' /etc/fstab" ) r . sudo ( 'umount {raspbian_mount_point}' ) r . comment ( 'Download kernel.' ) r . local ( 'wget https://github.com/dhruvvyas90/qemu-rpi-kernel/blob/master/{raspbian_kernel}?raw=true' ) r . local ( 'mv {raspbian_kernel} {libvirt_images_dir}' ) r . comment ( 'Creating libvirt machine.' ) r . local ( 'virsh define libvirt-raspbian.xml' ) r . comment ( 'You should now be able to boot the VM by running:' ) r . comment ( '' ) r . comment ( ' qemu-system-arm -kernel {libvirt_boot_dir}/{raspbian_kernel} ' '-cpu arm1176 -m 256 -M versatilepb -serial stdio -append "root=/dev/sda2 rootfstype=ext4 rw" ' '-hda {libvirt_images_dir}/{raspbian_image}' ) r . comment ( '' ) r . comment ( 'Or by running virt-manager.' )
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
5491	def create_config ( cls , cfgfile , nick , twtfile , twturl , disclose_identity , add_news ) : cfgfile_dir = os . path . dirname ( cfgfile ) if not os . path . exists ( cfgfile_dir ) : os . makedirs ( cfgfile_dir ) cfg = configparser . ConfigParser ( ) cfg . add_section ( "twtxt" ) cfg . set ( "twtxt" , "nick" , nick ) cfg . set ( "twtxt" , "twtfile" , twtfile ) cfg . set ( "twtxt" , "twturl" , twturl ) cfg . set ( "twtxt" , "disclose_identity" , str ( disclose_identity ) ) cfg . set ( "twtxt" , "character_limit" , "140" ) cfg . set ( "twtxt" , "character_warning" , "140" ) cfg . add_section ( "following" ) if add_news : cfg . set ( "following" , "twtxt" , "https://buckket.org/twtxt_news.txt" ) conf = cls ( cfgfile , cfg ) conf . write_config ( ) return conf
6952	def aov_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) ndets = phases . size binnedphaseinds = npdigitize ( phases , bins ) bin_s1_tops = [ ] bin_s2_tops = [ ] binndets = [ ] goodbins = 0 all_xbar = npmedian ( pmags ) for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_ndet = thisbin_mags . size thisbin_xbar = npmedian ( thisbin_mags ) # get s1 thisbin_s1_top = ( thisbin_ndet * ( thisbin_xbar - all_xbar ) * ( thisbin_xbar - all_xbar ) ) # get s2 thisbin_s2_top = npsum ( ( thisbin_mags - all_xbar ) * ( thisbin_mags - all_xbar ) ) bin_s1_tops . append ( thisbin_s1_top ) bin_s2_tops . append ( thisbin_s2_top ) binndets . append ( thisbin_ndet ) goodbins = goodbins + 1 # turn the quantities into arrays bin_s1_tops = nparray ( bin_s1_tops ) bin_s2_tops = nparray ( bin_s2_tops ) binndets = nparray ( binndets ) # calculate s1 first s1 = npsum ( bin_s1_tops ) / ( goodbins - 1.0 ) # then calculate s2 s2 = npsum ( bin_s2_tops ) / ( ndets - goodbins ) theta_aov = s1 / s2 return theta_aov
12213	def update_field_from_proxy ( field_obj , pref_proxy ) : attr_names = ( 'verbose_name' , 'help_text' , 'default' ) for attr_name in attr_names : setattr ( field_obj , attr_name , getattr ( pref_proxy , attr_name ) )
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) # report in the original locus order cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
1867	def PMOVMSKB ( cpu , op0 , op1 ) : arg0 = op0 . read ( ) arg1 = op1 . read ( ) res = 0 for i in reversed ( range ( 7 , op1 . size , 8 ) ) : res = ( res << 1 ) | ( ( arg1 >> i ) & 1 ) op0 . write ( Operators . EXTRACT ( res , 0 , op0 . size ) )
12133	def extract_log ( log_path , dict_type = dict ) : log_path = ( log_path if os . path . isfile ( log_path ) else os . path . join ( os . getcwd ( ) , log_path ) ) with open ( log_path , 'r' ) as log : splits = ( line . split ( ) for line in log ) uzipped = ( ( int ( split [ 0 ] ) , json . loads ( " " . join ( split [ 1 : ] ) ) ) for split in splits ) szipped = [ ( i , dict ( ( str ( k ) , v ) for ( k , v ) in d . items ( ) ) ) for ( i , d ) in uzipped ] return dict_type ( szipped )
4941	def unlink_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) # not capturing DoesNotExist intentionally to signal to view that link does not exist link_record = self . get ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) link_record . delete ( ) if update_user : # Remove the SailThru flags for enterprise learner. update_user . delay ( sailthru_vars = { 'is_enterprise_learner' : False , 'enterprise_name' : None , } , email = user_email ) except User . DoesNotExist : # not capturing DoesNotExist intentionally to signal to view that link does not exist pending_link = PendingEnterpriseCustomerUser . objects . get ( enterprise_customer = enterprise_customer , user_email = user_email ) pending_link . delete ( ) LOGGER . info ( 'Enterprise learner {%s} successfully unlinked from Enterprise Customer {%s}' , user_email , enterprise_customer . name )
4084	def get_newest_possible_languagetool_version ( ) : java_path = find_executable ( 'java' ) if not java_path : # Just ignore this and assume an old version of Java. It might not be # found because of a PATHEXT-related issue # (https://bugs.python.org/issue2200). return JAVA_6_COMPATIBLE_VERSION output = subprocess . check_output ( [ java_path , '-version' ] , stderr = subprocess . STDOUT , universal_newlines = True ) java_version = parse_java_version ( output ) if java_version >= ( 1 , 8 ) : return LATEST_VERSION elif java_version >= ( 1 , 7 ) : return JAVA_7_COMPATIBLE_VERSION elif java_version >= ( 1 , 6 ) : warn ( 'language-check would be able to use a newer version of ' 'LanguageTool if you had Java 7 or newer installed' ) return JAVA_6_COMPATIBLE_VERSION else : raise SystemExit ( 'You need at least Java 6 to use language-check' )
12273	def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
2939	def deserialize_logical ( self , node ) : term1_attrib = node . getAttribute ( 'left-field' ) term1_value = node . getAttribute ( 'left-value' ) op = node . nodeName . lower ( ) term2_attrib = node . getAttribute ( 'right-field' ) term2_value = node . getAttribute ( 'right-value' ) if op not in _op_map : _exc ( 'Invalid operator' ) if term1_attrib != '' and term1_value != '' : _exc ( 'Both, left-field and left-value attributes found' ) elif term1_attrib == '' and term1_value == '' : _exc ( 'left-field or left-value attribute required' ) elif term1_value != '' : left = term1_value else : left = operators . Attrib ( term1_attrib ) if term2_attrib != '' and term2_value != '' : _exc ( 'Both, right-field and right-value attributes found' ) elif term2_attrib == '' and term2_value == '' : _exc ( 'right-field or right-value attribute required' ) elif term2_value != '' : right = term2_value else : right = operators . Attrib ( term2_attrib ) return _op_map [ op ] ( left , right )
10947	def reset ( self ) : inds = list ( range ( self . state . obj_get_positions ( ) . shape [ 0 ] ) ) self . _rad_nms = self . state . param_particle_rad ( inds ) self . _pos_nms = self . state . param_particle_pos ( inds ) self . _initial_rad = np . copy ( self . state . state [ self . _rad_nms ] ) self . _initial_pos = np . copy ( self . state . state [ self . _pos_nms ] ) . reshape ( ( - 1 , 3 ) ) self . param_vals [ self . rscale_mask ] = 0
649	def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , "can't generate non-overlapping coincidences" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix
13097	def terminate_processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )
8158	def sql ( self , sql ) : self . _cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . _cur : matches . append ( r ) return matches
1481	def start_process_monitor ( self ) : # Now wait for any child to die Log . info ( "Start process monitor" ) while True : if len ( self . processes_to_monitor ) > 0 : ( pid , status ) = os . wait ( ) with self . process_lock : if pid in self . processes_to_monitor . keys ( ) : old_process_info = self . processes_to_monitor [ pid ] name = old_process_info . name command = old_process_info . command Log . info ( "%s (pid=%s) exited with status %d. command=%s" % ( name , pid , status , command ) ) # Log the stdout & stderr of the failed process self . _wait_process_std_out_err ( name , old_process_info . process ) # Just make it world readable if os . path . isfile ( "core.%d" % pid ) : os . system ( "chmod a+r core.%d" % pid ) if old_process_info . attempts >= self . max_runs : Log . info ( "%s exited too many times" % name ) sys . exit ( 1 ) time . sleep ( self . interval_between_runs ) p = self . _run_process ( name , command ) del self . processes_to_monitor [ pid ] self . processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command , old_process_info . attempts + 1 ) # Log down the pid file log_pid_for_process ( name , p . pid )
4860	def force_fresh_session ( view ) : @ wraps ( view ) def wrapper ( request , * args , * * kwargs ) : """ Wrap the function. """ if not request . GET . get ( FRESH_LOGIN_PARAMETER ) : # The enterprise_login_required decorator promises to set the fresh login URL # parameter for this URL when it was the agent that initiated the login process; # if that parameter isn't set, we can safely assume that the session is "stale"; # that isn't necessarily an issue, though. Redirect the user to # log out and then come back here - the enterprise_login_required decorator will # then take effect prior to us arriving back here again. enterprise_customer = get_enterprise_customer_or_404 ( kwargs . get ( 'enterprise_uuid' ) ) provider_id = enterprise_customer . identity_provider or '' sso_provider = get_identity_provider ( provider_id ) if sso_provider : # Parse the current request full path, quote just the path portion, # then reconstruct the full path string. # The path and query portions should be the only non-empty strings here. scheme , netloc , path , params , query , fragment = urlparse ( request . get_full_path ( ) ) redirect_url = urlunparse ( ( scheme , netloc , quote ( path ) , params , query , fragment ) ) return redirect ( '{logout_url}?{params}' . format ( logout_url = '/logout' , params = urlencode ( { 'redirect_url' : redirect_url } ) ) ) return view ( request , * args , * * kwargs ) return wrapper
8914	def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
2915	def cancel ( self ) : if self . _is_finished ( ) : for child in self . children : child . cancel ( ) return self . _set_state ( self . CANCELLED ) self . _drop_children ( ) self . task_spec . _on_cancel ( self )
9792	def is_ignored ( cls , path , patterns ) : status = None for pattern in cls . find_matching ( path , patterns ) : status = pattern . is_exclude return status
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
12797	def build_twisted_request ( self , method , url , extra_headers = { } , body_producer = None , full_url = False ) : uri = url if full_url else self . _url ( url ) raw_headers = self . get_headers ( ) if extra_headers : raw_headers . update ( extra_headers ) headers = http_headers . Headers ( ) for header in raw_headers : headers . addRawHeader ( header , raw_headers [ header ] ) agent = client . Agent ( reactor ) request = agent . request ( method , uri , headers , body_producer ) return ( reactor , request )
11628	def clear_sent_messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE_CLEAR_OFFSET' , defaults . MAILQUEUE_CLEAR_OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete_before = timezone . now ( ) - offset self . filter ( sent = True , last_attempt__lte = delete_before ) . delete ( )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
3999	def copy_to_local ( local_path , remote_name , remote_path , demote = True ) : if not container_path_exists ( remote_name , remote_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( remote_path , remote_name ) ) temp_identifier = str ( uuid . uuid1 ( ) ) copy_path_inside_container ( remote_name , remote_path , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) ) vm_path = os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) is_dir = vm_path_is_directory ( vm_path ) sync_local_path_from_vm ( local_path , vm_path , demote = demote , is_dir = is_dir )
7576	def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : ## create call string outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] ## call the shell function proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) ## cleanup oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
9198	def pop ( self , key , default = _sentinel ) : if default is not _sentinel : tup = self . _data . pop ( key . lower ( ) , default ) else : tup = self . _data . pop ( key . lower ( ) ) if tup is not default : return tup [ 1 ] else : return default
7177	def retype_file ( src , pyi_dir , targets , * , quiet = False , hg = False ) : with tokenize . open ( src ) as src_buffer : src_encoding = src_buffer . encoding src_node = lib2to3_parse ( src_buffer . read ( ) ) try : with open ( ( pyi_dir / src . name ) . with_suffix ( '.pyi' ) ) as pyi_file : pyi_txt = pyi_file . read ( ) except FileNotFoundError : if not quiet : print ( f'warning: .pyi file for source {src} not found in {pyi_dir}' , file = sys . stderr , ) else : pyi_ast = ast3 . parse ( pyi_txt ) assert isinstance ( pyi_ast , ast3 . Module ) reapply_all ( pyi_ast . body , src_node ) fix_remaining_type_comments ( src_node ) targets . mkdir ( parents = True , exist_ok = True ) with open ( targets / src . name , 'w' , encoding = src_encoding ) as target_file : target_file . write ( lib2to3_unparse ( src_node , hg = hg ) ) return targets / src . name
1743	def save_image ( tensor , filename , nrow = 8 , padding = 2 , pad_value = 0 ) : from PIL import Image grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value ) im = Image . fromarray ( pre_pillow_float_img_process ( grid ) ) im . save ( filename )
4355	def _pop_ack_callback ( self , msgid ) : if msgid not in self . ack_callbacks : return None return self . ack_callbacks . pop ( msgid )
4727	def s20_to_gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm_addr s20_to_gen" , self . envs [ "DEV_PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.s20_to_gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
4667	def refresh ( self ) : dict . __init__ ( self , self . blockchain . rpc . get_object ( self . identifier ) , blockchain_instance = self . blockchain , )
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
1002	def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : if self . lrnIterationIdx < 100 : alpha = 0.5 else : alpha = 0.1 self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + ( alpha * prevSeqLength ) )
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
1499	def ack ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in ack()" ) return if self . acking_enabled : ack_tuple = tuple_pb2 . AckTuple ( ) ack_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = ack_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns )
10867	def rmatrix ( self ) : t = self . param_dict [ self . lbl_theta ] r0 = np . array ( [ [ np . cos ( t ) , - np . sin ( t ) , 0 ] , [ np . sin ( t ) , np . cos ( t ) , 0 ] , [ 0 , 0 , 1 ] ] ) p = self . param_dict [ self . lbl_phi ] r1 = np . array ( [ [ np . cos ( p ) , 0 , np . sin ( p ) ] , [ 0 , 1 , 0 ] , [ - np . sin ( p ) , 0 , np . cos ( p ) ] ] ) return np . dot ( r1 , r0 )
264	def _stack_positions ( positions , pos_in_dollars = True ) : if pos_in_dollars : # convert holdings to percentages positions = get_percent_alloc ( positions ) # remove cash after normalizing positions positions = positions . drop ( 'cash' , axis = 'columns' ) # convert positions to long format positions = positions . stack ( ) positions . index = positions . index . set_names ( [ 'dt' , 'ticker' ] ) return positions
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
3769	def none_and_length_check ( all_inputs , length = None ) : if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
8392	def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
5750	def downloadURL ( url , filename ) : path_temp_bviewfile = os . path . join ( c . raw_data , c . bview_dir , 'tmp' , filename ) path_bviewfile = os . path . join ( c . raw_data , c . bview_dir , filename ) try : f = urlopen ( url ) except : return False if f . getcode ( ) != 200 : publisher . warning ( '{} unavailable, code: {}' . format ( url , f . getcode ( ) ) ) return False try : with open ( path_temp_bviewfile , 'w' ) as outfile : outfile . write ( f . read ( ) ) os . rename ( path_temp_bviewfile , path_bviewfile ) except : os . remove ( path_temp_bviewfile ) return False return True
7439	def _build_stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . DataFrame ( [ self . samples [ i ] . stats_dfs [ idx ] for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) return newdat
9112	def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
2483	def write_document ( document , out , validate = True ) : if validate : messages = [ ] messages = document . validate ( messages ) if messages : raise InvalidDocumentError ( messages ) writer = Writer ( document , out ) writer . write ( )
608	def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result
11990	def const_equal ( str_a , str_b ) : if len ( str_a ) != len ( str_b ) : return False result = True for i in range ( len ( str_a ) ) : result &= ( str_a [ i ] == str_b [ i ] ) return result
5208	def format_intraday ( data : pd . DataFrame , ticker , * * kwargs ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) data . columns = pd . MultiIndex . from_product ( [ [ ticker ] , data . rename ( columns = dict ( numEvents = 'num_trds' ) ) . columns ] , names = [ 'ticker' , 'field' ] ) data . index . name = None if kwargs . get ( 'price_only' , False ) : kw_xs = dict ( axis = 1 , level = 1 ) close = data . xs ( 'close' , * * kw_xs ) volume = data . xs ( 'volume' , * * kw_xs ) . iloc [ : , 0 ] return close . loc [ volume > 0 ] if volume . min ( ) > 0 else close else : return data
802	def modelsGetParams ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % ( type ( modelIDs ) , ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getParamsNamedTuple . _fields ] ) # NOTE: assertion will also fail when modelIDs contains duplicates assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) # Return the params and params hashes as a namedtuple return [ self . _models . getParamsNamedTuple . _make ( r ) for r in rows ]
995	def reset ( self , ) : if self . verbosity >= 3 : print "\n==== RESET =====" self . lrnActiveState [ 't-1' ] . fill ( 0 ) self . lrnActiveState [ 't' ] . fill ( 0 ) self . lrnPredictedState [ 't-1' ] . fill ( 0 ) self . lrnPredictedState [ 't' ] . fill ( 0 ) self . infActiveState [ 't-1' ] . fill ( 0 ) self . infActiveState [ 't' ] . fill ( 0 ) self . infPredictedState [ 't-1' ] . fill ( 0 ) self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't-1' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) # Flush the segment update queue self . segmentUpdates = { } self . _internalStats [ 'nInfersSinceReset' ] = 0 #To be removed self . _internalStats [ 'curPredictionScore' ] = 0 #New prediction score self . _internalStats [ 'curPredictionScore2' ] = 0 self . _internalStats [ 'curFalseNegativeScore' ] = 0 self . _internalStats [ 'curFalsePositiveScore' ] = 0 self . _internalStats [ 'curMissing' ] = 0 self . _internalStats [ 'curExtra' ] = 0 # When a reset occurs, set prevSequenceSignature to the signature of the # just-completed sequence and start accumulating histogram for the next # sequence. self . _internalStats [ 'prevSequenceSignature' ] = None if self . collectSequenceStats : if self . _internalStats [ 'confHistogram' ] . sum ( ) > 0 : sig = self . _internalStats [ 'confHistogram' ] . copy ( ) sig . reshape ( self . numberOfCols * self . cellsPerColumn ) self . _internalStats [ 'prevSequenceSignature' ] = sig self . _internalStats [ 'confHistogram' ] . fill ( 0 ) self . resetCalled = True # Clear out input history self . _prevInfPatterns = [ ] self . _prevLrnPatterns = [ ]
8596	def update_group ( self , group_id , * * kwargs ) : properties = { } # make the key camel-case transformable if 'create_datacenter' in kwargs : kwargs [ 'create_data_center' ] = kwargs . pop ( 'create_datacenter' ) for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'PUT' , data = json . dumps ( data ) ) return response
9861	def sync_update_info ( self , * _ ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_info ( ) ) loop . run_until_complete ( task )
12786	def _effective_filename ( self ) : # type: () -> str # same logic for the configuration filename. First, check if we were # initialized with a filename... config_filename = '' if self . filename : config_filename = self . filename # ... next, take the value from the environment env_filename = getenv ( self . env_filename_name ) if env_filename : self . _log . info ( 'Configuration filename was overridden with %r ' 'by the environment variable %s.' , env_filename , self . env_filename_name ) config_filename = env_filename return config_filename
2779	def get_data ( self , url , headers = dict ( ) , params = dict ( ) , render_json = True ) : url = urljoin ( self . end_point , url ) response = requests . get ( url , headers = headers , params = params , timeout = self . get_timeout ( ) ) if render_json : return response . json ( ) return response . content
2796	def load ( self , use_slug = False ) : identifier = None if use_slug or not self . id : identifier = self . slug else : identifier = self . id if not identifier : raise NotFoundError ( "One of self.id or self.slug must be set." ) data = self . get_data ( "images/%s" % identifier ) image_dict = data [ 'image' ] # Setting the attribute values for attr in image_dict . keys ( ) : setattr ( self , attr , image_dict [ attr ] ) return self
3109	def locked_put ( self , credentials ) : entity , _ = self . model_class . objects . get_or_create ( * * { self . key_name : self . key_value } ) setattr ( entity , self . property_name , credentials ) entity . save ( )
9321	def _validate_api_root ( self ) : if not self . _title : msg = "No 'title' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _versions : msg = "No 'versions' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _max_content_length is None : msg = "No 'max_content_length' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) )
13441	def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) #Copy catalog from local to cloud, which becomes the new "base" changeset util . copy ( lcat , ccat ) # Write meta-data both to local and cloud mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) #Let's copy Smart Previews if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) #Finally,let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
1175	def unlock ( self ) : if self . queue : function , argument = self . queue . popleft ( ) function ( argument ) else : self . locked = False
11761	def refresh ( self ) : # `values_list('name', 'value')` doesn't work because `value` is not a # setting (base class) field, it's a setting value (subclass) field. So # we have to get real instances. args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( SettingDict , self ) . update ( args ) self . empty_cache = False
13697	def try_read_file ( s ) : try : with open ( s , 'r' ) as f : data = f . read ( ) except FileNotFoundError : # Not a file name. return s except EnvironmentError as ex : print_err ( '\nFailed to read file: {}\n {}' . format ( s , ex ) ) return None return data
3497	def total_components_flux ( flux , components , consumption = True ) : direction = 1 if consumption else - 1 c_flux = [ elem * flux * direction for elem in components ] return sum ( [ flux for flux in c_flux if flux > 0 ] )
3974	def _get_compose_volumes ( app_name , assembled_specs ) : volumes = [ ] volumes . append ( _get_cp_volume_mount ( app_name ) ) volumes += get_app_volume_mounts ( app_name , assembled_specs ) return volumes
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
2854	def mpsse_gpio ( self ) : level_low = chr ( self . _level & 0xFF ) level_high = chr ( ( self . _level >> 8 ) & 0xFF ) dir_low = chr ( self . _direction & 0xFF ) dir_high = chr ( ( self . _direction >> 8 ) & 0xFF ) return str ( bytearray ( ( 0x80 , level_low , dir_low , 0x82 , level_high , dir_high ) ) )
6972	def _epd_residual2 ( coeffs , times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual
2705	def collect_phrases ( sent , ranks , spacy_nlp ) : tail = 0 last_idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word_id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last_idx ) == 1 ) : # keep collecting... rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word_id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : # just hit a phrase boundary for text , p in enumerate_chunks ( phrase , spacy_nlp ) : if p : id_list = [ rl . ids for rl in p ] rank_list = [ rl . rank for rl in p ] np_rl = RankedLexeme ( text = text , rank = rank_list , ids = id_list , pos = "np" , count = 1 ) if DEBUG : print ( np_rl ) yield np_rl phrase = [ ] last_idx = w . idx tail += 1
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] # sort them by size len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False # get the basename and the rest of the files fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] # check if the basename is in the basename of the rest of the files for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
13892	def _AssertIsLocal ( path ) : from six . moves . urllib . parse import urlparse if not _UrlIsLocal ( urlparse ( path ) ) : from . _exceptions import NotImplementedForRemotePathError raise NotImplementedForRemotePathError
4104	def CORRELATION ( x , y = None , maxlags = None , norm = 'unbiased' ) : assert norm in [ 'unbiased' , 'biased' , 'coeff' , None ] #transform lag into list if it is an integer x = np . array ( x ) if y is None : y = x else : y = np . array ( y ) # N is the max of x and y N = max ( len ( x ) , len ( y ) ) if len ( x ) < N : x = y . copy ( ) x . resize ( N ) if len ( y ) < N : y = y . copy ( ) y . resize ( N ) #default lag is N-1 if maxlags is None : maxlags = N - 1 assert maxlags < N , 'lag must be less than len(x)' realdata = np . isrealobj ( x ) and np . isrealobj ( y ) #create an autocorrelation array with same length as lag if realdata == True : r = np . zeros ( maxlags , dtype = float ) else : r = np . zeros ( maxlags , dtype = complex ) if norm == 'coeff' : rmsx = pylab_rms_flat ( x ) rmsy = pylab_rms_flat ( y ) for k in range ( 0 , maxlags + 1 ) : nk = N - k - 1 if realdata == True : sum = 0 for j in range ( 0 , nk + 1 ) : sum = sum + x [ j + k ] * y [ j ] else : sum = 0. + 0j for j in range ( 0 , nk + 1 ) : sum = sum + x [ j + k ] * y [ j ] . conjugate ( ) if k == 0 : if norm in [ 'biased' , 'unbiased' ] : r0 = sum / float ( N ) elif norm is None : r0 = sum else : r0 = 1. else : if norm == 'unbiased' : r [ k - 1 ] = sum / float ( N - k ) elif norm == 'biased' : r [ k - 1 ] = sum / float ( N ) elif norm is None : r [ k - 1 ] = sum elif norm == 'coeff' : r [ k - 1 ] = sum / ( rmsx * rmsy ) / float ( N ) r = np . insert ( r , 0 , r0 ) return r
11039	def read ( self , path , * * params ) : d = self . request ( 'GET' , '/v1/' + path , params = params ) return d . addCallback ( self . _handle_response )
4970	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerAdminForm , self ) . clean ( ) if 'catalog' in cleaned_data and not cleaned_data [ 'catalog' ] : cleaned_data [ 'catalog' ] = None return cleaned_data
8020	async def websocket_accept ( self , message , stream_name ) : is_first = not self . applications_accepting_frames self . applications_accepting_frames . add ( stream_name ) # accept the connection after the first upstream application accepts. if is_first : await self . accept ( )
7237	def randwindow ( self , window_shape ) : row = random . randrange ( window_shape [ 0 ] , self . shape [ 1 ] ) col = random . randrange ( window_shape [ 1 ] , self . shape [ 2 ] ) return self [ : , row - window_shape [ 0 ] : row , col - window_shape [ 1 ] : col ]
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
13042	def create_query ( section ) : query = { } if 'ports' in section : query [ 'ports' ] = [ section [ 'ports' ] ] if 'up' in section : query [ 'up' ] = bool ( section [ 'up' ] ) if 'search' in section : query [ 'search' ] = [ section [ 'search' ] ] if 'tags' in section : query [ 'tags' ] = [ section [ 'tags' ] ] if 'groups' in section : query [ 'groups' ] = [ section [ 'groups' ] ] return query
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
3693	def Tb ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ PSAT_DEFINITION ] ) : def list_methods ( ) : methods = [ ] if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tb' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tb' ] ) : methods . append ( CRC_ORG ) if CASRN in Yaws_data . index : methods . append ( YAWS ) if PSAT_DEFINITION not in IgnoreMethods : try : # For some chemicals, vapor pressure range will exclude Tb VaporPressure ( CASRN = CASRN ) . solve_prop ( 101325. ) methods . append ( PSAT_DEFINITION ) except : # pragma: no cover pass if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tb' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tb' ] ) elif Method == YAWS : return float ( Yaws_data . at [ CASRN , 'Tb' ] ) elif Method == PSAT_DEFINITION : return VaporPressure ( CASRN = CASRN ) . solve_prop ( 101325. ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
1322	def MoveToCenter ( self ) -> bool : if self . IsTopLevel ( ) : rect = self . BoundingRectangle screenWidth , screenHeight = GetScreenSize ( ) x , y = ( screenWidth - rect . width ( ) ) // 2 , ( screenHeight - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return SetWindowPos ( self . NativeWindowHandle , SWP . HWND_Top , x , y , 0 , 0 , SWP . SWP_NoSize ) return False
9509	def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
11936	def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : # Prevent taking the root of a negative number delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
6732	def add_class_methods_as_module_level_functions_for_fabric ( instance , module_name , method_name , module_alias = None ) : import imp from . decorators import task_or_dryrun # get the module as an object module_obj = sys . modules [ module_name ] module_alias = re . sub ( '[^a-zA-Z0-9]+' , '' , module_alias or '' ) # Iterate over the methods of the class and dynamically create a function # for each method that calls the method and add it to the current module # NOTE: inspect.ismethod actually executes the methods?! #for method in inspect.getmembers(instance, predicate=inspect.ismethod): method_obj = getattr ( instance , method_name ) if not method_name . startswith ( '_' ) : # get the bound method func = getattr ( instance , method_name ) # if module_name == 'buildbot' or module_alias == 'buildbot': # print('-'*80) # print('module_name:', module_name) # print('method_name:', method_name) # print('module_alias:', module_alias) # print('module_obj:', module_obj) # print('func.module:', func.__module__) # Convert executable to a Fabric task, if not done so already. if not hasattr ( func , 'is_task_or_dryrun' ) : func = task_or_dryrun ( func ) if module_name == module_alias or ( module_name . startswith ( 'satchels.' ) and module_name . endswith ( module_alias ) ) : # add the function to the current module setattr ( module_obj , method_name , func ) else : # Dynamically create a module for the virtual satchel. _module_obj = module_obj module_obj = create_module ( module_alias ) setattr ( module_obj , method_name , func ) post_import_modules . add ( module_alias ) fabric_name = '%s.%s' % ( module_alias or module_name , method_name ) func . wrapped . __func__ . fabric_name = fabric_name return func
4129	def _autocov ( s , * * kwargs ) : # only remove the mean once, if needed debias = kwargs . pop ( 'debias' , True ) axis = kwargs . get ( 'axis' , - 1 ) if debias : s = _remove_bias ( s , axis ) kwargs [ 'debias' ] = False return _crosscov ( s , s , * * kwargs )
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) # [{'LocalDateTime': '20160824161431.977000+480'}]' self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) # '20160824161431' self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type: # datetime.datetime return self . current_time_format
4340	def remix ( self , remix_dictionary = None , num_output_channels = None ) : if not ( isinstance ( remix_dictionary , dict ) or remix_dictionary is None ) : raise ValueError ( "remix_dictionary must be a dictionary or None." ) if remix_dictionary is not None : if not all ( [ isinstance ( i , int ) and i > 0 for i in remix_dictionary . keys ( ) ] ) : raise ValueError ( "remix dictionary must have positive integer keys." ) if not all ( [ isinstance ( v , list ) for v in remix_dictionary . values ( ) ] ) : raise ValueError ( "remix dictionary values must be lists." ) for v_list in remix_dictionary . values ( ) : if not all ( [ isinstance ( v , int ) and v > 0 for v in v_list ] ) : raise ValueError ( "elements of remix dictionary values must " "be positive integers" ) if not ( ( isinstance ( num_output_channels , int ) and num_output_channels > 0 ) or num_output_channels is None ) : raise ValueError ( "num_output_channels must be a positive integer or None." ) effect_args = [ 'remix' ] if remix_dictionary is None : effect_args . append ( '-' ) else : if num_output_channels is None : num_output_channels = max ( remix_dictionary . keys ( ) ) for channel in range ( 1 , num_output_channels + 1 ) : if channel in remix_dictionary . keys ( ) : out_channel = ',' . join ( [ str ( i ) for i in remix_dictionary [ channel ] ] ) else : out_channel = '0' effect_args . append ( out_channel ) self . effects . extend ( effect_args ) self . effects_log . append ( 'remix' ) return self
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) # flip y axis for mags if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) # set the x axis limit axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) # make a grid axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) # make the x and y axis labels plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) # fix the yaxis ticks (turns off offset and uses the full # value of the yaxis tick) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
5830	def create ( self , configuration , name , description ) : data = { "configuration" : configuration , "name" : name , "description" : description } failure_message = "Dataview creation failed" result = self . _get_success_json ( self . _post_json ( 'v1/data_views' , data , failure_message = failure_message ) ) data_view_id = result [ 'data' ] [ 'id' ] return data_view_id
12232	def register_prefs ( * args , * * kwargs ) : swap_settings_module = bool ( kwargs . get ( 'swap_settings_module' , True ) ) if __PATCHED_LOCALS_SENTINEL not in get_frame_locals ( 2 ) : raise SitePrefsException ( 'Please call `patch_locals()` right before the `register_prefs()`.' ) bind_proxy ( args , * * kwargs ) unpatch_locals ( ) swap_settings_module and proxy_settings_module ( )
4080	def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
5145	def _merge_config ( self , config , templates ) : if not templates : return config # type check if not isinstance ( templates , list ) : raise TypeError ( 'templates argument must be an instance of list' ) # merge templates with main configuration result = { } config_list = templates + [ config ] for merging in config_list : result = merge_config ( result , self . _load ( merging ) , self . list_identifiers ) return result
11265	def readline ( prev , filename = None , mode = 'r' , trim = str . rstrip , start = 1 , end = sys . maxsize ) : if prev is None : if filename is None : raise Exception ( 'No input available for readline.' ) elif is_str_type ( filename ) : file_list = [ filename , ] else : file_list = filename else : file_list = prev for fn in file_list : if isinstance ( fn , file_type ) : fd = fn else : fd = open ( fn , mode ) try : if start <= 1 and end == sys . maxsize : for line in fd : yield trim ( line ) else : for line_no , line in enumerate ( fd , 1 ) : if line_no < start : continue yield trim ( line ) if line_no >= end : break finally : if fd != fn : fd . close ( )
11615	def print_read ( self , rid ) : if self . rname is not None : print self . rname [ rid ] print '--' r = self . get_read_data ( rid ) aligned_loci = np . unique ( r . nonzero ( ) [ 1 ] ) for locus in aligned_loci : nzvec = r [ : , locus ] . todense ( ) . transpose ( ) [ 0 ] . A . flatten ( ) if self . lname is not None : print self . lname [ locus ] , else : print locus , print nzvec
3113	def _validate ( self , value ) : _LOGGER . info ( 'validate: Got type %s' , type ( value ) ) if value is not None and not isinstance ( value , client . Flow ) : raise TypeError ( 'Property {0} must be convertible to a flow ' 'instance; received: {1}.' . format ( self . _name , value ) )
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
11281	def clone ( self ) : new_object = copy . copy ( self ) if new_object . next : new_object . next = new_object . next . clone ( ) return new_object
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
8926	def bump ( ctx , verbose = False , pypi = False ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = False , ctx = ctx ) # Check for uncommitted changes if not scm . workdir_is_clean ( ) : notify . warning ( "You have uncommitted changes, will create a time-stamped version!" ) pep440 = scm . pep440_dev_version ( verbose = verbose , non_local = pypi ) # Rewrite 'setup.cfg' TODO: refactor to helper, see also release-prep # with util.rewrite_file(cfg.rootjoin('setup.cfg')) as lines: # ... setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if not pep440 : notify . info ( "Working directory contains a release version!" ) elif os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if re . match ( r"#? *tag_build *= *.*" , line ) : verb , _ = data [ i ] . split ( '=' , 1 ) data [ i ] = '{}= {}\n' . format ( verb , pep440 ) changed = True if changed : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) else : notify . warning ( "No 'tag_build' setting found in 'setup.cfg'!" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) if os . path . exists ( setup_cfg ) : # Update metadata and print version egg_info = shell . capture ( "python setup.py egg_info" , echo = True if verbose else None ) for line in egg_info . splitlines ( ) : if line . endswith ( 'PKG-INFO' ) : pkg_info_file = line . split ( None , 1 ) [ 1 ] with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : notify . info ( '\n' . join ( i for i in handle . readlines ( ) if i . startswith ( 'Version:' ) ) . strip ( ) ) ctx . run ( "python setup.py -q develop" , echo = True if verbose else None )
12192	def _instruction_list ( self , filters ) : return '\n\n' . join ( [ self . INSTRUCTIONS . strip ( ) , '*Supported methods:*' , 'If you send "@{}: help" to me I reply with these ' 'instructions.' . format ( self . user ) , 'If you send "@{}: version" to me I reply with my current ' 'version.' . format ( self . user ) , ] + [ filter . description ( ) for filter in filters ] )
2561	def recv_result_from_workers ( self ) : info = MPI . Status ( ) result = self . comm . recv ( source = MPI . ANY_SOURCE , tag = RESULT_TAG , status = info ) logger . debug ( "Received result from workers: {}" . format ( result ) ) return result
2133	def _workflow_node_structure ( node_results ) : # Build list address translation, and create backlink lists node_list_pos = { } for i , node_result in enumerate ( node_results ) : for rel in [ 'success' , 'failure' , 'always' ] : node_result [ '{0}_backlinks' . format ( rel ) ] = [ ] node_list_pos [ node_result [ 'id' ] ] = i # Populate backlink lists for node_result in node_results : for rel in [ 'success' , 'failure' , 'always' ] : for sub_node_id in node_result [ '{0}_nodes' . format ( rel ) ] : j = node_list_pos [ sub_node_id ] node_results [ j ] [ '{0}_backlinks' . format ( rel ) ] . append ( node_result [ 'id' ] ) # Find the root nodes root_nodes = [ ] for node_result in node_results : is_root = True for rel in [ 'success' , 'failure' , 'always' ] : if node_result [ '{0}_backlinks' . format ( rel ) ] != [ ] : is_root = False break if is_root : root_nodes . append ( node_result [ 'id' ] ) # Create network dictionary recursively from root nodes def branch_schema ( node_id ) : i = node_list_pos [ node_id ] node_dict = node_results [ i ] ret_dict = { "id" : node_id } for fd in NODE_STANDARD_FIELDS : val = node_dict . get ( fd , None ) if val is not None : if fd == 'unified_job_template' : job_type = node_dict [ 'summary_fields' ] [ 'unified_job_template' ] [ 'unified_job_type' ] ujt_key = JOB_TYPES [ job_type ] ret_dict [ ujt_key ] = val else : ret_dict [ fd ] = val for rel in [ 'success' , 'failure' , 'always' ] : sub_node_id_list = node_dict [ '{0}_nodes' . format ( rel ) ] if len ( sub_node_id_list ) == 0 : continue relationship_name = '{0}_nodes' . format ( rel ) ret_dict [ relationship_name ] = [ ] for sub_node_id in sub_node_id_list : ret_dict [ relationship_name ] . append ( branch_schema ( sub_node_id ) ) return ret_dict schema_dict = [ ] for root_node_id in root_nodes : schema_dict . append ( branch_schema ( root_node_id ) ) return schema_dict
5032	def _build_admin_context ( request , customer ) : opts = customer . _meta codename = get_permission_codename ( 'change' , opts ) has_change_permission = request . user . has_perm ( '%s.%s' % ( opts . app_label , codename ) ) return { 'has_change_permission' : has_change_permission , 'opts' : opts }
866	def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( "Setting custom configuration properties=%r; caller=%r" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )
9393	def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
8563	def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE' ) return response
6646	def _mirrorStructure ( dictionary , value ) : result = type ( dictionary ) ( ) for k in dictionary . keys ( ) : if isinstance ( dictionary [ k ] , dict ) : result [ k ] = _mirrorStructure ( dictionary [ k ] , value ) else : result [ k ] = value return result
10710	def get_water_heaters ( self ) : water_heaters = [ ] for location in self . locations : _location_id = location . get ( "id" ) for device in location . get ( "equipment" ) : if device . get ( "type" ) == "Water Heater" : water_heater_modes = self . api_interface . get_modes ( device . get ( "id" ) ) water_heater_usage = self . api_interface . get_usage ( device . get ( "id" ) ) water_heater = self . api_interface . get_device ( device . get ( "id" ) ) vacations = self . api_interface . get_vacations ( ) device_vacations = [ ] for vacation in vacations : for equipment in vacation . get ( "participatingEquipment" ) : if equipment . get ( "id" ) == water_heater . get ( "id" ) : device_vacations . append ( EcoNetVacation ( vacation , self . api_interface ) ) water_heaters . append ( EcoNetWaterHeater ( water_heater , water_heater_modes , water_heater_usage , _location_id , device_vacations , self . api_interface ) ) return water_heaters
3987	def _move_temp_binary_to_path ( tmp_binary_path ) : # pylint: disable=E1101 binary_path = _get_binary_location ( ) if not binary_path . endswith ( constants . DUSTY_BINARY_NAME ) : raise RuntimeError ( 'Refusing to overwrite binary {}' . format ( binary_path ) ) st = os . stat ( binary_path ) permissions = st . st_mode owner = st . st_uid group = st . st_gid shutil . move ( tmp_binary_path , binary_path ) os . chown ( binary_path , owner , group ) os . chmod ( binary_path , permissions ) return binary_path
6442	def _cond_k ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 ) and ( word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 ] == 'u' and word [ - suffix_len - 1 ] == 'e' ) )
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
13540	def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
6365	def to_dict ( self ) : return { 'tp' : self . _tp , 'tn' : self . _tn , 'fp' : self . _fp , 'fn' : self . _fn }
10334	def build_expand_node_neighborhood_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , BELGraph , str ] , None ] : @ uni_in_place_transformation def expand_node_neighborhood_by_hash ( universe : BELGraph , graph : BELGraph , node_hash : str ) -> None : """Expand around the neighborhoods of a node by identifier.""" node = manager . get_dsl_by_hash ( node_hash ) return expand_node_neighborhood ( universe , graph , node ) return expand_node_neighborhood_by_hash
5059	def send_email_notification_message ( user , enrolled_in , enterprise_customer , email_connection = None ) : if hasattr ( user , 'first_name' ) and hasattr ( user , 'username' ) : # PendingEnterpriseCustomerUsers don't have usernames or real names. We should # template slightly differently to make sure weird stuff doesn't happen. user_name = user . first_name if not user_name : user_name = user . username else : user_name = None # Users have an `email` attribute; PendingEnterpriseCustomerUsers have `user_email`. if hasattr ( user , 'email' ) : user_email = user . email elif hasattr ( user , 'user_email' ) : user_email = user . user_email else : raise TypeError ( _ ( '`user` must have one of either `email` or `user_email`.' ) ) msg_context = { 'user_name' : user_name , 'enrolled_in' : enrolled_in , 'organization_name' : enterprise_customer . name , } try : enterprise_template_config = enterprise_customer . enterprise_enrollment_template except ( ObjectDoesNotExist , AttributeError ) : enterprise_template_config = None plain_msg , html_msg = build_notification_message ( msg_context , enterprise_template_config ) subject_line = get_notification_subject_line ( enrolled_in [ 'name' ] , enterprise_template_config ) from_email_address = get_configuration_value_for_site ( enterprise_customer . site , 'DEFAULT_FROM_EMAIL' , default = settings . DEFAULT_FROM_EMAIL ) return mail . send_mail ( subject_line , plain_msg , from_email_address , [ user_email ] , html_message = html_msg , connection = email_connection )
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None #TODO: all two-phase properties? elif phase is None : return None else : raise Exception ( 'Property not recognized' )
9362	def text ( what = "sentence" , * args , * * kwargs ) : if what == "character" : return character ( * args , * * kwargs ) elif what == "characters" : return characters ( * args , * * kwargs ) elif what == "word" : return word ( * args , * * kwargs ) elif what == "words" : return words ( * args , * * kwargs ) elif what == "sentence" : return sentence ( * args , * * kwargs ) elif what == "sentences" : return sentences ( * args , * * kwargs ) elif what == "paragraph" : return paragraph ( * args , * * kwargs ) elif what == "paragraphs" : return paragraphs ( * args , * * kwargs ) elif what == "title" : return title ( * args , * * kwargs ) else : raise NameError ( 'No such method' )
3666	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Cpsms = [ i ( T ) for i in self . HeatCapacitySolids ] return mixing_simple ( zs , Cpsms ) else : raise Exception ( 'Method not valid' )
12864	def days_in_month ( year , month ) : eom = _days_per_month [ month - 1 ] if is_leap_year ( year ) and month == 2 : eom += 1 return eom
8023	def retry ( exceptions = ( Exception , ) , interval = 0 , max_retries = 10 , success = None , timeout = - 1 ) : if not exceptions and success is None : raise TypeError ( '`exceptions` and `success` parameter can not both be None' ) # For python 3 compatability exceptions = exceptions or ( _DummyException , ) _retries_error_msg = ( 'Exceeded maximum number of retries {} at ' 'an interval of {}s for function {}' ) _timeout_error_msg = 'Maximum timeout of {}s reached for function {}' @ decorator def wrapper ( func , * args , * * kwargs ) : signal . signal ( signal . SIGALRM , _timeout ( _timeout_error_msg . format ( timeout , func . __name__ ) ) ) run_func = functools . partial ( func , * args , * * kwargs ) logger = logging . getLogger ( func . __module__ ) if max_retries < 0 : iterator = itertools . count ( ) else : iterator = range ( max_retries ) if timeout > 0 : signal . alarm ( timeout ) for num , _ in enumerate ( iterator , 1 ) : try : result = run_func ( ) if success is None or success ( result ) : signal . alarm ( 0 ) return result except exceptions : logger . exception ( 'Exception experienced when trying function {}' . format ( func . __name__ ) ) if num == max_retries : raise logger . warning ( 'Retrying {} in {}s...' . format ( func . __name__ , interval ) ) time . sleep ( interval ) else : raise MaximumRetriesExceeded ( _retries_error_msg . format ( max_retries , interval , func . __name__ ) ) return wrapper
11402	def create_records ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : # Use the DOTALL flag to include newlines. regex = re . compile ( '<record.*?>.*?</record>' , re . DOTALL ) record_xmls = regex . findall ( marcxml ) return [ create_record ( record_xml , verbose = verbose , correct = correct , parser = parser , keep_singletons = keep_singletons ) for record_xml in record_xmls ]
7839	def set_action ( self , action ) : if action is None : if self . xmlnode . hasProp ( "action" ) : self . xmlnode . unsetProp ( "action" ) return if action not in ( "remove" , "update" ) : raise ValueError ( "Action must be 'update' or 'remove'" ) action = unicode ( action ) self . xmlnode . setProp ( "action" , action . encode ( "utf-8" ) )
92	def _quokka_normalize_extract ( extract ) : # TODO get rid of this deferred import from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == "square" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( "Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage " + "for parameter 'extract', got %s." % ( type ( extract ) , ) ) return bb
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 # = sys.float_info.min_exp MAX_EXP = 1024 # = sys.float_info.max_exp MANT_DIG = 53 # = sys.float_info.mant_dig BITS = 64 elif size == 4 : MIN_EXP = - 125 # C's FLT_MIN_EXP MAX_EXP = 128 # FLT_MAX_EXP MANT_DIG = 24 # FLT_MANT_DIG BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) # other values possible exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) # abs(x) == m * 2**e exp = e - ( MIN_EXP - 1 ) if exp > 0 : # Normal case. mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : # Subnormal case. if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 # Special case: rounding produced a MANT_DIG-bit mantissa. assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 # Raise on overflow (in some circumstances, may want to return # infinity instead). if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) # check constraints assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
12044	def where_cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]
11205	def name_from_string ( self , tzname_str ) : if not tzname_str . startswith ( '@' ) : return tzname_str name_splt = tzname_str . split ( ',-' ) try : offset = int ( name_splt [ 1 ] ) except : raise ValueError ( "Malformed timezone string." ) return self . load_name ( offset )
2989	def serialize_options ( opts ) : options = ( opts or { } ) . copy ( ) for key in opts . keys ( ) : if key not in DEFAULT_OPTIONS : LOG . warning ( "Unknown option passed to Flask-CORS: %s" , key ) # Ensure origins is a list of allowed origins with at least one entry. options [ 'origins' ] = sanitize_regex_param ( options . get ( 'origins' ) ) options [ 'allow_headers' ] = sanitize_regex_param ( options . get ( 'allow_headers' ) ) # This is expressly forbidden by the spec. Raise a value error so people # don't get burned in production. if r'.*' in options [ 'origins' ] and options [ 'supports_credentials' ] and options [ 'send_wildcard' ] : raise ValueError ( "Cannot use supports_credentials in conjunction with" "an origin string of '*'. See: " "http://www.w3.org/TR/cors/#resource-requests" ) serialize_option ( options , 'expose_headers' ) serialize_option ( options , 'methods' , upper = True ) if isinstance ( options . get ( 'max_age' ) , timedelta ) : options [ 'max_age' ] = str ( int ( options [ 'max_age' ] . total_seconds ( ) ) ) return options
6472	def color_ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color_ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color_ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color_ramp
13128	def tree2commands ( self , adapter , session , lastcmds , xsync ) : # do some preliminary sanity checks... # todo: do i really want to be using assert statements?... assert xsync . tag == constants . NODE_SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD_SYNCHDR assert xsync [ 1 ] . tag == constants . NODE_SYNCBODY version = xsync [ 0 ] . findtext ( 'VerProto' ) if version != constants . SYNCML_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML version "%s" (expected "%s")' % ( version , constants . SYNCML_VERSION_1_2 ) ) verdtd = xsync [ 0 ] . findtext ( 'VerDTD' ) if verdtd != constants . SYNCML_DTD_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML DTD version "%s" (expected "%s")' % ( verdtd , constants . SYNCML_DTD_VERSION_1_2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . isServer : log . debug ( 'received request SyncML message from "%s" (s%s.m%s)' , hdrcmd . target , hdrcmd . sessionID , hdrcmd . msgID ) else : log . debug ( 'received response SyncML message from "%s" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . sessionID , lastcmds [ 0 ] . msgID ) try : return self . _tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . isServer : raise # TODO: make this configurable as to whether or not any error # is sent back to the peer as a SyncML "standardized" error # status... code = '%s.%s' % ( e . __class__ . __module__ , e . __class__ . __name__ ) msg = '' . join ( traceback . format_exception_only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) # TODO: for some reason, the active exception is not being logged... return [ hdrcmd , state . Command ( name = constants . CMD_STATUS , cmdID = '1' , msgRef = session . pendingMsgID , cmdRef = 0 , sourceRef = xsync [ 0 ] . findtext ( 'Source/LocURI' ) , targetRef = xsync [ 0 ] . findtext ( 'Target/LocURI' ) , statusOf = constants . CMD_SYNCHDR , statusCode = constants . STATUS_COMMAND_FAILED , errorCode = code , errorMsg = msg , errorTrace = '' . join ( traceback . format_exception ( type ( e ) , e , sys . exc_info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD_FINAL ) ]
11638	def write_data ( data , filename ) : name , ext = get_file_extension ( filename ) func = json_write_data if ext == '.json' else yaml_write_data return func ( data , filename )
10270	def get_unweighted_upstream_leaves ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT return filter_nodes ( graph , [ node_is_upstream_leaf , data_missing_key_builder ( key ) ] )
13287	def load ( cls , query_name ) : template_path = os . path . join ( os . path . dirname ( __file__ ) , '../data/githubv4' , query_name + '.graphql' ) with open ( template_path ) as f : query_data = f . read ( ) return cls ( query_data , name = query_name )
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
12409	def package_info ( cls , package ) : if package not in cls . package_info_cache : package_json_url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . getLogger ( 'requests' ) . setLevel ( logging . WARN ) response = requests . get ( package_json_url ) response . raise_for_status ( ) cls . package_info_cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package_json_url , e ) cls . package_info_cache [ package ] = None return cls . package_info_cache [ package ]
12676	def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
13120	def argument_count ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . count ( * * vars ( arguments ) )
10615	def clone ( self ) : result = copy . copy ( self ) result . _compound_masses = copy . deepcopy ( self . _compound_masses ) return result
1747	def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
12399	def parse ( cls , s , required = False ) : req = pkg_resources . Requirement . parse ( s ) return cls ( req , required = required )
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
6266	def stop ( self ) -> float : self . stop_time = time . time ( ) return self . stop_time - self . start_time - self . offset
6057	def resized_array_2d_from_array_2d_and_resized_shape ( array_2d , resized_shape , origin = ( - 1 , - 1 ) , pad_value = 0.0 ) : y_is_even = int ( array_2d . shape [ 0 ] ) % 2 == 0 x_is_even = int ( array_2d . shape [ 1 ] ) % 2 == 0 if origin is ( - 1 , - 1 ) : if y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) elif not y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) if x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) elif not x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) origin = ( y_centre , x_centre ) resized_array = np . zeros ( shape = resized_shape ) if y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 elif not y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 if x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 elif not x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 for y_resized , y in enumerate ( range ( y_min , y_max ) ) : for x_resized , x in enumerate ( range ( x_min , x_max ) ) : if y >= 0 and y < array_2d . shape [ 0 ] and x >= 0 and x < array_2d . shape [ 1 ] : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] else : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = pad_value return resized_array
974	def mapBucketIndexToNonZeroBits ( self , index ) : if index < 0 : index = 0 if index >= self . _maxBuckets : index = self . _maxBuckets - 1 if not self . bucketMap . has_key ( index ) : if self . verbosity >= 2 : print "Adding additional buckets to handle index=" , index self . _createBucket ( index ) return self . bucketMap [ index ]
3521	def performable ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PerformableNode ( )
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
1442	def next_tuple ( self , latency_in_ns ) : self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) self . update_count ( self . NEXT_TUPLE_COUNT )
13480	def _str_replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )
12521	def to_file ( self , output_file , smooth_fwhm = 0 , outdtype = None ) : outmat , mask_indices , mask_shape = self . to_matrix ( smooth_fwhm , outdtype ) exporter = ExportData ( ) content = { 'data' : outmat , 'labels' : self . labels , 'mask_indices' : mask_indices , 'mask_shape' : mask_shape , } if self . others : content . update ( self . others ) log . debug ( 'Creating content in file {}.' . format ( output_file ) ) try : exporter . save_variables ( output_file , content ) except Exception as exc : raise Exception ( 'Error saving variables to file {}.' . format ( output_file ) ) from exc
5161	def __intermediate_addresses ( self , interface ) : address_list = self . get_copy ( interface , 'addresses' ) # do not ignore interfaces if they do not contain any address if not address_list : return [ { 'proto' : 'none' } ] result = [ ] static = { } dhcp = [ ] for address in address_list : family = address . get ( 'family' ) # dhcp if address [ 'proto' ] == 'dhcp' : address [ 'proto' ] = 'dhcp' if family == 'ipv4' else 'dhcpv6' dhcp . append ( self . __intermediate_address ( address ) ) continue if 'gateway' in address : uci_key = 'gateway' if family == 'ipv4' else 'ip6gw' interface [ uci_key ] = address [ 'gateway' ] # static address_key = 'ipaddr' if family == 'ipv4' else 'ip6addr' static . setdefault ( address_key , [ ] ) static [ address_key ] . append ( '{address}/{mask}' . format ( * * address ) ) static . update ( self . __intermediate_address ( address ) ) if static : # do not use CIDR notation when using a single ipv4 # see https://github.com/openwisp/netjsonconfig/issues/54 if len ( static . get ( 'ipaddr' , [ ] ) ) == 1 : network = ip_interface ( six . text_type ( static [ 'ipaddr' ] [ 0 ] ) ) static [ 'ipaddr' ] = str ( network . ip ) static [ 'netmask' ] = str ( network . netmask ) # do not use lists when using a single ipv6 address # (avoids to change output of existing configuration) if len ( static . get ( 'ip6addr' , [ ] ) ) == 1 : static [ 'ip6addr' ] = static [ 'ip6addr' ] [ 0 ] result . append ( static ) if dhcp : result += dhcp return result
13380	def join_dicts ( * dicts ) : out_dict = { } for d in dicts : for k , v in d . iteritems ( ) : if not type ( v ) in JOINERS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) JOINERS [ type ( v ) ] ( out_dict , k , v ) return out_dict
12761	def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
4640	def find_next ( self ) : if int ( self . num_retries ) < 0 : # pragma: no cover self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( # Only provide URLS if num_retries is bigger equal 0, # i.e. we want to do reconnects at all int ( self . num_retries ) >= 0 # the counter for this host/endpoint should be smaller than # num_retries and v <= self . num_retries # let's not retry with the same URL *if* we have others # available and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
1524	def dereference_symlinks ( src ) : while os . path . islink ( src ) : src = os . path . join ( os . path . dirname ( src ) , os . readlink ( src ) ) return src
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : #Bind to signature. May throw its own TypeError bound = sig . bind ( * args , * * kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
10115	def parse ( grid_str , mode = MODE_ZINC , charset = 'utf-8' ) : # Decode incoming text (or python3 will whine!) if isinstance ( grid_str , six . binary_type ) : grid_str = grid_str . decode ( encoding = charset ) # Split the separate grids up, the grammar definition has trouble splitting # them up normally. This will truncate the newline off the end of the last # row. _parse = functools . partial ( parse_grid , mode = mode , charset = charset ) if mode == MODE_JSON : if isinstance ( grid_str , six . string_types ) : grid_data = json . loads ( grid_str ) else : grid_data = grid_str if isinstance ( grid_data , dict ) : return _parse ( grid_data ) else : return list ( map ( _parse , grid_data ) ) else : return list ( map ( _parse , GRID_SEP . split ( grid_str . rstrip ( ) ) ) )
1620	def FindNextMultiLineCommentStart ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : # Only return this marker if the comment goes beyond this line if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
766	def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
13213	def rename ( self , from_name , to_name ) : log . info ( 'renaming database from %s to %s' % ( from_name , to_name ) ) self . _run_stmt ( 'alter database %s rename to %s' % ( from_name , to_name ) )
4430	async def _remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
1710	def Js ( val , Clamped = False ) : if isinstance ( val , PyJs ) : return val elif val is None : return undefined elif isinstance ( val , basestring ) : return PyJsString ( val , StringPrototype ) elif isinstance ( val , bool ) : return true if val else false elif isinstance ( val , float ) or isinstance ( val , int ) or isinstance ( val , long ) or ( NUMPY_AVAILABLE and isinstance ( val , ( numpy . int8 , numpy . uint8 , numpy . int16 , numpy . uint16 , numpy . int32 , numpy . uint32 , numpy . float32 , numpy . float64 ) ) ) : # This is supposed to speed things up. may not be the case if val in NUM_BANK : return NUM_BANK [ val ] return PyJsNumber ( float ( val ) , NumberPrototype ) elif isinstance ( val , FunctionType ) : return PyJsFunction ( val , FunctionPrototype ) #elif isinstance(val, ModuleType): # mod = {} # for name in dir(val): # value = getattr(val, name) # if isinstance(value, ModuleType): # continue # prevent recursive module conversion # try: # jsval = HJs(value) # except RuntimeError: # print 'Could not convert %s to PyJs object!' % name # continue # mod[name] = jsval # return Js(mod) #elif isintance(val, ClassType): elif isinstance ( val , dict ) : # convert to object temp = PyJsObject ( { } , ObjectPrototype ) for k , v in six . iteritems ( val ) : temp . put ( Js ( k ) , Js ( v ) ) return temp elif isinstance ( val , ( list , tuple ) ) : #Convert to array return PyJsArray ( val , ArrayPrototype ) # convert to typedarray elif isinstance ( val , JsObjectWrapper ) : return val . __dict__ [ '_obj' ] elif NUMPY_AVAILABLE and isinstance ( val , numpy . ndarray ) : if val . dtype == numpy . int8 : return PyJsInt8Array ( val , Int8ArrayPrototype ) elif val . dtype == numpy . uint8 and not Clamped : return PyJsUint8Array ( val , Uint8ArrayPrototype ) elif val . dtype == numpy . uint8 and Clamped : return PyJsUint8ClampedArray ( val , Uint8ClampedArrayPrototype ) elif val . dtype == numpy . int16 : return PyJsInt16Array ( val , Int16ArrayPrototype ) elif val . dtype == numpy . uint16 : return PyJsUint16Array ( val , Uint16ArrayPrototype ) elif val . dtype == numpy . int32 : return PyJsInt32Array ( val , Int32ArrayPrototype ) elif val . dtype == numpy . uint32 : return PyJsUint16Array ( val , Uint32ArrayPrototype ) elif val . dtype == numpy . float32 : return PyJsFloat32Array ( val , Float32ArrayPrototype ) elif val . dtype == numpy . float64 : return PyJsFloat64Array ( val , Float64ArrayPrototype ) else : # try to convert to js object return py_wrap ( val )
11922	def paginate_dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate_dataframe ( dataframe , self . request , view = self )
5490	def discover ( cls ) : file = os . path . join ( Config . config_dir , Config . config_name ) return cls . from_file ( file )
8273	def color ( self , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s return rng ( clr , d )
6478	def _normalised_numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
10134	def add_item ( self , key , value , after = False , index = None , pos_key = None , replace = True ) : if self . _validate_fn : self . _validate_fn ( value ) if ( index is not None ) and ( pos_key is not None ) : raise ValueError ( 'Either specify index or pos_key, not both.' ) elif pos_key is not None : try : index = self . index ( pos_key ) except ValueError : raise KeyError ( '%r not found' % pos_key ) if after and ( index is not None ) : # insert inserts *before* index, so increment by one. index += 1 if key in self . _values : if not replace : raise KeyError ( '%r is duplicate' % key ) if index is not None : # We are re-locating. del self [ key ] else : # We are updating self . _values [ key ] = value return if index is not None : # Place at given position self . _order . insert ( index , key ) else : # Place at end self . _order . append ( key ) self . _values [ key ] = value
10336	def bel_to_spia_matrices ( graph : BELGraph ) -> Mapping [ str , pd . DataFrame ] : index_nodes = get_matrix_index ( graph ) spia_matrices = build_spia_matrices ( index_nodes ) for u , v , edge_data in graph . edges ( data = True ) : # Both nodes are CentralDogma abundances if isinstance ( u , CentralDogma ) and isinstance ( v , CentralDogma ) : # Update matrix dict update_spia_matrices ( spia_matrices , u , v , edge_data ) # Subject is CentralDogmaAbundance and node is ListAbundance elif isinstance ( u , CentralDogma ) and isinstance ( v , ListAbundance ) : # Add a relationship from subject to each of the members in the object for node in v . members : # Skip if the member is not in CentralDogma if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , u , node , edge_data ) # Subject is ListAbundance and node is CentralDogmaAbundance elif isinstance ( u , ListAbundance ) and isinstance ( v , CentralDogma ) : # Add a relationship from each of the members of the subject to the object for node in u . members : # Skip if the member is not in CentralDogma if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , node , v , edge_data ) # Both nodes are ListAbundance elif isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for sub_member , obj_member in product ( u . members , v . members ) : # Update matrix if both are CentralDogma if isinstance ( sub_member , CentralDogma ) and isinstance ( obj_member , CentralDogma ) : update_spia_matrices ( spia_matrices , sub_member , obj_member , edge_data ) # else Not valid edge return spia_matrices
11562	def set_analog_latch ( self , pin , threshold_type , threshold_value , cb = None ) : if self . ANALOG_LATCH_GT <= threshold_type <= self . ANALOG_LATCH_LTE : if 0 <= threshold_value <= 1023 : self . _command_handler . set_analog_latch ( pin , threshold_type , threshold_value , cb ) return True else : return False
3122	def _verify_signature ( message , signature , certs ) : for pem in certs : verifier = Verifier . from_string ( pem , is_x509_cert = True ) if verifier . verify ( message , signature ) : return # If we have not returned, no certificate confirms the signature. raise AppIdentityError ( 'Invalid token signature' )
7647	def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
253	def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : # Close round-trip pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : # Retrieve first dt, stock-price pair from # stack prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : # Push additional stock-prices onto stack price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : # Need to normalize so that we can join pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips
9919	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = True ) except models . EmailAddress . DoesNotExist : return None token = models . PasswordResetToken . objects . create ( email = email ) token . send ( ) return token
6154	def position_CD ( Ka , out_type = 'fb_exact' ) : rs = 10 / ( 2 * np . pi ) # Load b and a ndarrays with the coefficients if out_type . lower ( ) == 'open_loop' : b = np . array ( [ Ka * 4000 * rs ] ) a = np . array ( [ 1 , 1275 , 31250 , 0 ] ) elif out_type . lower ( ) == 'fb_approx' : b = np . array ( [ 3.2 * Ka * rs ] ) a = np . array ( [ 1 , 25 , 3.2 * Ka * rs ] ) elif out_type . lower ( ) == 'fb_exact' : b = np . array ( [ 4000 * Ka * rs ] ) a = np . array ( [ 1 , 1250 + 25 , 25 * 1250 , 4000 * Ka * rs ] ) else : raise ValueError ( 'out_type must be: open_loop, fb_approx, or fc_exact' ) return b , a
5523	def upload_stream ( self , destination , * , offset = 0 ) : return self . get_stream ( "STOR " + str ( destination ) , "1xx" , offset = offset , )
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
12411	def close ( self ) : # Ensure we're not closed. self . require_not_closed ( ) if not self . streaming or self . asynchronous : # We're not streaming, auto-write content-length if not # already set. if 'Content-Length' not in self . headers : self . headers [ 'Content-Length' ] = self . tell ( ) # Flush out the current buffer. self . flush ( ) # We're done with the response; inform the HTTP connector # to close the response stream. self . _closed = True
12985	def getCompressMod ( self ) : if self . compressMode == COMPRESS_MODE_ZLIB : return zlib if self . compressMode == COMPRESS_MODE_BZ2 : return bz2 if self . compressMode == COMPRESS_MODE_LZMA : # Since lzma is not provided by python core in python2, search out some common alternatives. # Throw exception if we can find no lzma implementation. global _lzmaMod if _lzmaMod is not None : return _lzmaMod try : import lzma _lzmaMod = lzma return _lzmaMod except : # Python2 does not provide "lzma" module, search for common alternatives try : from backports import lzma _lzmaMod = lzma return _lzmaMod except : pass try : import lzmaffi as lzma _lzmaMod = lzma return _lzmaMod except : pass raise ImportError ( "Requested compress mode is lzma and could not find a module providing lzma support. Tried: 'lzma', 'backports.lzma', 'lzmaffi' and none of these were available. Please install one of these, or to use an unlisted implementation, set IndexedRedis.fields.compressed._lzmaMod to the module (must implement standard python compression interface)" )
6302	def add_package ( self , name ) : name , cls_name = parse_package_string ( name ) if name in self . package_map : return package = EffectPackage ( name ) package . load ( ) self . packages . append ( package ) self . package_map [ package . name ] = package # Load effect package dependencies self . polulate ( package . effect_packages )
436	def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : if shape is None : shape = [ 28 , 28 ] import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) fig = plt . figure ( fig_idx ) # show all feature images n_units = W . shape [ 1 ] num_r = int ( np . sqrt ( n_units ) ) #  25hidden unit -> 5 num_c = int ( np . ceil ( n_units / num_r ) ) count = int ( 1 ) for _row in range ( 1 , num_r + 1 ) : for _col in range ( 1 , num_c + 1 ) : if count > n_units : break fig . add_subplot ( num_r , num_c , count ) # ------------------------------------------------------------ # plt.imshow(np.reshape(W[:,count-1],(28,28)), cmap='gray') # ------------------------------------------------------------ feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) # feature[feature<0.0001] = 0 # value threshold # if count == 1 or count == 2: # print(np.mean(feature)) # if np.std(feature) < 0.03: # condition threshold # feature = np.zeros_like(feature) # if np.mean(feature) < -0.015: # condition threshold # feature = np.zeros_like(feature) plt . imshow ( np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = "nearest" ) # , vmin=np.min(feature), vmax=np.max(feature)) # plt.title(name) # ------------------------------------------------------------ # plt.imshow(np.reshape(W[:,count-1] ,(np.sqrt(size),np.sqrt(size))), cmap='gray', interpolation="nearest") plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) # distable tick plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
5556	def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ # order of operators is important: # prematurely return in cases of "<=" or ">=", otherwise # _strip_zoom() cannot parse config strings starting with "<" # or ">" ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
12634	def transform ( self ) : if self . dcmf1 is None or self . dcmf2 is None : return np . inf for field_name in self . field_weights : if ( str ( getattr ( self . dcmf1 , field_name , '' ) ) != str ( getattr ( self . dcmf2 , field_name , '' ) ) ) : return False return True
13592	def main ( target , label ) : check_environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = TagHandler ( git . list_tags ( ) ) print_information ( handler , label ) tag = handler . yield_tag ( target , label ) confirm ( tag )
11161	def autopep8 ( self , * * kwargs ) : # pragma: no cover self . assert_is_dir_and_exists ( ) for p in self . select_by_ext ( ".py" ) : with open ( p . abspath , "rb" ) as f : code = f . read ( ) . decode ( "utf-8" ) formatted_code = autopep8 . fix_code ( code , * * kwargs ) with open ( p . abspath , "wb" ) as f : f . write ( formatted_code . encode ( "utf-8" ) )
12540	def is_dicom_file ( filepath ) : if not os . path . exists ( filepath ) : raise IOError ( 'File {} not found.' . format ( filepath ) ) filename = os . path . basename ( filepath ) if filename == 'DICOMDIR' : return False try : _ = dicom . read_file ( filepath ) except Exception as exc : log . debug ( 'Checking if {0} was a DICOM, but returned ' 'False.' . format ( filepath ) ) return False return True
9582	def write_elements ( fd , mtp , data , is_name = False ) : fmt = etypes [ mtp ] [ 'fmt' ] if isinstance ( data , Sequence ) : if fmt == 's' or is_name : if isinstance ( data , bytes ) : if is_name and len ( data ) > 31 : raise ValueError ( 'Name "{}" is too long (max. 31 ' 'characters allowed)' . format ( data ) ) fmt = '{}s' . format ( len ( data ) ) data = ( data , ) else : fmt = '' . join ( '{}s' . format ( len ( s ) ) for s in data ) else : l = len ( data ) if l == 0 : # empty array fmt = '' if l > 1 : # more than one element to be written fmt = '{}{}' . format ( l , fmt ) else : data = ( data , ) num_bytes = struct . calcsize ( fmt ) if num_bytes <= 4 : # write SDE if num_bytes < 4 : # add pad bytes fmt += '{}x' . format ( 4 - num_bytes ) fd . write ( struct . pack ( 'hh' + fmt , etypes [ mtp ] [ 'n' ] , * chain ( [ num_bytes ] , data ) ) ) return # write tag: element type and number of bytes fd . write ( struct . pack ( 'b3xI' , etypes [ mtp ] [ 'n' ] , num_bytes ) ) # add pad bytes to fmt, if needed mod8 = num_bytes % 8 if mod8 : fmt += '{}x' . format ( 8 - mod8 ) # write data fd . write ( struct . pack ( fmt , * data ) )
133	def clip_out_of_image ( self , image ) : # load shapely lazily, which makes the dependency more optional import shapely . geometry # if fully out of image, clip everything away, nothing remaining if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) # shapely changes the order of points, we try here to preserve it as # much as possible polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) # could only not find closest points if new polys are empty return polygons_reordered
12369	def rename ( self , id , name ) : return super ( DomainRecords , self ) . update ( id , name = name ) [ self . singular ]
10817	def can_see_members ( self , user ) : if self . privacy_policy == PrivacyPolicy . PUBLIC : return True elif self . privacy_policy == PrivacyPolicy . MEMBERS : return self . is_member ( user ) or self . is_admin ( user ) elif self . privacy_policy == PrivacyPolicy . ADMINS : return self . is_admin ( user )
3329	def acquire_read ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : # If we are the writer, grant a new read lock, always. self . __writercount += 1 return while True : if self . __writer is None : # Only test anything if there is no current writer. if self . __upgradewritercount or self . __pendingwriters : if me in self . __readers : # Only grant a read lock if we already have one # in case writers are waiting for their turn. # This means that writers can't easily get starved # (but see below, readers can). self . __readers [ me ] += 1 return # No, we aren't a reader (yet), wait for our turn. else : # Grant a new read lock, always, in case there are # no pending writers (and no writer). self . __readers [ me ] = self . __readers . get ( me , 0 ) + 1 return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : # Timeout has expired, signal caller of this. raise RuntimeError ( "Acquiring read lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) # Commit the configuration otherwise the jija2_env won't have # a `globals` assignment. config . commit ( ) # Place a few globals in the template environment. from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
7046	def _bls_runner ( times , mags , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) : workarr_u = npones ( times . size ) workarr_v = npones ( times . size ) blsresult = eebls ( times , mags , workarr_u , workarr_v , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) return { 'power' : blsresult [ 0 ] , 'bestperiod' : blsresult [ 1 ] , 'bestpower' : blsresult [ 2 ] , 'transdepth' : blsresult [ 3 ] , 'transduration' : blsresult [ 4 ] , 'transingressbin' : blsresult [ 5 ] , 'transegressbin' : blsresult [ 6 ] }
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) # A binary representation of newRep. We will use this to test containment newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True # Midpoint midIdx = self . _maxBuckets / 2 # Start by checking the overlap at minIndex runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False # Compute running overlaps all the way to the midpoint for i in range ( self . minIndex + 1 , midIdx + 1 ) : # This is the bit that is going to change newBit = ( i - 1 ) % self . w # Update our running overlap if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 # Verify our rules if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False # At this point, runningOverlap contains the overlap for midIdx # Compute running overlaps all the way to maxIndex for i in range ( midIdx + 1 , self . maxIndex + 1 ) : # This is the bit that is going to change newBit = i % self . w # Update our running overlap if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 # Verify our rules if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
12289	def bootstrap_datapackage ( repo , force = False , options = None , noinput = False ) : print ( "Bootstrapping datapackage" ) # get the directory tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) # Initial data package json package = OrderedDict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , "" ) , ( 'description' , "" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise IncompleteParameters ( "Option field with title and description" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + ": " ) if len ( value ) == 0 : print ( "{} cannot be empty. Please re-enter." . format ( var . title ( ) ) ) package [ var ] = value # Now store the package... ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename
9181	def _validate_subjects ( cursor , model ) : subject_vocab = [ term [ 0 ] for term in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( 'subjects' , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects : raise exceptions . InvalidMetadata ( 'subjects' , invalid_subjects )
8750	def get_scalingip ( context , id , fields = None ) : LOG . info ( 'get_scalingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . SCALING , '_deallocated' : False } scaling_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , * * filters ) if not scaling_ip : raise q_exc . ScalingIpNotFound ( id = id ) return v . _make_scaling_ip_dict ( scaling_ip )
13254	def tz ( self ) : if not self . _tz : self . _tz = tzlocal . get_localzone ( ) . zone return self . _tz
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] # tl.logging.info("new %f" % probs) probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
10823	def query_requests ( cls , admin , eager = False ) : # Get direct pending request if hasattr ( admin , 'is_superadmin' ) and admin . is_superadmin : q1 = GroupAdmin . query . with_entities ( GroupAdmin . group_id ) else : q1 = GroupAdmin . query_by_admin ( admin ) . with_entities ( GroupAdmin . group_id ) q2 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q1 ) , ) # Get request from admin groups your are member of q3 = Membership . query_by_user ( user = admin , state = MembershipState . ACTIVE ) . with_entities ( Membership . id_group ) q4 = GroupAdmin . query . filter ( GroupAdmin . admin_type == 'Group' , GroupAdmin . admin_id . in_ ( q3 ) ) . with_entities ( GroupAdmin . group_id ) q5 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q4 ) ) query = q2 . union ( q5 ) return query
11515	def search_item_by_name_and_folder ( self , name , folder_id , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name parameters [ 'folderId' ] = folder_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbynameandfolder' , parameters ) return response [ 'items' ]
1667	def IsBlockInNameSpace ( nesting_state , is_forward_declaration ) : if is_forward_declaration : return len ( nesting_state . stack ) >= 1 and ( isinstance ( nesting_state . stack [ - 1 ] , _NamespaceInfo ) ) return ( len ( nesting_state . stack ) > 1 and nesting_state . stack [ - 1 ] . check_namespace_indentation and isinstance ( nesting_state . stack [ - 2 ] , _NamespaceInfo ) )
1967	def sched ( self ) : if len ( self . procs ) > 1 : logger . debug ( "SCHED:" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) logger . debug ( f"\tCurrent clock: {self.clocks}" ) logger . debug ( f"\tCurrent cpu: {self._current}" ) if len ( self . running ) == 0 : logger . debug ( "None running checking if there is some process waiting for a timeout" ) if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . clocks = min ( x for x in self . timers if x is not None ) + 1 self . check_timers ( ) assert len ( self . running ) != 0 , "DEADLOCK!" self . _current = self . running [ 0 ] return next_index = ( self . running . index ( self . _current ) + 1 ) % len ( self . running ) next_running_idx = self . running [ next_index ] if len ( self . procs ) > 1 : logger . debug ( f"\tTransfer control from process {self._current} to {next_running_idx}" ) self . _current = next_running_idx
4834	def get_catalog ( self , catalog_id ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , resource_id = catalog_id )
10941	def update_function ( self , param_vals ) : self . model = self . func ( param_vals , * self . func_args , * * self . func_kwargs ) d = self . calc_residuals ( ) return np . dot ( d . flat , d . flat )
13383	def expand_envvars ( env ) : out_env = { } for k , v in env . iteritems ( ) : out_env [ k ] = Template ( v ) . safe_substitute ( env ) # Expand twice to make sure we expand everything we possibly can for k , v in out_env . items ( ) : out_env [ k ] = Template ( v ) . safe_substitute ( out_env ) return out_env
1870	def MOVSX ( cpu , op0 , op1 ) : op0 . write ( Operators . SEXTEND ( op1 . read ( ) , op1 . size , op0 . size ) )
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
7958	def handle_read ( self ) : with self . lock : logger . debug ( "handle_read()" ) if self . _eof or self . _socket is None : return if self . _state == "tls-handshake" : while True : logger . debug ( "tls handshake read..." ) self . _continue_tls_handshake ( ) logger . debug ( " state: {0}" . format ( self . _tls_state ) ) if self . _tls_state != "want_read" : break elif self . _tls_state == "connected" : while self . _socket and not self . _eof : logger . debug ( "tls socket read..." ) try : data = self . _socket . read ( 4096 ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : break elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data ) else : while self . _socket and not self . _eof : logger . debug ( "raw socket read..." ) try : data = self . _socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data )
3694	def Tm ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ ] ) : def list_methods ( ) : methods = [ ] if CASRN in Tm_ON_data . index : methods . append ( OPEN_NTBKM ) if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_ORG ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == OPEN_NTBKM : return float ( Tm_ON_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tm' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
5203	def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) # for names with spaces in them result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) # Check that the .cc file has included its header if it exists. if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) # We check here rather than inside ProcessLine so that we see raw # lines rather than "cleaned" lines. CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
7558	def resolve_ambigs ( tmpseq ) : ## the order of rows in GETCONS for aidx in xrange ( 6 ) : #np.uint([82, 75, 83, 89, 87, 77]): ambig , res1 , res2 = GETCONS [ aidx ] ## get true wherever tmpseq is ambig idx , idy = np . where ( tmpseq == ambig ) halfmask = np . random . choice ( np . array ( [ True , False ] ) , idx . shape [ 0 ] ) for col in xrange ( idx . shape [ 0 ] ) : if halfmask [ col ] : tmpseq [ idx [ col ] , idy [ col ] ] = res1 else : tmpseq [ idx [ col ] , idy [ col ] ] = res2 return tmpseq
1903	def colored_level_name ( self , levelname ) : if self . colors_disabled : return self . plain_levelname_format . format ( levelname ) else : return self . colored_levelname_format . format ( self . color_map [ levelname ] , levelname )
8793	def get_all ( self , model ) : tags = { } for name , tag in self . tags . items ( ) : for mtag in model . tags : if tag . is_tag ( mtag ) : tags [ name ] = tag . get ( model ) return tags
10888	def coords ( self , norm = False , form = 'broadcast' ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . arange ( self . l [ i ] , self . r [ i ] ) / norm [ i ] for i in range ( self . dim ) ) return self . _format_vector ( v , form = form )
9029	def _step ( self , row , position , passed ) : if row in passed or not self . _row_should_be_placed ( row , position ) : return self . _place_row ( row , position ) passed = [ row ] + passed # print("{}{} at\t{} {}".format(" " * len(passed), row, position, # passed)) for i , produced_mesh in enumerate ( row . produced_meshes ) : self . _expand_produced_mesh ( produced_mesh , i , position , passed ) for i , consumed_mesh in enumerate ( row . consumed_meshes ) : self . _expand_consumed_mesh ( consumed_mesh , i , position , passed )
10794	def create_comparison_state ( image , position , radius = 5.0 , snr = 20 , method = 'constrained-cubic' , extrapad = 2 , zscale = 1.0 ) : # first pad the image slightly since they are pretty small image = common . pad ( image , extrapad , 0 ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = np . array ( image . shape ) , sigma = 1.0 / snr , radius = radius , psfargs = { 'params' : np . array ( [ 2.0 , 1.0 , 3.0 ] ) , 'error' : 1e-6 , 'threads' : 2 } , objargs = { 'method' : method } , stateargs = { 'sigmapad' : False , 'pad' : 4 , 'zscale' : zscale } ) s . obj . pos [ 0 ] = position + s . pad + extrapad s . reset ( ) s . model_to_true_image ( ) timage = 1 - np . pad ( image , s . pad , mode = 'constant' , constant_values = 0 ) timage = s . psf . execute ( timage ) return s , timage [ s . inner ]
13382	def dict_to_env ( d , pathsep = os . pathsep ) : out_env = { } for k , v in d . iteritems ( ) : if isinstance ( v , list ) : out_env [ k ] = pathsep . join ( v ) elif isinstance ( v , string_types ) : out_env [ k ] = v else : raise TypeError ( '{} not a valid env var type' . format ( type ( v ) ) ) return out_env
3826	async def get_self_info ( self , get_self_info_request ) : response = hangouts_pb2 . GetSelfInfoResponse ( ) await self . _pb_request ( 'contacts/getselfinfo' , get_self_info_request , response ) return response
7686	def downbeat ( annotation , sr = 22050 , length = None , * * kwargs ) : beat_click = mkclick ( 440 * 2 , sr = sr ) downbeat_click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to_interval_values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat_click ) + 1 y = filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat_click ) y += filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat_click ) return y
8259	def _sorted_copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) _list . sort ( sorted , comparison ) if reversed : _list . reverse ( sorted ) return sorted
715	def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : jobID = hyperSearchJob . getJobID ( ) filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) if os . path . exists ( filePath ) : _backupFile ( filePath ) d = dict ( hyperSearchJobID = jobID ) with open ( filePath , "wb" ) as jobIdPickleFile : pickle . dump ( d , jobIdPickleFile )
13342	def concatenate ( tup , axis = 0 ) : from distob import engine if len ( tup ) is 0 : raise ValueError ( 'need at least one array to concatenate' ) first = tup [ 0 ] others = tup [ 1 : ] # allow subclasses to provide their own implementations of concatenate: if ( hasattr ( first , 'concatenate' ) and hasattr ( type ( first ) , '__array_interface__' ) ) : return first . concatenate ( others , axis ) # convert all arguments to arrays/RemoteArrays if they are not already: arrays = [ ] for ar in tup : if isinstance ( ar , DistArray ) : if axis == ar . _distaxis : arrays . extend ( ar . _subarrays ) else : # Since not yet implemented arrays distributed on more than # one axis, will fetch and re-scatter on the new axis: arrays . append ( gather ( ar ) ) elif isinstance ( ar , RemoteArray ) : arrays . append ( ar ) elif isinstance ( ar , Remote ) : arrays . append ( _remote_to_array ( ar ) ) elif hasattr ( type ( ar ) , '__array_interface__' ) : # then treat as a local ndarray arrays . append ( ar ) else : arrays . append ( np . array ( ar ) ) if all ( isinstance ( ar , np . ndarray ) for ar in arrays ) : return np . concatenate ( arrays , axis ) total_length = 0 # validate dimensions are same, except for axis of concatenation: commonshape = list ( arrays [ 0 ] . shape ) commonshape [ axis ] = None # ignore this axis for shape comparison for ar in arrays : total_length += ar . shape [ axis ] shp = list ( ar . shape ) shp [ axis ] = None if shp != commonshape : raise ValueError ( 'incompatible shapes for concatenation' ) # set sensible target block size if splitting subarrays further: blocksize = ( ( total_length - 1 ) // engine . nengines ) + 1 rarrays = [ ] for ar in arrays : if isinstance ( ar , DistArray ) : rarrays . extend ( ar . _subarrays ) elif isinstance ( ar , RemoteArray ) : rarrays . append ( ar ) else : da = _scatter_ndarray ( ar , axis , blocksize ) for ra in da . _subarrays : rarrays . append ( ra ) del da del arrays # At this point rarrays is a list of RemoteArray to be concatenated eid = rarrays [ 0 ] . _id . engine if all ( ra . _id . engine == eid for ra in rarrays ) : # Arrays to be joined are all on the same engine if eid == engine . eid : # Arrays are all local return concatenate ( [ gather ( r ) for r in rarrays ] , axis ) else : return call ( concatenate , rarrays , axis ) else : # Arrays to be joined are on different engines. # TODO: consolidate any consecutive arrays already on same engine return DistArray ( rarrays , axis )
3757	def LFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'LFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'LFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'LFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'LFL' ] ) elif Method == SUZUKI : return Suzuki_LFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_LFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
3359	def insert ( self , index , object ) : self . _check ( object . id ) list . insert ( self , index , object ) # all subsequent entries now have been shifted up by 1 _dict = self . _dict for i , j in iteritems ( _dict ) : if j >= index : _dict [ i ] = j + 1 _dict [ object . id ] = index
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
9242	def detect_actual_closed_dates ( self , issues , kind ) : if self . options . verbose : print ( "Fetching closed dates for {} {}..." . format ( len ( issues ) , kind ) ) all_issues = copy . deepcopy ( issues ) for issue in all_issues : if self . options . verbose > 2 : print ( "." , end = "" ) if not issues . index ( issue ) % 30 : print ( "" ) self . find_closed_date_by_commit ( issue ) if not issue . get ( 'actual_date' , False ) : if issue . get ( 'closed_at' , False ) : print ( "Skipping closed non-merged issue: #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) ) all_issues . remove ( issue ) if self . options . verbose > 2 : print ( "." ) return all_issues
6990	def parallel_varfeatures_lcdir ( lcdir , outdir , fileglob = None , maxobjects = None , timecols = None , magcols = None , errcols = None , recursive = True , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob # now find the files LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting varfeatures...' % len ( matching ) ) return parallel_varfeatures ( matching , outdir , maxobjects = maxobjects , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
7034	def import_apikey ( lcc_server , apikey_text_json ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) respdict = json . loads ( apikey_text_json ) # # now that we have an API key dict, get the API key out of it and write it # to the APIKEYFILE # apikey = respdict [ 'apikey' ] expires = respdict [ 'expires' ] # write this to the apikey file if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) # chmod it to the correct value os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
4317	def info ( filepath ) : info_dictionary = { 'channels' : channels ( filepath ) , 'sample_rate' : sample_rate ( filepath ) , 'bitrate' : bitrate ( filepath ) , 'duration' : duration ( filepath ) , 'num_samples' : num_samples ( filepath ) , 'encoding' : encoding ( filepath ) , 'silent' : silent ( filepath ) } return info_dictionary
9960	def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
10870	def calc_pts_lag ( npts = 20 ) : scl = { 15 : 0.072144 , 20 : 0.051532 , 25 : 0.043266 } [ npts ] pts0 , wts0 = np . polynomial . laguerre . laggauss ( npts ) pts = np . sinh ( pts0 * scl ) wts = scl * wts0 * np . cosh ( pts0 * scl ) * np . exp ( pts0 ) return pts , wts
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. #g/mol to kg/mol dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
5544	def clip_array_with_vector ( array , array_affine , geometries , inverted = False , clip_buffer = 0 ) : # buffer input geometries and clean up buffered_geometries = [ ] for feature in geometries : feature_geom = to_shape ( feature [ "geometry" ] ) if feature_geom . is_empty : continue if feature_geom . geom_type == "GeometryCollection" : # for GeometryCollections apply buffer to every subgeometry # and make union buffered_geom = unary_union ( [ g . buffer ( clip_buffer ) for g in feature_geom ] ) else : buffered_geom = feature_geom . buffer ( clip_buffer ) if not buffered_geom . is_empty : buffered_geometries . append ( buffered_geom ) # mask raster by buffered geometries if buffered_geometries : if array . ndim == 2 : return ma . masked_array ( array , geometry_mask ( buffered_geometries , array . shape , array_affine , invert = inverted ) ) elif array . ndim == 3 : mask = geometry_mask ( buffered_geometries , ( array . shape [ 1 ] , array . shape [ 2 ] ) , array_affine , invert = inverted ) return ma . masked_array ( array , mask = np . stack ( ( mask for band in array ) ) ) # if no geometries, return unmasked array else : fill = False if inverted else True return ma . masked_array ( array , mask = np . full ( array . shape , fill , dtype = bool ) )
11121	def get_file_relative_path_by_name ( self , name , skip = 0 ) : if skip is None : paths = [ ] else : paths = None for path , info in self . walk_files_info ( ) : _ , n = os . path . split ( path ) if n == name : if skip is None : paths . append ( path ) elif skip > 0 : skip -= 1 else : paths = path break return paths
8513	def fit_and_score_estimator ( estimator , parameters , cv , X , y = None , scoring = None , iid = True , n_jobs = 1 , verbose = 1 , pre_dispatch = '2*n_jobs' ) : scorer = check_scoring ( estimator , scoring = scoring ) n_samples = num_samples ( X ) X , y = check_arrays ( X , y , allow_lists = True , sparse_format = 'csr' , allow_nans = True ) if y is not None : if len ( y ) != n_samples : raise ValueError ( 'Target variable (y) has a different number ' 'of samples (%i) than data (X: %i samples)' % ( len ( y ) , n_samples ) ) cv = check_cv ( cv = cv , y = y , classifier = is_classifier ( estimator ) ) out = Parallel ( n_jobs = n_jobs , verbose = verbose , pre_dispatch = pre_dispatch ) ( delayed ( _fit_and_score ) ( clone ( estimator ) , X , y , scorer , train , test , verbose , parameters , fit_params = None ) for train , test in cv . split ( X , y ) ) assert len ( out ) == cv . n_splits train_scores , test_scores = [ ] , [ ] n_train_samples , n_test_samples = [ ] , [ ] for test_score , n_test , train_score , n_train , _ in out : train_scores . append ( train_score ) test_scores . append ( test_score ) n_test_samples . append ( n_test ) n_train_samples . append ( n_train ) train_scores , test_scores = map ( list , check_arrays ( train_scores , test_scores , warn_nans = True , replace_nans = True ) ) if iid : if verbose > 0 and is_msmbuilder_estimator ( estimator ) : print ( '[CV] Using MSMBuilder API n_samples averaging' ) print ( '[CV] n_train_samples: %s' % str ( n_train_samples ) ) print ( '[CV] n_test_samples: %s' % str ( n_test_samples ) ) mean_test_score = np . average ( test_scores , weights = n_test_samples ) mean_train_score = np . average ( train_scores , weights = n_train_samples ) else : mean_test_score = np . average ( test_scores ) mean_train_score = np . average ( train_scores ) grid_scores = { 'mean_test_score' : mean_test_score , 'test_scores' : test_scores , 'mean_train_score' : mean_train_score , 'train_scores' : train_scores , 'n_test_samples' : n_test_samples , 'n_train_samples' : n_train_samples } return grid_scores
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
11765	def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : x , y = move n = 0 # n is number of moves in row while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 # Because we counted move itself twice return n >= self . k
10760	def from_rectilinear ( cls , x , y , z , formatter = numpy_formatter ) : x = np . asarray ( x , dtype = np . float64 ) y = np . asarray ( y , dtype = np . float64 ) z = np . ma . asarray ( z , dtype = np . float64 ) # Check arguments. if x . ndim != 1 : raise TypeError ( "'x' must be a 1D array but is a {:d}D array" . format ( x . ndim ) ) if y . ndim != 1 : raise TypeError ( "'y' must be a 1D array but is a {:d}D array" . format ( y . ndim ) ) if z . ndim != 2 : raise TypeError ( "'z' must be a 2D array but it a {:d}D array" . format ( z . ndim ) ) if x . size != z . shape [ 1 ] : raise TypeError ( ( "the length of 'x' must be equal to the number of columns in " "'z' but the length of 'x' is {:d} and 'z' has {:d} " "columns" ) . format ( x . size , z . shape [ 1 ] ) ) if y . size != z . shape [ 0 ] : raise TypeError ( ( "the length of 'y' must be equal to the number of rows in " "'z' but the length of 'y' is {:d} and 'z' has {:d} " "rows" ) . format ( y . size , z . shape [ 0 ] ) ) # Convert to curvilinear format and call constructor. y , x = np . meshgrid ( y , x , indexing = 'ij' ) return cls ( x , y , z , formatter )
408	def _tf_repeat ( self , a , repeats ) : # https://github.com/tensorflow/tensorflow/issues/8521 if len ( a . get_shape ( ) ) != 1 : raise AssertionError ( "This is not a 1D Tensor" ) a = tf . expand_dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf_flatten ( a ) return a
3385	def _reproject ( self , p ) : nulls = self . problem . nullspace equalities = self . problem . equalities # don't reproject if point is feasible if np . allclose ( equalities . dot ( p ) , self . problem . b , rtol = 0 , atol = self . feasibility_tol ) : new = p else : LOGGER . info ( "feasibility violated in sample" " %d, trying to reproject" % self . n_samples ) new = nulls . dot ( nulls . T . dot ( p ) ) # Projections may violate bounds # set to random point in space in that case if any ( new != p ) : LOGGER . info ( "reprojection failed in sample" " %d, using random point in space" % self . n_samples ) new = self . _random_point ( ) return new
11331	def progress ( length , * * kwargs ) : quiet = False progress_class = kwargs . pop ( "progress_class" , Progress ) kwargs [ "write_method" ] = istdout . info kwargs [ "width" ] = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) kwargs [ "length" ] = length pbar = progress_class ( * * kwargs ) pbar . update ( 0 ) yield pbar pbar . update ( length ) br ( )
13328	def add ( path ) : click . echo ( '\nAdding {} to cache......' . format ( path ) , nl = False ) try : r = cpenv . resolve ( path ) except Exception as e : click . echo ( bold_red ( 'FAILED' ) ) click . echo ( e ) return if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . add ( r . resolved [ 0 ] ) EnvironmentCache . save ( ) click . echo ( bold_green ( 'OK!' ) )
4587	def stop ( self ) : if self . is_running : log . info ( 'Stopping' ) self . is_running = False self . __class__ . _INSTANCE = None try : self . thread and self . thread . stop ( ) except : log . error ( 'Error stopping thread' ) traceback . print_exc ( ) self . thread = None return True
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : # This is Python2 if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : # noqa, pylint: disable=undefined-variable return s . encode ( encoding ) else : # And this is Python3 if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
3992	def get_nginx_configuration_spec ( port_spec_dict , docker_bridge_ip ) : nginx_http_config , nginx_stream_config = "" , "" for port_spec in port_spec_dict [ 'nginx' ] : if port_spec [ 'type' ] == 'http' : nginx_http_config += _nginx_http_spec ( port_spec , docker_bridge_ip ) elif port_spec [ 'type' ] == 'stream' : nginx_stream_config += _nginx_stream_spec ( port_spec , docker_bridge_ip ) return { 'http' : nginx_http_config , 'stream' : nginx_stream_config }
7235	def map ( self , features = None , query = None , styles = None , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 10 , center = None , image = None , image_bounds = None , cmap = 'viridis' , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , * * kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a key or set the MAPBOX_API_KEY environment variable. Use outside of GBDX Notebooks requires a MapBox API key, sign up for free at https://www.mapbox.com/pricing/" if features is None and query is not None : wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = None ) elif features is None and query is None and image is None : print ( 'Must provide either a list of features or a query or an image' ) return if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] geojson = { "type" : "FeatureCollection" , "features" : features } if center is None and features is not None : union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] elif center is None and image is not None : try : lon , lat = shape ( image ) . centroid . coords [ 0 ] except : lon , lat = box ( * image_bounds ) . centroid . coords [ 0 ] else : lat , lon = center map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorGeojsonLayer ( geojson , styles = styles , * * kwargs ) image_layer = self . _build_image_layer ( image , image_bounds , cmap ) template = BaseTemplate ( map_id , * * { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : 'dummy' } ) template . inject ( )
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , * * kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
3844	def from_participantid ( participant_id ) : return user . UserID ( chat_id = participant_id . chat_id , gaia_id = participant_id . gaia_id )
9129	def store_populate_failed ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate_failed ( resource ) _store_helper ( action , session = session ) return action
13429	def get_sites ( self ) : url = "/2/sites" data = self . _get_resource ( url ) sites = [ ] for entry in data [ 'sites' ] : sites . append ( self . site_from_json ( entry ) ) return sites
2950	def _start ( self , my_task , force = False ) : # If the threshold was already reached, there is nothing else to do. if my_task . _has_state ( Task . COMPLETED ) : return True , None if my_task . _has_state ( Task . READY ) : return True , None # Check whether we may fire. if self . split_task is None : return self . _check_threshold_unstructured ( my_task , force ) return self . _check_threshold_structured ( my_task , force )
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] # We either group on the job-name (if present) or fall back to the job-id key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' # Group each of the rows based on (job-name or job-id, status) grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] # Now that we have the rows grouped, create a summary table. # Use the original table as the driver in order to preserve the order. new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] # Written this way to ensure that if somehow a new status is introduced, # it shows up in our output. for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
6894	def parallel_starfeatures_lcdir ( lcdir , outdir , lc_catalog_pickle , neighbor_radius_arcsec , fileglob = None , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS , recursive = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob # now find the files LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting starfeatures...' % len ( matching ) ) return parallel_starfeatures ( matching , outdir , lc_catalog_pickle , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
13779	def FindMessageTypeByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) if full_name not in self . _descriptors : self . FindFileContainingSymbol ( full_name ) return self . _descriptors [ full_name ]
6543	def exec_command ( self , cmdstr ) : if self . is_terminated : raise TerminatedError ( "this TerminalClient instance has been terminated" ) log . debug ( "sending command: %s" , cmdstr ) c = Command ( self . app , cmdstr ) start = time . time ( ) c . execute ( ) elapsed = time . time ( ) - start log . debug ( "elapsed execution: {0}" . format ( elapsed ) ) self . status = Status ( c . status_line ) return c
2539	def set_pkg_excl_file ( self , doc , filename ) : self . assert_package_exists ( ) doc . package . add_exc_file ( filename )
3016	def from_json_keyfile_name ( cls , filename , scopes = '' , token_uri = None , revoke_uri = None ) : with open ( filename , 'r' ) as file_obj : client_credentials = json . load ( file_obj ) return cls . _from_parsed_json_keyfile ( client_credentials , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
11115	def save ( self ) : # open file repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) # save repository try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) # save timestamp repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
9531	def value_to_string ( self , obj ) : value = self . value_from_object ( obj ) return b64encode ( self . _dump ( value ) ) . decode ( 'ascii' )
4929	def transform_launch_points ( self , content_metadata_item ) : return [ { 'providerID' : self . enterprise_configuration . provider_id , 'launchURL' : content_metadata_item [ 'enrollment_url' ] , 'contentTitle' : content_metadata_item [ 'title' ] , 'contentID' : self . get_content_id ( content_metadata_item ) , 'launchType' : 3 , # This tells SAPSF to launch the course in a new browser window. 'mobileEnabled' : True , # Always return True per ENT-1401 'mobileLaunchURL' : content_metadata_item [ 'enrollment_url' ] , } ]
12628	def get_all_files ( folder ) : for path , dirlist , filelist in os . walk ( folder ) : for fn in filelist : yield op . join ( path , fn )
5137	def add ( self , string , start , end , line ) : if string . strip ( ) : # Only add if not entirely whitespace. self . start_lineno = min ( self . start_lineno , start [ 0 ] ) self . end_lineno = max ( self . end_lineno , end [ 0 ] )
206	def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) # Can use double linked-list for easy insertion at beginning/end ''' if e.type == Event.Type.END: existing.insert(0, e) else: existing.append(e) ''' existing [ e . type ] . append ( e )
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name # Pooling type AveragePooling2D pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
849	def getOutputElementCount ( self , name ) : if name == "resetOut" : print ( "WARNING: getOutputElementCount should not have been called with " "resetOut" ) return 1 elif name == "sequenceIdOut" : print ( "WARNING: getOutputElementCount should not have been called with " "sequenceIdOut" ) return 1 elif name == "dataOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'dataOut' " "on a RecordSensor node, but the encoder has not " "been set" ) return self . encoder . getWidth ( ) elif name == "sourceOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) elif name == "bucketIdxOut" : return 1 elif name == "actValueOut" : return 1 elif name == "categoryOut" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( "Unknown output %s" % name )
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 # J/x -> kWh/x result = result / compound . molar_mass # x/mol -> x/kg result = result * mass # x/kg -> x return result
7427	def refmap_init ( data , sample , force ) : ## make some persistent file handles for the refmap reads files sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
11505	def move_folder ( self , token , folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.folder.move' , parameters ) return response
561	def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
9415	def from_value ( cls , value ) : instance = OctaveUserClass . __new__ ( cls ) instance . _address = '%s_%s' % ( instance . _name , id ( instance ) ) instance . _ref ( ) . push ( instance . _address , value ) return instance
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( * * jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
13404	def prettify ( self , elem ) : from xml . etree import ElementTree from re import sub rawString = ElementTree . tostring ( elem , 'utf-8' ) parsedString = sub ( r'(?=<[^/].*>)' , '\n' , rawString ) # Adds newline after each closing tag return parsedString [ 1 : ]
13849	def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
11242	def indent_css ( f , output ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) . rstrip ( ) if len ( string ) > 0 : if string [ - 1 ] == ";" : output . write ( " " + string + "\n" ) else : output . write ( string + "\n" ) output . close ( ) f . close ( )
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
11690	def get_area ( self , geojson ) : geojson = json . load ( open ( geojson , 'r' ) ) self . area = Polygon ( geojson [ 'features' ] [ 0 ] [ 'geometry' ] [ 'coordinates' ] [ 0 ] )
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) # loop files and dirobjects for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) #if os.path.isdir(realFilePath) and df.startswith('.') and df.endswith(self.__objectDir[3:]): # fileDict = {'type':'objectdir', # 'exists':True, # 'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)), # } #else: # fileDict = {'type':'file', # 'exists':os.path.isfile(realFilePath), # 'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)), # } fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) # loop directories #for ddict in sorted([d for d in dirList if isinstance(d, dict) and len(d)], key=lambda k: list(k)[0]): for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) # call recursive _walk_dir if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) # return state list return state
10429	def getrowcount ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) return len ( object_handle . AXRows )
9350	def check_digit ( num ) : sum = 0 # drop last digit, then reverse the number digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : # select all digits at odd positions starting from 1 if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
4624	def _get_encrypted_masterpassword ( self ) : if not self . unlocked ( ) : raise WalletLocked aes = AESCipher ( self . password ) return "{}${}" . format ( self . _derive_checksum ( self . masterkey ) , aes . encrypt ( self . masterkey ) )
12626	def recursive_find_search ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in files if re . search ( regex , f ) ] ) return outlist
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
3701	def solubility_eutectic ( T , Tm , Hm , Cpl = 0 , Cps = 0 , gamma = 1 ) : dCp = Cpl - Cps x = exp ( - Hm / R / T * ( 1 - T / Tm ) + dCp * ( Tm - T ) / R / T - dCp / R * log ( Tm / T ) ) / gamma return x
9734	def get_image ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . image_count ) : component_position , image_info = QRTPacket . _get_exact ( RTImage , data , component_position ) append_components ( ( image_info , data [ component_position : - 1 ] ) ) return components
141	def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )
1626	def ReverseCloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if line [ pos ] not in ')}]>' : return ( line , 0 , - 1 ) # Check last line ( start_pos , stack ) = FindStartOfExpressionInLine ( line , pos , [ ] ) if start_pos > - 1 : return ( line , linenum , start_pos ) # Continue scanning backward while stack and linenum > 0 : linenum -= 1 line = clean_lines . elided [ linenum ] ( start_pos , stack ) = FindStartOfExpressionInLine ( line , len ( line ) - 1 , stack ) if start_pos > - 1 : return ( line , linenum , start_pos ) # Did not find start of expression before beginning of file, give up return ( line , 0 , - 1 )
3852	def _get_lookup_spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts_pb2 . EntityLookupSpec ( phone = identifier , create_offnetwork_gaia = True ) elif '@' in identifier : return hangups . hangouts_pb2 . EntityLookupSpec ( email = identifier , create_offnetwork_gaia = True ) else : return hangups . hangouts_pb2 . EntityLookupSpec ( gaia_id = identifier )
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : ## If SE then we enforce the minimum overlap distance to avoid the ## staircase syndrome of multiple reads overlapping just a little. overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] ## the *_buff variables here are because we have to play patty ## cake here with the rstart/rend vals because we want pysam to ## enforce the buffer for SE, but we want the reference sequence ## start and end positions to print correctly for downstream. rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer ## Reads that map to only very short segements of the reference ## sequence will return buffer end values that are before the ## start values causing pysam to complain. Very short mappings. if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp ## Buffering can't make start and end equal or pysam returns nothing. if rstart_buff == rend_buff : rend_buff += 1 ## store pairs rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) ## use dict to match up read pairs for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read ## sort dict keys so highest derep is first ('seed') sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) ## get blocks from the seed for filtering, bail out if seed is not paired try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" ## the starting blocks for the seed poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) ## store the seed ------------------------------------------- if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) ## If there's only one hit in this region then rkeys will only have ## one element and the call to `rkeys[1:]` will raise. Test for this. if len ( rkeys ) > 1 : ## store the hits to the seed ------------------------------- for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : ## enter values that will make this read get skipped read1 = rdict [ key ] [ 0 ] skip = True ## orient reads only if not skipping if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) ## store the seq if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : ## seq is excluded, though, we could save it and return ## it as a separate cluster that will be aligned separately. pass return clust
4833	def traverse_pagination ( response , endpoint , content_filter_query , query_params ) : results = response . get ( 'results' , [ ] ) page = 1 while response . get ( 'next' ) : page += 1 response = endpoint ( ) . post ( content_filter_query , * * dict ( query_params , page = page ) ) results += response . get ( 'results' , [ ] ) return results
11312	def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
11130	def start ( self ) : with self . _status_lock : if self . _running : raise RuntimeError ( "Already running" ) self . _running = True # Cannot re-use Observer after stopped self . _observer = Observer ( ) self . _observer . schedule ( self . _event_handler , self . _directory_location , recursive = True ) self . _observer . start ( ) # Load all in directory afterwards to ensure no undetected changes between loading all and observing self . _origin_mapped_data = self . _load_all_in_directory ( )
8790	def pop ( self , model ) : tags = self . _pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue
3598	def reviews ( self , packageName , filterByDevice = False , sort = 2 , nb_results = None , offset = None ) : # TODO: select the number of reviews to return path = REVIEWS_URL + "?doc={}&sort={}" . format ( requests . utils . quote ( packageName ) , sort ) if nb_results is not None : path += "&n={}" . format ( nb_results ) if offset is not None : path += "&o={}" . format ( offset ) if filterByDevice : path += "&dfil=1" data = self . executeRequestApi2 ( path ) output = [ ] for review in data . payload . reviewResponse . getResponse . review : output . append ( utils . parseProtobufObj ( review ) ) return output
2892	def connect_outgoing ( self , taskspec , sequence_flow_id , sequence_flow_name , documentation ) : self . connect ( taskspec ) s = SequenceFlow ( sequence_flow_id , sequence_flow_name , documentation , taskspec ) self . outgoing_sequence_flows [ taskspec . name ] = s self . outgoing_sequence_flows_by_id [ sequence_flow_id ] = s
8328	def _lastRecursiveChild ( self ) : lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
12095	def indexImages ( folder , fname = "index.html" ) : #TODO: REMOVE html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( " " , os . path . abspath ( folder + "/" + fname ) ) return
5954	def tool_factory ( clsname , name , driver , base = GromacsCommand ) : clsdict = { 'command_name' : name , 'driver' : driver , '__doc__' : property ( base . _get_gmx_docs ) } return type ( clsname , ( base , ) , clsdict )
3529	def get_user_from_context ( context ) : try : return context [ 'user' ] except KeyError : pass try : request = context [ 'request' ] return request . user except ( KeyError , AttributeError ) : pass return None
8966	def which ( command , path = None , verbose = 0 , exts = None ) : matched = whichgen ( command , path , verbose , exts ) try : match = next ( matched ) except StopIteration : raise WhichError ( "Could not find '%s' on the path." % command ) else : return match
12695	def contains_all ( set1 , set2 , warn ) : for elem in set2 : if elem not in set1 : raise ValueError ( warn ) return True
1646	def FindCheckMacro ( line ) : for macro in _CHECK_MACROS : i = line . find ( macro ) if i >= 0 : # Find opening parenthesis. Do a regular expression match here # to make sure that we are matching the expected CHECK macro, as # opposed to some other macro that happens to contain the CHECK # substring. matched = Match ( r'^(.*\b' + macro + r'\s*)\(' , line ) if not matched : continue return ( macro , len ( matched . group ( 1 ) ) ) return ( None , - 1 )
8149	def _should_run ( self , iteration , max_iterations ) : if iteration == 0 : # First frame always runs return True if max_iterations : if iteration < max_iterations : return True elif max_iterations is None : if self . _dynamic : return True else : return False return True if not self . _dynamic : return False return False
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
8363	def get_key_map ( self ) : kdict = { } for gdk_name in dir ( Gdk ) : nb_name = gdk_name . upper ( ) kdict [ nb_name ] = getattr ( Gdk , gdk_name ) return kdict
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
4065	def fields_types ( self , tname , qstring , itemtype ) : # check for a valid cached version template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
11913	def git_tag ( tag ) : print ( 'Tagging "{}"' . format ( tag ) ) msg = '"Released version {}"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )
9612	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } data . setdefault ( 'element_id' , self . element_id ) return self . _driver . _execute ( command , data , unpack )
1449	def get_all_zk_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "zookeeper" ) for location in state_locations : name = location [ 'name' ] hostport = location [ 'hostport' ] hostportlist = [ ] for hostportpair in hostport . split ( ',' ) : host = None port = None if ':' in hostport : hostandport = hostportpair . split ( ':' ) if len ( hostandport ) == 2 : host = hostandport [ 0 ] port = int ( hostandport [ 1 ] ) if not host or not port : raise Exception ( "Hostport for %s must be of the format 'host:port'." % ( name ) ) hostportlist . append ( ( host , port ) ) tunnelhost = location [ 'tunnelhost' ] rootpath = location [ 'rootpath' ] LOG . info ( "Connecting to zk hostports: " + str ( hostportlist ) + " rootpath: " + rootpath ) state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) state_managers . append ( state_manager ) return state_managers
12867	def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close_all' ) : self . database . close_all ( )
7169	def remove_entity ( self , name ) : self . entities . remove ( name ) self . padaos . remove_entity ( name )
13377	def walk_up ( start_dir , depth = 20 ) : root = start_dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
6388	def _sb_ends_in_short_syllable ( self , term ) : if not term : return False if len ( term ) == 2 : if term [ - 2 ] in self . _vowels and term [ - 1 ] not in self . _vowels : return True elif len ( term ) >= 3 : if ( term [ - 3 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 1 ] in self . _codanonvowels ) : return True return False
9556	def _apply_record_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for check , modulus in self . _record_checks : if i % modulus == 0 : # support sampling rdict = self . _as_dict ( r ) try : check ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
2663	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : external_block_id = str ( len ( self . blocks ) ) launch_cmd = self . launch_cmd . format ( block_id = external_block_id ) internal_block = self . provider . submit ( launch_cmd , 1 , 1 ) logger . debug ( "Launched block {}->{}" . format ( external_block_id , internal_block ) ) if not internal_block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) r . extend ( [ external_block_id ] ) self . blocks [ external_block_id ] = internal_block else : logger . error ( "No execution provider available" ) r = None return r
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) # Purge caching ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
4651	def appendWif ( self , wif ) : if wif : try : self . privatekey_class ( wif ) self . wifs . add ( wif ) except Exception : raise InvalidWifError
1441	def serialize_data_tuple ( self , stream_id , latency_in_ns ) : self . update_count ( self . TUPLE_SERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id )
7433	def _read_sample_names ( fname ) : try : with open ( fname , 'r' ) as infile : subsamples = [ x . split ( ) [ 0 ] for x in infile . readlines ( ) if x . strip ( ) ] except Exception as inst : print ( "Failed to read input file with sample names.\n{}" . format ( inst ) ) raise inst return subsamples
8499	def _colorize ( output ) : if not pygments : return output # Available styles # ['monokai', 'manni', 'rrt', 'perldoc', 'borland', 'colorful', 'default', # 'murphy', 'vs', 'trac', 'tango', 'fruity', 'autumn', 'bw', 'emacs', # 'vim', 'pastie', 'friendly', 'native'] return pygments . highlight ( output , pygments . lexers . PythonLexer ( ) , pygments . formatters . Terminal256Formatter ( style = 'monokai' ) )
9135	def get_connection ( module_name : str , connection : Optional [ str ] = None ) -> str : # 1. Use given connection if connection is not None : return connection module_name = module_name . lower ( ) module_config_cls = get_module_config_cls ( module_name ) module_config = module_config_cls . load ( ) return module_config . connection or config . connection
10534	def delete_project ( project_id ) : try : res = _pybossa_req ( 'delete' , 'project' , project_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
10049	def create_error_handlers ( blueprint ) : blueprint . errorhandler ( PIDInvalidAction ) ( create_api_errorhandler ( status = 403 , message = 'Invalid action' ) ) records_rest_error_handlers ( blueprint )
4530	def _addLoggingLevel ( levelName , levelNum , methodName = None ) : if not methodName : methodName = levelName . lower ( ) if hasattr ( logging , levelName ) : raise AttributeError ( '{} already defined in logging module' . format ( levelName ) ) if hasattr ( logging , methodName ) : raise AttributeError ( '{} already defined in logging module' . format ( methodName ) ) if hasattr ( logging . getLoggerClass ( ) , methodName ) : raise AttributeError ( '{} already defined in logger class' . format ( methodName ) ) # This method was inspired by the answers to Stack Overflow post # http://stackoverflow.com/q/2183233/2988730, especially # http://stackoverflow.com/a/13638084/2988730 def logForLevel ( self , message , * args , * * kwargs ) : if self . isEnabledFor ( levelNum ) : self . _log ( levelNum , message , args , * * kwargs ) def logToRoot ( message , * args , * * kwargs ) : logging . log ( levelNum , message , * args , * * kwargs ) logging . addLevelName ( levelNum , levelName ) setattr ( logging , levelName , levelNum ) setattr ( logging . getLoggerClass ( ) , methodName , logForLevel ) setattr ( logging , methodName , logToRoot )
13279	def child_begin_handler ( self , scache , * args ) : pdesc = self . pdesc depth = scache . depth sib_seq = self . sib_seq sibs_len = self . sibs_len pdesc_level = scache . pdesc_level desc = copy . deepcopy ( pdesc ) desc = reset_parent_desc_template ( desc ) desc [ 'depth' ] = depth desc [ 'parent_breadth_path' ] = copy . deepcopy ( desc [ 'breadth_path' ] ) desc [ 'sib_seq' ] = sib_seq desc [ 'parent_path' ] = copy . deepcopy ( desc [ 'path' ] ) desc [ 'path' ] . append ( sib_seq ) update_desc_lsib_path ( desc ) update_desc_rsib_path ( desc , sibs_len ) if ( depth == 1 ) : pass else : update_desc_lcin_path ( desc , pdesc_level ) update_desc_rcin_path ( desc , sibs_len , pdesc_level ) return ( desc )
1059	def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) # Return the wrapper so this can be used as a decorator via partial() return wrapper
8418	def nearest_int ( x ) : if x == 0 : return np . int64 ( 0 ) elif x > 0 : return np . int64 ( x + 0.5 ) else : return np . int64 ( x - 0.5 )
2671	def deploy_s3 ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , preserve_vpc = False ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Copy all the pip dependencies required to run your code into a temporary # folder then add the handler file in the root of this directory. # Zip the contents of this folder into a single file and output to the dist # directory. path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) use_s3 = True s3_file = upload_s3 ( cfg , path_to_zip_file , use_s3 ) existing_config = get_function_config ( cfg ) if existing_config : update_function ( cfg , path_to_zip_file , existing_config , use_s3 = use_s3 , s3_file = s3_file , preserve_vpc = preserve_vpc ) else : create_function ( cfg , path_to_zip_file , use_s3 = use_s3 , s3_file = s3_file )
7387	def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group ] . index ( node )
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
6380	def sim_manhattan ( src , tar , qval = 2 , alphabet = None ) : return Manhattan ( ) . sim ( src , tar , qval , alphabet )
9453	def play ( self , call_params ) : path = '/' + self . api_version + '/Play/' method = 'POST' return self . request ( path , method , call_params )
10998	def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )
8199	def transform_from_local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
13845	def get_unique_pathname ( path , root = '' ) : path = os . path . join ( root , path ) # consider the path supplied, then the paths with numbers appended potentialPaths = itertools . chain ( ( path , ) , __get_numbered_paths ( path ) ) potentialPaths = six . moves . filterfalse ( os . path . exists , potentialPaths ) return next ( potentialPaths )
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : # ENTER-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
2246	def symlink ( real_path , link_path , overwrite = False , verbose = 0 ) : path = normpath ( real_path ) link = normpath ( link_path ) if not os . path . isabs ( path ) : # if path is not absolute it must be specified relative to link if _can_symlink ( ) : path = os . path . relpath ( path , os . path . dirname ( link ) ) else : # nocover # On windows, we need to use absolute paths path = os . path . abspath ( path ) if verbose : print ( 'Symlink: {path} -> {link}' . format ( path = path , link = link ) ) if islink ( link ) : if verbose : print ( '... already exists' ) pointed = _readlink ( link ) if pointed == path : if verbose > 1 : print ( '... and points to the right place' ) return link if verbose > 1 : if not exists ( link ) : print ( '... but it is broken and points somewhere else: {}' . format ( pointed ) ) else : print ( '... but it points somewhere else: {}' . format ( pointed ) ) if overwrite : util_io . delete ( link , verbose = verbose > 1 ) elif exists ( link ) : if _win32_links is None : if verbose : print ( '... already exists, but its a file. This will error.' ) raise FileExistsError ( 'cannot overwrite a physical path: "{}"' . format ( path ) ) else : # nocover if verbose : print ( '... already exists, and is either a file or hard link. ' 'Assuming it is a hard link. ' 'On non-win32 systems this would error.' ) if _win32_links is None : os . symlink ( path , link ) else : # nocover _win32_links . _symlink ( path , link , overwrite = overwrite , verbose = verbose ) return link
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
8336	def findPreviousSiblings ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousSiblingGenerator , * * kwargs )
502	def _labelToCategoryNumber ( self , label ) : if label not in self . saved_categories : self . saved_categories . append ( label ) return pow ( 2 , self . saved_categories . index ( label ) )
9954	def custom_showwarning ( message , category , filename = "" , lineno = - 1 , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None when run with pythonw.exe: # warnings get lost return text = "%s: %s\n" % ( category . __name__ , message ) try : file . write ( text ) except OSError : # the file (probably stderr) is invalid - this warning gets lost. pass
10846	def reorder ( self , updates_ids , offset = None , utc = None ) : url = PATHS [ 'REORDER' ] % self . profile_id order_format = "order[]=%s&" post_data = '' if offset : post_data += 'offset=%s&' % offset if utc : post_data += 'utc=%s&' % utc for update in updates_ids : post_data += order_format % update return self . api . post ( url = url , data = post_data )
7047	def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for task %s' % repr ( task [ 2 : ] ) ) return { 'power' : nparray ( [ npnan for x in range ( task [ 2 ] ) ] ) , 'bestperiod' : npnan , 'bestpower' : npnan , 'transdepth' : npnan , 'transduration' : npnan , 'transingressbin' : npnan , 'transegressbin' : npnan }
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
7894	def send_message ( self , body ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "groupchat" , body = body ) self . manager . stream . send ( m )
1275	def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
3943	async def _on_push_data ( self , data_bytes ) : logger . debug ( 'Received chunk:\n{}' . format ( data_bytes ) ) for chunk in self . _chunk_parser . get_chunks ( data_bytes ) : # Consider the channel connected once the first chunk is received. if not self . _is_connected : if self . _on_connect_called : self . _is_connected = True await self . on_reconnect . fire ( ) else : self . _on_connect_called = True self . _is_connected = True await self . on_connect . fire ( ) # chunk contains a container array container_array = json . loads ( chunk ) # container array is an array of inner arrays for inner_array in container_array : # inner_array always contains 2 elements, the array_id and the # data_array. array_id , data_array = inner_array logger . debug ( 'Chunk contains data array with id %r:\n%r' , array_id , data_array ) await self . on_receive_array . fire ( data_array )
2535	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True doc . comment = comment else : raise CardinalityError ( 'Document::Comment' )
9605	def fluent ( func ) : @ wraps ( func ) def fluent_interface ( instance , * args , * * kwargs ) : ret = func ( instance , * args , * * kwargs ) if ret is not None : return ret return instance return fluent_interface
10644	def Sh ( L : float , h : float , D : float ) -> float : return h * L / D
2644	def push_file ( self , source , dest_dir ) : local_dest = dest_dir + '/' + os . path . basename ( source ) # Only attempt to copy if the target dir and source dir are different if os . path . dirname ( source ) != dest_dir : try : shutil . copyfile ( source , local_dest ) os . chmod ( local_dest , 0o777 ) except OSError as e : raise FileCopyException ( e , self . hostname ) return local_dest
8537	def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False # have we seen this packet? if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : # Note: we only account for payload (i.e.: tcp data) self . _length += len ( ip_packet . data . data ) self . _remaining += len ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : # Useful hack to allow you to load projects as animations. adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
5763	def topological_order_packages ( packages ) : from catkin_pkg . topological_order import _PackageDecorator from catkin_pkg . topological_order import _sort_decorated_packages decorators_by_name = { } for path , package in packages . items ( ) : decorators_by_name [ package . name ] = _PackageDecorator ( package , path ) # calculate transitive dependencies for decorator in decorators_by_name . values ( ) : decorator . depends_for_topological_order = set ( [ ] ) all_depends = decorator . package . build_depends + decorator . package . buildtool_depends + decorator . package . run_depends + decorator . package . test_depends # skip external dependencies, meaning names that are not known packages unique_depend_names = set ( [ d . name for d in all_depends if d . name in decorators_by_name . keys ( ) ] ) for name in unique_depend_names : if name in decorator . depends_for_topological_order : # avoid function call to improve performance # check within the loop since the set changes every cycle continue decorators_by_name [ name ] . _add_recursive_run_depends ( decorators_by_name , decorator . depends_for_topological_order ) ordered_pkg_tuples = _sort_decorated_packages ( decorators_by_name ) for pkg_path , pkg in ordered_pkg_tuples : if pkg_path is None : raise RuntimeError ( 'Circular dependency in: %s' % pkg ) return ordered_pkg_tuples
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) # e.g., 'task_00009' ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) # e.g., 'results/task_00009/result.p.gz' return ret
6339	def lcsseq ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) # row 0 and column 0 are initialized to 0 already for i , src_char in enumerate ( src ) : for j , tar_char in enumerate ( tar ) : if src_char == tar_char : lengths [ i + 1 , j + 1 ] = lengths [ i , j ] + 1 else : lengths [ i + 1 , j + 1 ] = max ( lengths [ i + 1 , j ] , lengths [ i , j + 1 ] ) # read the substring out from the matrix result = '' i , j = len ( src ) , len ( tar ) while i != 0 and j != 0 : if lengths [ i , j ] == lengths [ i - 1 , j ] : i -= 1 elif lengths [ i , j ] == lengths [ i , j - 1 ] : j -= 1 else : result = src [ i - 1 ] + result i -= 1 j -= 1 return result
8272	def _load ( self , top = 5 , blue = "blue" , archive = None , member = None ) : if archive is None : path = os . path . join ( self . cache , self . name + ".xml" ) xml = open ( path ) . read ( ) else : assert member is not None xml = archive . read ( member ) dom = parseString ( xml ) . documentElement attr = lambda e , a : e . attributes [ a ] . value for e in dom . getElementsByTagName ( "color" ) [ : top ] : w = float ( attr ( e , "weight" ) ) try : rgb = e . getElementsByTagName ( "rgb" ) [ 0 ] clr = color ( float ( attr ( rgb , "r" ) ) , float ( attr ( rgb , "g" ) ) , float ( attr ( rgb , "b" ) ) , float ( attr ( rgb , "a" ) ) , mode = "rgb" ) try : clr . name = attr ( e , "name" ) if clr . name == "blue" : clr = color ( blue ) except : pass except : name = attr ( e , "name" ) if name == "blue" : name = blue clr = color ( name ) for s in e . getElementsByTagName ( "shade" ) : self . ranges . append ( ( clr , shade ( attr ( s , "name" ) ) , w * float ( attr ( s , "weight" ) ) ) )
13150	def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
7480	def sub_build_clustbits ( data , usort , nseeds ) : ## load FULL concat fasta file into a dict. This could cause RAM issues. ## this file has iupac codes in it, not ambigs resolved, and is gzipped. LOGGER . info ( "loading full _catcons file into memory" ) allcons = { } conshandle = os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) with gzip . open ( conshandle , 'rb' ) as iocons : cons = itertools . izip ( * [ iter ( iocons ) ] * 2 ) for namestr , seq in cons : nnn , sss = [ i . strip ( ) for i in namestr , seq ] allcons [ nnn [ 1 : ] ] = sss ## set optim to approximately 4 chunks per core. Smaller allows for a bit ## cleaner looking progress bar. 40 cores will make 160 files. optim = ( ( nseeds // ( data . cpus * 4 ) ) + ( nseeds % ( data . cpus * 4 ) ) ) LOGGER . info ( "building clustbits, optim=%s, nseeds=%s, cpus=%s" , optim , nseeds , data . cpus ) ## iterate through usort grabbing seeds and matches with open ( usort , 'rb' ) as insort : ## iterator, seed null, and seqlist null isort = iter ( insort ) loci = 0 lastseed = 0 fseqs = [ ] seqlist = [ ] seqsize = 0 while 1 : ## grab the next line try : hit , seed , ori = isort . next ( ) . strip ( ) . split ( ) except StopIteration : break try : ## if same seed, append match if seed != lastseed : ## store the last fseq, count it, and clear it if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 fseqs = [ ] ## occasionally write to file if seqsize >= optim : if seqlist : loci += seqsize with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : LOGGER . debug ( "writing chunk - seqsize {} loci {} {}" . format ( seqsize , loci , clustsout . name ) ) clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) ## reset list and counter seqlist = [ ] seqsize = 0 ## store the new seed on top of fseq fseqs . append ( ">{}\n{}" . format ( seed , allcons [ seed ] ) ) lastseed = seed ## add match to the seed seq = allcons [ hit ] ## revcomp if orientation is reversed if ori == "-" : seq = fullcomp ( seq ) [ : : - 1 ] fseqs . append ( ">{}\n{}" . format ( hit , seq ) ) except KeyError as inst : ## Caught bad seed or hit? Log and continue. LOGGER . error ( "Bad Seed/Hit: seqsize {}\tloci {}\tseed {}\thit {}" . format ( seqsize , loci , seed , hit ) ) ## write whatever is left over to the clusts file if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 loci += seqsize if seqlist : with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) ## final progress and cleanup del allcons clustbits = glob . glob ( os . path . join ( data . tmpdir , data . name + ".chunk_*" ) ) ## return stuff return clustbits , loci
6596	def receive_one ( self ) : if self . nruns == 0 : return None ret = self . communicationChannel . receive_one ( ) if ret is not None : self . nruns -= 1 return ret
1314	def ControlFromPoint2 ( x : int , y : int ) -> Control : return Control . CreateControlFromElement ( _AutomationClient . instance ( ) . IUIAutomation . ElementFromHandle ( WindowFromPoint ( x , y ) ) )
3321	def delete ( self , token ) : self . _lock . acquire_write ( ) try : lock = self . _dict . get ( token ) _logger . debug ( "delete {}" . format ( lock_string ( lock ) ) ) if lock is None : return False # Remove url to lock mapping key = "URL2TOKEN:{}" . format ( lock . get ( "root" ) ) if key in self . _dict : # _logger.debug(" delete token {} from url {}".format(token, lock.get("root"))) tokList = self . _dict [ key ] if len ( tokList ) > 1 : # Note: shelve dictionary returns copies, so we must # reassign values: tokList . remove ( token ) self . _dict [ key ] = tokList else : del self . _dict [ key ] # Remove the lock del self . _dict [ token ] self . _flush ( ) finally : self . _lock . release ( ) return True
11676	def fit ( self , X , y = None ) : self . features_ = as_features ( X , stack = True , bare = True ) # TODO: could precompute things like squared norms if kernel == "rbf". # Probably should add support to sklearn instead of hacking it here. return self
11384	def module ( self ) : # we have to guard this value because: # https://thingspython.wordpress.com/2010/09/27/another-super-wrinkle-raising-typeerror/ if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : # http://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path self . _module = imp . load_source ( 'captain_script' , self . path ) #self._module = imp.load_source(self.module_name, self.path) return self . _module
13225	async def process_ltd_doc_products ( session , product_urls , github_api_token , mongo_collection = None ) : tasks = [ asyncio . ensure_future ( process_ltd_doc ( session , github_api_token , product_url , mongo_collection = mongo_collection ) ) for product_url in product_urls ] await asyncio . gather ( * tasks )
5577	def load_input_reader ( input_params , readonly = False ) : logger . debug ( "find input reader with params %s" , input_params ) if not isinstance ( input_params , dict ) : raise TypeError ( "input_params must be a dictionary" ) if "abstract" in input_params : driver_name = input_params [ "abstract" ] [ "format" ] elif "path" in input_params : if os . path . splitext ( input_params [ "path" ] ) [ 1 ] : input_file = input_params [ "path" ] driver_name = driver_from_file ( input_file ) else : logger . debug ( "%s is a directory" , input_params [ "path" ] ) driver_name = "TileDirectory" else : raise MapcheteDriverError ( "invalid input parameters %s" % input_params ) for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "driver_name" ] == driver_name ) : return v . load ( ) . InputData ( input_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
8291	def _keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords
4054	def collections_sub ( self , collection , * * kwargs ) : query_string = "/{t}/{u}/collections/{c}/collections" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _build_query ( query_string )
13838	def ConsumeInt32 ( self ) : try : result = ParseInteger ( self . token , is_signed = True , is_long = False ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
4276	def get_albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get_albums ( subdir ) : yield subname , self . albums [ subdir ]
13857	def sendmsg ( self , message , recipient_mobiles = [ ] , url = 'http://services.ambientmobile.co.za/sms' , concatenate_message = True , message_id = str ( time ( ) ) . replace ( "." , "" ) , reply_path = None , allow_duplicates = True , allow_invalid_numbers = True , ) : if not recipient_mobiles or not ( isinstance ( recipient_mobiles , list ) or isinstance ( recipient_mobiles , tuple ) ) : raise AmbientSMSError ( "Missing recipients" ) if not message or not len ( message ) : raise AmbientSMSError ( "Missing message" ) postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXMLList . append ( "<recipients>%s</recipients>" % "" . join ( [ "<mobile>%s</mobile>" % m for m in recipient_mobiles ] ) ) postXMLList . append ( "<msg>%s</msg>" % message ) postXMLList . append ( "<concat>%s</concat>" % ( 1 if concatenate_message else 0 ) ) postXMLList . append ( "<message_id>%s</message_id>" % message_id ) postXMLList . append ( "<allow_duplicates>%s</allow_duplicates>" % ( 1 if allow_duplicates else 0 ) ) postXMLList . append ( "<allow_invalid_numbers>%s</allow_invalid_numbers>" % ( 1 if allow_invalid_numbers else 0 ) ) if reply_path : postXMLList . append ( "<reply_path>%s</reply_path>" % reply_path ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) status = result . get ( "status" , None ) if status and int ( status ) in [ 0 , 1 , 2 ] : return result else : raise AmbientSMSError ( int ( status ) )
107	def max_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . max , cval = cval , preserve_dtype = preserve_dtype )
4462	def transpose ( label , n_semitones ) : # Otherwise, split off the note from the modifier match = re . match ( six . text_type ( '(?P<note>[A-G][b#]*)(?P<mod>.*)' ) , six . text_type ( label ) ) if not match : return label note = match . group ( 'note' ) new_note = librosa . midi_to_note ( librosa . note_to_midi ( note ) + n_semitones , octave = False ) return new_note + match . group ( 'mod' )
6560	def _bqm_from_1sat ( constraint ) : configurations = constraint . configurations num_configurations = len ( configurations ) bqm = dimod . BinaryQuadraticModel . empty ( constraint . vartype ) if num_configurations == 1 : val , = next ( iter ( configurations ) ) v , = constraint . variables bqm . add_variable ( v , - 1 if val > 0 else + 1 , vartype = dimod . SPIN ) else : bqm . add_variables_from ( ( v , 0.0 ) for v in constraint . variables ) return bqm
9970	def _get_col_index ( name ) : index = string . ascii_uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col
10504	def removecallback ( window_name ) : if window_name in _pollEvents . _callback : del _pollEvents . _callback [ window_name ] return _remote_removecallback ( window_name )
11158	def mirror_to ( self , dst ) : # pragma: no cover self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : # pragma: no cover raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : # pragma: no cover pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
13721	def query ( self , wql ) : try : self . __wql = [ 'wmic' , '-U' , self . args . domain + '\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( "wql: {}" . format ( self . __wql ) ) self . __output = subprocess . check_output ( self . __wql ) self . logger . debug ( "output: {}" . format ( self . __output ) ) self . logger . debug ( "wmi connect succeed." ) self . __wmi_output = self . __output . splitlines ( ) [ 1 : ] self . logger . debug ( "wmi_output: {}" . format ( self . __wmi_output ) ) self . __csv_header = csv . DictReader ( self . __wmi_output , delimiter = '|' ) self . logger . debug ( "csv_header: {}" . format ( self . __csv_header ) ) return list ( self . __csv_header ) except subprocess . CalledProcessError as e : self . unknown ( "Connect by wmi and run wql error: %s" % e )
13374	def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
9159	def delete_license_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_license_requests ( cursor , uuid_ , posted_uids ) resp = request . response resp . status_int = 200 return resp
4045	def num_collectionitems ( self , collection ) : query = "/{t}/{u}/collections/{c}/items" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _totals ( query )
7153	def many ( prompt , * args , * * kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
680	def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values
13644	def append_arguments ( klass , sub_parsers , default_epilog , general_arguments ) : entry_name = hump_to_underscore ( klass . __name__ ) . replace ( '_component' , '' ) # set sub command document epilog = default_epilog if default_epilog else 'This tool generate by `cliez` ' 'https://www.github.com/wangwenpei/cliez' sub_parser = sub_parsers . add_parser ( entry_name , help = klass . __doc__ , epilog = epilog ) sub_parser . description = klass . add_arguments . __doc__ # add slot arguments if hasattr ( klass , 'add_slot_args' ) : slot_args = klass . add_slot_args ( ) or [ ] for v in slot_args : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) sub_parser . description = klass . add_slot_args . __doc__ pass user_arguments = klass . add_arguments ( ) or [ ] for v in user_arguments : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) if not klass . exclude_global_option : for v in general_arguments : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) return sub_parser
7417	def update ( assembly , idict , count ) : data = iter ( open ( os . path . join ( assembly . dirs . outfiles , assembly . name + ".phy" ) , 'r' ) ) ntax , nchar = data . next ( ) . strip ( ) . split ( ) ## read in max N bp at a time for line in data : tax , seq = line . strip ( ) . split ( ) idict [ tax ] = idict [ tax ] [ 100000 : ] idict [ tax ] += seq [ count : count + 100000 ] del line return idict
3073	def _load_config ( self , client_secrets_file , client_id , client_secret ) : if client_id and client_secret : self . client_id , self . client_secret = client_id , client_secret return if client_secrets_file : self . _load_client_secrets ( client_secrets_file ) return if 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' in self . app . config : self . _load_client_secrets ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' ] ) return try : self . client_id , self . client_secret = ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_ID' ] , self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRET' ] ) except KeyError : raise ValueError ( 'OAuth2 configuration could not be found. Either specify the ' 'client_secrets_file or client_id and client_secret or set ' 'the app configuration variables ' 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE or ' 'GOOGLE_OAUTH2_CLIENT_ID and GOOGLE_OAUTH2_CLIENT_SECRET.' )
1155	def pop ( self ) : it = iter ( self ) try : value = next ( it ) except StopIteration : raise KeyError self . discard ( value ) return value
753	def _translateMetricsToJSON ( self , metrics , label ) : # Transcode the MetricValueElement values into JSON-compatible # structure metricsDict = metrics # Convert the structure to a display-friendly JSON string def _mapNumpyValues ( obj ) : """ """ import numpy if isinstance ( obj , numpy . float32 ) : return float ( obj ) elif isinstance ( obj , numpy . bool_ ) : return bool ( obj ) elif isinstance ( obj , numpy . ndarray ) : return obj . tolist ( ) else : raise TypeError ( "UNEXPECTED OBJ: %s; class=%s" % ( obj , obj . __class__ ) ) jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) return jsonString
7815	def run ( self ) : if self . args . roster_cache and os . path . exists ( self . args . roster_cache ) : logging . info ( u"Loading roster from {0!r}" . format ( self . args . roster_cache ) ) try : self . client . roster_client . load_roster ( self . args . roster_cache ) except ( IOError , ValueError ) , err : logging . error ( u"Could not load the roster: {0!r}" . format ( err ) ) self . client . connect ( ) self . client . run ( )
3086	def _is_ndb ( self ) : # issubclass will fail if one of the arguments is not a class, only # need worry about new-style classes since ndb and db models are # new-style if isinstance ( self . _model , type ) : if _NDB_MODEL is not None and issubclass ( self . _model , _NDB_MODEL ) : return True elif issubclass ( self . _model , db . Model ) : return False raise TypeError ( 'Model class not an NDB or DB model: {0}.' . format ( self . _model ) )
1648	def CheckAltTokens ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Avoid preprocessor lines if Match ( r'^\s*#' , line ) : return # Last ditch effort to avoid multi-line comments. This will not help # if the comment started before the current line or ended after the # current line, but it catches most of the false positives. At least, # it provides a way to workaround this warning for people who use # multi-line comments in preprocessor macros. # # TODO(unknown): remove this once cpplint has better support for # multi-line comments. if line . find ( '/*' ) >= 0 or line . find ( '*/' ) >= 0 : return for match in _ALT_TOKEN_REPLACEMENT_PATTERN . finditer ( line ) : error ( filename , linenum , 'readability/alt_tokens' , 2 , 'Use operator %s instead of %s' % ( _ALT_TOKEN_REPLACEMENT [ match . group ( 1 ) ] , match . group ( 1 ) ) )
6304	def find_effect_class ( self , path ) -> Type [ Effect ] : package_name , class_name = parse_package_string ( path ) if package_name : package = self . get_package ( package_name ) return package . find_effect_class ( class_name , raise_for_error = True ) for package in self . packages : effect_cls = package . find_effect_class ( class_name ) if effect_cls : return effect_cls raise EffectError ( "No effect class '{}' found in any packages" . format ( class_name ) )
1594	def choose_tasks ( self , stream_id , values ) : if stream_id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream_id ] : ret . extend ( target . choose_tasks ( values ) ) return ret
6586	def retries ( max_tries , exceptions = ( Exception , ) ) : def decorator ( func ) : def function ( * args , * * kwargs ) : retries_left = max_tries while retries_left > 0 : try : retries_left -= 1 return func ( * args , * * kwargs ) except exceptions as exc : # Don't retry for PandoraExceptions - unlikely that result # will change for same set of input parameters. if isinstance ( exc , PandoraException ) : raise if retries_left > 0 : time . sleep ( delay_exponential ( 0.5 , 2 , max_tries - retries_left ) ) else : raise return function return decorator
9280	def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
7980	def auth_timeout ( self ) : self . lock . acquire ( ) try : self . __logger . debug ( "Timeout while waiting for jabber:iq:auth result" ) if self . _auth_methods_left : self . _auth_methods_left . pop ( 0 ) finally : self . lock . release ( )
5487	def send_payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service_name , 'params' : params , 'id' : text_type ( uuid . uuid4 ( ) ) } ) data_binary = data . encode ( 'utf-8' ) url_request = Request ( self . service_url , data_binary , headers = self . headers ) return urlopen ( url_request ) . read ( )
7770	def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . settings ) session_handler = SessionHandler ( ) binding_handler = ResourceBindingHandler ( self . settings ) return [ tls_handler , sasl_handler , binding_handler , session_handler ]
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
4903	def link_to_modal ( link_text , index , autoescape = True ) : # pylint: disable=unused-argument link = ( '<a' ' href="#!"' ' class="text-underline view-course-details-link"' ' id="view-course-details-link-{index}"' ' data-toggle="modal"' ' data-target="#course-details-modal-{index}"' '>{link_text}</a>' ) . format ( index = index , link_text = link_text , ) return mark_safe ( link )
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
8899	def _dequeue_into_store ( transfersession ) : with connection . cursor ( ) as cursor : DBBackend . _dequeuing_delete_rmcb_records ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_buffered_records ( cursor , transfersession . id ) current_id = InstanceIDModel . get_current_instance_and_increment_counter ( ) DBBackend . _dequeuing_merge_conflict_buffer ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_merge_conflict_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_update_rmcs_last_saved_by ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_delete_mc_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_mc_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_buffer ( cursor , transfersession . id ) if getattr ( settings , 'MORANGO_DESERIALIZE_AFTER_DEQUEUING' , True ) : _deserialize_from_store ( transfersession . sync_session . profile )
5662	def return_segments ( shape , break_points ) : # print 'xxx' # print stops # print shape # print break_points # assert len(stops) == len(break_points) segs = [ ] bp = 0 # not used bp2 = 0 for i in range ( len ( break_points ) - 1 ) : bp = break_points [ i ] if break_points [ i ] is not None else bp2 bp2 = break_points [ i + 1 ] if break_points [ i + 1 ] is not None else bp segs . append ( shape [ bp : bp2 + 1 ] ) segs . append ( [ ] ) return segs
8003	def get_form ( self , form_type = "form" ) : if self . form : if self . form . type != form_type : raise ValueError ( "Bad form type in the jabber:iq:register element" ) return self . form form = Form ( form_type , instructions = self . instructions ) form . add_field ( "FORM_TYPE" , [ u"jabber:iq:register" ] , "hidden" ) for field in legacy_fields : field_type , field_label = legacy_fields [ field ] value = getattr ( self , field ) if value is None : continue if form_type == "form" : if not value : value = None form . add_field ( name = field , field_type = field_type , label = field_label , value = value , required = True ) else : form . add_field ( name = field , value = value ) return form
8885	def fit ( self , x , y = None ) : if self . _dtype is not None : iter2array ( x , dtype = self . _dtype ) else : iter2array ( x ) return self
11555	def enable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
7862	def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
10917	def get_residuals_update_tile ( st , padded_tile ) : inner_tile = st . ishape . intersection ( [ st . ishape , padded_tile ] ) return inner_tile . translate ( - st . pad )
1937	def get_abi ( self , hsh : bytes ) -> Dict [ str , Any ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) if sig is not None : return dict ( self . _function_abi_items_by_signature [ sig ] ) item = self . _fallback_function_abi_item if item is not None : return dict ( item ) # An item describing the default fallback function. return { 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'fallback' }
8205	def flush ( self , frame ) : self . sink . render ( self . size_or_default ( ) , frame , self . _drawqueue ) self . reset_drawqueue ( )
2558	def clean_pair ( cls , attribute , value ) : attribute = cls . clean_attribute ( attribute ) # Check for boolean attributes # (i.e. selected=True becomes selected="selected") if value is True : value = attribute if value is False : value = "false" return ( attribute , value )
9433	def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the file header if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
4087	def asyncClose ( fn ) : @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : f = asyncio . ensure_future ( fn ( * args , * * kwargs ) ) while not f . done ( ) : QApplication . instance ( ) . processEvents ( ) return wrapper
7596	def search_tournaments ( self , * * params : keys ) : url = self . api . TOURNAMENT + '/search' return self . _get_model ( url , PartialClan , * * params )
11513	def move_item ( self , token , item_id , src_folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'srcfolderid' ] = src_folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.move' , parameters ) return response
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : # code out of range return L_ANY if ( code & ( code - 1 ) ) != 0 : # choice was more than one language; use any return L_ANY return code
11646	def transform ( self , X ) : n = self . flip_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( self . flip_ . shape [ 0 ] ) ) return np . dot ( X , self . flip_ )
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
3017	def from_json_keyfile_dict ( cls , keyfile_dict , scopes = '' , token_uri = None , revoke_uri = None ) : return cls . _from_parsed_json_keyfile ( keyfile_dict , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) # note that we don't use safe_greenlets.spawn because we take care of it in _loop by ourselves self . _greenlet = gevent . spawn ( self . _loop )
9352	def street_number ( ) : length = int ( random . choice ( string . digits [ 1 : 6 ] ) ) return '' . join ( random . sample ( string . digits , length ) )
6830	def get_logs_between_commits ( self , a , b ) : print ( 'REAL' ) ret = self . local ( 'git --no-pager log --pretty=oneline %s...%s' % ( a , b ) , capture = True ) if self . verbose : print ( ret ) return str ( ret )
558	def bestModelInSprint ( self , sprintIdx ) : # Get all the swarms in this sprint swarms = self . getAllSwarms ( sprintIdx ) # Get the best model and score from each swarm bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )
10578	def get_element_masses ( self ) : result = [ 0 ] * len ( self . material . elements ) for compound in self . material . compounds : c = self . get_compound_mass ( compound ) f = [ c * x for x in emf ( compound , self . material . elements ) ] result = [ v + f [ ix ] for ix , v in enumerate ( result ) ] return result
8788	def get ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue return None
11216	def valid ( self , time : int = None ) -> bool : if time is None : epoch = datetime ( 1970 , 1 , 1 , 0 , 0 , 0 ) now = datetime . utcnow ( ) time = int ( ( now - epoch ) . total_seconds ( ) ) if isinstance ( self . valid_from , int ) and time < self . valid_from : return False if isinstance ( self . valid_to , int ) and time > self . valid_to : return False return True
12988	def remote_jupyter_proxy_url ( port ) : base_url = os . environ [ 'EXTERNAL_URL' ] host = urllib . parse . urlparse ( base_url ) . netloc # If port is None we're asking for the URL origin # so return the public hostname. if port is None : return host service_url_path = os . environ [ 'JUPYTERHUB_SERVICE_PREFIX' ] proxy_url_path = 'proxy/%d' % port user_url = urllib . parse . urljoin ( base_url , service_url_path ) full_url = urllib . parse . urljoin ( user_url , proxy_url_path ) return full_url
10347	def run_rcr ( graph , tag = 'dgxp' ) : # Step 1: Calculate the hypothesis subnetworks (just simple star graphs) hypotheses = defaultdict ( set ) increases = defaultdict ( set ) decreases = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : hypotheses [ u ] . add ( v ) if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : increases [ u ] . add ( v ) elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : decreases [ u ] . add ( v ) # Step 2: Calculate the matching of the data points to the causal relationships #: A dictionary from {tuple controller node: int count of correctly matching observations} correct = defaultdict ( int ) #: A dictionary from {tuple controller node: int count of incorrectly matching observations} contra = defaultdict ( int ) #: A dictionary from {tuple controller node: int count of ambiguous observations} ambiguous = defaultdict ( int ) #: A dictionary from {tuple controller node: int count of missing obvservations} missing = defaultdict ( int ) for controller , downstream_nodes in hypotheses . items ( ) : if len ( downstream_nodes ) < 4 : continue # need enough data to make reasonable calculations! for node in downstream_nodes : if node in increases [ controller ] and node in decreases [ controller ] : ambiguous [ controller ] += 1 elif node in increases [ controller ] : if graph . node [ node ] [ tag ] == 1 : correct [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : contra [ controller ] += 1 elif node in decreases [ controller ] : if graph . node [ node ] [ tag ] == 1 : contra [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : correct [ controller ] += 1 else : missing [ controller ] += 1 # Step 3: Keep only controller nodes who have 4 or more downstream nodes controllers = { controller for controller , downstream_nodes in hypotheses . items ( ) if 4 <= len ( downstream_nodes ) } # Step 4: Calculate concordance scores concordance_scores = { controller : scipy . stats . beta ( 0.5 , correct [ controller ] , contra [ controller ] ) for controller in controllers } # Step 5: Calculate richness scores # TODO # Calculate the population as the union of all downstream nodes for all controllers population = { node for controller in controllers for node in hypotheses [ controller ] } population_size = len ( population ) # Step 6: Export return pandas . DataFrame ( { 'contra' : contra , 'correct' : correct , 'concordance' : concordance_scores } )
9036	def walk_rows ( self , mapping = identity ) : row_in_grid = self . _walk . row_in_grid return map ( lambda row : mapping ( row_in_grid ( row ) ) , self . _rows )
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
7228	def paint ( self ) : snippet = { 'fill-extrusion-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-extrusion-color' : VectorStyle . get_style_value ( self . color ) , 'fill-extrusion-base' : VectorStyle . get_style_value ( self . base ) , 'fill-extrusion-height' : VectorStyle . get_style_value ( self . height ) } if self . translate : snippet [ 'fill-extrusion-translate' ] = self . translate return snippet
4021	def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
8271	def _save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + ".xml" ) f = open ( path , "w" ) f . write ( self . xml ) f . close ( )
3117	def oauth2_authorize ( request ) : return_url = request . GET . get ( 'return_url' , None ) if not return_url : return_url = request . META . get ( 'HTTP_REFERER' , '/' ) scopes = request . GET . getlist ( 'scopes' , django_util . oauth2_settings . scopes ) # Model storage (but not session storage) requires a logged in user if django_util . oauth2_settings . storage_model : if not request . user . is_authenticated ( ) : return redirect ( '{0}?next={1}' . format ( settings . LOGIN_URL , parse . quote ( request . get_full_path ( ) ) ) ) # This checks for the case where we ended up here because of a logged # out user but we had credentials for it in the first place else : user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) if user_oauth . has_credentials ( ) : return redirect ( return_url ) flow = _make_flow ( request = request , scopes = scopes , return_url = return_url ) auth_url = flow . step1_get_authorize_url ( ) return shortcuts . redirect ( auth_url )
7375	def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator
8380	def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y # A dashed line indicates the drag vector. s = self . graph . styles . default self . _ctx . nofill ( ) self . _ctx . nostroke ( ) if s . stroke : self . _ctx . strokewidth ( s . strokewidth ) self . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . _ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . _nsBezierPath . setLineDash_count_phase_ ( [ 2 , 4 ] , 2 , 50 ) except : pass self . _ctx . drawpath ( p ) r = node . __class__ ( None ) . r * 0.75 self . _ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d
2529	def parse ( self , fil ) : self . error = False self . graph = Graph ( ) self . graph . parse ( file = fil , format = 'xml' ) self . doc = document . Document ( ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'SpdxDocument' ] ) ) : self . parse_doc_fields ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'ExternalDocumentRef' ] ) ) : self . parse_ext_doc_ref ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'CreationInfo' ] ) ) : self . parse_creation_info ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'Package' ] ) ) : self . parse_package ( s ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'referencesFile' ] , None ) ) : self . parse_file ( o ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'reviewed' ] , None ) ) : self . parse_review ( o ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'annotation' ] , None ) ) : self . parse_annotation ( o ) validation_messages = [ ] # Report extra errors if self.error is False otherwise there will be # redundent messages validation_messages = self . doc . validate ( validation_messages ) if not self . error : if validation_messages : for msg in validation_messages : self . logger . log ( msg ) self . error = True return self . doc , self . error
6964	def get ( self ) : # generate the project's list of checkplots project_checkplots = self . currentproject [ 'checkplots' ] project_checkplotbasenames = [ os . path . basename ( x ) for x in project_checkplots ] project_checkplotindices = range ( len ( project_checkplots ) ) # get the sortkey and order project_cpsortkey = self . currentproject [ 'sortkey' ] if self . currentproject [ 'sortorder' ] == 'asc' : project_cpsortorder = 'ascending' elif self . currentproject [ 'sortorder' ] == 'desc' : project_cpsortorder = 'descending' # get the filterkey and condition project_cpfilterstatements = self . currentproject [ 'filterstatements' ] self . render ( 'cpindex.html' , project_checkplots = project_checkplots , project_cpsortorder = project_cpsortorder , project_cpsortkey = project_cpsortkey , project_cpfilterstatements = project_cpfilterstatements , project_checkplotbasenames = project_checkplotbasenames , project_checkplotindices = project_checkplotindices , project_checkplotfile = self . cplistfile , readonly = self . readonly , baseurl = self . baseurl )
5964	def topology ( struct = None , protein = 'protein' , top = 'system.top' , dirname = 'top' , posres = "posres.itp" , ff = "oplsaa" , water = "tip4p" , * * pdb2gmx_args ) : structure = realpath ( struct ) new_struct = protein + '.pdb' if posres is None : posres = protein + '_posres.itp' pdb2gmx_args . update ( { 'f' : structure , 'o' : new_struct , 'p' : top , 'i' : posres , 'ff' : ff , 'water' : water } ) with in_dir ( dirname ) : logger . info ( "[{dirname!s}] Building topology {top!r} from struct = {struct!r}" . format ( * * vars ( ) ) ) # perhaps parse output from pdb2gmx 4.5.x to get the names of the chain itp files? gromacs . pdb2gmx ( * * pdb2gmx_args ) return { 'top' : realpath ( dirname , top ) , 'struct' : realpath ( dirname , new_struct ) , 'posres' : realpath ( dirname , posres ) }
1080	def isocalendar ( self ) : year = self . _year week1monday = _isoweek1monday ( year ) today = _ymd2ord ( self . _year , self . _month , self . _day ) # Internally, week and day have origin 0 week , day = divmod ( today - week1monday , 7 ) if week < 0 : year -= 1 week1monday = _isoweek1monday ( year ) week , day = divmod ( today - week1monday , 7 ) elif week >= 52 : if today >= _isoweek1monday ( year + 1 ) : year += 1 week = 0 return year , week + 1 , day + 1
1452	def update ( self , key , value ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer ) self . value [ key ] . update ( value )
1864	def SARX ( cpu , dest , src , count ) : OperandSize = dest . size count = count . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = count & countMask tempDest = value = src . read ( ) sign = value & ( 1 << ( OperandSize - 1 ) ) while tempCount != 0 : cpu . CF = ( value & 0x1 ) != 0 # LSB value = ( value >> 1 ) | sign tempCount = tempCount - 1 res = dest . write ( value )
9724	async def load_project ( self , project_path ) : cmd = "loadproject %s" % project_path return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
12410	def all_package_versions ( package ) : info = PyPI . package_info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
8927	def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] # Automatically create wheels if possible if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
9909	def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) # The transaction is atomic so there is never a gap where a user # has no primary email address. with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
8583	def get_attached_volume ( self , datacenter_id , server_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes/%s' % ( datacenter_id , server_id , volume_id ) ) return response
13603	def error_message ( self , message , fh = None , prefix = "[error]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stderr if fh is sys . stderr : termcolor . cprint ( msg , color = "red" ) else : fh . write ( msg ) pass
2711	def json_iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )
5034	def get_enterprise_customer_user_queryset ( self , request , search_keyword , customer_uuid , page_size = PAGE_SIZE ) : page = request . GET . get ( 'page' , 1 ) learners = EnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) user_ids = learners . values_list ( 'user_id' , flat = True ) matching_users = User . objects . filter ( pk__in = user_ids ) if search_keyword is not None : matching_users = matching_users . filter ( Q ( email__icontains = search_keyword ) | Q ( username__icontains = search_keyword ) ) matching_user_ids = matching_users . values_list ( 'pk' , flat = True ) learners = learners . filter ( user_id__in = matching_user_ids ) return paginated_list ( learners , page , page_size )
1742	def make_grid ( tensor , nrow = 8 , padding = 2 , pad_value = 0 ) : if not ( isinstance ( tensor , np . ndarray ) or ( isinstance ( tensor , list ) and all ( isinstance ( t , np . ndarray ) for t in tensor ) ) ) : raise TypeError ( 'tensor or list of tensors expected, got {}' . format ( type ( tensor ) ) ) # if list of tensors, convert to a 4D mini-batch Tensor if isinstance ( tensor , list ) : tensor = np . stack ( tensor , 0 ) if tensor . ndim == 2 : # single image H x W tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] ) ) if tensor . ndim == 3 : if tensor . shape [ 0 ] == 1 : # if single-channel, single image, convert to 3-channel tensor = np . concatenate ( ( tensor , tensor , tensor ) , 0 ) tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] , tensor . shape [ 2 ] ) ) if tensor . ndim == 4 and tensor . shape [ 1 ] == 1 : # single-channel images tensor = np . concatenate ( ( tensor , tensor , tensor ) , 1 ) if tensor . shape [ 0 ] == 1 : return np . squeeze ( tensor ) # make the mini-batch of images into a grid nmaps = tensor . shape [ 0 ] xmaps = min ( nrow , nmaps ) ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) height , width = int ( tensor . shape [ 2 ] + padding ) , int ( tensor . shape [ 3 ] + padding ) grid = np . ones ( ( 3 , height * ymaps + padding , width * xmaps + padding ) ) * pad_value k = 0 for y in range ( ymaps ) : for x in range ( xmaps ) : if k >= nmaps : break grid [ : , y * height + padding : ( y + 1 ) * height , x * width + padding : ( x + 1 ) * width ] = tensor [ k ] k = k + 1 return grid
5134	def generate_pagerank_graph ( num_vertices = 250 , * * kwargs ) : g = minimal_random_graph ( num_vertices , * * kwargs ) r = np . zeros ( num_vertices ) for k , pr in nx . pagerank ( g ) . items ( ) : r [ k ] = pr g = set_types_rank ( g , rank = r , * * kwargs ) return g
4138	def scale_image ( in_fname , out_fname , max_width , max_height ) : # local import to avoid testing dependency on PIL: try : from PIL import Image except ImportError : import Image img = Image . open ( in_fname ) width_in , height_in = img . size scale_w = max_width / float ( width_in ) scale_h = max_height / float ( height_in ) if height_in * scale_w <= max_height : scale = scale_w else : scale = scale_h if scale >= 1.0 and in_fname == out_fname : return width_sc = int ( round ( scale * width_in ) ) height_sc = int ( round ( scale * height_in ) ) # resize the image img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) # insert centered thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) thumb . paste ( img , pos_insert ) thumb . save ( out_fname ) # Use optipng to perform lossless compression on the resized image if # software is installed if os . environ . get ( 'SKLEARN_DOC_OPTIPNG' , False ) : try : subprocess . call ( [ "optipng" , "-quiet" , "-o" , "9" , out_fname ] ) except Exception : warnings . warn ( 'Install optipng to reduce the size of the \ generated images' )
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
8440	def setup ( template , version = None ) : temple . check . is_git_ssh_path ( template ) temple . check . not_in_git_repo ( ) repo_path = temple . utils . get_repo_path ( template ) msg = ( 'You will be prompted for the parameters of your new project.' ' Please read the docs at https://github.com/{} before entering parameters.' ) . format ( repo_path ) print ( msg ) cc_repo_dir , config = temple . utils . get_cookiecutter_config ( template , version = version ) if not version : with temple . utils . cd ( cc_repo_dir ) : ret = temple . utils . shell ( 'git rev-parse HEAD' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) _generate_files ( repo_dir = cc_repo_dir , config = config , template = template , version = version )
6177	def reduce_chunk ( func , array ) : res = [ ] for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )
3362	def to_yaml ( model , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ "version" ] = YAML_SPEC return yaml . dump ( obj , * * kwargs )
4749	def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
8221	def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
2055	def ADDW ( cpu , dest , src , add ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc if src . type == 'register' and src . reg in ( 'PC' , 'R15' ) : src = aligned_pc else : src = src . read ( ) dest . write ( src + add . read ( ) )
4186	def window_taylor ( N , nbar = 4 , sll = - 30 ) : B = 10 ** ( - sll / 20 ) A = log ( B + sqrt ( B ** 2 - 1 ) ) / pi s2 = nbar ** 2 / ( A ** 2 + ( nbar - 0.5 ) ** 2 ) ma = arange ( 1 , nbar ) def calc_Fm ( m ) : numer = ( - 1 ) ** ( m + 1 ) * prod ( 1 - m ** 2 / s2 / ( A ** 2 + ( ma - 0.5 ) ** 2 ) ) denom = 2 * prod ( [ 1 - m ** 2 / j ** 2 for j in ma if j != m ] ) return numer / denom Fm = array ( [ calc_Fm ( m ) for m in ma ] ) def W ( n ) : return 2 * np . sum ( Fm * cos ( 2 * pi * ma * ( n - N / 2 + 1 / 2 ) / N ) ) + 1 w = array ( [ W ( n ) for n in range ( N ) ] ) # normalize (Note that this is not described in the original text) scale = W ( ( N - 1 ) / 2 ) w /= scale return w
5048	def delete ( self , request , customer_uuid ) : # TODO: pylint acts stupid - find a way around it without suppressing enterprise_customer = EnterpriseCustomer . objects . get ( uuid = customer_uuid ) # pylint: disable=no-member email_to_unlink = request . GET [ "unlink_email" ] try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = email_to_unlink ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : message = _ ( "Email {email} is not associated with Enterprise " "Customer {ec_name}" ) . format ( email = email_to_unlink , ec_name = enterprise_customer . name ) return HttpResponse ( message , content_type = "application/json" , status = 404 ) return HttpResponse ( json . dumps ( { } ) , content_type = "application/json" )
5027	def get_requirements ( requirements_file ) : lines = open ( requirements_file ) . readlines ( ) dependencies = [ ] dependency_links = [ ] for line in lines : package = line . strip ( ) if package . startswith ( '#' ) : # Skip pure comment lines continue if any ( package . startswith ( prefix ) for prefix in VCS_PREFIXES ) : # VCS reference for dev purposes, expect a trailing comment # with the normal requirement package_link , __ , package = package . rpartition ( '#' ) # Remove -e <version_control> string package_link = re . sub ( r'(.*)(?P<dependency_link>https?.*$)' , r'\g<dependency_link>' , package_link ) package = re . sub ( r'(egg=)?(?P<package_name>.*)==.*$' , r'\g<package_name>' , package ) package_version = re . sub ( r'.*[^=]==' , '' , line . strip ( ) ) if package : dependency_links . append ( '{package_link}#egg={package}-{package_version}' . format ( package_link = package_link , package = package , package_version = package_version , ) ) else : # Ignore any trailing comment package , __ , __ = package . partition ( '#' ) # Remove any whitespace and assume non-empty results are dependencies package = package . strip ( ) if package : dependencies . append ( package ) return dependencies , dependency_links
6394	def sim_levenshtein ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 1 , 1 ) ) : return Levenshtein ( ) . sim ( src , tar , mode , cost )
9486	def ensure_instruction ( instruction : int ) -> bytes : if PY36 : return instruction . to_bytes ( 2 , byteorder = "little" ) else : return instruction . to_bytes ( 1 , byteorder = "little" )
13590	def sigma_prime ( self ) : return _np . sqrt ( self . emit / self . beta ( self . E ) )
11804	def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : # Remove old val if there was one self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
2143	def ordered_dump ( data , Dumper = yaml . Dumper , * * kws ) : class OrderedDumper ( Dumper ) : pass def _dict_representer ( dumper , data ) : return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items ( ) ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , None , OrderedDumper , * * kws )
9208	def remove_prefix ( bytes_ ) : prefix_int = extract_prefix ( bytes_ ) prefix = varint . encode ( prefix_int ) return bytes_ [ len ( prefix ) : ]
9780	def build ( ctx , project , build ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
13060	def get_reffs ( self , objectId , subreference = None , collection = None , export_collection = False ) : if collection is not None : text = collection else : text = self . get_collection ( objectId ) reffs = self . chunk ( text , lambda level : self . resolver . getReffs ( objectId , level = level , subreference = subreference ) ) if export_collection is True : return text , reffs return reffs
2928	def write_to_package_zip ( self , filename , data ) : self . manifest [ filename ] = md5hash ( data ) self . package_zip . writestr ( filename , data )
13723	def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
1917	def fork ( self , state , expression , policy = 'ALL' , setstate = None ) : assert isinstance ( expression , Expression ) if setstate is None : setstate = lambda x , y : None # Find a set of solutions for expression solutions = state . concretize ( expression , policy ) if not solutions : raise ExecutorError ( "Forking on unfeasible constraint set" ) if len ( solutions ) == 1 : setstate ( state , solutions [ 0 ] ) return state logger . info ( "Forking. Policy: %s. Values: %s" , policy , ', ' . join ( f'0x{sol:x}' for sol in solutions ) ) self . _publish ( 'will_fork_state' , state , expression , solutions , policy ) # Build and enqueue a state for each solution children = [ ] for new_value in solutions : with state as new_state : new_state . constrain ( expression == new_value ) # and set the PC of the new state to the concrete pc-dest #(or other register or memory address to concrete) setstate ( new_state , new_value ) self . _publish ( 'did_fork_state' , new_state , expression , new_value , policy ) # enqueue new_state state_id = self . enqueue ( new_state ) # maintain a list of children for logging purpose children . append ( state_id ) logger . info ( "Forking current state into states %r" , children ) return None
8544	def get_datacenter ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
7998	def set_authenticated ( self , me , restart_stream = False ) : with self . lock : self . authenticated = True self . me = me if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . me ) )
5017	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
647	def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) seqList = [ ] for i in xrange ( nSeq ) : if max ( seqLength ) <= nCoinc : seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) else : len = random . choice ( seqLength ) seq = [ ] for x in xrange ( len ) : seq . append ( random . choice ( coincList ) ) seqList . append ( seq ) return seqList
3865	async def leave ( self ) : is_group_conversation = ( self . _conversation . type == hangouts_pb2 . CONVERSATION_TYPE_GROUP ) try : if is_group_conversation : await self . _client . remove_user ( hangouts_pb2 . RemoveUserRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , ) ) else : await self . _client . delete_conversation ( hangouts_pb2 . DeleteConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , delete_upper_bound_timestamp = parsers . to_timestamp ( datetime . datetime . now ( tz = datetime . timezone . utc ) ) ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to leave conversation: {}' . format ( e ) ) raise
9174	def bake ( binder , recipe_id , publisher , message , cursor ) : recipe = _get_recipe ( recipe_id , cursor ) includes = _formatter_callback_factory ( ) binder = collate_models ( binder , ruleset = recipe , includes = includes ) def flatten_filter ( model ) : return ( isinstance ( model , cnxepub . CompositeDocument ) or ( isinstance ( model , cnxepub . Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) def only_documents_filter ( model ) : return isinstance ( model , cnxepub . Document ) and not isinstance ( model , cnxepub . CompositeDocument ) for doc in cnxepub . flatten_to ( binder , flatten_filter ) : publish_composite_model ( cursor , doc , binder , publisher , message ) for doc in cnxepub . flatten_to ( binder , only_documents_filter ) : publish_collated_document ( cursor , doc , binder ) tree = cnxepub . model_to_tree ( binder ) publish_collated_tree ( cursor , tree ) return [ ]
2735	def load ( self ) : data = self . get_data ( 'floating_ips/%s' % self . ip , type = GET ) floating_ip = data [ 'floating_ip' ] # Setting the attribute values for attr in floating_ip . keys ( ) : setattr ( self , attr , floating_ip [ attr ] ) return self
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
6585	def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD_MAP [ input ] [ 1 ] ) except ( IndexError , KeyError ) : return self . screen . print_error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
9476	def add_edge ( self , n1_label , n2_label , directed = False ) : n1 = self . add_node ( n1_label ) n2 = self . add_node ( n2_label ) e = Edge ( n1 , n2 , directed ) self . _edges . append ( e ) return e
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) #TODO:explicitly delete all subfolders, star-delete doesn't work r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
11997	def verify_signature ( self , data ) : data = self . _remove_magic ( data ) data = urlsafe_nopadding_b64decode ( data ) options = self . _read_header ( data ) data = self . _add_magic ( data ) self . _unsign_data ( data , options )
5747	def date_asn_block ( self , ip , announce_date = None ) : assignations , announce_date , keys = self . run ( ip , announce_date ) pos = next ( ( i for i , j in enumerate ( assignations ) if j is not None ) , None ) if pos is not None : block = keys [ pos ] if block != '0.0.0.0/0' : return announce_date , assignations [ pos ] , block return None
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
1713	def ConstructObject ( self , py_obj ) : obj = self . NewObject ( ) for k , v in py_obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
9561	def _apply_check_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
2399	def initialize_dictionaries ( self , e_set , max_feats2 = 200 ) : if ( hasattr ( e_set , '_type' ) ) : if ( e_set . _type == "train" ) : #normal text (unstemmed) useful words/bigrams nvocab = util_functions . get_vocab ( e_set . _text , e_set . _score , max_feats2 = max_feats2 ) #stemmed and spell corrected vocab useful words/ngrams svocab = util_functions . get_vocab ( e_set . _clean_stem_text , e_set . _score , max_feats2 = max_feats2 ) #dictionary trained on proper vocab self . _normal_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = nvocab ) #dictionary trained on proper vocab self . _stem_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = svocab ) self . dict_initialized = True #Average spelling errors in set. needed later for spelling detection self . _mean_spelling_errors = sum ( e_set . _spelling_errors ) / float ( len ( e_set . _spelling_errors ) ) self . _spell_errors_per_character = sum ( e_set . _spelling_errors ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) #Gets the number and positions of grammar errors good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) self . _grammar_errors_per_character = ( sum ( good_pos_tags ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ) #Generate bag of words features bag_feats = self . gen_bag_feats ( e_set ) #Sum of a row of bag of words features (topical words in an essay) f_row_sum = numpy . sum ( bag_feats [ : , : ] ) #Average index of how "topical" essays are self . _mean_f_prop = f_row_sum / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ret = "ok" else : raise util_functions . InputError ( e_set , "needs to be an essay set of the train type." ) else : raise util_functions . InputError ( e_set , "wrong input. need an essay set object" ) return ret
1690	def UpdatePreprocessor ( self , line ) : if Match ( r'^\s*#\s*(if|ifdef|ifndef)\b' , line ) : # Beginning of #if block, save the nesting stack here. The saved # stack will allow us to restore the parsing state in the #else case. self . pp_stack . append ( _PreprocessorInfo ( copy . deepcopy ( self . stack ) ) ) elif Match ( r'^\s*#\s*(else|elif)\b' , line ) : # Beginning of #else block if self . pp_stack : if not self . pp_stack [ - 1 ] . seen_else : # This is the first #else or #elif block. Remember the # whole nesting stack up to this point. This is what we # keep after the #endif. self . pp_stack [ - 1 ] . seen_else = True self . pp_stack [ - 1 ] . stack_before_else = copy . deepcopy ( self . stack ) # Restore the stack to how it was before the #if self . stack = copy . deepcopy ( self . pp_stack [ - 1 ] . stack_before_if ) else : # TODO(unknown): unexpected #else, issue warning? pass elif Match ( r'^\s*#\s*endif\b' , line ) : # End of #if or #else blocks. if self . pp_stack : # If we saw an #else, we will need to restore the nesting # stack to its former state before the #else, otherwise we # will just continue from where we left off. if self . pp_stack [ - 1 ] . seen_else : # Here we can just use a shallow copy since we are the last # reference to it. self . stack = self . pp_stack [ - 1 ] . stack_before_else # Drop the corresponding #if self . pp_stack . pop ( ) else : # TODO(unknown): unexpected #endif, issue warning? pass
10626	def _calculate_T ( self , Hfr ) : # Create the initial guesses for temperature. x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) # Evaluate the enthalpy for the initial guesses. y = list ( ) y . append ( self . _calculate_Hfr ( x [ 0 ] ) - Hfr ) y . append ( self . _calculate_Hfr ( x [ 1 ] ) - Hfr ) # Solve for temperature. for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_Hfr ( x [ i ] ) - Hfr ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
3317	def digest_auth_user ( self , realm , user_name , environ ) : user = self . _get_realm_entry ( realm , user_name ) if user is None : return False password = user . get ( "password" ) environ [ "wsgidav.auth.roles" ] = user . get ( "roles" , [ ] ) return self . _compute_http_digest_a1 ( realm , user_name , password )
9761	def logs ( ctx , job , past , follow , hide_time ) : def get_experiment_logs ( ) : if past : try : response = PolyaxonClient ( ) . experiment . logs ( user , project_name , _experiment , stream = False ) get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . experiment . logs ( user , project_name , _experiment , message_handler = get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_logs ( ) : if past : try : response = PolyaxonClient ( ) . experiment_job . logs ( user , project_name , _experiment , _job , stream = False ) get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . experiment_job . logs ( user , project_name , _experiment , _job , message_handler = get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_logs ( ) else : get_experiment_logs ( )
3241	def _get_base ( server_certificate , * * conn ) : server_certificate [ '_version' ] = 1 # Get the initial cert details: cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , * * conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) # Cast dates from a datetime to something JSON serializable. server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
4188	def window_riemann ( N ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = sin ( n / float ( N ) * 2. * pi ) / ( n / float ( N ) * 2. * pi ) return w
5961	def set_correlparameters ( self , * * kwargs ) : self . ncorrel = kwargs . pop ( 'ncorrel' , self . ncorrel ) or 25000 nstep = kwargs . pop ( 'nstep' , None ) if nstep is None : # good step size leads to ~25,000 data points nstep = len ( self . array [ 0 ] ) / float ( self . ncorrel ) nstep = int ( numpy . ceil ( nstep ) ) # catch small data sets kwargs [ 'nstep' ] = nstep self . __correlkwargs . update ( kwargs ) # only contains legal kw for numkit.timeseries.tcorrel or force return self . __correlkwargs
6179	def merge_ph_times ( times_list , times_par_list , time_block ) : offsets = np . arange ( len ( times_list ) ) * time_block cum_sizes = np . cumsum ( [ ts . size for ts in times_list ] ) times = np . zeros ( cum_sizes [ - 1 ] ) times_par = np . zeros ( cum_sizes [ - 1 ] , dtype = 'uint8' ) i1 = 0 for i2 , ts , ts_par , offset in zip ( cum_sizes , times_list , times_par_list , offsets ) : times [ i1 : i2 ] = ts + offset times_par [ i1 : i2 ] = ts_par i1 = i2 return times , times_par
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
11416	def record_add_subfield_into ( rec , tag , subfield_code , value , subfield_position = None , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) if subfield_position is None : subfields . append ( ( subfield_code , value ) ) else : subfields . insert ( subfield_position , ( subfield_code , value ) )
4161	def _select_block ( str_in , start_tag , end_tag ) : start_pos = str_in . find ( start_tag ) if start_pos < 0 : raise ValueError ( 'start_tag not found' ) depth = 0 for pos in range ( start_pos , len ( str_in ) ) : if str_in [ pos ] == start_tag : depth += 1 elif str_in [ pos ] == end_tag : depth -= 1 if depth == 0 : break sel = str_in [ start_pos + 1 : pos ] return sel
6137	def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
3398	def update_costs ( self ) : for var in self . indicators : if var not in self . costs : self . costs [ var ] = var . cost else : if var . _get_primal ( ) > self . integer_threshold : self . costs [ var ] += var . cost self . model . objective . set_linear_coefficients ( self . costs )
9638	def emit ( self , record ) : try : self . redis_client . publish ( self . channel , self . format ( record ) ) except redis . RedisError : pass
13078	def make_cache_keys ( self , endpoint , kwargs ) : keys = sorted ( kwargs . keys ( ) ) i18n_cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys ] ) if "lang" in keys : cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys if k != "lang" ] ) else : cache_key = i18n_cache_key return i18n_cache_key , cache_key
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( * * jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( * * jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( * * jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( * * rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
13842	def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
5308	def hex_to_rgb ( value ) : value = value . lstrip ( '#' ) check_hex ( value ) length = len ( value ) step = int ( length / 3 ) return tuple ( int ( value [ i : i + step ] , 16 ) for i in range ( 0 , length , step ) )
503	def _labelListToCategoryNumber ( self , labelList ) : categoryNumber = 0 for label in labelList : categoryNumber += self . _labelToCategoryNumber ( label ) return categoryNumber
6995	def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_bucket = None , result_queue = None , result_bucket = None , pfresult_list = None , runcp_kwargs = None , process_list_slice = None , download_when_done = True , purge_queues_when_done = True , save_state_when_done = True , delete_queues_when_done = False , s3_client = None , sqs_client = None ) : if use_saved_state is not None and os . path . exists ( use_saved_state ) : with open ( use_saved_state , 'rb' ) as infd : saved_state = pickle . load ( infd ) # run the producer loop using the saved state's todo list return runcp_producer_loop ( saved_state [ 'in_progress' ] , saved_state [ 'args' ] [ 1 ] , saved_state [ 'args' ] [ 2 ] , saved_state [ 'args' ] [ 3 ] , saved_state [ 'args' ] [ 4 ] , * * saved_state [ 'kwargs' ] ) else : return runcp_producer_loop ( lightcurve_list , input_queue , input_bucket , result_queue , result_bucket , pfresult_list = pfresult_list , runcp_kwargs = runcp_kwargs , process_list_slice = process_list_slice , download_when_done = download_when_done , purge_queues_when_done = purge_queues_when_done , save_state_when_done = save_state_when_done , delete_queues_when_done = delete_queues_when_done , s3_client = s3_client , sqs_client = sqs_client )
747	def anomalyGetLabels ( self , start , end ) : return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end )
7473	def singlecat ( data , sample , bseeds , sidx , nloci ) : LOGGER . info ( "in single cat here" ) ## enter ref data? isref = 'reference' in data . paramsdict [ "assembly_method" ] ## grab seeds and hits info for this sample with h5py . File ( bseeds , 'r' ) as io5 : ## get hits just for this sample and sort them by sample order index hits = io5 [ "uarr" ] [ : ] hits = hits [ hits [ : , 1 ] == sidx , : ] #hits = hits[hits[:, 2].argsort()] ## get seeds just for this sample and sort them by sample order index seeds = io5 [ "seedsarr" ] [ : ] seeds = seeds [ seeds [ : , 1 ] == sidx , : ] #seeds = seeds[seeds[:, 2].argsort()] full = np . concatenate ( ( seeds , hits ) ) full = full [ full [ : , 0 ] . argsort ( ) ] ## still using max+20 len limit, rare longer merged reads get trimmed ## we need to allow room for indels to be added too maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 ## we'll fill a new catg and alleles arr for this sample in locus order, ## which is known from seeds and hits ocatg = np . zeros ( ( nloci , maxlen , 4 ) , dtype = np . uint32 ) onall = np . zeros ( nloci , dtype = np . uint8 ) ochrom = np . zeros ( ( nloci , 3 ) , dtype = np . int64 ) ## grab the sample's data and write to ocatg and onall if not sample . files . database : raise IPyradWarningExit ( "missing catg file - {}" . format ( sample . name ) ) with h5py . File ( sample . files . database , 'r' ) as io5 : ## get it and delete it catarr = io5 [ "catg" ] [ : ] tmp = catarr [ full [ : , 2 ] , : maxlen , : ] del catarr ocatg [ full [ : , 0 ] , : tmp . shape [ 1 ] , : ] = tmp del tmp ## get it and delete it nall = io5 [ "nalleles" ] [ : ] onall [ full [ : , 0 ] ] = nall [ full [ : , 2 ] ] del nall ## fill the reference data if isref : chrom = io5 [ "chroms" ] [ : ] ochrom [ full [ : , 0 ] ] = chrom [ full [ : , 2 ] ] del chrom ## get indel locations for this sample ipath = os . path . join ( data . dirs . across , data . name + ".tmp.indels.hdf5" ) with h5py . File ( ipath , 'r' ) as ih5 : indels = ih5 [ "indels" ] [ sidx , : , : maxlen ] ## insert indels into ocatg newcatg = inserted_indels ( indels , ocatg ) del ocatg , indels ## save individual tmp h5 data smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio , 'w' ) as oh5 : oh5 . create_dataset ( "icatg" , data = newcatg , dtype = np . uint32 ) oh5 . create_dataset ( "inall" , data = onall , dtype = np . uint8 ) if isref : oh5 . create_dataset ( "ichrom" , data = ochrom , dtype = np . int64 )
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
7557	def random_product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4
1505	def template_statemgr_yaml ( cl_args , zookeepers ) : statemgr_config_file_template = "%s/standalone/templates/statemgr.template.yaml" % cl_args [ "config_path" ] statemgr_config_file_actual = "%s/standalone/statemgr.yaml" % cl_args [ "config_path" ] template_file ( statemgr_config_file_template , statemgr_config_file_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '"%s"' % zk if ":" in zk else '"%s:2181"' % zk for zk in zookeepers ] ) } )
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
2091	def copy ( self , pk = None , new_name = None , * * kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] # Remove default values (anything where the value is None). self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) # Modify data to fit the call pattern of the tower-cli method for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : # copy client-side, the old mechanism newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , * * newresource ) else : # copy server-side, the new mechanism if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
13577	def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
10842	def move_to_top ( self ) : url = PATHS [ 'MOVE_TO_TOP' ] % self . id response = self . api . post ( url = url ) return Update ( api = self . api , raw_response = response )
9168	def post_publication_processing ( event , cursor ) : module_ident , ident_hash = event . module_ident , event . ident_hash celery_app = get_current_registry ( ) . celery_app # Check baking is not already queued. cursor . execute ( 'SELECT result_id::text ' 'FROM document_baking_result_associations ' 'WHERE module_ident = %s' , ( module_ident , ) ) for result in cursor . fetchall ( ) : state = celery_app . AsyncResult ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) return logger . debug ( 'Queued for processing module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) recipe_ids = _get_recipe_ids ( module_ident , cursor ) update_module_state ( cursor , module_ident , 'processing' , recipe_ids [ 0 ] ) # Commit the state change before preceding. cursor . connection . commit ( ) # Start of task # FIXME Looking up the task isn't the most clear usage here. task_name = 'cnxpublishing.subscribers.baking_processor' baking_processor = celery_app . tasks [ task_name ] result = baking_processor . delay ( module_ident , ident_hash ) baking_processor . backend . store_result ( result . id , None , 'QUEUED' ) # Save the mapping between a celery task and this event. track_baking_proc_state ( result , module_ident , cursor )
2503	def handle_lics ( self , lics ) : # Handle extracted licensing info type. if ( lics , RDF . type , self . spdx_namespace [ 'ExtractedLicensingInfo' ] ) in self . graph : return self . parse_only_extr_license ( lics ) # Assume resource, hence the path separator ident_start = lics . rfind ( '/' ) + 1 if ident_start == 0 : # special values such as spdx:noassertion special = self . to_special_value ( lics ) if special == lics : if self . LICS_REF_REGEX . match ( lics ) : # Is a license ref i.e LicenseRef-1 return document . License . from_identifier ( lics ) else : # Not a known license form raise SPDXValueError ( 'License' ) else : # is a special value return special else : # license url return document . License . from_identifier ( lics [ ident_start : ] )
12154	def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return "%.02f ms" % ( took * 1000.0 ) elif took < 60 : return "%.02f s" % ( took ) else : return "%.02f min" % ( took / 60.0 )
13542	def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
8441	def _parse_link_header ( headers ) : links = { } if 'link' in headers : link_headers = headers [ 'link' ] . split ( ', ' ) for link_header in link_headers : ( url , rel ) = link_header . split ( '; ' ) url = url [ 1 : - 1 ] rel = rel [ 5 : - 1 ] links [ rel ] = url return links
10021	def get_environments ( self ) : response = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) return response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ]
6956	def _transit_model ( times , t0 , per , rp , a , inc , ecc , w , u , limb_dark , exp_time_minutes = 2 , supersample_factor = 7 ) : params = batman . TransitParams ( ) # object to store transit parameters params . t0 = t0 # time of periastron params . per = per # orbital period params . rp = rp # planet radius (in stellar radii) params . a = a # semi-major axis (in stellar radii) params . inc = inc # orbital inclination (in degrees) params . ecc = ecc # the eccentricity of the orbit params . w = w # longitude of periastron (in degrees) params . u = u # limb darkening coefficient list params . limb_dark = limb_dark # limb darkening model to use t = times m = batman . TransitModel ( params , t , exp_time = exp_time_minutes / 60. / 24. , supersample_factor = supersample_factor ) return params , m
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
5804	def detect_client_auth_request ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0d' : return True return False
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
9090	def _get_default_namespace ( self ) -> Optional [ Namespace ] : return self . _get_query ( Namespace ) . filter ( Namespace . url == self . _get_namespace_url ( ) ) . one_or_none ( )
7392	def adjust_angles ( self , start_node , start_angle , end_node , end_angle ) : start_group = self . find_node_group_membership ( start_node ) end_group = self . find_node_group_membership ( end_node ) if start_group == 0 and end_group == len ( self . nodes . keys ( ) ) - 1 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) elif start_group == len ( self . nodes . keys ( ) ) - 1 and end_group == 0 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) elif start_group < end_group : if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) elif end_group < start_group : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) return start_angle , end_angle
12257	def smooth ( x , rho , penalty , axis = 0 , newshape = None ) : orig_shape = x . shape if newshape is not None : x = x . reshape ( newshape ) # Apply Laplacian smoothing (l2 norm on the parameters multiplied by # the laplacian) n = x . shape [ axis ] lap_op = spdiags ( [ ( 2 + rho / penalty ) * np . ones ( n ) , - 1 * np . ones ( n ) , - 1 * np . ones ( n ) ] , [ 0 , - 1 , 1 ] , n , n , format = 'csc' ) A = penalty * lap_op b = rho * np . rollaxis ( x , axis , 0 ) return np . rollaxis ( spsolve ( A , b ) , axis , 0 ) . reshape ( orig_shape )
10017	def upload_archive ( self , filename , key , auto_create_bucket = True ) : try : bucket = self . s3 . get_bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get_location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get_location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get_location ( ) != 'eu-west-1' ) : raise Exception ( "Existing bucket doesn't match region" ) except S3ResponseError : bucket = self . s3 . create_bucket ( self . aws . bucket , location = self . aws . region ) def __report_upload_progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( "Uploaded " + str ( sent ) + " bytes of " + str ( total ) + " (" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + "%)" ) # upload the new version k = Key ( bucket ) k . key = self . aws . bucket_path + key k . set_metadata ( 'time' , str ( time ( ) ) ) k . set_contents_from_filename ( filename , cb = __report_upload_progress , num_cb = 10 )
7247	def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) [ 'state' ]
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) # support in-place editing formatted request try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
10142	def upload_files ( selected_file , selected_host , only_link , file_name ) : try : answer = requests . post ( url = selected_host [ 0 ] + "upload.php" , files = { 'files[]' : selected_file } ) file_name_1 = re . findall ( r'"url": *"((h.+\/){0,1}(.+?))"[,\}]' , answer . text . replace ( "\\" , "" ) ) [ 0 ] [ 2 ] if only_link : return [ selected_host [ 1 ] + file_name_1 , "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) ] else : return "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) except requests . exceptions . ConnectionError : print ( file_name + ' couldn\'t be uploaded to ' + selected_host [ 0 ] )
6332	def decode ( self , code , terminator = '\0' ) : if code : if terminator not in code : raise ValueError ( 'Specified terminator, {}, absent from code.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : wordlist = [ '' ] * len ( code ) for i in range ( len ( code ) ) : wordlist = sorted ( code [ i ] + wordlist [ i ] for i in range ( len ( code ) ) ) rows = [ w for w in wordlist if w [ - 1 ] == terminator ] [ 0 ] return rows . rstrip ( terminator ) else : return ''
55	def copy ( self , keypoints = None , shape = None ) : result = copy . copy ( self ) if keypoints is not None : result . keypoints = keypoints if shape is not None : result . shape = shape return result
1685	def RepositoryName ( self ) : fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) # If the user specified a repository path, it exists, and the file is # contained in it, use the specified repository path if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : # allow case insensitive compare on Windows if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : # If there's a .svn file in the current directory, we recursively look # up the directory tree for the top of the SVN checkout root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by # searching up from the current path. root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Don't know what to do; header guard warnings may be wrong... return fullname
6693	def get_or_create_bucket ( self , name ) : from boto . s3 import connection if self . dryrun : print ( 'boto.connect_s3().create_bucket(%s)' % repr ( name ) ) else : conn = connection . S3Connection ( self . genv . aws_access_key_id , self . genv . aws_secret_access_key ) bucket = conn . create_bucket ( name ) return bucket
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) # this is now a weighted residual taking into account the measurement err return ( pmags - modelmags ) / perrs
11294	def main ( path ) : basepath = os . path . abspath ( os . path . expanduser ( str ( path ) ) ) echo . h2 ( "Available scripts in {}" . format ( basepath ) ) echo . br ( ) for root_dir , dirs , files in os . walk ( basepath , topdown = True ) : for f in fnmatch . filter ( files , '*.py' ) : try : filepath = os . path . join ( root_dir , f ) # super edge case, this makes sure the python script won't start # an interactive console session which would cause the session # to start and not allow the for loop to complete with open ( filepath , encoding = "UTF-8" ) as fp : body = fp . read ( ) is_console = "InteractiveConsole" in body is_console = is_console or "code" in body is_console = is_console and "interact(" in body if is_console : continue s = captain . Script ( filepath ) if s . can_run_from_cli ( ) : rel_filepath = s . call_path ( basepath ) p = s . parser echo . h3 ( rel_filepath ) desc = p . description if desc : echo . indent ( desc , indent = ( " " * 4 ) ) subcommands = s . subcommands if subcommands : echo . br ( ) echo . indent ( "Subcommands:" , indent = ( " " * 4 ) ) for sc in subcommands . keys ( ) : echo . indent ( sc , indent = ( " " * 6 ) ) echo . br ( ) except captain . ParseError : pass except Exception as e : #echo.exception(e) #echo.err("Failed to parse {} because {}", f, e.message) echo . err ( "Failed to parse {}" , f ) echo . verbose ( e . message ) echo . br ( )
8958	def walk ( self , * * kwargs ) : lead = '' if 'with_root' in kwargs and kwargs . pop ( 'with_root' ) : lead = self . root . rstrip ( os . sep ) + os . sep for base , dirs , files in os . walk ( self . root , * * kwargs ) : prefix = base [ len ( self . root ) : ] . lstrip ( os . sep ) bits = prefix . split ( os . sep ) if prefix else [ ] for dirname in dirs [ : ] : path = '/' . join ( bits + [ dirname ] ) inclusive = self . included ( path , is_dir = True ) if inclusive : yield lead + path + '/' elif inclusive is False : dirs . remove ( dirname ) for filename in files : path = '/' . join ( bits + [ filename ] ) if self . included ( path ) : yield lead + path
125	def Positive ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = True , mode = mode , reroll_count_max = reroll_count_max )
5343	def compose_mbox ( projects ) : mbox_archives = '/home/bitergia/mboxes' mailing_lists_projects = [ project for project in projects if 'mailing_lists' in projects [ project ] ] for mailing_lists in mailing_lists_projects : projects [ mailing_lists ] [ 'mbox' ] = [ ] for mailing_list in projects [ mailing_lists ] [ 'mailing_lists' ] : if 'listinfo' in mailing_list : name = mailing_list . split ( 'listinfo/' ) [ 1 ] elif 'mailing-list' in mailing_list : name = mailing_list . split ( 'mailing-list/' ) [ 1 ] else : name = mailing_list . split ( '@' ) [ 0 ] list_new = "%s %s/%s.mbox/%s.mbox" % ( name , mbox_archives , name , name ) projects [ mailing_lists ] [ 'mbox' ] . append ( list_new ) return projects
1038	def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
10424	def infer_missing_two_way_edges ( graph ) : for u , v , k , d in graph . edges ( data = True , keys = True ) : if d [ RELATION ] in TWO_WAY_RELATIONS : infer_missing_backwards_edge ( graph , u , v , k )
386	def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : if coords is None : coords = [ ] def _flip ( im , coords ) : im = flip_axis ( im , axis = 1 , is_random = False ) coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) if is_rescale : if is_center : # x_center' = 1 - x x = 1. - coord [ 0 ] else : # x_center' = 1 - x - w x = 1. - coord [ 0 ] - coord [ 2 ] else : if is_center : # x' = im.width - x x = im . shape [ 1 ] - coord [ 0 ] else : # x' = im.width - x - w x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) return im , coords_new if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : return _flip ( im , coords ) else : return im , coords else : return _flip ( im , coords )
13006	def utime ( self , * args , * * kwargs ) : os . utime ( self . extended_path , * args , * * kwargs )
11153	def sha512file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha512 , nbytes = nbytes , chunk_size = chunk_size )
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , * * kwargs ) : path = self . BezierPath ( * * kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
5146	def render ( self , files = True ) : self . validate ( ) # convert NetJSON config to intermediate data structure if self . intermediate_data is None : self . to_intermediate ( ) # support multiple renderers renderers = getattr ( self , 'renderers' , None ) or [ self . renderer ] # convert intermediate data structure to native configuration output = '' for renderer_class in renderers : renderer = renderer_class ( self ) output += renderer . render ( ) # remove reference to renderer instance (not needed anymore) del renderer # are we required to include # additional files? if files : # render additional files files_output = self . _render_files ( ) if files_output : # max 2 new lines output += files_output . replace ( '\n\n\n' , '\n\n' ) # return the configuration return output
11347	def html_to_text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped_data = s . unescape ( s . get_data ( ) ) return escape_for_xml ( unescaped_data , tags_to_keep = s . mathml_elements )
3257	def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : # @todo native_srs doesn't seem to get detected, even when in the DB # metadata (at least for postgis in geometry_columns) and then there # will be a misconfigured layer if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) # because name is the in FeatureType base class, work around that # and hack in these others that don't have xml properties feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
9986	def get_description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data
13077	def main_collections ( self , lang = None ) : return sorted ( [ { "id" : member . id , "label" : str ( member . get_label ( lang = lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in self . resolver . getMetadata ( ) . members ] , key = itemgetter ( "label" ) )
6871	def estimate_achievable_tmid_precision ( snr , t_ingress_min = 10 , t_duration_hr = 2.14 ) : t_ingress = t_ingress_min * u . minute t_duration = t_duration_hr * u . hour theta = t_ingress / t_duration sigma_tc = ( 1 / snr * t_duration * np . sqrt ( theta / 2 ) ) LOGINFO ( 'assuming t_ingress = {:.1f}' . format ( t_ingress ) ) LOGINFO ( 'assuming t_duration = {:.1f}' . format ( t_duration ) ) LOGINFO ( 'measured SNR={:.2f}\n\t' . format ( snr ) + '-->theoretical sigma_tc = {:.2e} = {:.2e} = {:.2e}' . format ( sigma_tc . to ( u . minute ) , sigma_tc . to ( u . hour ) , sigma_tc . to ( u . day ) ) ) return sigma_tc . to ( u . day ) . value
5803	def extract_chain ( server_handshake_bytes ) : output = [ ] chain_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0b' : chain_bytes = message_data break if chain_bytes : break if chain_bytes : # The first 3 bytes are the cert chain length pointer = 3 while pointer < len ( chain_bytes ) : cert_length = int_from_bytes ( chain_bytes [ pointer : pointer + 3 ] ) cert_start = pointer + 3 cert_end = cert_start + cert_length pointer = cert_end cert_bytes = chain_bytes [ cert_start : cert_end ] output . append ( Certificate . load ( cert_bytes ) ) return output
7765	def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
2811	def convert_reshape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reshape ...' ) if names == 'short' : tf_name = 'RESH' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) > 1 : if layers [ inputs [ 1 ] ] [ 0 ] == - 1 : print ( 'Cannot deduct batch size! It will be omitted, but result may be wrong.' ) reshape = keras . layers . Reshape ( layers [ inputs [ 1 ] + '_np' ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ 0 ] ] ) else : if inputs [ 0 ] in layers : reshape = keras . layers . Reshape ( params [ 'shape' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ 0 ] ] ) else : print ( 'Skip weight matrix transpose, but result may be wrong.' )
13812	def FindMethodByName ( self , name ) : for method in self . methods : if name == method . name : return method return None
12087	def html_singleAll ( self , template = "basic" ) : for fname in smartSort ( self . cells ) : if template == "fixed" : self . html_single_fixed ( fname ) else : self . html_single_basic ( fname )
10349	def lint_file ( in_file , out_file = None ) : for line in in_file : print ( line . strip ( ) , file = out_file )
13299	def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip_path , 'install' , package )
1784	def CMP ( cpu , src1 , src2 ) : arg0 = src1 . read ( ) arg1 = Operators . SEXTEND ( src2 . read ( ) , src2 . size , src1 . size ) # Affected Flags o..szapc cpu . _calculate_CMP_flags ( src1 . size , arg0 - arg1 , arg0 , arg1 )
362	def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : # We first define a download function, supporting both Python 2 and 3. def _download ( filename , working_directory , url_source ) : progress_bar = progressbar . ProgressBar ( ) def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : if ( totalSize != 0 ) : if not pbar . max_value : totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) pbar . max_value = int ( totalBlocks ) pbar . update ( count , force = True ) filepath = os . path . join ( working_directory , filename ) logging . info ( 'Downloading %s...\n' % filename ) urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) exists_or_mkdir ( working_directory , verbose = False ) filepath = os . path . join ( working_directory , filename ) if not os . path . exists ( filepath ) : _download ( filename , working_directory , url_source ) statinfo = os . stat ( filepath ) logging . info ( 'Succesfully downloaded %s %s bytes.' % ( filename , statinfo . st_size ) ) # , 'bytes.') if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) if ( extract ) : if tarfile . is_tarfile ( filepath ) : logging . info ( 'Trying to extract tar file' ) tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) logging . info ( '... Success!' ) elif zipfile . is_zipfile ( filepath ) : logging . info ( 'Trying to extract zip file' ) with zipfile . ZipFile ( filepath ) as zf : zf . extractall ( working_directory ) logging . info ( '... Success!' ) else : logging . info ( "Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported" ) return filepath
2354	def find_element ( self , strategy , locator ) : return self . driver_adapter . find_element ( strategy , locator , root = self . root )
3744	def _round_whole_even ( i ) : if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
4300	def _install_aldryn ( config_data ) : # pragma: no cover import requests media_project = os . path . join ( config_data . project_directory , 'dist' , 'media' ) static_main = False static_project = os . path . join ( config_data . project_directory , 'dist' , 'static' ) template_target = os . path . join ( config_data . project_directory , 'templates' ) tmpdir = tempfile . mkdtemp ( ) aldrynzip = requests . get ( data . ALDRYN_BOILERPLATE ) zip_open = zipfile . ZipFile ( BytesIO ( aldrynzip . content ) ) zip_open . extractall ( path = tmpdir ) for component in os . listdir ( os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' ) ) : src = os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' , component ) dst = os . path . join ( config_data . project_directory , component ) if os . path . isfile ( src ) : shutil . copy ( src , dst ) else : shutil . copytree ( src , dst ) shutil . rmtree ( tmpdir ) return media_project , static_main , static_project , template_target
5006	def handle_enterprise_logistration ( backend , user , * * kwargs ) : request = backend . strategy . request enterprise_customer = get_enterprise_customer_for_running_pipeline ( request , { 'backend' : backend . name , 'kwargs' : kwargs } ) if enterprise_customer is None : # This pipeline element is not being activated as a part of an Enterprise logistration return # proceed with the creation of a link between the user and the enterprise customer, then exit. enterprise_customer_user , _ = EnterpriseCustomerUser . objects . update_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enterprise_customer_user . update_session ( request )
4565	def pop_legacy_palette ( kwds , * color_defaults ) : palette = kwds . pop ( 'palette' , None ) if palette : legacy = [ k for k , _ in color_defaults if k in kwds ] if legacy : raise ValueError ( 'Cannot set palette and ' + ', ' . join ( legacy ) ) return palette values = [ kwds . pop ( k , v ) for k , v in color_defaults ] if values and color_defaults [ 0 ] [ 0 ] in ( 'colors' , 'palette' ) : values = values [ 0 ] return make . colors ( values or None )
4897	def get_course_duration ( self , obj ) : duration = obj . end - obj . start if obj . start and obj . end else None if duration : return strfdelta ( duration , '{W} weeks {D} days.' ) return ''
405	def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )
648	def generateHubSequences ( nCoinc = 10 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) for hub in hubs : coincList . remove ( hub ) seqList = [ ] for i in xrange ( nSeq ) : length = random . choice ( seqLength ) - 1 seq = random . sample ( coincList , length ) seq . insert ( length // 2 , random . choice ( hubs ) ) seqList . append ( seq ) return seqList
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
6944	def jhk_to_vmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , VJHK , VJH , VJK , VHK , VJ , VH , VK )
757	def _allow_new_attributes ( f ) : def decorated ( self , * args , * * kw ) : """The decorated function that replaces __init__() or __setstate__() """ # Run the original function if not hasattr ( self , '_canAddAttributes' ) : self . __dict__ [ '_canAddAttributes' ] = 1 else : self . _canAddAttributes += 1 assert self . _canAddAttributes >= 1 # Save add attribute counter count = self . _canAddAttributes f ( self , * args , * * kw ) # Restore _CanAddAttributes if deleted from dict (can happen in __setstte__) if hasattr ( self , '_canAddAttributes' ) : self . _canAddAttributes -= 1 else : self . _canAddAttributes = count - 1 assert self . _canAddAttributes >= 0 if self . _canAddAttributes == 0 : del self . _canAddAttributes decorated . __doc__ = f . __doc__ decorated . __name__ = f . __name__ return decorated
3775	def solve_prop ( self , goal , reset_method = True ) : if self . Tmin is None or self . Tmax is None : raise Exception ( 'Both a minimum and a maximum value are not present indicating there is not enough data for temperature dependency.' ) if not self . test_property_validity ( goal ) : raise Exception ( 'Input property is not considered plausible; no method would calculate it.' ) def error ( T ) : if reset_method : self . method = None return self . T_dependent_property ( T ) - goal try : return brenth ( error , self . Tmin , self . Tmax ) except ValueError : raise Exception ( 'To within the implemented temperature range, it is not possible to calculate the desired value.' )
9031	def _expand_produced_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_consumed ( ) : return row = mesh . consuming_row position = Point ( row_position . x - mesh . index_in_consuming_row + mesh_index , row_position . y + INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
5751	def already_downloaded ( filename ) : cur_file = os . path . join ( c . bview_dir , filename ) old_file = os . path . join ( c . bview_dir , 'old' , filename ) if not os . path . exists ( cur_file ) and not os . path . exists ( old_file ) : return False return True
12197	def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
13654	def Integer ( name , base = 10 , encoding = None ) : def _match ( request , value ) : return name , query . Integer ( value , base = base , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
725	def get ( self , number ) : if not number in self . _patterns : raise IndexError ( "Invalid number" ) return self . _patterns [ number ]
7746	def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if tag . endswith ( "}iq" ) or tag == "iq" : return Iq ( element , return_path = return_path , language = language ) if tag . endswith ( "}message" ) or tag == "message" : return Message ( element , return_path = return_path , language = language ) if tag . endswith ( "}presence" ) or tag == "presence" : return Presence ( element , return_path = return_path , language = language ) else : return Stanza ( element , return_path = return_path , language = language )
651	def sameTMParams ( tp1 , tp2 ) : result = True for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , "minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , "permanenceMax" , "globalDecay" , "activationThreshold" , "doPooling" , "segUpdateValidDuration" , "burnIn" , "pamLength" , "maxAge" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
1153	def _hash ( self ) : MAX = sys . maxint MASK = 2 * MAX + 1 n = len ( self ) h = 1927868237 * ( n + 1 ) h &= MASK for x in self : hx = hash ( x ) h ^= ( hx ^ ( hx << 16 ) ^ 89869747 ) * 3644798167 h &= MASK h = h * 69069 + 907133923 h &= MASK if h > MAX : h -= MASK + 1 if h == - 1 : h = 590923713 return h
1748	def _get_offset ( self , index ) : if not self . _in_range ( index ) : raise IndexError ( 'Map index out of range' ) if isinstance ( index , slice ) : index = slice ( index . start - self . start , index . stop - self . start ) else : index -= self . start return index
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
2212	def touch ( fpath , mode = 0o666 , dir_fd = None , verbose = 0 , * * kwargs ) : if verbose : print ( 'Touching file {}' . format ( fpath ) ) if six . PY2 : # nocover with open ( fpath , 'a' ) : os . utime ( fpath , None ) else : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fpath , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fpath , dir_fd = None if os . supports_fd else dir_fd , * * kwargs ) return fpath
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , * * kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , * * kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , * * kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
12033	def averageSweep ( self , sweepFirst = 0 , sweepLast = None ) : if sweepLast is None : sweepLast = self . sweeps - 1 nSweeps = sweepLast - sweepFirst + 1 runningSum = np . zeros ( len ( self . sweepY ) ) self . log . debug ( "averaging sweep %d to %d" , sweepFirst , sweepLast ) for sweep in np . arange ( nSweeps ) + sweepFirst : self . setsweep ( sweep ) runningSum += self . sweepY . flatten ( ) average = runningSum / nSweeps #TODO: standard deviation? return average
10977	def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
3873	async def leave_conversation ( self , conv_id ) : logger . info ( 'Leaving conversation: {}' . format ( conv_id ) ) await self . _conv_dict [ conv_id ] . leave ( ) del self . _conv_dict [ conv_id ]
9022	def add_row ( self , id_ ) : row = self . _parser . new_row ( id_ ) self . _rows . append ( row ) return row
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
6804	def init_raspbian_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) device_question = 'SD card present at %s? ' % self . env . sd_device if not yes and not raw_input ( device_question ) . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local_if_missing ( fn = '{raspbian_image_zip}' , cmd = 'wget {raspbian_download_url} -O raspbian_lite_latest.zip' ) r . lenv . img_fn = r . local ( "unzip -l {raspbian_image_zip} | sed -n 4p | awk '{{print $4}}'" , capture = True ) or '$IMG_FN' r . local ( 'echo {img_fn}' ) r . local ( '[ ! -f {img_fn} ] && unzip {raspbian_image_zip} {img_fn} || true' ) r . lenv . img_fn = r . local ( 'readlink -f {img_fn}' , capture = True ) r . local ( 'echo {img_fn}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2} || true' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'time dd bs=4M if={img_fn} of={sd_device}' ) # Flush all writes to disk. r . run ( 'sync' )
9154	def bezier ( self , points ) : coordinates = pgmagick . CoordinateList ( ) for point in points : x , y = float ( point [ 0 ] ) , float ( point [ 1 ] ) coordinates . append ( pgmagick . Coordinate ( x , y ) ) self . drawer . append ( pgmagick . DrawableBezier ( coordinates ) )
7650	def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : with _open ( path_or_file , mode = 'r' , fmt = fmt ) as fdesc : jam = JAMS ( * * json . load ( fdesc ) ) if validate : jam . validate ( strict = strict ) return jam
6606	def run_multiple ( self , workingArea , package_indices ) : if not package_indices : return [ ] job_desc = self . _compose_job_desc ( workingArea , package_indices ) clusterprocids = submit_jobs ( job_desc , cwd = workingArea . path ) # TODO: make configurable clusterids = clusterprocids2clusterids ( clusterprocids ) for clusterid in clusterids : change_job_priority ( [ clusterid ] , 10 ) self . clusterprocids_outstanding . extend ( clusterprocids ) return clusterprocids
2814	def convert_shape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting shape ...' ) def target_layer ( x ) : import tensorflow as tf return tf . shape ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
9609	def find_exception_by_code ( code ) : errorName = None for error in WebDriverError : if error . value . code == code : errorName = error break return errorName
1416	def get_execution_state ( self , topologyName , callback = None ) : isWatching = False # Temp dict used to return result # if callback is not provided. ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : """ Custom callback to get the topologies right now. """ ret [ "result" ] = data self . _get_execution_state_with_watch ( topologyName , callback , isWatching ) # The topologies are now populated with the data. return ret [ "result" ]
1479	def _wait_process_std_out_err ( self , name , process ) : proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) process . wait ( )
13681	def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
10973	def invitations ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_invitations ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , page = page , per_page = per_page , )
8396	def trans_new ( name , transform , inverse , breaks = None , minor_breaks = None , _format = None , domain = ( - np . inf , np . inf ) , doc = '' , * * kwargs ) : def _get ( func ) : if isinstance ( func , ( classmethod , staticmethod , MethodType ) ) : return func else : return staticmethod ( func ) klass_name = '{}_trans' . format ( name ) d = { 'transform' : _get ( transform ) , 'inverse' : _get ( inverse ) , 'domain' : domain , '__doc__' : doc , * * kwargs } if breaks : d [ 'breaks_' ] = _get ( breaks ) if minor_breaks : d [ 'minor_breaks' ] = _get ( minor_breaks ) if _format : d [ 'format' ] = _get ( _format ) return type ( klass_name , ( trans , ) , d )
9999	def cells_to_series ( cells , args ) : paramlen = len ( cells . formula . parameters ) is_multidx = paramlen > 1 if len ( cells . data ) == 0 : data = { } indexes = None elif paramlen == 0 : # Const Cells data = list ( cells . data . values ( ) ) indexes = [ np . nan ] else : if len ( args ) > 0 : defaults = tuple ( param . default for param in cells . formula . signature . parameters . values ( ) ) updated_args = [ ] for arg in args : if len ( arg ) > paramlen : arg = arg [ : paramlen ] elif len ( arg ) < paramlen : arg += defaults [ len ( arg ) : ] updated_args . append ( arg ) items = [ ( arg , cells . data [ arg ] ) for arg in updated_args if arg in cells . data ] else : items = [ ( key , value ) for key , value in cells . data . items ( ) ] if not is_multidx : # Peel 1-element tuple items = [ ( key [ 0 ] , value ) for key , value in items ] if len ( items ) == 0 : indexes , data = None , { } else : indexes , data = zip ( * items ) if is_multidx : indexes = pd . MultiIndex . from_tuples ( indexes ) result = pd . Series ( data = data , name = cells . name , index = indexes ) if indexes is not None and any ( i is not np . nan for i in indexes ) : result . index . names = list ( cells . formula . parameters ) return result
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt # tl.logging.info(CNN.shape) # (5, 5, 3, 64) # exit() n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) # active mode fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col) # (5, 1, 32) 5 5 # exit() # plt.imshow( # np.reshape(CNN[count-1,:,:,:], (n_row, n_col)), # cmap='gray', interpolation="nearest") # theano if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) # distable tick plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
9511	def replace_bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
539	def _finalize ( self ) : self . _logger . info ( "Finished: modelID=%r; %r records processed. Performing final activities" , self . _modelID , self . _currentRecordIndex + 1 ) # ========================================================================= # Dump the experiment metrics at the end of the task # ========================================================================= self . _updateModelDBResults ( ) # ========================================================================= # Check if the current model is the best. Create a milestone if necessary # If the model has been killed, it is not a candidate for "best model", # and its output cache should be destroyed # ========================================================================= if not self . _isKilled : self . __updateJobResults ( ) else : self . __deleteOutputCache ( self . _modelID ) # ========================================================================= # Close output stream, if necessary # ========================================================================= if self . _predictionLogger : self . _predictionLogger . close ( ) # ========================================================================= # Close input stream, if necessary # ========================================================================= if self . _inputSource : self . _inputSource . close ( )
11279	def _encode_ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
8311	def draw_math ( str , x , y , alpha = 1.0 ) : try : from web import _ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = _ctx . imagesize ( img ) _ctx . image ( img , x , y , alpha = alpha ) return w , h
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2500	def validate ( self , messages ) : messages = self . validate_creators ( messages ) messages = self . validate_created ( messages ) return messages
5682	def get_closest_stop ( self , lat , lon ) : cur = self . conn . cursor ( ) min_dist = float ( "inf" ) min_stop_I = None rows = cur . execute ( "SELECT stop_I, lat, lon FROM stops" ) for stop_I , lat_s , lon_s in rows : dist_now = wgs84_distance ( lat , lon , lat_s , lon_s ) if dist_now < min_dist : min_dist = dist_now min_stop_I = stop_I return min_stop_I
6024	def convolve ( self , array ) : if self . shape [ 0 ] % 2 == 0 or self . shape [ 1 ] % 2 == 0 : raise exc . KernelException ( "PSF Kernel must be odd" ) return scipy . signal . convolve2d ( array , self , mode = 'same' )
13430	def create_site ( self , params = { } ) : url = "/2/sites/" body = params data = self . _post_resource ( url , body ) return self . site_from_json ( data [ "site" ] )
5694	def create_table ( self , conn ) : # Make cursor cur = conn . cursor ( ) # Drop table if it already exists, to be recreated. This # could in the future abort if table already exists, and not # recreate it from scratch. #cur.execute('''DROP TABLE IF EXISTS %s'''%self.table) #conn.commit() if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : # "normal" table creation. cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : # When tabledef contains the full CREATE statement (for # virtual tables). cur . execute ( self . tabledef ) conn . commit ( )
64	def is_out_of_image ( self , image , fully = True , partly = False ) : if self . is_fully_within_image ( image ) : return False elif self . is_partly_within_image ( image ) : return partly else : return fully
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
3250	def get_short_version ( self ) : gs_version = self . get_version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs_version ) . strip ( '.' )
6540	def read_file ( filepath ) : with _FILE_CACHE_LOCK : if filepath not in _FILE_CACHE : _FILE_CACHE [ filepath ] = _read_file ( filepath ) return _FILE_CACHE [ filepath ]
2273	def _win32_read_junction ( path ) : if not jwfs . is_reparse_point ( path ) : raise ValueError ( 'not a junction' ) # --- Older version based on using shell commands --- # if not exists(path): # if six.PY2: # raise OSError('Cannot find path={}'.format(path)) # else: # raise FileNotFoundError('Cannot find path={}'.format(path)) # target_name = os.path.basename(path) # for type_or_size, name, pointed in _win32_dir(path, '*'): # if type_or_size == '<JUNCTION>' and name == target_name: # return pointed # raise ValueError('not a junction') # new version using the windows api handle = jwfs . api . CreateFile ( path , 0 , 0 , None , jwfs . api . OPEN_EXISTING , jwfs . api . FILE_FLAG_OPEN_REPARSE_POINT | jwfs . api . FILE_FLAG_BACKUP_SEMANTICS , None ) if handle == jwfs . api . INVALID_HANDLE_VALUE : raise WindowsError ( ) res = jwfs . reparse . DeviceIoControl ( handle , jwfs . api . FSCTL_GET_REPARSE_POINT , None , 10240 ) bytes = jwfs . create_string_buffer ( res ) p_rdb = jwfs . cast ( bytes , jwfs . POINTER ( jwfs . api . REPARSE_DATA_BUFFER ) ) rdb = p_rdb . contents if rdb . tag not in [ 2684354563 , jwfs . api . IO_REPARSE_TAG_SYMLINK ] : raise RuntimeError ( "Expected <2684354563 or 2684354572>, but got %d" % rdb . tag ) jwfs . handle_nonzero_success ( jwfs . api . CloseHandle ( handle ) ) subname = rdb . get_substitute_name ( ) # probably has something to do with long paths, not sure if subname . startswith ( '?\\' ) : subname = subname [ 2 : ] return subname
10353	def write_boilerplate ( name : str , version : Optional [ str ] = None , description : Optional [ str ] = None , authors : Optional [ str ] = None , contact : Optional [ str ] = None , copyright : Optional [ str ] = None , licenses : Optional [ str ] = None , disclaimer : Optional [ str ] = None , namespace_url : Optional [ Mapping [ str , str ] ] = None , namespace_patterns : Optional [ Mapping [ str , str ] ] = None , annotation_url : Optional [ Mapping [ str , str ] ] = None , annotation_patterns : Optional [ Mapping [ str , str ] ] = None , annotation_list : Optional [ Mapping [ str , Set [ str ] ] ] = None , pmids : Optional [ Iterable [ Union [ str , int ] ] ] = None , entrez_ids : Optional [ Iterable [ Union [ str , int ] ] ] = None , file : Optional [ TextIO ] = None , ) -> None : lines = make_knowledge_header ( name = name , version = version or '1.0.0' , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , namespace_url = namespace_url , namespace_patterns = namespace_patterns , annotation_url = annotation_url , annotation_patterns = annotation_patterns , annotation_list = annotation_list , ) for line in lines : print ( line , file = file ) if pmids is not None : for line in make_pubmed_abstract_group ( pmids ) : print ( line , file = file ) if entrez_ids is not None : for line in make_pubmed_gene_group ( entrez_ids ) : print ( line , file = file )
3341	def parse_xml_body ( environ , allow_empty = False ) : # clHeader = environ . get ( "CONTENT_LENGTH" , "" ) . strip ( ) # content_length = -1 # read all of stream if clHeader == "" : # No Content-Length given: read to end of stream # TODO: etree.parse() locks, if input is invalid? # pfroot = etree.parse(environ["wsgi.input"]).getroot() # requestbody = environ["wsgi.input"].read() # TODO: read() should be # called in a loop? requestbody = "" else : try : content_length = int ( clHeader ) if content_length < 0 : raise DAVError ( HTTP_BAD_REQUEST , "Negative content-length." ) except ValueError : raise DAVError ( HTTP_BAD_REQUEST , "content-length is not numeric." ) if content_length == 0 : requestbody = "" else : requestbody = environ [ "wsgi.input" ] . read ( content_length ) environ [ "wsgidav.all_input_read" ] = 1 if requestbody == "" : if allow_empty : return None else : raise DAVError ( HTTP_BAD_REQUEST , "Body must not be empty." ) try : rootEL = etree . fromstring ( requestbody ) except Exception as e : raise DAVError ( HTTP_BAD_REQUEST , "Invalid XML format." , src_exception = e ) # If dumps of the body are desired, then this is the place to do it pretty: if environ . get ( "wsgidav.dump_request_body" ) : _logger . info ( "{} XML request body:\n{}" . format ( environ [ "REQUEST_METHOD" ] , compat . to_native ( xml_to_bytes ( rootEL , pretty_print = True ) ) , ) ) environ [ "wsgidav.dump_request_body" ] = False return rootEL
3459	def _multi_deletion ( model , entity , element_lists , method = "fba" , solution = None , processes = None , * * kwargs ) : solver = sutil . interface_to_str ( model . problem . __name__ ) if method == "moma" and solver not in sutil . qp_solvers : raise RuntimeError ( "Cannot use MOMA since '{}' is not QP-capable." "Please choose a different solver or use FBA only." . format ( solver ) ) if processes is None : processes = CONFIGURATION . processes with model : if "moma" in method : add_moma ( model , solution = solution , linear = "linear" in method ) elif "room" in method : add_room ( model , solution = solution , linear = "linear" in method , * * kwargs ) args = set ( [ frozenset ( comb ) for comb in product ( * element_lists ) ] ) processes = min ( processes , len ( args ) ) def extract_knockout_results ( result_iter ) : result = pd . DataFrame ( [ ( frozenset ( ids ) , growth , status ) for ( ids , growth , status ) in result_iter ] , columns = [ 'ids' , 'growth' , 'status' ] ) result . set_index ( 'ids' , inplace = True ) return result if processes > 1 : worker = dict ( gene = _gene_deletion_worker , reaction = _reaction_deletion_worker ) [ entity ] chunk_size = len ( args ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , ) ) results = extract_knockout_results ( pool . imap_unordered ( worker , args , chunksize = chunk_size ) ) pool . close ( ) pool . join ( ) else : worker = dict ( gene = _gene_deletion , reaction = _reaction_deletion ) [ entity ] results = extract_knockout_results ( map ( partial ( worker , model ) , args ) ) return results
8823	def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
5884	def get_title ( self ) : title = '' # rely on opengraph in case we have the data if "title" in list ( self . article . opengraph . keys ( ) ) : return self . clean_title ( self . article . opengraph [ 'title' ] ) elif self . article . schema and "headline" in self . article . schema : return self . clean_title ( self . article . schema [ 'headline' ] ) # try to fetch the meta headline meta_headline = self . parser . getElementsByTag ( self . article . doc , tag = "meta" , attr = "name" , value = "headline" ) if meta_headline is not None and len ( meta_headline ) > 0 : title = self . parser . getAttribute ( meta_headline [ 0 ] , 'content' ) return self . clean_title ( title ) # otherwise use the title meta title_element = self . parser . getElementsByTag ( self . article . doc , tag = 'title' ) if title_element is not None and len ( title_element ) > 0 : title = self . parser . getText ( title_element [ 0 ] ) return self . clean_title ( title ) return title
5294	def get_params_for_field ( self , field_name , sort_type = None ) : if not sort_type : if self . initial_sort == field_name : sort_type = 'desc' if self . initial_sort_type == 'asc' else 'asc' else : sort_type = 'asc' self . initial_params [ self . sort_param_name ] = self . sort_fields [ field_name ] self . initial_params [ self . sort_type_param_name ] = sort_type return '?%s' % self . initial_params . urlencode ( )
2809	def convert_constant ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting constant ...' ) params_list = params [ 'value' ] . numpy ( ) def target_layer ( x , value = params_list ) : return tf . constant ( value . tolist ( ) , shape = value . shape ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name + '_np' ] = params_list # ad-hoc layers [ scope_name ] = lambda_layer ( layers [ list ( layers . keys ( ) ) [ 0 ] ] )
12948	def reload ( self , cascadeObjects = True ) : _id = self . _id if not _id : raise KeyError ( 'Object has never been saved! Cannot reload.' ) currentData = self . asDict ( False , forStorage = False ) # Get the object, and compare the unconverted "asDict" repr. # If any changes, we will apply the already-convered value from # the object, but we compare the unconverted values (what's in the DB). newDataObj = self . objects . get ( _id ) if not newDataObj : raise KeyError ( 'Object with id=%d is not in database. Cannot reload.' % ( _id , ) ) newData = newDataObj . asDict ( False , forStorage = False ) if currentData == newData and not self . foreignFields : return [ ] updatedFields = { } for thisField , newValue in newData . items ( ) : defaultValue = thisField . getDefaultValue ( ) currentValue = currentData . get ( thisField , defaultValue ) fieldIsUpdated = False if currentValue != newValue : fieldIsUpdated = True elif cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : # If we are cascading objects, and at this point the pk is the same if currentValue . isFetched ( ) : # If we have fetched the current set, we might need to update (pks already match) oldObjs = currentValue . getObjs ( ) newObjs = newValue . getObjs ( ) if oldObjs != newObjs : # This will check using __eq__, so one-level including pk fieldIsUpdated = True else : # Use hasSameValues with cascadeObjects=True to scan past one level for i in range ( len ( oldObjs ) ) : if not oldObjs [ i ] . hasSameValues ( newObjs [ i ] , cascadeObjects = True ) : fieldIsUpdated = True break if fieldIsUpdated is True : # Use "converted" values in the updatedFields dict, and apply on the object. updatedFields [ thisField ] = ( currentValue , newValue ) setattr ( self , thisField , newValue ) self . _origData [ thisField ] = newDataObj . _origData [ thisField ] return updatedFields
5200	def Select ( self , command , index ) : OutstationApplication . process_point_value ( 'Select' , command , index , None ) return opendnp3 . CommandStatus . SUCCESS
1852	def SAR ( cpu , dest , src ) : OperandSize = dest . size countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] count = src . read ( ) & countMask value = dest . read ( ) res = Operators . SAR ( OperandSize , value , Operators . ZEXTEND ( count , OperandSize ) ) dest . write ( res ) SIGN_MASK = ( 1 << ( OperandSize - 1 ) ) # We can't use this one as the 'true' expression gets eagerly calculated even on count == 0 + cpu.CF = Operators.ITE(count!=0, ((value >> Operators.ZEXTEND(count-1, OperandSize)) & 1) !=0, cpu.CF) # cpu.CF = Operators.ITE(count!=0, ((value >> Operators.ZEXTEND(count-1, OperandSize)) & 1) !=0, cpu.CF) if issymbolic ( count ) : # We can't use this one as the EXTRACT op needs the offset arguments to be concrete # cpu.CF = Operators.ITE(count!=0, Operands.EXTRACT(value,count-1,1) !=0, cpu.CF) cpu . CF = Operators . ITE ( Operators . AND ( count != 0 , count <= OperandSize ) , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : if count > OperandSize : count = OperandSize cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 # on count == 0 AF is unaffected, for count > 0, AF is undefined. # in either case, do not touch AF cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count == 1 , False , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
5768	def _advapi32_interpret_dsa_key_blob ( bit_size , public_blob , private_blob ) : len1 = 20 len2 = bit_size // 8 q_offset = len2 g_offset = q_offset + len1 x_offset = g_offset + len2 y_offset = x_offset p = int_from_bytes ( private_blob [ 0 : q_offset ] [ : : - 1 ] ) q = int_from_bytes ( private_blob [ q_offset : g_offset ] [ : : - 1 ] ) g = int_from_bytes ( private_blob [ g_offset : x_offset ] [ : : - 1 ] ) x = int_from_bytes ( private_blob [ x_offset : x_offset + len1 ] [ : : - 1 ] ) y = int_from_bytes ( public_blob [ y_offset : y_offset + len2 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'public_key' : core . Integer ( y ) , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'private_key' : core . Integer ( x ) , } ) return ( public_key_info , private_key_info )
7370	def permission_check ( data , command_permissions , command = None , permissions = None ) : if permissions : pass elif command : if hasattr ( command , 'permissions' ) : permissions = command . permissions else : return True # true if no permission is required else : msg = "{name} must be called with command or permissions argument" raise RuntimeError ( msg . format ( name = "_permission_check" ) ) return any ( data [ 'sender' ] [ 'id' ] in command_permissions [ permission ] for permission in permissions if permission in command_permissions )
8739	def _allocate_from_v6_subnet ( self , context , net_id , subnet , port_id , reuse_after , ip_address = None , * * kwargs ) : LOG . info ( "Attempting to allocate a v6 address - [{0}]" . format ( utils . pretty_kwargs ( network_id = net_id , subnet = subnet , port_id = port_id , ip_address = ip_address ) ) ) if ip_address : LOG . info ( "IP %s explicitly requested, deferring to standard " "allocation" % ip_address ) return self . _allocate_from_subnet ( context , net_id = net_id , subnet = subnet , port_id = port_id , reuse_after = reuse_after , ip_address = ip_address , * * kwargs ) else : mac = kwargs . get ( "mac_address" ) if mac : mac = kwargs [ "mac_address" ] . get ( "address" ) if subnet and subnet [ "ip_policy" ] : ip_policy_cidrs = subnet [ "ip_policy" ] . get_cidrs_ip_set ( ) else : ip_policy_cidrs = netaddr . IPSet ( [ ] ) for tries , ip_address in enumerate ( generate_v6 ( mac , port_id , subnet [ "cidr" ] ) ) : LOG . info ( "Attempt {0} of {1}" . format ( tries + 1 , CONF . QUARK . v6_allocation_attempts ) ) if tries > CONF . QUARK . v6_allocation_attempts - 1 : LOG . info ( "Exceeded v6 allocation attempts, bailing" ) raise ip_address_failure ( net_id ) ip_address = netaddr . IPAddress ( ip_address ) . ipv6 ( ) LOG . info ( "Generated a new v6 address {0}" . format ( str ( ip_address ) ) ) if ( ip_policy_cidrs is not None and ip_address in ip_policy_cidrs ) : LOG . info ( "Address {0} excluded by policy" . format ( str ( ip_address ) ) ) continue try : with context . session . begin ( ) : address = db_api . ip_address_create ( context , address = ip_address , subnet_id = subnet [ "id" ] , version = subnet [ "ip_version" ] , network_id = net_id , address_type = kwargs . get ( 'address_type' , ip_types . FIXED ) ) return address except db_exception . DBDuplicateEntry : # This shouldn't ever happen, since we hold a unique MAC # address from the previous IPAM step. LOG . info ( "{0} exists but was already " "allocated" . format ( str ( ip_address ) ) ) LOG . debug ( "Duplicate entry found when inserting subnet_id" " %s ip_address %s" , subnet [ "id" ] , ip_address )
12388	def set ( self , target , value ) : if not self . _set : return if self . path is None : # There is no path defined on this resource. # We can do no magic to set the value. self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : # Attempt to resolve access to this attribute. self . get ( target ) if self . _segments [ target . __class__ ] : # Attribute is not fully resolved; an interim segment is null. return # Resolve access to the parent object. # For a single-segment path this will effectively be a no-op. parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) # Make the setter. func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) # Apply the setter now. func ( target , value ) # Replace this function with the constructed setter. def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
8833	def soft_equals ( a , b ) : if isinstance ( a , str ) or isinstance ( b , str ) : return str ( a ) == str ( b ) if isinstance ( a , bool ) or isinstance ( b , bool ) : return bool ( a ) is bool ( b ) return a == b
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
13855	def getTextFromNode ( node ) : t = "" for n in node . childNodes : if n . nodeType == n . TEXT_NODE : t += n . nodeValue else : raise NotTextNodeError return t
12031	def get_protocol_sequence ( self , sweep ) : self . setsweep ( sweep ) return list ( self . protoSeqX ) , list ( self . protoSeqY )
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
9470	def conference_list_members ( self , call_params ) : path = '/' + self . api_version + '/ConferenceListMembers/' method = 'POST' return self . request ( path , method , call_params )
10797	def users ( ) : from invenio_groups . models import Group , Membership , PrivacyPolicy , SubscriptionPolicy admin = accounts . datastore . create_user ( email = 'admin@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) reader = accounts . datastore . create_user ( email = 'reader@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False # Just for testing purposes, convert it to an IINField and display the contents of the two bytes. iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
2191	def expired ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = self . _get_certificate ( cfgstr = cfgstr ) if certificate is None : # We dont have a certificate, so we are expired is_expired = True elif products is None : # We dont have a product to check, so assume not expired is_expired = False elif not all ( map ( os . path . exists , products ) ) : # We are expired if the expected product does not exist is_expired = True else : # We are expired if the hash of the existing product data # does not match the expected hash in the certificate product_file_hash = self . _product_file_hash ( products ) certificate_hash = certificate . get ( 'product_file_hash' , None ) is_expired = product_file_hash != certificate_hash return is_expired
4901	def get_course_enrollments ( self , enterprise_customer , days ) : return CourseEnrollment . objects . filter ( created__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : # Initial logbook menu can add additional menus, all others can only remove themselves. QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
908	def handleInputRecord ( self , inputRecord ) : assert inputRecord , "Invalid inputRecord: %r" % inputRecord results = self . __phaseManager . handleInputRecord ( inputRecord ) metrics = self . __metricsMgr . update ( results ) # Execute task-postIter callbacks for cb in self . __userCallbacks [ 'postIter' ] : cb ( self . __model ) results . metrics = metrics # Return the input and predictions for this record return results
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
393	def discount_episode_rewards ( rewards = None , gamma = 0.99 , mode = 0 ) : if rewards is None : raise Exception ( "rewards should be a list" ) discounted_r = np . zeros_like ( rewards , dtype = np . float32 ) running_add = 0 for t in reversed ( xrange ( 0 , rewards . size ) ) : if mode == 0 : if rewards [ t ] != 0 : running_add = 0 running_add = running_add * gamma + rewards [ t ] discounted_r [ t ] = running_add return discounted_r
11299	def register ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s is not a subclass of BaseProvider' % provider_class . __name__ ) if provider_class in self . _registered_providers : raise AlreadyRegistered ( '%s is already registered' % provider_class . __name__ ) if issubclass ( provider_class , DjangoProvider ) : # set up signal handler for cache invalidation signals . post_save . connect ( self . invalidate_stored_oembeds , sender = provider_class . _meta . model ) # don't build the regex yet - if not all urlconfs have been loaded # and processed at this point, the DjangoProvider instances will fail # when attempting to reverse urlpatterns that haven't been created. # Rather, the regex-list will be populated once, on-demand. self . _registered_providers . append ( provider_class ) # flag for re-population self . invalidate_providers ( )
8028	def groupify ( function ) : @ wraps ( function ) def wrapper ( paths , * args , * * kwargs ) : # pylint: disable=missing-docstring groups = { } for path in paths : key = function ( path , * args , * * kwargs ) if key is not None : groups . setdefault ( key , set ( ) ) . add ( path ) return groups return wrapper
960	def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : # Create a path by joining the filename with our local json schema root jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , "jsonschema" , opfJsonSchemaFilename ) # Validate jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
11705	def reproduce_asexually ( self , egg_word , sperm_word ) : egg = self . generate_gamete ( egg_word ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) # Eliminate duplicates self . generation = 1 self . divinity = god
1227	def tf_loss_per_instance ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : raise NotImplementedError
3310	def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
2687	def get_library_config ( name ) : try : proc = Popen ( [ 'pkg-config' , '--cflags' , '--libs' , name ] , stdout = PIPE , stderr = PIPE ) except OSError : print ( 'pkg-config is required for building PyAV' ) exit ( 1 ) raw_cflags , err = proc . communicate ( ) if proc . wait ( ) : return known , unknown = parse_cflags ( raw_cflags . decode ( 'utf8' ) ) if unknown : print ( "pkg-config returned flags we don't understand: {}" . format ( unknown ) ) exit ( 1 ) return known
838	def infer ( self , inputPattern , computeScores = True , overCategories = True , partitionId = None ) : # Calculate sparsity. If sparsity is too low, we do not want to run # inference with this vector sparsity = 0.0 if self . minSparsity > 0.0 : sparsity = ( float ( len ( inputPattern . nonzero ( ) [ 0 ] ) ) / len ( inputPattern ) ) if len ( self . _categoryList ) == 0 or sparsity < self . minSparsity : # No categories learned yet; i.e. first inference w/ online learning or # insufficient sparsity winner = None inferenceResult = numpy . zeros ( 1 ) dist = numpy . ones ( 1 ) categoryDist = numpy . ones ( 1 ) else : maxCategoryIdx = max ( self . _categoryList ) inferenceResult = numpy . zeros ( maxCategoryIdx + 1 ) dist = self . _getDistances ( inputPattern , partitionId = partitionId ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) # Loop through the indices of the nearest neighbors. if self . exact : # Is there an exact match in the distances? exactMatches = numpy . where ( dist < 0.00001 ) [ 0 ] if len ( exactMatches ) > 0 : for i in exactMatches [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ i ] ] += 1.0 else : sorted = dist . argsort ( ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 # Prepare inference results. if inferenceResult . any ( ) : winner = inferenceResult . argmax ( ) inferenceResult /= inferenceResult . sum ( ) else : winner = None categoryDist = min_score_per_category ( maxCategoryIdx , self . _categoryList , dist ) categoryDist . clip ( 0 , 1.0 , categoryDist ) if self . verbosity >= 1 : print "%s infer:" % ( g_debugPrefix ) print " active inputs:" , _labeledInput ( inputPattern , cellsPerCol = self . cellsPerCol ) print " winner category:" , winner print " pct neighbors of each category:" , inferenceResult print " dist of each prototype:" , dist print " dist of each category:" , categoryDist result = ( winner , inferenceResult , dist , categoryDist ) return result
3269	def md_jdbc_virtual_table ( key , node ) : name = node . find ( "name" ) sql = node . find ( "sql" ) escapeSql = node . find ( "escapeSql" ) escapeSql = escapeSql . text if escapeSql is not None else None keyColumn = node . find ( "keyColumn" ) keyColumn = keyColumn . text if keyColumn is not None else None n_g = node . find ( "geometry" ) geometry = JDBCVirtualTableGeometry ( n_g . find ( "name" ) , n_g . find ( "type" ) , n_g . find ( "srid" ) ) parameters = [ ] for n_p in node . findall ( "parameter" ) : p_name = n_p . find ( "name" ) p_defaultValue = n_p . find ( "defaultValue" ) p_defaultValue = p_defaultValue . text if p_defaultValue is not None else None p_regexpValidator = n_p . find ( "regexpValidator" ) p_regexpValidator = p_regexpValidator . text if p_regexpValidator is not None else None parameters . append ( JDBCVirtualTableParam ( p_name , p_defaultValue , p_regexpValidator ) ) return JDBCVirtualTable ( name , sql , escapeSql , geometry , keyColumn , parameters )
4487	def update ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) url = self . _upload_url # peek at the file to check if it is an ampty file which needs special # handling in requests. If we pass a file like object to data that # turns out to be of length zero then no file is created on the OSF if fp . peek ( 1 ) : response = self . _put ( url , data = fp ) else : response = self . _put ( url , data = b'' ) if response . status_code != 200 : msg = ( 'Could not update {} (status ' 'code: {}).' . format ( self . path , response . status_code ) ) raise RuntimeError ( msg )
1405	def validate_extra_link ( self , extra_link ) : if EXTRA_LINK_NAME_KEY not in extra_link or EXTRA_LINK_FORMATTER_KEY not in extra_link : raise Exception ( "Invalid extra.links format. " + "Extra link must include a 'name' and 'formatter' field" ) self . validated_formatter ( extra_link [ EXTRA_LINK_FORMATTER_KEY ] ) return extra_link
2239	def _syspath_modname_to_modpath ( modname , sys_path = None , exclude = None ) : def _isvalid ( modpath , base ) : # every directory up to the module, should have an init subdir = dirname ( modpath ) while subdir and subdir != base : if not exists ( join ( subdir , '__init__.py' ) ) : return False subdir = dirname ( subdir ) return True _fname_we = modname . replace ( '.' , os . path . sep ) candidate_fnames = [ _fname_we + '.py' , # _fname_we + '.pyc', # _fname_we + '.pyo', ] # Add extension library suffixes candidate_fnames += [ _fname_we + ext for ext in _platform_pylib_exts ( ) ] if sys_path is None : sys_path = sys . path # the empty string in sys.path indicates cwd. Change this to a '.' candidate_dpaths = [ '.' if p == '' else p for p in sys_path ] if exclude : def normalize ( p ) : if sys . platform . startswith ( 'win32' ) : # nocover return realpath ( p ) . lower ( ) else : return realpath ( p ) # Keep only the paths not in exclude real_exclude = { normalize ( p ) for p in exclude } candidate_dpaths = [ p for p in candidate_dpaths if normalize ( p ) not in real_exclude ] for dpath in candidate_dpaths : # Check for directory-based modules (has presidence over files) modpath = join ( dpath , _fname_we ) if exists ( modpath ) : if isfile ( join ( modpath , '__init__.py' ) ) : if _isvalid ( modpath , dpath ) : return modpath # If that fails, check for file-based modules for fname in candidate_fnames : modpath = join ( dpath , fname ) if isfile ( modpath ) : if _isvalid ( modpath , dpath ) : return modpath
10225	def get_correlation_graph ( graph : BELGraph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE_RELATIONS : continue if not result . has_edge ( u , v ) : result . add_edge ( u , v , * * { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
6915	def collection_worker ( task ) : lcfile , outdir , kwargs = task try : fakelcresults = make_fakelc ( lcfile , outdir , * * kwargs ) return fakelcresults except Exception as e : LOGEXCEPTION ( 'could not process %s into a fakelc' % lcfile ) return None
6486	def course_discovery ( request ) : results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : size , from_ , page = _process_pagination_values ( request ) field_dictionary = _process_field_values ( request ) # Analytics - log search request track . emit ( 'edx.course_discovery.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = course_discovery_search ( search_term = search_term , size = size , from_ = from_ , field_dictionary = field_dictionary , ) # Analytics - log search results before sending to browser track . emit ( 'edx.course_discovery.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) status_code = 200 except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } # Allow for broad exceptions here - this is an entry point from external reference except Exception as err : # pylint: disable=broad-except results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
3279	def add_provider ( self , share , provider , readonly = False ) : # Make sure share starts with, or is '/' share = "/" + share . strip ( "/" ) assert share not in self . provider_map if compat . is_basestring ( provider ) : # Syntax: # <mount_path>: <folder_path> # We allow a simple string as 'provider'. In this case we interpret # it as a file system root folder that is published. provider = FilesystemProvider ( provider , readonly ) elif type ( provider ) in ( dict , ) : if "provider" in provider : # Syntax: # <mount_path>: {"provider": <class_path>, "args": <pos_args>, "kwargs": <named_args} prov_class = dynamic_import_class ( provider [ "provider" ] ) provider = prov_class ( * provider . get ( "args" , [ ] ) , * * provider . get ( "kwargs" , { } ) ) else : # Syntax: # <mount_path>: {"root": <path>, "redaonly": <bool>} provider = FilesystemProvider ( provider [ "root" ] , bool ( provider . get ( "readonly" , False ) ) ) elif type ( provider ) in ( list , tuple ) : raise ValueError ( "Provider {}: tuple/list syntax is no longer supported" . format ( provider ) ) # provider = FilesystemProvider(provider[0], provider[1]) if not isinstance ( provider , DAVProvider ) : raise ValueError ( "Invalid provider {}" . format ( provider ) ) provider . set_share_path ( share ) if self . mount_path : provider . set_mount_path ( self . mount_path ) # TODO: someday we may want to configure different lock/prop # managers per provider provider . set_lock_manager ( self . lock_manager ) provider . set_prop_manager ( self . prop_manager ) self . provider_map [ share ] = provider # self.provider_map[share] = {"provider": provider, "allow_anonymous": False} # Store the list of share paths, ordered by length, so route lookups # will return the most specific match self . sorted_share_list = [ s . lower ( ) for s in self . provider_map . keys ( ) ] self . sorted_share_list = sorted ( self . sorted_share_list , key = len , reverse = True ) return provider
8688	def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self . get ( key_name ) == { }
8241	def compound ( clr , flip = False ) : def _wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( 30 * d ) c . brightness = _wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate_ryb ( 30 * d ) c . saturation = _wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate_ryb ( 160 * d ) c . saturation = _wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) # colors.append(c) return colors
2614	def _write_submit_script ( self , template , script_filename , job_name , configs ) : try : submit_script = Template ( template ) . substitute ( jobname = job_name , * * configs ) # submit_script = Template(template).safe_substitute(jobname=job_name, **configs) with open ( script_filename , 'w' ) as f : f . write ( submit_script ) except KeyError as e : logger . error ( "Missing keys for submit script : %s" , e ) raise ( SchedulerMissingArgs ( e . args , self . sitename ) ) except IOError as e : logger . error ( "Failed writing to submit script: %s" , script_filename ) raise ( ScriptPathError ( script_filename , e ) ) except Exception as e : print ( "Template : " , template ) print ( "Args : " , job_name ) print ( "Kwargs : " , configs ) logger . error ( "Uncategorized error: %s" , e ) raise ( e ) return True
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
2278	def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
7188	def get_offset_and_prefix ( body , skip_assignments = False ) : assert body . type in ( syms . file_input , syms . suite ) _offset = 0 prefix = '' for _offset , child in enumerate ( body . children ) : if child . type == syms . simple_stmt : stmt = child . children [ 0 ] if stmt . type == syms . expr_stmt : expr = stmt . children if not skip_assignments : break if ( len ( expr ) != 2 or expr [ 0 ] . type != token . NAME or expr [ 1 ] . type != syms . annassign or _eq in expr [ 1 ] . children ) : break elif stmt . type not in ( syms . import_name , syms . import_from , token . STRING ) : break elif child . type == token . INDENT : assert isinstance ( child , Leaf ) prefix = child . value elif child . type != token . NEWLINE : break prefix , child . prefix = child . prefix , prefix return _offset , prefix
12396	def get_method ( self , * args , * * kwargs ) : for method in self . gen_methods ( * args , * * kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . DispatchError ( msg % ( ( args , kwargs ) , self . inst ) )
1269	def _fly ( self , board , layers , things , the_plot ) : # Disappear if we've hit a bunker. if self . character in the_plot [ 'bunker_hitters' ] : return self . _teleport ( ( - 1 , - 1 ) ) # End the game if we've hit the player. if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) self . _south ( board , the_plot )
10479	def _getActions ( self ) : actions = _a11y . AXUIElement . _getActions ( self ) # strip leading AX from actions - help distinguish them from attributes return [ action [ 2 : ] for action in actions ]
6575	def from_json_list ( cls , api_client , data ) : return [ cls . from_json ( api_client , item ) for item in data ]
1285	def footnote_item ( self , key , text ) : back = ( '<a href="#fnref-%s" class="footnote">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id="fn-%s">%s</li>\n' % ( escape ( key ) , text ) return html
6508	def generate_field_filters ( cls , * * kwargs ) : generator = _load_class ( getattr ( settings , "SEARCH_FILTER_GENERATOR" , None ) , cls ) ( ) return ( generator . field_dictionary ( * * kwargs ) , generator . filter_dictionary ( * * kwargs ) , generator . exclude_dictionary ( * * kwargs ) , )
11044	def create_marathon_acme ( client_creator , cert_store , acme_email , allow_multiple_certs , marathon_addrs , marathon_timeout , sse_timeout , mlb_addrs , group , reactor ) : marathon_client = MarathonClient ( marathon_addrs , timeout = marathon_timeout , sse_kwargs = { 'timeout' : sse_timeout } , reactor = reactor ) marathon_lb_client = MarathonLbClient ( mlb_addrs , reactor = reactor ) return MarathonAcme ( marathon_client , group , cert_store , marathon_lb_client , client_creator , reactor , acme_email , allow_multiple_certs )
11200	def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) # Get transitions - if there are none, fixed offset transitions = self . transitions ( dt . year ) if transitions is None : return dt + self . utcoffset ( dt ) # Get the transition times in UTC dston , dstoff = transitions dston -= self . _std_offset dstoff -= self . _std_offset utc_transitions = ( dston , dstoff ) dt_utc = dt . replace ( tzinfo = None ) isdst = self . _naive_isdst ( dt_utc , utc_transitions ) if isdst : dt_wall = dt + self . _dst_offset else : dt_wall = dt + self . _std_offset _fold = int ( not isdst and self . is_ambiguous ( dt_wall ) ) return enfold ( dt_wall , fold = _fold )
9201	def extract_cycles ( series , left = False , right = False ) : points = deque ( ) for x in reversals ( series , left = left , right = right ) : points . append ( x ) while len ( points ) >= 3 : # Form ranges X and Y from the three most recent points X = abs ( points [ - 2 ] - points [ - 1 ] ) Y = abs ( points [ - 3 ] - points [ - 2 ] ) if X < Y : # Read the next point break elif len ( points ) == 3 : # Y contains the starting point # Count Y as one-half cycle and discard the first point yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( ) else : # Count Y as one cycle and discard the peak and the valley of Y yield points [ - 3 ] , points [ - 2 ] , 1.0 last = points . pop ( ) points . pop ( ) points . pop ( ) points . append ( last ) else : # Count the remaining ranges as one-half cycles while len ( points ) > 1 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( )
3798	def setup_a_alpha_and_derivatives ( self , i , T = None ) : self . a , self . Tc , self . S1 , self . S2 = self . ais [ i ] , self . Tcs [ i ] , self . S1s [ i ] , self . S2s [ i ]
2368	def type ( self ) : robot_tables = [ table for table in self . tables if not isinstance ( table , UnknownTable ) ] if len ( robot_tables ) == 0 : return None for table in self . tables : if isinstance ( table , TestcaseTable ) : return "suite" return "resource"
13442	def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) #Copy base from cloud to local util . copy ( ccat , lcat ) #Apply changesets cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) # Write meta-data both to local and cloud mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) #Let's copy Smart Previews if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) #Finally, let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
11465	def cd ( self , folder ) : if folder . startswith ( '/' ) : self . _ftp . cwd ( folder ) else : for subfolder in folder . split ( '/' ) : if subfolder : self . _ftp . cwd ( subfolder )
8213	def export_svg ( self ) : d = "" if len ( self . _points ) > 0 : d += "M " + str ( self . _points [ 0 ] . x ) + " " + str ( self . _points [ 0 ] . y ) + " " for pt in self . _points : if pt . cmd == MOVETO : d += "M " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == LINETO : d += "L " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == CURVETO : d += "C " d += str ( pt . ctrl1 . x ) + " " + str ( pt . ctrl1 . y ) + " " d += str ( pt . ctrl2 . x ) + " " + str ( pt . ctrl2 . y ) + " " d += str ( pt . x ) + " " + str ( pt . y ) + " " c = "rgb(" c += str ( int ( self . path_color . r * 255 ) ) + "," c += str ( int ( self . path_color . g * 255 ) ) + "," c += str ( int ( self . path_color . b * 255 ) ) + ")" s = '<?xml version="1.0"?>\n' s += '<svg width="' + str ( _ctx . WIDTH ) + 'pt" height="' + str ( _ctx . HEIGHT ) + 'pt">\n' s += '<g>\n' s += '<path d="' + d + '" fill="none" stroke="' + c + '" stroke-width="' + str ( self . strokewidth ) + '" />\n' s += '</g>\n' s += '</svg>\n' f = open ( self . file + ".svg" , "w" ) f . write ( s ) f . close ( )
4900	def handle ( self , * args , * * options ) : if not CourseEnrollment : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) days , enterprise_customer = self . parse_arguments ( * args , * * options ) if enterprise_customer : try : lrs_configuration = XAPILRSConfiguration . objects . get ( active = True , enterprise_customer = enterprise_customer ) except XAPILRSConfiguration . DoesNotExist : raise CommandError ( 'No xAPI Configuration found for "{enterprise_customer}"' . format ( enterprise_customer = enterprise_customer . name ) ) # Send xAPI analytics data to the configured LRS self . send_xapi_statements ( lrs_configuration , days ) else : for lrs_configuration in XAPILRSConfiguration . objects . filter ( active = True ) : self . send_xapi_statements ( lrs_configuration , days )
11013	def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except SubprocessError : context . exit ( 1 )
2841	def write_iodir ( self , iodir = None ) : if iodir is not None : self . iodir = iodir self . _device . writeList ( self . IODIR , self . iodir )
9304	def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { # RFC 7231, e.g. 'Mon, 09 Sep 2011 23:36:00 GMT' r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , # RFC 850 (e.g. Sunday, 06-Nov-94 08:49:37 GMT) # assumes current century r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , # C time, e.g. 'Wed Dec 4 00:00:00 2002' r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , # x-amz-date format dates, e.g. 20100325T010101Z r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , # ISO 8601 / RFC 3339, e.g. '2009-03-25T10:11:12.13-01:00' r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date
6847	def purge_keys ( self ) : r = self . local_renderer r . env . default_ip = self . hostname_to_ip ( self . env . default_hostname ) r . env . home_dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {host_string}' ) if self . env . default_hostname : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_hostname}' ) if r . env . default_ip : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_ip}' )
4829	def _get_results ( self , identity_provider , param_name , param_value , result_field_name ) : try : kwargs = { param_name : param_value } returned = self . client . providers ( identity_provider ) . users . get ( * * kwargs ) results = returned . get ( 'results' , [ ] ) except HttpNotFoundError : LOGGER . error ( 'username not found for third party provider={provider}, {querystring_param}={id}' . format ( provider = identity_provider , querystring_param = param_name , id = param_value ) ) results = [ ] for row in results : if row . get ( param_name ) == param_value : return row . get ( result_field_name ) return None
230	def plot_style_factor_exposures ( tot_style_factor_exposure , factor_name = None , ax = None ) : if ax is None : ax = plt . gca ( ) if factor_name is None : factor_name = tot_style_factor_exposure . name ax . plot ( tot_style_factor_exposure . index , tot_style_factor_exposure , label = factor_name ) avg = tot_style_factor_exposure . mean ( ) ax . axhline ( avg , linestyle = '-.' , label = 'Mean = {:.3}' . format ( avg ) ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) _ , _ , y1 , y2 = plt . axis ( ) lim = max ( abs ( y1 ) , abs ( y2 ) ) ax . set ( title = 'Exposure to {}' . format ( factor_name ) , ylabel = '{} \n weighted exposure' . format ( factor_name ) , ylim = ( - lim , lim ) ) ax . legend ( frameon = True , framealpha = 0.5 ) return ax
150	def deepcopy ( self ) : # Manual copy is far faster than deepcopy for PolygonsOnImage, # so use manual copy here too polys = [ poly . deepcopy ( ) for poly in self . polygons ] return PolygonsOnImage ( polys , tuple ( self . shape ) )
935	def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model
12933	def as_dict ( self , * args , * * kwargs ) : self_as_dict = super ( ClinVarAllele , self ) . as_dict ( * args , * * kwargs ) self_as_dict [ 'hgvs' ] = self . hgvs self_as_dict [ 'clnalleleid' ] = self . clnalleleid self_as_dict [ 'clnsig' ] = self . clnsig self_as_dict [ 'clndn' ] = self . clndn self_as_dict [ 'clndisdb' ] = self . clndisdb self_as_dict [ 'clnvi' ] = self . clnvi return self_as_dict
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
2232	def _register_numpy_extensions ( self ) : # system checks import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : # nocover numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : # ndarrays of objects cannot be hashed directly. return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : """ Example: >>> import ubelt as ub >>> if not ub.modname_to_modpath('numpy'): ... raise pytest.skip() >>> import numpy as np >>> data_f32 = np.zeros((3, 3, 3), dtype=np.float64) >>> data_i64 = np.zeros((3, 3, 3), dtype=np.int64) >>> data_i32 = np.zeros((3, 3, 3), dtype=np.int32) >>> hash_f64 = _hashable_sequence(data_f32, types=True) >>> hash_i64 = _hashable_sequence(data_i64, types=True) >>> hash_i32 = _hashable_sequence(data_i64, types=True) >>> assert hash_i64 != hash_f64 >>> assert hash_i64 != hash_i32 """ if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : # tobytes() views the array in 1D (via ravel()) # encode the shape as well header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : """ Example: >>> import ubelt as ub >>> if not ub.modname_to_modpath('numpy'): ... raise pytest.skip() >>> import numpy as np >>> rng = np.random.RandomState(0) >>> _hashable_sequence(rng, types=True) """ hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
3660	def _coeff_ind_from_T ( self , T ) : # DO NOT CHANGE if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
10125	def flip_y ( self , center = None ) : if center is None : self . poly . flop ( ) else : self . poly . flop ( center [ 1 ] ) return self
13468	def normalize_slice ( slice_obj , length ) : if isinstance ( slice_obj , slice ) : start , stop , step = slice_obj . start , slice_obj . stop , slice_obj . step if start is None : start = 0 if stop is None : stop = length if step is None : step = 1 if start < 0 : start += length if stop < 0 : stop += length elif isinstance ( slice_obj , int ) : start = slice_obj if start < 0 : start += length stop = start + 1 step = 1 else : raise TypeError if ( 0 <= start <= length ) and ( 0 <= stop <= length ) : return start , stop , step raise IndexError
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
4278	def process_dir ( self , album , force = False ) : for f in album : if isfile ( f . dst_path ) and not force : self . logger . info ( "%s exists - skipping" , f . filename ) self . stats [ f . type + '_skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src_path , album . dst_path , self . settings )
8254	def _context ( self ) : tags1 = None for clr in self : overlap = [ ] if clr . is_black : name = "black" elif clr . is_white : name = "white" elif clr . is_grey : name = "grey" else : name = clr . nearest_hue ( primary = True ) if name == "orange" and clr . brightness < 0.6 : name = "brown" tags2 = context [ name ] if tags1 is None : tags1 = tags2 else : for tag in tags2 : if tag in tags1 : if tag not in overlap : overlap . append ( tag ) tags1 = overlap overlap . sort ( ) return overlap
3637	def clubStaff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . __request__ ( method , url ) return rc
8575	def update_nic ( self , datacenter_id , server_id , nic_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
1752	def _reg_name ( self , reg_id ) : if reg_id >= X86_REG_ENDING : logger . warning ( "Trying to get register name for a non-register" ) return None cs_reg_name = self . cpu . instruction . reg_name ( reg_id ) if cs_reg_name is None or cs_reg_name . lower ( ) == '(invalid)' : return None return self . cpu . _regfile . _alias ( cs_reg_name . upper ( ) )
9781	def get ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : response = PolyaxonClient ( ) . build_job . get_build ( user , project_name , _build ) cache . cache ( config_manager = BuildJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_build_details ( response )
7514	def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## snps was created using only the selected samples. LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## the 2nd read edges are +5 for the spacer seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq1 == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 ## select the remaining names in order seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) #LOGGER.info("s1 %s", s1.tostring()) #LOGGER.info("s2 %s", s2.tostring()) ## get snp string and add to store snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] #npis = str(snpstring1+snpstring2).count("*") #nvars = str(snpstring1+snpstring2).count("-") + npis outstr += "\n" + snppad + "" . join ( snpstring1 ) + " " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) #"|LOCID={},DBID={},NVAR={},NPIS={}|"\ #.format(1+iloc+start, iloc, nvars, npis) return outstr , samplecov , locuscov
13757	def get_path_extension ( path ) : file_path , file_ext = os . path . splitext ( path ) return file_ext . lstrip ( '.' )
9731	def get_force_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlateSingle , data , component_position ) component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) append_components ( ( plate , force ) ) return components
10310	def safe_add_edge ( graph , u , v , key , attr_dict , * * attr ) : if key < 0 : graph . add_edge ( u , v , key = key , attr_dict = attr_dict , * * attr ) else : graph . add_edge ( u , v , attr_dict = attr_dict , * * attr )
13795	def handle_reduce ( self , reduce_function_names , mapped_docs ) : reduce_functions = [ ] # This gets a large list of reduction functions, given their names. for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , * * kwargs : None ) # Transform lots of (key, value) pairs into one (keys, values) pair. keys , values = zip ( ( key , value ) for ( ( key , doc_id ) , value ) in mapped_docs ) # This gets the list of results from the reduction functions. results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
140	def to_keypoints ( self ) : # TODO get rid of this deferred import from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = point [ 0 ] , y = point [ 1 ] ) for point in self . exterior ]
9441	def reload_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadConfig/' method = 'POST' return self . request ( path , method , call_params )
11047	def _parse_field_value ( line ) : if line . startswith ( ':' ) : # Ignore the line return None , None if ':' not in line : # Treat the entire line as the field, use empty string as value return line , '' # Else field is before the ':' and value is after field , value = line . split ( ':' , 1 ) # If value starts with a space, remove it. value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
11052	def sync ( self ) : self . log . info ( 'Starting a sync...' ) def log_success ( result ) : self . log . info ( 'Sync completed successfully' ) return result def log_failure ( failure ) : self . log . failure ( 'Sync failed' , failure , LogLevel . error ) return failure return ( self . marathon_client . get_apps ( ) . addCallback ( self . _apps_acme_domains ) . addCallback ( self . _filter_new_domains ) . addCallback ( self . _issue_certs ) . addCallbacks ( log_success , log_failure ) )
4502	def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
6774	def deploy ( self , site = None ) : r = self . local_renderer self . deploy_logrotate ( ) cron_crontabs = [ ] # if self.verbose: # print('hostname: "%s"' % (hostname,), file=sys.stderr) for _site , site_data in self . iter_sites ( site = site ) : r . env . cron_stdout_log = r . format ( r . env . stdout_log_template ) r . env . cron_stderr_log = r . format ( r . env . stderr_log_template ) r . sudo ( 'touch {cron_stdout_log}' ) r . sudo ( 'touch {cron_stderr_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stdout_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stderr_log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs_selected:' , self . env . crontabs_selected , file = sys . stderr ) for selected_crontab in self . env . crontabs_selected : lines = self . env . crontabs_available . get ( selected_crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron_crontabs . append ( r . format ( line ) ) if not cron_crontabs : return cron_crontabs = self . env . crontab_headers + cron_crontabs cron_crontabs . append ( '\n' ) r . env . crontabs_rendered = '\n' . join ( cron_crontabs ) fn = self . write_to_file ( content = r . env . crontabs_rendered ) print ( 'fn:' , fn ) r . env . put_remote_path = r . put ( local_path = fn ) if isinstance ( r . env . put_remote_path , ( tuple , list ) ) : r . env . put_remote_path = r . env . put_remote_path [ 0 ] r . sudo ( 'crontab -u {cron_user} {put_remote_path}' )
8022	def cast ( cls , fx_spot , domestic_curve = None , foreign_curve = None ) : assert domestic_curve . origin == foreign_curve . origin return cls ( fx_spot , domestic_curve = domestic_curve , foreign_curve = foreign_curve )
5965	def make_main_index ( struct , selection = '"Protein"' , ndx = 'main.ndx' , oldndx = None ) : logger . info ( "Building the main index file {ndx!r}..." . format ( * * vars ( ) ) ) # pass 1: select # get a list of groups # need the first "" to get make_ndx to spit out the group list. _ , out , _ = gromacs . make_ndx ( f = struct , n = oldndx , o = ndx , stdout = False , input = ( "" , "q" ) ) groups = cbook . parse_ndxlist ( out ) # find the matching groups, # there is a nasty bug in GROMACS where make_ndx may have multiple # groups, which caused the previous approach to fail big time. # this is a work around the make_ndx bug. # striping the "" allows compatibility with existing make_ndx selection commands. selection = selection . strip ( "\"" ) selected_groups = [ g for g in groups if g [ 'name' ] . lower ( ) == selection . lower ( ) ] if len ( selected_groups ) > 1 : logging . warn ( "make_ndx created duplicated groups, performing work around" ) if len ( selected_groups ) <= 0 : msg = "no groups found for selection {0}, available groups are {1}" . format ( selection , groups ) logging . error ( msg ) raise ValueError ( msg ) # Found at least one matching group, we're OK # index of last group last = len ( groups ) - 1 assert last == groups [ - 1 ] [ 'nr' ] group = selected_groups [ 0 ] # pass 2: # 1) last group is __main__ # 2) __environment__ is everything else (eg SOL, ions, ...) _ , out , _ = gromacs . make_ndx ( f = struct , n = ndx , o = ndx , stdout = False , # make copy selected group, this now has index last + 1 input = ( "{0}" . format ( group [ 'nr' ] ) , # rename this to __main__ "name {0} __main__" . format ( last + 1 ) , # make a complement to this group, it get index last + 2 "! \"__main__\"" , # rename this to __environment__ "name {0} __environment__" . format ( last + 2 ) , # list the groups "" , # quit "q" ) ) return cbook . parse_ndxlist ( out )
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
3094	def oauth_aware ( self , method ) : def setup_oauth ( request_handler , * args , * * kwargs ) : if self . _in_error : self . _display_error_message ( request_handler ) return user = users . get_current_user ( ) # Don't use @login_decorator as this could be used in a # POST request. if not user : request_handler . redirect ( users . create_login_url ( request_handler . request . uri ) ) return self . _create_flow ( request_handler ) self . flow . params [ 'state' ] = _build_state_value ( request_handler , user ) self . credentials = self . _storage_class ( self . _credentials_class , None , self . _credentials_property_name , user = user ) . get ( ) try : resp = method ( request_handler , * args , * * kwargs ) finally : self . credentials = None return resp return setup_oauth
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
12063	def getAvgBySweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweepLength if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dictFlat ( cm . matrixToDicts ( abf . APs ) ) : if T0 < AP [ 'sweepT' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
13602	def warn_message ( self , message , fh = None , prefix = "[warn]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stdout if fh is sys . stdout : termcolor . cprint ( msg , color = "yellow" ) else : fh . write ( msg ) pass
8553	def delete_ipblock ( self , ipblock_id ) : response = self . _perform_request ( url = '/ipblocks/' + ipblock_id , method = 'DELETE' ) return response
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) # Check timestamp is not older than ttl age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
3233	def list_rules ( client = None , * * kwargs ) : result = client . list_rules ( * * kwargs ) if not result . get ( "Rules" ) : result . update ( { "Rules" : [ ] } ) return result
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/task_00009.p.gz' return ret
1739	def in_op ( self , other ) : if not is_object ( other ) : raise MakeError ( 'TypeError' , "You can\'t use 'in' operator to search in non-objects" ) return other . has_property ( to_string ( self ) )
11041	def get_single_header ( headers , key ) : raw_headers = headers . getRawHeaders ( key ) if raw_headers is None : return None # Take the final header as the authorative header , _ = cgi . parse_header ( raw_headers [ - 1 ] ) return header
3069	def wrap_http_for_jwt_access ( credentials , http ) : orig_request_method = http . request wrap_http_for_auth ( credentials , http ) # The new value of ``http.request`` set by ``wrap_http_for_auth``. authenticated_request_method = http . request # The closure that will replace 'httplib2.Http.request'. def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if 'aud' in credentials . _kwargs : # Preemptively refresh token, this is not done for OAuth2 if ( credentials . access_token is None or credentials . access_token_expired ) : credentials . refresh ( None ) return request ( authenticated_request_method , uri , method , body , headers , redirections , connection_type ) else : # If we don't have an 'aud' (audience) claim, # create a 1-time token with the uri root as the audience headers = _initialize_headers ( headers ) _apply_user_agent ( headers , credentials . user_agent ) uri_root = uri . split ( '?' , 1 ) [ 0 ] token , unused_expiry = credentials . _create_token ( { 'aud' : uri_root } ) headers [ 'Authorization' ] = 'Bearer ' + token return request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) # Replace the request method with our own closure. http . request = new_request # Set credentials as a property of the request method. http . request . credentials = credentials
11732	def registerGoodClass ( self , class_ ) : # Class itself added to "good" list self . _valid_classes . append ( class_ ) # Recurse into any inner classes for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
12822	def _path_root ( draw , result_type ) : # Based on https://en.wikipedia.org/wiki/Path_(computing) def tp ( s = '' ) : return _str_to_path ( s , result_type ) if os . name != 'nt' : return tp ( os . sep ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) name = _filename ( result_type ) char = characters ( min_codepoint = ord ( "A" ) , max_codepoint = ord ( "z" ) ) . map ( lambda c : tp ( str ( c ) ) ) relative = sep # [drive_letter]:\ drive = builds ( lambda * x : tp ( ) . join ( x ) , char , just ( tp ( ':' ) ) , sep ) # \\?\[drive_spec]:\ extended = builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , drive ) network = one_of ( [ # \\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , name , sep , name , sep ) , # \\?\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , name , sep , name , sep ) , # \\?\UNC\[server]\[sharename]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , just ( tp ( 'UNC' ) ) , sep , name , sep , name , sep ) , # \\.\[physical_device]\ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '.' ) ) , sep , name , sep ) , ] ) final = one_of ( relative , drive , extended , network ) return draw ( final )
4226	def _data_root_Linux ( ) : fallback = os . path . expanduser ( '~/.local/share' ) root = os . environ . get ( 'XDG_DATA_HOME' , None ) or fallback return os . path . join ( root , 'python_keyring' )
13847	def splitext_files_only ( filepath ) : return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) # "-bind-to core" is crucial for good performance args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
3176	def update ( self , campaign_id , feedback_id , data ) : self . campaign_id = campaign_id self . feedback_id = feedback_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id , 'feedback' , feedback_id ) , data = data )
13434	def _setup_index ( index ) : index = int ( index ) if index > 0 : index -= 1 elif index == 0 : # Zero indicies should not be allowed by default. raise ValueError return index
6438	def dist ( self , src , tar , weights = 'exponential' , max_length = 8 ) : return self . dist_abs ( src , tar , weights , max_length , True )
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
2772	def load ( self ) : data = self . get_data ( 'load_balancers/%s' % self . id , type = GET ) load_balancer = data [ 'load_balancer' ] # Setting the attribute values for attr in load_balancer . keys ( ) : if attr == 'health_check' : health_check = HealthCheck ( * * load_balancer [ 'health_check' ] ) setattr ( self , attr , health_check ) elif attr == 'sticky_sessions' : sticky_ses = StickySesions ( * * load_balancer [ 'sticky_sessions' ] ) setattr ( self , attr , sticky_ses ) elif attr == 'forwarding_rules' : rules = list ( ) for rule in load_balancer [ 'forwarding_rules' ] : rules . append ( ForwardingRule ( * * rule ) ) setattr ( self , attr , rules ) else : setattr ( self , attr , load_balancer [ attr ] ) return self
7849	def has_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = u'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = u"d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
10791	def save_wisdom ( wisdomfile ) : if wisdomfile is None : return if wisdomfile : pickle . dump ( pyfftw . export_wisdom ( ) , open ( wisdomfile , 'wb' ) , protocol = 2 )
8515	def format_timedelta ( td_object ) : def get_total_seconds ( td ) : # timedelta.total_seconds not in py2.6 return ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 1e6 ) / 1e6 seconds = int ( get_total_seconds ( td_object ) ) periods = [ ( 'year' , 60 * 60 * 24 * 365 ) , ( 'month' , 60 * 60 * 24 * 30 ) , ( 'day' , 60 * 60 * 24 ) , ( 'hour' , 60 * 60 ) , ( 'minute' , 60 ) , ( 'second' , 1 ) ] strings = [ ] for period_name , period_seconds in periods : if seconds > period_seconds : period_value , seconds = divmod ( seconds , period_seconds ) if period_value == 1 : strings . append ( "%s %s" % ( period_value , period_name ) ) else : strings . append ( "%s %ss" % ( period_value , period_name ) ) return ", " . join ( strings )
7929	def _start_thread ( self ) : with self . lock : if self . threads and self . queue . empty ( ) : return if len ( self . threads ) >= self . max_threads : return thread_n = self . last_thread_n + 1 self . last_thread_n = thread_n thread = threading . Thread ( target = self . _run , name = "{0!r} #{1}" . format ( self , thread_n ) , args = ( thread_n , ) ) self . threads . append ( thread ) thread . daemon = True thread . start ( )
175	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : assert color is not None assert alpha is not None assert size is not None color_lines = color_lines if color_lines is not None else np . float32 ( color ) color_points = color_points if color_points is not None else np . float32 ( color ) * 0.5 alpha_lines = alpha_lines if alpha_lines is not None else np . float32 ( alpha ) alpha_points = alpha_points if alpha_points is not None else np . float32 ( alpha ) size_lines = size_lines if size_lines is not None else size size_points = size_points if size_points is not None else size * 3 image = self . draw_lines_on_image ( image , color = np . array ( color_lines ) . astype ( np . uint8 ) , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) image = self . draw_points_on_image ( image , color = np . array ( color_points ) . astype ( np . uint8 ) , alpha = alpha_points , size = size_points , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image
3525	def uservoice ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return UserVoiceNode ( )
6963	def initialize ( self , currentdir , assetpath , cplist , cplistfile , executor , readonly , baseurl ) : self . currentdir = currentdir self . assetpath = assetpath self . currentproject = cplist self . cplistfile = cplistfile self . executor = executor self . readonly = readonly self . baseurl = baseurl
1053	def extract_stack ( f = None , limit = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit list = [ ] n = 0 while f is not None and ( limit is None or n < limit ) : lineno = f . f_lineno co = f . f_code filename = co . co_filename name = co . co_name linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : line = line . strip ( ) else : line = None list . append ( ( filename , lineno , name , line ) ) f = f . f_back n = n + 1 list . reverse ( ) return list
4931	def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
5195	def main ( ) : app = OutstationApplication ( ) _log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be inserted here if desired. See outstation_cmd.py for examples. app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) # Rule 2 word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) # Rule 3 if word [ - 3 : ] == 'que' : # This diverges from the paper by also returning 'que' itself # unstemmed if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] # Base case will mean returning the words as is noun = word verb = word # Rule 4 for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 # Technically this diverges from the paper by considering the # length of the stem without the new suffix if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
6378	def manhattan ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Manhattan ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
7860	def _request_tls ( self ) : self . requested = True element = ElementTree . Element ( STARTTLS_TAG ) self . stream . write_element ( element )
8532	def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to ( msg_b . args ) : return False , 'argument signature of methods do not match' return True , None
11021	def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
13517	def resistance ( self ) : self . total_resistance_coef = frictional_resistance_coef ( self . length , self . speed ) + residual_resistance_coef ( self . slenderness_coefficient , self . prismatic_coefficient , froude_number ( self . speed , self . length ) ) RT = 1 / 2 * self . total_resistance_coef * 1025 * self . surface_area * self . speed ** 2 return RT
8528	def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
3984	def get_same_container_repos ( app_or_library_name ) : specs = get_expanded_libs_specs ( ) spec = specs . get_app_or_lib ( app_or_library_name ) return get_same_container_repos_from_spec ( spec )
5153	def evaluate_vars ( data , context = None ) : context = context or { } if isinstance ( data , ( dict , list ) ) : if isinstance ( data , dict ) : loop_items = data . items ( ) elif isinstance ( data , list ) : loop_items = enumerate ( data ) for key , value in loop_items : data [ key ] = evaluate_vars ( value , context ) elif isinstance ( data , six . string_types ) : vars_found = var_pattern . findall ( data ) for var in vars_found : var = var . strip ( ) # if found multiple variables, create a new regexp pattern for each # variable, otherwise different variables would get the same value # (see https://github.com/openwisp/netjsonconfig/issues/55) if len ( vars_found ) > 1 : pattern = r'\{\{(\s*%s\s*)\}\}' % var # in case of single variables, use the precompiled # regexp pattern to save computation else : pattern = var_pattern if var in context : data = re . sub ( pattern , context [ var ] , data ) return data
8256	def _darkest ( self ) : min , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for clr in self : if clr . r + clr . g + clr . b < n : min , n = clr , clr . r + clr . g + clr . b return min
8954	def glob2re ( part ) : return "[^/]*" . join ( re . escape ( bit ) . replace ( r'\[\^' , '[^' ) . replace ( r'\[' , '[' ) . replace ( r'\]' , ']' ) for bit in part . split ( "*" ) )
4157	def arma_estimate ( X , P , Q , lag ) : R = CORRELATION ( X , maxlags = lag , norm = 'unbiased' ) R0 = R [ 0 ] #C Estimate the AR parameters (no error weighting is used). #C Number of equation errors is M-Q . MPQ = lag - Q + P N = len ( X ) Y = np . zeros ( N - P , dtype = complex ) for K in range ( 0 , MPQ ) : KPQ = K + Q - P + 1 if KPQ < 0 : Y [ K ] = R [ - KPQ ] . conjugate ( ) if KPQ == 0 : Y [ K ] = R0 if KPQ > 0 : Y [ K ] = R [ KPQ ] # The resize is very important for the normalissation. Y . resize ( lag ) if P <= 4 : res = arcovar_marple ( Y . copy ( ) , P ) #! Eq. (10.12) ar_params = res [ 0 ] else : res = arcovar ( Y . copy ( ) , P ) #! Eq. (10.12) ar_params = res [ 0 ] # the .copy is used to prevent a reference somewhere. this is a bug # to be tracked down. Y . resize ( N - P ) #C Filter the original time series for k in range ( P , N ) : SUM = X [ k ] #SUM += sum([ar_params[j]*X[k-j-1] for j in range(0,P)]) for j in range ( 0 , P ) : SUM = SUM + ar_params [ j ] * X [ k - j - 1 ] #! Eq. (10.17) Y [ k - P ] = SUM # Estimate the MA parameters (a "long" AR of order at least 2*IQ #C is suggested) #Y.resize(N-P) ma_params , rho = ma ( Y , Q , 2 * Q ) #! Eq. (10.3) return ar_params , ma_params , rho
1663	def UpdateIncludeState ( filename , include_dict , io = codecs ) : headerfile = None try : headerfile = io . open ( filename , 'r' , 'utf8' , 'replace' ) except IOError : return False linenum = 0 for line in headerfile : linenum += 1 clean_line = CleanseComments ( line ) match = _RE_PATTERN_INCLUDE . search ( clean_line ) if match : include = match . group ( 2 ) include_dict . setdefault ( include , linenum ) return True
11276	def disown ( debug ) : # Get the current PID pid = os . getpid ( ) cgroup_file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup_file , "r" ) except IOError : print ( "Could not open cgroup file: " , cgroup_file ) return False # Read each line for line in infile : # Check if the line contains "ardexa.service" if line . find ( "ardexa.service" ) == - 1 : continue # if the lines contains "name=", replace it with nothing line = line . replace ( "name=" , "" ) # Split the line by commas items_list = line . split ( ':' ) accounts = items_list [ 1 ] dir_str = accounts + "/ardexa.disown" # If accounts is empty, continue if not accounts : continue # Create the dir and all subdirs full_dir = "/sys/fs/cgroup/" + dir_str if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) if debug >= 1 : print ( "Making directory: " , full_dir ) else : if debug >= 1 : print ( "Directory already exists: " , full_dir ) # Add the PID to the file full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) # If this item contains a comma, then separate it, and reverse # some OSes will need cpuacct,cpu reversed to actually work if accounts . find ( "," ) != - 1 : acct_list = accounts . split ( ',' ) accounts = acct_list [ 1 ] + "," + acct_list [ 0 ] dir_str = accounts + "/ardexa.disown" # Create the dir and all subdirs. But it may not work. So use a TRY full_dir = "/sys/fs/cgroup/" + dir_str try : if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) except : continue # Add the PID to the file full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) infile . close ( ) # For debug purposes only if debug >= 1 : prog_list = [ "cat" , cgroup_file ] run_program ( prog_list , debug , False ) # If there are any "ardexa.service" in the proc file. If so, exit with error prog_list = [ "grep" , "-q" , "ardexa.service" , cgroup_file ] if run_program ( prog_list , debug , False ) : # There are entries still left in the file return False return True
8351	def handle_pi ( self , text ) : if text [ : 3 ] == "xml" : text = u"xml version='1.0' encoding='%SOUP-ENCODING%'" self . _toStringSubclass ( text , ProcessingInstruction )
418	def delete_datasets ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Dataset . delete_many ( kwargs ) logging . info ( "[Database] Delete Dataset SUCCESS" )
9591	def set_window_size ( self , width , height , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_SIZE , { 'width' : int ( width ) , 'height' : int ( height ) , 'window_handle' : window_handle } )
99	def angle_between_vectors ( v1 , v2 ) : l1 = np . linalg . norm ( v1 ) l2 = np . linalg . norm ( v2 ) v1_u = ( v1 / l1 ) if l1 > 0 else np . float32 ( v1 ) * 0 v2_u = ( v2 / l2 ) if l2 > 0 else np . float32 ( v2 ) * 0 return np . arccos ( np . clip ( np . dot ( v1_u , v2_u ) , - 1.0 , 1.0 ) )
12692	def write_tersoff_potential ( parameters ) : lines = [ ] for ( e1 , e2 , e3 ) , params in parameters . items ( ) : if len ( params ) != 14 : raise ValueError ( 'tersoff three body potential expects 14 parameters' ) lines . append ( ' ' . join ( [ e1 , e2 , e3 ] + [ '{:16.8g}' . format ( _ ) for _ in params ] ) ) return '\n' . join ( lines )
3549	def rssi ( self , timeout_sec = TIMEOUT_SEC ) : # Kick off query to get RSSI, then wait for it to return asyncronously # when the _rssi_changed() function is called. self . _rssi_read . clear ( ) self . _peripheral . readRSSI ( ) if not self . _rssi_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for RSSI value!' ) return self . _rssi
11076	def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
7140	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . accounts [ 0 ] . transfer ( address , amount , priority = priority , payment_id = payment_id , unlock_time = unlock_time , relay = relay )
12956	def _rem_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_key_for_index ( indexedField , val ) , pk )
11217	def _pop_claims_from_payload ( self ) : claims_in_payload = [ k for k in self . payload . keys ( ) if k in registered_claims . values ( ) ] for name in claims_in_payload : self . registered_claims [ name ] = self . payload . pop ( name )
4273	def url ( self ) : url = self . name . encode ( 'utf-8' ) return url_quote ( url ) + '/' + self . url_ext
367	def rotation ( x , rg = 20 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if is_random : theta = np . pi / 180 * np . random . uniform ( - rg , rg ) else : theta = np . pi / 180 * rg rotation_matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) , 0 ] , [ np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
7024	def _read_checkplot_picklefile ( checkplotpickle ) : if checkplotpickle . endswith ( '.gz' ) : try : with gzip . open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd , encoding = 'latin1' ) return cpdict
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
9334	def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared
7223	def save ( self , recipe ) : # test if this is a create vs. an update if 'id' in recipe and recipe [ 'id' ] is not None : # update -> use put op self . logger . debug ( "Updating existing recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe [ 'id' ] } r = self . gbdx_connection . put ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise return recipe [ 'id' ] else : # create -> use post op self . logger . debug ( "Creating new recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json' % { 'base_url' : self . base_url } r = self . gbdx_connection . post ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise recipe_json = r . json ( ) return recipe_json [ 'id' ]
10264	def collapse_orthologies_by_namespace ( graph : BELGraph , victim_namespace : Strings , survivor_namespace : str ) -> None : _collapse_edge_by_namespace ( graph , victim_namespace , survivor_namespace , ORTHOLOGOUS )
475	def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] # Normalize digits by 0 before looking words up in the vocabulary. return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ]
5790	def peek_openssl_error ( ) : error = libcrypto . ERR_peek_error ( ) lib = int ( ( error >> 24 ) & 0xff ) func = int ( ( error >> 12 ) & 0xfff ) reason = int ( error & 0xfff ) return ( lib , func , reason )
7806	def verify_jid_against_srv_name ( self , jid , srv_type ) : srv_prefix = u"_" + srv_type + u"." srv_prefix_l = len ( srv_prefix ) for srv in self . alt_names . get ( "SRVName" , [ ] ) : logger . debug ( "checking {0!r} against {1!r}" . format ( jid , srv ) ) if not srv . startswith ( srv_prefix ) : logger . debug ( "{0!r} does not start with {1!r}" . format ( srv , srv_prefix ) ) continue try : srv_jid = JID ( srv [ srv_prefix_l : ] ) except ValueError : continue if srv_jid == jid : logger . debug ( "Match!" ) return True return False
9160	def delete_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_roles = request . json with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_role_requests ( cursor , uuid_ , posted_roles ) resp = request . response resp . status_int = 200 return resp
112	def is_activated ( self , images , augmenter , parents , default ) : if self . activator is None : return default else : return self . activator ( images , augmenter , parents , default )
11952	def execute ( varsfile , templatefile , outputfile = None , configfile = None , dryrun = False , build = False , push = False , verbose = False ) : if dryrun and ( build or push ) : jocker_lgr . error ( 'dryrun requested, cannot build.' ) sys . exit ( 100 ) _set_global_verbosity_level ( verbose ) j = Jocker ( varsfile , templatefile , outputfile , configfile , dryrun , build , push ) formatted_text = j . generate ( ) if dryrun : g = j . dryrun ( formatted_text ) if build or push : j . build_image ( ) if push : j . push_image ( ) if dryrun : return g
1240	def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )
1127	def Seq ( first_rule , * rest_of_rules , * * kwargs ) : @ llrule ( kwargs . get ( "loc" , None ) , first_rule . expected ) def rule ( parser ) : result = first_rule ( parser ) if result is unmatched : return result results = [ result ] for rule in rest_of_rules : result = rule ( parser ) if result is unmatched : return result results . append ( result ) return tuple ( results ) return rule
574	def clippedObj ( obj , maxElementSize = 64 ) : # Is it a named tuple? if hasattr ( obj , '_asdict' ) : obj = obj . _asdict ( ) # Printing a dict? if isinstance ( obj , dict ) : objOut = dict ( ) for key , val in obj . iteritems ( ) : objOut [ key ] = clippedObj ( val ) # Printing a list? elif hasattr ( obj , '__iter__' ) : objOut = [ ] for val in obj : objOut . append ( clippedObj ( val ) ) # Some other object else : objOut = str ( obj ) if len ( objOut ) > maxElementSize : objOut = objOut [ 0 : maxElementSize ] + '...' return objOut
5495	def make_aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
1726	def except_token ( source , start , token , throw = True ) : start = pass_white ( source , start ) if start < len ( source ) and source [ start ] == token : return start + 1 if throw : raise SyntaxError ( 'Missing token. Expected %s' % token ) return None
4955	def get_actor ( self , username , email ) : return Agent ( name = username , mbox = 'mailto:{email}' . format ( email = email ) , )
8120	def intersection ( self , b ) : if not self . intersects ( b ) : return None mx , my = max ( self . x , b . x ) , max ( self . y , b . y ) return Bounds ( mx , my , min ( self . x + self . width , b . x + b . width ) - mx , min ( self . y + self . height , b . y + b . height ) - my )
12342	def images ( self ) : tifs = _pattern ( self . _image_path , extension = 'tif' ) pngs = _pattern ( self . _image_path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
5999	def mapping_matrix ( self ) : return mapper_util . mapping_matrix_from_sub_to_pix ( sub_to_pix = self . sub_to_pix , pixels = self . pixels , regular_pixels = self . grid_stack . regular . shape [ 0 ] , sub_to_regular = self . grid_stack . sub . sub_to_regular , sub_grid_fraction = self . grid_stack . sub . sub_grid_fraction )
5587	def extract_subset ( self , input_data_tiles = None , out_tile = None ) : if self . METADATA [ "data_type" ] == "raster" : mosaic = create_mosaic ( input_data_tiles ) return extract_from_array ( in_raster = prepare_array ( mosaic . data , nodata = self . nodata , dtype = self . output_params [ "dtype" ] ) , in_affine = mosaic . affine , out_tile = out_tile ) elif self . METADATA [ "data_type" ] == "vector" : return [ feature for feature in list ( chain . from_iterable ( [ features for _ , features in input_data_tiles ] ) ) if shape ( feature [ "geometry" ] ) . intersects ( out_tile . bbox ) ]
11248	def median ( data ) : ordered = sorted ( data ) length = len ( ordered ) if length % 2 == 0 : return ( ordered [ math . floor ( length / 2 ) - 1 ] + ordered [ math . floor ( length / 2 ) ] ) / 2.0 elif length % 2 != 0 : return ordered [ math . floor ( length / 2 ) ]
12247	def create_bucket ( self , * args , * * kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , * * kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
299	def plot_slippage_sweep ( returns , positions , transactions , slippage_params = ( 3 , 8 , 10 , 12 , 15 , 20 , 50 ) , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) slippage_sweep = pd . DataFrame ( ) for bps in slippage_params : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) label = str ( bps ) + " bps" slippage_sweep [ label ] = ep . cum_returns ( adj_returns , 1 ) slippage_sweep . plot ( alpha = 1.0 , lw = 0.5 , ax = ax ) ax . set_title ( 'Cumulative returns given additional per-dollar slippage' ) ax . set_ylabel ( '' ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) return ax
1675	def _ExpandDirectories ( filenames ) : expanded = set ( ) for filename in filenames : if not os . path . isdir ( filename ) : expanded . add ( filename ) continue for root , _ , files in os . walk ( filename ) : for loopfile in files : fullname = os . path . join ( root , loopfile ) if fullname . startswith ( '.' + os . path . sep ) : fullname = fullname [ len ( '.' + os . path . sep ) : ] expanded . add ( fullname ) filtered = [ ] for filename in expanded : if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : filtered . append ( filename ) return filtered
12352	def rebuild ( self , image , wait = True ) : return self . _action ( 'rebuild' , image = image , wait = wait )
6033	def from_shape_pixel_scale_and_sub_grid_size ( cls , shape , pixel_scale , sub_grid_size = 2 ) : regular_grid = RegularGrid . from_shape_and_pixel_scale ( shape = shape , pixel_scale = pixel_scale ) sub_grid = SubGrid . from_shape_pixel_scale_and_sub_grid_size ( shape = shape , pixel_scale = pixel_scale , sub_grid_size = sub_grid_size ) blurring_grid = np . array ( [ [ 0.0 , 0.0 ] ] ) return GridStack ( regular_grid , sub_grid , blurring_grid )
13902	def ensure_specifier_exists ( db_spec ) : local_match = LOCAL_RE . match ( db_spec ) remote_match = REMOTE_RE . match ( db_spec ) plain_match = PLAIN_RE . match ( db_spec ) if local_match : db_name = local_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True elif remote_match : hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) server = shortcuts . get_server ( server_url = ( 'http://%s:%s' % ( hostname , portnum ) ) ) if database not in server : server . create ( database ) return True elif plain_match : db_name = plain_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True return False
13770	def render_asset ( self , name ) : result = "" if self . has_asset ( name ) : asset = self . get_asset ( name ) if asset . files : for f in asset . files : result += f . render_include ( ) + "\r\n" return result
7823	def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( "extra-challenge" ) match = SERVER_FINAL_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad final message syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) error = match . group ( "error" ) if error : logger . debug ( "Server returned SCRAM error: {0!r}" . format ( error ) ) return Failure ( u"scram-" + error . decode ( "utf-8" ) ) verifier = match . group ( "verifier" ) if not verifier : logger . debug ( "No verifier value in the final message" ) return Failure ( "bad-succes" ) server_key = self . HMAC ( self . _salted_password , b"Server Key" ) server_signature = self . HMAC ( server_key , self . _auth_message ) if server_signature != a2b_base64 ( verifier ) : logger . debug ( "Server verifier does not match" ) return Failure ( "bad-succes" ) self . _finished = True return Response ( None )
7053	def _read_pklc ( lcfile ) : if lcfile . endswith ( '.gz' ) : try : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
12658	def append_dict_values ( list_of_dicts , keys = None ) : if keys is None : keys = list ( list_of_dicts [ 0 ] . keys ( ) ) dict_of_lists = DefaultOrderedDict ( list ) for d in list_of_dicts : for k in keys : dict_of_lists [ k ] . append ( d [ k ] ) return dict_of_lists
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
3023	def _in_gae_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name in ( 'GAE_PRODUCTION' , 'GAE_LOCAL' ) try : import google . appengine # noqa: unused import except ImportError : pass else : server_software = os . environ . get ( _SERVER_SOFTWARE , '' ) if server_software . startswith ( 'Google App Engine/' ) : SETTINGS . env_name = 'GAE_PRODUCTION' return True elif server_software . startswith ( 'Development/' ) : SETTINGS . env_name = 'GAE_LOCAL' return True return False
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
3996	def _cleanup_path ( path ) : try : yield finally : if os . path . exists ( path ) : if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path )
12603	def duplicated_rows ( df , col_name ) : _check_cols ( df , [ col_name ] ) dups = df [ pd . notnull ( df [ col_name ] ) & df . duplicated ( subset = [ col_name ] ) ] return dups
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
8991	def rows_before ( self ) : rows_before = [ ] for mesh in self . consumed_meshes : if mesh . is_produced ( ) : row = mesh . producing_row if rows_before not in rows_before : rows_before . append ( row ) return rows_before
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
13507	def get_positions ( self ) : url = "/2/positions" data = self . _get_resource ( url ) positions = [ ] for entry in data [ 'positions' ] : positions . append ( self . position_from_json ( entry ) ) return positions
12412	def write ( self , chunk , serialize = False , format = None ) : # Ensure we're not closed. self . require_not_closed ( ) if chunk is None : # There is nothing here. return if serialize or format is not None : # Forward to the serializer to serialize the chunk # before it gets written to the response. self . serialize ( chunk , format = format ) return # `serialize` invokes write(...) if type ( chunk ) is six . binary_type : # Update the stream length. self . _length += len ( chunk ) # If passed a byte string, we hope the user encoded it properly. self . _stream . write ( chunk ) elif isinstance ( chunk , six . string_types ) : encoding = self . encoding if encoding is not None : # If passed a string, we can encode it for the user. chunk = chunk . encode ( encoding ) else : # Bail; we don't have an encoding. raise exceptions . InvalidOperation ( 'Attempting to write textual data without an encoding.' ) # Update the stream length. self . _length += len ( chunk ) # Write the encoded data into the byte stream. self . _stream . write ( chunk ) elif isinstance ( chunk , collections . Iterable ) : # If passed some kind of iterator, attempt to recurse into # oblivion. for section in chunk : self . write ( section ) else : # Bail; we have no idea what to do with this. raise exceptions . InvalidOperation ( 'Attempting to write something not recognized.' )
2449	def set_pkg_supplier ( self , doc , entity ) : self . assert_package_exists ( ) if not self . package_supplier_set : self . package_supplier_set = True if validations . validate_pkg_supplier ( entity ) : doc . package . supplier = entity return True else : raise SPDXValueError ( 'Package::Supplier' ) else : raise CardinalityError ( 'Package::Supplier' )
5647	def write_temporal_networks_by_route_type ( gtfs , extract_output_dir ) : util . makedirs ( extract_output_dir ) for route_type in route_types . TRANSIT_ROUTE_TYPES : pandas_data_frame = temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = route_type ) tag = route_types . ROUTE_TYPE_TO_LOWERCASE_TAG [ route_type ] out_file_name = os . path . join ( extract_output_dir , tag + ".tnet" ) pandas_data_frame . to_csv ( out_file_name , encoding = 'utf-8' , index = False )
2994	def _getJson ( url , token = '' , version = '' ) : if token : return _getJsonIEXCloud ( url , token , version ) return _getJsonOrig ( url )
4628	def get_private ( self ) : encoded = "%s %d" % ( self . brainkey , self . sequence ) a = _bytes ( encoded ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . prefix )
1211	def run ( self ) : if not self . state . document . settings . file_insertion_enabled : raise self . warning ( '"%s" directive disabled.' % self . name ) source = self . state_machine . input_lines . source ( self . lineno - self . state_machine . input_offset - 1 ) source_dir = os . path . dirname ( os . path . abspath ( source ) ) path = rst . directives . path ( self . arguments [ 0 ] ) path = os . path . normpath ( os . path . join ( source_dir , path ) ) path = utils . relative_path ( None , path ) path = nodes . reprunicode ( path ) # get options (currently not use directive-specific options) encoding = self . options . get ( 'encoding' , self . state . document . settings . input_encoding ) e_handler = self . state . document . settings . input_encoding_error_handler tab_width = self . options . get ( 'tab-width' , self . state . document . settings . tab_width ) # open the inclding file try : self . state . document . settings . record_dependencies . add ( path ) include_file = io . FileInput ( source_path = path , encoding = encoding , error_handler = e_handler ) except UnicodeEncodeError as error : raise self . severe ( 'Problems with "%s" directive path:\n' 'Cannot encode input file path "%s" ' '(wrong locale?).' % ( self . name , SafeString ( path ) ) ) except IOError as error : raise self . severe ( 'Problems with "%s" directive path:\n%s.' % ( self . name , ErrorString ( error ) ) ) # read from the file try : rawtext = include_file . read ( ) except UnicodeError as error : raise self . severe ( 'Problem with "%s" directive:\n%s' % ( self . name , ErrorString ( error ) ) ) config = self . state . document . settings . env . config converter = M2R ( no_underscore_emphasis = config . no_underscore_emphasis ) include_lines = statemachine . string2lines ( converter ( rawtext ) , tab_width , convert_whitespace = True ) self . state_machine . insert_input ( include_lines , path ) return [ ]
8333	def findPrevious ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findAllPrevious , name , attrs , text , * * kwargs )
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) # Read the entire buffer into a python bytes object. # read_pixel_bytes: New in version 2.32. pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) # type: bytes width , height = g [ 2 ] , g [ 3 ] # Probably for SSE alignment reasons, the pixbuf has extra data in each line. # The args after "raw" help handle this; see # http://effbot.org/imagingbook/decoder.htm#the-raw-decoder return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
12997	def skyimage_figure ( cluster ) : pf_image = figure ( x_range = ( 0 , 1 ) , y_range = ( 0 , 1 ) , title = 'Image of {0}' . format ( cluster . name ) ) pf_image . image_url ( url = [ cluster . image_path ] , x = 0 , y = 0 , w = 1 , h = 1 , anchor = 'bottom_left' ) pf_image . toolbar_location = None pf_image . axis . visible = False return pf_image
3713	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeGases ] return mixing_simple ( zs , Vms ) elif method == IDEAL : return ideal_gas ( T , P ) elif method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP_zs ( T = T , P = P , zs = zs ) return self . eos [ 0 ] . V_g else : raise Exception ( 'Method not valid' )
6476	def set_text ( self , point , text ) : if not self . option . legend : return if not isinstance ( point , Point ) : point = Point ( point ) for offset , char in enumerate ( str ( text ) ) : self . screen . canvas [ point . y ] [ point . x + offset ] = char
8322	def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except ValueError : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , "utf_8" , "replace" ) except : return "" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( "true" , "1" , "yes" ) : return True else : return False
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
5968	def energy_minimize ( dirname = 'em' , mdp = config . templates [ 'em.mdp' ] , struct = 'solvate/ionized.gro' , top = 'top/system.top' , output = 'em.pdb' , deffnm = "em" , mdrunner = None , mdrun_args = None , * * kwargs ) : structure = realpath ( struct ) topology = realpath ( top ) mdp_template = config . get_template ( mdp ) deffnm = deffnm . strip ( ) mdrun_args = { } if mdrun_args is None else mdrun_args # write the processed topology to the default output kwargs . setdefault ( 'pp' , 'processed.top' ) # filter some kwargs that might come through when feeding output # from previous stages such as solvate(); necessary because *all* # **kwargs must be *either* substitutions in the mdp file *or* valid # command line parameters for ``grompp``. kwargs . pop ( 'ndx' , None ) # mainselection is not used but only passed through; right now we # set it to the default that is being used in all argument lists # but that is not pretty. TODO. mainselection = kwargs . pop ( 'mainselection' , '"Protein"' ) # only interesting when passed from solvate() qtot = kwargs . pop ( 'qtot' , 0 ) # mdp is now the *output* MDP that will be generated from mdp_template mdp = deffnm + '.mdp' tpr = deffnm + '.tpr' logger . info ( "[{dirname!s}] Energy minimization of struct={struct!r}, top={top!r}, mdp={mdp!r} ..." . format ( * * vars ( ) ) ) cbook . add_mdp_includes ( topology , kwargs ) if qtot != 0 : # At the moment this is purely user-reported and really only here because # it might get fed into the function when using the keyword-expansion pipeline # usage paradigm. wmsg = "Total charge was reported as qtot = {qtot:g} <> 0; probably a problem." . format ( * * vars ( ) ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = BadParameterWarning ) with in_dir ( dirname ) : unprocessed = cbook . edit_mdp ( mdp_template , new_mdp = mdp , * * kwargs ) check_mdpargs ( unprocessed ) gromacs . grompp ( f = mdp , o = tpr , c = structure , r = structure , p = topology , * * unprocessed ) mdrun_args . update ( v = True , stepout = 10 , deffnm = deffnm , c = output ) if mdrunner is None : mdrun = run . get_double_or_single_prec_mdrun ( ) mdrun ( * * mdrun_args ) else : if type ( mdrunner ) is type : # class # user wants full control and provides simulation.MDrunner **class** # NO CHECKING --- in principle user can supply any callback they like mdrun = mdrunner ( * * mdrun_args ) mdrun . run ( ) else : # anything with a run() method that takes mdrun arguments... try : mdrunner . run ( mdrunargs = mdrun_args ) except AttributeError : logger . error ( "mdrunner: Provide a gromacs.run.MDrunner class or instance or a callback with a run() method" ) raise TypeError ( "mdrunner: Provide a gromacs.run.MDrunner class or instance or a callback with a run() method" ) # em.gro --> gives 'Bad box in file em.gro' warning --- why?? # --> use em.pdb instead. if not os . path . exists ( output ) : errmsg = "Energy minimized system NOT produced." logger . error ( errmsg ) raise GromacsError ( errmsg ) final_struct = realpath ( output ) logger . info ( "[{dirname!s}] energy minimized structure {final_struct!r}" . format ( * * vars ( ) ) ) return { 'struct' : final_struct , 'top' : topology , 'mainselection' : mainselection , }
13461	def event_update_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) updates = Update . objects . filter ( event__slug = slug ) if event . recently_ended ( ) : # if the event is over, use chronological order updates = updates . order_by ( 'id' ) else : # if not, use reverse chronological updates = updates . order_by ( '-id' ) return render ( request , 'happenings/updates/update_list.html' , { 'event' : event , 'object_list' : updates , } )
3615	def get_raw_record ( self , instance , update_fields = None ) : tmp = { 'objectID' : self . objectID ( instance ) } if update_fields : if isinstance ( update_fields , str ) : update_fields = ( update_fields , ) for elt in update_fields : key = self . __translate_fields . get ( elt , None ) if key : tmp [ key ] = self . __named_fields [ key ] ( instance ) else : for key , value in self . __named_fields . items ( ) : tmp [ key ] = value ( instance ) if self . geo_field : loc = self . geo_field ( instance ) if isinstance ( loc , tuple ) : tmp [ '_geoloc' ] = { 'lat' : loc [ 0 ] , 'lng' : loc [ 1 ] } elif isinstance ( loc , dict ) : self . _validate_geolocation ( loc ) tmp [ '_geoloc' ] = loc elif isinstance ( loc , list ) : [ self . _validate_geolocation ( geo ) for geo in loc ] tmp [ '_geoloc' ] = loc if self . tags : if callable ( self . tags ) : tmp [ '_tags' ] = self . tags ( instance ) if not isinstance ( tmp [ '_tags' ] , list ) : tmp [ '_tags' ] = list ( tmp [ '_tags' ] ) logger . debug ( 'BUILD %s FROM %s' , tmp [ 'objectID' ] , self . model ) return tmp
1109	def compare ( self , a , b ) : cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line
4523	def color_scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
9603	def from_object ( cls , obj ) : return cls ( obj . get ( 'sessionId' , None ) , obj . get ( 'status' , 0 ) , obj . get ( 'value' , None ) )
11799	def prune ( self , var , value , removals ) : self . curr_domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 # start index of diffusion-based populations for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
10079	def _publish_new ( self , id_ = None ) : minter = current_pidstore . minters [ current_app . config [ 'DEPOSIT_PID_MINTER' ] ] id_ = id_ or uuid . uuid4 ( ) record_pid = minter ( id_ , self ) self [ '_deposit' ] [ 'pid' ] = { 'type' : record_pid . pid_type , 'value' : record_pid . pid_value , 'revision_id' : 0 , } data = dict ( self . dumps ( ) ) data [ '$schema' ] = self . record_schema with self . _process_files ( id_ , data ) : record = self . published_record_class . create ( data , id_ = id_ ) return record
2526	def get_annotation_type ( self , r_term ) : for _ , _ , typ in self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationType' ] , None ) ) : if typ is not None : return typ else : self . error = True msg = 'Annotation must have exactly one annotation type.' self . logger . log ( msg ) return
948	def run ( self ) : self . __logger . debug ( "run(): Starting task <%s>" , self . __task [ 'taskLabel' ] ) # Set up the task # Create our main loop-control iterator if self . __cmdOptions . privateOptions [ 'testMode' ] : numIters = 10 else : numIters = self . __task [ 'iterationCount' ] if numIters >= 0 : iterTracker = iter ( xrange ( numIters ) ) else : iterTracker = iter ( itertools . count ( ) ) # Initialize periodic activities periodic = PeriodicActivityMgr ( requestedActivities = self . _createPeriodicActivities ( ) ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPFTaskDriver.setup(), instead? Is it always # desired in Nupic? self . __model . resetSequenceStates ( ) # Have Task Driver perform its initial setup activities, including setup # callbacks self . __taskDriver . setup ( ) # Run it! while True : # Check controlling iterator first try : next ( iterTracker ) except StopIteration : break # Read next input record try : inputRecord = self . __datasetReader . next ( ) except StopIteration : break # Process input record result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) self . __predictionLogger . writeRecord ( result ) # Run periodic activities periodic . tick ( ) # Dump the experiment metrics at the end of the task self . _getAndEmitExperimentMetrics ( final = True ) # Have Task Driver perform its final activities self . __taskDriver . finalize ( ) # Reset sequence states in the model, so it starts looking for a new # sequence # TODO: should this be done in OPFTaskDriver.setup(), instead? Is it always # desired in Nupic? self . __model . resetSequenceStates ( )
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] # remove the ',' in the last item tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
4967	def _validate_course ( self ) : # Verify that the selected mode is valid for the given course . course_details = self . cleaned_data . get ( self . Fields . COURSE ) if course_details : course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) valid_course_modes = course_details [ "course_modes" ] if all ( course_mode != mode [ "slug" ] for mode in valid_course_modes ) : error = ValidationError ( ValidationMessages . COURSE_MODE_INVALID_FOR_COURSE . format ( course_mode = course_mode , course_id = course_details [ "course_id" ] , ) ) raise ValidationError ( { self . Fields . COURSE_MODE : error } )
6086	def unmasked_blurred_image_of_planes_and_galaxies_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : return [ plane . unmasked_blurred_image_of_galaxies_from_psf ( padded_grid_stack , psf ) for plane in planes ]
8475	def _getClassInstance ( path , args = None ) : if not path . endswith ( ".py" ) : return None if args is None : args = { } classname = AtomShieldsScanner . _getClassName ( path ) basename = os . path . basename ( path ) . replace ( ".py" , "" ) sys . path . append ( os . path . dirname ( path ) ) try : mod = __import__ ( basename , globals ( ) , locals ( ) , [ classname ] , - 1 ) class_ = getattr ( mod , classname ) instance = class_ ( * * args ) except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) return None finally : sys . path . remove ( os . path . dirname ( path ) ) return instance
887	def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : # Enforce maxSegmentsPerCell. while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) # Create the segment. segment = connections . createSegment ( cell ) # Do TM-specific bookkeeping for the segment. if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : # A flatIdx was recycled. lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( "All segments should be created with the TM createSegment method." ) return segment
1912	def SInt ( value , width ) : return Operators . ITEBV ( width , Bit ( value , width - 1 ) == 1 , GetNBits ( value , width ) - 2 ** width , GetNBits ( value , width ) )
2165	def format_commands ( self , ctx , formatter ) : self . format_command_subsection ( ctx , formatter , self . list_misc_commands ( ) , 'Commands' ) self . format_command_subsection ( ctx , formatter , self . list_resource_commands ( ) , 'Resources' )
13537	def _child_allowed ( self , child_rule ) : num_kids = self . node . children . count ( ) num_kids_allowed = len ( self . rule . children ) if not self . rule . multiple_paths : num_kids_allowed = 1 if num_kids >= num_kids_allowed : raise AttributeError ( 'Rule %s only allows %s children' % ( self . rule_name , self . num_kids_allowed ) ) # verify not a duplicate for node in self . node . children . all ( ) : if node . data . rule_label == child_rule . class_label : raise AttributeError ( 'Child rule already exists' ) # check if the given rule is allowed as a child if child_rule not in self . rule . children : raise AttributeError ( 'Rule %s is not a valid child of Rule %s' % ( child_rule . __name__ , self . rule_name ) )
8635	def get_milestones ( session , project_ids = [ ] , milestone_ids = [ ] , user_details = None , limit = 10 , offset = 0 ) : get_milestones_data = { } if milestone_ids : get_milestones_data [ 'milestones[]' ] = milestone_ids if project_ids : get_milestones_data [ 'projects[]' ] = project_ids get_milestones_data [ 'limit' ] = limit get_milestones_data [ 'offset' ] = offset # Add projections if they exist if user_details : get_milestones_data . update ( user_details ) # GET /api/projects/0.1/milestones/ response = make_get_request ( session , 'milestones' , params_data = get_milestones_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3883	def from_entity ( entity , self_user_id ) : user_id = UserID ( chat_id = entity . id . chat_id , gaia_id = entity . id . gaia_id ) return User ( user_id , entity . properties . display_name , entity . properties . first_name , entity . properties . photo_url , entity . properties . email , ( self_user_id == user_id ) or ( self_user_id is None ) )
12727	def stop_erps ( self , stop_erps ) : _set_params ( self . ode_obj , 'StopERP' , stop_erps , self . ADOF + self . LDOF )
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # delete endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1899	def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : # if True check if constraints are feasible self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
9045	def gradient ( self ) : self . _init_svd ( ) C0 = self . _C0 . gradient ( ) [ "Lu" ] . T C1 = self . _C1 . gradient ( ) [ "Lu" ] . T grad = { "C0.Lu" : kron ( C0 , self . _X ) . T , "C1.Lu" : kron ( C1 , self . _I ) . T } return grad
6480	def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
10316	def relation_set_has_contradictions ( relations : Set [ str ] ) -> bool : has_increases = any ( relation in CAUSAL_INCREASE_RELATIONS for relation in relations ) has_decreases = any ( relation in CAUSAL_DECREASE_RELATIONS for relation in relations ) has_cnc = any ( relation == CAUSES_NO_CHANGE for relation in relations ) return 1 < sum ( [ has_cnc , has_decreases , has_increases ] )
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : # is numpy array return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
9469	def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
10037	def execute ( helper , config , args ) : out ( "Available solution stacks" ) for stack in helper . list_available_solution_stacks ( ) : out ( " " + str ( stack ) ) return 0
2084	def parse_args ( self , ctx , args ) : if not args and self . no_args_is_help and not ctx . resilient_parsing : click . echo ( ctx . get_help ( ) ) ctx . exit ( ) return super ( ActionSubcommand , self ) . parse_args ( ctx , args )
10111	def iterrows ( lines_or_file , namedtuples = False , dicts = False , encoding = 'utf-8' , * * kw ) : if namedtuples and dicts : raise ValueError ( 'either namedtuples or dicts can be chosen as output format' ) elif namedtuples : _reader = NamedTupleReader elif dicts : _reader = UnicodeDictReader else : _reader = UnicodeReader with _reader ( lines_or_file , encoding = encoding , * * fix_kw ( kw ) ) as r : for item in r : yield item
10196	def aggregate_events ( aggregations , start_date = None , end_date = None , update_bookmark = True ) : start_date = dateutil_parse ( start_date ) if start_date else None end_date = dateutil_parse ( end_date ) if end_date else None results = [ ] for a in aggregations : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) results . append ( aggregator . run ( start_date , end_date , update_bookmark ) ) return results
8054	def handler ( self , conn , * args ) : # lines from cmd.Cmd self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\r\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) # end lines from cmd.Cmd if stop : self . shell = None conn . close ( ) return not stop
7738	def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
7899	def request_configuration_form ( self ) : iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "get" ) iq . new_query ( MUC_OWNER_NS , "query" ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_form_success , self . process_configuration_form_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
8642	def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } # POST /api/projects/0.1/milestones/ response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) # if z = 1 v_i is allowed non-zero # v_i - Mz <= 0 and v_i + Mz >= 0 constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
4146	def WelchPeriodogram ( data , NFFT = None , sampling = 1. , * * kargs ) : from pylab import psd spectrum = Spectrum ( data , sampling = 1. ) P = psd ( data , NFFT , Fs = sampling , * * kargs ) spectrum . psd = P [ 0 ] #spectrum.__Spectrum_sides = 'twosided' return P , spectrum
2309	def predict ( self , data , graph = None , nruns = 6 , njobs = None , gpus = 0 , verbose = None , plot = False , plot_generated_pair = False , return_list_results = False ) : verbose , njobs = SETTINGS . get_default ( ( 'verbose' , verbose ) , ( 'nb_jobs' , njobs ) ) if njobs != 1 : list_out = Parallel ( n_jobs = njobs ) ( delayed ( run_SAM ) ( data , skeleton = graph , lr_gen = self . lr , lr_disc = self . dlr , regul_param = self . l1 , nh = self . nh , dnh = self . dnh , gpu = bool ( gpus ) , train_epochs = self . train , test_epochs = self . test , batch_size = self . batchsize , plot = plot , verbose = verbose , gpu_no = idx % max ( gpus , 1 ) ) for idx in range ( nruns ) ) else : list_out = [ run_SAM ( data , skeleton = graph , lr_gen = self . lr , lr_disc = self . dlr , regul_param = self . l1 , nh = self . nh , dnh = self . dnh , gpu = bool ( gpus ) , train_epochs = self . train , test_epochs = self . test , batch_size = self . batchsize , plot = plot , verbose = verbose , gpu_no = 0 ) for idx in range ( nruns ) ] if return_list_results : return list_out else : W = list_out [ 0 ] for w in list_out [ 1 : ] : W += w W /= nruns return nx . relabel_nodes ( nx . DiGraph ( W ) , { idx : i for idx , i in enumerate ( data . columns ) } )
10999	def psf_slice ( self , zint , size = 11 , zoffset = 0. , getextent = False ) : # calculate the current pixel value in 1/k, making sure we are above the slab zint = max ( self . _p2k ( self . _tz ( zint ) ) , 0 ) offset = np . array ( [ zoffset * ( zint > 0 ) , 0 , 0 ] ) scale = [ self . param_dict [ self . zscale ] , 1.0 , 1.0 ] # create the coordinate vectors for where to actually calculate the tile = util . Tile ( left = 0 , size = size , centered = True ) vecs = tile . coords ( form = 'flat' ) vecs = [ self . _p2k ( s * i + o ) for i , s , o in zip ( vecs , scale , offset ) ] psf = self . psffunc ( * vecs [ : : - 1 ] , zint = zint , * * self . pack_args ( ) ) . T vec = tile . coords ( form = 'meshed' ) # create a smoothly varying point spread function by cutting off the psf # at a certain value and smoothly taking it to zero if self . cutoffval is not None and not self . cutbyval : # find the edges of the PSF edge = psf > psf . max ( ) * self . cutoffval dd = nd . morphology . distance_transform_edt ( ~ edge ) # calculate the new PSF and normalize it to the new support psf = psf * np . exp ( - dd ** 4 ) psf /= psf . sum ( ) if getextent : # the size is determined by the edge plus a 2 pad for the # exponential damping to zero at the edge size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec # perform a cut by value instead if self . cutoffval is not None and self . cutbyval : cutval = self . cutoffval * psf . max ( ) dd = ( psf - cutval ) / cutval dd [ dd > 0 ] = 0. # calculate the new PSF and normalize it to the new support psf = psf * np . exp ( - ( dd / self . cutfallrate ) ** 4 ) psf /= psf . sum ( ) # let the small values determine the edges edge = psf > cutval * self . cutedgeval if getextent : # the size is determined by the edge plus a 2 pad for the # exponential damping to zero at the edge size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec return psf , vec
9081	def find ( self , query , * * kwargs ) : if 'providers' not in kwargs : providers = self . get_providers ( ) else : pargs = kwargs [ 'providers' ] if isinstance ( pargs , list ) : providers = self . get_providers ( ids = pargs ) else : providers = self . get_providers ( * * pargs ) kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . find ( query , * * kwarguments ) } for p in providers ]
7073	def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
11445	def _clean_xml ( self , path_to_xml ) : try : if os . path . isfile ( path_to_xml ) : tree = ET . parse ( path_to_xml ) root = tree . getroot ( ) else : root = ET . fromstring ( path_to_xml ) except Exception , e : self . logger . error ( "Could not read OAI XML, aborting filter!" ) raise e strip_xml_namespace ( root ) return root
5413	def lookup_job_tasks ( self , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user_ids = None if user_ids == { '*' } else user_ids job_ids = None if job_ids == { '*' } else job_ids job_names = None if job_names == { '*' } else job_names task_ids = None if task_ids == { '*' } else task_ids task_attempts = None if task_attempts == { '*' } else task_attempts if labels or create_time_min or create_time_max : raise NotImplementedError ( 'Lookup by labels and create_time not yet supported by stub.' ) operations = [ x for x in self . _operations if ( ( not statuses or x . get_field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user_ids or x . get_field ( 'user' , None ) in user_ids ) and ( not job_ids or x . get_field ( 'job-id' , None ) in job_ids ) and ( not job_names or x . get_field ( 'job-name' , None ) in job_names ) and ( not task_ids or x . get_field ( 'task-id' , None ) in task_ids ) and ( not task_attempts or x . get_field ( 'task-attempt' , None ) in task_attempts ) ) ] if max_tasks > 0 : operations = operations [ : max_tasks ] return operations
2024	def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )
13867	def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
13665	def update_item ( filename , item , uuid ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # apply modifications to the JSON data wrt UUID # TODO: handle this in a neat way if 'products' in products_data [ - 1 ] : # handle orders object [ products_data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] else : # handle products object [ products_data [ i ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] # save the modified JSON data into the temp file json . dump ( products_data , temp_file ) return True
4124	def data_cosine ( N = 1024 , A = 0.1 , sampling = 1024. , freq = 200 ) : t = arange ( 0 , float ( N ) / sampling , 1. / sampling ) x = cos ( 2. * pi * t * freq ) + A * randn ( t . size ) return x
479	def word_to_id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk_id
7430	def _resolveambig ( subseq ) : N = [ ] for col in subseq : rand = np . random . binomial ( 1 , 0.5 ) N . append ( [ _AMBIGS [ i ] [ rand ] for i in col ] ) return np . array ( N )
2469	def set_file_copyright ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_copytext_set : self . file_copytext_set = True if validations . validate_file_cpyright ( text ) : if isinstance ( text , string_types ) : self . file ( doc ) . copyright = str_from_text ( text ) else : self . file ( doc ) . copyright = text # None or NoAssert return True else : raise SPDXValueError ( 'File::CopyRight' ) else : raise CardinalityError ( 'File::CopyRight' ) else : raise OrderError ( 'File::CopyRight' )
4108	def chirp ( t , f0 = 0. , t1 = 1. , f1 = 100. , form = 'linear' , phase = 0 ) : valid_forms = [ 'linear' , 'quadratic' , 'logarithmic' ] if form not in valid_forms : raise ValueError ( "Invalid form. Valid form are %s" % valid_forms ) t = numpy . array ( t ) phase = 2. * pi * phase / 360. if form == "linear" : a = pi * ( f1 - f0 ) / t1 b = 2. * pi * f0 y = numpy . cos ( a * t ** 2 + b * t + phase ) elif form == "quadratic" : a = ( 2 / 3. * pi * ( f1 - f0 ) / t1 / t1 ) b = 2. * pi * f0 y = numpy . cos ( a * t ** 3 + b * t + phase ) elif form == "logarithmic" : a = 2. * pi * t1 / numpy . log ( f1 - f0 ) b = 2. * pi * f0 x = ( f1 - f0 ) ** ( 1. / t1 ) y = numpy . cos ( a * x ** t + b * t + phase ) return y
9499	def convert_completezip ( path ) : for filepath in path . glob ( '**/index_auto_generated.cnxml' ) : filepath . rename ( filepath . parent / 'index.cnxml' ) logger . debug ( 'removed {}' . format ( filepath ) ) for filepath in path . glob ( '**/index.cnxml.html' ) : filepath . unlink ( ) return parse_litezip ( path )
5187	def inventory ( self , * * kwargs ) : inventory = self . _query ( 'inventory' , * * kwargs ) for inv in inventory : yield Inventory ( node = inv [ 'certname' ] , time = inv [ 'timestamp' ] , environment = inv [ 'environment' ] , facts = inv [ 'facts' ] , trusted = inv [ 'trusted' ] )
13364	def setup_engines ( client = None ) : if not client : try : client = ipyparallel . Client ( ) except : raise DistobClusterError ( u"""Could not connect to an ipyparallel cluster. Make sure a cluster is started (e.g. to use the CPUs of a single computer, can type 'ipcluster start')""" ) eids = client . ids if not eids : raise DistobClusterError ( u'No ipyparallel compute engines are available' ) nengines = len ( eids ) dv = client [ eids ] dv . use_dill ( ) with dv . sync_imports ( quiet = True ) : import distob # create global ObjectEngine distob.engine on each engine ars = [ ] for i in eids : dv . targets = i ars . append ( dv . apply_async ( _remote_setup_engine , i , nengines ) ) dv . wait ( ars ) for ar in ars : if not ar . successful ( ) : raise ar . r # create global ObjectHub distob.engine on the client host if distob . engine is None : distob . engine = ObjectHub ( - 1 , client )
1427	def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) # Special parameters for update command def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
12083	def filesByExtension ( fnames ) : byExt = { "abf" : [ ] , "jpg" : [ ] , "tif" : [ ] } # prime it with empties for fname in fnames : ext = os . path . splitext ( fname ) [ 1 ] . replace ( "." , '' ) . lower ( ) if not ext in byExt . keys ( ) : byExt [ ext ] = [ ] byExt [ ext ] = byExt [ ext ] + [ fname ] return byExt
11360	def fix_dashes ( string ) : string = string . replace ( u'\u05BE' , '-' ) string = string . replace ( u'\u1806' , '-' ) string = string . replace ( u'\u2E3A' , '-' ) string = string . replace ( u'\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )
11826	def print_boggle ( board ) : n2 = len ( board ) n = exact_sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print
8802	def do_notify ( context , event_type , payload ) : LOG . debug ( 'IP_BILL: notifying {}' . format ( payload ) ) notifier = n_rpc . get_notifier ( 'network' ) notifier . info ( context , event_type , payload )
3084	def oauth2decorator_from_clientsecrets ( filename , scope , message = None , cache = None ) : return OAuth2DecoratorFromClientSecrets ( filename , scope , message = message , cache = cache )
10543	def create_task ( project_id , info , n_answers = 30 , priority_0 = 0 , quorum = 0 ) : try : task = dict ( project_id = project_id , info = info , calibration = 0 , priority_0 = priority_0 , n_answers = n_answers , quorum = quorum ) res = _pybossa_req ( 'post' , 'task' , payload = task ) if res . get ( 'id' ) : return Task ( res ) else : return res except : # pragma: no cover raise
12049	def scanABFfolder ( abfFolder ) : assert os . path . isdir ( abfFolder ) filesABF = forwardSlash ( sorted ( glob . glob ( abfFolder + "/*.*" ) ) ) filesSWH = [ ] if os . path . exists ( abfFolder + "/swhlab4/" ) : filesSWH = forwardSlash ( sorted ( glob . glob ( abfFolder + "/swhlab4/*.*" ) ) ) groups = getABFgroups ( filesABF ) return filesABF , filesSWH , groups
7904	def join ( self , room , nick , handler , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if not room . node or room . resource : raise ValueError ( "Invalid room JID" ) room_jid = JID ( room . node , room . domain , nick ) cur_rs = self . rooms . get ( room_jid . bare ( ) . as_unicode ( ) ) if cur_rs and cur_rs . joined : raise RuntimeError ( "Room already joined" ) rs = MucRoomState ( self , self . stream . me , room_jid , handler ) self . rooms [ room_jid . bare ( ) . as_unicode ( ) ] = rs rs . join ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) return rs
6653	def start ( self , builddir , program , forward_args ) : child = None try : prog_path = self . findProgram ( builddir , program ) if prog_path is None : return start_env , start_vars = self . buildProgEnvAndVars ( prog_path , builddir ) if self . getScript ( 'start' ) : cmd = [ os . path . expandvars ( string . Template ( x ) . safe_substitute ( * * start_vars ) ) for x in self . getScript ( 'start' ) ] + forward_args else : cmd = shlex . split ( './' + prog_path ) + forward_args logger . debug ( 'starting program: %s' , cmd ) child = subprocess . Popen ( cmd , cwd = builddir , env = start_env ) child . wait ( ) if child . returncode : return "process exited with status %s" % child . returncode child = None except OSError as e : import errno if e . errno == errno . ENOEXEC : return ( "the program %s cannot be run (perhaps your target " + "needs to define a 'start' script to start it on its " "intended execution target?)" ) % prog_path finally : if child is not None : _tryTerminate ( child )
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
7930	def resolve_address ( self , hostname , callback , allow_cname = True ) : if self . settings [ "ipv6" ] : if self . settings [ "ipv4" ] : family = socket . AF_UNSPEC else : family = socket . AF_INET6 elif self . settings [ "ipv4" ] : family = socket . AF_INET else : logger . warning ( "Neither IPv6 or IPv4 allowed." ) callback ( [ ] ) return try : ret = socket . getaddrinfo ( hostname , 0 , family , socket . SOCK_STREAM , 0 ) except socket . gaierror , err : logger . warning ( "Couldn't resolve {0!r}: {1}" . format ( hostname , err ) ) callback ( [ ] ) return except IOError as err : logger . warning ( "Couldn't resolve {0!r}, unexpected error: {1}" . format ( hostname , err ) ) callback ( [ ] ) return if family == socket . AF_UNSPEC : tmp = ret if self . settings [ "prefer_ipv6" ] : ret = [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET6 ] ret += [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET ] else : ret = [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET ] ret += [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET6 ] callback ( [ ( addr [ 0 ] , addr [ 4 ] [ 0 ] ) for addr in ret ] )
6542	def on_tool_finish ( self , tool ) : with self . _lock : if tool in self . current_tools : self . current_tools . remove ( tool ) self . completed_tools . append ( tool )
4315	def validate_input_file_list ( input_filepath_list ) : if not isinstance ( input_filepath_list , list ) : raise TypeError ( "input_filepath_list must be a list." ) elif len ( input_filepath_list ) < 2 : raise ValueError ( "input_filepath_list must have at least 2 files." ) for input_filepath in input_filepath_list : validate_input_file ( input_filepath )
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
9118	def _add_admin ( self , app , * * kwargs ) : from flask_admin import Admin from flask_admin . contrib . sqla import ModelView admin = Admin ( app , * * kwargs ) for flask_admin_model in self . flask_admin_models : if isinstance ( flask_admin_model , tuple ) : # assume its a 2 tuple if len ( flask_admin_model ) != 2 : raise TypeError model , view = flask_admin_model admin . add_view ( view ( model , self . session ) ) else : admin . add_view ( ModelView ( flask_admin_model , self . session ) ) return admin
5477	def parse_rfc3339_utc_string ( rfc3339_utc_string ) : # The timestamp from the Google Operations are all in RFC3339 format, but # they are sometimes formatted to millisconds, microseconds, sometimes # nanoseconds, and sometimes only seconds: # * 2016-11-14T23:05:56Z # * 2016-11-14T23:05:56.010Z # * 2016-11-14T23:05:56.010429Z # * 2016-11-14T23:05:56.010429380Z m = re . match ( r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2}).?(\d*)Z' , rfc3339_utc_string ) # It would be unexpected to get a different date format back from Google. # If we raise an exception here, we can break people completely. # Instead, let's just return None and people can report that some dates # are not showing up. # We might reconsider this approach in the future; it was originally # established when dates were only used for display. if not m : return None groups = m . groups ( ) if len ( groups [ 6 ] ) not in ( 0 , 3 , 6 , 9 ) : return None # Create a UTC datestamp from parsed components # 1- Turn components 0-5 from strings to integers # 2- If the last component does not exist, set it to 0. # If it does exist, make sure to interpret it as milliseconds. g = [ int ( val ) for val in groups [ : 6 ] ] fraction = groups [ 6 ] if not fraction : micros = 0 elif len ( fraction ) == 3 : micros = int ( fraction ) * 1000 elif len ( fraction ) == 6 : micros = int ( fraction ) elif len ( fraction ) == 9 : # When nanoseconds are provided, we round micros = int ( round ( int ( fraction ) / 1000 ) ) else : assert False , 'Fraction length not 0, 6, or 9: {}' . len ( fraction ) try : return datetime ( g [ 0 ] , g [ 1 ] , g [ 2 ] , g [ 3 ] , g [ 4 ] , g [ 5 ] , micros , tzinfo = pytz . utc ) except ValueError as e : assert False , 'Could not parse RFC3339 datestring: {} exception: {}' . format ( rfc3339_utc_string , e )
12260	def columns ( x , rho , proxop ) : xnext = np . zeros_like ( x ) for ix in range ( x . shape [ 1 ] ) : xnext [ : , ix ] = proxop ( x [ : , ix ] , rho ) return xnext
7089	def jd_corr ( jd , ra , dec , obslon = None , obslat = None , obsalt = None , jd_type = 'bjd' ) : if not HAVEKERNEL : LOGERROR ( 'no JPL kernel available, can\'t continue!' ) return # Source unit-vector ## Assume coordinates in ICRS ## Set distance to unit (kilometers) # convert the angles to degrees rarad = np . radians ( ra ) decrad = np . radians ( dec ) cosra = np . cos ( rarad ) sinra = np . sin ( rarad ) cosdec = np . cos ( decrad ) sindec = np . sin ( decrad ) # this assumes that the target is very far away src_unitvector = np . array ( [ cosdec * cosra , cosdec * sinra , sindec ] ) # Convert epochs to astropy.time.Time ## Assume JD(UTC) if ( obslon is None ) or ( obslat is None ) or ( obsalt is None ) : t = astime . Time ( jd , scale = 'utc' , format = 'jd' ) else : t = astime . Time ( jd , scale = 'utc' , format = 'jd' , location = ( '%.5fd' % obslon , '%.5fd' % obslat , obsalt ) ) # Get Earth-Moon barycenter position ## NB: jplephem uses Barycentric Dynamical Time, e.g. JD(TDB) ## and gives positions relative to solar system barycenter barycenter_earthmoon = jplkernel [ 0 , 3 ] . compute ( t . tdb . jd ) # Get Moon position vectors from the center of Earth to the Moon # this means we get the following vectors from the ephemerides # Earth Barycenter (3) -> Moon (301) # Earth Barycenter (3) -> Earth (399) # so the final vector is [3,301] - [3,399] # units are in km moonvector = ( jplkernel [ 3 , 301 ] . compute ( t . tdb . jd ) - jplkernel [ 3 , 399 ] . compute ( t . tdb . jd ) ) # Compute Earth position vectors (this is for the center of the earth with # respect to the solar system barycenter) # all these units are in km pos_earth = ( barycenter_earthmoon - moonvector * 1.0 / ( 1.0 + EMRAT ) ) if jd_type == 'bjd' : # Compute BJD correction ## Assume source vectors parallel at Earth and Solar System ## Barycenter ## i.e. source is at infinity # the romer_delay correction is (r.dot.n)/c where: # r is the vector from SSB to earth center # n is the unit vector from correction_seconds = np . dot ( pos_earth . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY elif jd_type == 'hjd' : # Compute HJD correction via Sun ephemeris # this is the position vector of the center of the sun in km # Solar System Barycenter (0) -> Sun (10) pos_sun = jplkernel [ 0 , 10 ] . compute ( t . tdb . jd ) # this is the vector from the center of the sun to the center of the # earth sun_earth_vec = pos_earth - pos_sun # calculate the heliocentric correction correction_seconds = np . dot ( sun_earth_vec . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY # TDB is the appropriate time scale for these ephemerides new_jd = t . tdb . jd + correction_days return new_jd
411	def minibatches ( inputs = None , targets = None , batch_size = None , allow_dynamic_batch_size = False , shuffle = False ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( "The length of inputs and targets should be equal" ) if shuffle : indices = np . arange ( len ( inputs ) ) np . random . shuffle ( indices ) # for start_idx in range(0, len(inputs) - batch_size + 1, batch_size): # chulei: handling the case where the number of samples is not a multiple of batch_size, avoiding wasting samples for start_idx in range ( 0 , len ( inputs ) , batch_size ) : end_idx = start_idx + batch_size if end_idx > len ( inputs ) : if allow_dynamic_batch_size : end_idx = len ( inputs ) else : break if shuffle : excerpt = indices [ start_idx : end_idx ] else : excerpt = slice ( start_idx , end_idx ) if ( isinstance ( inputs , list ) or isinstance ( targets , list ) ) and ( shuffle == True ) : # zsdonghao: for list indexing when shuffle==True yield [ inputs [ i ] for i in excerpt ] , [ targets [ i ] for i in excerpt ] else : yield inputs [ excerpt ] , targets [ excerpt ]
9599	def elements ( self , using , value ) : return self . _execute ( Command . FIND_ELEMENTS , { 'using' : using , 'value' : value } )
12660	def copy ( configfile = '' , destpath = '' , overwrite = False , sub_node = '' ) : log . info ( 'Running {0} {1} {2}' . format ( os . path . basename ( __file__ ) , whoami ( ) , locals ( ) ) ) assert ( os . path . isfile ( configfile ) ) if os . path . exists ( destpath ) : if os . listdir ( destpath ) : raise FolderAlreadyExists ( 'Folder {0} already exists. Please clean ' 'it or change destpath.' . format ( destpath ) ) else : log . info ( 'Creating folder {0}' . format ( destpath ) ) path ( destpath ) . makedirs_p ( ) from boyle . files . file_tree_map import FileTreeMap file_map = FileTreeMap ( ) try : file_map . from_config_file ( configfile ) except Exception as e : raise FileTreeMapError ( str ( e ) ) if sub_node : sub_map = file_map . get_node ( sub_node ) if not sub_map : raise FileTreeMapError ( 'Could not find sub node ' '{0}' . format ( sub_node ) ) file_map . _filetree = { } file_map . _filetree [ sub_node ] = sub_map try : file_map . copy_to ( destpath , overwrite = overwrite ) except Exception as e : raise FileTreeMapError ( str ( e ) )
9680	def config ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x3C ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 256 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) # Add the bin bounds to the dictionary of data [bytes 0-29] for i in range ( 0 , 15 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) # Add the Bin Particle Volumes (BPV) [bytes 32-95] for i in range ( 0 , 16 ) : data [ "BPV {0}" . format ( i ) ] = self . _calculate_float ( config [ 4 * i + 32 : 4 * i + 36 ] ) # Add the Bin Particle Densities (BPD) [bytes 96-159] for i in range ( 0 , 16 ) : data [ "BPD {0}" . format ( i ) ] = self . _calculate_float ( config [ 4 * i + 96 : 4 * i + 100 ] ) # Add the Bin Sample Volume Weight (BSVW) [bytes 160-223] for i in range ( 0 , 16 ) : data [ "BSVW {0}" . format ( i ) ] = self . _calculate_float ( config [ 4 * i + 160 : 4 * i + 164 ] ) # Add the Gain Scaling Coefficient (GSC) and sample flow rate (SFR) data [ "GSC" ] = self . _calculate_float ( config [ 224 : 228 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 228 : 232 ] ) # Add laser dac (LDAC) and Fan dac (FanDAC) data [ "LaserDAC" ] = config [ 232 ] data [ "FanDAC" ] = config [ 233 ] # If past firmware 15, add other things if self . firmware [ 'major' ] > 15. : data [ 'TOF_SFR' ] = config [ 234 ] sleep ( 0.1 ) return data
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
10211	def cmdclass ( path , enable = None , user = None ) : import warnings from setuptools . command . install import install from setuptools . command . develop import develop from os . path import dirname , join , exists , realpath from traceback import extract_stack try : # IPython/Jupyter 4.0 from notebook . nbextensions import install_nbextension from notebook . services . config import ConfigManager except ImportError : # Pre-schism try : from IPython . html . nbextensions import install_nbextension from IPython . html . services . config import ConfigManager except ImportError : warnings . warn ( "No jupyter notebook found in your environment. " "Hence jupyter nbextensions were not installed. " "If you would like to have them," "please issue 'pip install jupyter'." ) return { } # Check if the user flag was set. if user is None : user = not _is_root ( ) # Get the path of the extension calling_file = extract_stack ( ) [ - 2 ] [ 0 ] fullpath = realpath ( calling_file ) if not exists ( fullpath ) : raise Exception ( 'Could not find path of setup file.' ) extension_dir = join ( dirname ( fullpath ) , path ) # Installs the nbextension def run_nbextension_install ( develop ) : import sys sysprefix = hasattr ( sys , 'real_prefix' ) if sysprefix : install_nbextension ( extension_dir , symlink = develop , sys_prefix = sysprefix ) else : install_nbextension ( extension_dir , symlink = develop , user = user ) if enable is not None : print ( "Enabling the extension ..." ) cm = ConfigManager ( ) cm . update ( 'notebook' , { "load_extensions" : { enable : True } } ) # Command used for standard installs class InstallCommand ( install ) : def run ( self ) : print ( "Installing Python module..." ) install . run ( self ) print ( "Installing nbextension ..." ) run_nbextension_install ( False ) # Command used for development installs (symlinks the JS) class DevelopCommand ( develop ) : def run ( self ) : print ( "Installing Python module..." ) develop . run ( self ) print ( "Installing nbextension ..." ) run_nbextension_install ( True ) return { 'install' : InstallCommand , 'develop' : DevelopCommand , }
6	def cg ( f_Ax , b , cg_iters = 10 , callback = None , verbose = False , residual_tol = 1e-10 ) : p = b . copy ( ) r = b . copy ( ) x = np . zeros_like ( b ) rdotr = r . dot ( r ) fmtstr = "%10i %10.3g %10.3g" titlestr = "%10s %10s %10s" if verbose : print ( titlestr % ( "iter" , "residual norm" , "soln norm" ) ) for i in range ( cg_iters ) : if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i , rdotr , np . linalg . norm ( x ) ) ) z = f_Ax ( p ) v = rdotr / p . dot ( z ) x += v * p r -= v * z newrdotr = r . dot ( r ) mu = newrdotr / rdotr p = r + mu * p rdotr = newrdotr if rdotr < residual_tol : break if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i + 1 , rdotr , np . linalg . norm ( x ) ) ) # pylint: disable=W0631 return x
7722	def set_password ( self , password ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : child . unlinkNode ( ) child . freeNode ( ) break if password is not None : self . xmlnode . newTextChild ( self . xmlnode . ns ( ) , "password" , to_utf8 ( password ) )
9671	def _from_name ( self , string ) : components = string . split ( ' ' ) if frozenset ( components ) in self . features : return self . features [ frozenset ( components ) ] rest , sound_class = components [ : - 1 ] , components [ - 1 ] if sound_class in [ 'diphthong' , 'cluster' ] : if string . startswith ( 'from ' ) and 'to ' in string : extension = { 'diphthong' : 'vowel' , 'cluster' : 'consonant' } [ sound_class ] string_ = ' ' . join ( string . split ( ' ' ) [ 1 : - 1 ] ) from_ , to_ = string_ . split ( ' to ' ) v1 , v2 = frozenset ( from_ . split ( ' ' ) + [ extension ] ) , frozenset ( to_ . split ( ' ' ) + [ extension ] ) if v1 in self . features and v2 in self . features : s1 , s2 = ( self . features [ v1 ] , self . features [ v2 ] ) if sound_class == 'diphthong' : return Diphthong . from_sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 else : return Cluster . from_sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 else : # try to generate the sounds if they are not there s1 , s2 = self . _from_name ( from_ + ' ' + extension ) , self . _from_name ( to_ + ' ' + extension ) if not ( isinstance ( s1 , UnknownSound ) or isinstance ( s2 , UnknownSound ) ) : # noqa: F405 if sound_class == 'diphthong' : return Diphthong . from_sounds ( # noqa: F405 s1 + s2 , s1 , s2 , self ) return Cluster . from_sounds ( s1 + s2 , s1 , s2 , self ) # noqa: F405 raise ValueError ( 'components could not be found in system' ) else : raise ValueError ( 'name string is erroneously encoded' ) if sound_class not in self . sound_classes : raise ValueError ( 'no sound class specified' ) args = { self . _feature_values . get ( comp , '?' ) : comp for comp in rest } if '?' in args : raise ValueError ( 'string contains unknown features' ) args [ 'grapheme' ] = '' args [ 'ts' ] = self sound = self . sound_classes [ sound_class ] ( * * args ) if sound . featureset not in self . features : sound . generated = True return sound return self . features [ sound . featureset ]
6841	def supported_locales ( ) : family = distrib_family ( ) if family == 'debian' : return _parse_locales ( '/usr/share/i18n/SUPPORTED' ) elif family == 'arch' : return _parse_locales ( '/etc/locale.gen' ) elif family == 'redhat' : return _supported_locales_redhat ( ) else : raise UnsupportedFamily ( supported = [ 'debian' , 'arch' , 'redhat' ] )
3618	def set_settings ( self ) : if not self . settings : return try : self . __index . set_settings ( self . settings ) logger . info ( 'APPLY SETTINGS ON %s' , self . index_name ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'SETTINGS NOT APPLIED ON %s: %s' , self . model , e )
1323	def SetActive ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : handle = self . NativeWindowHandle if IsIconic ( handle ) : ret = ShowWindow ( handle , SW . Restore ) elif not IsWindowVisible ( handle ) : ret = ShowWindow ( handle , SW . Show ) ret = SetForegroundWindow ( handle ) # may fail if foreground windows's process is not python time . sleep ( waitTime ) return ret return False
6299	def parse_package_string ( path ) : parts = path . split ( '.' ) # Is the last entry in the path capitalized? if parts [ - 1 ] [ 0 ] . isupper ( ) : return "." . join ( parts [ : - 1 ] ) , parts [ - 1 ] return path , ""
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 # Get the distance and angle between point and light source. d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 # If no angle is defined, # light is emitted evenly in all directions # and carries as far as the defined radius # (e.g. like a radial gradient). if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 # Normalize the light's direction and spread # between 0 and 360. angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 # Objects that fall within the spreaded direction # of the light are illuminated. d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 # Wrapping from 0 to 360: # a light source with a direction of 10 degrees # and a spread of 45 degrees illuminates # objects between 0 and 35 degrees and 350 and 360 degrees. if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 # Wrapping from 360 to 0. if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
1528	def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
2850	def _mpsse_enable ( self ) : # Reset MPSSE by sending mask = 0 and mode = 0 self . _check ( ftdi . set_bitmode , 0 , 0 ) # Enable MPSSE by sending mask = 0 and mode = 2 self . _check ( ftdi . set_bitmode , 0 , 2 )
5918	def outfile ( self , p ) : if self . outdir is not None : return os . path . join ( self . outdir , os . path . basename ( p ) ) else : return p
3871	def next_event ( self , event_id , prev = False ) : i = self . events . index ( self . _events_dict [ event_id ] ) if prev and i > 0 : return self . events [ i - 1 ] elif not prev and i + 1 < len ( self . events ) : return self . events [ i + 1 ] else : return None
4479	def file_empty ( fp ) : # for python 2 we need to use a homemade peek() if six . PY2 : contents = fp . read ( ) fp . seek ( 0 ) return not bool ( contents ) else : return not fp . peek ( )
6556	def projection ( self , variables ) : # resolve iterables or mutability problems by casting the variables to a set variables = set ( variables ) if not variables . issubset ( self . variables ) : raise ValueError ( "Cannot project to variables not in the constraint." ) idxs = [ i for i , v in enumerate ( self . variables ) if v in variables ] configurations = frozenset ( tuple ( config [ i ] for i in idxs ) for config in self . configurations ) variables = tuple ( self . variables [ i ] for i in idxs ) return self . from_configurations ( configurations , variables , self . vartype )
426	def check_unfinished_task ( self , task_name = None , * * kwargs ) : if not isinstance ( task_name , str ) : # is None: raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) # ## find task # task = self.db.Task.find_one(kwargs) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( "[Database] No unfinished task - task_name: {}" . format ( task_name ) ) return False else : logging . info ( "[Database] Find {} unfinished task - task_name: {}" . format ( n_task , task_name ) ) return True
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
6580	def _ensure_started ( self ) : if self . _process and self . _process . poll ( ) is None : return if not getattr ( self , "_cmd" ) : raise RuntimeError ( "Player command is not configured" ) log . debug ( "Starting playback command: %r" , self . _cmd ) self . _process = SilentPopen ( self . _cmd ) self . _post_start ( )
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
12513	def _crop_img_to ( image , slices , copy = True ) : img = check_img ( image ) data = img . get_data ( ) affine = img . get_affine ( ) cropped_data = data [ slices ] if copy : cropped_data = cropped_data . copy ( ) linear_part = affine [ : 3 , : 3 ] old_origin = affine [ : 3 , 3 ] new_origin_voxel = np . array ( [ s . start for s in slices ] ) new_origin = old_origin + linear_part . dot ( new_origin_voxel ) new_affine = np . eye ( 4 ) new_affine [ : 3 , : 3 ] = linear_part new_affine [ : 3 , 3 ] = new_origin new_img = nib . Nifti1Image ( cropped_data , new_affine ) return new_img
7074	def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_range = ( 1.0 , 20.0 ) , iqr_stdev_range = ( 1.0 , 20.0 ) , ngridpoints = 32 , ngridworkers = None ) : # make the output directory where all the pkls from the variability # threshold runs will go outdir = os . path . join ( simbasedir , 'recvar-threshold-pkls' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) # get the info from the simbasedir with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # get the magbinmedians to use for the recovery processing magbinmedians = siminfo [ 'magrms' ] [ magcols [ 0 ] ] [ 'binned_sdssr_median' ] # generate the grids for stetson and inveta stetson_grid = np . linspace ( stetson_stdev_range [ 0 ] , stetson_stdev_range [ 1 ] , num = ngridpoints ) inveta_grid = np . linspace ( inveta_stdev_range [ 0 ] , inveta_stdev_range [ 1 ] , num = ngridpoints ) iqr_grid = np . linspace ( iqr_stdev_range [ 0 ] , iqr_stdev_range [ 1 ] , num = ngridpoints ) # generate the grid stet_inveta_iqr_grid = [ ] for stet in stetson_grid : for inveta in inveta_grid : for iqr in iqr_grid : grid_point = [ stet , inveta , iqr ] stet_inveta_iqr_grid . append ( grid_point ) # the output dict grid_results = { 'stetson_grid' : stetson_grid , 'inveta_grid' : inveta_grid , 'iqr_grid' : iqr_grid , 'stet_inveta_iqr_grid' : stet_inveta_iqr_grid , 'magbinmedians' : magbinmedians , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'simbasedir' : os . path . abspath ( simbasedir ) , 'recovery' : [ ] } # set up the pool pool = mp . Pool ( ngridworkers ) # run the grid search per magbinmedian for magbinmedian in magbinmedians : LOGINFO ( 'running stetson J-inveta grid-search ' 'for magbinmedian = %.3f...' % magbinmedian ) tasks = [ ( simbasedir , gp , magbinmedian ) for gp in stet_inveta_iqr_grid ] thisbin_results = pool . map ( magbin_varind_gridsearch_worker , tasks ) grid_results [ 'recovery' ] . append ( thisbin_results ) pool . close ( ) pool . join ( ) LOGINFO ( 'done.' ) with open ( os . path . join ( simbasedir , 'fakevar-recovery-per-magbin.pkl' ) , 'wb' ) as outfd : pickle . dump ( grid_results , outfd , pickle . HIGHEST_PROTOCOL ) return grid_results
3941	async def _fetch_channel_sid ( self ) : logger . info ( 'Requesting new gsessionid and SID...' ) # Set SID and gsessionid to None so they aren't sent in by send_maps. self . _sid_param = None self . _gsessionid_param = None res = await self . send_maps ( [ ] ) self . _sid_param , self . _gsessionid_param = _parse_sid_response ( res . body ) logger . info ( 'New SID: {}' . format ( self . _sid_param ) ) logger . info ( 'New gsessionid: {}' . format ( self . _gsessionid_param ) )
4643	def items ( self ) : query = "SELECT {}, {} from {}" . format ( self . __key__ , self . __value__ , self . __tablename__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) r = [ ] for key , value in cursor . fetchall ( ) : r . append ( ( key , value ) ) return r
10950	def sample ( field , inds = None , slicer = None , flat = True ) : if inds is not None : out = field . ravel ( ) [ inds ] elif slicer is not None : out = field [ slicer ] . ravel ( ) else : out = field if flat : return out . ravel ( ) return out
7481	def cleanup_tempfiles ( data ) : ## remove align-related tmp files tmps1 = glob . glob ( os . path . join ( data . tmpdir , "*.fa" ) ) tmps2 = glob . glob ( os . path . join ( data . tmpdir , "*.npy" ) ) for tmp in tmps1 + tmps2 : if os . path . exists ( tmp ) : os . remove ( tmp ) ## remove cluster related files removal = [ os . path . join ( data . dirs . across , data . name + ".utemp" ) , os . path . join ( data . dirs . across , data . name + ".htemp" ) , os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) , os . path . join ( data . dirs . across , data . name + "_cathaps.tmp" ) , os . path . join ( data . dirs . across , data . name + "_catshuf.tmp" ) , os . path . join ( data . dirs . across , data . name + "_catsort.tmp" ) , os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) , os . path . join ( data . dirs . across , data . name + ".tmp.indels.hdf5" ) , ] for rfile in removal : if os . path . exists ( rfile ) : os . remove ( rfile ) ## remove singlecat related h5 files smpios = glob . glob ( os . path . join ( data . dirs . across , '*.tmp.h5' ) ) for smpio in smpios : if os . path . exists ( smpio ) : os . remove ( smpio )
12446	def resource ( * * kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . __name__ ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string_types ) : # Tuple-ify the method if we got just a string. methods = methods , # Construct a handler. handler = ( function , methods ) if name not in _resources : # Initiate the handlers list. _handlers [ name ] = [ ] # Construct a light-weight resource using the passed kwargs # as the arguments for the meta. from armet import resources kwargs [ 'name' ] = name class LightweightResource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in _handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) # Construct and add this resource. _resources [ name ] = LightweightResource # Add this to the handlers. _handlers [ name ] . append ( handler ) # Return the resource. return _resources [ name ] # Return the inner method. return inner
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
10588	def _get_account_and_descendants_ ( self , account , result ) : result . append ( account ) for child in account . accounts : self . _get_account_and_descendants_ ( child , result )
13244	def temp_db ( db , name = None ) : if name is None : name = temp_name ( ) db . create ( name ) if not db . exists ( name ) : raise DatabaseError ( 'failed to create database %s!' ) try : yield name finally : db . drop ( name ) if db . exists ( name ) : raise DatabaseError ( 'failed to drop database %s!' )
2683	def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
10833	def query_by_admin ( cls , admin ) : return cls . query . filter_by ( admin_type = resolve_admin_type ( admin ) , admin_id = admin . get_id ( ) )
10556	def update_helping_material ( helpingmaterial ) : try : helpingmaterial_id = helpingmaterial . id helpingmaterial = _forbidden_attributes ( helpingmaterial ) res = _pybossa_req ( 'put' , 'helpingmaterial' , helpingmaterial_id , payload = helpingmaterial . data ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : # pragma: no cover raise
2947	def deserialize ( cls , serializer , wf_spec , s_state , * * kwargs ) : return serializer . deserialize_trigger ( wf_spec , s_state , * * kwargs )
5167	def __intermediate_dns_search ( self , uci , address ) : # allow override if 'dns_search' in uci : return uci [ 'dns_search' ] # ignore if "proto" is none if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
12109	def _review_all ( self , launchers ) : # Run review of launch args if necessary if self . launch_args is not None : proceed = self . review_args ( self . launch_args , show_repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review_args , self . review_command , self . review_launcher ] for ( count , launcher ) in enumerate ( launchers ) : # Run reviews for all launchers if desired... if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False # But allow the user to skip these extra reviews if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip_remaining = self . input_options ( [ 'Y' , 'n' , 'quit' ] , '\nSkip remaining reviews?' , default = 'y' ) if skip_remaining == 'y' : break elif skip_remaining == 'quit' : return False if self . input_options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . _launch_all ( launchers )
5341	def __get_menu_entries ( self , kibiter_major ) : menu_entries = [ ] for entry in self . panels_menu : if entry [ 'source' ] not in self . data_sources : continue parent_menu_item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash_name = get_dashboard_name ( subentry [ 'panel' ] ) except FileNotFoundError : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue # The name for the entry is in self.panels_menu child_item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel_id" : dash_name } parent_menu_item [ 'dashboards' ] . append ( child_item ) menu_entries . append ( parent_menu_item ) return menu_entries
1629	def GetHeaderGuardCPPVariable ( filename ) : # Restores original filename in case that cpplint is invoked from Emacs's # flymake. filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) # Replace 'c++' with 'cpp'. filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep # On Windows using directory separator will leave us with # "bogus escape error" unless we properly escape regex. if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
4354	def _save_ack_callback ( self , msgid , callback ) : if msgid in self . ack_callbacks : return False self . ack_callbacks [ msgid ] = callback
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
7507	def _renamer ( self , tre ) : ## get the tre with numbered tree tip labels names = tre . get_leaves ( ) ## replace numbered names with snames for name in names : name . name = self . samples [ int ( name . name ) ] ## return with only topology and leaf labels return tre . write ( format = 9 )
3638	def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
1943	def protect_memory_callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . _emu . mem_protect ( start , size , convert_permissions ( perms ) )
10181	def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
5315	def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : # we assume it's a path to a color file colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
9754	def get ( ctx , job ) : def get_experiment ( ) : try : response = PolyaxonClient ( ) . experiment . get_experiment ( user , project_name , _experiment ) cache . cache ( config_manager = ExperimentManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load experiment `{}` info.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_experiment_details ( response ) def get_experiment_job ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_job ( user , project_name , _experiment , _job ) cache . cache ( config_manager = ExperimentJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . resources : get_resources ( response . resources . to_dict ( ) , header = "Job resources:" ) response = Printer . add_status_color ( response . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'definition' , 'experiment' , 'unique_name' , 'resources' ] ) ) Printer . print_header ( "Job info:" ) dict_tabulate ( response ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job ( ) else : get_experiment ( )
2890	def create_task ( self ) : return self . spec_class ( self . spec , self . get_task_spec_name ( ) , lane = self . get_lane ( ) , description = self . node . get ( 'name' , None ) )
13890	def CreateLink ( target_path , link_path , override = True ) : _AssertIsLocal ( target_path ) _AssertIsLocal ( link_path ) if override and IsLink ( link_path ) : DeleteLink ( link_path ) # Create directories leading up to link dirname = os . path . dirname ( link_path ) if dirname : CreateDirectory ( dirname ) if sys . platform != 'win32' : return os . symlink ( target_path , link_path ) # @UndefinedVariable else : #import ntfsutils.junction #return ntfsutils.junction.create(target_path, link_path) import jaraco . windows . filesystem return jaraco . windows . filesystem . symlink ( target_path , link_path ) from . _easyfs_win32 import CreateSymbolicLink try : dw_flags = 0 if target_path and os . path . isdir ( target_path ) : dw_flags = 1 return CreateSymbolicLink ( target_path , link_path , dw_flags ) except Exception as e : reraise ( e , 'Creating link "%(link_path)s" pointing to "%(target_path)s"' % locals ( ) )
5333	def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
4324	def convert ( self , samplerate = None , n_channels = None , bitdepth = None ) : bitdepths = [ 8 , 16 , 24 , 32 , 64 ] if bitdepth is not None : if bitdepth not in bitdepths : raise ValueError ( "bitdepth must be one of {}." . format ( str ( bitdepths ) ) ) self . output_format . extend ( [ '-b' , '{}' . format ( bitdepth ) ] ) if n_channels is not None : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( "n_channels must be a positive integer." ) self . output_format . extend ( [ '-c' , '{}' . format ( n_channels ) ] ) if samplerate is not None : if not is_number ( samplerate ) or samplerate <= 0 : raise ValueError ( "samplerate must be a positive number." ) self . rate ( samplerate ) return self
570	def _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) : msg = StringIO . StringIO ( ) print >> msg , "Exception occurred while running model %s: %r (%s)" % ( modelID , e , type ( e ) ) traceback . print_exc ( None , msg ) completionReason = jobsDAO . CMPL_REASON_ERROR completionMsg = msg . getvalue ( ) logger . error ( completionMsg ) # Write results to the model database for the error case. Ignore # InvalidConnectionException, as this is usually caused by orphaned models # # TODO: do we really want to set numRecords to 0? Last updated value might # be useful for debugging if type ( e ) is not InvalidConnectionException : jobsDAO . modelUpdateResults ( modelID , results = None , numRecords = 0 ) # TODO: Make sure this wasn't the best model in job. If so, set the best # appropriately # If this was an exception that should mark the job as failed, do that # now. if type ( e ) == JobFailException : workerCmpReason = jobsDAO . jobGetFields ( jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : jobsDAO . jobSetFields ( jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = ": " . join ( str ( i ) for i in e . args ) ) , useConnectionID = False , ignoreUnchanged = True ) return ( completionReason , completionMsg )
4649	def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
12522	def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
11955	def is_dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except ValueError : return False if val > 255 or val < 0 : return False return True
6629	def read ( self , filenames ) : for fn in filenames : try : self . configs [ fn ] = ordered_json . load ( fn ) except IOError : self . configs [ fn ] = OrderedDict ( ) except Exception as e : self . configs [ fn ] = OrderedDict ( ) logging . warning ( "Failed to read settings file %s, it will be ignored. The error was: %s" , fn , e )
7076	def periodrec_worker ( task ) : pfpkl , simbasedir , period_tolerance = task try : return periodicvar_recovery ( pfpkl , simbasedir , period_tolerance = period_tolerance ) except Exception as e : LOGEXCEPTION ( 'periodic var recovery failed for %s' % repr ( task ) ) return None
5084	def unlink_learners ( self ) : sap_inactive_learners = self . client . get_inactive_sap_learners ( ) enterprise_customer = self . enterprise_configuration . enterprise_customer if not sap_inactive_learners : LOGGER . info ( 'Enterprise customer {%s} has no SAPSF inactive learners' , enterprise_customer . name ) return provider_id = enterprise_customer . identity_provider tpa_provider = get_identity_provider ( provider_id ) if not tpa_provider : LOGGER . info ( 'Enterprise customer {%s} has no associated identity provider' , enterprise_customer . name ) return None for sap_inactive_learner in sap_inactive_learners : social_auth_user = get_user_from_social_auth ( tpa_provider , sap_inactive_learner [ 'studentID' ] ) if not social_auth_user : continue try : # Unlink user email from related Enterprise Customer EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = social_auth_user . email , ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : LOGGER . info ( 'Learner with email {%s} is not associated with Enterprise Customer {%s}' , social_auth_user . email , enterprise_customer . name )
7322	def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : # Relative paths are relative to the template's parent directory attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) # Check that the attachment exists if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : # read the matrix header and array vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) # move on to next field fd . seek ( next_pos ) # pack and return the array if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
13740	def connect ( self ) : if not self . connected ( ) : self . _ws = create_connection ( self . WS_URI ) message = { 'type' : self . WS_TYPE , 'product_id' : self . WS_PRODUCT_ID } self . _ws . send ( dumps ( message ) ) # There will be only one keep alive thread per client instance with self . _lock : if not self . _thread : thread = Thread ( target = self . _keep_alive_thread , args = [ ] ) thread . start ( )
10012	def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
668	def sample ( self , rgen ) : x = rgen . poisson ( self . lambdaParameter ) return x , self . logDensity ( x )
5947	def unlink_f ( path ) : try : os . unlink ( path ) except OSError as err : if err . errno != errno . ENOENT : raise
2640	def runner ( incoming_q , outgoing_q ) : logger . debug ( "[RUNNER] Starting" ) def execute_task ( bufs ) : """Deserialize the buffer and execute the task. Returns the serialized result or exception. """ user_ns = locals ( ) user_ns . update ( { '__builtins__' : __builtins__ } ) f , args , kwargs = unpack_apply_message ( bufs , user_ns , copy = False ) fname = getattr ( f , '__name__' , 'f' ) prefix = "parsl_" fname = prefix + "f" argname = prefix + "args" kwargname = prefix + "kwargs" resultname = prefix + "result" user_ns . update ( { fname : f , argname : args , kwargname : kwargs , resultname : resultname } ) code = "{0} = {1}(*{2}, **{3})" . format ( resultname , fname , argname , kwargname ) try : logger . debug ( "[RUNNER] Executing: {0}" . format ( code ) ) exec ( code , user_ns , user_ns ) except Exception as e : logger . warning ( "Caught exception; will raise it: {}" . format ( e ) ) raise e else : logger . debug ( "[RUNNER] Result: {0}" . format ( user_ns . get ( resultname ) ) ) return user_ns . get ( resultname ) while True : try : # Blocking wait on the queue msg = incoming_q . get ( block = True , timeout = 10 ) except queue . Empty : # Handle case where no items were in the queue logger . debug ( "[RUNNER] Queue is empty" ) except IOError as e : logger . debug ( "[RUNNER] Broken pipe: {}" . format ( e ) ) try : # Attempt to send a stop notification to the management thread outgoing_q . put ( None ) except Exception : pass break except Exception as e : logger . debug ( "[RUNNER] Caught unknown exception: {}" . format ( e ) ) else : # Handle received message if not msg : # Empty message is a die request logger . debug ( "[RUNNER] Received exit request" ) outgoing_q . put ( None ) break else : # Received a valid message, handle it logger . debug ( "[RUNNER] Got a valid task with ID {}" . format ( msg [ "task_id" ] ) ) try : response_obj = execute_task ( msg [ 'buffer' ] ) response = { "task_id" : msg [ "task_id" ] , "result" : serialize_object ( response_obj ) } logger . debug ( "[RUNNER] Returing result: {}" . format ( deserialize_object ( response [ "result" ] ) ) ) except Exception as e : logger . debug ( "[RUNNER] Caught task exception: {}" . format ( e ) ) response = { "task_id" : msg [ "task_id" ] , "exception" : serialize_object ( e ) } outgoing_q . put ( response ) logger . debug ( "[RUNNER] Terminating" )
5917	def check_output ( self , make_ndx_output , message = None , err = None ) : if message is None : message = "" else : message = '\n' + message def format ( output , w = 60 ) : hrule = "====[ GromacsError (diagnostic output) ]" . ljust ( w , "=" ) return hrule + '\n' + str ( output ) + hrule rc = True if self . _is_empty_group ( make_ndx_output ) : warnings . warn ( "Selection produced empty group.{message!s}" . format ( * * vars ( ) ) , category = GromacsValueWarning ) rc = False if self . _has_syntax_error ( make_ndx_output ) : rc = False out_formatted = format ( make_ndx_output ) raise GromacsError ( "make_ndx encountered a Syntax Error, " "%(message)s\noutput:\n%(out_formatted)s" % vars ( ) ) if make_ndx_output . strip ( ) == "" : rc = False out_formatted = format ( err ) raise GromacsError ( "make_ndx produced no output, " "%(message)s\nerror output:\n%(out_formatted)s" % vars ( ) ) return rc
8741	def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) # TODO(blogan): Since the extension logic will reject any requests without # floating_network_id, is this still needed? if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
6092	def cos_and_sin_from_x_axis ( self ) : phi_radians = np . radians ( self . phi ) return np . cos ( phi_radians ) , np . sin ( phi_radians )
3161	def get ( self , conversation_id , * * queryparams ) : self . conversation_id = conversation_id return self . _mc_client . _get ( url = self . _build_path ( conversation_id ) , * * queryparams )
6048	def relocated_grid_stack_from_grid_stack ( self , grid_stack ) : border_grid = grid_stack . regular [ self ] return GridStack ( regular = self . relocated_grid_from_grid_jit ( grid = grid_stack . regular , border_grid = border_grid ) , sub = self . relocated_grid_from_grid_jit ( grid = grid_stack . sub , border_grid = border_grid ) , blurring = None , pix = self . relocated_grid_from_grid_jit ( grid = grid_stack . pix , border_grid = border_grid ) )
6811	def pre_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_pre_deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
10648	def remove_component ( self , name ) : component_to_remove = None for c in self . components : if c . name == name : component_to_remove = c if component_to_remove is not None : self . components . remove ( component_to_remove )
11789	def sample ( self ) : if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
7774	def _compute_response ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) : # pylint: disable-msg=C0103,R0913 logger . debug ( "_compute_response{0!r}" . format ( ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) ) ) if authzid : a1 = b":" . join ( ( urp_hash , nonce , cnonce , authzid ) ) else : a1 = b":" . join ( ( urp_hash , nonce , cnonce ) ) a2 = b"AUTHENTICATE:" + digest_uri return b2a_hex ( _kd_value ( b2a_hex ( _h_value ( a1 ) ) , b":" . join ( ( nonce , nonce_count , cnonce , b"auth" , b2a_hex ( _h_value ( a2 ) ) ) ) ) )
4827	def get_course_enrollment ( self , username , course_id ) : endpoint = getattr ( self . client . enrollment , '{username},{course_id}' . format ( username = username , course_id = course_id ) ) try : result = endpoint . get ( ) except HttpNotFoundError : # This enrollment data endpoint returns a 404 if either the username or course_id specified isn't valid LOGGER . error ( 'Course enrollment details not found for invalid username or course; username=[%s], course=[%s]' , username , course_id ) return None # This enrollment data endpoint returns an empty string if the username and course_id is valid, but there's # no matching enrollment found if not result : LOGGER . info ( 'Failed to find course enrollment details for user [%s] and course [%s]' , username , course_id ) return None return result
435	def tsne_embedding ( embeddings , reverse_dictionary , plot_only = 500 , second = 5 , saveable = False , name = 'tsne' , fig_idx = 9862 ) : import matplotlib . pyplot as plt def plot_with_labels ( low_dim_embs , labels , figsize = ( 18 , 18 ) , second = 5 , saveable = True , name = 'tsne' , fig_idx = 9862 ) : if low_dim_embs . shape [ 0 ] < len ( labels ) : raise AssertionError ( "More labels than embeddings" ) if saveable is False : plt . ion ( ) plt . figure ( fig_idx ) plt . figure ( figsize = figsize ) # in inches for i , label in enumerate ( labels ) : x , y = low_dim_embs [ i , : ] plt . scatter ( x , y ) plt . annotate ( label , xy = ( x , y ) , xytext = ( 5 , 2 ) , textcoords = 'offset points' , ha = 'right' , va = 'bottom' ) if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second ) try : from sklearn . manifold import TSNE from six . moves import xrange tsne = TSNE ( perplexity = 30 , n_components = 2 , init = 'pca' , n_iter = 5000 ) # plot_only = 500 low_dim_embs = tsne . fit_transform ( embeddings [ : plot_only , : ] ) labels = [ reverse_dictionary [ i ] for i in xrange ( plot_only ) ] plot_with_labels ( low_dim_embs , labels , second = second , saveable = saveable , name = name , fig_idx = fig_idx ) except ImportError : _err = "Please install sklearn and matplotlib to visualize embeddings." tl . logging . error ( _err ) raise ImportError ( _err )
2862	def _i2c_write_bytes ( self , data ) : for byte in data : # Write byte. self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) # Make sure pins are back in idle state with clock low and data high. self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) # Read bit for ACK/NAK. self . _command . append ( '\x22\x00' ) # Increase expected response bytes. self . _expected += len ( data )
6935	def add_cmds_cpdir ( cpdir , cmdpkl , cpfileglob = 'checkplot*.pkl*' , require_cmd_magcolor = True , save_cmd_pngs = False ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
2840	def write_gpio ( self , gpio = None ) : if gpio is not None : self . gpio = gpio self . _device . writeList ( self . GPIO , self . gpio )
13446	def messages_from_response ( response ) : messages = [ ] if hasattr ( response , 'context' ) and response . context and 'messages' in response . context : messages = response . context [ 'messages' ] elif hasattr ( response , 'cookies' ) : # no "context" set-up or no messages item, check for message info in # the cookies morsel = response . cookies . get ( 'messages' ) if not morsel : return [ ] # use the decoder in the CookieStore to process and get a list of # messages from django . contrib . messages . storage . cookie import CookieStorage store = CookieStorage ( FakeRequest ( ) ) messages = store . _decode ( morsel . value ) else : return [ ] return [ ( m . message , m . level ) for m in messages ]
2575	def launch_task ( self , task_id , executable , * args , * * kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , * * kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
9014	def _fill_pattern_collection ( self , pattern_collection , values ) : pattern = values . get ( PATTERNS , [ ] ) for pattern_to_parse in pattern : parsed_pattern = self . _pattern ( pattern_to_parse ) pattern_collection . append ( parsed_pattern )
10065	def json_serializer ( pid , data , * args ) : if data is not None : response = Response ( json . dumps ( data . dumps ( ) ) , mimetype = 'application/json' ) else : response = Response ( mimetype = 'application/json' ) return response
8036	def lexrank ( sentences , continuous = False , sim_threshold = 0.1 , alpha = 0.9 , use_divrank = False , divrank_alpha = 0.25 ) : # configure ranker ranker_params = { 'max_iter' : 1000 } if use_divrank : ranker = divrank_scipy ranker_params [ 'alpha' ] = divrank_alpha ranker_params [ 'd' ] = alpha else : ranker = networkx . pagerank_scipy ranker_params [ 'alpha' ] = alpha graph = networkx . DiGraph ( ) # sentence -> tf sent_tf_list = [ ] for sent in sentences : words = tools . word_segmenter_ja ( sent ) tf = collections . Counter ( words ) sent_tf_list . append ( tf ) sent_vectorizer = DictVectorizer ( sparse = True ) sent_vecs = sent_vectorizer . fit_transform ( sent_tf_list ) # compute similarities between senteces sim_mat = 1 - pairwise_distances ( sent_vecs , sent_vecs , metric = 'cosine' ) if continuous : linked_rows , linked_cols = numpy . where ( sim_mat > 0 ) else : linked_rows , linked_cols = numpy . where ( sim_mat >= sim_threshold ) # create similarity graph graph . add_nodes_from ( range ( sent_vecs . shape [ 0 ] ) ) for i , j in zip ( linked_rows , linked_cols ) : if i == j : continue weight = sim_mat [ i , j ] if continuous else 1.0 graph . add_edge ( i , j , { 'weight' : weight } ) scores = ranker ( graph , * * ranker_params ) return scores , sim_mat
5952	def start_logging ( logfile = "gromacs.log" ) : from . import log log . create ( "gromacs" , logfile = logfile ) logging . getLogger ( "gromacs" ) . info ( "GromacsWrapper %s STARTED logging to %r" , __version__ , logfile )
6431	def dist_abs ( self , src , tar ) : if src == tar : return 6 if src == '' or tar == '' : return 0 src = list ( mra ( src ) ) tar = list ( mra ( tar ) ) if abs ( len ( src ) - len ( tar ) ) > 2 : return 0 length_sum = len ( src ) + len ( tar ) if length_sum < 5 : min_rating = 5 elif length_sum < 8 : min_rating = 4 elif length_sum < 12 : min_rating = 3 else : min_rating = 2 for _ in range ( 2 ) : new_src = [ ] new_tar = [ ] minlen = min ( len ( src ) , len ( tar ) ) for i in range ( minlen ) : if src [ i ] != tar [ i ] : new_src . append ( src [ i ] ) new_tar . append ( tar [ i ] ) src = new_src + src [ minlen : ] tar = new_tar + tar [ minlen : ] src . reverse ( ) tar . reverse ( ) similarity = 6 - max ( len ( src ) , len ( tar ) ) if similarity >= min_rating : return similarity return 0
10451	def grabfocus ( self , window_name , object_name = None ) : if not object_name : handle , name , app = self . _get_window_handle ( window_name ) else : handle = self . _get_object_handle ( window_name , object_name ) return self . _grabfocus ( handle )
377	def pixel_value_scale ( im , val = 0.9 , clip = None , is_random = False ) : clip = clip if clip is not None else ( - np . inf , np . inf ) if is_random : scale = 1 + np . random . uniform ( - val , val ) im = im * scale else : im = im * val if len ( clip ) == 2 : im = np . clip ( im , clip [ 0 ] , clip [ 1 ] ) else : raise Exception ( "clip : tuple of 2 numbers" ) return im
12554	def sav_to_pandas_rpy2 ( input_file ) : import pandas . rpy . common as com w = com . robj . r ( 'foreign::read.spss("%s", to.data.frame=TRUE)' % input_file ) return com . convert_robj ( w )
2315	def orient_undirected_graph ( self , data , graph , * * kwargs ) : # Building setup w/ arguments. self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_pc ( data , fixedEdges = fe , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11994	def set_algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature_algorithms = self . _update_dict ( signature , self . DEFAULT_SIGNATURE ) self . encryption_algorithms = self . _update_dict ( encryption , self . DEFAULT_ENCRYPTION ) self . serialization_algorithms = self . _update_dict ( serialization , self . DEFAULT_SERIALIZATION ) self . compression_algorithms = self . _update_dict ( compression , self . DEFAULT_COMPRESSION )
2904	def new_workflow ( self , workflow_spec , read_only = False , * * kwargs ) : return BpmnWorkflow ( workflow_spec , read_only = read_only , * * kwargs )
10865	def _tile ( self , n ) : zsc = np . array ( [ 1.0 / self . zscale , 1 , 1 ] ) pos , rad = self . pos [ n ] , self . rad [ n ] pos = self . _trans ( pos ) return Tile ( pos - zsc * rad , pos + zsc * rad ) . pad ( self . support_pad )
11617	def _brahmic ( data , scheme_map , * * kw ) : if scheme_map . from_scheme . name == northern . GURMUKHI : data = northern . GurmukhiScheme . replace_tippi ( text = data ) marks = scheme_map . marks virama = scheme_map . virama consonants = scheme_map . consonants non_marks_viraama = scheme_map . non_marks_viraama to_roman = scheme_map . to_scheme . is_roman max_key_length_from_scheme = scheme_map . max_key_length_from_scheme buf = [ ] i = 0 to_roman_had_consonant = found = False append = buf . append # logging.debug(pprint.pformat(scheme_map.consonants)) # We dont just translate each brAhmic character one after another in order to prefer concise transliterations when possible - for example  -> jn in optitrans rather than j~n. while i <= len ( data ) : # The longest token in the source scheme has length `max_key_length_from_scheme`. Iterate # over `data` while taking `max_key_length_from_scheme` characters at a time. If we don`t # find the character group in our scheme map, lop off a character and # try again. # # If we've finished reading through `data`, then `token` will be empty # and the loop below will be skipped. token = data [ i : i + max_key_length_from_scheme ] while token : if len ( token ) == 1 : if token in marks : append ( marks [ token ] ) found = True elif token in virama : append ( virama [ token ] ) found = True else : if to_roman_had_consonant : append ( 'a' ) append ( non_marks_viraama . get ( token , token ) ) found = True else : if token in non_marks_viraama : if to_roman_had_consonant : append ( 'a' ) append ( non_marks_viraama . get ( token ) ) found = True if found : to_roman_had_consonant = to_roman and token in consonants i += len ( token ) break else : token = token [ : - 1 ] # Continuing the outer while loop. # We've exhausted the token; this must be some other character. Due to # the implicit 'a', we must explicitly end any lingering consonants # before we can handle the current token. if not found : if to_roman_had_consonant : append ( next ( iter ( virama . values ( ) ) ) ) if i < len ( data ) : append ( data [ i ] ) to_roman_had_consonant = False i += 1 found = False if to_roman_had_consonant : append ( 'a' ) return '' . join ( buf )
10171	def post ( self , * * kwargs ) : data = request . get_json ( force = False ) if data is None : data = { } result = { } for query_name , config in data . items ( ) : if config is None or not isinstance ( config , dict ) or ( set ( config . keys ( ) ) != { 'stat' , 'params' } and set ( config . keys ( ) ) != { 'stat' } ) : raise InvalidRequestInputError ( 'Invalid Input. It should be of the form ' '{ STATISTIC_NAME: { "stat": STAT_TYPE, ' '"params": STAT_PARAMS \}}' ) stat = config [ 'stat' ] params = config . get ( 'params' , { } ) try : query_cfg = current_stats . queries [ stat ] except KeyError : raise UnknownQueryError ( stat ) permission = current_stats . permission_factory ( stat , params ) if permission is not None and not permission . can ( ) : message = ( 'You do not have a permission to query the ' 'statistic "{}" with those ' 'parameters' . format ( stat ) ) if current_user . is_authenticated : abort ( 403 , message ) abort ( 401 , message ) try : query = query_cfg . query_class ( * * query_cfg . query_config ) result [ query_name ] = query . run ( * * params ) except ValueError as e : raise InvalidRequestInputError ( e . args [ 0 ] ) except NotFoundError as e : return None return self . make_response ( result )
8936	def fail ( message , exitcode = 1 ) : sys . stderr . write ( 'ERROR: {}\n' . format ( message ) ) sys . stderr . flush ( ) sys . exit ( exitcode )
8630	def get_projects ( session , query ) : # GET /api/projects/0.1/projects response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12539	def find_all_dicom_files ( root_path ) : dicoms = set ( ) try : for fpath in get_all_files ( root_path ) : if is_dicom_file ( fpath ) : dicoms . add ( fpath ) except IOError as ioe : raise IOError ( 'Error reading file {0}.' . format ( fpath ) ) from ioe return dicoms
7809	def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : # PyPy doesn't have .getpeercert data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
6671	def is_file ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isfile ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
11143	def is_name_allowed ( self , path ) : assert isinstance ( path , basestring ) , "given path must be a string" name = os . path . basename ( path ) if not len ( name ) : return False , "empty name is not allowed" # exact match for em in [ self . __repoLock , self . __repoFile , self . __dirInfo , self . __dirLock ] : if name == em : return False , "name '%s' is reserved for pyrep internal usage" % em # pattern match for pm in [ self . __fileInfo , self . __fileLock ] : #,self.__objectDir]: if name == pm or ( name . endswith ( pm [ 3 : ] ) and name . startswith ( '.' ) ) : return False , "name pattern '%s' is not allowed as result may be reserved for pyrep internal usage" % pm # name is ok return True , None
2189	def _rectify_products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
7249	def cancel ( self , workflow_id ) : self . logger . debug ( 'Canceling workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s/cancel' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . post ( url , data = '' ) r . raise_for_status ( )
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : # Create a new session with a valid token if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
9117	def reset_jails ( confirm = True , keep_cleanser_master = True ) : if value_asbool ( confirm ) and not yesno ( """\nObacht! This will destroy all existing and or currently running jails on the host. Are you sure that you want to continue?""" ) : exit ( "Glad I asked..." ) reset_cleansers ( confirm = False ) jails = [ 'appserver' , 'webserver' , 'worker' ] if not value_asbool ( keep_cleanser_master ) : jails . append ( 'cleanser' ) with fab . warn_only ( ) : for jail in jails : fab . run ( 'ezjail-admin delete -fw {jail}' . format ( jail = jail ) ) # remove authorized keys for no longer existing key (they are regenerated for each new worker) fab . run ( 'rm /usr/jails/cleanser/usr/home/cleanser/.ssh/authorized_keys' )
13782	def _ConvertEnumDescriptor ( self , enum_proto , package = None , file_desc = None , containing_type = None , scope = None ) : if package : enum_name = '.' . join ( ( package , enum_proto . name ) ) else : enum_name = enum_proto . name if file_desc is None : file_name = None else : file_name = file_desc . name values = [ self . _MakeEnumValueDescriptor ( value , index ) for index , value in enumerate ( enum_proto . value ) ] desc = descriptor . EnumDescriptor ( name = enum_proto . name , full_name = enum_name , filename = file_name , file = file_desc , values = values , containing_type = containing_type , options = enum_proto . options ) scope [ '.%s' % enum_name ] = desc self . _enum_descriptors [ enum_name ] = desc return desc
7364	async def set_tz ( self ) : settings = await self . api . account . settings . get ( ) tz = settings . time_zone . tzinfo_name os . environ [ 'TZ' ] = tz time . tzset ( )
8922	def localize_datetime ( dt , tz_name = 'UTC' ) : tz_aware_dt = dt if dt . tzinfo is None : utc = pytz . timezone ( 'UTC' ) aware = utc . localize ( dt ) timezone = pytz . timezone ( tz_name ) tz_aware_dt = aware . astimezone ( timezone ) else : logger . warn ( 'tzinfo already set' ) return tz_aware_dt
5492	def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
11341	def set_target_temperature ( self , temperature , mode = config . SCHEDULE_HOLD ) : if temperature < self . min_temperature : temperature = self . min_temperature if temperature > self . max_temperature : temperature = self . max_temperature modes = [ config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "SetPointTemp" : temperature , "ScheduleMode" : mode } )
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
13756	def copy_file ( src , dest ) : dir_path = os . path . dirname ( dest ) if not os . path . exists ( dir_path ) : os . makedirs ( dir_path ) shutil . copy2 ( src , dest )
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
10443	def getobjectinfo ( self , window_name , object_name ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : # During the test, when the window closed and reopened # ErrorInvalidUIElement exception will be thrown self . _windows = { } # Call the method again, after updating apps obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) props = [ ] if obj_info : for obj_prop in obj_info . keys ( ) : if not obj_info [ obj_prop ] or obj_prop == "obj" : # Don't add object handle to the list continue props . append ( obj_prop ) return props
10904	def compare_data_model_residuals ( s , tile , data_vmin = 'calc' , data_vmax = 'calc' , res_vmin = - 0.1 , res_vmax = 0.1 , edgepts = 'calc' , do_imshow = True , data_cmap = plt . cm . bone , res_cmap = plt . cm . RdBu ) : # This could be modified to alpha the borderline... or to embiggen # the image and slice it more finely residuals = s . residuals [ tile . slicer ] . squeeze ( ) data = s . data [ tile . slicer ] . squeeze ( ) model = s . model [ tile . slicer ] . squeeze ( ) if data . ndim != 2 : raise ValueError ( 'tile does not give a 2D slice' ) im = np . zeros ( [ data . shape [ 0 ] , data . shape [ 1 ] , 4 ] ) if data_vmin == 'calc' : data_vmin = 0.5 * ( data . min ( ) + model . min ( ) ) if data_vmax == 'calc' : data_vmax = 0.5 * ( data . max ( ) + model . max ( ) ) #1. Get masks: upper_mask , center_mask , lower_mask = trisect_image ( im . shape , edgepts ) #2. Get colorbar'd images gm = data_cmap ( center_data ( model , data_vmin , data_vmax ) ) dt = data_cmap ( center_data ( data , data_vmin , data_vmax ) ) rs = res_cmap ( center_data ( residuals , res_vmin , res_vmax ) ) for a in range ( 4 ) : im [ : , : , a ] [ upper_mask ] = rs [ : , : , a ] [ upper_mask ] im [ : , : , a ] [ center_mask ] = gm [ : , : , a ] [ center_mask ] im [ : , : , a ] [ lower_mask ] = dt [ : , : , a ] [ lower_mask ] if do_imshow : return plt . imshow ( im ) else : return im
6770	def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
7154	def prepare_options ( options ) : options_ , verbose_options = [ ] , [ ] for option in options : if is_string ( option ) : options_ . append ( option ) verbose_options . append ( option ) else : options_ . append ( option [ 0 ] ) verbose_options . append ( option [ 1 ] ) return options_ , verbose_options
6059	def numpy_array_2d_to_fits ( array_2d , file_path , overwrite = False ) : if overwrite and os . path . exists ( file_path ) : os . remove ( file_path ) new_hdr = fits . Header ( ) hdu = fits . PrimaryHDU ( np . flipud ( array_2d ) , new_hdr ) hdu . writeto ( file_path )
629	def _bitForCoordinate ( cls , coordinate , n ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getUInt32 ( n )
7389	def node_theta ( self , node ) : group = self . find_node_group_membership ( node ) return self . group_theta ( group )
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
1949	def write_back_register ( self , reg , val ) : if self . write_backs_disabled : return if issymbolic ( val ) : logger . warning ( "Skipping Symbolic write-back" ) return if reg in self . flag_registers : self . _emu . reg_write ( self . _to_unicorn_id ( 'EFLAGS' ) , self . _cpu . read_register ( 'EFLAGS' ) ) return self . _emu . reg_write ( self . _to_unicorn_id ( reg ) , val )
11093	def n_file ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_file ( recursive = True ) : n += 1 return n
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
3078	def email ( self ) : if not self . credentials : return None try : return self . credentials . id_token [ 'email' ] except KeyError : current_app . logger . error ( 'Invalid id_token {0}' . format ( self . credentials . id_token ) )
6328	def _add_to_ngcorpus ( self , corpus , words , count ) : if words [ 0 ] not in corpus : corpus [ words [ 0 ] ] = Counter ( ) if len ( words ) == 1 : corpus [ words [ 0 ] ] [ None ] += count else : self . _add_to_ngcorpus ( corpus [ words [ 0 ] ] , words [ 1 : ] , count )
9446	def transfer_call ( self , call_params ) : path = '/' + self . api_version + '/TransferCall/' method = 'POST' return self . request ( path , method , call_params )
9006	def to_svg ( self , converter = None ) : if converter is None : from knittingpattern . convert . InstructionSVGCache import default_svg_cache converter = default_svg_cache ( ) return converter . to_svg ( self )
10836	def filter ( self , * * kwargs ) : if not len ( self ) : self . all ( ) new_list = filter ( lambda item : [ True for arg in kwargs if item [ arg ] == kwargs [ arg ] ] != [ ] , self ) return Profiles ( self . api , new_list )
7902	def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . set_stream ( stream )
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , * * self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad_func
3867	async def set_notification_level ( self , level ) : await self . _client . set_conversation_notification_level ( hangouts_pb2 . SetConversationNotificationLevelRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , level = level , ) )
4933	def get_content_id ( self , content_metadata_item ) : content_id = content_metadata_item . get ( 'key' , '' ) if content_metadata_item [ 'content_type' ] == 'program' : content_id = content_metadata_item . get ( 'uuid' , '' ) return content_id
2715	def create ( self , * * kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get_data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
6951	def jhk_to_sdssz ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSZ_JHK , SDSSZ_JH , SDSSZ_JK , SDSSZ_HK , SDSSZ_J , SDSSZ_H , SDSSZ_K )
13204	def _parse_documentclass ( self ) : command = LatexCommand ( 'documentclass' , { 'name' : 'options' , 'required' : False , 'bracket' : '[' } , { 'name' : 'class_name' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no documentclass' ) self . _document_options = [ ] try : content = parsed [ 'options' ] self . _document_options = [ opt . strip ( ) for opt in content . split ( ',' ) ] except KeyError : self . _logger . warning ( 'lsstdoc has no documentclass options' ) self . _document_options = [ ]
4067	def update_item ( self , payload , last_modified = None ) : to_send = self . check_items ( [ payload ] ) [ 0 ] if last_modified is None : modified = payload [ "version" ] else : modified = last_modified ident = payload [ "key" ] headers = { "If-Unmodified-Since-Version" : str ( modified ) } headers . update ( self . default_headers ( ) ) req = requests . patch ( url = self . endpoint + "/{t}/{u}/items/{id}" . format ( t = self . library_type , u = self . library_id , id = ident ) , headers = headers , data = json . dumps ( to_send ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return # Find current git branch. branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
1766	def decode_instruction ( self , pc ) : # No dynamic code!!! #TODO! # Check if instruction was already decoded if pc in self . _instruction_cache : return self . _instruction_cache [ pc ] text = b'' # Read Instruction from memory for address in range ( pc , pc + self . max_instr_width ) : # This reads a byte from memory ignoring permissions # and concretize it if symbolic if not self . memory . access_ok ( address , 'x' ) : break c = self . memory [ address ] if issymbolic ( c ) : # In case of fully symbolic memory, eagerly get a valid ptr if isinstance ( self . memory , LazySMemory ) : try : vals = visitors . simplify_array_select ( c ) c = bytes ( [ vals [ 0 ] ] ) except visitors . ArraySelectSimplifier . ExpressionNotSimple : c = struct . pack ( 'B' , solver . get_value ( self . memory . constraints , c ) ) elif isinstance ( c , Constant ) : c = bytes ( [ c . value ] ) else : logger . error ( 'Concretize executable memory %r %r' , c , text ) raise ConcretizeMemory ( self . memory , address = pc , size = 8 * self . max_instr_width , policy = 'INSTRUCTION' ) text += c # Pad potentially incomplete instruction with zeroes code = text . ljust ( self . max_instr_width , b'\x00' ) try : # decode the instruction from code insn = self . disasm . disassemble_instruction ( code , pc ) except StopIteration as e : raise DecodeException ( pc , code ) # Check that the decoded instruction is contained in executable memory if not self . memory . access_ok ( slice ( pc , pc + insn . size ) , 'x' ) : logger . info ( "Trying to execute instructions from non-executable memory" ) raise InvalidMemoryAccess ( pc , 'x' ) insn . operands = self . _wrap_operands ( insn . operands ) self . _instruction_cache [ pc ] = insn return insn
4621	def _decrypt_masterpassword ( self ) : aes = AESCipher ( self . password ) checksum , encrypted_master = self . config [ self . config_key ] . split ( "$" ) try : decrypted_master = aes . decrypt ( encrypted_master ) except Exception : self . _raise_wrongmasterpassexception ( ) if checksum != self . _derive_checksum ( decrypted_master ) : self . _raise_wrongmasterpassexception ( ) self . decrypted_master = decrypted_master
1486	def _modules_to_main ( modList ) : if not modList : return main = sys . modules [ '__main__' ] for modname in modList : if isinstance ( modname , str ) : try : mod = __import__ ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n. ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely. Specific error was:\n' % modname ) print_exec ( sys . stderr ) else : setattr ( main , mod . __name__ , mod )
10422	def count_annotation_values_filtered ( graph : BELGraph , annotation : str , source_predicate : Optional [ NodePredicate ] = None , target_predicate : Optional [ NodePredicate ] = None , ) -> Counter : if source_predicate and target_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and source_predicate ( graph , u ) and target_predicate ( graph , v ) ) elif source_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and source_predicate ( graph , u ) ) elif target_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and target_predicate ( graph , u ) ) else : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) )
4917	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer_catalog = self . get_object ( ) # Maintain plus characters in course key. course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = True if course_run_ids : contains_content_items = enterprise_customer_catalog . contains_courses ( course_run_ids ) if program_uuids : contains_content_items = ( contains_content_items and enterprise_customer_catalog . contains_programs ( program_uuids ) ) return Response ( { 'contains_content_items' : contains_content_items } )
6595	def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
5706	def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
3703	def Rackett ( T , Tc , Pc , Zc ) : return R * Tc / Pc * Zc ** ( 1 + ( 1 - T / Tc ) ** ( 2 / 7. ) )
6537	def mod_sys_path ( paths ) : old_path = sys . path sys . path = paths + sys . path try : yield finally : sys . path = old_path
11437	def _fields_sort_by_indicators ( fields ) : field_dict = { } field_positions_global = [ ] for field in fields : field_dict . setdefault ( field [ 1 : 3 ] , [ ] ) . append ( field ) field_positions_global . append ( field [ 4 ] ) indicators = field_dict . keys ( ) indicators . sort ( ) field_list = [ ] for indicator in indicators : for field in field_dict [ indicator ] : field_list . append ( field [ : 4 ] + ( field_positions_global . pop ( 0 ) , ) ) return field_list
10050	def create_blueprint ( endpoints ) : blueprint = Blueprint ( 'invenio_deposit_rest' , __name__ , url_prefix = '' , ) create_error_handlers ( blueprint ) for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) if 'files_serializers' in options : files_serializers = options . get ( 'files_serializers' ) files_serializers = { mime : obj_or_import_string ( func ) for mime , func in files_serializers . items ( ) } del options [ 'files_serializers' ] else : files_serializers = { } if 'record_serializers' in options : serializers = options . get ( 'record_serializers' ) serializers = { mime : obj_or_import_string ( func ) for mime , func in serializers . items ( ) } else : serializers = { } file_list_route = options . pop ( 'file_list_route' , '{0}/files' . format ( options [ 'item_route' ] ) ) file_item_route = options . pop ( 'file_item_route' , '{0}/files/<path:key>' . format ( options [ 'item_route' ] ) ) options . setdefault ( 'search_class' , DepositSearch ) search_class = obj_or_import_string ( options [ 'search_class' ] ) # records rest endpoints will use the deposit class as record class options . setdefault ( 'record_class' , Deposit ) record_class = obj_or_import_string ( options [ 'record_class' ] ) # backward compatibility for indexer class options . setdefault ( 'indexer_class' , None ) for rule in records_rest_url_rules ( endpoint , * * options ) : blueprint . add_url_rule ( * * rule ) search_class_kwargs = { } if options . get ( 'search_index' ) : search_class_kwargs [ 'index' ] = options [ 'search_index' ] if options . get ( 'search_type' ) : search_class_kwargs [ 'doc_type' ] = options [ 'search_type' ] ctx = dict ( read_permission_factory = obj_or_import_string ( options . get ( 'read_permission_factory_imp' ) ) , create_permission_factory = obj_or_import_string ( options . get ( 'create_permission_factory_imp' ) ) , update_permission_factory = obj_or_import_string ( options . get ( 'update_permission_factory_imp' ) ) , delete_permission_factory = obj_or_import_string ( options . get ( 'delete_permission_factory_imp' ) ) , record_class = record_class , search_class = partial ( search_class , * * search_class_kwargs ) , default_media_type = options . get ( 'default_media_type' ) , ) deposit_actions = DepositActionResource . as_view ( DepositActionResource . view_name . format ( endpoint ) , serializers = serializers , pid_type = options [ 'pid_type' ] , ctx = ctx , ) blueprint . add_url_rule ( '{0}/actions/<any({1}):action>' . format ( options [ 'item_route' ] , ',' . join ( extract_actions_from_class ( record_class ) ) , ) , view_func = deposit_actions , methods = [ 'POST' ] , ) deposit_files = DepositFilesResource . as_view ( DepositFilesResource . view_name . format ( endpoint ) , serializers = files_serializers , pid_type = options [ 'pid_type' ] , ctx = ctx , ) blueprint . add_url_rule ( file_list_route , view_func = deposit_files , methods = [ 'GET' , 'POST' , 'PUT' ] , ) deposit_file = DepositFileResource . as_view ( DepositFileResource . view_name . format ( endpoint ) , serializers = files_serializers , pid_type = options [ 'pid_type' ] , ctx = ctx , ) blueprint . add_url_rule ( file_item_route , view_func = deposit_file , methods = [ 'GET' , 'PUT' , 'DELETE' ] , ) return blueprint
12387	def parse_segment ( text ) : if not len ( text ) : return NoopQuerySegment ( ) q = QuerySegment ( ) # First we need to split the segment into key/value pairs. This is done # by attempting to split the sequence for each equality comparison. Then # discard any that did not split properly. Then chose the smallest key # (greedily chose the first comparator we encounter in the string) # followed by the smallest value (greedily chose the largest comparator # possible.) # translate into [('=', 'foo=bar')] equalities = zip ( constants . OPERATOR_EQUALITIES , itertools . repeat ( text ) ) # Translate into [('=', ['foo', 'bar'])] equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) # Remove unsplit entries and translate into [('=': ['foo', 'bar'])] # Note that the result from this stage is iterated over twice. equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) # Get the smallest key and use the length of that to remove other items key_len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key_len , equalities ) # Get the smallest value length. thus we have the earliest key and the # smallest value. op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse_directive ( key ) if directive : op = constants . OPERATOR_EQUALITY_FALLBACK q . directive = directive # Process negation. This comes in both foo.not= and foo!= forms. path = key . split ( constants . SEP_PATH ) last = path [ - 1 ] # Check for != if last . endswith ( constants . OPERATOR_NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated # Check for foo.not= if last == constants . PATH_NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP_VALUE ) # Check for suffixed operators (foo.gte=bar). Prioritize suffixed # entries over actual equality checks. if path [ - 1 ] in constants . OPERATOR_SUFFIXES : # The case where foo.gte<=bar, which obviously makes no sense. if op not in constants . OPERATOR_FALLBACK : raise ValueError ( 'Both path-style operator and equality style operator ' 'provided. Please provide only a single style operator.' ) q . operator = constants . OPERATOR_SUFFIX_MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR_EQUALITY_MAP [ op ] if not len ( path ) : raise ValueError ( 'No attribute navigation path provided.' ) q . path = path return q
2963	def get_source_chains ( self , blockade_id ) : result = { } if not blockade_id : raise ValueError ( "invalid blockade_id" ) lines = self . get_chain_rules ( "FORWARD" ) for line in lines : parts = line . split ( ) if len ( parts ) < 4 : continue try : partition_index = parse_partition_index ( blockade_id , parts [ 0 ] ) except ValueError : continue # not a rule targetting a blockade chain source = parts [ 3 ] if source : result [ source ] = partition_index return result
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) # Temporarily include the host header - AWS requires it to be included # in the signed headers, but Requests doesn't include it in a # PreparedRequest if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] # Aggregate for upper/lowercase header name collisions in header names, # AMZ requires values of colliding headers be concatenated into a # single header with lowercase name. Although this is not possible with # Requests, since it uses a case-insensitive dict to hold headers, this # is here just in case you duck type with a regular dict cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) # Flatten cano_headers dict to string and generate signed_headers cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
898	def addSpatialNoise ( self , sequence , amount ) : newSequence = [ ] for pattern in sequence : if pattern is not None : pattern = self . patternMachine . addNoise ( pattern , amount ) newSequence . append ( pattern ) return newSequence
3758	def UFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'UFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'UFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'UFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'UFL' ] ) elif Method == SUZUKI : return Suzuki_UFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_UFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
6490	def _process_filters ( filter_dictionary ) : def filter_item ( field ) : """ format elasticsearch filter to pass if value matches OR field is not included """ if filter_dictionary [ field ] is not None : return { "or" : [ _get_filter_field ( field , filter_dictionary [ field ] ) , { "missing" : { "field" : field } } ] } return { "missing" : { "field" : field } } return [ filter_item ( field ) for field in filter_dictionary ]
4864	def get_groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name__in = ENTERPRISE_PERMISSION_GROUPS ) ] return [ ]
9875	def aggregate_tree ( l_tree ) : def _aggregate_phase1 ( tree ) : # phase1 removes any supplied prefixes which are superfluous because # they are already included in another supplied prefix. For example, # 2001:67c:208c:10::/64 would be removed if 2001:67c:208c::/48 was # also supplied. n_tree = radix . Radix ( ) for prefix in tree . prefixes ( ) : if tree . search_worst ( prefix ) . prefix == prefix : n_tree . add ( prefix ) return n_tree def _aggregate_phase2 ( tree ) : # phase2 identifies adjacent prefixes that can be combined under a # single, shorter-length prefix. For example, 2001:67c:208c::/48 and # 2001:67c:208d::/48 can be combined into the single prefix # 2001:67c:208c::/47. n_tree = radix . Radix ( ) for rnode in tree : p = text ( ip_network ( text ( rnode . prefix ) ) . supernet ( ) ) r = tree . search_covered ( p ) if len ( r ) == 2 : if r [ 0 ] . prefixlen == r [ 1 ] . prefixlen == rnode . prefixlen : n_tree . add ( p ) else : n_tree . add ( rnode . prefix ) else : n_tree . add ( rnode . prefix ) return n_tree l_tree = _aggregate_phase1 ( l_tree ) if len ( l_tree . prefixes ( ) ) == 1 : return l_tree while True : r_tree = _aggregate_phase2 ( l_tree ) if l_tree . prefixes ( ) == r_tree . prefixes ( ) : break else : l_tree = r_tree del r_tree return l_tree
1356	def get_argument_environ ( self ) : try : return self . get_argument ( constants . PARAM_ENVIRON ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
5636	def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec # headline head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] # main sections if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) # API section md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
3444	def load_json_model ( filename ) : if isinstance ( filename , string_types ) : with open ( filename , "r" ) as file_handle : return model_from_dict ( json . load ( file_handle ) ) else : return model_from_dict ( json . load ( filename ) )
9836	def __comment ( self ) : tok = self . __consume ( ) self . DXfield . add_comment ( tok . value ( ) ) self . set_parser ( 'general' )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
714	def loadSavedHyperSearchJob ( cls , permWorkDir , outputLabel ) : jobID = cls . __loadHyperSearchJobID ( permWorkDir = permWorkDir , outputLabel = outputLabel ) searchJob = _HyperSearchJob ( nupicJobID = jobID ) return searchJob
1363	def get_argument_offset ( self ) : try : offset = self . get_argument ( constants . PARAM_OFFSET ) return offset except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
6706	def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) # by convention, use 2 as OOV word # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
3307	def _run_cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi # from cheroot.ssl.builtin import BuiltinSSLAdapter # import cheroot.ssl.pyopenssl except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import Cheroot." ) _logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgi . Server . version , util . PYTHON_VERSION ) wsgi . Server . version = server_name # Support SSL ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) ssl_adapter = config . get ( "ssl_adapter" , "builtin" ) protocol = "http" if ssl_certificate and ssl_private_key : ssl_adapter = server . get_ssl_adapter_class ( ssl_adapter ) wsgi . Server . ssl_adapter = ssl_adapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl_adapter ) ) elif ssl_certificate or ssl_private_key : raise RuntimeError ( "Option 'ssl_certificate' and 'ssl_private_key' must be used together." ) # elif ssl_adapter: # print("WARNING: Ignored option 'ssl_adapter' (requires 'ssl_certificate').") _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) server = wsgi . Server ( * * server_args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick # undo the monkey patch _logger . info ( "wsgi.Server is ready" ) startup_event . set ( ) org_tick ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
7682	def display ( annotation , meta = True , * * kwargs ) : for namespace , func in six . iteritems ( VIZ_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) axes = func ( ann , * * kwargs ) # Title should correspond to original namespace, not the coerced version axes . set_title ( annotation . namespace ) if meta : description = pprint_jobject ( annotation . annotation_metadata , indent = 2 ) anchored_box = AnchoredText ( description . strip ( '\n' ) , loc = 2 , frameon = True , bbox_to_anchor = ( 1.02 , 1.0 ) , bbox_transform = axes . transAxes , borderpad = 0.0 ) axes . add_artist ( anchored_box ) axes . figure . subplots_adjust ( right = 0.8 ) return axes except NamespaceError : pass raise NamespaceError ( 'Unable to visualize annotation of namespace="{:s}"' . format ( annotation . namespace ) )
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
3642	def sell ( self , item_id , bid , buy_now , duration = 3600 , fast = False ) : method = 'POST' url = 'auctionhouse' # TODO: auto send to tradepile data = { 'buyNowPrice' : buy_now , 'startingBid' : bid , 'duration' : duration , 'itemData' : { 'id' : item_id } } rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } ) if not fast : # tradeStatus check like webapp do self . tradeStatus ( rc [ 'id' ] ) return rc [ 'id' ]
8308	def ensure_pycairo_context ( self , ctx ) : if self . cairocffi and isinstance ( ctx , self . cairocffi . Context ) : from shoebot . util . cairocffi . cairocffi_to_pycairo import _UNSAFE_cairocffi_context_to_pycairo return _UNSAFE_cairocffi_context_to_pycairo ( ctx ) else : return ctx
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] # Maybe slightly ugly but I don't want to basically reimplement all but uri formation of the api method return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
11842	def ModelBasedVacuumAgent ( ) : model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status ## Update the model here if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
4700	def get_sizeof_descriptor_table ( version = "Denali" ) : if version == "Denali" : return sizeof ( DescriptorTableDenali ) elif version == "Spec20" : return sizeof ( DescriptorTableSpec20 ) elif version == "Spec12" : return 0 else : raise RuntimeError ( "Error version!" )
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) # on windows os.path.relpath raises a ValueError if the two paths are on # different drives, in that case we just ignore the exception as the two # paths are anyway not relative relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) # copy extra files for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
7887	def filter_mechanism_list ( mechanisms , properties , allow_insecure = False , server_side = False ) : # pylint: disable=W0212 result = [ ] for mechanism in mechanisms : try : if server_side : klass = SERVER_MECHANISMS_D [ mechanism ] else : klass = CLIENT_MECHANISMS_D [ mechanism ] except KeyError : logger . debug ( " skipping {0} - not supported" . format ( mechanism ) ) continue secure = properties . get ( "security-layer" ) if not allow_insecure and not klass . _pyxmpp_sasl_secure and not secure : logger . debug ( " skipping {0}, as it is not secure" . format ( mechanism ) ) continue if not klass . are_properties_sufficient ( properties ) : logger . debug ( " skipping {0}, as the properties are not sufficient" . format ( mechanism ) ) continue result . append ( mechanism ) return result
3005	def _get_storage_model ( ) : storage_model_settings = getattr ( django . conf . settings , 'GOOGLE_OAUTH2_STORAGE_MODEL' , None ) if storage_model_settings is not None : return ( storage_model_settings [ 'model' ] , storage_model_settings [ 'user_property' ] , storage_model_settings [ 'credentials_property' ] ) else : return None , None , None
8641	def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9512	def gaps ( self , min_length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min_length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
9066	def delta ( self ) : v = float ( self . _logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
6992	def flare_model ( flareparams , times , mags , errs ) : ( amplitude , flare_peak_time , rise_gaussian_stdev , decay_time_constant ) = flareparams zerolevel = np . median ( mags ) modelmags = np . full_like ( times , zerolevel ) # before peak gaussian rise... modelmags [ times < flare_peak_time ] = ( mags [ times < flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times < flare_peak_time ] - flare_peak_time ) * ( times [ times < flare_peak_time ] - flare_peak_time ) ) / ( 2.0 * rise_gaussian_stdev * rise_gaussian_stdev ) ) ) # after peak exponential decay... modelmags [ times > flare_peak_time ] = ( mags [ times > flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times > flare_peak_time ] - flare_peak_time ) ) / ( decay_time_constant ) ) ) return modelmags , times , mags , errs
9125	def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
12974	def compat_convertHashedIndexes ( self , fetchAll = True ) : saver = IndexedRedisSave ( self . mdl ) if fetchAll is True : objs = self . all ( ) saver . compat_convertHashedIndexes ( objs ) else : didWarnOnce = False pks = self . getPrimaryKeys ( ) for pk in pks : obj = self . get ( pk ) if not obj : if didWarnOnce is False : sys . stderr . write ( 'WARNING(once)! An object (type=%s , pk=%d) disappered while ' 'running compat_convertHashedIndexes! This probably means an application ' 'is using the model while converting indexes. This is a very BAD IDEA (tm).' ) didWarnOnce = True continue saver . compat_convertHashedIndexes ( [ obj ] )
2050	def _swap_mode ( self ) : assert self . mode in ( cs . CS_MODE_ARM , cs . CS_MODE_THUMB ) if self . mode == cs . CS_MODE_ARM : self . mode = cs . CS_MODE_THUMB else : self . mode = cs . CS_MODE_ARM
6517	def execute_reports ( config , path , collector , on_report_finish = None , output_file = None ) : reports = get_reports ( ) for report in config . get ( 'requested_reports' , [ ] ) : if report . get ( 'type' ) and report [ 'type' ] in reports : cfg = config . get ( 'report' , { } ) . get ( report [ 'type' ] , { } ) cfg . update ( report ) reporter = reports [ report [ 'type' ] ] ( cfg , path , output_file = output_file , ) reporter . produce ( collector ) if on_report_finish : on_report_finish ( report )
6253	def available_templates ( value ) : templates = list_templates ( ) if value not in templates : raise ArgumentTypeError ( "Effect template '{}' does not exist.\n Available templates: {} " . format ( value , ", " . join ( templates ) ) ) return value
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
4225	def load_config ( ) : filename = 'keyringrc.cfg' keyring_cfg = os . path . join ( platform . config_root ( ) , filename ) if not os . path . exists ( keyring_cfg ) : return config = configparser . RawConfigParser ( ) config . read ( keyring_cfg ) _load_keyring_path ( config ) # load the keyring class name, and then load this keyring try : if config . has_section ( "backend" ) : keyring_name = config . get ( "backend" , "default-keyring" ) . strip ( ) else : raise configparser . NoOptionError ( 'backend' , 'default-keyring' ) except ( configparser . NoOptionError , ImportError ) : logger = logging . getLogger ( 'keyring' ) logger . warning ( "Keyring config file contains incorrect values.\n" + "Config file: %s" % keyring_cfg ) return return load_keyring ( keyring_name )
8414	def round_any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) # Make sure the discovered services are added to the list of known # services, and kick off characteristic discovery for each one. # NOTE: For some reason the services parameter is never set to a good # value, instead you must query peripheral.services() to enumerate the # discovered services. for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) # Kick off characteristic discovery for this service. Just discover # all characteristics for now. peripheral . discoverCharacteristics_forService_ ( None , service )
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
4100	def aic_eigen ( s , N ) : import numpy as np kaic = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kaic . append ( - 2. * ( n - k ) * N * np . log ( gk / ak ) + 2. * k * ( 2. * n - k ) ) return kaic
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
4218	def _get_env ( self , env_var ) : value = os . environ . get ( env_var ) if not value : raise ValueError ( 'Missing environment variable:%s' % env_var ) return value
3200	def create ( self , data ) : if 'recipients' not in data : raise KeyError ( 'The campaign must have recipients' ) if 'list_id' not in data [ 'recipients' ] : raise KeyError ( 'The campaign recipients must have a list_id' ) if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) if 'type' not in data : raise KeyError ( 'The campaign must have a type' ) if not data [ 'type' ] in [ 'regular' , 'plaintext' , 'rss' , 'variate' , 'abspilt' ] : raise ValueError ( 'The campaign type must be one of "regular", "plaintext", "rss", or "variate"' ) if data [ 'type' ] == 'variate' : if 'variate_settings' not in data : raise KeyError ( 'The variate campaign must have variate_settings' ) if 'winner_criteria' not in data [ 'variate_settings' ] : raise KeyError ( 'The campaign variate_settings must have a winner_criteria' ) if data [ 'variate_settings' ] [ 'winner_criteria' ] not in [ 'opens' , 'clicks' , 'total_revenue' , 'manual' ] : raise ValueError ( 'The campaign variate_settings ' 'winner_criteria must be one of "opens", "clicks", "total_revenue", or "manual"' ) if data [ 'type' ] == 'rss' : if 'rss_opts' not in data : raise KeyError ( 'The rss campaign must have rss_opts' ) if 'feed_url' not in data [ 'rss_opts' ] : raise KeyError ( 'The campaign rss_opts must have a feed_url' ) if not data [ 'rss_opts' ] [ 'frequency' ] in [ 'daily' , 'weekly' , 'monthly' ] : raise ValueError ( 'The rss_opts frequency must be one of "daily", "weekly", or "monthly"' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . campaign_id = response [ 'id' ] else : self . campaign_id = None return response
4558	def make_segments ( strip , length ) : if len ( strip ) % length : raise ValueError ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except ValueError : return s
13023	def select ( self , sql_string , cols , * args , * * kwargs ) : working_columns = None if kwargs . get ( 'columns' ) is not None : working_columns = kwargs . pop ( 'columns' ) query = self . _assemble_select ( sql_string , cols , * args , * kwargs ) return self . _execute ( query , working_columns = working_columns )
6712	def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
10244	def get_citation_years ( graph : BELGraph ) -> List [ Tuple [ int , int ] ] : return create_timeline ( count_citation_years ( graph ) )
13686	def embed_data ( request ) : result = _EmbedDataFixture ( request ) result . delete_data_dir ( ) result . create_data_dir ( ) yield result result . delete_data_dir ( )
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
7355	def seq_to_str ( obj , sep = "," ) : if isinstance ( obj , string_classes ) : return obj elif isinstance ( obj , ( list , tuple ) ) : return sep . join ( [ str ( x ) for x in obj ] ) else : return str ( obj )
10217	def prerender ( graph : BELGraph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel_hgnc from bio2bel_hgnc . models import HumanGene graph : BELGraph = graph . copy ( ) enrich_protein_and_rna_origins ( graph ) collapse_all_variants ( graph ) genes : Set [ Gene ] = get_nodes_by_function ( graph , GENE ) hgnc_symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc_manager = bio2bel_hgnc . Manager ( ) human_genes = ( hgnc_manager . session . query ( HumanGene . symbol , HumanGene . location ) . filter ( HumanGene . symbol . in_ ( hgnc_symbols ) ) . all ( ) ) for human_gene in human_genes : result [ human_gene . symbol ] = { 'name' : human_gene . symbol , 'chr' : ( human_gene . location . split ( 'q' ) [ 0 ] if 'q' in human_gene . location else human_gene . location . split ( 'p' ) [ 0 ] ) , } df = get_df ( ) for _ , ( gene_id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc_symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
12226	def on_pref_update ( * args , * * kwargs ) : Preference . update_prefs ( * args , * * kwargs ) Preference . read_prefs ( get_prefs ( ) )
11351	def merge_from_list ( self , list_args ) : def xs ( name , parser_args , list_args ) : """build the generator of matching list_args""" for args , kwargs in list_args : if len ( set ( args ) & parser_args ) > 0 : yield args , kwargs else : if 'dest' in kwargs : if kwargs [ 'dest' ] == name : yield args , kwargs for args , kwargs in xs ( self . name , self . parser_args , list_args ) : self . merge_args ( args ) self . merge_kwargs ( kwargs )
626	def _topWCoordinates ( cls , coordinates , w ) : orders = numpy . array ( [ cls . _orderForCoordinate ( c ) for c in coordinates . tolist ( ) ] ) indices = numpy . argsort ( orders ) [ - w : ] return coordinates [ indices ]
9540	def number_range_exclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) <= min or type ( v ) >= max : raise ValueError ( v ) return checker
12297	def plugins_show ( what = None , name = None , version = None , details = False ) : global pluginmgr return pluginmgr . show ( what , name , version , details )
2463	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True if validations . validate_file_comment ( text ) : self . file ( doc ) . comment = str_from_text ( text ) return True else : raise SPDXValueError ( 'File::Comment' ) else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
805	def _initEphemerals ( self ) : self . _firstComputeCall = True self . _accuracy = None self . _protoScores = None self . _categoryDistances = None self . _knn = knn_classifier . KNNClassifier ( * * self . knnParams ) for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , '_scanInfo' , '_protoScores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )
9164	def includeme ( config ) : api_key_authn_policy = APIKeyAuthenticationPolicy ( ) config . include ( 'openstax_accounts' ) openstax_authn_policy = config . registry . getUtility ( IOpenstaxAccountsAuthenticationPolicy ) # Set up api & user authentication policies. policies = [ api_key_authn_policy , openstax_authn_policy ] authn_policy = MultiAuthenticationPolicy ( policies ) config . set_authentication_policy ( authn_policy ) # Set up the authorization policy. authz_policy = ACLAuthorizationPolicy ( ) config . set_authorization_policy ( authz_policy )
2669	def cleanup_old_versions ( src , keep_last_versions , config_file = 'config.yaml' , profile_name = None , ) : if keep_last_versions <= 0 : print ( "Won't delete all versions. Please do this manually" ) else : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) response = client . list_versions_by_function ( FunctionName = cfg . get ( 'function_name' ) , ) versions = response . get ( 'Versions' ) if len ( response . get ( 'Versions' ) ) < keep_last_versions : print ( 'Nothing to delete. (Too few versions published)' ) else : version_numbers = [ elem . get ( 'Version' ) for elem in versions [ 1 : - keep_last_versions ] ] for version_number in version_numbers : try : client . delete_function ( FunctionName = cfg . get ( 'function_name' ) , Qualifier = version_number , ) except botocore . exceptions . ClientError as e : print ( 'Skipping Version {}: {}' . format ( version_number , e . message ) )
8902	def _multiple_self_ref_fk_check ( class_model ) : self_fk = [ ] for f in class_model . _meta . concrete_fields : if f . related_model in self_fk : return True if f . related_model == class_model : self_fk . append ( class_model ) return False
13640	def get_version ( ) : version_module_path = os . path . join ( os . path . dirname ( __file__ ) , "txspinneret" , "_version.py" ) # The version module contains a variable called __version__ with open ( version_module_path ) as version_module : exec ( version_module . read ( ) ) return locals ( ) [ "__version__" ]
10779	def _remove_closest_particle ( self , p ) : #1. find closest pos: dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) #2. delete self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
10338	def update_spia_matrices ( spia_matrices : Dict [ str , pd . DataFrame ] , u : CentralDogma , v : CentralDogma , edge_data : EdgeData , ) -> None : if u . namespace . upper ( ) != 'HGNC' or v . namespace . upper ( ) != 'HGNC' : return u_name = u . name v_name = v . name relation = edge_data [ RELATION ] if relation in CAUSAL_INCREASE_RELATIONS : # If it has pmod check which one and add it to the corresponding matrix if v . variants and any ( isinstance ( variant , ProteinModification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , ProteinModification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia_matrices [ "activation_ubiquination" ] [ u_name ] [ v_name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia_matrices [ "activation_phosphorylation" ] [ u_name ] [ v_name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : # Normal increase, add activation spia_matrices [ 'expression' ] [ u_name ] [ v_name ] = 1 else : spia_matrices [ 'activation' ] [ u_name ] [ v_name ] = 1 elif relation in CAUSAL_DECREASE_RELATIONS : # If it has pmod check which one and add it to the corresponding matrix if v . variants and any ( isinstance ( variant , ProteinModification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , ProteinModification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia_matrices [ 'inhibition_ubiquination' ] [ u_name ] [ v_name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia_matrices [ "inhibition_phosphorylation" ] [ u_name ] [ v_name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : # Normal decrease, check which matrix spia_matrices [ "repression" ] [ u_name ] [ v_name ] = 1 else : spia_matrices [ "inhibition" ] [ u_name ] [ v_name ] = 1 elif relation == ASSOCIATION : spia_matrices [ "binding_association" ] [ u_name ] [ v_name ] = 1
336	def run_model ( model , returns_train , returns_test = None , bmark = None , samples = 500 , ppc = False , progressbar = True ) : if model == 'alpha_beta' : model , trace = model_returns_t_alpha_beta ( returns_train , bmark , samples , progressbar = progressbar ) elif model == 't' : model , trace = model_returns_t ( returns_train , samples , progressbar = progressbar ) elif model == 'normal' : model , trace = model_returns_normal ( returns_train , samples , progressbar = progressbar ) elif model == 'best' : model , trace = model_best ( returns_train , returns_test , samples = samples , progressbar = progressbar ) else : raise NotImplementedError ( 'Model {} not found.' 'Use alpha_beta, t, normal, or best.' . format ( model ) ) if ppc : ppc_samples = pm . sample_ppc ( trace , samples = samples , model = model , size = len ( returns_test ) , progressbar = progressbar ) return trace , ppc_samples [ 'returns' ] return trace
3030	def _extract_id_token ( id_token ) : if type ( id_token ) == bytes : segments = id_token . split ( b'.' ) else : segments = id_token . split ( u'.' ) if len ( segments ) != 3 : raise VerifyJwtTokenError ( 'Wrong number of segments in token: {0}' . format ( id_token ) ) return json . loads ( _helpers . _from_bytes ( _helpers . _urlsafe_b64decode ( segments [ 1 ] ) ) )
11126	def dump_copy ( self , path , relativePath , name = None , description = None , replace = False , verbose = False ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' if name is None : _ , name = os . path . split ( path ) # ensure directory added self . add_directory ( relativePath ) # ger real path realPath = os . path . join ( self . __path , relativePath ) # get directory info dict dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage if name in dict . __getitem__ ( dirInfoDict , "files" ) : if not replace : if verbose : warnings . warn ( "a file with the name '%s' is already defined in repository dictionary info. Set replace flag to True if you want to replace the existing file" % ( name ) ) return # convert dump and pull methods to strings dump = "raise Exception(\"dump is ambiguous for copied file '$FILE_PATH' \")" pull = "raise Exception(\"pull is ambiguous for copied file '$FILE_PATH' \")" # dump file try : shutil . copyfile ( path , os . path . join ( realPath , name ) ) except Exception as e : if verbose : warnings . warn ( e ) return # set info klass = None # save the new file to the repository dict . __getitem__ ( dirInfoDict , "files" ) [ name ] = { "dump" : dump , "pull" : pull , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "class" : klass , "description" : description } # save repository self . save ( )
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
4610	def recoverPubkeyParameter ( message , digest , signature , pubkey ) : if not isinstance ( message , bytes ) : message = bytes ( message , "utf-8" ) # pragma: no cover for i in range ( 0 , 4 ) : if SECP256K1_MODULE == "secp256k1" : # pragma: no cover sig = pubkey . ecdsa_recoverable_deserialize ( signature , i ) p = secp256k1 . PublicKey ( pubkey . ecdsa_recover ( message , sig ) ) if p . serialize ( ) == pubkey . serialize ( ) : return i elif SECP256K1_MODULE == "cryptography" and not isinstance ( pubkey , PublicKey ) : p = recover_public_key ( digest , signature , i , message ) p_comp = hexlify ( compressedPubkey ( p ) ) pubkey_comp = hexlify ( compressedPubkey ( pubkey ) ) if p_comp == pubkey_comp : return i else : # pragma: no cover p = recover_public_key ( digest , signature , i ) p_comp = hexlify ( compressedPubkey ( p ) ) p_string = hexlify ( p . to_string ( ) ) if isinstance ( pubkey , PublicKey ) : # pragma: no cover pubkey_string = bytes ( repr ( pubkey ) , "ascii" ) else : # pragma: no cover pubkey_string = hexlify ( pubkey . to_string ( ) ) if p_string == pubkey_string or p_comp == pubkey_string : # pragma: no cover return i
604	def _addBase ( self , position , xlabel = None , ylabel = None ) : ax = self . _fig . add_subplot ( position ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) return ax
9665	def clean_all ( G , settings ) : quiet = settings [ "quiet" ] recon = settings [ "recon" ] sprint = settings [ "sprint" ] error = settings [ "error" ] all_outputs = [ ] for node in G . nodes ( data = True ) : if "output" in node [ 1 ] : for item in get_all_outputs ( node [ 1 ] ) : all_outputs . append ( item ) all_outputs . append ( ".shastore" ) retcode = 0 for item in sorted ( all_outputs ) : if os . path . isfile ( item ) : if recon : sprint ( "Would remove file: {}" . format ( item ) ) continue sprint ( "Attempting to remove file '{}'" , level = "verbose" ) try : os . remove ( item ) sprint ( "Removed file" , level = "verbose" ) except : errmes = "Error: file '{}' failed to be removed" error ( errmes . format ( item ) ) retcode = 1 if not retcode and not recon : sprint ( "All clean" , color = True ) return retcode
2357	def is_element_displayed ( self , strategy , locator ) : return self . driver_adapter . is_element_displayed ( strategy , locator , root = self . root )
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
1459	def load_pex ( path_to_pex , include_deps = True ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) if abs_path_to_pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs_path_to_pex ) ) # add dependencies to path if include_deps : for dep in _get_deps_list ( abs_path_to_pex ) : to_join = os . path . join ( os . path . dirname ( abs_path_to_pex ) , dep ) if to_join not in sys . path : Log . debug ( "Add a new dependency to the path: %s" % dep ) sys . path . insert ( 0 , to_join ) Log . debug ( "Python path: %s" % str ( sys . path ) )
8063	def do_set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . _vars : self . print_response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . _vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var_changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
830	def pprint ( self , output , prefix = "" ) : print prefix , description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) - 1 ) : offset = description [ i ] [ 1 ] nextoffset = description [ i + 1 ] [ 1 ] print "%s |" % bitsToString ( output [ offset : nextoffset ] ) , print
12809	def received ( self , messages ) : if messages : if self . _queue : self . _queue . put_nowait ( messages ) if self . _callback : self . _callback ( messages )
5909	def parse_groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
7353	def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = "netMHC" ) : # run NetMHC's help command and parse discriminating substrings out of # the resulting str output with open ( os . devnull , 'w' ) as devnull : help_output = check_output ( [ program_name , "-h" ] , stderr = devnull ) help_output_str = help_output . decode ( "ascii" , "ignore" ) substring_to_netmhc_class = { "-listMHC" : NetMHC4 , "--Alleles" : NetMHC3 , } successes = [ ] for substring , netmhc_class in substring_to_netmhc_class . items ( ) : if substring in help_output_str : successes . append ( netmhc_class ) if len ( successes ) > 1 : raise SystemError ( "Command %s is valid for multiple NetMHC versions. " "This is likely an mhctools bug." % program_name ) if len ( successes ) == 0 : raise SystemError ( "Command %s is not a valid way of calling any NetMHC software." % program_name ) netmhc_class = successes [ 0 ] return netmhc_class ( alleles = alleles , default_peptide_lengths = default_peptide_lengths , program_name = program_name )
860	def getTemporalDelay ( inferenceElement , key = None ) : # ----------------------------------------------------------------------- # For next step prediction, we shift by 1 if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 # ----------------------------------------------------------------------- # For classification, anomaly scores, the inferences immediately succeed the # inputs if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 # ----------------------------------------------------------------------- # For multistep prediction, the delay is based on the key in the inference # dictionary if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) # ----------------------------------------------------------------------- # default: return 0 return 0
10172	def _get_oldest_event_timestamp ( self ) : # Retrieve the oldest event in order to start aggregation # from there query_events = Search ( using = self . client , index = self . event_index ) [ 0 : 1 ] . sort ( { 'timestamp' : { 'order' : 'asc' } } ) result = query_events . execute ( ) # There might not be any events yet if the first event have been # indexed but the indices have not been refreshed yet. if len ( result ) == 0 : return None return parser . parse ( result [ 0 ] [ 'timestamp' ] )
6200	def _sim_timestamps ( self , max_rate , bg_rate , emission , i_start , rs , ip_start = 0 , scale = 10 , sort = True ) : counts_chunk = sim_timetrace_bg ( emission , max_rate , bg_rate , self . t_step , rs = rs ) nrows = emission . shape [ 0 ] if bg_rate is not None : nrows += 1 assert counts_chunk . shape == ( nrows , emission . shape [ 1 ] ) max_counts = counts_chunk . max ( ) if max_counts == 0 : return np . array ( [ ] , dtype = np . int64 ) , np . array ( [ ] , dtype = np . int64 ) time_start = i_start * scale time_stop = time_start + counts_chunk . shape [ 1 ] * scale ts_range = np . arange ( time_start , time_stop , scale , dtype = 'int64' ) # Loop for each particle to compute timestamps times_chunk_p = [ ] par_index_chunk_p = [ ] for ip , counts_chunk_ip in enumerate ( counts_chunk ) : # Compute timestamps for particle ip for all bins with counts times_c_ip = [ ] for v in range ( 1 , max_counts + 1 ) : times_c_ip . append ( ts_range [ counts_chunk_ip >= v ] ) # Stack the timestamps from different "counts" t = np . hstack ( times_c_ip ) # Append current particle times_chunk_p . append ( t ) par_index_chunk_p . append ( np . full ( t . size , ip + ip_start , dtype = 'u1' ) ) # Merge the arrays of different particles times_chunk = np . hstack ( times_chunk_p ) par_index_chunk = np . hstack ( par_index_chunk_p ) if sort : # Sort timestamps inside the merged chunk index_sort = times_chunk . argsort ( kind = 'mergesort' ) times_chunk = times_chunk [ index_sort ] par_index_chunk = par_index_chunk [ index_sort ] return times_chunk , par_index_chunk
6464	def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( ' %-12s' % ( palette , ) ) return 0
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
5629	def _get_digest ( self ) : return hmac . new ( self . _secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . _secret else None
10362	def self_edge_filter ( _ : BELGraph , source : BaseEntity , target : BaseEntity , __ : str ) -> bool : return source == target
2358	def registerDriver ( iface , driver , class_implements = [ ] ) : for class_item in class_implements : classImplements ( class_item , iface ) component . provideAdapter ( factory = driver , adapts = [ iface ] , provides = IDriver )
5619	def execute ( mp , td_resampling = "nearest" , td_matching_method = "gdal" , td_matching_max_zoom = None , td_matching_precision = 8 , td_fallback_to_higher_zoom = False , clip_pixelbuffer = 0 , * * kwargs ) : # read clip geometry if "clip" in mp . params [ "input" ] : clip_geom = mp . open ( "clip" ) . read ( ) if not clip_geom : logger . debug ( "no clip data over tile" ) return "empty" else : clip_geom = [ ] with mp . open ( "raster" , matching_method = td_matching_method , matching_max_zoom = td_matching_max_zoom , matching_precision = td_matching_precision , fallback_to_higher_zoom = td_fallback_to_higher_zoom , resampling = td_resampling ) as raster : raster_data = raster . read ( ) if raster . is_empty ( ) or raster_data [ 0 ] . mask . all ( ) : logger . debug ( "raster empty" ) return "empty" if clip_geom : # apply original nodata mask and clip clipped = mp . clip ( np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data ) , clip_geom , clip_buffer = clip_pixelbuffer , inverted = True ) return np . where ( clipped . mask , clipped , mp . params [ "output" ] . nodata ) else : return np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data )
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
11707	def generate_gamete ( self , egg_or_sperm_word ) : p_rate_of_mutation = [ 0.9 , 0.1 ] should_use_mutant_pool = ( npchoice ( [ 0 , 1 ] , 1 , p = p_rate_of_mutation ) [ 0 ] == 1 ) if should_use_mutant_pool : pool = tokens . secondary_tokens else : pool = tokens . primary_tokens return get_matches ( egg_or_sperm_word , pool , 23 )
1659	def CheckCasts ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Check to see if they're using an conversion function cast. # I just try to capture the most common basic types, though there are more. # Parameterless conversion functions, such as bool(), are allowed as they are # probably a member operator declaration or default constructor. match = Search ( r'(\bnew\s+(?:const\s+)?|\S<\s*(?:const\s+)?)?\b' r'(int|float|double|bool|char|int32|uint32|int64|uint64)' r'(\([^)].*)' , line ) expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) if match and not expecting_function : matched_type = match . group ( 2 ) # matched_new_or_template is used to silence two false positives: # - New operators # - Template arguments with function types # # For template arguments, we match on types immediately following # an opening bracket without any spaces. This is a fast way to # silence the common case where the function type is the first # template argument. False negative with less-than comparison is # avoided because those operators are usually followed by a space. # # function<double(double)> // bracket + no space = false positive # value < double(42) // bracket + space = true positive matched_new_or_template = match . group ( 1 ) # Avoid arrays by looking for brackets that come after the closing # parenthesis. if Match ( r'\([^()]+\)\s*\[' , match . group ( 3 ) ) : return # Other things to ignore: # - Function pointers # - Casts to pointer types # - Placement new # - Alias declarations matched_funcptr = match . group ( 3 ) if ( matched_new_or_template is None and not ( matched_funcptr and ( Match ( r'\((?:[^() ]+::\s*\*\s*)?[^() ]+\)\s*\(' , matched_funcptr ) or matched_funcptr . startswith ( '(*)' ) ) ) and not Match ( r'\s*using\s+\S+\s*=\s*' + matched_type , line ) and not Search ( r'new\(\S+\)\s*' + matched_type , line ) ) : error ( filename , linenum , 'readability/casting' , 4 , 'Using deprecated casting style. ' 'Use static_cast<%s>(...) instead' % matched_type ) if not expecting_function : CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , r'\((int|float|double|bool|char|u?int(16|32|64))\)' , error ) # This doesn't catch all cases. Consider (const char * const)"hello". # # (char *) "foo" should always be a const_cast (reinterpret_cast won't # compile). if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , r'\((char\s?\*+\s?)\)\s*"' , error ) : pass else : # Check pointer casts for other than string constants CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , r'\((\w+\s?\*+\s?)\)' , error ) # In addition, we look for people taking the address of a cast. This # is dangerous -- casts can assign to temporaries, so the pointer doesn't # point where you think. # # Some non-identifier character is required before the '&' for the # expression to be recognized as a cast. These are casts: # expression = &static_cast<int*>(temporary()); # function(&(int*)(temporary())); # # This is not a cast: # reference_type&(int* function_param); match = Search ( r'(?:[^\w]&\(([^)*][^)]*)\)[\w(])|' r'(?:[^\w]&(static|dynamic|down|reinterpret)_cast\b)' , line ) if match : # Try a better error message when the & is bound to something # dereferenced by the casted pointer, as opposed to the casted # pointer itself. parenthesis_error = False match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\b)<' , line ) if match : _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) if x2 >= 0 : extended_line = clean_lines . elided [ y2 ] [ x2 : ] if y2 < clean_lines . NumLines ( ) - 1 : extended_line += clean_lines . elided [ y2 + 1 ] if Match ( r'\s*(?:->|\[)' , extended_line ) : parenthesis_error = True if parenthesis_error : error ( filename , linenum , 'readability/casting' , 4 , ( 'Are you taking an address of something dereferenced ' 'from a cast? Wrapping the dereferenced expression in ' 'parentheses will make the binding more obvious' ) ) else : error ( filename , linenum , 'runtime/casting' , 4 , ( 'Are you taking an address of a cast? ' 'This is dangerous: could be a temp var. ' 'Take the address before doing the cast, rather than after' ) )
11883	def scanProcessForOpenFile ( pid , searchPortion , isExactMatch = True , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e prefixDir = "/proc/%d/fd" % ( pid , ) processFDs = os . listdir ( prefixDir ) matchedFDs = [ ] matchedFilenames = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor == totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) == totalPath . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor in totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) in totalPath . lower ( ) ) for fd in processFDs : fdPath = os . readlink ( prefixDir + '/' + fd ) if isMatch ( searchPortion , fdPath ) : matchedFDs . append ( fd ) matchedFilenames . append ( fdPath ) if len ( matchedFDs ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'fds' : matchedFDs , 'filenames' : matchedFilenames , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
7371	def main ( args_list = None ) : args = parse_args ( args_list ) binding_predictions = run_predictor ( args ) df = binding_predictions . to_dataframe ( ) logger . info ( '\n%s' , df ) if args . output_csv : df . to_csv ( args . output_csv , index = False ) print ( "Wrote: %s" % args . output_csv )
10772	def filled_contour ( self , min = None , max = None ) : # pylint: disable=redefined-builtin,redefined-outer-name # Get the contour vertices. if min is None : min = np . finfo ( np . float64 ) . min if max is None : max = np . finfo ( np . float64 ) . max vertices , codes = ( self . _contour_generator . create_filled_contour ( min , max ) ) return self . formatter ( ( min , max ) , vertices , codes )
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
12006	def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
5427	def _wait_for_any_job ( provider , job_ids , poll_interval ) : if not job_ids : return while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = job_ids ) running_jobs = set ( ) failed_jobs = set ( ) for t in tasks : status = t . get_field ( 'task-status' ) job_id = t . get_field ( 'job-id' ) if status in [ 'FAILURE' , 'CANCELED' ] : failed_jobs . add ( job_id ) if status == 'RUNNING' : running_jobs . add ( job_id ) remaining_jobs = running_jobs . difference ( failed_jobs ) if failed_jobs or len ( remaining_jobs ) != len ( job_ids ) : return remaining_jobs SLEEP_FUNCTION ( poll_interval )
10386	def match_simple_metapath ( graph , node , simple_metapath ) : if 0 == len ( simple_metapath ) : yield node , else : for neighbor in graph . edges [ node ] : if graph . nodes [ neighbor ] [ FUNCTION ] == simple_metapath [ 0 ] : for path in match_simple_metapath ( graph , neighbor , simple_metapath [ 1 : ] ) : if node not in path : yield ( node , ) + path
10594	def Nu_x ( self , L , theta , Ts , * * statef ) : Tf = statef [ 'T' ] thetar = radians ( theta ) if self . _isgas : self . Tr = Ts - 0.38 * ( Ts - Tf ) beta = self . _fluid . beta ( T = Tf ) else : # for liquids self . Tr = Ts - 0.5 * ( Ts - Tf ) beta = self . _fluid . beta ( T = self . Tr ) if Ts > Tf : # hot surface if 0.0 < theta < 45.0 : g = const . g * cos ( thetar ) else : g = const . g else : # cold surface if - 45.0 < theta < 0.0 : g = const . g * cos ( thetar ) else : g = const . g nu = self . _fluid . nu ( T = self . Tr ) alpha = self . _fluid . alpha ( T = self . Tr ) Gr = dq . Gr ( L , Ts , Tf , beta , nu , g ) Pr = dq . Pr ( nu , alpha ) Ra = Gr * Pr eq = [ self . equation_dict [ r ] for r in self . regions if r . contains_point ( theta , Ra ) ] [ 0 ] return eq ( self , Ra , Pr )
12523	def check_call ( cmd_args ) : p = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) ( output , err ) = p . communicate ( ) return output
12043	def algo_exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
13217	def connection_dsn ( self , name = None ) : return ' ' . join ( "%s=%s" % ( param , value ) for param , value in self . _connect_options ( name ) )
12274	def iso_reference_str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso_reference_valid_char ( c ) if c in ISO_REFERENCE_VALID_NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso_reference_char2int ( c ) ) ) return int ( '' . join ( numbers ) )
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : # Return if not collecting stats if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 # Compute the prediction score, how well the prediction from the last # time step predicted the current bottom-up input ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) # Store the stats that don't depend on burn-in stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 # If we are passed the burn-in period, update the accumulated stats # Here's what various burn-in values mean: # 0: try to predict the first element of each sequence and all subsequent # 1: try to predict the second element of each sequence and all subsequent # etc. if stats [ 'nInfersSinceReset' ] <= self . burnIn : return # Burn-in related stats stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : # Collect cell confidences for every cell that correctly predicted current # bottom up input. Normalize confidence across each column cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] # Update cell confidence histogram: add column-normalized confidence # scores to the histogram self . _internalStats [ 'confHistogram' ] += cc
248	def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df
11948	def init ( base_level = DEFAULT_BASE_LOGGING_LEVEL , verbose_level = DEFAULT_VERBOSE_LOGGING_LEVEL , logging_config = None ) : if logging_config is None : logging_config = { } logging_config = logging_config or LOGGER # TODO: (IMPRV) only perform file related actions if file handler is # TODO: (IMPRV) defined. log_file = LOGGER [ 'handlers' ] [ 'file' ] [ 'filename' ] log_dir = os . path . dirname ( os . path . expanduser ( log_file ) ) if os . path . isfile ( log_dir ) : sys . exit ( 'file {0} exists - log directory cannot be created ' 'there. please remove the file and try again.' . format ( log_dir ) ) try : if not os . path . exists ( log_dir ) and not len ( log_dir ) == 0 : os . makedirs ( log_dir ) dictconfig . dictConfig ( logging_config ) lgr = logging . getLogger ( 'user' ) lgr . setLevel ( base_level ) return lgr except ValueError as e : sys . exit ( 'could not initialize logger.' ' verify your logger config' ' and permissions to write to {0} ({1})' . format ( log_file , e ) )
5279	def make_pmml_pipeline ( obj , active_fields = None , target_fields = None ) : steps = _filter_steps ( _get_steps ( obj ) ) pipeline = PMMLPipeline ( steps ) if active_fields is not None : pipeline . active_fields = numpy . asarray ( active_fields ) if target_fields is not None : pipeline . target_fields = numpy . asarray ( target_fields ) return pipeline
11015	def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
5813	def detect_other_protocol ( server_handshake_bytes ) : if server_handshake_bytes [ 0 : 5 ] == b'HTTP/' : return 'HTTP' if server_handshake_bytes [ 0 : 4 ] == b'220 ' : if re . match ( b'^[^\r\n]*ftp' , server_handshake_bytes , re . I ) : return 'FTP' else : return 'SMTP' if server_handshake_bytes [ 0 : 4 ] == b'220-' : return 'FTP' if server_handshake_bytes [ 0 : 4 ] == b'+OK ' : return 'POP3' if server_handshake_bytes [ 0 : 4 ] == b'* OK' or server_handshake_bytes [ 0 : 9 ] == b'* PREAUTH' : return 'IMAP' return None
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : ## an array to fill with consensus site calls cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) ## iterate over columns for col in xrange ( arr . shape [ 1 ] ) : ## the site of focus carr = arr [ : , col ] ## make mask of N and - sites mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] ## skip if only empties (e.g., N-) if not marr . shape [ 0 ] : cons [ col ] = 78 ## skip if not variable elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] ## estimate variable site call else : ## get allele freqs (first-most, second, third = p, q, r) counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] ## based on biallelic depth bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : ## if depth is too high, reduce to sampled int if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq ## make statistical base call if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) #LOGGER.info("ishet, prob, b1, b2: %s %s %s %s", ishet, prob, base1, base2) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase ## make majrule base call else : #if bidepth >= mindepth_majrule: if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
209	def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
6526	def parse ( cls , content , is_pyproject = False ) : parsed = pytoml . loads ( content ) if is_pyproject : parsed = parsed . get ( 'tool' , { } ) parsed = parsed . get ( 'tidypy' , { } ) return parsed
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
8909	def fetch_by_url ( self , url ) : service = self . collection . find_one ( { 'url' : url } ) if not service : raise ServiceNotFound return Service ( service )
6499	def search ( self , query_string = None , field_dictionary = None , filter_dictionary = None , exclude_dictionary = None , facet_terms = None , exclude_ids = None , use_field_match = False , * * kwargs ) : # pylint: disable=too-many-arguments, too-many-locals, too-many-branches, arguments-differ log . debug ( "searching index with %s" , query_string ) elastic_queries = [ ] elastic_filters = [ ] # We have a query string, search all fields for matching text within the "content" node if query_string : if six . PY2 : query_string = query_string . encode ( 'utf-8' ) . translate ( None , RESERVED_CHARACTERS ) else : query_string = query_string . translate ( query_string . maketrans ( '' , '' , RESERVED_CHARACTERS ) ) elastic_queries . append ( { "query_string" : { "fields" : [ "content.*" ] , "query" : query_string } } ) if field_dictionary : if use_field_match : elastic_queries . extend ( _process_field_queries ( field_dictionary ) ) else : elastic_filters . extend ( _process_field_filters ( field_dictionary ) ) if filter_dictionary : elastic_filters . extend ( _process_filters ( filter_dictionary ) ) # Support deprecated argument of exclude_ids if exclude_ids : if not exclude_dictionary : exclude_dictionary = { } if "_id" not in exclude_dictionary : exclude_dictionary [ "_id" ] = [ ] exclude_dictionary [ "_id" ] . extend ( exclude_ids ) if exclude_dictionary : elastic_filters . append ( _process_exclude_dictionary ( exclude_dictionary ) ) query_segment = { "match_all" : { } } if elastic_queries : query_segment = { "bool" : { "must" : elastic_queries } } query = query_segment if elastic_filters : filter_segment = { "bool" : { "must" : elastic_filters } } query = { "filtered" : { "query" : query_segment , "filter" : filter_segment , } } body = { "query" : query } if facet_terms : facet_query = _process_facet_terms ( facet_terms ) if facet_query : body [ "facets" ] = facet_query try : es_response = self . _es . search ( index = self . index_name , body = body , * * kwargs ) except exceptions . ElasticsearchException as ex : message = six . text_type ( ex ) if 'QueryParsingException' in message : log . exception ( "Malformed search query: %s" , message ) raise QueryParseError ( 'Malformed search query.' ) else : # log information and re-raise log . exception ( "error while searching index - %s" , str ( message ) ) raise return _translate_hits ( es_response )
811	def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : # Sanity checking if not isinstance ( statsInfo , dict ) : raise RuntimeError ( "statsInfo must be a dict -- " "found '%s' instead" % type ( statsInfo ) ) filename = resource_filename ( "nupic.datafiles" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) # Use cached stats if found AND if it has the right data if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , "rb" ) ) except : # Ok to ignore errors -- we will just re-generate the file print "Warning: unable to load stats for %s -- " "will regenerate" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print "generateStats: re-generating stats file %s because " "keys %s are not available" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print "Generating statistics for file '%s' with filters '%s'" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters # Convert collector description to collector object stats = [ ] for field in statsInfo : # field = key from statsInfo if statsInfo [ field ] == "number" : # This wants a field name e.g. consumption and the field type as the value statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == "category" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( "Unknown stats type '%s' for field '%s'" % ( statsInfo [ field ] , field ) ) # Now collect the stats if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor # Assemble the results and return r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , "wb" ) pickle . dump ( r , f ) f . close ( ) # caller may need to know name of cached file r [ "_filename" ] = statsFilename return r
3156	def delete ( self , list_id , segment_id ) : return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
2145	def request ( self , method , url , * args , * * kwargs ) : # If the URL has the api/vX at the front strip it off # This is common to have if you are extracting a URL from an existing object. # For example, any of the 'related' fields of an object will have this import re url = re . sub ( "^/?api/v[0-9]+/" , "" , url ) # Piece together the full URL. use_version = not url . startswith ( '/o/' ) url = '%s%s' % ( self . get_prefix ( use_version ) , url . lstrip ( '/' ) ) # Ansible Tower expects authenticated requests; add the authentication # from settings if it's provided. kwargs . setdefault ( 'auth' , BasicTowerAuth ( settings . username , settings . password , self ) ) # POST and PUT requests will send JSON by default; make this # the content_type by default. This makes it such that we don't have # to constantly write that in our code, which gets repetitive. headers = kwargs . get ( 'headers' , { } ) if method . upper ( ) in ( 'PATCH' , 'POST' , 'PUT' ) : headers . setdefault ( 'Content-Type' , 'application/json' ) kwargs [ 'headers' ] = headers # If debugging is on, print the URL and data being sent. debug . log ( '%s %s' % ( method , url ) , fg = 'blue' , bold = True ) if method in ( 'POST' , 'PUT' , 'PATCH' ) : debug . log ( 'Data: %s' % kwargs . get ( 'data' , { } ) , fg = 'blue' , bold = True ) if method == 'GET' or kwargs . get ( 'params' , None ) : debug . log ( 'Params: %s' % kwargs . get ( 'params' , { } ) , fg = 'blue' , bold = True ) debug . log ( '' ) # If this is a JSON request, encode the data value. if headers . get ( 'Content-Type' , '' ) == 'application/json' : kwargs [ 'data' ] = json . dumps ( kwargs . get ( 'data' , { } ) ) r = self . _make_request ( method , url , args , kwargs ) # Sanity check: Did the server send back some kind of internal error? # If so, bubble this up. if r . status_code >= 500 : raise exc . ServerError ( 'The Tower server sent back a server error. ' 'Please try again later.' ) # Sanity check: Did we fail to authenticate properly? # If so, fail out now; this is always a failure. if r . status_code == 401 : raise exc . AuthError ( 'Invalid Tower authentication credentials (HTTP 401).' ) # Sanity check: Did we get a forbidden response, which means that # the user isn't allowed to do this? Report that. if r . status_code == 403 : raise exc . Forbidden ( "You don't have permission to do that (HTTP 403)." ) # Sanity check: Did we get a 404 response? # Requests with primary keys will return a 404 if there is no response, # and we want to consistently trap these. if r . status_code == 404 : raise exc . NotFound ( 'The requested object could not be found.' ) # Sanity check: Did we get a 405 response? # A 405 means we used a method that isn't allowed. Usually this # is a bad request, but it requires special treatment because the # API sends it as a logic error in a few situations (e.g. trying to # cancel a job that isn't running). if r . status_code == 405 : raise exc . MethodNotAllowed ( "The Tower server says you can't make a request with the " "%s method to that URL (%s)." % ( method , url ) , ) # Sanity check: Did we get some other kind of error? # If so, write an appropriate error message. if r . status_code >= 400 : raise exc . BadRequest ( 'The Tower server claims it was sent a bad request.\n\n' '%s %s\nParams: %s\nData: %s\n\nResponse: %s' % ( method , url , kwargs . get ( 'params' , None ) , kwargs . get ( 'data' , None ) , r . content . decode ( 'utf8' ) ) ) # Django REST Framework intelligently prints API keys in the # order that they are defined in the models and serializer. # # We want to preserve this behavior when it is possible to do so # with minimal effort, because while the order has no explicit meaning, # we make some effort to order keys in a convenient manner. # # To this end, make this response into an APIResponse subclass # (defined below), which has a `json` method that doesn't lose key # order. r . __class__ = APIResponse # Return the response object. return r
10335	def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : """Remove a node by identifier.""" node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
13052	def nmap ( nmap_args , ips ) : config = Config ( ) arguments = [ 'nmap' , '-Pn' ] arguments . extend ( ips ) arguments . extend ( nmap_args ) output_file = '' now = datetime . datetime . now ( ) if not '-oA' in nmap_args : output_name = 'nmap_jackal_{}' . format ( now . strftime ( "%Y-%m-%d %H:%M" ) ) path_name = os . path . join ( config . get ( 'nmap' , 'directory' ) , output_name ) print_notification ( "Writing output of nmap to {}" . format ( path_name ) ) if not os . path . exists ( config . get ( 'nmap' , 'directory' ) ) : os . makedirs ( config . get ( 'nmap' , 'directory' ) ) output_file = path_name + '.xml' arguments . extend ( [ '-oA' , path_name ] ) else : output_file = nmap_args [ nmap_args . index ( '-oA' ) + 1 ] + '.xml' print_notification ( "Starting nmap" ) subprocess . call ( arguments ) with open ( output_file , 'r' ) as f : return f . read ( )
928	def _getEndTime ( self , t ) : assert isinstance ( t , datetime . datetime ) if self . _aggTimeDelta : return t + self . _aggTimeDelta else : year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 return t . replace ( year = year , month = month )
1390	def get_status ( self ) : status = None if self . physical_plan and self . physical_plan . topology : status = self . physical_plan . topology . state if status == 1 : return "Running" elif status == 2 : return "Paused" elif status == 3 : return "Killed" else : return "Unknown"
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
11030	def _request ( self , failure , endpoints , * args , * * kwargs ) : # We've run out of endpoints, fail if not endpoints : return failure endpoint = endpoints . pop ( 0 ) d = super ( MarathonClient , self ) . request ( * args , url = endpoint , * * kwargs ) # If something goes wrong, call ourselves again with the remaining # endpoints d . addErrback ( self . _request , endpoints , * args , * * kwargs ) return d
1944	def _hook_syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg_read(self._to_unicorn_id('RIP')))} to perform syscall" ) self . sync_unicorn_to_manticore ( ) from . . native . cpu . abstractcpu import Syscall self . _to_raise = Syscall ( ) uc . emu_stop ( )
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
9399	def restart ( self ) : if self . _engine : self . _engine . repl . terminate ( ) executable = self . _executable if executable : os . environ [ 'OCTAVE_EXECUTABLE' ] = executable if 'OCTAVE_EXECUTABLE' not in os . environ and 'OCTAVE' in os . environ : os . environ [ 'OCTAVE_EXECUTABLE' ] = os . environ [ 'OCTAVE' ] self . _engine = OctaveEngine ( stdin_handler = self . _handle_stdin , logger = self . logger ) # Add local Octave scripts. self . _engine . eval ( 'addpath("%s");' % HERE . replace ( osp . sep , '/' ) )
9686	def pm ( self ) : resp = [ ] data = { } # Send the command byte self . cnxn . xfer ( [ 0x32 ] ) # Wait 10 ms sleep ( 10e-3 ) # read the histogram for i in range ( 12 ) : r = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] resp . append ( r ) # convert to real things and store in dictionary! data [ 'PM1' ] = self . _calculate_float ( resp [ 0 : 4 ] ) data [ 'PM2.5' ] = self . _calculate_float ( resp [ 4 : 8 ] ) data [ 'PM10' ] = self . _calculate_float ( resp [ 8 : ] ) sleep ( 0.1 ) return data
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : # Ensure we have a response object. request = self . _request Deserializer = None if format : # An explicit format was given; do not attempt to auto-detect # a deserializer. Deserializer = self . meta . deserializers [ format ] if not Deserializer : # Determine an appropriate deserializer to use by # introspecting the request object and looking at # the `Content-Type` header. media_ranges = request . get ( 'Content-Type' ) if media_ranges : # Parse the media ranges and determine the deserializer # that is the closest match. media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : # Client didn't provide a content-type; we're supposed # to auto-detect. # TODO: Implement this. pass if Deserializer : try : # Attempt to deserialize the data using the determined # deserializer. deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : # Failed to deserialize the data. pass # Failed to determine a deserializer; or failed to deserialize. raise http . exceptions . UnsupportedMediaType ( )
11535	def available_drivers ( ) : global __modules global __available if type ( __modules ) is not list : __modules = list ( __modules ) if not __available : __available = [ d . ahioDriverInfo . NAME for d in __modules if d . ahioDriverInfo . AVAILABLE ] return __available
10793	def separate_particles_into_groups ( s , region_size = 40 , bounds = None ) : imtile = ( s . oshape . translate ( - s . pad ) if bounds is None else util . Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) # does all particle including out of image, is that correct? region = util . Tile ( region_size , dim = s . dim ) trange = np . ceil ( imtile . shape . astype ( 'float' ) / region . shape ) translations = util . Tile ( trange ) . coords ( form = 'vector' ) translations = translations . reshape ( - 1 , translations . shape [ - 1 ] ) groups = [ ] positions = s . obj_get_positions ( ) for v in translations : tmptile = region . copy ( ) . translate ( region . shape * v - s . pad ) groups . append ( find_particles_in_tile ( positions , tmptile ) ) return [ g for g in groups if len ( g ) > 0 ]
11119	def get_file_info ( self , relativePath , name = None ) : # normalize relative path and name relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' can't be a file name." if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) # initialize message errorMessage = "" # get directory info dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) if dirInfoDict is None : return None , errorMessage # get file info fileInfo = dict . __getitem__ ( dirInfoDict , "files" ) . get ( name , None ) if fileInfo is None : errorMessage = "file %s does not exist in relative path '%s'" % ( name , relativePath ) return fileInfo , errorMessage
1423	def copy ( self , new_object ) : new_object . classdesc = self . classdesc for name in self . classdesc . fields_names : new_object . __setattr__ ( name , getattr ( self , name ) )
1151	def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except NameError : unicodetype = ( ) try : message = str ( message ) except UnicodeEncodeError : pass s = "%s: %s: %s\n" % ( lineno , category . __name__ , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += " %s\n" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except UnicodeDecodeError : pass s = "%s:%s" % ( filename , s ) return s
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
2020	def ADDMOD ( self , a , b , c ) : try : result = Operators . ITEBV ( 256 , c == 0 , 0 , ( a + b ) % c ) except ZeroDivisionError : result = 0 return result
13041	def pipe_worker ( pipename , filename , object_type , query , format_string , unique = False ) : print_notification ( "[{}] Starting pipe" . format ( pipename ) ) object_type = object_type ( ) try : while True : uniq = set ( ) # Remove the previous file if it exists if os . path . exists ( filename ) : os . remove ( filename ) # Create the named pipe os . mkfifo ( filename ) # This function will block until a process opens it with open ( filename , 'w' ) as pipe : print_success ( "[{}] Providing data" . format ( pipename ) ) # Search the database objects = object_type . search ( * * query ) for obj in objects : data = fmt . format ( format_string , * * obj . to_dict ( ) ) if unique : if not data in uniq : uniq . add ( data ) pipe . write ( data + '\n' ) else : pipe . write ( data + '\n' ) os . unlink ( filename ) except KeyboardInterrupt : print_notification ( "[{}] Shutting down named pipe" . format ( pipename ) ) except Exception as e : print_error ( "[{}] Error: {}, stopping named pipe" . format ( e , pipename ) ) finally : os . remove ( filename )
1437	def update_reduced_metric ( self , name , value , key = None ) : if name not in self . metrics : Log . error ( "In update_reduced_metric(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , ReducedMetric ) : self . metrics [ name ] . update ( value ) elif key is not None and isinstance ( self . metrics [ name ] , MultiReducedMetric ) : self . metrics [ name ] . update ( key , value ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
10417	def variants_of ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return _get_filtered_variants_of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS_VARIANT and pybel . struct . has_protein_modification ( v ) ) }
13233	def load ( directory_name , module_name ) : directory_name = os . path . expanduser ( directory_name ) if os . path . isdir ( directory_name ) and directory_name not in sys . path : sys . path . append ( directory_name ) try : return importlib . import_module ( module_name ) except ImportError : pass
11050	def _dispatch_event ( self ) : data = self . _prepare_data ( ) if data is not None : self . _handler ( self . _event , data ) self . _reset_event_data ( )
6545	def is_connected ( self ) : # need to wrap in try/except b/c of wc3270's socket connection dynamics try : # this is basically a no-op, but it results in the the current status # getting updated self . exec_command ( b"Query(ConnectionState)" ) # connected status is like 'C(192.168.1.1)', disconnected is 'N' return self . status . connection_state . startswith ( b"C(" ) except NotConnectedException : return False
10696	def hsv_to_rgb ( hsv ) : h , s , v = hsv c = v * s h /= 60 x = c * ( 1 - abs ( ( h % 2 ) - 1 ) ) m = v - c if h < 1 : res = ( c , x , 0 ) elif h < 2 : res = ( x , c , 0 ) elif h < 3 : res = ( 0 , c , x ) elif h < 4 : res = ( 0 , x , c ) elif h < 5 : res = ( x , 0 , c ) elif h < 6 : res = ( c , 0 , x ) else : raise ColorException ( "Unable to convert from HSV to RGB" ) r , g , b = res return round ( ( r + m ) * 255 , 3 ) , round ( ( g + m ) * 255 , 3 ) , round ( ( b + m ) * 255 , 3 )
693	def loadExperiment ( path ) : if not os . path . isdir ( path ) : path = os . path . dirname ( path ) descriptionPyModule = loadExperimentDescriptionScriptFromDir ( path ) expIface = getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) return expIface . getModelDescription ( ) , expIface . getModelControl ( )
3632	def baseId ( resource_id , return_version = False ) : version = 0 resource_id = resource_id + 0xC4000000 # 3288334336 # TODO: version is broken due ^^, needs refactoring while resource_id > 0x01000000 : # 16777216 version += 1 if version == 1 : resource_id -= 0x80000000 # 2147483648 # 0x50000000 # 1342177280 ? || 0x2000000 # 33554432 elif version == 2 : resource_id -= 0x03000000 # 50331648 else : resource_id -= 0x01000000 # 16777216 if return_version : return resource_id , version - 67 # just correct "magic number" return resource_id
1370	def get_subparser ( parser , command ) : # pylint: disable=protected-access subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] # there will probably only be one subparser_action, # but better save than sorry for subparsers_action in subparsers_actions : # get all subparsers for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
1170	def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = " " * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )
12136	def fields ( self ) : parse = list ( string . Formatter ( ) . parse ( self . pattern ) ) return [ f for f in zip ( * parse ) [ 1 ] if f is not None ]
9234	def run ( self ) : if not self . options . project or not self . options . user : print ( "Project and/or user missing. " "For help run:\n pygcgen --help" ) return if not self . options . quiet : print ( "Generating changelog..." ) log = None try : log = self . generator . compound_changelog ( ) except ChangelogGeneratorError as err : print ( "\n\033[91m\033[1m{}\x1b[0m" . format ( err . args [ 0 ] ) ) exit ( 1 ) if not log : if not self . options . quiet : print ( "Empty changelog generated. {} not written." . format ( self . options . output ) ) return if self . options . no_overwrite : out = checkname ( self . options . output ) else : out = self . options . output with codecs . open ( out , "w" , "utf-8" ) as fh : fh . write ( log ) if not self . options . quiet : print ( "Done!" ) print ( "Generated changelog written to {}" . format ( out ) )
589	def setAutoDetectWaitRecords ( self , waitRecords ) : if not isinstance ( waitRecords , int ) : raise HTMPredictionModelInvalidArgument ( "Invalid argument type \'%s\'. WaitRecord " "must be a number." % ( type ( waitRecords ) ) ) if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( "Invalid value. autoDetectWaitRecord value " "must be valid record within output stream. Current minimum ROWID in " "output stream is %d." % ( self . saved_states [ 0 ] . ROWID ) ) self . _autoDetectWaitRecords = waitRecords # Update all the states in the classifier's cache for state in self . saved_states : self . _updateState ( state )
10539	def update_category ( category ) : try : res = _pybossa_req ( 'put' , 'category' , category . id , payload = category . data ) if res . get ( 'id' ) : return Category ( res ) else : return res except : # pragma: no cover raise
3247	def get_users ( group , * * conn ) : group_details = get_group_api ( group [ 'GroupName' ] , * * conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
1945	def _hook_write_mem ( self , uc , access , address , size , value , data ) : self . _mem_delta [ address ] = ( value , size ) return True
10519	def onedown ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
11715	def console_output ( self , instance = None ) : if instance is None : instance = self . instance ( ) for stage in instance [ 'stages' ] : for job in stage [ 'jobs' ] : if job [ 'result' ] not in self . final_results : continue artifact = self . artifact ( instance [ 'counter' ] , stage [ 'name' ] , job [ 'name' ] , stage [ 'counter' ] ) output = artifact . get ( 'cruise-output/console.log' ) yield ( { 'pipeline' : self . name , 'pipeline_counter' : instance [ 'counter' ] , 'stage' : stage [ 'name' ] , 'stage_counter' : stage [ 'counter' ] , 'job' : job [ 'name' ] , 'job_result' : job [ 'result' ] , } , output . body )
4127	def plot ( self , * * kargs ) : from pylab import plot , linspace , xlabel , ylabel , grid time = linspace ( 1 * self . dt , self . N * self . dt , self . N ) plot ( time , self . data , * * kargs ) xlabel ( 'Time' ) ylabel ( 'Amplitude' ) grid ( True )
7278	def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
12947	def saveToExternal ( self , redisCon ) : if type ( redisCon ) == dict : conn = redis . Redis ( * * redisCon ) elif hasattr ( conn , '__class__' ) and issubclass ( conn . __class__ , redis . Redis ) : conn = redisCon else : raise ValueError ( 'saveToExternal "redisCon" param must either be a dictionary of connection parameters, or redis.Redis, or extension thereof' ) saver = self . saver # Fetch next PK from external forceID = saver . _getNextID ( conn ) # Redundant because of changes in save method myCopy = self . copy ( False ) return saver . save ( myCopy , usePipeline = True , forceID = forceID , conn = conn )
3858	def _on_watermark_notification ( self , notif ) : # Update the conversation: if self . get_user ( notif . user_id ) . is_self : logger . info ( 'latest_read_timestamp for {} updated to {}' . format ( self . id_ , notif . read_timestamp ) ) self_conversation_state = ( self . _conversation . self_conversation_state ) self_conversation_state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( notif . read_timestamp ) ) # Update the participants' watermarks: previous_timestamp = self . _watermarks . get ( notif . user_id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read_timestamp > previous_timestamp : logger . info ( ( 'latest_read_timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id_ , notif . user_id . chat_id , notif . read_timestamp ) ) self . _watermarks [ notif . user_id ] = notif . read_timestamp
1466	def setDefault ( self , constant , start , end ) : starttime = start / 60 * 60 if starttime < start : starttime += 60 endtime = end / 60 * 60 while starttime <= endtime : # STREAMCOMP-1559 # Second check is a work around, because the response from tmaster # contains value 0, if it is queries for the current timestamp, # since the bucket is created in the tmaster, but is not filled # by the metrics. if starttime not in self . timeline or self . timeline [ starttime ] == 0 : self . timeline [ starttime ] = constant starttime += 60
4326	def delay ( self , positions ) : if not isinstance ( positions , list ) : raise ValueError ( "positions must be a a list of numbers" ) if not all ( ( is_number ( p ) and p >= 0 ) for p in positions ) : raise ValueError ( "positions must be positive nubmers" ) effect_args = [ 'delay' ] effect_args . extend ( [ '{:f}' . format ( p ) for p in positions ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'delay' ) return self
11389	def can_run_from_cli ( self ) : ret = False ast_tree = ast . parse ( self . body , self . path ) calls = self . _find_calls ( ast_tree , __name__ , "exit" ) for call in calls : if re . search ( "{}\(" . format ( re . escape ( call ) ) , self . body ) : ret = True break return ret
13875	def CopyFiles ( source_dir , target_dir , create_target_dir = False , md5_check = False ) : import fnmatch # Check if we were given a directory or a directory with mask if IsDir ( source_dir ) : # Yes, it's a directory, copy everything from it source_mask = '*' else : # Split directory and mask source_dir , source_mask = os . path . split ( source_dir ) # Create directory if necessary if not IsDir ( target_dir ) : if create_target_dir : CreateDirectory ( target_dir ) else : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( target_dir ) # List and match files filenames = ListFiles ( source_dir ) # Check if we have a source directory if filenames is None : return # Copy files for i_filename in filenames : if md5_check and i_filename . endswith ( '.md5' ) : continue # md5 files will be copied by CopyFile when copying their associated files if fnmatch . fnmatch ( i_filename , source_mask ) : source_path = source_dir + '/' + i_filename target_path = target_dir + '/' + i_filename if IsDir ( source_path ) : # If we found a directory, copy it recursively CopyFiles ( source_path , target_path , create_target_dir = True , md5_check = md5_check ) else : CopyFile ( source_path , target_path , md5_check = md5_check )
10153	def _extract_operation_from_view ( self , view , args ) : op = { 'responses' : { 'default' : { 'description' : 'UNDOCUMENTED RESPONSE' } } , } # If 'produces' are not defined in the view, try get from renderers renderer = args . get ( 'renderer' , '' ) if "json" in renderer : # allows for "json" or "simplejson" produces = [ 'application/json' ] elif renderer == 'xml' : produces = [ 'text/xml' ] else : produces = None if produces : op . setdefault ( 'produces' , produces ) # Get explicit accepted content-types consumes = args . get ( 'content_type' ) if consumes is not None : # convert to a list, if it's not yet one consumes = to_list ( consumes ) # It is possible to add callables for content_type, so we have to # to filter those out, since we cannot evaluate those here. consumes = [ x for x in consumes if not callable ( x ) ] op [ 'consumes' ] = consumes # Get parameters from view schema is_colander = self . _is_colander_schema ( args ) if is_colander : schema = self . _extract_transform_colander_schema ( args ) parameters = self . parameters . from_schema ( schema ) else : # Bail out for now parameters = None if parameters : op [ 'parameters' ] = parameters # Get summary from docstring if isinstance ( view , six . string_types ) : if 'klass' in args : ob = args [ 'klass' ] view_ = getattr ( ob , view . lower ( ) ) docstring = trim ( view_ . __doc__ ) else : docstring = str ( trim ( view . __doc__ ) ) if docstring and self . summary_docstrings : op [ 'summary' ] = docstring # Get response definitions if 'response_schemas' in args : op [ 'responses' ] = self . responses . from_schema_mapping ( args [ 'response_schemas' ] ) # Get response tags if 'tags' in args : op [ 'tags' ] = args [ 'tags' ] # Get response operationId if 'operation_id' in args : op [ 'operationId' ] = args [ 'operation_id' ] # Get security policies if 'api_security' in args : op [ 'security' ] = args [ 'api_security' ] return op
1311	def KeyboardInput ( wVk : int , wScan : int , dwFlags : int = KeyboardEventFlag . KeyDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( KEYBDINPUT ( wVk , wScan , dwFlags , time_ , None ) )
10167	def get_md_status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : # The final 2 entries on this line: [n/m] [UUUU_] # [n/m] means that ideally the array would have n devices however, currently, m devices are in use. # Obviously when m >= n then things are good. ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] # [UUUU_] represents the status of each device, either U for up or _ for down. ret [ 'config' ] = splitted [ - 2 ] return ret
12170	def emit ( self , event , * args , * * kwargs ) : listeners = self . _listeners [ event ] listeners = itertools . chain ( listeners , self . _once [ event ] ) self . _once [ event ] = [ ] for listener in listeners : self . _loop . call_soon ( functools . partial ( self . _dispatch , event , listener , * args , * * kwargs , ) ) return self
11504	def delete_folder ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.delete' , parameters ) return response
12958	def _compat_rem_str_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _compat_get_str_key_for_index ( indexedField , val ) , pk )
914	def lscsum ( lx , epsilon = None ) : lx = numpy . asarray ( lx ) base = lx . max ( ) # If the input is the log of 0's, catch this condition before we generate # an exception, and return the log(0) if numpy . isinf ( base ) : return base # If the user specified an epsilon and we are below it, return epsilon if ( epsilon is not None ) and ( base < epsilon ) : return epsilon x = numpy . exp ( lx - base ) ssum = x . sum ( ) result = numpy . log ( ssum ) + base # try: # conventional = numpy.log(numpy.exp(lx).sum()) # if not similar(result, conventional): # if numpy.isinf(conventional).any() and not numpy.isinf(result).any(): # # print "Scaled log sum avoided underflow or overflow." # pass # else: # import sys # print >>sys.stderr, "Warning: scaled log sum did not match." # print >>sys.stderr, "Scaled log result:" # print >>sys.stderr, result # print >>sys.stderr, "Conventional result:" # print >>sys.stderr, conventional # except FloatingPointError, e: # # print "Scaled log sum avoided underflow or overflow." # pass return result
13668	def slinky ( filename , seconds_available , bucket_name , aws_key , aws_secret ) : if not os . environ . get ( 'AWS_ACCESS_KEY_ID' ) and os . environ . get ( 'AWS_SECRET_ACCESS_KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create_temp_s3_link ( filename , seconds_available , bucket_name )
13698	def wait ( self , timeout = None ) : if timeout is None : timeout = self . _timeout while self . _process . check_readable ( timeout ) : self . _flush ( )
827	def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( "_" ) ) raise ValueError ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )
13248	def get_bibliography ( lsst_bib_names = None , bibtex = None ) : bibtex_data = get_lsst_bibtex ( bibtex_filenames = lsst_bib_names ) # Parse with pybtex into BibliographyData instances pybtex_data = [ pybtex . database . parse_string ( _bibtex , 'bibtex' ) for _bibtex in bibtex_data . values ( ) ] # Also parse local bibtex content if bibtex is not None : pybtex_data . append ( pybtex . database . parse_string ( bibtex , 'bibtex' ) ) # Merge BibliographyData bib = pybtex_data [ 0 ] if len ( pybtex_data ) > 1 : for other_bib in pybtex_data [ 1 : ] : for key , entry in other_bib . entries . items ( ) : bib . add_entry ( key , entry ) return bib
188	def clip_out_of_image ( self ) : lss_cut = [ ls_clipped for ls in self . line_strings for ls_clipped in ls . clip_out_of_image ( self . shape ) ] return LineStringsOnImage ( lss_cut , shape = self . shape )
3893	def _get_parser ( extra_args ) : parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter , ) dirs = appdirs . AppDirs ( 'hangups' , 'hangups' ) default_token_path = os . path . join ( dirs . user_cache_dir , 'refresh_token.txt' ) parser . add_argument ( '--token-path' , default = default_token_path , help = 'path used to store OAuth refresh token' ) parser . add_argument ( '-d' , '--debug' , action = 'store_true' , help = 'log detailed debugging messages' ) for extra_arg in extra_args : parser . add_argument ( extra_arg , required = True ) return parser
10705	def get_locations ( ) : arequest = requests . get ( LOCATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
823	def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
10620	def amount ( self ) : return sum ( self . get_compound_amount ( c ) for c in self . material . compounds )
7348	async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier , * * kwargs ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , access_token = oauth_token , access_token_secret = oauth_token_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . access_token . get ( _suffix = "" , oauth_verifier = oauth_verifier ) return parse_token ( response )
11528	def add_scalar_data ( self , token , community_id , producer_display_name , metric_name , producer_revision , submit_time , value , * * kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time parameters [ 'value' ] = value optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' , 'submission_id' , 'submission_uuid' , 'unit' , 'reproduction_command' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'submission_id' : parameters [ 'submissionId' ] = kwargs [ key ] elif key == 'submission_uuid' : parameters [ 'submissionUuid' ] = kwargs [ key ] elif key == 'unit' : parameters [ 'unit' ] = kwargs [ key ] elif key == 'reproduction_command' : parameters [ 'reproductionCommand' ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.tracker.scalar.add' , parameters ) return response
1637	def CheckComment ( line , filename , linenum , next_line_start , error ) : commentpos = line . find ( '//' ) if commentpos != - 1 : # Check if the // may be in quotes. If so, ignore it if re . sub ( r'\\.' , '' , line [ 0 : commentpos ] ) . count ( '"' ) % 2 == 0 : # Allow one space for new scopes, two spaces otherwise: if ( not ( Match ( r'^.*{ *//' , line ) and next_line_start == commentpos ) and ( ( commentpos >= 1 and line [ commentpos - 1 ] not in string . whitespace ) or ( commentpos >= 2 and line [ commentpos - 2 ] not in string . whitespace ) ) ) : error ( filename , linenum , 'whitespace/comments' , 2 , 'At least two spaces is best between code and comments' ) # Checks for common mistakes in TODO comments. comment = line [ commentpos : ] match = _RE_PATTERN_TODO . match ( comment ) if match : # One whitespace is correct; zero whitespace is handled elsewhere. leading_whitespace = match . group ( 1 ) if len ( leading_whitespace ) > 1 : error ( filename , linenum , 'whitespace/todo' , 2 , 'Too many spaces before TODO' ) username = match . group ( 2 ) if not username : error ( filename , linenum , 'readability/todo' , 2 , 'Missing username in TODO; it should look like ' '"// TODO(my_username): Stuff."' ) middle_whitespace = match . group ( 3 ) # Comparisons made explicit for correctness -- pylint: disable=g-explicit-bool-comparison if middle_whitespace != ' ' and middle_whitespace != '' : error ( filename , linenum , 'whitespace/todo' , 2 , 'TODO(my_username) should be followed by a space' ) # If the comment contains an alphanumeric character, there # should be a space somewhere between it and the // unless # it's a /// or //! Doxygen comment. if ( Match ( r'//[^ ]*\w' , comment ) and not Match ( r'(///|//\!)(\s+|$)' , comment ) ) : error ( filename , linenum , 'whitespace/comments' , 4 , 'Should have a space between // and comment' )
41	def update_priorities ( self , idxes , priorities ) : assert len ( idxes ) == len ( priorities ) for idx , priority in zip ( idxes , priorities ) : assert priority > 0 assert 0 <= idx < len ( self . _storage ) self . _it_sum [ idx ] = priority ** self . _alpha self . _it_min [ idx ] = priority ** self . _alpha self . _max_priority = max ( self . _max_priority , priority )
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
10022	def update_environment ( self , environment_name , description = None , option_settings = [ ] , tier_type = None , tier_name = None , tier_version = '1.0' ) : out ( "Updating environment: " + str ( environment_name ) ) messages = self . ebs . validate_configuration_settings ( self . app_name , option_settings , environment_name = environment_name ) messages = messages [ 'ValidateConfigurationSettingsResponse' ] [ 'ValidateConfigurationSettingsResult' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( "[" + message [ 'Severity' ] + "] " + str ( environment_name ) + " - '" + message [ 'Namespace' ] + ":" + message [ 'OptionName' ] + "': " + message [ 'Message' ] ) self . ebs . update_environment ( environment_name = environment_name , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
2120	def associate_success_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , * * kwargs )
2789	def snapshot ( self , name ) : return self . get_data ( "volumes/%s/snapshots/" % self . id , type = POST , params = { "name" : name } )
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
4685	def unlock_wallet ( self , * args , * * kwargs ) : self . blockchain . wallet . unlock ( * args , * * kwargs ) return self
4810	def train_model ( best_processed_path , weight_path = '../weight/model_weight.h5' , verbose = 2 ) : x_train_char , x_train_type , y_train = prepare_feature ( best_processed_path , option = 'train' ) x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) validation_set = False if os . path . isdir ( os . path . join ( best_processed_path , 'val' ) ) : validation_set = True x_val_char , x_val_type , y_val = prepare_feature ( best_processed_path , option = 'val' ) if not os . path . isdir ( os . path . dirname ( weight_path ) ) : os . makedirs ( os . path . dirname ( weight_path ) ) # make directory if weight does not exist callbacks_list = [ ReduceLROnPlateau ( ) , ModelCheckpoint ( weight_path , save_best_only = True , save_weights_only = True , monitor = 'val_loss' , mode = 'min' , verbose = 1 ) ] # train model model = get_convo_nn2 ( ) train_params = [ ( 10 , 256 ) , ( 3 , 512 ) , ( 3 , 2048 ) , ( 3 , 4096 ) , ( 3 , 8192 ) ] for ( epochs , batch_size ) in train_params : print ( "train with {} epochs and {} batch size" . format ( epochs , batch_size ) ) if validation_set : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list , validation_data = ( [ x_val_char , x_val_type ] , y_val ) ) else : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list ) return model
13674	def add_directory ( self , * args , * * kwargs ) : exc = kwargs . get ( 'exclusions' , None ) for path in args : self . files . append ( DirectoryPath ( path , self , exclusions = exc ) )
4199	def identify_names ( code ) : finder = NameFinder ( ) finder . visit ( ast . parse ( code ) ) example_code_obj = { } for name , full_name in finder . get_mapping ( ) : # name is as written in file (e.g. np.asarray) # full_name includes resolved import path (e.g. numpy.asarray) module , attribute = full_name . rsplit ( '.' , 1 ) # get shortened module name module_short = get_short_module_name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module_short' : module_short } example_code_obj [ name ] = cobj return example_code_obj
4973	def clean_channel_worker_username ( self ) : channel_worker_username = self . cleaned_data [ 'channel_worker_username' ] . strip ( ) try : User . objects . get ( username = channel_worker_username ) except User . DoesNotExist : raise ValidationError ( ValidationMessages . INVALID_CHANNEL_WORKER . format ( channel_worker_username = channel_worker_username ) ) return channel_worker_username
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] ## set generators qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice ## print progress update 1 to the engine stdout print ( self . _chunksize ) ## set to store rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) ## print progress bar update to engine stdout if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break ## store into database fillsets [ : ] = tmpr del tmpr
2876	def add_bpmn_xml ( self , bpmn , svg = None , filename = None ) : xpath = xpath_eval ( bpmn ) processes = xpath ( './/bpmn:process' ) for process in processes : process_parser = self . PROCESS_PARSER_CLASS ( self , process , svg , filename = filename , doc_xpath = xpath ) if process_parser . get_id ( ) in self . process_parsers : raise ValidationException ( 'Duplicate process ID' , node = process , filename = filename ) if process_parser . get_name ( ) in self . process_parsers_by_name : raise ValidationException ( 'Duplicate process name' , node = process , filename = filename ) self . process_parsers [ process_parser . get_id ( ) ] = process_parser self . process_parsers_by_name [ process_parser . get_name ( ) ] = process_parser
3215	def get_vpc_flow_logs ( vpc , * * conn ) : fl_result = describe_flow_logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) fl_ids = [ ] for fl in fl_result : fl_ids . append ( fl [ "FlowLogId" ] ) return fl_ids
7878	def serialize ( element ) : if getattr ( _THREAD , "serializer" , None ) is None : _THREAD . serializer = XMPPSerializer ( "jabber:client" ) _THREAD . serializer . emit_head ( None , None ) return _THREAD . serializer . emit_stanza ( element )
9807	def teardown ( file ) : # pylint:disable=redefined-builtin config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print_error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
5372	def _file_exists_in_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get ( bucket = bucket_name , object = object_name , projection = 'noAcl' ) try : request . execute ( ) return True except errors . HttpError : return False
11670	def _get_Ks ( self ) : Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
13085	def set ( self , section , key , value ) : if not section in self . config : self . config . add_section ( section ) self . config . set ( section , key , value )
540	def __createModelCheckpoint ( self ) : if self . _model is None or self . _modelCheckpointGUID is None : return # Create an output store, if one doesn't exist already if self . _predictionLogger is None : self . _createPredictionLogger ( ) predictions = StringIO . StringIO ( ) self . _predictionLogger . checkpoint ( checkpointSink = predictions , maxRows = int ( Configuration . get ( 'nupic.model.checkpoint.maxPredictionRows' ) ) ) self . _model . save ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : str ( self . _modelCheckpointGUID ) } , ignoreUnchanged = True ) self . _logger . info ( "Checkpointed Hypersearch Model: modelID: %r, " "checkpointID: %r" , self . _modelID , checkpointID ) return
612	def _getExperimentDescriptionSchema ( ) : installPath = os . path . dirname ( os . path . abspath ( __file__ ) ) schemaFilePath = os . path . join ( installPath , "experimentDescriptionSchema.json" ) return json . loads ( open ( schemaFilePath , 'r' ) . read ( ) )
8270	def _xml ( self ) : grouped = self . _weight_by_hue ( ) xml = "<colors query=\"" + self . name + "\" tags=\"" + ", " . join ( self . tags ) + "\">\n\n" for total_weight , normalized_weight , hue , ranges in grouped : if hue == self . blue : hue = "blue" clr = color ( hue ) xml += "\t<color name=\"" + clr . name + "\" weight=\"" + str ( normalized_weight ) + "\">\n " xml += "\t\t<rgb r=\"" + str ( clr . r ) + "\" g=\"" + str ( clr . g ) + "\" " xml += "b=\"" + str ( clr . b ) + "\" a=\"" + str ( clr . a ) + "\" />\n " for clr , rng , wgt in ranges : xml += "\t\t<shade name=\"" + str ( rng ) + "\" weight=\"" + str ( wgt / total_weight ) + "\" />\n " xml = xml . rstrip ( " " ) + "\t</color>\n\n" xml += "</colors>" return xml
3268	def md_dynamic_default_values_info ( name , node ) : configurations = node . find ( "configurations" ) if configurations is not None : configurations = [ ] for n in node . findall ( "configuration" ) : dimension = n . find ( "dimension" ) dimension = dimension . text if dimension is not None else None policy = n . find ( "policy" ) policy = policy . text if policy is not None else None defaultValueExpression = n . find ( "defaultValueExpression" ) defaultValueExpression = defaultValueExpression . text if defaultValueExpression is not None else None configurations . append ( DynamicDefaultValuesConfiguration ( dimension , policy , defaultValueExpression ) ) return DynamicDefaultValues ( name , configurations )
11807	def encode ( plaintext , code ) : from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
8736	def construct_datetime ( cls , * args , * * kwargs ) : if len ( args ) == 1 : arg = args [ 0 ] method = cls . __get_dt_constructor ( type ( arg ) . __module__ , type ( arg ) . __name__ , ) result = method ( arg ) try : result = result . replace ( tzinfo = kwargs . pop ( 'tzinfo' ) ) except KeyError : pass if kwargs : first_key = kwargs . keys ( ) [ 0 ] tmpl = ( "{first_key} is an invalid keyword " "argument for this function." ) raise TypeError ( tmpl . format ( * * locals ( ) ) ) else : result = datetime . datetime ( * args , * * kwargs ) return result
11724	def init_config ( self , app ) : config_apps = [ 'APP_' , 'RATELIMIT_' ] flask_talisman_debug_mode = [ "'unsafe-inline'" ] for k in dir ( config ) : if any ( [ k . startswith ( prefix ) for prefix in config_apps ] ) : app . config . setdefault ( k , getattr ( config , k ) ) if app . config [ 'DEBUG' ] : app . config . setdefault ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) headers = app . config [ 'APP_DEFAULT_SECURE_HEADERS' ] # ensure `content_security_policy` is not set to {} if headers . get ( 'content_security_policy' ) != { } : headers . setdefault ( 'content_security_policy' , { } ) csp = headers [ 'content_security_policy' ] # ensure `default-src` is not set to [] if csp . get ( 'default-src' ) != [ ] : csp . setdefault ( 'default-src' , [ ] ) # add default `content_security_policy` value when debug csp [ 'default-src' ] += flask_talisman_debug_mode
6427	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) src_comp = bz2 . compress ( src , self . _level ) [ 10 : ] tar_comp = bz2 . compress ( tar , self . _level ) [ 10 : ] concat_comp = bz2 . compress ( src + tar , self . _level ) [ 10 : ] concat_comp2 = bz2 . compress ( tar + src , self . _level ) [ 10 : ] return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
7610	def get_location ( self , location_id : int , timeout : int = None ) : url = self . api . LOCATIONS + '/' + str ( location_id ) return self . _get_model ( url , timeout = timeout )
6763	def dumpload ( self , site = None , role = None ) : r = self . database_renderer ( site = site , role = role ) r . run ( 'pg_dump -c --host={host_string} --username={db_user} ' '--blobs --format=c {db_name} -n public | ' 'pg_restore -U {db_postgresql_postgres_user} --create ' '--dbname={db_name}' )
6111	def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
13538	def get_location ( self , location_id ) : url = "/2/locations/%s" % location_id return self . location_from_json ( self . _get_resource ( url ) [ "location" ] )
1221	def process ( self , tensor ) : for processor in self . preprocessors : tensor = processor . process ( tensor = tensor ) return tensor
2482	def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
2351	def register ( ) : registerDriver ( ISelenium , Selenium , class_implements = [ Firefox , Chrome , Ie , Edge , Opera , Safari , BlackBerry , PhantomJS , Android , Remote , EventFiringWebDriver , ] , )
8320	def parse_categories ( self , markup ) : categories = [ ] m = re . findall ( self . re [ "category" ] , markup ) for category in m : category = category . split ( "|" ) page = category [ 0 ] . strip ( ) display = u"" if len ( category ) > 1 : display = category [ 1 ] . strip ( ) #if not categories.has_key(page): # categories[page] = WikipediaLink(page, u"", display) if not page in categories : categories . append ( page ) return categories
10868	def j2 ( x ) : to_return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to_return [ x == 0 ] = 0 return to_return
13731	def validate_is_not_none ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 # May raise a permission denied self . check_object_permissions ( self . request , obj ) return obj
2565	def async_process ( fn ) : def run ( * args , * * kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
1965	def sys_mmap_pgoff ( self , address , size , prot , flags , fd , offset ) : return self . sys_mmap2 ( address , size , prot , flags , fd , offset )
1366	def validateInterval ( self , startTime , endTime ) : start = int ( startTime ) end = int ( endTime ) if start > end : raise Exception ( "starttime is greater than endtime." )
227	def get_max_median_position_concentration ( positions ) : expos = get_percent_alloc ( positions ) expos = expos . drop ( 'cash' , axis = 1 ) longs = expos . where ( expos . applymap ( lambda x : x > 0 ) ) shorts = expos . where ( expos . applymap ( lambda x : x < 0 ) ) alloc_summary = pd . DataFrame ( ) alloc_summary [ 'max_long' ] = longs . max ( axis = 1 ) alloc_summary [ 'median_long' ] = longs . median ( axis = 1 ) alloc_summary [ 'median_short' ] = shorts . median ( axis = 1 ) alloc_summary [ 'max_short' ] = shorts . min ( axis = 1 ) return alloc_summary
5760	def configure_ci_jobs ( config_url , rosdistro_name , ci_build_name , groovy_script = None , dry_run = False ) : config = get_config_index ( config_url ) build_files = get_ci_build_files ( config , rosdistro_name ) build_file = build_files [ ci_build_name ] index = get_index ( config . rosdistro_index_url ) # get targets targets = [ ] for os_name in build_file . targets . keys ( ) : for os_code_name in build_file . targets [ os_name ] . keys ( ) : for arch in build_file . targets [ os_name ] [ os_code_name ] : targets . append ( ( os_name , os_code_name , arch ) ) print ( 'The build file contains the following targets:' ) for os_name , os_code_name , arch in targets : print ( ' -' , os_name , os_code_name , arch ) dist_file = get_distribution_file ( index , rosdistro_name , build_file ) if not dist_file : print ( 'No distribution file matches the build file' ) return ci_view_name = get_ci_view_name ( rosdistro_name ) # all further configuration will be handled by either the Jenkins API # or by a generated groovy script from ros_buildfarm . jenkins import connect jenkins = connect ( config . jenkins_url ) if groovy_script is None else False view_configs = { } views = { ci_view_name : configure_ci_view ( jenkins , ci_view_name , dry_run = dry_run ) } if not jenkins : view_configs . update ( views ) groovy_data = { 'dry_run' : dry_run , 'expected_num_views' : len ( view_configs ) , } ci_job_names = [ ] job_configs = OrderedDict ( ) is_disabled = False for os_name , os_code_name , arch in targets : try : job_name , job_config = configure_ci_job ( config_url , rosdistro_name , ci_build_name , os_name , os_code_name , arch , config = config , build_file = build_file , index = index , dist_file = dist_file , jenkins = jenkins , views = views , is_disabled = is_disabled , groovy_script = groovy_script , dry_run = dry_run , trigger_timer = build_file . jenkins_job_schedule ) ci_job_names . append ( job_name ) if groovy_script is not None : print ( "Configuration for job '%s'" % job_name ) job_configs [ job_name ] = job_config except JobValidationError as e : print ( e . message , file = sys . stderr ) groovy_data [ 'expected_num_jobs' ] = len ( job_configs ) groovy_data [ 'job_prefixes_and_names' ] = { } if groovy_script is not None : print ( "Writing groovy script '%s' to reconfigure %d jobs" % ( groovy_script , len ( job_configs ) ) ) content = expand_template ( 'snippet/reconfigure_jobs.groovy.em' , groovy_data ) write_groovy_script_and_configs ( groovy_script , content , job_configs , view_configs )
13439	def lock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : return False else : with open ( lockfile , "w" ) : pass return True
1404	def load_configs ( self ) : self . statemgr_config . set_state_locations ( self . configs [ STATEMGRS_KEY ] ) if EXTRA_LINKS_KEY in self . configs : for extra_link in self . configs [ EXTRA_LINKS_KEY ] : self . extra_links . append ( self . validate_extra_link ( extra_link ) )
7754	def process_stanza ( self , stanza ) : self . fix_in_stanza ( stanza ) to_jid = stanza . to_jid if not self . process_all_stanzas and to_jid and ( to_jid != self . me and to_jid . bare ( ) != self . me . bare ( ) ) : return self . route_stanza ( stanza ) try : if isinstance ( stanza , Iq ) : if self . process_iq ( stanza ) : return True elif isinstance ( stanza , Message ) : if self . process_message ( stanza ) : return True elif isinstance ( stanza , Presence ) : if self . process_presence ( stanza ) : return True except ProtocolError , err : typ = stanza . stanza_type if typ != 'error' and ( typ != 'result' or stanza . stanza_type != 'iq' ) : response = stanza . make_error_response ( err . xmpp_name ) self . send ( response ) err . log_reported ( ) else : err . log_ignored ( ) return logger . debug ( "Unhandled %r stanza: %r" % ( stanza . stanza_type , stanza . serialize ( ) ) ) return False
7719	def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) ctxt . xpathRegisterNs ( "muc" , self . ns . getContent ( ) ) ret = ctxt . xpathEval ( to_utf8 ( expr ) ) ctxt . xpathFreeContext ( ) return ret
7597	def get_top_war_clans ( self , country_key = '' , * * params : keys ) : url = self . api . TOP + '/war/' + str ( country_key ) return self . _get_model ( url , PartialClan , * * params )
3371	def get_solver_name ( mip = False , qp = False ) : if len ( solvers ) == 0 : raise SolverNotFound ( "no solvers installed" ) # Those lists need to be updated as optlang implements more solvers mip_order = [ "gurobi" , "cplex" , "glpk" ] lp_order = [ "glpk" , "cplex" , "gurobi" ] qp_order = [ "gurobi" , "cplex" ] if mip is False and qp is False : for solver_name in lp_order : if solver_name in solvers : return solver_name # none of them are in the list order - so return the first one return list ( solvers ) [ 0 ] elif qp : # mip does not yet matter for this determination for solver_name in qp_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no qp-capable solver found" ) else : for solver_name in mip_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no mip-capable solver found" )
10097	def create_new_version ( self , name , subject , text = '' , template_id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES_SPECIFIC_LOCALE_VERSIONS_ENDPOINT % ( template_id , locale ) else : url = self . TEMPLATES_NEW_VERSION_ENDPOINT % template_id return self . _api_request ( url , self . HTTP_POST , payload = payload , timeout = timeout )
10089	def files ( self ) : files_ = super ( Deposit , self ) . files if files_ : sort_by_ = files_ . sort_by def sort_by ( * args , * * kwargs ) : """Only in draft state.""" if 'draft' != self . status : raise PIDInvalidAction ( ) return sort_by_ ( * args , * * kwargs ) files_ . sort_by = sort_by return files_
2981	def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
10639	def extract ( self , other ) : # Extract the specified mass flow rate. if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mfr ( other ) # Extract the specified mass flow rateof the specified compound. elif self . _is_compound_mfr_tuple ( other ) : return self . _extract_compound_mfr ( other [ 0 ] , other [ 1 ] ) # Extract all of the specified compound. elif type ( other ) is str : return self . _extract_compound ( other ) # TODO: Test # Extract all of the compounds of the specified material. elif type ( other ) is Material : return self . _extract_material ( other ) # If not one of the above, it must be an invalid argument. else : raise TypeError ( "Invalid extraction argument." )
2362	def t_heredoc ( self , t ) : t . lexer . is_tabbed = False self . _init_heredoc ( t ) t . lexer . begin ( 'heredoc' )
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , * * kwargs ) : # We just want a smoothed field model of the image so that the residuals # are simply the particles without other complications m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , * * kwargs ) [ 0 ] return pos
11898	def _get_image_from_file ( dir_path , image_file ) : # Save ourselves the effort if PIL is not present, and return None now if not PIL_ENABLED : return None # Put together full path path = os . path . join ( dir_path , image_file ) # Try to read the image img = None try : img = Image . open ( path ) except IOError as exptn : print ( 'Error loading image file %s: %s' % ( path , exptn ) ) # Return image or None return img
3905	def add_conversation_tab ( self , conv_id , switch = False ) : conv_widget = self . get_conv_widget ( conv_id ) self . _tabbed_window . set_tab ( conv_widget , switch = switch , title = conv_widget . title )
11778	def leave1out ( learner , dataset ) : return cross_validation ( learner , dataset , k = len ( dataset . examples ) )
4298	def _convert_config_to_stdin ( config , parser ) : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) args = [ ] for key , val in config . items ( SECTION ) : keyp = '--{0}' . format ( key ) action = parser . _option_string_actions [ keyp ] if action . const : try : if config . getboolean ( SECTION , key ) : args . append ( keyp ) except ValueError : args . extend ( [ keyp , val ] ) # Pass it as is to get the error from ArgumentParser. elif any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : # Some keys with empty values shouldn't be passed into args to use their defaults # from ArgumentParser. if val != '' : args . extend ( [ keyp , val ] ) else : args . extend ( [ keyp , val ] ) return args
3420	def create_mat_dict ( model ) : rxns = model . reactions mets = model . metabolites mat = OrderedDict ( ) mat [ "mets" ] = _cell ( [ met_id for met_id in create_mat_metabolite_id ( model ) ] ) mat [ "metNames" ] = _cell ( mets . list_attr ( "name" ) ) mat [ "metFormulas" ] = _cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ "metCharge" ] = array ( mets . list_attr ( "charge" ) ) * 1. except TypeError : # can't have any None entries for charge, or this will fail pass mat [ "genes" ] = _cell ( model . genes . list_attr ( "id" ) ) # make a matrix for rxnGeneMat # reactions are rows, genes are columns rxn_gene = scipy_sparse . dok_matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn_gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn_gene [ i , model . genes . index ( gene ) ] = 1 mat [ "rxnGeneMat" ] = rxn_gene mat [ "grRules" ] = _cell ( rxns . list_attr ( "gene_reaction_rule" ) ) mat [ "rxns" ] = _cell ( rxns . list_attr ( "id" ) ) mat [ "rxnNames" ] = _cell ( rxns . list_attr ( "name" ) ) mat [ "subSystems" ] = _cell ( rxns . list_attr ( "subsystem" ) ) stoich_mat = create_stoichiometric_matrix ( model ) mat [ "S" ] = stoich_mat if stoich_mat is not None else [ [ ] ] # multiply by 1 to convert to float, working around scipy bug # https://github.com/scipy/scipy/issues/4537 mat [ "lb" ] = array ( rxns . list_attr ( "lower_bound" ) ) * 1. mat [ "ub" ] = array ( rxns . list_attr ( "upper_bound" ) ) * 1. mat [ "b" ] = array ( mets . list_attr ( "_bound" ) ) * 1. mat [ "c" ] = array ( rxns . list_attr ( "objective_coefficient" ) ) * 1. mat [ "rev" ] = array ( rxns . list_attr ( "reversibility" ) ) * 1 mat [ "description" ] = str ( model . id ) return mat
4783	def is_equal_to_ignoring_case ( self , other ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( other , str_types ) : raise TypeError ( 'given arg must be a string' ) if self . val . lower ( ) != other . lower ( ) : self . _err ( 'Expected <%s> to be case-insensitive equal to <%s>, but was not.' % ( self . val , other ) ) return self
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
7218	def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . text
9101	def write_directory ( self , directory : str ) -> bool : current_md5_hash = self . get_namespace_hash ( ) md5_hash_path = os . path . join ( directory , f'{self.module_name}.belns.md5' ) if not os . path . exists ( md5_hash_path ) : old_md5_hash = None else : with open ( md5_hash_path ) as file : old_md5_hash = file . read ( ) . strip ( ) if old_md5_hash == current_md5_hash : return False with open ( os . path . join ( directory , f'{self.module_name}.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = False ) with open ( md5_hash_path , 'w' ) as file : print ( current_md5_hash , file = file ) if self . has_names : with open ( os . path . join ( directory , f'{self.module_name}-names.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = True ) with open ( os . path . join ( directory , f'{self.module_name}.belns.mapping' ) , 'w' ) as file : self . write_bel_namespace_mappings ( file , desc = 'writing mapping' ) return True
7950	def send_element ( self , element ) : with self . lock : if self . _eof or self . _socket is None or not self . _serializer : logger . debug ( "Dropping element: {0}" . format ( element_to_unicode ( element ) ) ) return data = self . _serializer . emit_stanza ( element ) self . _write ( data . encode ( "utf-8" ) )
1543	def get_clusters ( ) : instance = tornado . ioloop . IOLoop . instance ( ) # pylint: disable=unnecessary-lambda try : return instance . run_sync ( lambda : API . get_clusters ( ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
3545	def _update_advertised ( self , advertised ) : # Advertisement data was received, pull out advertised service UUIDs and # name from advertisement data. if 'kCBAdvDataServiceUUIDs' in advertised : self . _advertised = self . _advertised + map ( cbuuid_to_uuid , advertised [ 'kCBAdvDataServiceUUIDs' ] )
11468	def rm ( self , filename ) : try : self . _ftp . delete ( filename ) except error_perm : # target is either a directory # either it does not exist try : current_folder = self . _ftp . pwd ( ) self . cd ( filename ) except error_perm : print ( '550 Delete operation failed %s ' 'does not exist!' % ( filename , ) ) else : self . cd ( current_folder ) print ( '550 Delete operation failed %s ' 'is a folder. Use rmdir function ' 'to delete it.' % ( filename , ) )
3947	def decode ( message , pblite , ignore_first_item = False ) : if not isinstance ( pblite , list ) : logger . warning ( 'Ignoring invalid message: expected list, got %r' , type ( pblite ) ) return if ignore_first_item : pblite = pblite [ 1 : ] # If the last item of the list is a dict, use it as additional field/value # mappings. This seems to be an optimization added for dealing with really # high field numbers. if pblite and isinstance ( pblite [ - 1 ] , dict ) : extra_fields = { int ( field_number ) : value for field_number , value in pblite [ - 1 ] . items ( ) } pblite = pblite [ : - 1 ] else : extra_fields = { } fields_values = itertools . chain ( enumerate ( pblite , start = 1 ) , extra_fields . items ( ) ) for field_number , value in fields_values : if value is None : continue try : field = message . DESCRIPTOR . fields_by_number [ field_number ] except KeyError : # If the tag number is unknown and the value is non-trivial, log a # message to aid reverse-engineering the missing field in the # message. if value not in [ [ ] , '' , 0 ] : logger . debug ( 'Message %r contains unknown field %s with value ' '%r' , message . __class__ . __name__ , field_number , value ) continue if field . label == FieldDescriptor . LABEL_REPEATED : _decode_repeated_field ( message , field , value ) else : _decode_field ( message , field , value )
4134	def get_md5sum ( src_file ) : with open ( src_file , 'r' ) as src_data : src_content = src_data . read ( ) # data needs to be encoded in python3 before hashing if sys . version_info [ 0 ] == 3 : src_content = src_content . encode ( 'utf-8' ) src_md5 = hashlib . md5 ( src_content ) . hexdigest ( ) return src_md5
9756	def update ( ctx , name , description , tags ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment . update_experiment ( user , project_name , _experiment , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment updated." ) get_experiment_details ( response )
7079	def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : service = 'Mast.Tic.Crossmatch' xmatch_input = { 'fields' : [ { 'name' : 'ra' , 'type' : 'float' } , { 'name' : 'dec' , 'type' : 'float' } ] } xmatch_input [ 'data' ] = [ { 'ra' : x , 'dec' : y } for ( x , y ) in zip ( ra , decl ) ] params = { 'raColumn' : 'ra' , 'decColumn' : 'dec' , 'radius' : radius_arcsec / 3600.0 } return mast_query ( service , params , data = xmatch_input , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
1465	def get_command_handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli_help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }
6581	def play ( self , song ) : self . _callbacks . play ( song ) self . _load_track ( song ) time . sleep ( 2 ) # Give the backend time to load the track while True : try : self . _callbacks . pre_poll ( ) self . _ensure_started ( ) self . _loop_hook ( ) readers , _ , _ = select . select ( self . _get_select_readers ( ) , [ ] , [ ] , 1 ) for handle in readers : if handle . fileno ( ) == self . _control_fd : self . _callbacks . input ( handle . readline ( ) . strip ( ) , song ) else : value = self . _read_from_process ( handle ) if self . _player_stopped ( value ) : return finally : self . _callbacks . post_poll ( )
6937	def parallel_update_objectinfo_cplist ( cplist , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : # work around the Darwin segfault after fork if no network activity in # main thread bug: https://bugs.python.org/issue30385#msg293958 if sys . platform == 'darwin' : import requests requests . get ( 'http://captive.apple.com/hotspot-detect.html' ) # handle the start and end indices if ( liststartindex is not None ) and ( maxobjects is None ) : cplist = cplist [ liststartindex : ] elif ( liststartindex is None ) and ( maxobjects is not None ) : cplist = cplist [ : maxobjects ] elif ( liststartindex is not None ) and ( maxobjects is not None ) : cplist = ( cplist [ liststartindex : liststartindex + maxobjects ] ) tasks = [ ( x , { 'fast_mode' : fast_mode , 'findercmap' : findercmap , 'finderconvolve' : finderconvolve , 'deredden_object' : deredden_object , 'custom_bandpasses' : custom_bandpasses , 'gaia_submit_timeout' : gaia_submit_timeout , 'gaia_submit_tries' : gaia_submit_tries , 'gaia_max_timeout' : gaia_max_timeout , 'gaia_mirror' : gaia_mirror , 'complete_query_later' : complete_query_later , 'lclistpkl' : lclistpkl , 'nbrradiusarcsec' : nbrradiusarcsec , 'maxnumneighbors' : maxnumneighbors , 'plotdpi' : plotdpi , 'findercachedir' : findercachedir , 'verbose' : verbose } ) for x in cplist ] resultfutures = [ ] results = [ ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( cp_objectinfo_worker , tasks ) results = [ x for x in resultfutures ] executor . shutdown ( ) return results
5216	def fut_ticker ( gen_ticker : str , dt , freq : str , log = logs . LOG_LEVEL ) -> str : logger = logs . get_logger ( fut_ticker , level = log ) dt = pd . Timestamp ( dt ) t_info = gen_ticker . split ( ) asset = t_info [ - 1 ] if asset in [ 'Index' , 'Curncy' , 'Comdty' ] : ticker = ' ' . join ( t_info [ : - 1 ] ) prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , asset elif asset == 'Equity' : ticker = t_info [ 0 ] prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , ' ' . join ( t_info [ 1 : ] ) else : logger . error ( f'unkonwn asset type for ticker: {gen_ticker}' ) return '' month_ext = 4 if asset == 'Comdty' else 2 months = pd . date_range ( start = dt , periods = max ( idx + month_ext , 3 ) , freq = freq ) logger . debug ( f'pulling expiry dates for months: {months}' ) def to_fut ( month ) : return prefix + const . Futures [ month . strftime ( '%b' ) ] + month . strftime ( '%y' ) [ - 1 ] + ' ' + postfix fut = [ to_fut ( m ) for m in months ] logger . debug ( f'trying futures: {fut}' ) # noinspection PyBroadException try : fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e1 : logger . error ( f'error downloading futures contracts (1st trial) {e1}:\n{fut}' ) # noinspection PyBroadException try : fut = fut [ : - 1 ] logger . debug ( f'trying futures (2nd trial): {fut}' ) fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e2 : logger . error ( f'error downloading futures contracts (2nd trial) {e2}:\n{fut}' ) return '' sub_fut = fut_matu [ pd . DatetimeIndex ( fut_matu . last_tradeable_dt ) > dt ] logger . debug ( f'futures full chain:\n{fut_matu.to_string()}' ) logger . debug ( f'getting index {idx} from:\n{sub_fut.to_string()}' ) return sub_fut . index . values [ idx ]
12572	def put ( self , key , value , attrs = None , format = None , append = False , * * kwargs ) : if not isinstance ( value , np . ndarray ) : super ( NumpyHDFStore , self ) . put ( key , value , format , append , * * kwargs ) else : group = self . get_node ( key ) # remove the node if we are not appending if group is not None and not append : self . _handle . removeNode ( group , recursive = True ) group = None if group is None : paths = key . split ( '/' ) # recursively create the groups path = '/' for p in paths : if not len ( p ) : continue new_path = path if not path . endswith ( '/' ) : new_path += '/' new_path += p group = self . get_node ( new_path ) if group is None : group = self . _handle . createGroup ( path , p ) path = new_path ds_name = kwargs . get ( 'ds_name' , self . _array_dsname ) ds = self . _handle . createArray ( group , ds_name , value ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) self . _handle . flush ( ) return ds
4503	def SPI ( ledtype = None , num = 0 , * * kwargs ) : from . . . project . types . ledtype import make if ledtype is None : raise ValueError ( 'Must provide ledtype value!' ) ledtype = make ( ledtype ) if num == 0 : raise ValueError ( 'Must provide num value >0!' ) if ledtype not in SPI_DRIVERS . keys ( ) : raise ValueError ( '{} is not a valid LED type.' . format ( ledtype ) ) return SPI_DRIVERS [ ledtype ] ( num , * * kwargs )
13879	def AppendToFile ( filename , contents , eol_style = EOL_STYLE_NATIVE , encoding = None , binary = False ) : _AssertIsLocal ( filename ) assert isinstance ( contents , six . text_type ) ^ binary , 'Must always receive unicode contents, unless binary=True' if not binary : # Replaces eol on each line by the given eol_style. contents = _HandleContentsEol ( contents , eol_style ) # Handle encoding here, and always write in binary mode. We can't use io.open because it # tries to do its own line ending handling. contents = contents . encode ( encoding or sys . getfilesystemencoding ( ) ) oss = open ( filename , 'ab' ) try : oss . write ( contents ) finally : oss . close ( )
792	def jobCancel ( self , jobID ) : self . _logger . info ( 'Canceling jobID=%s' , jobID ) # NOTE: jobSetFields does retries on transient mysql failures self . jobSetFields ( jobID , { "cancel" : True } , useConnectionID = False )
11456	def load_config ( from_key , to_key ) : from . mappings import mappings kbs = { } for key , values in mappings [ 'config' ] . iteritems ( ) : parse_dict = { } for mapping in values : # {'inspire': 'Norwegian', 'cds': 'nno'} # -> {"Norwegian": "nno"} parse_dict [ mapping [ from_key ] ] = mapping [ to_key ] kbs [ key ] = parse_dict return kbs
11680	def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT_RDWR ) self . sock . close ( ) self . state = DISCONNECTED
13591	def n_p ( self ) : return 2 * _sltr . GeV2joule ( self . E ) * _spc . epsilon_0 / ( self . beta * _spc . elementary_charge ) ** 2
5840	def check_predict_status ( self , view_id , predict_request_id ) : failure_message = "Get status on predict failed" bare_response = self . _get_success_json ( self . _get ( 'v1/data_views/' + str ( view_id ) + '/predict/' + str ( predict_request_id ) + '/status' , None , failure_message = failure_message ) ) result = bare_response [ "data" ] # result.update({"message": bare_response["message"]}) return result
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
3251	def save ( self , obj , content_type = "application/xml" ) : rest_url = obj . href data = obj . message ( ) headers = { "Content-type" : content_type , "Accept" : content_type } logger . debug ( "{} {}" . format ( obj . save_method , obj . href ) ) resp = self . http_request ( rest_url , method = obj . save_method . lower ( ) , data = data , headers = headers ) if resp . status_code not in ( 200 , 201 ) : raise FailedRequestError ( 'Failed to save to Geoserver catalog: {}, {}' . format ( resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp
11626	def build ( self , pre = None , shortest = False ) : res = super ( Q , self ) . build ( pre , shortest = shortest ) if self . escape : return repr ( res ) elif self . html_js_escape : return ( "'" + res . encode ( "string_escape" ) . replace ( "<" , "\\x3c" ) . replace ( ">" , "\\x3e" ) + "'" ) else : return "" . join ( [ self . quote , res , self . quote ] )
8207	def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
8892	def get_default ( self ) : if self . has_default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
12734	def set_body_states ( self , states ) : for state in states : self . get_body ( state . name ) . state = state
6399	def encode ( self , word ) : word = unicode_normalize ( 'NFC' , text_type ( word . upper ( ) ) ) for i , j in self . _substitutions : word = word . replace ( i , j ) word = word . translate ( self . _trans ) return '' . join ( c for c in self . _delete_consecutive_repeats ( word ) if c in self . _uc_set )
8615	def wait_for_completion ( self , response , timeout = 3600 , initial_wait = 5 , scaleup = 10 ) : if not response : return logger = logging . getLogger ( __name__ ) wait_period = initial_wait next_increase = time . time ( ) + wait_period * scaleup if timeout : timeout = time . time ( ) + timeout while True : request = self . get_request ( request_id = response [ 'requestId' ] , status = True ) if request [ 'metadata' ] [ 'status' ] == 'DONE' : break elif request [ 'metadata' ] [ 'status' ] == 'FAILED' : raise PBFailedRequest ( 'Request {0} failed to complete: {1}' . format ( response [ 'requestId' ] , request [ 'metadata' ] [ 'message' ] ) , response [ 'requestId' ] ) current_time = time . time ( ) if timeout and current_time > timeout : raise PBTimeoutError ( 'Timed out waiting for request {0}.' . format ( response [ 'requestId' ] ) , response [ 'requestId' ] ) if current_time > next_increase : wait_period *= 2 next_increase = time . time ( ) + wait_period * scaleup scaleup *= 2 logger . info ( "Request %s is in state '%s'. Sleeping for %i seconds..." , response [ 'requestId' ] , request [ 'metadata' ] [ 'status' ] , wait_period ) time . sleep ( wait_period )
4532	def clone ( self ) : args = { k : getattr ( self , k ) for k in self . CLONE_ATTRS } args [ 'color_list' ] = copy . copy ( self . color_list ) return self . __class__ ( [ ] , * * args )
1050	def format_exception ( etype , value , tb , limit = None ) : if tb : list = [ 'Traceback (most recent call last):\n' ] list = list + format_tb ( tb , limit ) else : list = [ ] list = list + format_exception_only ( etype , value ) return list
1380	def get_version_number ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : for line in release_info : trunks = line [ : - 1 ] . split ( ' ' ) if trunks [ 0 ] == 'heron.build.version' : return trunks [ - 1 ] . replace ( "'" , "" ) return 'unknown'
3846	def parse_typing_status_message ( p ) : return TypingStatusMessage ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , timestamp = from_timestamp ( p . timestamp ) , status = p . type , )
3971	def _composed_app_dict ( app_name , assembled_specs , port_specs ) : logging . info ( "Compose Compiler: Compiling dict for app {}" . format ( app_name ) ) app_spec = assembled_specs [ 'apps' ] [ app_name ] compose_dict = app_spec [ "compose" ] _apply_env_overrides ( env_overrides_for_app_or_service ( app_name ) , compose_dict ) if 'image' in app_spec and 'build' in app_spec : raise RuntimeError ( "image and build are both specified in the spec for {}" . format ( app_name ) ) elif 'image' in app_spec : logging . info compose_dict [ 'image' ] = app_spec [ 'image' ] elif 'build' in app_spec : compose_dict [ 'build' ] = _get_build_path ( app_spec ) else : raise RuntimeError ( "Neither image nor build was specified in the spec for {}" . format ( app_name ) ) compose_dict [ 'entrypoint' ] = [ ] compose_dict [ 'command' ] = _compile_docker_command ( app_spec ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( app_name ) logging . info ( "Compose Compiler: compiled command {}" . format ( compose_dict [ 'command' ] ) ) compose_dict [ 'links' ] = _links_for_app ( app_spec , assembled_specs ) logging . info ( "Compose Compiler: links {}" . format ( compose_dict [ 'links' ] ) ) compose_dict [ 'volumes' ] = compose_dict [ 'volumes' ] + _get_compose_volumes ( app_name , assembled_specs ) logging . info ( "Compose Compiler: volumes {}" . format ( compose_dict [ 'volumes' ] ) ) port_list = _get_ports_list ( app_name , port_specs ) if port_list : compose_dict [ 'ports' ] = port_list logging . info ( "Compose Compiler: ports {}" . format ( port_list ) ) compose_dict [ 'user' ] = 'root' return compose_dict
9314	def _format_datetime ( dttm ) : if dttm . tzinfo is None or dttm . tzinfo . utcoffset ( dttm ) is None : # dttm is timezone-naive; assume UTC zoned = pytz . utc . localize ( dttm ) else : zoned = dttm . astimezone ( pytz . utc ) ts = zoned . strftime ( "%Y-%m-%dT%H:%M:%S" ) ms = zoned . strftime ( "%f" ) precision = getattr ( dttm , "precision" , None ) if precision == "second" : pass # Already precise to the second elif precision == "millisecond" : ts = ts + "." + ms [ : 3 ] elif zoned . microsecond > 0 : ts = ts + "." + ms . rstrip ( "0" ) return ts + "Z"
9568	def get_chat_id ( self , message ) : if message . chat . type == 'private' : return message . user . id return message . chat . id
9025	def kivy_svg ( self ) : from kivy . graphics . svg import Svg path = self . temporary_path ( ".svg" ) try : return Svg ( path ) finally : remove_file ( path )
6826	def clone ( self , remote_url , path = None , use_sudo = False , user = None ) : cmd = 'git clone --quiet %s' % remote_url if path is not None : cmd = cmd + ' %s' % path if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
4994	def unlink_inactive_learners ( channel_code , channel_pk ) : start = time . time ( ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Processing learners to unlink inactive users using configuration: [%s]' , integrated_channel ) # Note: learner data transmission code paths don't raise any uncaught exception, so we don't need a broad # try-except block here. integrated_channel . unlink_inactive_learners ( ) duration = time . time ( ) - start LOGGER . info ( 'Unlink inactive learners task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
4811	def evaluate ( best_processed_path , model ) : x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) y_predict = model . predict ( [ x_test_char , x_test_type ] ) y_predict = ( y_predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1_score ( y_test , y_predict ) precision = precision_score ( y_test , y_predict ) recall = recall_score ( y_test , y_predict ) return f1score , precision , recall
1353	def make_error_response ( self , message ) : response = self . make_response ( constants . RESPONSE_STATUS_FAILURE ) response [ constants . RESPONSE_KEY_MESSAGE ] = message return response
11650	def fit ( self , X , y = None ) : if is_integer ( X ) : dim = X else : X = as_features ( X ) dim = X . dim M = self . smoothness # figure out the smooth-enough elements of our basis inds = np . mgrid [ ( slice ( M + 1 ) , ) * dim ] . reshape ( dim , ( M + 1 ) ** dim ) . T self . inds_ = inds [ ( inds ** 2 ) . sum ( axis = 1 ) <= M ** 2 ] return self
5325	def measure_memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj_id = id ( obj ) if obj_id in seen : return 0 # Important mark as seen *before* entering recursion to gracefully handle # self-referential objects seen . add ( obj_id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure_memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure_memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , '__dict__' ) : size += cls . measure_memory ( obj . __dict__ , seen ) elif hasattr ( obj , '__iter__' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure_memory ( i , seen ) for i in obj ] ) return size
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
2418	def write_package ( package , out ) : out . write ( '# Package\n\n' ) write_value ( 'PackageName' , package . name , out ) if package . has_optional_field ( 'version' ) : write_value ( 'PackageVersion' , package . version , out ) write_value ( 'PackageDownloadLocation' , package . download_location , out ) if package . has_optional_field ( 'summary' ) : write_text_value ( 'PackageSummary' , package . summary , out ) if package . has_optional_field ( 'source_info' ) : write_text_value ( 'PackageSourceInfo' , package . source_info , out ) if package . has_optional_field ( 'file_name' ) : write_value ( 'PackageFileName' , package . file_name , out ) if package . has_optional_field ( 'supplier' ) : write_value ( 'PackageSupplier' , package . supplier , out ) if package . has_optional_field ( 'originator' ) : write_value ( 'PackageOriginator' , package . originator , out ) if package . has_optional_field ( 'check_sum' ) : write_value ( 'PackageChecksum' , package . check_sum . to_tv ( ) , out ) write_value ( 'PackageVerificationCode' , format_verif_code ( package ) , out ) if package . has_optional_field ( 'description' ) : write_text_value ( 'PackageDescription' , package . description , out ) if isinstance ( package . license_declared , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseDeclared' , u'({0})' . format ( package . license_declared ) , out ) else : write_value ( 'PackageLicenseDeclared' , package . license_declared , out ) if isinstance ( package . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseConcluded' , u'({0})' . format ( package . conc_lics ) , out ) else : write_value ( 'PackageLicenseConcluded' , package . conc_lics , out ) # Write sorted list of licenses. for lics in sorted ( package . licenses_from_files ) : write_value ( 'PackageLicenseInfoFromFiles' , lics , out ) if package . has_optional_field ( 'license_comment' ) : write_text_value ( 'PackageLicenseComments' , package . license_comment , out ) # cr_text is either free form text or NONE or NOASSERTION. if isinstance ( package . cr_text , six . string_types ) : write_text_value ( 'PackageCopyrightText' , package . cr_text , out ) else : write_value ( 'PackageCopyrightText' , package . cr_text , out ) if package . has_optional_field ( 'homepage' ) : write_value ( 'PackageHomePage' , package . homepage , out ) # Write sorted files. for spdx_file in sorted ( package . files ) : write_separators ( out ) write_file ( spdx_file , out )
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
10974	def new ( ) : form = GroupForm ( request . form ) if form . validate_on_submit ( ) : try : group = Group . create ( admins = [ current_user ] , * * form . data ) flash ( _ ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) except IntegrityError : flash ( _ ( 'Group creation failure' ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , )
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
13236	def to_timezone ( self , dt ) : if timezone . is_aware ( dt ) : return dt . astimezone ( self . timezone ) else : return timezone . make_aware ( dt , self . timezone )
1757	def write_int ( self , where , expression , size = None , force = False ) : if size is None : size = self . address_bit_size assert size in SANE_SIZES self . _publish ( 'will_write_memory' , where , expression , size ) data = [ Operators . CHR ( Operators . EXTRACT ( expression , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] self . _memory . write ( where , data , force ) self . _publish ( 'did_write_memory' , where , expression , size )
1096	def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
6488	def _get_filter_field ( field_name , field_value ) : filter_field = None if isinstance ( field_value , ValueRange ) : range_values = { } if field_value . lower : range_values . update ( { "gte" : field_value . lower_string } ) if field_value . upper : range_values . update ( { "lte" : field_value . upper_string } ) filter_field = { "range" : { field_name : range_values } } elif _is_iterable ( field_value ) : filter_field = { "terms" : { field_name : field_value } } else : filter_field = { "term" : { field_name : field_value } } return filter_field
8186	def draw ( self , dx = 0 , dy = 0 , weighted = False , directed = False , highlight = [ ] , traffic = None ) : self . update ( ) # Draw the graph background. s = self . styles . default s . graph_background ( s ) # Center the graph on the canvas. _ctx . push ( ) _ctx . translate ( self . x + dx , self . y + dy ) # Indicate betweenness centrality. if traffic : if isinstance ( traffic , bool ) : traffic = 5 for n in self . nodes_by_betweenness ( ) [ : traffic ] : try : s = self . styles [ n . style ] except : s = self . styles . default if s . graph_traffic : s . graph_traffic ( s , n , self . alpha ) # Draw the edges and their labels. s = self . styles . default if s . edges : s . edges ( s , self . edges , self . alpha , weighted , directed ) # Draw each node in the graph. # Apply individual style to each node (or default). for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node : s . node ( s , n , self . alpha ) # Highlight the given shortest path. try : s = self . styles . highlight except : s = self . styles . default if s . path : s . path ( s , self , highlight ) # Draw node id's as labels on each node. for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node_label : s . node_label ( s , n , self . alpha ) # Events for clicked and dragged nodes. # Nodes will resist being dragged by attraction and repulsion, # put the event listener on top to get more direct feedback. #self.events.update() _ctx . pop ( )
6160	def eye_plot ( x , L , S = 0 ) : plt . figure ( figsize = ( 6 , 4 ) ) idx = np . arange ( 0 , L + 1 ) plt . plot ( idx , x [ S : S + L + 1 ] , 'b' ) k_max = int ( ( len ( x ) - S ) / L ) - 1 for k in range ( 1 , k_max ) : plt . plot ( idx , x [ S + k * L : S + L + 1 + k * L ] , 'b' ) plt . grid ( ) plt . xlabel ( 'Time Index - n' ) plt . ylabel ( 'Amplitude' ) plt . title ( 'Eye Plot' ) return 0
12465	def read_config ( filename , args ) : # Initial vars config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) # Append download-cache for old pip versions if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) # Expand user and environ vars in config filename is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) # Read config if it exists on disk if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None # Apply config for each possible section for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) # Make auto convert here for integers and boolean values for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value # Update config with default values if necessary for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) # Update bootstrap config from parsed args keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
7734	def nfkc ( data ) : if isinstance ( data , list ) : data = u"" . join ( data ) return unicodedata . normalize ( "NFKC" , data )
12587	def all_childnodes_to_nifti1img ( h5group ) : child_nodes = [ ] def append_parent_if_dataset ( name , obj ) : if isinstance ( obj , h5py . Dataset ) : if name . split ( '/' ) [ - 1 ] == 'data' : child_nodes . append ( obj . parent ) vols = [ ] h5group . visititems ( append_parent_if_dataset ) for c in child_nodes : vols . append ( hdfgroup_to_nifti1image ( c ) ) return vols
6267	def set_time ( self , value : float ) : if value < 0 : value = 0 self . offset += self . get_time ( ) - value
7611	def get_top_clans ( self , location_id = 'global' , * * params : keys ) : url = self . api . LOCATIONS + '/' + str ( location_id ) + '/rankings/clans' return self . _get_model ( url , PartialClan , * * params )
5665	def evaluate_earliest_arrival_time_at_target ( self , dep_time , transfer_margin ) : minimum = dep_time + self . _walk_to_target_duration dep_time_plus_transfer_margin = dep_time + transfer_margin for label in self . _labels : if label . departure_time >= dep_time_plus_transfer_margin and label . arrival_time_target < minimum : minimum = label . arrival_time_target return float ( minimum )
3551	def list_descriptors ( self ) : paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , 'Descriptors' ) return map ( BluezGattDescriptor , get_provider ( ) . _get_objects_by_path ( paths ) )
11785	def attrnum ( self , attr ) : if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
10223	def get_regulatory_pairs ( graph : BELGraph ) -> Set [ NodePair ] : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_DECREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( ( u , v ) ) return results
5623	def path_exists ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : urlopen ( path ) . info ( ) return True except HTTPError as e : if e . code == 404 : return False else : raise elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return True else : return False else : logger . debug ( "%s exists: %s" , path , os . path . exists ( path ) ) return os . path . exists ( path )
6413	def ghmean ( nums ) : m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_h , 12 ) != round ( m_g , 12 ) : m_g , m_h = ( m_g * m_h ) ** ( 1 / 2 ) , ( 2 * m_g * m_h ) / ( m_g + m_h ) return m_g
13395	def _update_settings ( self , new_settings , enforce_helpstring = True ) : for raw_setting_name , value in six . iteritems ( new_settings ) : setting_name = raw_setting_name . replace ( "_" , "-" ) setting_already_exists = setting_name in self . _instance_settings value_is_list_len_2 = isinstance ( value , list ) and len ( value ) == 2 treat_as_tuple = not setting_already_exists and value_is_list_len_2 if isinstance ( value , tuple ) or treat_as_tuple : self . _instance_settings [ setting_name ] = value else : if setting_name not in self . _instance_settings : if enforce_helpstring : msg = "You must specify param '%s' as a tuple of (helpstring, value)" raise InternalCashewException ( msg % setting_name ) else : # Create entry with blank helpstring. self . _instance_settings [ setting_name ] = ( '' , value , ) else : # Save inherited helpstring, replace default value. orig = self . _instance_settings [ setting_name ] self . _instance_settings [ setting_name ] = ( orig [ 0 ] , value , )
9218	def _smixins ( self , name ) : return ( self . _mixins [ name ] if name in self . _mixins else False )
5528	def open ( config , mode = "continue" , zoom = None , bounds = None , single_input_file = None , with_cache = False , debug = False ) : return Mapchete ( MapcheteConfig ( config , mode = mode , zoom = zoom , bounds = bounds , single_input_file = single_input_file , debug = debug ) , with_cache = with_cache )
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : # already first/last return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : # This is the common part of all message types payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } # Depending on the message type add the appropriate fields if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
7568	def comp ( seq ) : ## makes base to its small complement then makes upper return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
3423	def resettable ( f ) : def wrapper ( self , new_value ) : context = get_context ( self ) if context : old_value = getattr ( self , f . __name__ ) # Don't clutter the context with unchanged variables if old_value == new_value : return context ( partial ( f , self , old_value ) ) f ( self , new_value ) return wrapper
7273	def set_rate ( self , rate ) : self . _rate = self . _player_interface_property ( 'Rate' , dbus . Double ( rate ) ) return self . _rate
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) # Chop off initial '@' alias = self . _field [ 1 : ] self . _alias = alias return self
2287	def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , * * kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , * * kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , * * kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
7640	def convert_jams ( jams_file , output_prefix , csv = False , comment_char = '#' , namespaces = None ) : if namespaces is None : raise ValueError ( 'No namespaces provided. Try ".*" for all namespaces.' ) jam = jams . load ( jams_file ) # Get all the annotations # Filter down to the unique ones # For each annotation # generate the comment string # generate the output filename # dump to csv # Make a counter object for each namespace type counter = collections . Counter ( ) annotations = [ ] for query in namespaces : annotations . extend ( jam . search ( namespace = query ) ) if csv : suffix = 'csv' sep = ',' else : suffix = 'lab' sep = '\t' for ann in annotations : index = counter [ ann . namespace ] counter [ ann . namespace ] += 1 filename = os . path . extsep . join ( [ get_output_name ( output_prefix , ann . namespace , index ) , suffix ] ) comment = get_comments ( jam , ann ) # Dump to disk lab_dump ( ann , comment , filename , sep , comment_char )
4438	async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
13344	def mean ( a , axis = None , dtype = None , out = None , keepdims = False ) : if ( isinstance ( a , np . ndarray ) or isinstance ( a , RemoteArray ) or isinstance ( a , DistArray ) ) : return a . mean ( axis = axis , dtype = dtype , out = out , keepdims = keepdims ) else : return np . mean ( a , axis = axis , dtype = dtype , out = out , keepdims = keepdims )
1568	def invoke_hook_bolt_ack ( self , heron_tuple , process_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_ack_info = BoltAckInfo ( heron_tuple = heron_tuple , acking_task_id = self . get_task_id ( ) , process_latency_ms = process_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_ack ( bolt_ack_info )
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : # Register the BP blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) # Finally, attach the blueprints to its parent processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
6869	def normalize_magseries ( times , mags , mingap = 4.0 , normto = 'globalmedian' , magsarefluxes = False , debugmode = False ) : ngroups , timegroups = find_lc_timegroups ( times , mingap = mingap ) # find all the non-nan indices finite_ind = np . isfinite ( mags ) if any ( finite_ind ) : # find the global median global_mag_median = np . median ( mags [ finite_ind ] ) # go through the groups and normalize them to the median for # each group for tgind , tg in enumerate ( timegroups ) : finite_ind = np . isfinite ( mags [ tg ] ) # find this timegroup's median mag and normalize the mags in # it to this median group_median = np . median ( ( mags [ tg ] ) [ finite_ind ] ) if magsarefluxes : mags [ tg ] = mags [ tg ] / group_median else : mags [ tg ] = mags [ tg ] - group_median if debugmode : LOGDEBUG ( 'group %s: elems %s, ' 'finite elems %s, median mag %s' % ( tgind , len ( mags [ tg ] ) , len ( finite_ind ) , group_median ) ) # now that everything is normalized to 0.0, add the global median # offset back to all the mags and write the result back to the dict if isinstance ( normto , str ) and normto == 'globalmedian' : if magsarefluxes : mags = mags * global_mag_median else : mags = mags + global_mag_median # if the normto is a float, add everything to that float and return elif isinstance ( normto , float ) : if magsarefluxes : mags = mags * normto else : mags = mags + normto # anything else just returns the normalized mags as usual return times , mags else : LOGERROR ( 'measurements are all nan!' ) return None , None
11614	def export_posterior_probability ( self , filename , title = "Posterior Probability" ) : self . probability . save ( h5file = filename , title = title )
4805	def _err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global _soft_err _soft_err . append ( out ) return self else : raise AssertionError ( out )
4506	def find_serial_devices ( self ) : if self . devices is not None : return self . devices self . devices = { } hardware_id = "(?i)" + self . hardware_id # forces case insensitive for ports in serial . tools . list_ports . grep ( hardware_id ) : port = ports [ 0 ] try : id = self . get_device_id ( port ) ver = self . _get_device_version ( port ) except : log . debug ( 'Error getting device_id for %s, %s' , port , self . baudrate ) if True : raise continue if getattr ( ports , '__len__' , lambda : 0 ) ( ) : log . debug ( 'Multi-port device %s:%s:%s with %s ports found' , self . hardware_id , id , ver , len ( ports ) ) if id < 0 : log . debug ( 'Serial device %s:%s:%s with id %s < 0' , self . hardware_id , id , ver ) else : self . devices [ id ] = port , ver return self . devices
4386	def router_function ( fn ) : # type: (Callable) -> Callable @ wraps ( fn ) def wrapper ( * args , * * kwargs ) : # type: (Any, Any) -> Callable if platform_is_windows ( ) : # pragma: no cover, skipt Windows test raise RuntimeError ( "Router interface is not available on Win32 systems.\n" "Configure AMS routes using the TwinCAT router service." ) return fn ( * args , * * kwargs ) return wrapper
197	def Fog ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return CloudLayer ( intensity_mean = ( 220 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.5 ) , intensity_coarse_scale = 2 , alpha_min = ( 0.7 , 0.9 ) , alpha_multiplier = 0.3 , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 4.0 , - 2.0 ) , sparsity = 0.9 , density_multiplier = ( 0.4 , 0.9 ) , name = name , deterministic = deterministic , random_state = random_state )
8659	def filter_by ( zips = _zips , * * kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
1241	def _next_position_then_increment ( self ) : start = self . _capacity - 1 position = start + self . _position self . _position = ( self . _position + 1 ) % self . _capacity return position
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
13776	def get_abs_and_rel_paths ( self , root_path , file_name , input_dir ) : # todo: change relative path resolving [bug on duplicate dir names in path] relative_dir = root_path . replace ( input_dir , '' ) return os . path . join ( root_path , file_name ) , relative_dir + '/' + file_name
13200	def format_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . title is None : return None output_text = convert_lsstdoc_tex ( self . title , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
11754	def parse_definite_clause ( s ) : assert is_definite_clause ( s ) if is_symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
4644	def get ( self , key , default = None ) : if key in self : return self . __getitem__ ( key ) else : return default
1417	def _get_execution_state_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_execution_state_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) # pylint: disable=unused-variable, unused-argument @ self . client . DataWatch ( path ) def watch_execution_state ( data , stats ) : """ invoke callback to watch execute state """ if data : executionState = ExecutionState ( ) executionState . ParseFromString ( data ) callback ( executionState ) else : callback ( None ) # Returning False will result in no future watches # being triggered. If isWatching is True, then # the future watches will be triggered. return isWatching
912	def write ( self , proto ) : super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) proto . fieldNames = self . _fieldNames proto . fieldTypes = self . _fieldTypes if self . _predictedField : proto . predictedField = self . _predictedField proto . predictionSteps = self . _predictionSteps
10525	def get_google_playlist ( self , playlist ) : logger . info ( "Loading playlist {0}" . format ( playlist ) ) for google_playlist in self . api . get_all_user_playlist_contents ( ) : if google_playlist [ 'name' ] == playlist or google_playlist [ 'id' ] == playlist : return google_playlist else : logger . warning ( "Playlist {0} does not exist." . format ( playlist ) ) return { }
752	def setResultsPerChoice ( self , resultsPerChoice ) : # Keep track of the results obtained for each choice. self . _resultsPerChoice = [ [ ] ] * len ( self . choices ) for ( choiceValue , values ) in resultsPerChoice : choiceIndex = self . choices . index ( choiceValue ) self . _resultsPerChoice [ choiceIndex ] = list ( values )
13286	def get_dataframe_from_variable ( nc , data_var ) : time_var = nc . get_variables_by_attributes ( standard_name = 'time' ) [ 0 ] depth_vars = nc . get_variables_by_attributes ( axis = lambda v : v is not None and v . lower ( ) == 'z' ) depth_vars += nc . get_variables_by_attributes ( standard_name = lambda v : v in [ 'height' , 'depth' 'surface_altitude' ] , positive = lambda x : x is not None ) # Find the correct depth variable depth_var = None for d in depth_vars : try : if d . _name in data_var . coordinates . split ( " " ) or d . _name in data_var . dimensions : depth_var = d break except AttributeError : continue times = netCDF4 . num2date ( time_var [ : ] , units = time_var . units , calendar = getattr ( time_var , 'calendar' , 'standard' ) ) original_times_size = times . size if depth_var is None and hasattr ( data_var , 'sensor_depth' ) : depth_type = get_type ( data_var . sensor_depth ) depths = np . asarray ( [ data_var . sensor_depth ] * len ( times ) ) . flatten ( ) values = data_var [ : ] . flatten ( ) elif depth_var is None : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) depth_type = get_type ( depths ) values = data_var [ : ] . flatten ( ) else : depths = depth_var [ : ] depth_type = get_type ( depths ) if len ( data_var . shape ) > 1 : times = np . repeat ( times , depths . size ) depths = np . tile ( depths , original_times_size ) values = data_var [ : , : ] . flatten ( ) else : values = data_var [ : ] . flatten ( ) if getattr ( depth_var , 'positive' , 'down' ) . lower ( ) == 'up' : logger . warning ( "Converting depths to positive down before returning the DataFrame" ) depths = depths * - 1 # https://github.com/numpy/numpy/issues/4595 # We can't call astype on a MaskedConstant if ( isinstance ( depths , np . ma . core . MaskedConstant ) or ( hasattr ( depths , 'mask' ) and depths . mask . all ( ) ) ) : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) df = pd . DataFrame ( { 'time' : times , 'value' : values . astype ( data_var . dtype ) , 'unit' : data_var . units if hasattr ( data_var , 'units' ) else np . nan , 'depth' : depths . astype ( depth_type ) } ) df . set_index ( [ pd . DatetimeIndex ( df [ 'time' ] ) , pd . Float64Index ( df [ 'depth' ] ) ] , inplace = True ) return df
13569	def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , * * kwargs ) return inner
3728	def Zc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ COMBINED ] ) : def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Zc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Zc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Zc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Zc' ] ) : methods . append ( PSRK ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Zc' ] ) : methods . append ( YAWS ) if Tc ( CASRN ) and Vc ( CASRN ) and Pc ( CASRN ) : methods . append ( COMBINED ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == IUPAC : _Zc = float ( _crit_IUPAC . at [ CASRN , 'Zc' ] ) elif Method == PSRK : _Zc = float ( _crit_PSRKR4 . at [ CASRN , 'Zc' ] ) elif Method == MATTHEWS : _Zc = float ( _crit_Matthews . at [ CASRN , 'Zc' ] ) elif Method == CRC : _Zc = float ( _crit_CRC . at [ CASRN , 'Zc' ] ) elif Method == YAWS : _Zc = float ( _crit_Yaws . at [ CASRN , 'Zc' ] ) elif Method == COMBINED : _Zc = Vc ( CASRN ) * Pc ( CASRN ) / Tc ( CASRN ) / R elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Zc
265	def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )
9937	def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : # check if storage location exists for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
8744	def get_floatingip ( context , id , fields = None ) : LOG . info ( 'get_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . FLOATING , '_deallocated' : False } floating_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , * * filters ) if not floating_ip : raise q_exc . FloatingIpNotFound ( id = id ) return v . _make_floating_ip_dict ( floating_ip )
8807	def _make_job_dict ( job ) : body = { "id" : job . get ( 'id' ) , "action" : job . get ( 'action' ) , "completed" : job . get ( 'completed' ) , "tenant_id" : job . get ( 'tenant_id' ) , "created_at" : job . get ( 'created_at' ) , "transaction_id" : job . get ( 'transaction_id' ) , "parent_id" : job . get ( 'parent_id' , None ) } if not body [ 'transaction_id' ] : body [ 'transaction_id' ] = job . get ( 'id' ) completed = 0 for sub in job . subtransactions : if sub . get ( 'completed' ) : completed += 1 pct = 100 if job . get ( 'completed' ) else 0 if len ( job . subtransactions ) > 0 : pct = float ( completed ) / len ( job . subtransactions ) * 100.0 body [ 'transaction_percent' ] = int ( pct ) body [ 'completed_subtransactions' ] = completed body [ 'subtransactions' ] = len ( job . subtransactions ) return body
6166	def my_psd ( x , NFFT = 2 ** 10 , Fs = 1 ) : Px , f = pylab . mlab . psd ( x , NFFT , Fs ) return Px . flatten ( ) , f
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
10505	def stopEventLoop ( ) : stopper = PyObjCAppHelperRunLoopStopper_wrap . currentRunLoopStopper ( ) if stopper is None : if NSApp ( ) is not None : NSApp ( ) . terminate_ ( None ) return True return False NSTimer . scheduledTimerWithTimeInterval_target_selector_userInfo_repeats_ ( 0.0 , stopper , 'performStop:' , None , False ) return True
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
11374	def license_is_oa ( license ) : for oal in OA_LICENSES : if re . search ( oal , license ) : return True return False
9133	def get_data_dir ( module_name : str ) -> str : module_name = module_name . lower ( ) data_dir = os . path . join ( BIO2BEL_DIR , module_name ) os . makedirs ( data_dir , exist_ok = True ) return data_dir
3789	def property_derivative_T ( self , T , P , zs , ws , order = 1 ) : sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_T ( T , P , zs , ws , method , order ) except : pass return None
13166	def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
789	def jobInfo ( self , jobID ) : row = self . _getOneMatchingRowWithRetries ( self . _jobs , dict ( job_id = jobID ) , [ self . _jobs . pubToDBNameDict [ n ] for n in self . _jobs . jobInfoNamedTuple . _fields ] ) if row is None : raise RuntimeError ( "jobID=%s not found within the jobs table" % ( jobID ) ) # Create a namedtuple with the names to values return self . _jobs . jobInfoNamedTuple . _make ( row )
7710	def request_roster ( self , version = None ) : processor = self . stanza_processor request = Iq ( stanza_type = "get" ) request . set_payload ( RosterPayload ( version = version ) ) processor . set_response_handlers ( request , self . _get_success , self . _get_error ) processor . send ( request )
2545	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True doc . reviews [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
9907	def send_confirmation ( self ) : confirmation = EmailConfirmation . objects . create ( email = self ) confirmation . send ( )
11818	def expected_utility ( a , s , U , mdp ) : return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
5517	def limit ( self , value ) : self . _limit = value self . _start = None self . _sum = 0
5151	def merge_config ( template , config , list_identifiers = None ) : result = template . copy ( ) for key , value in config . items ( ) : if isinstance ( value , dict ) : node = result . get ( key , OrderedDict ( ) ) result [ key ] = merge_config ( node , value ) elif isinstance ( value , list ) and isinstance ( result . get ( key ) , list ) : result [ key ] = merge_list ( result [ key ] , value , list_identifiers ) else : result [ key ] = value return result
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
3254	def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) # DELETE /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules/<granule_id>.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) # maybe return a list of all granules? return None
2263	def dict_take ( dict_ , keys , default = util_const . NoParam ) : if default is util_const . NoParam : for key in keys : yield dict_ [ key ] else : for key in keys : yield dict_ . get ( key , default )
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
7078	def tic_conesearch ( ra , decl , radius_arcmin = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 10.0 , refresh = 5.0 , maxtimeout = 90.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'ra' : ra , 'dec' : decl , 'radius' : radius_arcmin / 60.0 } service = 'Mast.Catalogs.Tic.Cone' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
2194	def encoding ( self ) : if self . redirect is not None : return self . redirect . encoding else : return super ( TeeStringIO , self ) . encoding
13489	def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
8505	def _default_value_only ( self ) : line = self . source [ self . col_offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(['"][^)]+?['"], ?(.*?)\)''' ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )
9474	def BFS ( self , root = None ) : if not root : root = self . root ( ) queue = deque ( ) queue . append ( root ) nodes = [ ] while len ( queue ) > 0 : x = queue . popleft ( ) nodes . append ( x ) for child in x . children ( ) : queue . append ( child ) return nodes
1255	def setup_scaffold ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) if self . graph_summary is None : ready_op = tf . report_uninitialized_variables ( var_list = global_variables ) ready_for_local_init_op = None local_init_op = None else : ready_op = None ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) local_init_op = self . graph_summary else : # Global and local variable initializers. global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] local_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) ready_op = tf . report_uninitialized_variables ( var_list = ( global_variables + local_variables ) ) ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) if self . graph_summary is None : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , # Synchronize values of trainable variables. * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) else : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , self . graph_summary , # Synchronize values of trainable variables. * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) def init_fn ( scaffold , session ) : if self . saver_spec is not None and self . saver_spec . get ( 'load' , True ) : directory = self . saver_spec [ 'directory' ] file = self . saver_spec . get ( 'file' ) if file is None : file = tf . train . latest_checkpoint ( checkpoint_dir = directory , latest_filename = None # Corresponds to argument of saver.save() in Model.save(). ) elif not os . path . isfile ( file ) : file = os . path . join ( directory , file ) if file is not None : try : scaffold . saver . restore ( sess = session , save_path = file ) session . run ( fetches = self . list_buffer_index_reset_op ) except tf . errors . NotFoundError : raise TensorForceError ( "Error: Existing checkpoint could not be loaded! Set \"load\" to false in saver_spec." ) # TensorFlow scaffold object # TODO explain what it does. self . scaffold = tf . train . Scaffold ( init_op = init_op , init_feed_dict = None , init_fn = init_fn , ready_op = ready_op , ready_for_local_init_op = ready_for_local_init_op , local_init_op = local_init_op , summary_op = None , saver = self . saver , copy_from_scaffold = None )
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
10328	def rank_causalr_hypothesis ( graph , node_to_regulation , regulator_node ) : upregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } downregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } targets = [ node for node in node_to_regulation if node != regulator_node ] predicted_regulations = run_cna ( graph , regulator_node , targets ) # + signed hypothesis for _ , target_node , predicted_regulation in predicted_regulations : if ( predicted_regulation is Effect . inhibition or predicted_regulation is Effect . activation ) and ( predicted_regulation . value == node_to_regulation [ target_node ] ) : upregulation_hypothesis [ 'correct' ] += 1 downregulation_hypothesis [ 'incorrect' ] += 1 elif predicted_regulation is Effect . ambiguous : upregulation_hypothesis [ 'ambiguous' ] += 1 downregulation_hypothesis [ 'ambiguous' ] += 1 elif predicted_regulation is Effect . no_effect : continue else : downregulation_hypothesis [ 'correct' ] += 1 upregulation_hypothesis [ 'incorrect' ] += 1 upregulation_hypothesis [ 'score' ] = upregulation_hypothesis [ 'correct' ] - upregulation_hypothesis [ 'incorrect' ] downregulation_hypothesis [ 'score' ] = downregulation_hypothesis [ 'correct' ] - downregulation_hypothesis [ 'incorrect' ] return upregulation_hypothesis , downregulation_hypothesis
1982	def sync ( f ) : def new_function ( self , * args , * * kw ) : self . _lock . acquire ( ) try : return f ( self , * args , * * kw ) finally : self . _lock . release ( ) return new_function
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
12434	def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : # A SAFE request is allowed to redirect using a 301 response . status = http . client . MOVED_PERMANENTLY else : # All other requests must use a 307 response . status = http . client . TEMPORARY_REDIRECT else : # Modern redirects are allowed. Let's have some fun. # Hopefully you're client supports this. # The RFC explicitly discourages UserAgent sniffing. response . status = http . client . PERMANENT_REDIRECT # Terminate the connection. response . close ( )
7429	def draw ( self , axes ) : ## create a toytree object from the treemix tree result tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) ## get coords for admix in self . results . admixture : ## parse admix event pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) ## add line for admixture edge mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) ## add points at admixture sink axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) ## add scale bar for edge lengths axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
2914	def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( * * self . parent . data )
12048	def determineProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) #it should be in the first 5k of the file f . close ( ) protoComment = "unknown" if b"SWHLab4[" in raw : protoComment = raw . split ( b"SWHLab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : protoComment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : protoComment = "?" if not type ( protoComment ) is str : protoComment = protoComment . decode ( "utf-8" ) return protoComment
11857	def settings ( request ) : settings = Setting . objects . all ( ) . as_dict ( default = '' ) context = { 'SETTINGS' : settings , } return context
464	def clear_all_placeholder_variables ( printable = True ) : tl . logging . info ( 'clear all .....................................' ) gl = globals ( ) . copy ( ) for var in gl : if var [ 0 ] == '_' : continue if 'func' in str ( globals ( ) [ var ] ) : continue if 'module' in str ( globals ( ) [ var ] ) : continue if 'class' in str ( globals ( ) [ var ] ) : continue if printable : tl . logging . info ( " clear_all ------- %s" % str ( globals ( ) [ var ] ) ) del globals ( ) [ var ]
11496	def get_user_by_email ( self , email ) : parameters = dict ( ) parameters [ 'email' ] = email response = self . request ( 'midas.user.get' , parameters ) return response
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
12662	def load_mask ( image , allow_empty = True ) : img = check_img ( image , make_it_3d = True ) values = np . unique ( img . get_data ( ) ) if len ( values ) == 1 : # We accept a single value if it is not 0 (full true mask). if values [ 0 ] == 0 and not allow_empty : raise ValueError ( 'Given mask is invalid because it masks all data' ) elif len ( values ) == 2 : # If there are 2 different values, one of them must be 0 (background) if 0 not in values : raise ValueError ( 'Background of the mask must be represented with 0.' ' Given mask contains: {}.' . format ( values ) ) elif len ( values ) != 2 : # If there are more than 2 values, the mask is invalid raise ValueError ( 'Given mask is not made of 2 values: {}. ' 'Cannot interpret as true or false' . format ( values ) ) return nib . Nifti1Image ( as_ndarray ( get_img_data ( img ) , dtype = bool ) , img . get_affine ( ) , img . get_header ( ) )
5283	def get_success_url ( self ) : if self . success_url : url = self . success_url else : # Default to returning to the same page url = self . request . get_full_path ( ) return url
12845	def _relay_message ( self , message ) : info ( "relaying message: {message}" ) if not message . was_sent_by ( self . _id_factory ) : self . pipe . send ( message ) self . pipe . deliver ( )
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
412	def save_model ( self , network = None , model_name = 'model' , * * kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) # put project_name into kwargs params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
11927	def run_server ( self , port ) : try : self . server = MultiThreadedHTTPServer ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : # failed to bind port logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve_forever ( ) except KeyboardInterrupt : logger . info ( "^C received, shutting down server" ) self . shutdown_server ( )
7574	def get_threaded_view ( ipyclient , split = True ) : ## engine ids ## e.g., [0, 1, 2, 3, 4, 5, 6, 7, 8] eids = ipyclient . ids ## get host names ## e.g., ['a', 'a', 'b', 'b', 'a', 'c', 'c', 'c', 'c'] dview = ipyclient . direct_view ( ) hosts = dview . apply_sync ( socket . gethostname ) ## group ids into a dict by their hostnames ## e.g., {a: [0, 1, 4], b: [2, 3], c: [5, 6, 7, 8]} hostdict = defaultdict ( list ) for host , eid in zip ( hosts , eids ) : hostdict [ host ] . append ( eid ) ## Now split threads on the same host into separate proc if there are many hostdictkeys = hostdict . keys ( ) for key in hostdictkeys : gids = hostdict [ key ] maxt = len ( gids ) if len ( gids ) >= 4 : maxt = 2 ## if 4 nodes and 4 ppn, put one sample per host if ( len ( gids ) == 4 ) and ( len ( hosts ) >= 4 ) : maxt = 4 if len ( gids ) >= 6 : maxt = 3 if len ( gids ) >= 8 : maxt = 4 if len ( gids ) >= 16 : maxt = 4 ## split ids into groups of maxt threaded = [ gids [ i : i + maxt ] for i in xrange ( 0 , len ( gids ) , maxt ) ] lth = len ( threaded ) ## if anything was split (lth>1) update hostdict with new proc if lth > 1 : hostdict . pop ( key ) for hostid in range ( lth ) : hostdict [ str ( key ) + "_" + str ( hostid ) ] = threaded [ hostid ] ## make sure split numbering is correct #threaded = hostdict.values() #assert len(ipyclient.ids) <= len(list(itertools.chain(*threaded))) LOGGER . info ( "threaded_view: %s" , dict ( hostdict ) ) return hostdict
7226	def paint ( self ) : # TODO Figure out why i cant use some of these props snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , #'line-cap': self.cap, #'line-join': self.join, 'line-width' : VectorStyle . get_style_value ( self . width ) , #'line-gap-width': self.gap_width, #'line-blur': self.blur, } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
10900	def check_consistency ( self ) : error = False regex = re . compile ( '([a-zA-Z_][a-zA-Z0-9_]*)' ) # there at least must be the full model, not necessarily partial updates if 'full' not in self . modelstr : raise ModelError ( 'Model must contain a `full` key describing ' 'the entire image formation' ) # Check that the two model descriptors are consistent for name , eq in iteritems ( self . modelstr ) : var = regex . findall ( eq ) for v in var : # remove the derivative signs if there (dP -> P) v = re . sub ( r"^d" , '' , v ) if v not in self . varmap : log . error ( "Variable '%s' (eq. '%s': '%s') not found in category map %r" % ( v , name , eq , self . varmap ) ) error = True if error : raise ModelError ( 'Inconsistent varmap and modelstr descriptions' )
2134	def _get_schema ( self , wfjt_id ) : node_res = get_resource ( 'node' ) node_results = node_res . list ( workflow_job_template = wfjt_id , all_pages = True ) [ 'results' ] return self . _workflow_node_structure ( node_results )
9519	def interleave ( infile_1 , infile_2 , outfile , suffix1 = None , suffix2 = None ) : seq_reader_1 = sequences . file_reader ( infile_1 ) seq_reader_2 = sequences . file_reader ( infile_2 ) f_out = utils . open_file_write ( outfile ) for seq_1 in seq_reader_1 : try : seq_2 = next ( seq_reader_2 ) except : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_1 . id , ' ... cannot continue' ) if suffix1 is not None and not seq_1 . id . endswith ( suffix1 ) : seq_1 . id += suffix1 if suffix2 is not None and not seq_2 . id . endswith ( suffix2 ) : seq_2 . id += suffix2 print ( seq_1 , file = f_out ) print ( seq_2 , file = f_out ) try : seq_2 = next ( seq_reader_2 ) except : seq_2 = None if seq_2 is not None : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_2 . id , ' ... cannot continue' ) utils . close ( f_out )
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line # Word index, which word to insert on (cycles between 1->len(words)) wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : # Reached the end?, try starting at the front again. wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : # There are no spaces to expand, just prepend one. line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 # Don't push a single word all the way to the right. if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
1012	def _cleanUpdatesList ( self , col , cellIdx , seg ) : # TODO: check if the situation described in the docstring above actually # occurs. for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c == col and i == cellIdx : for update in updateList : if update [ 1 ] . segment == seg : self . _removeSegmentUpdate ( update )
11750	def _register_blueprint ( self , app , bp , bundle_path , child_path , description ) : base_path = sanitize_path ( self . _journey_path + bundle_path + child_path ) app . register_blueprint ( bp , url_prefix = base_path ) return { 'name' : bp . name , 'path' : child_path , 'import_name' : bp . import_name , 'description' : description , 'routes' : self . get_blueprint_routes ( app , base_path ) }
10632	def get_compound_mfr ( self , compound ) : if compound in self . material . compounds : return self . _compound_mfrs [ self . material . get_compound_index ( compound ) ] else : return 0.0
4383	def is_denied ( self , role , method , resource ) : return ( role , method , resource ) in self . _denied
13477	def NonUniformImage ( x , y , z , ax = None , fig = None , cmap = None , alpha = None , scalex = True , scaley = True , add_cbar = True , * * kwargs ) : if ax is None and fig is None : fig , ax = _setup_axes ( ) elif ax is None : ax = fig . gca ( ) elif fig is None : fig = ax . get_figure ( ) norm = kwargs . get ( 'norm' , None ) im = _mplim . NonUniformImage ( ax , * * kwargs ) vmin = kwargs . pop ( 'vmin' , _np . min ( z ) ) vmax = kwargs . pop ( 'vmax' , _np . max ( z ) ) # im.set_clim(vmin=vmin, vmax=vmax) if cmap is not None : im . set_cmap ( cmap ) m = _cm . ScalarMappable ( cmap = im . get_cmap ( ) , norm = norm ) m . set_array ( z ) if add_cbar : cax , cb = _cb ( ax = ax , im = m , fig = fig ) if alpha is not None : im . set_alpha ( alpha ) im . set_data ( x , y , z ) ax . images . append ( im ) if scalex : xmin = min ( x ) xmax = max ( x ) ax . set_xlim ( xmin , xmax ) if scaley : ymin = min ( y ) ymax = max ( y ) ax . set_ylim ( ymin , ymax ) return _SI ( im = im , cb = cb , cax = cax )
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
7971	def _run_timeout_threads ( self , handler ) : # pylint: disable-msg=W0212 for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue thread = TimeoutThread ( method , daemon = self . daemon , exc_queue = self . exc_queue ) self . timeout_threads . append ( thread ) thread . start ( )
10652	def prepare_to_run ( self , clock , period_count ) : for c in self . components : c . prepare_to_run ( clock , period_count ) for a in self . activities : a . prepare_to_run ( clock , period_count )
259	def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )
5973	def generate_submit_array ( templates , directories , * * kwargs ) : dirname = kwargs . setdefault ( 'dirname' , os . path . curdir ) reldirs = [ relpath ( p , start = dirname ) for p in asiterable ( directories ) ] missing = [ p for p in ( os . path . join ( dirname , subdir ) for subdir in reldirs ) if not os . path . exists ( p ) ] if len ( missing ) > 0 : logger . debug ( "template=%(template)r: dirname=%(dirname)r reldirs=%(reldirs)r" , vars ( ) ) logger . error ( "Some directories are not accessible from the array script: " "%(missing)r" , vars ( ) ) def write_script ( template ) : qsystem = detect_queuing_system ( template ) if qsystem is None or not qsystem . has_arrays ( ) : logger . warning ( "Not known how to make a job array for %(template)r; skipping..." , vars ( ) ) return None kwargs [ 'jobarray_string' ] = qsystem . array ( reldirs ) return generate_submit_scripts ( template , * * kwargs ) [ 0 ] # returns list of length 1 # must use config.get_templates() because we need to access the file for detecting return [ write_script ( template ) for template in config . get_templates ( templates ) ]
13748	def get_counter ( self , name , start = 0 ) : item = self . get_item ( hash_key = name , start = start ) counter = Counter ( dynamo_item = item , pool = self ) return counter
286	def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax
11222	def is_compressed_json_file ( abspath ) : abspath = abspath . lower ( ) fname , ext = os . path . splitext ( abspath ) if ext in [ ".json" , ".js" ] : is_compressed = False elif ext == ".gz" : is_compressed = True else : raise ValueError ( "'%s' is not a valid json file. " "extension has to be '.json' or '.js' for uncompressed, '.gz' " "for compressed." % abspath ) return is_compressed
2394	def calc_list_average ( l ) : total = 0.0 for value in l : total += value return total / len ( l )
8714	def file_format ( self ) : log . info ( 'Formating, can take minutes depending on flash size...' ) res = self . __exchange ( 'file.format()' , timeout = 300 ) if 'format done' not in res : log . error ( res ) else : log . info ( res ) return res
491	def close ( self ) : self . _logger . info ( "Closing" ) if self . _pool is not None : self . _pool . close ( ) self . _pool = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
7955	def getpeercert ( self ) : with self . lock : if not self . _socket or self . _tls_state != "connected" : raise ValueError ( "Not TLS-connected" ) return get_certificate_from_ssl_socket ( self . _socket )
174	def draw_points_on_image ( self , image , color = ( 0 , 128 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : from . kps import KeypointsOnImage kpsoi = KeypointsOnImage . from_xy_array ( self . coords , shape = image . shape ) image = kpsoi . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = copy , raise_if_out_of_image = raise_if_out_of_image ) return image
414	def delete_model ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Model . delete_many ( kwargs ) logging . info ( "[Database] Delete Model SUCCESS" )
8002	def __from_xml ( self , xmlnode ) : self . __logger . debug ( "Converting jabber:iq:register element from XML" ) if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:iq:register element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != REGISTER_NS or xmlnode . name != "query" : raise ValueError ( "XML node is not a jabber:iq:register element" ) for element in xml_element_iter ( xmlnode . children ) : ns = get_node_ns_uri ( element ) if ns == DATAFORM_NS and element . name == "x" and not self . form : self . form = Form ( element ) elif ns != REGISTER_NS : continue name = element . name if name == "instructions" and not self . instructions : self . instructions = from_utf8 ( element . getContent ( ) ) elif name == "registered" : self . registered = True elif name == "remove" : self . remove = True elif name in legacy_fields and not getattr ( self , name ) : value = from_utf8 ( element . getContent ( ) ) if value is None : value = u"" self . __logger . debug ( u"Setting legacy field %r to %r" % ( name , value ) ) setattr ( self , name , value )
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
5722	def _restore_resources ( resources ) : resources = deepcopy ( resources ) for resource in resources : schema = resource [ 'schema' ] for fk in schema . get ( 'foreignKeys' , [ ] ) : _ , name = _restore_path ( fk [ 'reference' ] [ 'resource' ] ) fk [ 'reference' ] [ 'resource' ] = name return resources
2299	def predict_features ( self , df_features , df_target , nh = 20 , idx = 0 , dropout = 0. , activation_function = th . nn . ReLU , lr = 0.01 , l1 = 0.1 , batch_size = - 1 , train_epochs = 1000 , test_epochs = 1000 , device = None , verbose = None , nb_runs = 3 ) : device , verbose = SETTINGS . get_default ( ( 'device' , device ) , ( 'verbose' , verbose ) ) x = th . FloatTensor ( scale ( df_features . values ) ) . to ( device ) y = th . FloatTensor ( scale ( df_target . values ) ) . to ( device ) out = [ ] for i in range ( nb_runs ) : model = FSGNN_model ( [ x . size ( ) [ 1 ] + 1 , nh , 1 ] , dropout = dropout , activation_function = activation_function ) . to ( device ) out . append ( model . train ( x , y , lr = 0.01 , l1 = 0.1 , batch_size = - 1 , train_epochs = train_epochs , test_epochs = test_epochs , device = device , verbose = verbose ) ) return list ( np . mean ( np . array ( out ) , axis = 0 ) )
4838	def get_course_and_course_run ( self , course_run_id ) : # Parse the course ID from the course run ID. course_id = parse_course_key ( course_run_id ) # Retrieve the course metadata from the catalog service. course = self . get_course_details ( course_id ) course_run = None if course : # Find the specified course run. course_run = None course_runs = [ course_run for course_run in course [ 'course_runs' ] if course_run [ 'key' ] == course_run_id ] if course_runs : course_run = course_runs [ 0 ] return course , course_run
10958	def update_from_model_change ( self , oldmodel , newmodel , tile ) : self . _loglikelihood -= self . _calc_loglikelihood ( oldmodel , tile = tile ) self . _loglikelihood += self . _calc_loglikelihood ( newmodel , tile = tile ) self . _residuals [ tile . slicer ] = self . _data [ tile . slicer ] - newmodel
1326	def from_keras ( cls , model , bounds , input_shape = None , channel_axis = 3 , preprocessing = ( 0 , 1 ) ) : import tensorflow as tf if input_shape is None : try : input_shape = model . input_shape [ 1 : ] except AttributeError : raise ValueError ( 'Please specify input_shape manually or ' 'provide a model with an input_shape attribute' ) with tf . keras . backend . get_session ( ) . as_default ( ) : inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) logits = model ( inputs ) return cls ( inputs , logits , bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing )
640	def set ( cls , prop , value ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) cls . _properties [ prop ] = str ( value )
10253	def remove_highlight_edges ( graph : BELGraph , edges = None ) : for u , v , k , _ in graph . edges ( keys = True , data = True ) if edges is None else edges : if is_edge_highlighted ( graph , u , v , k ) : del graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ]
1085	def timetz ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
9357	def sentences ( quantity = 2 , as_list = False ) : result = [ sntc . strip ( ) for sntc in random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ] if as_list : return result else : return ' ' . join ( result )
4147	def DaniellPeriodogram ( data , P , NFFT = None , detrend = 'mean' , sampling = 1. , scale_by_freq = True , window = 'hamming' ) : psd = speriodogram ( data , NFFT = NFFT , detrend = detrend , sampling = sampling , scale_by_freq = scale_by_freq , window = window ) if len ( psd ) % 2 == 1 : datatype = 'real' else : datatype = 'complex' N = len ( psd ) _slice = 2 * P + 1 if datatype == 'real' : #must get odd value newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 0 : newN = psd . size / _slice else : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 1 : newN = psd . size / _slice newpsd = np . zeros ( int ( newN ) ) # keep integer division for i in range ( 0 , newpsd . size ) : count = 0 #needed to know the number of valid averaged values for n in range ( i * _slice - P , i * _slice + P + 1 ) : #+1 to have P values on each sides if n > 0 and n < N : #needed to start the average count += 1 newpsd [ i ] += psd [ n ] newpsd [ i ] /= float ( count ) #todo: check this if datatype == 'complex' : freq = np . linspace ( 0 , sampling , len ( newpsd ) ) else : df = 1. / sampling freq = np . linspace ( 0 , sampling / 2. , len ( newpsd ) ) #psd.refreq(2*psd.size()/A.freq()); #psd.retime(-1./psd.freq()+1./A.size()); return newpsd , freq
5723	def _buffer_incomplete_responses ( raw_output , buf ) : if raw_output : if buf : # concatenate buffer and new output raw_output = b"" . join ( [ buf , raw_output ] ) buf = None if b"\n" not in raw_output : # newline was not found, so assume output is incomplete and store in buffer buf = raw_output raw_output = None elif not raw_output . endswith ( b"\n" ) : # raw output doesn't end in a newline, so store everything after the last newline (if anything) # in the buffer, and parse everything before it remainder_offset = raw_output . rindex ( b"\n" ) + 1 buf = raw_output [ remainder_offset : ] raw_output = raw_output [ : remainder_offset ] return ( raw_output , buf )
10961	def create_img ( ) : # 1. particles + coverslip rad = 0.5 * np . random . randn ( POS . shape [ 0 ] ) + 4.5 # 4.5 +- 0.5 px particles part = objs . PlatonicSpheresCollection ( POS , rad , zscale = 0.89 ) slab = objs . Slab ( zpos = 4.92 , angles = ( - 4.7e-3 , - 7.3e-4 ) ) objects = comp . ComponentCollection ( [ part , slab ] , category = 'obj' ) # 2. psf, ilm p = exactpsf . FixedSSChebLinePSF ( kfki = 1.07 , zslab = - 29.3 , alpha = 1.17 , n2n1 = 0.98 , sigkf = - 0.33 , zscale = 0.89 , laser_wavelength = 0.45 ) i = ilms . BarnesStreakLegPoly2P1D ( npts = ( 16 , 10 , 8 , 4 ) , zorder = 8 ) b = ilms . LegendrePoly2P1D ( order = ( 7 , 2 , 2 ) , category = 'bkg' ) off = comp . GlobalScalar ( name = 'offset' , value = - 2.11 ) mdl = models . ConfocalImageModel ( ) st = states . ImageState ( util . NullImage ( shape = [ 48 , 64 , 64 ] ) , [ objects , p , i , b , off ] , mdl = mdl , model_as_data = True ) b . update ( b . params , BKGVALS ) i . update ( i . params , ILMVALS ) im = st . model + np . random . randn ( * st . model . shape ) * 0.03 return util . Image ( im )
9333	def full_like ( array , value , dtype = None ) : shared = empty_like ( array , dtype ) shared [ : ] = value return shared
7897	def get_room_jid ( self , nick = None ) : if nick is None : return self . room_jid return JID ( self . room_jid . node , self . room_jid . domain , nick )
2222	def _rectify_base ( base ) : if base is NoParam or base == 'default' : return DEFAULT_ALPHABET elif base in [ 26 , 'abc' , 'alpha' ] : return _ALPHABET_26 elif base in [ 16 , 'hex' ] : return _ALPHABET_16 elif base in [ 10 , 'dec' ] : return _ALPHABET_10 else : if not isinstance ( base , ( list , tuple ) ) : raise TypeError ( 'Argument `base` must be a key, list, or tuple; not {}' . format ( type ( base ) ) ) return base
2864	def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
5585	def output_is_valid ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : return ( is_numpy_or_masked_array ( process_data ) or is_numpy_or_masked_array_with_tags ( process_data ) ) elif self . METADATA [ "data_type" ] == "vector" : return is_feature_list ( process_data )
3320	def refresh ( self , token , timeout ) : assert token in self . _dict , "Lock must exist" assert timeout == - 1 or timeout > 0 if timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX self . _lock . acquire_write ( ) try : # Note: shelve dictionary returns copies, so we must reassign # values: lock = self . _dict [ token ] lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout self . _dict [ token ] = lock self . _flush ( ) finally : self . _lock . release ( ) return lock
1991	def ls ( self , glob_str ) : path = os . path . join ( self . uri , glob_str ) return [ os . path . split ( s ) [ 1 ] for s in glob . glob ( path ) ]
567	def __validateExperimentControl ( self , control ) : # Validate task list taskList = control . get ( 'tasks' , None ) if taskList is not None : taskLabelsList = [ ] for task in taskList : validateOpfJsonValue ( task , "opfTaskSchema.json" ) validateOpfJsonValue ( task [ 'taskControl' ] , "opfTaskControlSchema.json" ) taskLabel = task [ 'taskLabel' ] assert isinstance ( taskLabel , types . StringTypes ) , "taskLabel type: %r" % type ( taskLabel ) assert len ( taskLabel ) > 0 , "empty string taskLabel not is allowed" taskLabelsList . append ( taskLabel . lower ( ) ) taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , taskLabelsList ) assert len ( taskLabelDuplicates ) == 0 , "Duplcate task labels are not allowed: %s" % taskLabelDuplicates return
7742	def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( self . _unprepared_pending ) : self . _configure_io_handler ( handler ) self . check_events ( )
12772	def follow_markers ( self , start = 0 , end = 1e100 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( self . markers ) : if frame_no < start : continue if frame_no >= end : break for states in self . _step_to_marker_frame ( frame_no ) : yield states
5378	def _build_pipeline_docker_command ( cls , script_name , inputs , outputs , envs ) : # We upload the user script as an environment argument # and write it to SCRIPT_DIR (preserving its local file name). # # The docker_command: # * writes the script body to a file # * installs gcloud if there are recursive copies to do # * sets environment variables for inputs with wildcards # * sets environment variables for recursive input directories # * recursively copies input directories # * creates output directories # * sets environment variables for recursive output directories # * sets the DATA_ROOT environment variable to /mnt/data # * sets the working directory to ${DATA_ROOT} # * executes the user script # * recursively copies output directories recursive_input_dirs = [ var for var in inputs if var . recursive and var . value ] recursive_output_dirs = [ var for var in outputs if var . recursive and var . value ] install_cloud_sdk = '' if recursive_input_dirs or recursive_output_dirs : install_cloud_sdk = INSTALL_CLOUD_SDK export_input_dirs = '' copy_input_dirs = '' if recursive_input_dirs : export_input_dirs = providers_util . build_recursive_localize_env ( providers_util . DATA_MOUNT_POINT , inputs ) copy_input_dirs = providers_util . build_recursive_localize_command ( providers_util . DATA_MOUNT_POINT , inputs , job_model . P_GCS ) export_output_dirs = '' copy_output_dirs = '' if recursive_output_dirs : export_output_dirs = providers_util . build_recursive_gcs_delocalize_env ( providers_util . DATA_MOUNT_POINT , outputs ) copy_output_dirs = providers_util . build_recursive_delocalize_command ( providers_util . DATA_MOUNT_POINT , outputs , job_model . P_GCS ) docker_paths = [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in outputs if var . value ] mkdirs = '\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers_util . DATA_MOUNT_POINT , path ) for path in docker_paths ] ) inputs_with_wildcards = [ var for var in inputs if not var . recursive and var . docker_path and '*' in os . path . basename ( var . docker_path ) ] export_inputs_with_wildcards = '\n' . join ( [ 'export {0}="{1}/{2}"' . format ( var . name , providers_util . DATA_MOUNT_POINT , var . docker_path ) for var in inputs_with_wildcards ] ) export_empty_envs = '\n' . join ( [ 'export {0}=""' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER_COMMAND . format ( mk_runtime_dirs = MK_RUNTIME_DIRS_COMMAND , script_path = '%s/%s' % ( providers_util . SCRIPT_DIR , script_name ) , install_cloud_sdk = install_cloud_sdk , export_inputs_with_wildcards = export_inputs_with_wildcards , export_input_dirs = export_input_dirs , copy_input_dirs = copy_input_dirs , mk_output_dirs = mkdirs , export_output_dirs = export_output_dirs , export_empty_envs = export_empty_envs , tmpdir = providers_util . TMP_DIR , working_dir = providers_util . WORKING_DIR , copy_output_dirs = copy_output_dirs )
1846	def JS ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . SF , target . read ( ) , cpu . PC )
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
216	def setdefault ( self , key : str , value : str ) -> str : set_key = key . lower ( ) . encode ( "latin-1" ) set_value = value . encode ( "latin-1" ) for idx , ( item_key , item_value ) in enumerate ( self . _list ) : if item_key == set_key : return item_value . decode ( "latin-1" ) self . _list . append ( ( set_key , set_value ) ) return value
1708	def connect ( command , data = None , env = None , cwd = None ) : # TODO: support piped commands command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
9791	def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
13033	def write_index_translation ( translation_filename , entity_ids , relation_ids ) : translation = triple_pb . Translation ( ) entities = [ ] for name , index in entity_ids . items ( ) : translation . entities . add ( element = name , index = index ) relations = [ ] for name , index in relation_ids . items ( ) : translation . relations . add ( element = name , index = index ) with open ( translation_filename , "wb" ) as f : f . write ( translation . SerializeToString ( ) )
11834	def connect ( self , A , B , distance = 1 ) : self . connect1 ( A , B , distance ) if not self . directed : self . connect1 ( B , A , distance )
9053	def posteriori_mean ( self ) : from numpy_sugar . linalg import rsolve Sigma = self . posteriori_covariance ( ) eta = self . _ep . _posterior . eta return dot ( Sigma , eta + rsolve ( GLMM . covariance ( self ) , self . mean ( ) ) )
4867	def to_representation ( self , instance ) : updated_course = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( updated_course [ 'key' ] ) for course_run in updated_course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_course
7215	def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ]
13162	def raw_sql ( cls , cur , query : str , values : tuple ) : yield from cur . execute ( query , values ) return ( yield from cur . fetchall ( ) )
477	def moses_multi_bleu ( hypotheses , references , lowercase = False ) : if np . size ( hypotheses ) == 0 : return np . float32 ( 0.0 ) # Get MOSES multi-bleu script try : multi_bleu_path , _ = urllib . request . urlretrieve ( "https://raw.githubusercontent.com/moses-smt/mosesdecoder/" "master/scripts/generic/multi-bleu.perl" ) os . chmod ( multi_bleu_path , 0o755 ) except Exception : # pylint: disable=W0702 tl . logging . info ( "Unable to fetch multi-bleu.perl script, using local." ) metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) bin_dir = os . path . abspath ( os . path . join ( metrics_dir , ".." , ".." , "bin" ) ) multi_bleu_path = os . path . join ( bin_dir , "tools/multi-bleu.perl" ) # Dump hypotheses and references to tempfiles hypothesis_file = tempfile . NamedTemporaryFile ( ) hypothesis_file . write ( "\n" . join ( hypotheses ) . encode ( "utf-8" ) ) hypothesis_file . write ( b"\n" ) hypothesis_file . flush ( ) reference_file = tempfile . NamedTemporaryFile ( ) reference_file . write ( "\n" . join ( references ) . encode ( "utf-8" ) ) reference_file . write ( b"\n" ) reference_file . flush ( ) # Calculate BLEU using multi-bleu script with open ( hypothesis_file . name , "r" ) as read_pred : bleu_cmd = [ multi_bleu_path ] if lowercase : bleu_cmd += [ "-lc" ] bleu_cmd += [ reference_file . name ] try : bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) bleu_out = bleu_out . decode ( "utf-8" ) bleu_score = re . search ( r"BLEU = (.+?)," , bleu_out ) . group ( 1 ) bleu_score = float ( bleu_score ) except subprocess . CalledProcessError as error : if error . output is not None : tl . logging . warning ( "multi-bleu.perl script returned non-zero exit code" ) tl . logging . warning ( error . output ) bleu_score = np . float32 ( 0.0 ) # Close temp files hypothesis_file . close ( ) reference_file . close ( ) return np . float32 ( bleu_score )
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
9583	def write_var_header ( fd , header ) : # write tag bytes, # and array flags + class and nzmax (null bytes) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miUINT32' ] [ 'n' ] , 8 ) ) fd . write ( struct . pack ( 'b3x4x' , mclasses [ header [ 'mclass' ] ] ) ) # write dimensions array write_elements ( fd , 'miINT32' , header [ 'dims' ] ) # write var name write_elements ( fd , 'miINT8' , asbytes ( header [ 'name' ] ) , is_name = True )
2569	def construct_end_message ( self ) : app_count = self . dfk . task_count site_count = len ( [ x for x in self . dfk . config . executors if x . managed ] ) app_fails = len ( [ t for t in self . dfk . tasks if self . dfk . tasks [ t ] [ 'status' ] in FINAL_FAILURE_STATES ] ) message = { 'uuid' : self . uuid , 'end' : time . time ( ) , 't_apps' : app_count , 'sites' : site_count , 'c_time' : None , 'failed' : app_fails , 'test' : self . test_mode , } return json . dumps ( message )
11192	def item ( proto_dataset_uri , input_file , relpath_in_dataset ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( proto_dataset_uri , config_path = CONFIG_PATH ) if relpath_in_dataset == "" : relpath_in_dataset = os . path . basename ( input_file ) proto_dataset . put_item ( input_file , relpath_in_dataset )
5916	def _translate_residue ( self , selection , default_atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = "Selection {selection!r} is not valid." . format ( * * vars ( ) ) logger . error ( errmsg ) raise ValueError ( errmsg ) gmx_resid = self . gmx_resid ( int ( m . group ( 'resid' ) ) ) # magic offset correction residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx_resname = utilities . convert_aa_code ( residue ) # only works for AA else : gmx_resname = residue # use 3-letter for any resname gmx_atomname = m . group ( 'atom' ) if gmx_atomname is None : gmx_atomname = default_atomname return { 'resname' : gmx_resname , 'resid' : gmx_resid , 'atomname' : gmx_atomname }
2436	def add_reviewer ( self , doc , reviewer ) : # Each reviewer marks the start of a new review object. # FIXME: this state does not make sense self . reset_reviews ( ) if validations . validate_reviewer ( reviewer ) : doc . add_review ( review . Review ( reviewer = reviewer ) ) return True else : raise SPDXValueError ( 'Review::Reviewer' )
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
431	def save_image ( image , image_path = '_temp.png' ) : try : # RGB imageio . imwrite ( image_path , image ) except Exception : # Greyscale imageio . imwrite ( image_path , image [ : , : , 0 ] )
6562	def iter_complete_graphs ( start , stop , factory = None ) : _ , nodes = start nodes = list ( nodes ) # we'll be appending if factory is None : factory = count ( ) while len ( nodes ) < stop : # we need to construct a new graph each time, this is actually faster than copy and add # the new edges in any case G = nx . complete_graph ( nodes ) yield G v = next ( factory ) while v in G : v = next ( factory ) nodes . append ( v )
4476	def slice_clip ( filename , start , stop , n_samples , sr , mono = True ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = stop - start soundf . seek ( start ) y = soundf . read ( n_target ) . T if mono : y = librosa . to_mono ( y ) # Resample to initial sr y = librosa . resample ( y , soundf . samplerate , sr ) # Clip to the target length exactly y = librosa . util . fix_length ( y , n_samples ) return y
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
13277	def update_desc_lcin_path ( desc , pdesc_level ) : parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == 0 ) : if ( parent_breadth == 0 ) : pass else : parent_lsib_breadth = parent_breadth - 1 plsib_desc = pdesc_level [ parent_lsib_breadth ] if ( plsib_desc [ 'leaf' ] ) : pass else : lcin_path = copy . deepcopy ( plsib_desc [ 'path' ] ) lcin_path . append ( plsib_desc [ 'sons_count' ] - 1 ) desc [ 'lcin_path' ] = lcin_path else : pass return ( desc )
12077	def figure ( self , forceNew = False ) : if plt . _pylab_helpers . Gcf . get_num_fig_managers ( ) > 0 and forceNew is False : self . log . debug ( "figure already seen, not creating one." ) return if self . subplot : self . log . debug ( "subplot mode enabled, not creating new figure" ) else : self . log . debug ( "creating new figure" ) plt . figure ( figsize = ( self . figure_width , self . figure_height ) )
3426	def medium ( self , medium ) : def set_active_bound ( reaction , bound ) : if reaction . reactants : reaction . lower_bound = - bound elif reaction . products : reaction . upper_bound = bound # Set the given media bounds media_rxns = list ( ) exchange_rxns = frozenset ( self . exchanges ) for rxn_id , bound in iteritems ( medium ) : rxn = self . reactions . get_by_id ( rxn_id ) if rxn not in exchange_rxns : LOGGER . warn ( "%s does not seem to be an" " an exchange reaction. Applying bounds anyway." , rxn . id ) media_rxns . append ( rxn ) set_active_bound ( rxn , bound ) media_rxns = frozenset ( media_rxns ) # Turn off reactions not present in media for rxn in ( exchange_rxns - media_rxns ) : set_active_bound ( rxn , 0 )
11663	def as_integer_type ( ary ) : ary = np . asanyarray ( ary ) if is_integer_type ( ary ) : return ary rounded = np . rint ( ary ) if np . any ( rounded != ary ) : raise ValueError ( "argument array must contain only integers" ) return rounded . astype ( int )
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
13384	def get_store_env_tmp ( ) : tempdir = tempfile . gettempdir ( ) temp_name = 'envstore{0:0>3d}' temp_path = unipath ( tempdir , temp_name . format ( random . getrandbits ( 9 ) ) ) if not os . path . exists ( temp_path ) : return temp_path else : return get_store_env_tmp ( )
13147	def shrink_indexes_in_place ( self , triples ) : _ent_roots = self . UnionFind ( self . _ent_id ) _rel_roots = self . UnionFind ( self . _rel_id ) for t in triples : _ent_roots . add ( t . head ) _ent_roots . add ( t . tail ) _rel_roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = _ent_roots . find ( t . head ) r = _rel_roots . find ( t . relation ) t = _ent_roots . find ( t . tail ) triples [ i ] = kgedata . TripleIndex ( h , r , t ) ents = bidict ( ) available_ent_idx = 0 for previous_idx , ent_exist in enumerate ( _ent_roots . roots ( ) ) : if not ent_exist : self . _ents . inverse . pop ( previous_idx ) else : ents [ self . _ents . inverse [ previous_idx ] ] = available_ent_idx available_ent_idx += 1 rels = bidict ( ) available_rel_idx = 0 for previous_idx , rel_exist in enumerate ( _rel_roots . roots ( ) ) : if not rel_exist : self . _rels . inverse . pop ( previous_idx ) else : rels [ self . _rels . inverse [ previous_idx ] ] = available_rel_idx available_rel_idx += 1 self . _ents = ents self . _rels = rels self . _ent_id = available_ent_idx self . _rel_id = available_rel_idx
4336	def overdrive ( self , gain_db = 20.0 , colour = 20.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'db_level must be a number.' ) if not is_number ( colour ) : raise ValueError ( 'colour must be a number.' ) effect_args = [ 'overdrive' , '{:f}' . format ( gain_db ) , '{:f}' . format ( colour ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'overdrive' ) return self
119	def terminate ( self ) : if self . _pool is not None : self . _pool . terminate ( ) self . _pool . join ( ) self . _pool = None
5829	def check_for_rate_limiting ( response , response_lambda , timeout = 1 , attempts = 0 ) : if attempts >= 3 : raise RateLimitingException ( ) if response . status_code == 429 : sleep ( timeout ) new_timeout = timeout + 1 new_attempts = attempts + 1 return check_for_rate_limiting ( response_lambda ( timeout , attempts ) , response_lambda , timeout = new_timeout , attempts = new_attempts ) return response
950	def corruptVector ( v1 , noiseLevel , numActiveCols ) : size = len ( v1 ) v2 = np . zeros ( size , dtype = "uint32" ) bitsToSwap = int ( noiseLevel * numActiveCols ) # Copy the contents of v1 into v2 for i in range ( size ) : v2 [ i ] = v1 [ i ] for _ in range ( bitsToSwap ) : i = random . randrange ( size ) if v2 [ i ] == 1 : v2 [ i ] = 0 else : v2 [ i ] = 1 return v2
66	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( "Usage of argument 'thickness' in BoundingBox.draw_on_image() " "is deprecated. The argument was renamed to 'size'." ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( "Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s." % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int # When y values get into the range (H-0.5, H), the *_int functions round them to H. # That is technically sensible, but in the case of drawing means that the border lies # just barely outside of the image, making the border disappear, even though the BB # is fully inside the image. Here we correct for that because of beauty reasons. # Same is the case for x coordinates. if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result
632	def destroySegment ( self , segment ) : # Remove the synapses from all data structures outside this Segment. for synapse in segment . _synapses : self . _removeSynapseFromPresynapticMap ( synapse ) self . _numSynapses -= len ( segment . _synapses ) # Remove the segment from the cell's list. segments = self . _cells [ segment . cell ] . _segments i = segments . index ( segment ) del segments [ i ] # Free the flatIdx and remove the final reference so the Segment can be # garbage-collected. self . _freeFlatIdxs . append ( segment . flatIdx ) self . _segmentForFlatIdx [ segment . flatIdx ] = None
1788	def DAS ( cpu ) : oldAL = cpu . AL oldCF = cpu . CF cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL - 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( oldCF , cpu . AL > oldAL ) , cpu . CF ) cpu . CF = Operators . ITE ( Operators . OR ( oldAL > 0x99 , oldCF ) , True , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , Operators . OR ( oldAL > 0x99 , oldCF ) , cpu . AL - 0x60 , cpu . AL ) # """ if (cpu.AL & 0x0f) > 9 or cpu.AF: cpu.AL = cpu.AL - 6; cpu.CF = Operators.OR(oldCF, cpu.AL > oldAL) cpu.AF = True else: cpu.AF = False if ((oldAL > 0x99) or oldCF): cpu.AL = cpu.AL - 0x60 cpu.CF = True """ cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
8089	def text ( self , txt , x , y , width = None , height = 1000000 , outline = False , draw = True , * * kwargs ) : txt = self . Text ( txt , x , y , width , height , outline = outline , ctx = None , * * kwargs ) if outline : path = txt . path if draw : path . draw ( ) return path else : return txt
1031	def b32decode ( s , casefold = False , map01 = None ) : quanta , leftover = divmod ( len ( s ) , 8 ) if leftover : raise TypeError ( 'Incorrect padding' ) # Handle section 2.4 zero and one mapping. The flag map01 will be either # False, or the character to map the digit 1 (one) to. It should be # either L (el) or I (eye). if map01 : s = s . translate ( string . maketrans ( b'01' , b'O' + map01 ) ) if casefold : s = s . upper ( ) # Strip off pad characters from the right. We need to count the pad # characters because this will tell us how many null bytes to remove from # the end of the decoded string. padchars = 0 mo = re . search ( '(?P<pad>[=]*)$' , s ) if mo : padchars = len ( mo . group ( 'pad' ) ) if padchars > 0 : s = s [ : - padchars ] # Now decode the full quanta parts = [ ] acc = 0 shift = 35 for c in s : val = _b32rev . get ( c ) if val is None : raise TypeError ( 'Non-base32 digit found' ) acc += _b32rev [ c ] << shift shift -= 5 if shift < 0 : parts . append ( binascii . unhexlify ( '%010x' % acc ) ) acc = 0 shift = 35 # Process the last, partial quanta last = binascii . unhexlify ( '%010x' % acc ) if padchars == 0 : last = '' # No characters elif padchars == 1 : last = last [ : - 1 ] elif padchars == 3 : last = last [ : - 2 ] elif padchars == 4 : last = last [ : - 3 ] elif padchars == 6 : last = last [ : - 4 ] else : raise TypeError ( 'Incorrect padding' ) parts . append ( last ) return EMPTYSTRING . join ( parts )
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
6390	def _sb_has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
13196	def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
3696	def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : """ we internally distinguish between tasks executed by backend and tasks executed with no specific backend. """ backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] # stopper won't be set unless wait_for_threads is True stopper = threading . Event ( ) # launching threads for tasks by backend if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : # Start new Threads and add them to the threads list to complete t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) # launch thread for global tasks if len ( global_tasks ) > 0 : # FIXME timer is applied to all global_tasks, does it make sense? # All tasks are executed in the same thread sequentially gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) # Give enough time create and run all threads stopper . set ( ) # All threads must stop in the next iteration # Wait for all threads to complete for t in threads : t . join ( ) # Checking for exceptions in threads to log them self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
9377	def calculate_stats ( data_list , stats_to_calculate = [ 'mean' , 'std' ] , percentiles_to_calculate = [ ] ) : stats_to_numpy_method_map = { 'mean' : numpy . mean , 'avg' : numpy . mean , 'std' : numpy . std , 'standard_deviation' : numpy . std , 'median' : numpy . median , 'min' : numpy . amin , 'max' : numpy . amax } calculated_stats = { } calculated_percentiles = { } if len ( data_list ) == 0 : return calculated_stats , calculated_percentiles for stat in stats_to_calculate : if stat in stats_to_numpy_method_map . keys ( ) : calculated_stats [ stat ] = stats_to_numpy_method_map [ stat ] ( data_list ) else : logger . error ( "Unsupported stat : " + str ( stat ) ) for percentile in percentiles_to_calculate : if isinstance ( percentile , float ) or isinstance ( percentile , int ) : calculated_percentiles [ percentile ] = numpy . percentile ( data_list , percentile ) else : logger . error ( "Unsupported percentile requested (should be int or float): " + str ( percentile ) ) return calculated_stats , calculated_percentiles
9809	def version ( cli , platform ) : version_client = PolyaxonClient ( ) . version cli = cli or not any ( [ cli , platform ] ) if cli : try : server_version = version_client . get_cli_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get cli version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) cli_version = get_version ( PROJECT_CLI_NAME ) Printer . print_header ( 'Current cli version: {}.' . format ( cli_version ) ) Printer . print_header ( 'Supported cli versions:' ) dict_tabulate ( server_version . to_dict ( ) ) if platform : try : platform_version = version_client . get_platform_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get platform version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) chart_version = version_client . get_chart_version ( ) Printer . print_header ( 'Current platform version: {}.' . format ( chart_version . version ) ) Printer . print_header ( 'Supported platform versions:' ) dict_tabulate ( platform_version . to_dict ( ) )
9057	def economic_qs_zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )
6856	def query ( query , use_sudo = True , * * kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
3552	def _state_changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) # Handle when powered on. if state == 5 : self . _powered_off . clear ( ) self . _powered_on . set ( ) # Handle when powered off. elif state == 4 : self . _powered_on . clear ( ) self . _powered_off . set ( )
11264	def stdout ( prev , endl = '\n' , thru = False ) : for i in prev : sys . stdout . write ( str ( i ) + endl ) if thru : yield i
5563	def output ( self ) : output_params = dict ( self . _raw [ "output" ] , grid = self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . output_pyramid . metatiling ) if "path" in output_params : output_params . update ( path = absolute_path ( path = output_params [ "path" ] , base_dir = self . config_dir ) ) if "format" not in output_params : raise MapcheteConfigError ( "output format not specified" ) if output_params [ "format" ] not in available_output_formats ( ) : raise MapcheteConfigError ( "format %s not available in %s" % ( output_params [ "format" ] , str ( available_output_formats ( ) ) ) ) writer = load_output_writer ( output_params ) try : writer . is_valid_with_config ( output_params ) except Exception as e : logger . exception ( e ) raise MapcheteConfigError ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver_name" ] , e ) ) return writer
7739	def check_bidi ( data ) : has_l = False has_ral = False for char in data : if stringprep . in_table_d1 ( char ) : has_ral = True elif stringprep . in_table_d2 ( char ) : has_l = True if has_l and has_ral : raise StringprepError ( "Both RandALCat and LCat characters present" ) if has_ral and ( not stringprep . in_table_d1 ( data [ 0 ] ) or not stringprep . in_table_d1 ( data [ - 1 ] ) ) : raise StringprepError ( "The first and the last character must" " be RandALCat" ) return data
8398	def breaks ( self , limits ) : # clip the breaks to the domain, # e.g. probabilities will be in [0, 1] domain vmin = np . max ( [ self . domain [ 0 ] , limits [ 0 ] ] ) vmax = np . min ( [ self . domain [ 1 ] , limits [ 1 ] ] ) breaks = np . asarray ( self . breaks_ ( [ vmin , vmax ] ) ) # Some methods(mpl_breaks, extended_breaks) that # calculate breaks take the limits as guide posts and # not hard limits. breaks = breaks . compress ( ( breaks >= self . domain [ 0 ] ) & ( breaks <= self . domain [ 1 ] ) ) return breaks
2125	def disassociate_always_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'always' ) , parent , child )
12194	def _validate_first_message ( cls , msg ) : data = cls . _unpack_message ( msg ) logger . debug ( data ) if data != cls . RTM_HANDSHAKE : raise SlackApiError ( 'Unexpected response: {!r}' . format ( data ) ) logger . info ( 'Joined real-time messaging.' )
8195	def load ( self , id ) : self . clear ( ) # Root node. self . add_node ( id , root = True ) # Directly connected nodes have priority. for w , id2 in self . get_links ( id ) : self . add_edge ( id , id2 , weight = w ) if len ( self ) > self . max : break # Now get all the other nodes in the cluster. for w , id2 , links in self . get_cluster ( id ) : for id3 in links : self . add_edge ( id3 , id2 , weight = w ) self . add_edge ( id , id3 , weight = w ) #if len(links) == 0: # self.add_edge(id, id2) if len ( self ) > self . max : break # Provide a backlink to the previous root. if self . event . clicked : g . add_node ( self . event . clicked )
10623	def extract ( self , other ) : # Extract the specified mass. if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mass ( other ) # Extract the specified mass of the specified compound. elif self . _is_compound_mass_tuple ( other ) : return self . _extract_compound_mass ( other [ 0 ] , other [ 1 ] ) # Extract all of the specified compound. elif type ( other ) is str : return self . _extract_compound ( other ) # TODO: Test # Extract all of the compounds of the specified material. elif type ( other ) is Material : return self . _extract_material ( other ) # If not one of the above, it must be an invalid argument. else : raise TypeError ( "Invalid extraction argument." )
13094	def start_processes ( self ) : self . relay = subprocess . Popen ( [ 'ntlmrelayx.py' , '-6' , '-tf' , self . targets_file , '-w' , '-l' , self . directory , '-of' , self . output_file ] , cwd = self . directory ) self . responder = subprocess . Popen ( [ 'responder' , '-I' , self . interface_name ] )
13601	def print_loading ( self , wait , message ) : tags = [ '\\' , '|' , '/' , '-' ] for i in range ( wait ) : time . sleep ( 0.25 ) sys . stdout . write ( "%(message)s... %(tag)s\r" % { 'message' : message , 'tag' : tags [ i % 4 ] } ) sys . stdout . flush ( ) pass sys . stdout . write ( "%s... Done...\n" % message ) sys . stdout . flush ( ) pass
8338	def findParents ( self , name = None , attrs = { } , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , None , limit , self . parentGenerator , * * kwargs )
13519	def prop_power ( self , propulsion_eff = 0.7 , sea_margin = 0.2 ) : PP = ( 1 + sea_margin ) * self . resistance ( ) * self . speed / propulsion_eff return PP
12694	def is_disjoint ( set1 , set2 , warn ) : for elem in set2 : if elem in set1 : raise ValueError ( warn ) return True
6644	def availableVersions ( self ) : r = [ ] for t in self . vcs . tags ( ) : logger . debug ( "available version tag: %s" , t ) # ignore empty tags: if not len ( t . strip ( ) ) : continue try : r . append ( GitCloneVersion ( t , t , self ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
322	def get_top_drawdowns ( returns , top = 10 ) : returns = returns . copy ( ) df_cum = ep . cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 drawdowns = [ ] for t in range ( top ) : peak , valley , recovery = get_max_drawdown_underwater ( underwater ) # Slice out draw-down period if not pd . isnull ( recovery ) : underwater . drop ( underwater [ peak : recovery ] . index [ 1 : - 1 ] , inplace = True ) else : # drawdown has not ended yet underwater = underwater . loc [ : peak ] drawdowns . append ( ( peak , valley , recovery ) ) if ( len ( returns ) == 0 ) or ( len ( underwater ) == 0 ) : break return drawdowns
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
219	def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f"Package {package!r} could not be found." assert ( spec . origin is not None ) , "Directory 'statics' in package {package!r} could not be found." directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) assert os . path . isdir ( directory ) , "Directory 'statics' in package {package!r} could not be found." directories . append ( directory ) return directories
8753	def partition_vifs ( xapi_client , interfaces , security_group_states ) : added = [ ] updated = [ ] removed = [ ] for vif in interfaces : # Quark should not action on isonet vifs in regions that use FLIP if ( 'floating_ip' in CONF . QUARK . environment_capabilities and is_isonet_vif ( vif ) ) : continue vif_has_groups = vif in security_group_states if vif . tagged and vif_has_groups and security_group_states [ vif ] [ sg_cli . SECURITY_GROUP_ACK ] : # Already ack'd these groups and VIF is tagged, reapply. # If it's not tagged, fall through and have it self-heal continue if vif . tagged : if vif_has_groups : updated . append ( vif ) else : removed . append ( vif ) else : if vif_has_groups : added . append ( vif ) # if not tagged and no groups, skip return added , updated , removed
933	def _getModelCheckpointFilePath ( checkpointDir ) : path = os . path . join ( checkpointDir , "model.data" ) path = os . path . abspath ( path ) return path
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
10549	def get_results ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'result' , params = params ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : # pragma: no cover raise
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : # remove nans first finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] # remove zero errors nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
13250	def get_authoryear_from_entry ( entry , paren = False ) : def _format_last ( person ) : """Reformat a pybtex Person into a last name. Joins all parts of a last name and strips "{}" wrappers. """ return ' ' . join ( [ n . strip ( '{}' ) for n in person . last_names ] ) if len ( entry . persons [ 'author' ] ) > 0 : # Grab author list persons = entry . persons [ 'author' ] elif len ( entry . persons [ 'editor' ] ) > 0 : # Grab editor list persons = entry . persons [ 'editor' ] else : raise AuthorYearError try : year = entry . fields [ 'year' ] except KeyError : raise AuthorYearError if paren and len ( persons ) == 1 : template = '{author} ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif not paren and len ( persons ) == 1 : template = '{author} {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) == 2 : template = '{author1} and {author2} ({year})' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) == 2 : template = '{author1} and {author2} {year}' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) > 2 : template = '{author} et al {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) > 2 : template = '{author} et al ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year )
11943	def _get ( self , * args , * * kwargs ) : messages , all_retrieved = super ( StorageMixin , self ) . _get ( * args , * * kwargs ) if self . user . is_authenticated ( ) : inbox_messages = self . backend . inbox_list ( self . user ) else : inbox_messages = [ ] return messages + inbox_messages , all_retrieved
9641	def _display_details ( var_data ) : meta_keys = ( key for key in list ( var_data . keys ( ) ) if key . startswith ( 'META_' ) ) for key in meta_keys : display_key = key [ 5 : ] . capitalize ( ) pprint ( '{0}: {1}' . format ( display_key , var_data . pop ( key ) ) ) pprint ( var_data )
516	def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )
7068	def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
623	def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
8450	def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )
9259	def delete_by_time ( self , issues , older_tag , newer_tag ) : if not older_tag and not newer_tag : # in case if no tags are specified - return unchanged array return copy . deepcopy ( issues ) newer_tag_time = self . get_time_of_tag ( newer_tag ) older_tag_time = self . get_time_of_tag ( older_tag ) filtered = [ ] for issue in issues : if issue . get ( 'actual_date' ) : rslt = older_tag_time < issue [ 'actual_date' ] <= newer_tag_time if rslt : filtered . append ( copy . deepcopy ( issue ) ) return filtered
10927	def _run1 ( self ) : if self . check_update_J ( ) : self . update_J ( ) else : if self . check_Broyden_J ( ) : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) : self . update_eig_J ( ) #1. Assuming that J starts updated: delta_vals = self . find_LM_updates ( self . calc_grad ( ) ) #2. Increase damping until we get a good step: er1 = self . update_function ( self . param_vals + delta_vals ) good_step = ( find_best_step ( [ self . error , er1 ] ) == 1 ) if not good_step : er0 = self . update_function ( self . param_vals ) if np . abs ( er0 - self . error ) / er0 > 1e-7 : raise RuntimeError ( 'Function updates are not exact.' ) CLOG . debug ( 'Bad step, increasing damping' ) CLOG . debug ( '\t\t%f\t%f' % ( self . error , er1 ) ) grad = self . calc_grad ( ) for _try in range ( self . _max_inner_loop ) : self . increase_damping ( ) delta_vals = self . find_LM_updates ( grad ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = ( find_best_step ( [ self . error , er1 ] ) == 1 ) if good_step : break else : er0 = self . update_function ( self . param_vals ) CLOG . warn ( 'Stuck!' ) if np . abs ( er0 - self . error ) / er0 > 1e-7 : raise RuntimeError ( 'Function updates are not exact.' ) #state is updated, now params: if good_step : self . _last_error = self . error self . error = er1 CLOG . debug ( 'Good step\t%f\t%f' % ( self . _last_error , self . error ) ) self . update_param_vals ( delta_vals , incremental = True ) self . decrease_damping ( )
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
2338	def dagify_min_edge ( g ) : while not nx . is_directed_acyclic_graph ( g ) : cycle = next ( nx . simple_cycles ( g ) ) scores = [ ] edges = [ ] for i , j in zip ( cycle [ : 1 ] , cycle [ : 1 ] ) : edges . append ( ( i , j ) ) scores . append ( g [ i ] [ j ] [ 'weight' ] ) i , j = edges [ scores . index ( min ( scores ) ) ] gc = deepcopy ( g ) gc . remove_edge ( i , j ) gc . add_edge ( j , i ) if len ( list ( nx . simple_cycles ( gc ) ) ) < len ( list ( nx . simple_cycles ( g ) ) ) : g . add_edge ( j , i , weight = min ( scores ) ) g . remove_edge ( i , j ) return g
5555	def _element_at_zoom ( name , element , zoom ) : # If element is a dictionary, analyze subitems. if isinstance ( element , dict ) : if "format" in element : # we have an input or output driver here return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element # If there is only one subelement, collapse unless it is # input. In such case, return a dictionary. if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) # If subelement is empty, return None if len ( out_elements ) == 0 : return None return out_elements # If element is a zoom level statement, filter element. elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) # If element is a string but not a zoom level statement, return # element. else : return element # Return all other types as they are. else : return element
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
9988	def new_cells ( self , name = None , formula = None ) : # Outside formulas only return self . _impl . new_cells ( name , formula ) . interface
369	def crop_multi ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x [ 0 ] . shape [ row_index ] , x [ 0 ] . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) results = [ ] for data in x : results . append ( data [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] ) return np . asarray ( results ) else : # central crop h_offset = ( h - hrg ) / 2 w_offset = ( w - wrg ) / 2 results = [ ] for data in x : results . append ( data [ h_offset : h - h_offset , w_offset : w - w_offset ] ) return np . asarray ( results )
3343	def calc_base64 ( s ) : s = compat . to_bytes ( s ) s = compat . base64_encodebytes ( s ) . strip ( ) # return bytestring return compat . to_native ( s )
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
7658	def append_records ( self , records ) : for obs in records : if isinstance ( obs , Observation ) : self . append ( * * obs . _asdict ( ) ) else : self . append ( * * obs )
3699	def Tliquidus ( Tms = None , ws = None , xs = None , CASRNs = None , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if none_and_length_check ( [ Tms ] ) : methods . append ( 'Maximum' ) methods . append ( 'Simple' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == 'Maximum' : _Tliq = max ( Tms ) elif Method == 'Simple' : _Tliq = mixing_simple ( xs , Tms ) elif Method == 'None' : return None else : raise Exception ( 'Failure in in function' ) return _Tliq
8584	def attach_volume ( self , datacenter_id , server_id , volume_id ) : data = '{ "id": "' + volume_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/volumes' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : # FIXME: settings have statements, variables have rows WTF? :-( for statement in table . rows : if statement [ 0 ] != "" : yield statement
5127	def stop_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = False
10201	def register_events ( ) : return [ dict ( event_type = 'file-download' , templates = 'invenio_stats.contrib.file_download' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_file_unique_id ] ) ) , dict ( event_type = 'record-view' , templates = 'invenio_stats.contrib.record_view' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_record_unique_id ] ) ) ]
1382	def register_watch ( self , callback ) : RETRY_COUNT = 5 # Retry in case UID is previously # generated, just in case... for _ in range ( RETRY_COUNT ) : # Generate a random UUID. uid = uuid . uuid4 ( ) if uid not in self . watches : Log . info ( "Registering a watch with uid: " + str ( uid ) ) try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) return None self . watches [ uid ] = callback return uid return None
11040	def write ( self , path , * * data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
4758	def main ( args ) : trun = cij . runner . trun_from_file ( args . trun_fpath ) rehome ( trun [ "conf" ] [ "OUTPUT" ] , args . output , trun ) postprocess ( trun ) cij . emph ( "main: reports are uses tmpl_fpath: %r" % args . tmpl_fpath ) cij . emph ( "main: reports are here args.output: %r" % args . output ) html_fpath = os . sep . join ( [ args . output , "%s.html" % args . tmpl_name ] ) cij . emph ( "html_fpath: %r" % html_fpath ) try : # Create and store HTML report with open ( html_fpath , 'w' ) as html_file : html_file . write ( dset_to_html ( trun , args . tmpl_fpath ) ) except ( IOError , OSError , ValueError ) as exc : import traceback traceback . print_exc ( ) cij . err ( "rprtr:main: exc: %s" % exc ) return 1 return 0
3056	def _create_file_if_needed ( filename ) : if os . path . exists ( filename ) : return False else : # Equivalent to "touch". open ( filename , 'a+b' ) . close ( ) logger . info ( 'Credential file {0} created' . format ( filename ) ) return True
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
5605	def write_raster_window ( in_tile = None , in_data = None , out_profile = None , out_tile = None , out_path = None , tags = None , bucket_resource = None ) : if not isinstance ( out_path , str ) : raise TypeError ( "out_path must be a string" ) logger . debug ( "write %s" , out_path ) if out_path == "memoryfile" : raise DeprecationWarning ( "Writing to memoryfile with write_raster_window() is deprecated. " "Please use RasterWindowMemoryFile." ) out_tile = in_tile if out_tile is None else out_tile _validate_write_window_params ( in_tile , out_tile , in_data , out_profile ) # extract data window_data = extract_from_array ( in_raster = in_data , in_affine = in_tile . affine , out_tile = out_tile ) if in_tile != out_tile else in_data # use transform instead of affine if "affine" in out_profile : out_profile [ "transform" ] = out_profile . pop ( "affine" ) # write if there is any band with non-masked data if window_data . all ( ) is not ma . masked : try : if out_path . startswith ( "s3://" ) : with RasterWindowMemoryFile ( in_tile = out_tile , in_data = window_data , out_profile = out_profile , out_tile = out_tile , tags = tags ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with rasterio . open ( out_path , 'w' , * * out_profile ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . write ( window_data . astype ( out_profile [ "dtype" ] , copy = False ) ) _write_tags ( dst , tags ) except Exception as e : logger . exception ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "array window empty" , out_path ) )
7882	def _make_prefix ( self , declared_prefixes ) : used_prefixes = set ( self . _prefixes . values ( ) ) used_prefixes |= set ( declared_prefixes . values ( ) ) while True : prefix = u"ns{0}" . format ( self . _next_id ) self . _next_id += 1 if prefix not in used_prefixes : break return prefix
6784	def unlock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile_path ) r . run_or_local ( 'rm -f {lockfile_path}' )
9980	def extract_names ( source ) : if source is None : return None source = dedent ( source ) funcdef = find_funcdef ( source ) params = extract_params ( source ) names = [ ] if isinstance ( funcdef , ast . FunctionDef ) : stmts = funcdef . body elif isinstance ( funcdef , ast . Lambda ) : stmts = [ funcdef . body ] else : raise ValueError ( "must not happen" ) for stmt in stmts : for node in ast . walk ( stmt ) : if isinstance ( node , ast . Name ) : if node . id not in names and node . id not in params : names . append ( node . id ) return names
1342	def samples ( dataset = 'imagenet' , index = 0 , batchsize = 1 , shape = ( 224 , 224 ) , data_format = 'channels_last' ) : from PIL import Image images , labels = [ ] , [ ] basepath = os . path . dirname ( __file__ ) samplepath = os . path . join ( basepath , 'data' ) files = os . listdir ( samplepath ) for idx in range ( index , index + batchsize ) : i = idx % 20 # get filename and label file = [ n for n in files if '{}_{:02d}_' . format ( dataset , i ) in n ] [ 0 ] label = int ( file . split ( '.' ) [ 0 ] . split ( '_' ) [ - 1 ] ) # open file path = os . path . join ( samplepath , file ) image = Image . open ( path ) if dataset == 'imagenet' : image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) if dataset != 'mnist' and data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) images . append ( image ) labels . append ( label ) labels = np . array ( labels ) images = np . stack ( images ) return images , labels
6320	def create_entrypoint ( self ) : with open ( os . path . join ( self . template_dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project_name = self . project_name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
12164	def add_listener ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _listeners [ event ] . append ( listener ) self . _check_limit ( event ) return self
12770	def step ( self , substeps = 2 ) : # by default we step by following our loaded marker data. self . frame_no += 1 try : next ( self . follower ) except ( AttributeError , StopIteration ) as err : self . reset ( )
7943	def _start_connect ( self ) : family , addr = self . _dst_addrs . pop ( 0 ) self . _socket = socket . socket ( family , socket . SOCK_STREAM ) self . _socket . setblocking ( False ) self . _dst_addr = addr self . _family = family try : self . _socket . connect ( addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] in BLOCKING_ERRORS : self . _set_state ( "connecting" ) self . _write_queue . append ( ContinueConnect ( ) ) self . _write_queue_cond . notify ( ) self . event ( ConnectingEvent ( addr ) ) return elif self . _dst_addrs : self . _set_state ( "connect" ) return elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise self . _connected ( )
2665	def readinto ( self , buf , * * kwargs ) : self . i2c . readfrom_into ( self . device_address , buf , * * kwargs ) if self . _debug : print ( "i2c_device.readinto:" , [ hex ( i ) for i in buf ] )
6857	def create_user ( name , password , host = 'localhost' , * * kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , * * kwargs ) puts ( "Created MySQL user '%s'." % name )
12894	def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
2135	def associate_notification_template ( self , workflow , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , workflow , notification_template )
13235	def make_naive ( value , timezone ) : value = value . astimezone ( timezone ) if hasattr ( timezone , 'normalize' ) : # available for pytz time zones value = timezone . normalize ( value ) return value . replace ( tzinfo = None )
11382	def do_url_scheme ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 1 : raise template . TemplateSyntaxError ( '%s takes no parameters.' % args [ 0 ] ) return OEmbedURLSchemeNode ( )
6793	def load_django_settings ( self ) : r = self . local_renderer # Save environment variables so we can restore them later. _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : # Allow us to import local app modules. if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) #TODO:remove this once bug in django-celery has been fixed os . environ [ 'ALLOW_CELERY' ] = '0' # print('settings_module:', r.format(r.env.settings_module)) os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) # os.environ['CELERY_LOADER'] = 'django' # os.environ['SITE'] = r.genv.SITE or r.genv.default_site # os.environ['ROLE'] = r.genv.ROLE or r.genv.default_role # In Django >= 1.7, fixes the error AppRegistryNotReady: Apps aren't loaded yet # Disabling, in Django >= 1.10, throws exception: # RuntimeError: Model class django.contrib.contenttypes.models.ContentType # doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS. # try: # from django.core.wsgi import get_wsgi_application # application = get_wsgi_application() # except (ImportError, RuntimeError): # raise # print('Unable to get wsgi application.') # traceback.print_exc() # In Django >= 1.7, fixes the error AppRegistryNotReady: Apps aren't loaded yet try : import django django . setup ( ) except AttributeError : # This doesn't exist in Django < 1.7, so ignore it. pass # Load Django settings. settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings # get_settings() doesn't raise ImportError but returns None instead if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : # Restore environment variables. for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) # load the private key from the AWS keypair pem privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) # connect to the server try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
7772	def _unquote ( data ) : if not data . startswith ( b'"' ) or not data . endswith ( b'"' ) : return data return QUOTE_RE . sub ( b"\\1" , data [ 1 : - 1 ] )
6257	def find ( self , path : Path ) : # Update paths from settings to make them editable runtime # This is only possible for FileSystemFinders if getattr ( self , 'settings_attr' , None ) : self . paths = getattr ( settings , self . settings_attr ) path_found = None for entry in self . paths : abspath = entry / path if abspath . exists ( ) : path_found = abspath return path_found
8445	def ls ( github_user , template , long_format ) : github_urls = temple . ls . ls ( github_user , template = template ) for ssh_path , info in github_urls . items ( ) : if long_format : print ( ssh_path , '-' , info [ 'description' ] or '(no project description found)' ) else : print ( ssh_path )
5444	def parse_uri ( self , raw_uri , recursive ) : # Assume recursive URIs are directory paths. if recursive : raw_uri = directory_fmt ( raw_uri ) # Get the file provider, validate the raw URI, and rewrite the path # component of the URI for docker and remote. file_provider = self . parse_file_provider ( raw_uri ) self . _validate_paths_or_fail ( raw_uri , recursive ) uri , docker_uri = self . rewrite_uris ( raw_uri , file_provider ) uri_parts = job_model . UriParts ( directory_fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker_uri , uri_parts , file_provider
1169	def rlecode_hqx ( s ) : if not s : return '' result = [ ] prev = s [ 0 ] count = 1 # Add a dummy character to get the loop to go one extra round. # The dummy must be different from the last character of s. # In the same step we remove the first character, which has # already been stored in prev. if s [ - 1 ] == '!' : s = s [ 1 : ] + '?' else : s = s [ 1 : ] + '!' for c in s : if c == prev and count < 255 : count += 1 else : if count == 1 : if prev != '\x90' : result . append ( prev ) else : result += [ '\x90' , '\x00' ] elif count < 4 : if prev != '\x90' : result += [ prev ] * count else : result += [ '\x90' , '\x00' ] * count else : if prev != '\x90' : result += [ prev , '\x90' , chr ( count ) ] else : result += [ '\x90' , '\x00' , '\x90' , chr ( count ) ] count = 1 prev = c return '' . join ( result )
12460	def parse_args ( args ) : from argparse import ArgumentParser description = ( 'Bootstrap Python projects and libraries with virtualenv ' 'and pip.' ) parser = ArgumentParser ( description = description ) parser . add_argument ( '--version' , action = 'version' , version = __version__ ) parser . add_argument ( '-c' , '--config' , default = DEFAULT_CONFIG , help = 'Path to config file. By default: {0}' . format ( DEFAULT_CONFIG ) ) parser . add_argument ( '-p' , '--pre-requirements' , default = [ ] , nargs = '+' , help = 'List of pre-requirements to check, separated by space.' ) parser . add_argument ( '-e' , '--env' , help = 'Virtual environment name. By default: {0}' . format ( CONFIG [ __script__ ] [ 'env' ] ) ) parser . add_argument ( '-r' , '--requirements' , help = 'Path to requirements file. By default: {0}' . format ( CONFIG [ __script__ ] [ 'requirements' ] ) ) parser . add_argument ( '-d' , '--install-dev-requirements' , action = 'store_true' , default = None , help = 'Install prefixed or suffixed "dev" requirements after ' 'installation of original requirements file or library completed ' 'without errors.' ) parser . add_argument ( '-C' , '--hook' , help = 'Execute this hook after bootstrap process.' ) parser . add_argument ( '--ignore-activated' , action = 'store_true' , default = None , help = 'Ignore pre-activated virtualenv, like on Travis CI.' ) parser . add_argument ( '--recreate' , action = 'store_true' , default = None , help = 'Recreate virtualenv on every run.' ) parser . add_argument ( '-q' , '--quiet' , action = 'store_true' , default = None , help = 'Minimize output, show only error messages.' ) return parser . parse_args ( args )
13693	def register ( self , service , name = '' ) : try : is_model = issubclass ( service , orb . Model ) except StandardError : is_model = False # expose an ORB table dynamically as a service if is_model : self . services [ service . schema ( ) . dbname ( ) ] = ( ModelService , service ) else : super ( OrbApiFactory , self ) . register ( service , name = name )
1243	def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] # if all priorities sum to ~0 choose randomly otherwise random sample if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
2844	def enable_FTDI_driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) # Mac OS commands to enable FTDI driver. _check_running_as_root ( ) subprocess . check_call ( 'kextload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . check_call ( 'kextload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) # Linux commands to enable FTDI driver. _check_running_as_root ( ) subprocess . check_call ( 'modprobe -q ftdi_sio' , shell = True ) subprocess . check_call ( 'modprobe -q usbserial' , shell = True )
1104	def set_seq1 ( self , a ) : if a is self . a : return self . a = a self . matching_blocks = self . opcodes = None
9567	def byteswap ( fmt , data , offset = 0 ) : data = BytesIO ( data ) data . seek ( offset ) data_swapped = BytesIO ( ) for f in fmt : swapped = data . read ( int ( f ) ) [ : : - 1 ] data_swapped . write ( swapped ) return data_swapped . getvalue ( )
11141	def load_repository ( self , path , verbose = True , ntrials = 3 ) : assert isinstance ( ntrials , int ) , "ntrials must be integer" assert ntrials > 0 , "ntrials must be >0" repo = None for _trial in range ( ntrials ) : try : self . __load_repository ( path = path , verbose = True ) except Exception as err1 : try : from . OldRepository import Repository REP = Repository ( path ) except Exception as err2 : #traceback.print_exc() error = "Unable to load repository using neiher new style (%s) nor old style (%s)" % ( err1 , err2 ) if self . DEBUG_PRINT_FAILED_TRIALS : print ( "Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute" % ( _trial , inspect . stack ( ) [ 1 ] [ 3 ] , str ( error ) ) ) else : error = None repo = REP break else : error = None repo = self break # check and return assert error is None , error return repo
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
3475	def compartments ( self ) : if self . _compartments is None : self . _compartments = { met . compartment for met in self . _metabolites if met . compartment is not None } return self . _compartments
9532	def dumps ( obj , key = None , salt = 'django.core.signing' , serializer = JSONSerializer , compress = False ) : data = serializer ( ) . dumps ( obj ) # Flag for if it's been compressed or not is_compressed = False if compress : # Avoid zlib dependency unless compress is being used compressed = zlib . compress ( data ) if len ( compressed ) < ( len ( data ) - 1 ) : data = compressed is_compressed = True base64d = b64_encode ( data ) if is_compressed : base64d = b'.' + base64d return TimestampSigner ( key , salt = salt ) . sign ( base64d )
7721	def get_history ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : maxchars = from_utf8 ( child . prop ( "maxchars" ) ) if maxchars is not None : maxchars = int ( maxchars ) maxstanzas = from_utf8 ( child . prop ( "maxstanzas" ) ) if maxstanzas is not None : maxstanzas = int ( maxstanzas ) maxseconds = from_utf8 ( child . prop ( "maxseconds" ) ) if maxseconds is not None : maxseconds = int ( maxseconds ) # TODO: since -- requires parsing of Jabber dateTime profile since = None return HistoryParameters ( maxchars , maxstanzas , maxseconds , since )
11925	def parse ( self , source ) : rt , title , title_pic , markdown = libparser . parse ( source ) if rt == - 1 : raise SeparatorNotFound elif rt == - 2 : raise PostTitleNotFound # change to unicode title , title_pic , markdown = map ( to_unicode , ( title , title_pic , markdown ) ) # render to html html = self . markdown . render ( markdown ) summary = self . markdown . render ( markdown [ : 200 ] ) return { 'title' : title , 'markdown' : markdown , 'html' : html , 'summary' : summary , 'title_pic' : title_pic }
12999	def hr_diagram_figure ( cluster ) : temps , lums = round_teff_luminosity ( cluster ) x , y = temps , lums colors , color_mapper = hr_diagram_color_helper ( temps ) x_range = [ max ( x ) + max ( x ) * 0.05 , min ( x ) - min ( x ) * 0.05 ] source = ColumnDataSource ( data = dict ( x = x , y = y , color = colors ) ) pf = figure ( y_axis_type = 'log' , x_range = x_range , name = 'hr' , tools = 'box_select,lasso_select,reset,hover' , title = 'H-R Diagram for {0}' . format ( cluster . name ) ) pf . select ( BoxSelectTool ) . select_every_mousemove = False pf . select ( LassoSelectTool ) . select_every_mousemove = False hover = pf . select ( HoverTool ) [ 0 ] hover . tooltips = [ ( "Temperature (Kelvin)" , "@x{0}" ) , ( "Luminosity (solar units)" , "@y{0.00}" ) ] _diagram ( source = source , plot_figure = pf , name = 'hr' , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) return pf
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) # based on BrutuZ (https://github.com/matiasb/python-unrar/pull/4) # and Cubixmeister work data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) # return file-like object return data . get_bytes ( )
2874	def get_process_parser ( self , process_id_or_name ) : if process_id_or_name in self . process_parsers_by_name : return self . process_parsers_by_name [ process_id_or_name ] else : return self . process_parsers [ process_id_or_name ]
3624	def decode ( geohash ) : lat , lon , lat_err , lon_err = decode_exactly ( geohash ) # Format to the number of decimals that are known lats = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lat_err ) ) ) ) - 1 , lat ) lons = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lon_err ) ) ) ) - 1 , lon ) if '.' in lats : lats = lats . rstrip ( '0' ) if '.' in lons : lons = lons . rstrip ( '0' ) return lats , lons
11051	def listen_events ( self , reconnects = 0 ) : self . log . info ( 'Listening for events from Marathon...' ) self . _attached = False def on_finished ( result , reconnects ) : # If the callback fires then the HTTP request to the event stream # went fine, but the persistent connection for the SSE stream was # dropped. Just reconnect for now- if we can't actually connect # then the errback will fire rather. self . log . warn ( 'Connection lost listening for events, ' 'reconnecting... ({reconnects} so far)' , reconnects = reconnects ) reconnects += 1 return self . listen_events ( reconnects ) def log_failure ( failure ) : self . log . failure ( 'Failed to listen for events' , failure ) return failure return self . marathon_client . get_events ( { 'event_stream_attached' : self . _sync_on_event_stream_attached , 'api_post_event' : self . _sync_on_api_post_event } ) . addCallbacks ( on_finished , log_failure , callbackArgs = [ reconnects ] )
1735	def _ensure_regexp ( source , n ) : #<- this function has to be improved markers = '(+~"\'=[%:?!*^|&-,;/\\' k = 0 while True : k += 1 if n - k < 0 : return True char = source [ n - k ] if char in markers : return True if char != ' ' and char != '\n' : break return False
10269	def node_is_upstream_leaf ( graph : BELGraph , node : BaseEntity ) -> bool : return 0 == len ( graph . predecessors ( node ) ) and 1 == len ( graph . successors ( node ) )
9822	def list ( page ) : # pylint:disable=redefined-builtin user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
6062	def mass_within_circle_in_units ( self , radius : dim . Length , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = radius , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
741	def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )
7339	def log_error ( msg = None , exc_info = None , logger = None , * * kwargs ) : if logger is None : logger = _logger if not exc_info : exc_info = sys . exc_info ( ) if msg is None : msg = "" exc_class , exc_msg , _ = exc_info if all ( info is not None for info in exc_info ) : logger . error ( msg , exc_info = exc_info )
12111	def save ( self , filename , metadata = { } , * * data ) : intersection = set ( metadata . keys ( ) ) & set ( data . keys ( ) ) if intersection : msg = 'Key(s) overlap between data and metadata: %s' raise Exception ( msg % ',' . join ( intersection ) )
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
1740	def maybe_download_and_extract ( ) : dest_directory = '.' filename = DATA_URL . split ( '/' ) [ - 1 ] filepath = os . path . join ( dest_directory , filename ) if not os . path . exists ( filepath ) : def _progress ( count , block_size , total_size ) : sys . stdout . write ( '\r>> Downloading %s %.1f%%' % ( filename , float ( count * block_size ) / float ( total_size ) * 100.0 ) ) sys . stdout . flush ( ) filepath , _ = urllib . request . urlretrieve ( DATA_URL , filepath , _progress ) print ( ) statinfo = os . stat ( filepath ) print ( 'Successfully downloaded' , filename , statinfo . st_size , 'bytes.' ) extracted_dir_path = os . path . join ( dest_directory , 'trees' ) if not os . path . exists ( extracted_dir_path ) : zip_ref = zipfile . ZipFile ( filepath , 'r' ) zip_ref . extractall ( dest_directory ) zip_ref . close ( )
1475	def _get_streaming_processes ( self ) : retval = { } instance_plans = self . _get_instance_plans ( self . packing_plan , self . shard ) instance_info = [ ] for instance_plan in instance_plans : global_task_id = instance_plan . task_id component_index = instance_plan . component_index component_name = instance_plan . component_name instance_id = "container_%s_%s_%d" % ( str ( self . shard ) , component_name , global_task_id ) instance_info . append ( ( instance_id , component_name , global_task_id , component_index ) ) stmgr_cmd_lst = [ self . stmgr_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--topologydefn_file=%s' % self . topology_defn_file , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--stmgr_id=%s' % self . stmgr_ids [ self . shard ] , '--instance_ids=%s' % ',' . join ( map ( lambda x : x [ 0 ] , instance_info ) ) , '--myhost=%s' % self . master_host , '--data_port=%s' % str ( self . master_port ) , '--local_data_port=%s' % str ( self . tmaster_controller_port ) , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--shell_port=%s' % str ( self . shell_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) , '--ckptmgr_id=%s' % self . ckptmgr_ids [ self . shard ] , '--metricscachemgr_mode=%s' % self . metricscache_manager_mode . lower ( ) ] stmgr_env = self . shell_env . copy ( ) if self . shell_env is not None else { } stmgr_cmd = Command ( stmgr_cmd_lst , stmgr_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : stmgr_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ self . stmgr_ids [ self . shard ] ] = stmgr_cmd # metricsmgr_metrics_sink_config_file = 'metrics_sinks.yaml' retval [ self . metricsmgr_ids [ self . shard ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ self . shard ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) if self . pkg_type == 'jar' or self . pkg_type == 'tar' : retval . update ( self . _get_java_instance_cmd ( instance_info ) ) elif self . pkg_type == 'pex' : retval . update ( self . _get_python_instance_cmd ( instance_info ) ) elif self . pkg_type == 'so' : retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) elif self . pkg_type == 'dylib' : retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) else : raise ValueError ( "Unrecognized package type: %s" % self . pkg_type ) return retval
10725	def _variant_levels ( level , variant ) : return ( level + variant , level + variant ) if variant != 0 else ( variant , level )
9182	def validate_model ( cursor , model ) : # Check the license is one valid for publication. _validate_license ( model ) _validate_roles ( model ) # Other required metadata includes: title, summary required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) # Ensure that derived-from values are either None # or point at a live record in the archive. _validate_derived_from ( cursor , model ) # FIXME Valid language code? # Are the given 'subjects' _validate_subjects ( cursor , model )
12397	def gen_method_keys ( self , * args , * * kwargs ) : token = args [ 0 ] for mro_type in type ( token ) . __mro__ [ : - 1 ] : name = mro_type . __name__ yield name
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
4059	def _citation_processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
7465	def _parse_01 ( ofiles , individual = False ) : ## parse results from outfiles cols = [ ] dats = [ ] for ofile in ofiles : ## parse file with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) ## get shape from ... shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : ## get mean results across reps cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) #10. dat [ : , 3 ] = cols . astype ( str ) ## format as a DF df = pd . DataFrame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : ## get mean results across reps #return cols res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . DataFrame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
9344	def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
11193	def metadata ( proto_dataset_uri , relpath_in_dataset , key , value ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) proto_dataset . add_item_metadata ( handle = relpath_in_dataset , key = key , value = value )
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 # Technically, type_id and message can appear in any order, so we need # a little loop here. while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : # The only reason _InternalParse would return early is if it encountered # an end-group tag. raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
10149	def _ref ( self , resp , base_name = None ) : name = base_name or resp . get ( 'title' , '' ) or resp . get ( 'name' , '' ) pointer = self . json_pointer + name self . response_registry [ name ] = resp return { '$ref' : pointer }
8763	def update_security_group_rule ( context , id , security_group_rule ) : LOG . info ( "update_security_group_rule for tenant %s" % ( context . tenant_id ) ) new_rule = security_group_rule [ "security_group_rule" ] # Only allow updatable fields new_rule = _filter_update_security_group_rule ( new_rule ) with context . session . begin ( ) : rule = db_api . security_group_rule_find ( context , id = id , scope = db_api . ONE ) if not rule : raise sg_ext . SecurityGroupRuleNotFound ( id = id ) db_rule = db_api . security_group_rule_update ( context , rule , * * new_rule ) group_id = db_rule . group_id group = db_api . security_group_find ( context , id = group_id , scope = db_api . ONE ) if not group : raise sg_ext . SecurityGroupNotFound ( id = group_id ) if group : _perform_async_update_rule ( context , group_id , group , rule . id , RULE_UPDATE ) return v . _make_security_group_rule_dict ( db_rule )
6776	def _configure_users ( self , site = None , full = 0 , only_data = 0 ) : site = site or ALL full = int ( full ) if full and not only_data : packager = self . get_satchel ( 'packager' ) packager . install_required ( type = SYSTEM , service = self . name ) r = self . local_renderer params = self . get_user_vhosts ( site = site ) # [(user, password, vhost)] with settings ( warn_only = True ) : self . add_admin_user ( ) params = sorted ( list ( params ) ) if not only_data : for user , password , vhost in params : r . env . broker_user = user r . env . broker_password = password r . env . broker_vhost = vhost with settings ( warn_only = True ) : r . sudo ( 'rabbitmqctl add_user {broker_user} {broker_password}' ) r . sudo ( 'rabbitmqctl add_vhost {broker_vhost}' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {broker_user} ".*" ".*" ".*"' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {admin_username} ".*" ".*" ".*"' ) return params
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , * * kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , * * kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
3709	def calculate ( self , T , method ) : if method == RACKETT : Vm = Rackett ( T , self . Tc , self . Pc , self . Zc ) elif method == YAMADA_GUNN : Vm = Yamada_Gunn ( T , self . Tc , self . Pc , self . omega ) elif method == BHIRUD_NORMAL : Vm = Bhirud_normal ( T , self . Tc , self . Pc , self . omega ) elif method == TOWNSEND_HALES : Vm = Townsend_Hales ( T , self . Tc , self . Vc , self . omega ) elif method == HTCOSTALD : Vm = COSTALD ( T , self . Tc , self . Vc , self . omega ) elif method == YEN_WOODS_SAT : Vm = Yen_Woods_saturation ( T , self . Tc , self . Vc , self . Zc ) elif method == MMSNM0 : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega ) elif method == MMSNM0FIT : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega , self . SNM0_delta_SRK ) elif method == CAMPBELL_THODOS : Vm = Campbell_Thodos ( T , self . Tb , self . Tc , self . Pc , self . MW , self . dipole ) elif method == HTCOSTALDFIT : Vm = COSTALD ( T , self . Tc , self . COSTALD_Vchar , self . COSTALD_omega_SRK ) elif method == RACKETTFIT : Vm = Rackett ( T , self . Tc , self . Pc , self . RACKETT_Z_RA ) elif method == PERRYDIPPR : A , B , C , D = self . DIPPR_coeffs Vm = 1. / EQ105 ( T , A , B , C , D ) elif method == CRC_INORG_L : rho = CRC_inorganic ( T , self . CRC_INORG_L_rho , self . CRC_INORG_L_k , self . CRC_INORG_L_Tm ) Vm = rho_to_Vm ( rho , self . CRC_INORG_L_MW ) elif method == VDI_PPDS : A , B , C , D = self . VDI_PPDS_coeffs tau = 1. - T / self . VDI_PPDS_Tc rho = self . VDI_PPDS_rhoc + A * tau ** 0.35 + B * tau ** ( 2 / 3. ) + C * tau + D * tau ** ( 4 / 3. ) Vm = rho_to_Vm ( rho , self . VDI_PPDS_MW ) elif method == CRC_INORG_L_CONST : Vm = self . CRC_INORG_L_CONST_Vm elif method == COOLPROP : Vm = 1. / CoolProp_T_dependent_property ( T , self . CASRN , 'DMOLAR' , 'l' ) elif method in self . tabular_data : Vm = self . interpolate ( T , method ) return Vm
879	def __getLogger ( cls ) : if cls . __logger is None : cls . __logger = opf_utils . initLogger ( cls ) return cls . __logger
8169	def run_context ( self ) : with LiveExecution . lock : if self . edited_source is None : yield True , self . known_good , self . ns return ns_snapshot = copy . copy ( self . ns ) try : yield False , self . edited_source , self . ns self . known_good = self . edited_source self . edited_source = None self . call_good_cb ( ) return except Exception as ex : tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) self . edited_source = None self . ns . clear ( ) self . ns . update ( ns_snapshot )
3648	def applyConsumable ( self , item_id , resource_id ) : # TODO: catch exception when consumable is not found etc. # TODO: multiple players like in quickSell method = 'POST' url = 'item/resource/%s' % resource_id data = { 'apply' : [ { 'id' : item_id } ] } self . __request__ ( method , url , data = json . dumps ( data ) )
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : # Tuple to string 1 return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : # Tuple to string 2 return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' # get IDAHO images which intersect box results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] # specify band information band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id # specify location information location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
2769	def get_all_firewalls ( self ) : data = self . get_data ( "firewalls" ) firewalls = list ( ) for jsoned in data [ 'firewalls' ] : firewall = Firewall ( * * jsoned ) firewall . token = self . token in_rules = list ( ) for rule in jsoned [ 'inbound_rules' ] : in_rules . append ( InboundRule ( * * rule ) ) firewall . inbound_rules = in_rules out_rules = list ( ) for rule in jsoned [ 'outbound_rules' ] : out_rules . append ( OutboundRule ( * * rule ) ) firewall . outbound_rules = out_rules firewalls . append ( firewall ) return firewalls
13638	def settings ( path = None , with_path = None ) : if path : Settings . bind ( path , with_path = with_path ) return Settings . _wrapped
13280	def child_end_handler ( self , scache ) : desc = self . desc desc_level = scache . desc_level breadth = desc_level . __len__ ( ) desc [ 'breadth' ] = breadth desc [ 'breadth_path' ] . append ( breadth ) desc_level . append ( desc )
10467	def getAnyAppWithWindow ( cls ) : # Refresh the runningApplications list apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) if hasattr ( ref , 'windows' ) and len ( ref . windows ( ) ) > 0 : return ref raise ValueError ( 'No GUI application found.' )
11829	def expand ( self , problem ) : return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
1881	def constrain ( self , constraint ) : constraint = self . migrate_expression ( constraint ) self . _constraints . add ( constraint )
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
6698	def preseed_package ( pkg_name , preseed ) : for q_name , _ in preseed . items ( ) : q_type , q_answer = _ run_as_root ( 'echo "%(pkg_name)s %(q_name)s %(q_type)s %(q_answer)s" | debconf-set-selections' % locals ( ) )
10993	def _check_for_inception ( self , root_dict ) : for key in root_dict : if isinstance ( root_dict [ key ] , dict ) : root_dict [ key ] = ResponseObject ( root_dict [ key ] ) return root_dict
4020	def _init_docker_vm ( ) : if not _dusty_vm_exists ( ) : log_to_client ( 'Initializing new Dusty VM with Docker Machine' ) machine_options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG_BOOT2DOCKER_URL , '--virtualbox-memory' , str ( get_config_value ( constants . CONFIG_VM_MEM_SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM_NIC_TYPE ] check_call_demoted ( [ 'docker-machine' , 'create' ] + machine_options + [ constants . VM_MACHINE_NAME ] , redirect_stderr = True )
8752	def is_isonet_vif ( vif ) : nicira_iface_id = vif . record . get ( 'other_config' ) . get ( 'nicira-iface-id' ) if nicira_iface_id : return True return False
778	def _getMatchingRowsNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = None ) : assert fieldsToMatch , repr ( fieldsToMatch ) assert all ( k in tableInfo . dbFieldNames for k in fieldsToMatch . iterkeys ( ) ) , repr ( fieldsToMatch ) assert selectFieldNames , repr ( selectFieldNames ) assert all ( f in tableInfo . dbFieldNames for f in selectFieldNames ) , repr ( selectFieldNames ) # NOTE: make sure match expressions and values are in the same order matchPairs = fieldsToMatch . items ( ) matchExpressionGen = ( p [ 0 ] + ( ' IS ' + { True : 'TRUE' , False : 'FALSE' } [ p [ 1 ] ] if isinstance ( p [ 1 ] , bool ) else ' IS NULL' if p [ 1 ] is None else ' IN %s' if isinstance ( p [ 1 ] , self . _SEQUENCE_TYPES ) else '=%s' ) for p in matchPairs ) matchFieldValues = [ p [ 1 ] for p in matchPairs if ( not isinstance ( p [ 1 ] , ( bool ) ) and p [ 1 ] is not None ) ] query = 'SELECT %s FROM %s WHERE (%s)' % ( ',' . join ( selectFieldNames ) , tableInfo . tableName , ' AND ' . join ( matchExpressionGen ) ) sqlParams = matchFieldValues if maxRows is not None : query += ' LIMIT %s' sqlParams . append ( maxRows ) conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows : assert maxRows is None or len ( rows ) <= maxRows , "%d !<= %d" % ( len ( rows ) , maxRows ) assert len ( rows [ 0 ] ) == len ( selectFieldNames ) , "%d != %d" % ( len ( rows [ 0 ] ) , len ( selectFieldNames ) ) else : rows = tuple ( ) return rows
4705	def write ( self , path ) : with open ( path , "wb" ) as fout : fout . write ( self . m_buf )
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
15	def q_retrace ( R , D , q_i , v , rho_i , nenvs , nsteps , gamma ) : rho_bar = batch_to_seq ( tf . minimum ( 1.0 , rho_i ) , nenvs , nsteps , True ) # list of len steps, shape [nenvs] rs = batch_to_seq ( R , nenvs , nsteps , True ) # list of len steps, shape [nenvs] ds = batch_to_seq ( D , nenvs , nsteps , True ) # list of len steps, shape [nenvs] q_is = batch_to_seq ( q_i , nenvs , nsteps , True ) vs = batch_to_seq ( v , nenvs , nsteps + 1 , True ) v_final = vs [ - 1 ] qret = v_final qrets = [ ] for i in range ( nsteps - 1 , - 1 , - 1 ) : check_shape ( [ qret , ds [ i ] , rs [ i ] , rho_bar [ i ] , q_is [ i ] , vs [ i ] ] , [ [ nenvs ] ] * 6 ) qret = rs [ i ] + gamma * qret * ( 1.0 - ds [ i ] ) qrets . append ( qret ) qret = ( rho_bar [ i ] * ( qret - q_is [ i ] ) ) + vs [ i ] qrets = qrets [ : : - 1 ] qret = seq_to_batch ( qrets , flat = True ) return qret
13735	def get_api_error ( response ) : error_class = _status_code_to_class . get ( response . status_code , APIError ) return error_class ( response )
5677	def get_trip_trajectories_within_timespan ( self , start , end , use_shapes = True , filter_name = None ) : trips = [ ] trip_df = self . get_tripIs_active_in_range ( start , end ) print ( "gtfs_viz.py: fetched " + str ( len ( trip_df ) ) + " trip ids" ) shape_cache = { } # loop over all trips: for row in trip_df . itertuples ( ) : trip_I = row . trip_I day_start_ut = row . day_start_ut shape_id = row . shape_id trip = { } name , route_type = self . get_route_name_and_type_of_tripI ( trip_I ) trip [ 'route_type' ] = int ( route_type ) trip [ 'name' ] = str ( name ) if filter_name and ( name != filter_name ) : continue stop_lats = [ ] stop_lons = [ ] stop_dep_times = [ ] shape_breaks = [ ] stop_seqs = [ ] # get stop_data and store it: stop_time_df = self . get_trip_stop_time_data ( trip_I , day_start_ut ) for stop_row in stop_time_df . itertuples ( ) : stop_lats . append ( float ( stop_row . lat ) ) stop_lons . append ( float ( stop_row . lon ) ) stop_dep_times . append ( float ( stop_row . dep_time_ut ) ) try : stop_seqs . append ( int ( stop_row . seq ) ) except TypeError : stop_seqs . append ( None ) if use_shapes : try : shape_breaks . append ( int ( stop_row . shape_break ) ) except ( TypeError , ValueError ) : shape_breaks . append ( None ) if use_shapes : # get shape data (from cache, if possible) if shape_id not in shape_cache : shape_cache [ shape_id ] = shapes . get_shape_points2 ( self . conn . cursor ( ) , shape_id ) shape_data = shape_cache [ shape_id ] # noinspection PyBroadException try : trip [ 'times' ] = shapes . interpolate_shape_times ( shape_data [ 'd' ] , shape_breaks , stop_dep_times ) trip [ 'lats' ] = shape_data [ 'lats' ] trip [ 'lons' ] = shape_data [ 'lons' ] start_break = shape_breaks [ 0 ] end_break = shape_breaks [ - 1 ] trip [ 'times' ] = trip [ 'times' ] [ start_break : end_break + 1 ] trip [ 'lats' ] = trip [ 'lats' ] [ start_break : end_break + 1 ] trip [ 'lons' ] = trip [ 'lons' ] [ start_break : end_break + 1 ] except : # In case interpolation fails: trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons else : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons trips . append ( trip ) return { "trips" : trips }
7790	def update_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state ( ) self . _items_list . sort ( ) if item . state == 'purged' : self . _purged += 1 if self . _purged > 0.25 * self . max_items : self . purge_items ( ) return state finally : self . _lock . release ( )
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
10770	def shapely_formatter ( _ , vertices , codes = None ) : elements = [ ] if codes is None : for vertices_ in vertices : if np . all ( vertices_ [ 0 , : ] == vertices_ [ - 1 , : ] ) : # Contour is single point. if len ( vertices ) < 3 : elements . append ( Point ( vertices_ [ 0 , : ] ) ) # Contour is closed. else : elements . append ( LinearRing ( vertices_ ) ) # Contour is open. else : elements . append ( LineString ( vertices_ ) ) else : for vertices_ , codes_ in zip ( vertices , codes ) : starts = np . nonzero ( codes_ == MPLPATHCODE . MOVETO ) [ 0 ] stops = np . nonzero ( codes_ == MPLPATHCODE . CLOSEPOLY ) [ 0 ] try : rings = [ LinearRing ( vertices_ [ start : stop + 1 , : ] ) for start , stop in zip ( starts , stops ) ] elements . append ( Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) except ValueError as err : # Verify error is from degenerate (single point) polygon. if np . any ( stop - start - 1 == 0 ) : # Polygon is single point, remove the polygon. if stops [ 0 ] < starts [ 0 ] + 2 : pass # Polygon has single point hole, remove the hole. else : rings = [ LinearRing ( vertices_ [ start : stop + 1 , : ] ) for start , stop in zip ( starts , stops ) if stop >= start + 2 ] elements . append ( Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) else : raise ( err ) return elements
12925	def match_to_clinvar ( genome_file , clin_file ) : clin_curr_line = _next_line ( clin_file ) genome_curr_line = _next_line ( genome_file ) # Ignores all the lines that start with a hashtag while clin_curr_line . startswith ( '#' ) : clin_curr_line = _next_line ( clin_file ) while genome_curr_line . startswith ( '#' ) : genome_curr_line = _next_line ( genome_file ) # Advance through both files simultaneously to find matches while clin_curr_line and genome_curr_line : # Advance a file when positions aren't equal. clin_curr_pos = VCFLine . get_pos ( clin_curr_line ) genome_curr_pos = VCFLine . get_pos ( genome_curr_line ) try : if clin_curr_pos [ 'chrom' ] > genome_curr_pos [ 'chrom' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'chrom' ] < genome_curr_pos [ 'chrom' ] : clin_curr_line = _next_line ( clin_file ) continue if clin_curr_pos [ 'pos' ] > genome_curr_pos [ 'pos' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'pos' ] < genome_curr_pos [ 'pos' ] : clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break # If we get here, start positions match. # Look for allele matching. genome_vcf_line = GenomeVCFLine ( vcf_line = genome_curr_line , skip_info = True ) # We can skip if genome has no allele information for this point. if not genome_vcf_line . genotype_allele_indexes : genome_curr_line = _next_line ( genome_file ) continue # Match only if ClinVar and Genome ref_alleles match. clinvar_vcf_line = ClinVarVCFLine ( vcf_line = clin_curr_line ) if not genome_vcf_line . ref_allele == clinvar_vcf_line . ref_allele : try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break # Determine genome alleles and zygosity. Zygosity is assumed to be one # of: heterozygous, homozygous, or hemizygous. genotype_allele_indexes = genome_vcf_line . genotype_allele_indexes genome_alleles = [ genome_vcf_line . alleles [ x ] for x in genotype_allele_indexes ] if len ( genome_alleles ) == 1 : zygosity = 'Hem' elif len ( genome_alleles ) == 2 : if genome_alleles [ 0 ] . sequence == genome_alleles [ 1 ] . sequence : zygosity = 'Hom' genome_alleles = [ genome_alleles [ 0 ] ] else : zygosity = 'Het' else : raise ValueError ( 'This code only expects to work on genomes ' + 'with one or two alleles called at each ' + 'location. The following line violates this:' + str ( genome_vcf_line ) ) # Look for matches to ClinVar alleles. for genome_allele in genome_alleles : for allele in clinvar_vcf_line . alleles : if genome_allele . sequence == allele . sequence : # The 'records' attribute is specific to ClinVarAlleles. if hasattr ( allele , 'records' ) : yield ( genome_vcf_line , allele , zygosity ) # Done matching, move on. try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) except StopIteration : break
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
4286	def write ( self , album ) : page = self . template . render ( * * self . generate_context ( album ) ) output_file = os . path . join ( album . dst_path , album . output_file ) with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
3010	def _get_scopes ( self ) : if _credentials_from_request ( self . request ) : return ( self . _scopes | _credentials_from_request ( self . request ) . scopes ) else : return self . _scopes
1994	def save_state ( self , state , state_id = None ) : assert isinstance ( state , StateBase ) if state_id is None : state_id = self . _get_id ( ) else : self . rm_state ( state_id ) self . _store . save_state ( state , f'{self._prefix}{state_id:08x}{self._suffix}' ) return state_id
10414	def node_exclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def exclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : """Pass only for a node that isn't in the enclosed node list :return: If the node isn't contained within the enclosed node list """ return node not in node_set return exclusion_filter
11007	def get_bets ( self , type = None , order_by = None , state = None , project_id = None , page = None , page_size = None ) : if page is None : page = 1 if page_size is None : page_size = 100 if state == 'all' : _states = [ ] # all states == no filter elif state == 'closed' : _states = self . CLOSED_STATES else : _states = self . ACTIVE_STATES url = urljoin ( self . settings [ 'bets_url' ] , 'bets?page={}&page_size={}' . format ( page , page_size ) ) url += '&state={}' . format ( ',' . join ( _states ) ) if type is not None : url += '&type={}' . format ( type ) if order_by in [ '-last_stake' , 'last_stake' ] : url += '&order_by={}' . format ( order_by ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) res = self . _req ( url ) return res [ 'bets' ] [ 'results' ]
7816	def add_handler ( self , handler ) : if not isinstance ( handler , EventHandler ) : raise TypeError , "Not an EventHandler" with self . lock : if handler in self . handlers : return self . handlers . append ( handler ) self . _update_handlers ( )
8214	def gtk_mouse_button_down ( self , widget , event ) : if self . menu_enabled and event . button == 3 : menu = self . uimanager . get_widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( ShoebotWindow , self ) . gtk_mouse_button_down ( widget , event )
9346	def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
4934	def parse_datetime_to_epoch ( datestamp , magnitude = 1.0 ) : parsed_datetime = parse_lms_api_datetime ( datestamp ) time_since_epoch = parsed_datetime - UNIX_EPOCH return int ( time_since_epoch . total_seconds ( ) * magnitude )
187	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : # TODO improve efficiency here by copying only once for ls in self . line_strings : image = ls . draw_on_image ( image , color = color , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return image
4983	def get ( self , request , enterprise_uuid , course_id ) : # Check to see if access to the course run is restricted for this user. embargo_url = EmbargoApiClient . redirect_if_blocked ( [ course_id ] , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) enterprise_customer , course , course_run , modes = self . get_base_details ( request , enterprise_uuid , course_id ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_uuid ) data_sharing_consent = DataSharingConsent . objects . proxied_get ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer ) enrollment_client = EnrollmentApiClient ( ) enrolled_course = enrollment_client . get_course_enrollment ( request . user . username , course_id ) try : enterprise_course_enrollment = EnterpriseCourseEnrollment . objects . get ( enterprise_customer_user__enterprise_customer = enterprise_customer , enterprise_customer_user__user_id = request . user . id , course_id = course_id ) except EnterpriseCourseEnrollment . DoesNotExist : enterprise_course_enrollment = None if enrolled_course and enterprise_course_enrollment : # The user is already enrolled in the course through the Enterprise Customer, so redirect to the course # info page. return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) return self . get_enterprise_course_enrollment_page ( request , enterprise_customer , course , course_run , modes , enterprise_course_enrollment , data_sharing_consent , )
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
902	def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( "In updateAnomalyLikelihoods." ) print ( "Number of anomaly scores:" , len ( anomalyScores ) ) print ( "First 20:" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( "Params:" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( "Must have at least one anomalyScore" ) if not isValidEstimatorParams ( params ) : raise ValueError ( "'params' is not a valid params structure" ) # For backward compatibility. if "historicalLikelihoods" not in params : params [ "historicalLikelihoods" ] = [ 1.0 ] # Compute moving averages of these new scores using the previous values # as well as likelihood for these scores using the old estimator historicalValues = params [ "movingAverage" ] [ "historicalValues" ] total = params [ "movingAverage" ] [ "total" ] windowSize = params [ "movingAverage" ] [ "windowSize" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) # Filter the likelihood values. First we prepend the historical likelihoods # to the current set. Then we filter the values. We peel off the likelihoods # to return and the last windowSize values to store for later. likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] # Update the estimator newParams = { "distribution" : params [ "distribution" ] , "movingAverage" : { "historicalValues" : historicalValues , "total" : total , "windowSize" : windowSize , } , "historicalLikelihoods" : historicalLikelihoods , } assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize if verbosity > 3 : print ( "Number of likelihoods:" , len ( likelihoods ) ) print ( "First 20 likelihoods:" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( "Leaving updateAnomalyLikelihoods." ) return ( likelihoods , aggRecordList , newParams )
5811	def raise_hostname ( certificate , hostname ) : is_ip = re . match ( '^\\d+\\.\\d+\\.\\d+\\.\\d+$' , hostname ) or hostname . find ( ':' ) != - 1 if is_ip : hostname_type = 'IP address %s' % hostname else : hostname_type = 'domain name %s' % hostname message = 'Server certificate verification failed - %s does not match' % hostname_type valid_ips = ', ' . join ( certificate . valid_ips ) valid_domains = ', ' . join ( certificate . valid_domains ) if valid_domains : message += ' valid domains: %s' % valid_domains if valid_domains and valid_ips : message += ' or' if valid_ips : message += ' valid IP addresses: %s' % valid_ips raise TLSVerificationError ( message , certificate )
991	def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : assert stop >= start startRows = [ ] copyRows = [ ] ts = None inc = None if tsCol is None : tsCol = reader . getTimestampFieldIdx ( ) for i , row in enumerate ( reader ) : # Get the first timestamp and the increment. if ts is None : ts = row [ tsCol ] elif inc is None : inc = row [ tsCol ] - ts # Keep a list of all rows and a list of rows to copy. if i >= start and i <= stop : copyRows . append ( row ) startRows . append ( row ) # Insert the copied rows. if insertLocation is None : insertLocation = stop + 1 startRows [ insertLocation : insertLocation ] = copyRows # Update the timestamps. for row in startRows : row [ tsCol ] = ts writer . appendRecord ( row ) ts += inc
11777	def replicated_dataset ( dataset , weights , n = None ) : n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
10845	def shuffle ( self , count = None , utc = None ) : url = PATHS [ 'SHUFFLE' ] % self . profile_id post_data = '' if count : post_data += 'count=%s&' % count if utc : post_data += 'utc=%s' % utc return self . api . post ( url = url , data = post_data )
8550	def update_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value if attr == 'source_mac' : data [ 'sourceMac' ] = value elif attr == 'source_ip' : data [ 'sourceIp' ] = value elif attr == 'target_ip' : data [ 'targetIp' ] = value elif attr == 'port_range_start' : data [ 'portRangeStart' ] = value elif attr == 'port_range_end' : data [ 'portRangeEnd' ] = value elif attr == 'icmp_type' : data [ 'icmpType' ] = value elif attr == 'icmp_code' : data [ 'icmpCode' ] = value else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
9060	def beta ( self ) : from numpy_sugar . linalg import rsolve return rsolve ( self . _X [ "VT" ] , rsolve ( self . _X [ "tX" ] , self . mean ( ) ) )
9210	def capture ( target_url , user_agent = "archiveis (https://github.com/pastpages/archiveis)" , proxies = { } ) : # Put together the URL that will save our request domain = "http://archive.vn" save_url = urljoin ( domain , "/submit/" ) # Configure the request headers headers = { 'User-Agent' : user_agent , "host" : "archive.vn" , } # Request a unique identifier for our activity logger . debug ( "Requesting {}" . format ( domain + "/" ) ) get_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , ) if proxies : get_kwargs [ 'proxies' ] = proxies response = requests . get ( domain + "/" , * * get_kwargs ) response . raise_for_status ( ) # It will need to be parsed from the homepage response headers html = str ( response . content ) try : unique_id = html . split ( 'name="submitid' , 1 ) [ 1 ] . split ( 'value="' , 1 ) [ 1 ] . split ( '"' , 1 ) [ 0 ] logger . debug ( "Unique identifier: {}" . format ( unique_id ) ) except IndexError : logger . warn ( "Unable to extract unique identifier from archive.is. Submitting without it." ) unique_id = None # Send the capture request to archive.is with the unique id included data = { "url" : target_url , "anyway" : 1 , } if unique_id : data . update ( { "submitid" : unique_id } ) post_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , data = data ) if proxies : post_kwargs [ 'proxies' ] = proxies logger . debug ( "Requesting {}" . format ( save_url ) ) response = requests . post ( save_url , * * post_kwargs ) response . raise_for_status ( ) # There are a couple ways the header can come back if 'Refresh' in response . headers : memento = str ( response . headers [ 'Refresh' ] ) . split ( ';url=' ) [ 1 ] logger . debug ( "Memento from Refresh header: {}" . format ( memento ) ) return memento if 'Location' in response . headers : memento = response . headers [ 'Location' ] logger . debug ( "Memento from Location header: {}" . format ( memento ) ) return memento logger . debug ( "Memento not found in response headers. Inspecting history." ) for i , r in enumerate ( response . history ) : logger . debug ( "Inspecting history request #{}" . format ( i ) ) logger . debug ( r . headers ) if 'Location' in r . headers : memento = r . headers [ 'Location' ] logger . debug ( "Memento from the Location header of {} history response: {}" . format ( i + 1 , memento ) ) return memento # If there's nothing at this point, throw an error logger . error ( "No memento returned by archive.is" ) logger . error ( "Status code: {}" . format ( response . status_code ) ) logger . error ( response . headers ) logger . error ( response . text ) raise Exception ( "No memento returned by archive.is" )
11026	def sort_pem_objects ( pem_objects ) : keys , certs , ca_certs = [ ] , [ ] , [ ] for pem_object in pem_objects : if isinstance ( pem_object , pem . Key ) : keys . append ( pem_object ) else : # This assumes all pem objects provided are either of type pem.Key # or pem.Certificate. Technically, there are CSR and CRL types, but # we should never be passed those. if _is_ca ( pem_object ) : ca_certs . append ( pem_object ) else : certs . append ( pem_object ) [ key ] , [ cert ] = keys , certs return key , cert , ca_certs
11042	def request ( self , method , url = None , * * kwargs ) : url = self . _compose_url ( url , kwargs ) kwargs . setdefault ( 'timeout' , self . _timeout ) d = self . _client . request ( method , url , reactor = self . _reactor , * * kwargs ) d . addCallback ( self . _log_request_response , method , url , kwargs ) d . addErrback ( self . _log_request_error , url ) return d
9247	def generate_sub_section ( self , issues , prefix ) : log = "" if issues : if not self . options . simple_list : log += u"{0}\n\n" . format ( prefix ) for issue in issues : merge_string = self . get_string_for_issue ( issue ) log += u"- {0}\n" . format ( merge_string ) log += "\n" return log
6813	def post_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) self . vprint ( 'post_deploy:' , service ) funcs = common . service_post_deployers . get ( service ) if funcs : self . vprint ( 'Running post-deployments for service %s...' % ( service , ) ) for func in funcs : try : func ( ) except Exception as e : print ( 'Post deployment error: %s' % e , file = sys . stderr ) print ( traceback . format_exc ( ) , file = sys . stderr )
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : # try reading header to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : # try reading data to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : # Try again later -> call continue_read later Log . debug ( "Try again error" ) else : # Fatal error Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
5177	def resources ( self , type_ = None , title = None , * * kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) return resources
4339	def pitch ( self , n_semitones , quick = False ) : if not is_number ( n_semitones ) : raise ValueError ( "n_semitones must be a positive number" ) if n_semitones < - 12 or n_semitones > 12 : logger . warning ( "Using an extreme pitch shift. " "Quality of results will be poor" ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'pitch' ] if quick : effect_args . append ( '-q' ) effect_args . append ( '{:f}' . format ( n_semitones * 100. ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'pitch' ) return self
9826	def experiments ( ctx , metrics , declarations , independent , group , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiments ( username = user , project_name = project_name , independent = independent , group = group , metrics = metrics , declarations = declarations , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiments for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiments for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiments found for project `{}/{}`.' . format ( user , project_name ) ) if metrics : objects = get_experiments_with_metrics ( response ) elif declarations : objects = get_experiments_with_declarations ( response ) else : objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiments:" ) objects . pop ( 'project_name' , None ) dict_tabulate ( objects , is_list_dict = True )
12650	def is_fnmatch_regex ( string ) : is_regex = False regex_chars = [ '!' , '*' , '$' ] for c in regex_chars : if string . find ( c ) > - 1 : return True return is_regex
3401	def find_external_compartment ( model ) : if model . boundary : counts = pd . Series ( tuple ( r . compartments ) [ 0 ] for r in model . boundary ) most = counts . value_counts ( ) most = most . index [ most == most . max ( ) ] . to_series ( ) else : most = None like_external = compartment_shortlist [ "e" ] + [ "e" ] matches = pd . Series ( [ co in like_external for co in model . compartments ] , index = model . compartments ) if matches . sum ( ) == 1 : compartment = matches . index [ matches ] [ 0 ] LOGGER . info ( "Compartment `%s` sounds like an external compartment. " "Using this one without counting boundary reactions" % compartment ) return compartment elif most is not None and matches . sum ( ) > 1 and matches [ most ] . sum ( ) == 1 : compartment = most [ matches [ most ] ] [ 0 ] LOGGER . warning ( "There are several compartments that look like an " "external compartment but `%s` has the most boundary " "reactions, so using that as the external " "compartment." % compartment ) return compartment elif matches . sum ( ) > 1 : raise RuntimeError ( "There are several compartments (%s) that look " "like external compartments but we can't tell " "which one to use. Consider renaming your " "compartments please." ) if most is not None : return most [ 0 ] LOGGER . warning ( "Could not identify an external compartment by name and" " choosing one with the most boundary reactions. That " "might be complete nonsense or change suddenly. " "Consider renaming your compartments using " "`Model.compartments` to fix this." ) # No info in the model, so give up raise RuntimeError ( "The heuristic for discovering an external compartment " "relies on names and boundary reactions. Yet, there " "are neither compartments with recognized names nor " "boundary reactions in the model." )
12458	def iterkeys ( data , * * kwargs ) : return iter ( data . keys ( * * kwargs ) ) if IS_PY3 else data . iterkeys ( * * kwargs )
11621	def set_script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise IllegalInput ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : # n = -1 is the default script .. self . curr_script = n self . delta = n * DELTA return
878	def newPosition ( self , whichVars = None ) : # TODO: incorporate data from choice variables.... # TODO: make sure we're calling this when appropriate. # Get the global best position for this swarm generation globalBestPosition = None # If speculative particles are enabled, use the global best considering # even particles in the current generation. This gives better results # but does not provide repeatable results because it depends on # worker timing if self . _hsObj . _speculativeParticles : genIdx = self . genIdx else : genIdx = self . genIdx - 1 if genIdx >= 0 : ( bestModelId , _ ) = self . _resultsDB . bestModelIdAndErrScore ( self . swarmId , genIdx ) if bestModelId is not None : ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( bestModelId ) globalBestPosition = Particle . getPositionFromState ( particleState ) # Update each variable for ( varName , var ) in self . permuteVars . iteritems ( ) : if whichVars is not None and varName not in whichVars : continue if globalBestPosition is None : var . newPosition ( None , self . _rng ) else : var . newPosition ( globalBestPosition [ varName ] , self . _rng ) # get the new position position = self . getPosition ( ) # Log the new position if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : msg = StringIO . StringIO ( ) print >> msg , "New particle position: \n%s" % ( pprint . pformat ( position , indent = 4 ) ) print >> msg , "Particle variables:" for ( varName , var ) in self . permuteVars . iteritems ( ) : print >> msg , " %s: %s" % ( varName , str ( var ) ) self . logger . debug ( msg . getvalue ( ) ) msg . close ( ) return position
10972	def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_requests ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , requests = True , page = page , per_page = per_page , )
5301	def parse_colors ( path ) : if path . endswith ( ".txt" ) : return parse_rgb_txt_file ( path ) elif path . endswith ( ".json" ) : return parse_json_color_file ( path ) raise TypeError ( "colorful only supports .txt and .json files for colors" )
9786	def resources ( ctx , gpu ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . build_job . resources ( user , project_name , _build , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
470	def read_words ( filename = "nietzsche.txt" , replace = None ) : if replace is None : replace = [ '\n' , '<eos>' ] with tf . gfile . GFile ( filename , "r" ) as f : try : # python 3.4 or older context_list = f . read ( ) . replace ( * replace ) . split ( ) except Exception : # python 3.5 f . seek ( 0 ) replace = [ x . encode ( 'utf-8' ) for x in replace ] context_list = f . read ( ) . replace ( * replace ) . split ( ) return context_list
4166	def tf2zp ( b , a ) : from numpy import roots assert len ( b ) == len ( a ) , "length of the vectors a and b must be identical. fill with zeros if needed." g = b [ 0 ] / a [ 0 ] z = roots ( b ) p = roots ( a ) return z , p , g
13870	def NormalizePath ( path ) : if path . endswith ( '/' ) or path . endswith ( '\\' ) : slash = os . path . sep else : slash = '' return os . path . normpath ( path ) + slash
4892	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : # pylint: disable=invalid-name LearnerDataTransmissionAudit = apps . get_model ( 'integrated_channel' , 'LearnerDataTransmissionAudit' ) completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing return [ LearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) ]
12751	def joint_torques ( self ) : return as_flat_array ( getattr ( j , 'amotor' , j ) . feedback [ - 1 ] [ : j . ADOF ] for j in self . joints )
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
6973	def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsarefluxes = False , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd = ( fsv [ : : ] [ finind ] , fdv [ : : ] [ finind ] , fkv [ : : ] [ finind ] , xcc [ : : ] [ finind ] , ycc [ : : ] [ finind ] , bgv [ : : ] [ finind ] , bge [ : : ] [ finind ] , iha [ : : ] [ finind ] , izd [ : : ] [ finind ] , ) stimes , smags , serrs , separams = sigclip_magseries_with_extparams ( times , mags , errs , [ fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ] , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd = separams # smooth the signal if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , * * epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) # initial fit coeffs initcoeffs = np . zeros ( 22 ) # fit the smoothed mags and find the EPD function coefficients leastsqfit = leastsq ( _epd_residual , initcoeffs , args = ( smoothedmags , sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd ) , full_output = True ) # if the fit succeeds, then get the EPD mags if leastsqfit [ - 1 ] in ( 1 , 2 , 3 , 4 ) : fitcoeffs = leastsqfit [ 0 ] epdfit = _epd_function ( fitcoeffs , ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd ) epdmags = npmedian ( fmags ) + fmags - epdfit retdict = { 'times' : ftimes , 'mags' : epdmags , 'errs' : ferrs , 'fitcoeffs' : fitcoeffs , 'fitinfo' : leastsqfit , 'fitmags' : epdfit , 'mags_median' : npmedian ( epdmags ) , 'mags_mad' : npmedian ( npabs ( epdmags - npmedian ( epdmags ) ) ) } return retdict # if the solution fails, return nothing else : LOGERROR ( 'EPD fit did not converge' ) return None
3973	def _get_ports_list ( app_name , port_specs ) : if app_name not in port_specs [ 'docker_compose' ] : return [ ] return [ "{}:{}" . format ( port_spec [ 'mapped_host_port' ] , port_spec [ 'in_container_port' ] ) for port_spec in port_specs [ 'docker_compose' ] [ app_name ] ]
3527	def piwik ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PiwikNode ( )
7581	def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : ## stat lines if "Estimated Ln Prob of Data" in line : self . est_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean_lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) ## matrix lines nonline = psearch . search ( line ) popline = dsearch . search ( line ) #if ") : " in line: if nonline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : # Check if siblings exist; don't process the paragraph in that case. if element . next is not None : return element elif element . prev is not None : return element # Remove the Para wrapper from the lone paragraph. # `Plain` is a container that isn't rendered as a paragraph. return Plain ( * element . content )
5207	def format_output ( data : pd . DataFrame , source , col_maps = None ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) if source == 'bdp' : req_cols = [ 'ticker' , 'field' , 'value' ] else : req_cols = [ 'ticker' , 'field' , 'name' , 'value' , 'position' ] if any ( col not in data for col in req_cols ) : return pd . DataFrame ( ) if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) if source == 'bdp' : res = pd . DataFrame ( pd . concat ( [ pd . Series ( { * * { 'ticker' : t } , * * grp . set_index ( 'field' ) . value . to_dict ( ) } ) for t , grp in data . groupby ( 'ticker' ) ] , axis = 1 , sort = False ) ) . transpose ( ) . set_index ( 'ticker' ) else : res = pd . DataFrame ( pd . concat ( [ grp . loc [ : , [ 'name' , 'value' ] ] . set_index ( 'name' ) . transpose ( ) . reset_index ( drop = True ) . assign ( ticker = t ) for ( t , _ ) , grp in data . groupby ( [ 'ticker' , 'position' ] ) ] , sort = False ) ) . reset_index ( drop = True ) . set_index ( 'ticker' ) res . columns . name = None if col_maps is None : col_maps = dict ( ) return res . rename ( columns = lambda vv : col_maps . get ( vv , vv . lower ( ) . replace ( ' ' , '_' ) . replace ( '-' , '_' ) ) ) . apply ( pd . to_numeric , errors = 'ignore' , downcast = 'float' )
6218	def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
4577	def get_server ( self , key , * * kwds ) : kwds = dict ( self . kwds , * * kwds ) server = self . servers . get ( key ) if server : # Make sure it's the right server. server . check_keywords ( self . constructor , kwds ) else : # Make a new server server = _CachedServer ( self . constructor , key , kwds ) self . servers [ key ] = server return server
4509	def set_device_id ( self , dev , id ) : if id < 0 or id > 255 : raise ValueError ( "ID must be an unsigned byte!" ) com , code , ok = io . send_packet ( CMDTYPE . SETID , 1 , dev , self . baudrate , 5 , id ) if not ok : raise_error ( code )
1421	def load ( file_object ) : marshaller = JavaObjectUnmarshaller ( file_object ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
13356	def run_global_hook ( hook_name , * args ) : hook_finder = HookFinder ( get_global_hook_path ( ) ) hook = hook_finder ( hook_name ) if hook : hook . run ( * args )
8472	def setup ( ) : # # Check if dir is writable # if not os.access(AtomShieldsScanner.HOME, os.W_OK): # AtomShieldsScanner.HOME = os.path.expanduser("~/.atomshields") # AtomShieldsScanner.CHECKERS_DIR = os.path.join(AtomShieldsScanner.HOME, "checkers") # AtomShieldsScanner.REPORTS_DIR = os.path.join(AtomShieldsScanner.HOME, "reports") if not os . path . isdir ( AtomShieldsScanner . CHECKERS_DIR ) : os . makedirs ( AtomShieldsScanner . CHECKERS_DIR ) if not os . path . isdir ( AtomShieldsScanner . REPORTS_DIR ) : os . makedirs ( AtomShieldsScanner . REPORTS_DIR ) # Copy all checkers for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "checkers" ) , "*.py" ) : AtomShieldsScanner . installChecker ( f ) # Copy all reports for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "reports" ) , "*.py" ) : AtomShieldsScanner . installReport ( f ) AtomShieldsScanner . _executeMassiveMethod ( path = AtomShieldsScanner . CHECKERS_DIR , method = "install" , args = { } ) config_dir = os . path . dirname ( AtomShieldsScanner . CONFIG_PATH ) if not os . path . isdir ( config_dir ) : os . makedirs ( config_dir )
13316	def create ( name_or_path = None , config = None ) : # Get the real path of the environment if utils . is_system_path ( name_or_path ) : path = unipath ( name_or_path ) else : path = unipath ( get_home_path ( ) , name_or_path ) if os . path . exists ( path ) : raise OSError ( '{} already exists' . format ( path ) ) env = VirtualEnvironment ( path ) utils . ensure_path_exists ( env . path ) if config : if utils . is_git_repo ( config ) : Git ( '' ) . clone ( config , env . path ) else : shutil . copy2 ( config , env . config_path ) else : with open ( env . config_path , 'w' ) as f : f . write ( defaults . environment_config ) utils . ensure_path_exists ( env . hook_path ) utils . ensure_path_exists ( env . modules_path ) env . run_hook ( 'precreate' ) virtualenv . create_environment ( env . path ) if not utils . is_home_environment ( env . path ) : EnvironmentCache . add ( env ) EnvironmentCache . save ( ) try : env . update ( ) except : utils . rmtree ( path ) logger . debug ( 'Failed to update, rolling back...' ) raise else : env . run_hook ( 'postcreate' ) return env
132	def is_out_of_image ( self , image , fully = True , partly = False ) : # TODO this is inconsistent with line strings, which return a default # value in these cases if len ( self . exterior ) == 0 : raise Exception ( "Cannot determine whether the polygon is inside the image, because it contains no points." ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )
12713	def join_to ( self , joint , other_body = None , * * kwargs ) : self . world . join ( joint , self , other_body , * * kwargs )
5992	def plot_figure ( array , as_subplot , units , kpc_per_arcsec , figsize , aspect , cmap , norm , norm_min , norm_max , linthresh , linscale , xticks_manual , yticks_manual ) : fig = plotter_util . setup_figure ( figsize = figsize , as_subplot = as_subplot ) norm_min , norm_max = get_normalization_min_max ( array = array , norm_min = norm_min , norm_max = norm_max ) norm_scale = get_normalization_scale ( norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale ) extent = get_extent ( array = array , units = units , kpc_per_arcsec = kpc_per_arcsec , xticks_manual = xticks_manual , yticks_manual = yticks_manual ) plt . imshow ( array , aspect = aspect , cmap = cmap , norm = norm_scale , extent = extent ) return fig
3181	def delete ( self , batch_webhook_id ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _delete ( url = self . _build_path ( batch_webhook_id ) )
13238	def _daily_periods ( self , range_start , range_end ) : specific = set ( self . exceptions . keys ( ) ) return heapq . merge ( self . exception_periods ( range_start , range_end ) , * [ sched . daily_periods ( range_start = range_start , range_end = range_end , exclude_dates = specific ) for sched in self . _recurring_schedules ] )
6758	def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) # Remove local namespace settings from the global namespace # by converting <satchel_name>_<variable_name> to <variable_name>. local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
11668	def quadratic ( Ks , dim , rhos , required = None ) : # Estimated with alpha=1, beta=0: # B_{k,d,1,0} is the same as B_{k,d,0,1} in linear() # and the full estimator is # B / (n - 1) * mean(rho ^ -dim) N = rhos . shape [ 0 ] Ks = np . asarray ( Ks ) Bs = ( Ks - 1 ) / np . pi ** ( dim / 2 ) * gamma ( dim / 2 + 1 ) # shape (num_Ks,) est = Bs / ( N - 1 ) * np . mean ( rhos ** ( - dim ) , axis = 0 ) return est
10096	def create_new_locale ( self , template_id , locale , version_name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version_name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . _api_request ( self . TEMPLATES_LOCALES_ENDPOINT % template_id , self . HTTP_POST , payload = payload , timeout = timeout )
6530	def purge_config_cache ( location = None ) : cache_path = get_cache_path ( location ) if location : os . remove ( cache_path ) else : shutil . rmtree ( cache_path )
9804	def activate ( username ) : try : PolyaxonClient ( ) . user . activate_user ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not activate user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "User `{}` was activated successfully." . format ( username ) )
13305	def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
9076	def begin ( self ) : self . connect ( self . host , self . port ) if self . user : self . starttls ( ) self . login ( self . user , self . password )
9032	def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
7898	def process_configuration_form_success ( self , stanza ) : if stanza . get_query_ns ( ) != MUC_OWNER_NS : raise ValueError ( "Bad result namespace" ) # TODO: ProtocolError query = stanza . get_query ( ) form = None for el in xml_element_ns_iter ( query . children , DATAFORM_NS ) : form = Form ( el ) break if not form : raise ValueError ( "No form received" ) # TODO: ProtocolError self . configuration_form = form self . handler . configuration_form_received ( form )
4182	def window_blackman_nuttall ( N ) : a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
10724	def xformer ( signature ) : funcs = [ f for ( f , _ ) in xformers ( signature ) ] def the_func ( objects ) : """ Returns the a list of objects, transformed. :param objects: a list of objects :type objects: list of object :returns: transformed objects :rtype: list of object (in dbus types) """ if len ( objects ) != len ( funcs ) : raise IntoDPValueError ( objects , "objects" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( objects ) ) ) return [ x for ( x , _ ) in ( f ( a ) for ( f , a ) in zip ( funcs , objects ) ) ] return the_func
7025	def make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = False , wrap = False , model_over_lc = False ) : # set up the figure plt . close ( 'all' ) plt . figure ( figsize = ( 8 , 4.8 ) ) if model_over_lc : model_z = 100 lc_z = 0 else : model_z = 0 lc_z = 100 if not wrap : plt . plot ( phase , fitmags , linewidth = 3.0 , color = 'red' , zorder = model_z ) plt . plot ( phase , pmags , marker = 'o' , markersize = 1.0 , linestyle = 'none' , rasterized = True , color = 'k' , zorder = lc_z ) # set the x axis ticks and label plt . gca ( ) . set_xticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) else : plt . plot ( np . concatenate ( [ phase - 1.0 , phase ] ) , np . concatenate ( [ fitmags , fitmags ] ) , linewidth = 3.0 , color = 'red' , zorder = model_z ) plt . plot ( np . concatenate ( [ phase - 1.0 , phase ] ) , np . concatenate ( [ pmags , pmags ] ) , marker = 'o' , markersize = 1.0 , linestyle = 'none' , rasterized = True , color = 'k' , zorder = lc_z ) plt . gca ( ) . set_xlim ( ( - 0.8 , 0.8 ) ) # set the x axis ticks and label plt . gca ( ) . set_xticks ( [ - 0.8 , - 0.7 , - 0.6 , - 0.5 , - 0.4 , - 0.3 , - 0.2 , - 0.1 , 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 ] ) # set the y axis limit and label ymin , ymax = plt . ylim ( ) if not magsarefluxes : plt . gca ( ) . invert_yaxis ( ) plt . ylabel ( 'magnitude' ) else : plt . ylabel ( 'flux' ) plt . xlabel ( 'phase' ) plt . title ( 'period: %.6f, folded at %.6f, fit epoch: %.6f' % ( period , mintime , magseriesepoch ) ) plt . savefig ( plotfit ) plt . close ( )
1786	def CMPXCHG8B ( cpu , dest ) : size = dest . size cmp_reg_name_l = { 64 : 'EAX' , 128 : 'RAX' } [ size ] cmp_reg_name_h = { 64 : 'EDX' , 128 : 'RDX' } [ size ] src_reg_name_l = { 64 : 'EBX' , 128 : 'RBX' } [ size ] src_reg_name_h = { 64 : 'ECX' , 128 : 'RCX' } [ size ] # EDX:EAX or RDX:RAX cmph = cpu . read_register ( cmp_reg_name_h ) cmpl = cpu . read_register ( cmp_reg_name_l ) srch = cpu . read_register ( src_reg_name_h ) srcl = cpu . read_register ( src_reg_name_l ) cmp0 = Operators . CONCAT ( size , cmph , cmpl ) src0 = Operators . CONCAT ( size , srch , srcl ) arg_dest = dest . read ( ) cpu . ZF = arg_dest == cmp0 dest . write ( Operators . ITEBV ( size , cpu . ZF , Operators . CONCAT ( size , srch , srcl ) , arg_dest ) ) cpu . write_register ( cmp_reg_name_l , Operators . ITEBV ( size // 2 , cpu . ZF , cmpl , Operators . EXTRACT ( arg_dest , 0 , size // 2 ) ) ) cpu . write_register ( cmp_reg_name_h , Operators . ITEBV ( size // 2 , cpu . ZF , cmph , Operators . EXTRACT ( arg_dest , size // 2 , size // 2 ) ) )
10459	def isEmpty ( cls , datatype = None ) : if not datatype : datatype = AppKit . NSString if not isinstance ( datatype , types . ListType ) : datatype = [ datatype ] pp = pprint . PrettyPrinter ( ) logging . debug ( 'Desired datatypes: %s' % pp . pformat ( datatype ) ) opt_dict = { } logging . debug ( 'Results filter is: %s' % pp . pformat ( opt_dict ) ) try : log_msg = 'Request to verify pasteboard is empty' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) # canReadObjectForClasses_options_() seems to return an int (> 0 if # True) # Need to negate to get the sense we want (True if can not read the # data type from the pasteboard) its_empty = not bool ( pb . canReadObjectForClasses_options_ ( datatype , opt_dict ) ) except ValueError as error : logging . error ( error ) raise return bool ( its_empty )
4467	def serialize ( transform , * * kwargs ) : params = transform . get_params ( ) return jsonpickle . encode ( params , * * kwargs )
2073	def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
7246	def launch ( self , workflow ) : # hit workflow api try : r = self . gbdx_connection . post ( self . workflows_url , json = workflow ) try : r . raise_for_status ( ) except : print ( "GBDX API Status Code: %s" % r . status_code ) print ( "GBDX API Response: %s" % r . text ) r . raise_for_status ( ) workflow_id = r . json ( ) [ 'id' ] return workflow_id except TypeError : self . logger . debug ( 'Workflow not launched!' )
10151	def _build_paths ( self ) : paths = { } tags = [ ] for service in self . services : path , path_obj = self . _extract_path_from_service ( service ) service_tags = getattr ( service , 'tags' , [ ] ) self . _check_tags ( service_tags ) tags = self . _get_tags ( tags , service_tags ) for method , view , args in service . definitions : if method . lower ( ) in map ( str . lower , self . ignore_methods ) : continue op = self . _extract_operation_from_view ( view , args ) if any ( ctype in op . get ( 'consumes' , [ ] ) for ctype in self . ignore_ctypes ) : continue # XXX: Swagger doesn't support different schemas for for a same method # with different ctypes as cornice. If this happens, you may ignore one # content-type from the documentation otherwise we raise an Exception # Related to https://github.com/OAI/OpenAPI-Specification/issues/146 previous_definition = path_obj . get ( method . lower ( ) ) if previous_definition : raise CorniceSwaggerException ( ( "Swagger doesn't support multiple " "views for a same method. You may " "ignore one." ) ) # If tag not defined and a default tag is provided if 'tags' not in op and self . default_tags : if callable ( self . default_tags ) : op [ 'tags' ] = self . default_tags ( service , method ) else : op [ 'tags' ] = self . default_tags op_tags = op . get ( 'tags' , [ ] ) self . _check_tags ( op_tags ) # Add service tags if service_tags : new_tags = service_tags + op_tags op [ 'tags' ] = list ( OrderedDict . fromkeys ( new_tags ) ) # Add method tags to root tags tags = self . _get_tags ( tags , op_tags ) # If operation id is not defined and a default generator is provided if 'operationId' not in op and self . default_op_ids : if not callable ( self . default_op_ids ) : raise CorniceSwaggerException ( 'default_op_id should be a callable.' ) op [ 'operationId' ] = self . default_op_ids ( service , method ) # If security options not defined and default is provided if 'security' not in op and self . default_security : if callable ( self . default_security ) : op [ 'security' ] = self . default_security ( service , method ) else : op [ 'security' ] = self . default_security if not isinstance ( op . get ( 'security' , [ ] ) , list ) : raise CorniceSwaggerException ( 'security should be a list or callable' ) path_obj [ method . lower ( ) ] = op paths [ path ] = path_obj return paths , tags
7513	def locichunk ( args ) : ## parse args data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args ## this slice hslice = [ start , start + optim ] ## get filter db info co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## get seqs db io5 = h5py . File ( data . clust_database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ## which loci passed all filters keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] ## write loci that passed after trimming edges, then write snp string for iloc in keep : edg = aedge [ iloc ] #LOGGER.info("!!!!!! iloc edg %s, %s", iloc, edg) args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter_pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter_singles ( * args ) store . append ( outstr ) ## write to file and clear store tmpo = os . path . join ( data . dirs . outfiles , data . name + ".loci.{}" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( "\n" . join ( store ) + "\n" ) ## close handles io5 . close ( ) co5 . close ( ) ## return sample counter return samplecov , locuscov , start
8950	def error ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;37;41mERROR: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
2376	def run ( self , args ) : self . args = self . parse_and_process_args ( args ) if self . args . version : print ( __version__ ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . _load_rule_file ( filename ) if self . args . list : self . list_rules ( ) return 0 if self . args . describe : self . _describe_rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , "other" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) continue if os . path . isdir ( filename ) : self . _process_folder ( filename ) else : self . _process_file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0
3633	def cardInfo ( self , resource_id ) : # TODO: add referer to headers (futweb) base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : # not a player? url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
9558	def _apply_unique_checks ( self , i , r , unique_sets , summarize = False , context = None ) : for key , code , message in self . _unique_checks : value = None values = unique_sets [ key ] if isinstance ( key , basestring ) : # assume key is a field name fi = self . _field_names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : # assume key is a list or tuple, i.e., compound key value = [ ] for f in key : fi = self . _field_names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) # enable hashing if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )
9455	def schedule_play ( self , call_params ) : path = '/' + self . api_version + '/SchedulePlay/' method = 'POST' return self . request ( path , method , call_params )
8940	def _to_pypi ( self , docs_base , release ) : url = None with self . _zipped ( docs_base ) as handle : reply = requests . post ( self . params [ 'url' ] , auth = get_pypi_auth ( ) , allow_redirects = False , files = dict ( content = ( self . cfg . project . name + '.zip' , handle , 'application/zip' ) ) , data = { ':action' : 'doc_upload' , 'name' : self . cfg . project . name } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( * * vars ( reply ) ) ) elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for POST to {url}" . format ( * * data ) ) return url
3939	def get_chunks ( self , new_data_bytes ) : self . _buf += new_data_bytes while True : buf_decoded = _best_effort_decode ( self . _buf ) buf_utf16 = buf_decoded . encode ( 'utf-16' ) [ 2 : ] length_str_match = LEN_REGEX . match ( buf_decoded ) if length_str_match is None : break else : length_str = length_str_match . group ( 1 ) # Both lengths are in number of bytes in UTF-16 encoding. # The length of the submission: length = int ( length_str ) * 2 # The length of the submission length and newline: length_length = len ( ( length_str + '\n' ) . encode ( 'utf-16' ) [ 2 : ] ) if len ( buf_utf16 ) - length_length < length : break submission = buf_utf16 [ length_length : length_length + length ] yield submission . decode ( 'utf-16' ) # Drop the length and the submission itself from the beginning # of the buffer. drop_length = ( len ( ( length_str + '\n' ) . encode ( ) ) + len ( submission . decode ( 'utf-16' ) . encode ( ) ) ) self . _buf = self . _buf [ drop_length : ]
7830	def _new_from_xml ( cls , xmlnode ) : field_type = xmlnode . prop ( "type" ) label = from_utf8 ( xmlnode . prop ( "label" ) ) name = from_utf8 ( xmlnode . prop ( "var" ) ) child = xmlnode . children values = [ ] options = [ ] required = False desc = None while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "required" : required = True elif child . name == "desc" : desc = from_utf8 ( child . getContent ( ) ) elif child . name == "value" : values . append ( from_utf8 ( child . getContent ( ) ) ) elif child . name == "option" : options . append ( Option . _new_from_xml ( child ) ) child = child . next if field_type and not field_type . endswith ( "-multi" ) and len ( values ) > 1 : raise BadRequestProtocolError ( "Multiple values for a single-value field" ) return cls ( name , values , field_type , label , options , required , desc )
3583	def _print_tree ( self ) : # This is based on the bluez sample code get-managed-objects.py. objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( " %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( " %s = %s" % ( key , properties [ key ] ) )
13466	def set_moments ( self , sx , sxp , sxxp ) : self . _sx = sx self . _sxp = sxp self . _sxxp = sxxp emit = _np . sqrt ( sx ** 2 * sxp ** 2 - sxxp ** 2 ) self . _store_emit ( emit = emit )
6505	def add_properties ( self ) : for property_name in [ p [ 0 ] for p in inspect . getmembers ( self . __class__ ) if isinstance ( p [ 1 ] , property ) ] : self . _results_fields [ property_name ] = getattr ( self , property_name , None )
3753	def Ceiling ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _Ceiling = None else : raise Exception ( 'Failure in in function' ) return _Ceiling
9506	def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
6969	def _old_epd_diffmags ( coeff , fsv , fdv , fkv , xcc , ycc , bgv , bge , mag ) : return - ( coeff [ 0 ] * fsv ** 2. + coeff [ 1 ] * fsv + coeff [ 2 ] * fdv ** 2. + coeff [ 3 ] * fdv + coeff [ 4 ] * fkv ** 2. + coeff [ 5 ] * fkv + coeff [ 6 ] + coeff [ 7 ] * fsv * fdv + coeff [ 8 ] * fsv * fkv + coeff [ 9 ] * fdv * fkv + coeff [ 10 ] * np . sin ( 2 * np . pi * xcc ) + coeff [ 11 ] * np . cos ( 2 * np . pi * xcc ) + coeff [ 12 ] * np . sin ( 2 * np . pi * ycc ) + coeff [ 13 ] * np . cos ( 2 * np . pi * ycc ) + coeff [ 14 ] * np . sin ( 4 * np . pi * xcc ) + coeff [ 15 ] * np . cos ( 4 * np . pi * xcc ) + coeff [ 16 ] * np . sin ( 4 * np . pi * ycc ) + coeff [ 17 ] * np . cos ( 4 * np . pi * ycc ) + coeff [ 18 ] * bgv + coeff [ 19 ] * bge - mag )
2654	def pull_file ( self , remote_source , local_dir ) : local_dest = local_dir + '/' + os . path . basename ( remote_source ) try : os . makedirs ( local_dir ) except OSError as e : if e . errno != errno . EEXIST : logger . exception ( "Failed to create script_dir: {0}" . format ( script_dir ) ) raise BadScriptPath ( e , self . hostname ) # Easier to check this than to waste time trying to pull file and # realize there's a problem. if os . path . exists ( local_dest ) : logger . exception ( "Remote file copy will overwrite a local file:{0}" . format ( local_dest ) ) raise FileExists ( None , self . hostname , filename = local_dest ) try : self . sftp_client . get ( remote_source , local_dest ) except Exception as e : logger . exception ( "File pull failed" ) raise FileCopyException ( e , self . hostname ) return local_dest
457	def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : # Make sure that we keep our distance from the main body if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
13564	def get_field_names ( obj , ignore_auto = True , ignore_relations = True , exclude = [ ] ) : from django . db . models import ( AutoField , ForeignKey , ManyToManyField , ManyToOneRel , OneToOneField , OneToOneRel ) for field in obj . _meta . get_fields ( ) : if ignore_auto and isinstance ( field , AutoField ) : continue if ignore_relations and ( isinstance ( field , ForeignKey ) or isinstance ( field , ManyToManyField ) or isinstance ( field , ManyToOneRel ) or isinstance ( field , OneToOneRel ) or isinstance ( field , OneToOneField ) ) : # optimization is killing coverage measure, have to put no-op that # does something a = 1 a continue if field . name in exclude : continue yield field . name
6703	def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer # print('self.genv.user:', self.genv.user) # print('self.env.passwords:', self.env.passwords) r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] # print('self.genv.user:', self.genv.user) # print('self.env.passwords:', self.env.passwords) r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , #"Login password for '%s': " % r.genv.user: r.env.new_password, # "Login password for '%s': " % r.genv.user: r.env.old_password, } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : # Otherwise, use the password or key set in the config. self . genv . password = r . env . new_password else : # Default password fails and there's no current password, so clear. self . genv . password = None print ( 'using password:' , self . genv . password ) # Note, the correct current password should be set in host.initrole(), not here. #r.genv.password = r.env.new_password #r.genv.password = r.env.new_password with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : # We need to disconnect to reset the session or else Linux will again prompt # us to change our password. disconnect_all ( ) # Further logins should require the new password. self . genv . password = r . env . new_password
3407	def eval_gpr ( expr , knockouts ) : if isinstance ( expr , Expression ) : return eval_gpr ( expr . body , knockouts ) elif isinstance ( expr , Name ) : return expr . id not in knockouts elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : return any ( eval_gpr ( i , knockouts ) for i in expr . values ) elif isinstance ( op , And ) : return all ( eval_gpr ( i , knockouts ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name__ ) elif expr is None : return True else : raise TypeError ( "unsupported operation " + repr ( expr ) )
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
8836	def minus ( * args ) : if len ( args ) == 1 : return - to_numeric ( args [ 0 ] ) return to_numeric ( args [ 0 ] ) - to_numeric ( args [ 1 ] )
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
762	def getRandomWithMods ( inputSpace , maxChanges ) : size = len ( inputSpace ) ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] value = copy . deepcopy ( inputSpace [ ind ] ) if maxChanges == 0 : return value return modifyBits ( value , maxChanges )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
12642	def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
12832	def validate_xml_name ( name ) : if len ( name ) == 0 : raise RuntimeError ( 'empty XML name' ) if __INVALID_NAME_CHARS & set ( name ) : raise RuntimeError ( 'XML name contains invalid character' ) if name [ 0 ] in __INVALID_NAME_START_CHARS : raise RuntimeError ( 'XML name starts with invalid character' )
1723	def translate_file ( input_path , output_path ) : js = get_file_contents ( input_path ) py_code = translate_js ( js ) lib_name = os . path . basename ( output_path ) . split ( '.' ) [ 0 ] head = '__all__ = [%s]\n\n# Don\'t look below, you will not understand this Python code :) I don\'t.\n\n' % repr ( lib_name ) tail = '\n\n# Add lib to the module scope\n%s = var.to_python()' % lib_name out = head + py_code + tail write_file_contents ( output_path , out )
7455	def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
8556	def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
9496	def parse_module ( path , excludes = None ) : file = path / MODULE_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == MODULE_FILENAME , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Module ( id , file , resources )
4842	def get_program_type_by_slug ( self , slug ) : return self . _load_data ( self . PROGRAM_TYPES_ENDPOINT , resource_id = slug , default = None , )
4939	def get_link_by_email ( self , user_email ) : try : user = User . objects . get ( email = user_email ) try : return self . get ( user_id = user . id ) except EnterpriseCustomerUser . DoesNotExist : pass except User . DoesNotExist : pass try : return PendingEnterpriseCustomerUser . objects . get ( user_email = user_email ) except PendingEnterpriseCustomerUser . DoesNotExist : pass return None
11568	def open ( self , verbose ) : # open a serial port if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : # in case the port is already open, let's close it and then # reopen it self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : # opened failed - will report back to caller raise
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
7618	def beat ( ref , est , * * kwargs ) : namespace = 'beat' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , _ = ref . to_event_values ( ) est_times , _ = est . to_event_values ( ) return mir_eval . beat . evaluate ( ref_times , est_times , * * kwargs )
13630	def _renderResource ( resource , request ) : meth = getattr ( resource , 'render_' + nativeString ( request . method ) , None ) if meth is None : try : allowedMethods = resource . allowedMethods except AttributeError : allowedMethods = _computeAllowedMethods ( resource ) raise UnsupportedMethod ( allowedMethods ) return meth ( request )
6368	def precision_gain ( self ) : if self . population ( ) == 0 : return float ( 'NaN' ) random_precision = self . cond_pos_pop ( ) / self . population ( ) return self . precision ( ) / random_precision
8607	def add_group_user ( self , group_id , user_id ) : data = { "id" : user_id } response = self . _perform_request ( url = '/um/groups/%s/users' % group_id , method = 'POST' , data = json . dumps ( data ) ) return response
12632	def copy_groups_to_folder ( dicom_groups , folder_path , groupby_field_name ) : if dicom_groups is None or not dicom_groups : raise ValueError ( 'Expected a boyle.dicom.sets.DicomFileSet.' ) if not os . path . exists ( folder_path ) : os . makedirs ( folder_path , exist_ok = False ) for dcmg in dicom_groups : if groupby_field_name is not None and len ( groupby_field_name ) > 0 : dfile = DicomFile ( dcmg ) dir_name = '' for att in groupby_field_name : dir_name = os . path . join ( dir_name , dfile . get_attributes ( att ) ) dir_name = str ( dir_name ) else : dir_name = os . path . basename ( dcmg ) group_folder = os . path . join ( folder_path , dir_name ) os . makedirs ( group_folder , exist_ok = False ) log . debug ( 'Copying files to {}.' . format ( group_folder ) ) import shutil dcm_files = dicom_groups [ dcmg ] for srcf in dcm_files : destf = os . path . join ( group_folder , os . path . basename ( srcf ) ) while os . path . exists ( destf ) : destf += '+' shutil . copy2 ( srcf , destf )
7248	def get_stdout ( self , workflow_id , task_id ) : url = '%(wf_url)s/%(wf_id)s/tasks/%(task_id)s/stdout' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id , 'task_id' : task_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . text
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : # Shuffling the list of boids ensures fluid movement. # If you need the boids to retain their position in the list # each update, set the shuffled parameter to False. from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 # cohesion m2 = 1.0 # separation m3 = 1.0 # alignment m4 = 1.0 # goal # The flock scatters randomly with a Boids.scatter chance. # This means their cohesion (m1) is reversed, # and their joint alignment (m3) is dimished, # causing boids to oscillate in confusion. # Setting Boids.scatter(chance=0) ensures they never scatter. if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 # A flock can have a goal defined with Boids.goal(x,y,z), # a place of interest to flock around. if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : # A boid that is perching will continue to do so # until Boid._perch_t reaches zero. if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
13262	def task ( func , * * config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
6433	def eudex_hamming ( src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : return Eudex ( ) . dist_abs ( src , tar , weights , max_length , normalized )
9498	def parse_litezip ( path ) : struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( x ) for x in path . iterdir ( ) if x . is_dir ( ) and x . name . startswith ( 'm' ) ] ) return tuple ( sorted ( struct ) )
5029	def transmit_content_metadata ( self , user ) : exporter = self . get_content_metadata_exporter ( user ) transmitter = self . get_content_metadata_transmitter ( ) transmitter . transmit ( exporter . export ( ) )
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
896	def read ( cls , proto ) : tm = object . __new__ ( cls ) # capnp fails to save a tuple, so proto.columnDimensions was forced to # serialize as a list. We prefer a tuple, however, because columnDimensions # should be regarded as immutable. tm . columnDimensions = tuple ( proto . columnDimensions ) tm . cellsPerColumn = int ( proto . cellsPerColumn ) tm . activationThreshold = int ( proto . activationThreshold ) tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) tm . minThreshold = int ( proto . minThreshold ) tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , EPSILON_ROUND ) tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) tm . connections = Connections . read ( proto . connections ) #pylint: disable=W0212 tm . _random = Random ( ) tm . _random . read ( proto . random ) #pylint: enable=W0212 tm . activeCells = [ int ( x ) for x in proto . activeCells ] tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] flatListLength = tm . connections . segmentFlatListLength ( ) tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength tm . lastUsedIterationForSegment = [ 0 ] * flatListLength tm . activeSegments = [ ] tm . matchingSegments = [ ] for protoSegment in proto . activeSegments : tm . activeSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . matchingSegments : tm . matchingSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . numActivePotentialSynapsesForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( int ( protoSegment . number ) ) tm . iteration = long ( proto . iteration ) for protoSegment in proto . lastUsedIterationForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( long ( protoSegment . number ) ) return tm
9946	def cur_space ( self , name = None ) : if name is None : return self . _impl . model . currentspace . interface else : self . _impl . model . currentspace = self . _impl . spaces [ name ] return self . cur_space ( )
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
12004	def _read_header ( self , data ) : # pylint: disable=W0212 version = self . _read_version ( data ) version_info = self . _get_version_info ( version ) header_data = data [ : version_info [ 'header_size' ] ] header = version_info [ 'header' ] header = header . _make ( unpack ( version_info [ 'header_format' ] , header_data ) ) header = dict ( header . _asdict ( ) ) flags = list ( "{0:0>8b}" . format ( header [ 'flags' ] ) ) flags = dict ( version_info [ 'flags' ] . _make ( flags ) . _asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts_start = version_info [ 'header_size' ] ts_end = ts_start + version_info [ 'timestamp_size' ] timestamp_data = data [ ts_start : ts_end ] timestamp = unpack ( version_info [ 'timestamp_format' ] , timestamp_data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
6093	def grid_angle_to_profile ( self , grid_thetas ) : theta_coordinate_to_profile = np . add ( grid_thetas , - self . phi_radians ) return np . cos ( theta_coordinate_to_profile ) , np . sin ( theta_coordinate_to_profile )
12339	def compress ( images , delete_tif = False , folder = None ) : if type ( images ) == str : # only one image return [ compress_blocking ( images , delete_tif , folder ) ] filenames = copy ( images ) # as images property will change when looping return Parallel ( n_jobs = _pools ) ( delayed ( compress_blocking ) ( image = image , delete_tif = delete_tif , folder = folder ) for image in filenames )
3059	def _get_backend ( filename ) : filename = os . path . abspath ( filename ) with _backends_lock : if filename not in _backends : _backends [ filename ] = _MultiprocessStorageBackend ( filename ) return _backends [ filename ]
10036	def execute ( helper , config , args ) : environment_name = args . environment ( events , next_token ) = helper . describe_events ( environment_name , start_time = datetime . now ( ) . isoformat ( ) ) # swap C-Names for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
8473	def _addConfig ( instance , config , parent_section ) : try : section_name = "{p}/{n}" . format ( p = parent_section , n = instance . NAME . lower ( ) ) config . add_section ( section_name ) for k in instance . CONFIG . keys ( ) : config . set ( section_name , k , instance . CONFIG [ k ] ) except Exception as e : print "[!] %s" % e
2931	def pre_parse_and_validate_signavio ( self , bpmn , filename ) : self . _check_for_disconnected_boundary_events_signavio ( bpmn , filename ) self . _fix_call_activities_signavio ( bpmn , filename ) return bpmn
13181	def _get_printable_columns ( columns , row ) : if not columns : return row # Extract the column values, in the order specified. return tuple ( row [ c ] for c in columns )
11961	def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
1730	def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
10714	def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
11812	def present ( self , results ) : for ( score , d ) in results : doc = self . documents [ d ] print ( "%5.2f|%25s | %s" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )
10622	def get_element_mass ( self , element ) : result = numpy . zeros ( 1 ) for compound in self . material . compounds : result += self . get_compound_mass ( compound ) * numpy . array ( stoich . element_mass_fractions ( compound , [ element ] ) ) return result [ 0 ]
12322	def to_holvi_dict ( self ) : self . _jsondata [ "items" ] = [ ] for item in self . items : self . _jsondata [ "items" ] . append ( item . to_holvi_dict ( ) ) self . _jsondata [ "issue_date" ] = self . issue_date . isoformat ( ) self . _jsondata [ "due_date" ] = self . due_date . isoformat ( ) self . _jsondata [ "receiver" ] = self . receiver . to_holvi_dict ( ) return { k : v for ( k , v ) in self . _jsondata . items ( ) if k in self . _valid_keys }
7705	def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid ) index = self . _jids [ jid ] for i in range ( index , len ( self . _jids ) ) : self . _jids [ self . _items [ i ] . jid ] -= 1 del self . _jids [ jid ] del self . _items [ index ]
13771	def collect_links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has_bundles ( ) : asset . collect_files ( ) if env is None : env = self . config . env if env == static_bundle . ENV_PRODUCTION : self . _minify ( emulate = True ) self . _add_url_prefix ( )
601	def addGraph ( self , data , position = 111 , xlabel = None , ylabel = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . plot ( data ) plt . draw ( )
2488	def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
13234	def make_aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : # available for pytz time zones return timezone . localize ( value , is_dst = None ) else : # may be wrong around DST changes return value . replace ( tzinfo = timezone )
10238	def count_citations ( graph : BELGraph , * * annotations ) -> Counter : citations = defaultdict ( set ) annotation_dict_filter = build_edge_data_filter ( annotations ) for u , v , _ , d in filter_edges ( graph , annotation_dict_filter ) : if CITATION not in d : continue citations [ u , v ] . add ( ( d [ CITATION ] [ CITATION_TYPE ] , d [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return Counter ( itt . chain . from_iterable ( citations . values ( ) ) )
4721	def trun_exit ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun:exit" ) rcode = 0 for hook in reversed ( trun [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::exit { rcode: %r }" % rcode , rcode ) return rcode
5185	def catalog ( self , node ) : catalogs = self . catalogs ( path = node ) return next ( x for x in catalogs )
6432	def encode ( self , word ) : word = word . upper ( ) # Rule 3 word = self . _delete_consecutive_repeats ( word ) # Rule 4 # Rule 5 i = 0 while i < len ( word ) : for match_len in range ( 4 , 1 , - 1 ) : if word [ i : i + match_len ] in self . _rules [ match_len ] : repl = self . _rules [ match_len ] [ word [ i : i + match_len ] ] word = word [ : i ] + repl + word [ i + match_len : ] i += len ( repl ) break else : i += 1 word = word [ : 1 ] + word [ 1 : ] . translate ( self . _del_trans ) # Rule 6 return word
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) # Windows and OS X if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
6112	def single_value ( cls , value , shape , pixel_scale , origin = ( 0.0 , 0.0 ) ) : array = np . ones ( shape ) * value return cls ( array , pixel_scale , origin )
4204	def rlevinson ( a , efinal ) : a = numpy . array ( a ) realdata = numpy . isrealobj ( a ) assert a [ 0 ] == 1 , 'First coefficient of the prediction polynomial must be unity' p = len ( a ) if p < 2 : raise ValueError ( 'Polynomial should have at least two coefficients' ) if realdata == True : U = numpy . zeros ( ( p , p ) ) # This matrix will have the prediction # polynomials of orders 1:p else : U = numpy . zeros ( ( p , p ) , dtype = complex ) U [ : , p - 1 ] = numpy . conj ( a [ - 1 : : - 1 ] ) # Prediction coefficients of order p p = p - 1 e = numpy . zeros ( p ) # First we find the prediction coefficients of smaller orders and form the # Matrix U # Initialize the step down e [ - 1 ] = efinal # Prediction error of order p # Step down for k in range ( p - 1 , 0 , - 1 ) : [ a , e [ k - 1 ] ] = levdown ( a , e [ k ] ) U [ : , k ] = numpy . concatenate ( ( numpy . conj ( a [ - 1 : : - 1 ] . transpose ( ) ) , [ 0 ] * ( p - k ) ) ) e0 = e [ 0 ] / ( 1. - abs ( a [ 1 ] ** 2 ) ) #% Because a[1]=1 (true polynomial) U [ 0 , 0 ] = 1 #% Prediction coefficient of zeroth order kr = numpy . conj ( U [ 0 , 1 : ] ) #% The reflection coefficients kr = kr . transpose ( ) #% To make it into a column vector # % Once we have the matrix U and the prediction error at various orders, we can # % use this information to find the autocorrelation coefficients. R = numpy . zeros ( 1 , dtype = complex ) #% Initialize recursion k = 1 R0 = e0 # To take care of the zero indexing problem R [ 0 ] = - numpy . conj ( U [ 0 , 1 ] ) * R0 # R[1]=-a1[1]*R[0] # Actual recursion for k in range ( 1 , p ) : r = - sum ( numpy . conj ( U [ k - 1 : : - 1 , k ] ) * R [ - 1 : : - 1 ] ) - kr [ k ] * e [ k - 1 ] R = numpy . insert ( R , len ( R ) , r ) # Include R(0) and make it a column vector. Note the dot transpose #R = [R0 R].'; R = numpy . insert ( R , 0 , e0 ) return R , U , kr , e
11581	def run ( self ) : # To add a command to the command dispatch table, append here. self . command_dispatch . update ( { self . REPORT_VERSION : [ self . report_version , 2 ] } ) self . command_dispatch . update ( { self . REPORT_FIRMWARE : [ self . report_firmware , 1 ] } ) self . command_dispatch . update ( { self . ANALOG_MESSAGE : [ self . analog_message , 2 ] } ) self . command_dispatch . update ( { self . DIGITAL_MESSAGE : [ self . digital_message , 2 ] } ) self . command_dispatch . update ( { self . ENCODER_DATA : [ self . encoder_data , 3 ] } ) self . command_dispatch . update ( { self . SONAR_DATA : [ self . sonar_data , 3 ] } ) self . command_dispatch . update ( { self . STRING_DATA : [ self . _string_data , 2 ] } ) self . command_dispatch . update ( { self . I2C_REPLY : [ self . i2c_reply , 2 ] } ) self . command_dispatch . update ( { self . CAPABILITY_RESPONSE : [ self . capability_response , 2 ] } ) self . command_dispatch . update ( { self . PIN_STATE_RESPONSE : [ self . pin_state_response , 2 ] } ) self . command_dispatch . update ( { self . ANALOG_MAPPING_RESPONSE : [ self . analog_mapping_response , 2 ] } ) self . command_dispatch . update ( { self . STEPPER_DATA : [ self . stepper_version_response , 2 ] } ) while not self . is_stopped ( ) : if len ( self . pymata . command_deque ) : # get next byte from the deque and process it data = self . pymata . command_deque . popleft ( ) # this list will be populated with the received data for the command command_data = [ ] # process sysex commands if data == self . START_SYSEX : # next char is the actual sysex command # wait until we can get data from the deque while len ( self . pymata . command_deque ) == 0 : pass sysex_command = self . pymata . command_deque . popleft ( ) # retrieve the associated command_dispatch entry for this command dispatch_entry = self . command_dispatch . get ( sysex_command ) # get a "pointer" to the method that will process this command method = dispatch_entry [ 0 ] # now get the rest of the data excluding the END_SYSEX byte end_of_sysex = False while not end_of_sysex : # wait for more data to arrive while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) if data != self . END_SYSEX : command_data . append ( data ) else : end_of_sysex = True # invoke the method to process the command method ( command_data ) # go to the beginning of the loop to process the next command continue # is this a command byte in the range of 0x80-0xff - these are the non-sysex messages elif 0x80 <= data <= 0xff : # look up the method for the command in the command dispatch table # for the digital reporting the command value is modified with port number # the handler needs the port to properly process, so decode that from the command and # place in command_data if 0x90 <= data <= 0x9f : port = data & 0xf command_data . append ( port ) data = 0x90 # the pin number for analog data is embedded in the command so, decode it elif 0xe0 <= data <= 0xef : pin = data & 0xf command_data . append ( pin ) data = 0xe0 else : pass dispatch_entry = self . command_dispatch . get ( data ) # this calls the method retrieved from the dispatch table method = dispatch_entry [ 0 ] # get the number of parameters that this command provides num_args = dispatch_entry [ 1 ] # look at the number of args that the selected method requires # now get that number of bytes to pass to the called method for i in range ( num_args ) : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) command_data . append ( data ) # go execute the command with the argument list method ( command_data ) # go to the beginning of the loop to process the next command continue else : time . sleep ( .1 )
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
4450	def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
2736	def create ( self , * args , * * kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'droplet_id' : self . droplet_id } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
11636	def refresh_access_token ( self , ) : logger . debug ( "REFRESHING TOKEN" ) self . token_time = time . time ( ) credentials = { 'token_time' : self . token_time } if self . oauth_version == 'oauth1' : self . access_token , self . access_token_secret = self . oauth . get_access_token ( self . access_token , self . access_token_secret , params = { "oauth_session_handle" : self . session_handle } ) credentials . update ( { 'access_token' : self . access_token , 'access_token_secret' : self . access_token_secret , 'session_handle' : self . session_handle , 'token_time' : self . token_time } ) else : headers = self . generate_oauth2_headers ( ) raw_access = self . oauth . get_raw_access_token ( data = { "refresh_token" : self . refresh_token , 'redirect_uri' : self . callback_uri , 'grant_type' : 'refresh_token' } , headers = headers ) credentials . update ( self . oauth2_access_parser ( raw_access ) ) return credentials
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
8732	def divide_timedelta_float ( td , divisor ) : # td is comprised of days, seconds, microseconds dsm = [ getattr ( td , attr ) for attr in ( 'days' , 'seconds' , 'microseconds' ) ] dsm = map ( lambda elem : elem / divisor , dsm ) return datetime . timedelta ( * dsm )
5041	def enroll_users_in_program ( cls , enterprise_customer , program_details , course_mode , emails , cohort = None ) : existing_users , unregistered_emails = cls . get_users_by_email ( emails ) course_ids = get_course_runs_from_program ( program_details ) successes = [ ] pending = [ ] failures = [ ] for user in existing_users : succeeded = cls . enroll_user ( enterprise_customer , user , course_mode , * course_ids ) if succeeded : successes . append ( user ) else : failures . append ( user ) for email in unregistered_emails : pending_user = enterprise_customer . enroll_user_pending_registration ( email , course_mode , * course_ids , cohort = cohort ) pending . append ( pending_user ) return successes , pending , failures
10156	def merge_dicts ( base , changes ) : for k , v in changes . items ( ) : if isinstance ( v , dict ) : merge_dicts ( base . setdefault ( k , { } ) , v ) else : base . setdefault ( k , v )
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
12478	def get_sys_path ( rcpath , app_name , section_name = None ) : # first check if it is an existing path if op . exists ( rcpath ) : return op . realpath ( op . expanduser ( rcpath ) ) # look for the rcfile try : settings = rcfile ( app_name , section_name ) except : raise # look for the variable within the rcfile configutarions try : sys_path = op . expanduser ( settings [ rcpath ] ) except KeyError : raise IOError ( 'Could not find an existing variable with name {0} in' ' section {1} of {2}rc config setup. Maybe it is a ' ' folder that could not be found.' . format ( rcpath , section_name , app_name ) ) # found the variable, now check if it is an existing path else : if not op . exists ( sys_path ) : raise IOError ( 'Could not find the path {3} indicated by the ' 'variable {0} in section {1} of {2}rc config ' 'setup.' . format ( rcpath , section_name , app_name , sys_path ) ) # expand the path and return return op . realpath ( op . expanduser ( sys_path ) )
12604	def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
10372	def node_has_namespace ( node : BaseEntity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : # pylint: disable=len-as-condition return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
737	def _mergeFiles ( key , chunkCount , outputFile , fields ) : title ( ) # Open all chun files files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] # Open output file with FileRecordStream ( outputFile , write = True , fields = fields ) as o : # Open all chunk files files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] records = [ f . getNextRecord ( ) for f in files ] # This loop will run until all files are exhausted while not all ( r is None for r in records ) : # Cleanup None values (files that were exhausted) indices = [ i for i , r in enumerate ( records ) if r is not None ] records = [ records [ i ] for i in indices ] files = [ files [ i ] for i in indices ] # Find the current record r = min ( records , key = itemgetter ( * key ) ) # Write it to the file o . appendRecord ( r ) # Find the index of file that produced the current record index = records . index ( r ) # Read a new record from the file records [ index ] = files [ index ] . getNextRecord ( ) # Cleanup chunk files for i , f in enumerate ( files ) : f . close ( ) os . remove ( 'chunk_%d.csv' % i )
6974	def rfepd_magseries ( times , mags , errs , externalparam_arrs , magsarefluxes = False , epdsmooth = True , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , rf_subsample = 1.0 , rf_ntrees = 300 , rf_extraparams = { 'criterion' : 'mse' , 'oob_score' : False , 'n_jobs' : - 1 } ) : # get finite times, mags, errs finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] finalparam_arrs = [ ] for ep in externalparam_arrs : finalparam_arrs . append ( ep [ : : ] [ finind ] ) stimes , smags , serrs , eparams = sigclip_magseries_with_extparams ( times , mags , errs , externalparam_arrs , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) # smoothing is optional for RFR because we train on a fraction of the mag # series and so should not require a smoothed input to fit a function to if epdsmooth : # smooth the signal if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , * * epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) else : smoothedmags = smags # set up the regressor if isinstance ( rf_extraparams , dict ) : RFR = RandomForestRegressor ( n_estimators = rf_ntrees , * * rf_extraparams ) else : RFR = RandomForestRegressor ( n_estimators = rf_ntrees ) # collect the features features = np . column_stack ( eparams ) # fit, then generate the predicted values, then get corrected values # we fit on a randomly selected subsample of all the mags if rf_subsample < 1.0 : featureindices = np . arange ( smoothedmags . size ) # these are sorted because time-order should be important training_indices = np . sort ( npr . choice ( featureindices , size = int ( rf_subsample * smoothedmags . size ) , replace = False ) ) else : training_indices = np . arange ( smoothedmags . size ) RFR . fit ( features [ training_indices , : ] , smoothedmags [ training_indices ] ) # predict on the full feature set flux_corrections = RFR . predict ( np . column_stack ( finalparam_arrs ) ) corrected_fmags = npmedian ( fmags ) + fmags - flux_corrections retdict = { 'times' : ftimes , 'mags' : corrected_fmags , 'errs' : ferrs , 'feature_importances' : RFR . feature_importances_ , 'regressor' : RFR , 'mags_median' : npmedian ( corrected_fmags ) , 'mags_mad' : npmedian ( npabs ( corrected_fmags - npmedian ( corrected_fmags ) ) ) } return retdict
2700	def text_rank ( path ) : graph = build_graph ( json_iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks
3839	async def set_presence ( self , set_presence_request ) : response = hangouts_pb2 . SetPresenceResponse ( ) await self . _pb_request ( 'presence/setpresence' , set_presence_request , response ) return response
4341	def repeat ( self , count = 1 ) : if not isinstance ( count , int ) or count < 1 : raise ValueError ( "count must be a postive integer." ) effect_args = [ 'repeat' , '{}' . format ( count ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'repeat' )
4128	def readwav ( filename ) : from scipy . io . wavfile import read as readwav samplerate , signal = readwav ( filename ) return signal , samplerate
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : return func ( * args , * * kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
11697	def count ( self ) : xml = get_changeset ( self . id ) actions = [ action . tag for action in xml . getchildren ( ) ] self . create = actions . count ( 'create' ) self . modify = actions . count ( 'modify' ) self . delete = actions . count ( 'delete' ) self . verify_editor ( ) try : if ( self . create / len ( actions ) > self . percentage and self . create > self . create_threshold and ( self . powerfull_editor or self . create > self . top_threshold ) ) : self . label_suspicious ( 'possible import' ) elif ( self . modify / len ( actions ) > self . percentage and self . modify > self . modify_threshold ) : self . label_suspicious ( 'mass modification' ) elif ( ( self . delete / len ( actions ) > self . percentage and self . delete > self . delete_threshold ) or self . delete > self . top_threshold ) : self . label_suspicious ( 'mass deletion' ) except ZeroDivisionError : print ( 'It seems this changeset was redacted' )
2668	def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
6334	def dist ( self , src , tar ) : if src == tar : return 0.0 return self . dist_abs ( src , tar ) / ( len ( src ) + len ( tar ) )
6715	def install ( packages , upgrade = False , use_sudo = False , python_cmd = 'python' ) : argv = [ ] if upgrade : argv . append ( "-U" ) if isinstance ( packages , six . string_types ) : argv . append ( packages ) else : argv . extend ( packages ) _easy_install ( argv , python_cmd , use_sudo )
8816	def update_network ( context , id , network ) : LOG . info ( "update_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) net_dict = network [ "network" ] utils . pop_param ( net_dict , "network_plugin" ) if not context . is_admin and "ipam_strategy" in net_dict : utils . pop_param ( net_dict , "ipam_strategy" ) net = db_api . network_update ( context , net , * * net_dict ) return v . _make_network_dict ( net )
2761	def get_certificate ( self , id ) : return Certificate . get_object ( api_token = self . token , cert_id = id )
2684	def cached_download ( url , name ) : clean_name = os . path . normpath ( name ) if clean_name != name : raise ValueError ( "{} is not normalized." . format ( name ) ) for dir_ in iter_data_dirs ( ) : path = os . path . join ( dir_ , name ) if os . path . exists ( path ) : return path dir_ = next ( iter_data_dirs ( True ) ) path = os . path . join ( dir_ , name ) log . info ( "Downloading {} to {}" . format ( url , path ) ) response = urlopen ( url ) if response . getcode ( ) != 200 : raise ValueError ( "HTTP {}" . format ( response . getcode ( ) ) ) dir_ = os . path . dirname ( path ) try : os . makedirs ( dir_ ) except OSError as e : if e . errno != errno . EEXIST : raise tmp_path = path + '.tmp' with open ( tmp_path , 'wb' ) as fh : while True : chunk = response . read ( 8196 ) if chunk : fh . write ( chunk ) else : break os . rename ( tmp_path , path ) return path
10583	def create_account ( self , name , number = None , description = None ) : new_account = GeneralLedgerAccount ( name , description , number , self . account_type ) new_account . set_parent_path ( self . path ) self . accounts . append ( new_account ) return new_account
3451	def find_blocked_reactions ( model , reaction_list = None , zero_cutoff = None , open_exchanges = False , processes = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) with model : if open_exchanges : for reaction in model . exchanges : reaction . bounds = ( min ( reaction . lower_bound , - 1000 ) , max ( reaction . upper_bound , 1000 ) ) if reaction_list is None : reaction_list = model . reactions # Limit the search space to reactions which have zero flux. If the # reactions already carry flux in this solution, # then they cannot be blocked. model . slim_optimize ( ) solution = get_solution ( model , reactions = reaction_list ) reaction_list = solution . fluxes [ solution . fluxes . abs ( ) < zero_cutoff ] . index . tolist ( ) # Run FVA to find reactions where both the minimal and maximal flux # are zero (below the cut off). flux_span = flux_variability_analysis ( model , fraction_of_optimum = 0. , reaction_list = reaction_list , processes = processes ) return flux_span [ flux_span . abs ( ) . max ( axis = 1 ) < zero_cutoff ] . index . tolist ( )
12708	def body_to_world ( self , position ) : return np . array ( self . ode_body . getRelPointPos ( tuple ( position ) ) )
13819	def _ConvertValueMessage ( value , message ) : if isinstance ( value , dict ) : _ConvertStructMessage ( value , message . struct_value ) elif isinstance ( value , list ) : _ConvertListValueMessage ( value , message . list_value ) elif value is None : message . null_value = 0 elif isinstance ( value , bool ) : message . bool_value = value elif isinstance ( value , six . string_types ) : message . string_value = value elif isinstance ( value , _INT_OR_FLOAT ) : message . number_value = value else : raise ParseError ( 'Unexpected type for Value message.' )
909	def __advancePhase ( self ) : self . __currentPhase = self . __phaseCycler . next ( ) self . __currentPhase . enterPhase ( ) return
8879	def fit ( self , X , y = None ) : # Check data X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
4526	def run ( self , next_task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next_task . event . set ( )
5437	def args_to_job_params ( envs , labels , inputs , inputs_recursive , outputs , outputs_recursive , mounts , input_file_param_util , output_file_param_util , mount_param_util ) : # Parse environmental variables and labels. env_data = parse_pair_args ( envs , job_model . EnvParam ) label_data = parse_pair_args ( labels , job_model . LabelParam ) # For input files, we need to: # * split the input into name=uri pairs (name optional) # * get the environmental variable name, or automatically set if null. # * create the input file param input_data = set ( ) for ( recursive , args ) in ( ( False , inputs ) , ( True , inputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , nullable_idx = 0 ) name = input_file_param_util . get_variable_name ( name ) input_data . add ( input_file_param_util . make_param ( name , value , recursive ) ) # For output files, we need to: # * split the input into name=uri pairs (name optional) # * get the environmental variable name, or automatically set if null. # * create the output file param output_data = set ( ) for ( recursive , args ) in ( ( False , outputs ) , ( True , outputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , 0 ) name = output_file_param_util . get_variable_name ( name ) output_data . add ( output_file_param_util . make_param ( name , value , recursive ) ) mount_data = set ( ) for arg in mounts : # Mounts can look like `--mount VAR=PATH` or `--mount VAR=PATH {num}`, # where num is the size of the disk in Gb. We assume a space is the # separator between path and disk size. if ' ' in arg : key_value_pair , disk_size = arg . split ( ' ' ) name , value = split_pair ( key_value_pair , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size ) ) else : name , value = split_pair ( arg , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size = None ) ) return { 'envs' : env_data , 'inputs' : input_data , 'outputs' : output_data , 'labels' : label_data , 'mounts' : mount_data , }
1642	def CheckBracesSpacing ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] # Except after an opening paren, or after another opening brace (in case of # an initializer list, for instance), you should have spaces before your # braces when they are delimiting blocks, classes, namespaces etc. # And since you should never have braces at the beginning of a line, # this is an easy test. Except that braces used for initialization don't # follow the same rule; we often don't want spaces before those. match = Match ( r'^(.*[^ ({>]){' , line ) if match : # Try a bit harder to check for brace initialization. This # happens in one of the following forms: # Constructor() : initializer_list_{} { ... } # Constructor{}.MemberFunction() # Type variable{}; # FunctionCall(type{}, ...); # LastArgument(..., type{}); # LOG(INFO) << type{} << " ..."; # map_of_type[{...}] = ...; # ternary = expr ? new type{} : nullptr; # OuterTemplate<InnerTemplateConstructor<Type>{}> # # We check for the character following the closing brace, and # silence the warning if it's one of those listed above, i.e. # "{.;,)<>]:". # # To account for nested initializer list, we allow any number of # closing braces up to "{;,)<". We can't simply silence the # warning on first sight of closing brace, because that would # cause false negatives for things that are not initializer lists. # Silence this: But not this: # Outer{ if (...) { # Inner{...} if (...){ // Missing space before { # }; } # # There is a false negative with this approach if people inserted # spurious semicolons, e.g. "if (cond){};", but we will catch the # spurious semicolon with a separate check. leading_text = match . group ( 1 ) ( endline , endlinenum , endpos ) = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) trailing_text = '' if endpos > - 1 : trailing_text = endline [ endpos : ] for offset in xrange ( endlinenum + 1 , min ( endlinenum + 3 , clean_lines . NumLines ( ) - 1 ) ) : trailing_text += clean_lines . elided [ offset ] # We also suppress warnings for `uint64_t{expression}` etc., as the style # guide recommends brace initialization for integral types to avoid # overflow/truncation. if ( not Match ( r'^[\s}]*[{.;,)<>\]:]' , trailing_text ) and not _IsType ( clean_lines , nesting_state , leading_text ) ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before {' ) # Make sure '} else {' has spaces. if Search ( r'}else' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before else' ) # You shouldn't have a space before a semicolon at the end of the line. # There's a special case for "for" since the style guide allows space before # the semicolon there. if Search ( r':\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Semicolon defining empty statement. Use {} instead.' ) elif Search ( r'^\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Line contains only semicolon. If this should be an empty statement, ' 'use {} instead.' ) elif ( Search ( r'\s+;\s*$' , line ) and not Search ( r'\bfor\b' , line ) ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Extra space before last semicolon. If this should be an empty ' 'statement, use {} instead.' )
13193	def geom_to_xml_element ( geom ) : if geom . srs . srid != 4326 : raise NotImplementedError ( "Only WGS 84 lat/long geometries (SRID 4326) are supported." ) # GeoJSON output is far more standard than GML, so go through that return geojson_to_gml ( json . loads ( geom . geojson ) )
4056	def _json_processor ( self , retrieved ) : json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict # send entries to _tags_data if there's no JSON try : items = [ json . loads ( e [ "content" ] [ 0 ] [ "value" ] , * * json_kwargs ) for e in retrieved . entries ] except KeyError : return self . _tags_data ( retrieved ) return items
12165	def once ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _once [ event ] . append ( listener ) self . _check_limit ( event ) return self
8026	def getPaths ( roots , ignores = None ) : paths , count , ignores = [ ] , 0 , ignores or [ ] # Prepare the ignores list for most efficient use ignore_re = multiglob_compile ( ignores , prefix = False ) for root in roots : # For safety, only use absolute, real paths. root = os . path . realpath ( root ) # Handle directly-referenced filenames properly # (And override ignores to "do as I mean, not as I say") if os . path . isfile ( root ) : paths . append ( root ) continue for fldr in os . walk ( root ) : out . write ( "Gathering file paths to compare... (%d files examined)" % count ) # Don't even descend into IGNOREd directories. for subdir in fldr [ 1 ] : dirpath = os . path . join ( fldr [ 0 ] , subdir ) if ignore_re . match ( dirpath ) : fldr [ 1 ] . remove ( subdir ) for filename in fldr [ 2 ] : filepath = os . path . join ( fldr [ 0 ] , filename ) if ignore_re . match ( filepath ) : continue # Skip IGNOREd files. paths . append ( filepath ) count += 1 out . write ( "Found %s files to be compared for duplication." % ( len ( paths ) ) , newline = True ) return paths
4119	def twosided_2_onesided ( data ) : assert len ( data ) % 2 == 0 N = len ( data ) psd = np . array ( data [ 0 : N // 2 + 1 ] ) * 2. psd [ 0 ] /= 2. psd [ - 1 ] = data [ - 1 ] return psd
3001	def cryptoDF ( token = '' , version = '' ) : df = pd . DataFrame ( crypto ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
11388	def parse ( self ) : if self . parsed : return self . callbacks = { } # search for main and any main_* callable objects regex = re . compile ( "^{}_?" . format ( self . function_name ) , flags = re . I ) mains = set ( ) body = self . body ast_tree = ast . parse ( self . body , self . path ) for n in ast_tree . body : if hasattr ( n , 'name' ) : if regex . match ( n . name ) : mains . add ( n . name ) if hasattr ( n , 'value' ) : ns = n . value if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'targets' ) : ns = n . targets [ 0 ] if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'names' ) : for ns in n . names : if hasattr ( ns , 'name' ) : if regex . match ( ns . name ) : mains . add ( ns . name ) if getattr ( ns , 'asname' , None ) : if regex . match ( ns . asname ) : mains . add ( ns . asname ) if len ( mains ) > 0 : module = self . module for function_name in mains : cb = getattr ( module , function_name , None ) if cb and callable ( cb ) : self . callbacks [ function_name ] = cb else : raise ParseError ( "no main function found" ) self . parsed = True return len ( self . callbacks ) > 0
3861	def add_event ( self , event_ ) : conv_event = self . _wrap_event ( event_ ) if conv_event . id_ not in self . _events_dict : self . _events . append ( conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : # If this happens, there's probably a bug. logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return None return conv_event
12450	def _get_effect_statement ( self , effect , methods ) : statements = [ ] if len ( methods ) > 0 : statement = self . _get_empty_statement ( effect ) for method in methods : if ( method [ 'conditions' ] is None or len ( method [ 'conditions' ] ) == 0 ) : statement [ 'Resource' ] . append ( method [ 'resource_arn' ] ) else : cond_statement = self . _get_empty_statement ( effect ) cond_statement [ 'Resource' ] . append ( method [ 'resource_arn' ] ) cond_statement [ 'Condition' ] = method [ 'conditions' ] statements . append ( cond_statement ) statements . append ( statement ) return statements
1724	def execute ( self , js = None , use_compilation_plan = False ) : try : cache = self . __dict__ [ 'cache' ] except KeyError : cache = self . __dict__ [ 'cache' ] = { } hashkey = hashlib . md5 ( js . encode ( 'utf-8' ) ) . digest ( ) try : compiled = cache [ hashkey ] except KeyError : code = translate_js ( js , '' , use_compilation_plan = use_compilation_plan ) compiled = cache [ hashkey ] = compile ( code , '<EvalJS snippet>' , 'exec' ) exec ( compiled , self . _context )
10259	def get_modifications_count ( graph : BELGraph ) -> Mapping [ str , int ] : return remove_falsy_values ( { 'Translocations' : len ( get_translocated ( graph ) ) , 'Degradations' : len ( get_degradations ( graph ) ) , 'Molecular Activities' : len ( get_activities ( graph ) ) , } )
8194	def _density ( self ) : return 2.0 * len ( self . edges ) / ( len ( self . nodes ) * ( len ( self . nodes ) - 1 ) )
10805	def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]
12749	def load_asf ( self , source , * * kwargs ) : if hasattr ( source , 'read' ) : p = parser . parse_asf ( source , self . world , self . jointgroup , * * kwargs ) else : with open ( source ) as handle : p = parser . parse_asf ( handle , self . world , self . jointgroup , * * kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
1630	def CheckForHeaderGuard ( filename , clean_lines , error ) : # Don't check for header guards if there are error suppression # comments somewhere in this file. # # Because this is silencing a warning for a nonexistent line, we # only support the very specific NOLINT(build/header_guard) syntax, # and not the general NOLINT or NOLINT(*) syntax. raw_lines = clean_lines . lines_without_raw_strings for i in raw_lines : if Search ( r'//\s*NOLINT\(build/header_guard\)' , i ) : return # Allow pragma once instead of header guards for i in raw_lines : if Search ( r'^\s*#pragma\s+once' , i ) : return cppvar = GetHeaderGuardCPPVariable ( filename ) ifndef = '' ifndef_linenum = 0 define = '' endif = '' endif_linenum = 0 for linenum , line in enumerate ( raw_lines ) : linesplit = line . split ( ) if len ( linesplit ) >= 2 : # find the first occurrence of #ifndef and #define, save arg if not ifndef and linesplit [ 0 ] == '#ifndef' : # set ifndef to the header guard presented on the #ifndef line. ifndef = linesplit [ 1 ] ifndef_linenum = linenum if not define and linesplit [ 0 ] == '#define' : define = linesplit [ 1 ] # find the last occurrence of #endif, save entire line if line . startswith ( '#endif' ) : endif = line endif_linenum = linenum if not ifndef or not define or ifndef != define : error ( filename , 0 , 'build/header_guard' , 5 , 'No #ifndef header guard found, suggested CPP variable is: %s' % cppvar ) return # The guard should be PATH_FILE_H_, but we also allow PATH_FILE_H__ # for backward compatibility. if ifndef != cppvar : error_level = 0 if ifndef != cppvar + '_' : error_level = 5 ParseNolintSuppressions ( filename , raw_lines [ ifndef_linenum ] , ifndef_linenum , error ) error ( filename , ifndef_linenum , 'build/header_guard' , error_level , '#ifndef header guard has wrong style, please use: %s' % cppvar ) # Check for "//" comments on endif line. ParseNolintSuppressions ( filename , raw_lines [ endif_linenum ] , endif_linenum , error ) match = Match ( r'#endif\s*//\s*' + cppvar + r'(_)?\b' , endif ) if match : if match . group ( 1 ) == '_' : # Issue low severity warning for deprecated double trailing underscore error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif // %s"' % cppvar ) return # Didn't find the corresponding "//" comment. If this file does not # contain any "//" comments at all, it could be that the compiler # only wants "/**/" comments, look for those instead. no_single_line_comments = True for i in xrange ( 1 , len ( raw_lines ) - 1 ) : line = raw_lines [ i ] if Match ( r'^(?:(?:\'(?:\.|[^\'])*\')|(?:"(?:\.|[^"])*")|[^\'"])*//' , line ) : no_single_line_comments = False break if no_single_line_comments : match = Match ( r'#endif\s*/\*\s*' + cppvar + r'(_)?\s*\*/' , endif ) if match : if match . group ( 1 ) == '_' : # Low severity warning for double trailing underscore error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif /* %s */"' % cppvar ) return # Didn't find anything error ( filename , endif_linenum , 'build/header_guard' , 5 , '#endif line should be "#endif // %s"' % cppvar )
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
1100	def unified_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True # fromdate = '\t{}'.format(fromfiledate) if fromfiledate else '' fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' # todate = '\t{}'.format(tofiledate) if tofiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' # yield '--- {}{}{}'.format(fromfile, fromdate, lineterm) yield '--- %s%s%s' % ( fromfile , fromdate , lineterm ) # yield '+++ {}{}{}'.format(tofile, todate, lineterm) yield '+++ %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] file1_range = _format_range_unified ( first [ 1 ] , last [ 2 ] ) file2_range = _format_range_unified ( first [ 3 ] , last [ 4 ] ) # yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm) yield '@@ -%s +%s @@%s' % ( file1_range , file2_range , lineterm ) for tag , i1 , i2 , j1 , j2 in group : if tag == 'equal' : for line in a [ i1 : i2 ] : yield ' ' + line continue if tag in ( 'replace' , 'delete' ) : for line in a [ i1 : i2 ] : yield '-' + line if tag in ( 'replace' , 'insert' ) : for line in b [ j1 : j2 ] : yield '+' + line
1772	def push ( cpu , value , size ) : assert size in ( 8 , 16 , cpu . address_bit_size ) cpu . STACK = cpu . STACK - size // 8 base , _ , _ = cpu . get_descriptor ( cpu . read_register ( 'SS' ) ) address = cpu . STACK + base cpu . write_int ( address , value , size )
11413	def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
4139	def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : # create something to replace the thumbnail default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
9924	def create ( self , * args , * * kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , * * kwargs ) if is_primary : email . set_primary ( ) return email
5730	def read ( self , count ) : new_index = self . index + count if new_index > self . len : buf = self . raw_text [ self . index : ] # return to the end, don't fail else : buf = self . raw_text [ self . index : new_index ] self . index = new_index return buf
12778	def render ( self , dt ) : for frame in self . _frozen : for body in frame : self . draw_body ( body ) for body in self . world . bodies : self . draw_body ( body ) if hasattr ( self . world , 'markers' ) : # draw line between anchor1 and anchor2 for marker joints. window . glColor4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . glLineWidth ( 3 ) for j in self . world . markers . joints . values ( ) : window . glBegin ( window . GL_LINES ) window . glVertex3f ( * j . getAnchor ( ) ) window . glVertex3f ( * j . getAnchor2 ( ) ) window . glEnd ( )
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : ## do not allow bad vals if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
2461	def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) # A file name marks the start of a new file instance. # The builder must be reset # FIXME: this state does not make sense self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
2296	def featurize_row ( self , x , y ) : x = x . ravel ( ) y = y . ravel ( ) b = np . ones ( x . shape ) dx = np . cos ( np . dot ( self . W2 , np . vstack ( ( x , b ) ) ) ) . mean ( 1 ) dy = np . cos ( np . dot ( self . W2 , np . vstack ( ( y , b ) ) ) ) . mean ( 1 ) if ( sum ( dx ) > sum ( dy ) ) : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( x , y , b ) ) ) ) . mean ( 1 ) ) ) else : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( y , x , b ) ) ) ) . mean ( 1 ) ) )
1367	def start_connect ( self ) : Log . debug ( "In start_connect() of %s" % self . _get_classname ( ) ) # TODO: specify buffer size, exception handling self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) # when ready, handle_connect is called self . _connecting = True self . connect ( self . endpoint )
8340	def toEncoding ( self , s , encoding = None ) : if isinstance ( s , unicode ) : if encoding : s = s . encode ( encoding ) elif isinstance ( s , str ) : if encoding : s = s . encode ( encoding ) else : s = unicode ( s ) else : if encoding : s = self . toEncoding ( str ( s ) , encoding ) else : s = unicode ( s ) return s
3496	def reaction_weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise ValueError ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula_weight ]
4980	def get ( self , request , enterprise_uuid , course_id ) : enrollment_course_mode = request . GET . get ( 'course_mode' ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) # Redirect the learner to LMS dashboard in case no course mode is # provided as query parameter `course_mode` if not enrollment_course_mode : return redirect ( LMS_DASHBOARD_URL ) enrollment_api_client = EnrollmentApiClient ( ) course_modes = enrollment_api_client . get_course_modes ( course_id ) # Verify that the request user belongs to the enterprise against the # provided `enterprise_uuid`. enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_customer . uuid ) if not course_modes : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTHCE000' log_message = ( 'No course_modes for course_id {course_id} for enterprise_catalog_uuid ' '{enterprise_catalog_uuid}.' 'The following error was presented to ' 'user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_catalog_uuid = enterprise_catalog_uuid , course_id = course_id , error_code = error_code ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'slug' ] == enrollment_course_mode : selected_course_mode = course_mode break if not selected_course_mode : return redirect ( LMS_DASHBOARD_URL ) # Create the Enterprise backend database records for this course # enrollment __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) DataSharingConsent . objects . update_or_create ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer_user . enterprise_customer , defaults = { 'granted' : True } , ) audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' , 'honor' ] ) if selected_course_mode [ 'slug' ] in audit_modes : # In case of Audit course modes enroll the learner directly through # enrollment API client and redirect the learner to dashboard. enrollment_api_client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode [ 'slug' ] ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) # redirect the enterprise learner to the ecommerce flow in LMS # Note: LMS start flow automatically detects the paid mode premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
2267	def invert_dict ( dict_ , unique_vals = True ) : if unique_vals : if isinstance ( dict_ , OrderedDict ) : inverted = OrderedDict ( ( val , key ) for key , val in dict_ . items ( ) ) else : inverted = { val : key for key , val in dict_ . items ( ) } else : # Handle non-unique keys using groups inverted = defaultdict ( set ) for key , value in dict_ . items ( ) : inverted [ value ] . add ( key ) inverted = dict ( inverted ) return inverted
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
3102	def run_flow ( flow , storage , flags = None , http = None ) : if flags is None : flags = argparser . parse_args ( ) logging . getLogger ( ) . setLevel ( getattr ( logging , flags . logging_level ) ) if not flags . noauth_local_webserver : success = False port_number = 0 for port in flags . auth_host_port : port_number = port try : httpd = ClientRedirectServer ( ( flags . auth_host_name , port ) , ClientRedirectHandler ) except socket . error : pass else : success = True break flags . noauth_local_webserver = not success if not success : print ( _FAILED_START_MESSAGE ) if not flags . noauth_local_webserver : oauth_callback = 'http://{host}:{port}/' . format ( host = flags . auth_host_name , port = port_number ) else : oauth_callback = client . OOB_CALLBACK_URN flow . redirect_uri = oauth_callback authorize_url = flow . step1_get_authorize_url ( ) if not flags . noauth_local_webserver : import webbrowser webbrowser . open ( authorize_url , new = 1 , autoraise = True ) print ( _BROWSER_OPENED_MESSAGE . format ( address = authorize_url ) ) else : print ( _GO_TO_LINK_MESSAGE . format ( address = authorize_url ) ) code = None if not flags . noauth_local_webserver : httpd . handle_request ( ) if 'error' in httpd . query_params : sys . exit ( 'Authentication request was rejected.' ) if 'code' in httpd . query_params : code = httpd . query_params [ 'code' ] else : print ( 'Failed to find "code" in the query parameters ' 'of the redirect.' ) sys . exit ( 'Try running with --noauth_local_webserver.' ) else : code = input ( 'Enter verification code: ' ) . strip ( ) try : credential = flow . step2_exchange ( code , http = http ) except client . FlowExchangeError as e : sys . exit ( 'Authentication has failed: {0}' . format ( e ) ) storage . put ( credential ) credential . set_store ( storage ) print ( 'Authentication successful.' ) return credential
4626	def decrypt ( self , wif ) : if not self . unlocked ( ) : raise WalletLocked return format ( bip38 . decrypt ( wif , self . masterkey ) , "wif" )
4152	def ipy_notebook_skeleton ( ) : py_version = sys . version_info notebook_skeleton = { "cells" : [ ] , "metadata" : { "kernelspec" : { "display_name" : "Python " + str ( py_version [ 0 ] ) , "language" : "python" , "name" : "python" + str ( py_version [ 0 ] ) } , "language_info" : { "codemirror_mode" : { "name" : "ipython" , "version" : py_version [ 0 ] } , "file_extension" : ".py" , "mimetype" : "text/x-python" , "name" : "python" , "nbconvert_exporter" : "python" , "pygments_lexer" : "ipython" + str ( py_version [ 0 ] ) , "version" : '{0}.{1}.{2}' . format ( * sys . version_info [ : 3 ] ) } } , "nbformat" : 4 , "nbformat_minor" : 0 } return notebook_skeleton
11397	def add_cms_link ( self ) : intnote = record_get_field_values ( self . record , '690' , filter_subfield_code = "a" , filter_subfield_value = 'INTNOTE' ) if intnote : val_088 = record_get_field_values ( self . record , tag = '088' , filter_subfield_code = "a" ) for val in val_088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record_add_field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )
8180	def add_node ( self , id , radius = 8 , style = style . DEFAULT , category = "" , label = None , root = False , properties = { } ) : if self . has_key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . __dict__ . has_key [ "name" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , * * kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , * * kwargs )
3448	def minimal_medium ( model , min_objective_value = 0.1 , exports = False , minimize_components = False , open_exchanges = False ) : exchange_rxns = find_boundary_types ( model , "exchange" ) if isinstance ( open_exchanges , bool ) : open_bound = 1000 else : open_bound = open_exchanges with model as mod : if open_exchanges : LOGGER . debug ( "Opening exchanges for %d imports." , len ( exchange_rxns ) ) for rxn in exchange_rxns : rxn . bounds = ( - open_bound , open_bound ) LOGGER . debug ( "Applying objective value constraints." ) obj_const = mod . problem . Constraint ( mod . objective . expression , lb = min_objective_value , name = "medium_obj_constraint" ) mod . add_cons_vars ( [ obj_const ] ) mod . solver . update ( ) mod . objective = Zero LOGGER . debug ( "Adding new media objective." ) tol = mod . solver . configuration . tolerances . feasibility if minimize_components : add_mip_obj ( mod ) if isinstance ( minimize_components , bool ) : minimize_components = 1 seen = set ( ) best = num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None exclusion = mod . problem . Constraint ( Zero , ub = 0 ) mod . add_cons_vars ( [ exclusion ] ) mod . solver . update ( ) media = [ ] for i in range ( minimize_components ) : LOGGER . info ( "Finding alternative medium #%d." , ( i + 1 ) ) vars = [ mod . variables [ "ind_" + s ] for s in seen ] if len ( seen ) > 0 : exclusion . set_linear_coefficients ( dict . fromkeys ( vars , 1 ) ) exclusion . ub = best - 1 num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL or num_components > best : break medium = _as_medium ( exchange_rxns , tol , exports = exports ) media . append ( medium ) seen . update ( medium [ medium > 0 ] . index ) if len ( media ) > 1 : medium = pd . concat ( media , axis = 1 , sort = True ) . fillna ( 0.0 ) medium . sort_index ( axis = 1 , inplace = True ) else : medium = media [ 0 ] else : add_linear_obj ( mod ) mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None medium = _as_medium ( exchange_rxns , tol , exports = exports ) return medium
1408	def _is_continue_to_work ( self ) : if not self . _is_topology_running ( ) : return False max_spout_pending = self . pplan_helper . context . get_cluster_config ( ) . get ( api_constants . TOPOLOGY_MAX_SPOUT_PENDING ) if not self . acking_enabled and self . output_helper . is_out_queue_available ( ) : return True elif self . acking_enabled and self . output_helper . is_out_queue_available ( ) and len ( self . in_flight_tuples ) < max_spout_pending : return True elif self . acking_enabled and not self . in_stream . is_empty ( ) : return True else : return False
5589	def hillshade ( elevation , tile , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : azimuth = float ( azimuth ) altitude = float ( altitude ) z = float ( z ) scale = float ( scale ) xres = tile . tile . pixel_x_size yres = - tile . tile . pixel_y_size slope , aspect = calculate_slope_aspect ( elevation , xres , yres , z = z , scale = scale ) deg2rad = math . pi / 180.0 shaded = np . sin ( altitude * deg2rad ) * np . sin ( slope ) + np . cos ( altitude * deg2rad ) * np . cos ( slope ) * np . cos ( ( azimuth - 90.0 ) * deg2rad - aspect ) # shaded now has values between -1.0 and +1.0 # stretch to 0 - 255 and invert shaded = ( ( ( shaded + 1.0 ) / 2 ) * - 255.0 ) . astype ( "uint8" ) # add one pixel padding using the edge values return ma . masked_array ( data = np . pad ( shaded , 1 , mode = 'edge' ) , mask = elevation . mask )
5718	def pull_datapackage ( descriptor , name , backend , * * backend_options ) : # Deprecated warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) # Save datapackage name datapackage_name = name # Get storage plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( * * backend_options ) # Iterate over tables resources = [ ] for table in storage . buckets : # Prepare schema = storage . describe ( table ) base = os . path . dirname ( descriptor ) path , name = _restore_path ( table ) fullpath = os . path . join ( base , path ) # Write data helpers . ensure_dir ( fullpath ) with io . open ( fullpath , 'wb' ) as file : model = Schema ( deepcopy ( schema ) ) data = storage . iter ( table ) writer = csv . writer ( file , encoding = 'utf-8' ) writer . writerow ( model . headers ) for row in data : writer . writerow ( row ) # Add resource resource = { 'schema' : schema , 'path' : path } if name is not None : resource [ 'name' ] = name resources . append ( resource ) # Write descriptor mode = 'w' encoding = 'utf-8' if six . PY2 : mode = 'wb' encoding = None resources = _restore_resources ( resources ) helpers . ensure_dir ( descriptor ) with io . open ( descriptor , mode = mode , encoding = encoding ) as file : descriptor = { 'name' : datapackage_name , 'resources' : resources , } json . dump ( descriptor , file , indent = 4 ) return storage
993	def _getFirstOnBit ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : if input < self . minval : # Don't clip periodic inputs. Out-of-range input is always an error if self . clipInput and not self . periodic : if self . verbosity > 0 : print "Clipped input %s=%.2f to minval %.2f" % ( self . name , input , self . minval ) input = self . minval else : raise Exception ( 'input (%s) less than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : # Don't clip periodic inputs. Out-of-range input is always an error if input >= self . maxval : raise Exception ( 'input (%s) greater than periodic range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) else : if input > self . maxval : if self . clipInput : if self . verbosity > 0 : print "Clipped input %s=%.2f to maxval %.2f" % ( self . name , input , self . maxval ) input = self . maxval else : raise Exception ( 'input (%s) greater than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : centerbin = int ( ( input - self . minval ) * self . nInternal / self . range ) + self . padding else : centerbin = int ( ( ( input - self . minval ) + self . resolution / 2 ) / self . resolution ) + self . padding # We use the first bit to be set in the encoded output as the bucket index minbin = centerbin - self . halfwidth return [ minbin ]
7173	def calc_intent ( self , query ) : matches = self . calc_intents ( query ) if len ( matches ) == 0 : return MatchData ( '' , '' ) best_match = max ( matches , key = lambda x : x . conf ) best_matches = ( match for match in matches if match . conf == best_match . conf ) return min ( best_matches , key = lambda x : sum ( map ( len , x . matches . values ( ) ) ) )
11	def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] if self . compute_Q : logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] logs += [ ( 'episode' , self . n_episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
12381	def delete ( self , request , response ) : if self . slug is None : # Mass-DELETE is not implemented. raise http . exceptions . NotImplemented ( ) # Ensure we're allowed to destroy a resource. self . assert_operations ( 'destroy' ) # Delegate to `destroy` to destroy the item. self . destroy ( ) # Build the response object. self . response . status = http . client . NO_CONTENT self . make_response ( )
590	def _allocateSpatialFDR ( self , rfInput ) : if self . _sfdr : return # Retrieve the necessary extra arguments that were handled automatically autoArgs = dict ( ( name , getattr ( self , name ) ) for name in self . _spatialArgNames ) # Instantiate the spatial pooler class. if ( ( self . SpatialClass == CPPSpatialPooler ) or ( self . SpatialClass == PYSpatialPooler ) ) : autoArgs [ 'columnDimensions' ] = [ self . columnCount ] autoArgs [ 'inputDimensions' ] = [ self . inputWidth ] autoArgs [ 'potentialRadius' ] = self . inputWidth self . _sfdr = self . SpatialClass ( * * autoArgs )
1063	def readheaders ( self ) : self . dict = { } self . unixfrom = '' self . headers = lst = [ ] self . status = '' headerseen = "" firstline = 1 startofline = unread = tell = None if hasattr ( self . fp , 'unread' ) : unread = self . fp . unread elif self . seekable : tell = self . fp . tell while 1 : if tell : try : startofline = tell ( ) except IOError : startofline = tell = None self . seekable = 0 line = self . fp . readline ( ) if not line : self . status = 'EOF in headers' break # Skip unix From name time lines if firstline and line . startswith ( 'From ' ) : self . unixfrom = self . unixfrom + line continue firstline = 0 if headerseen and line [ 0 ] in ' \t' : # It's a continuation line. lst . append ( line ) x = ( self . dict [ headerseen ] + "\n " + line . strip ( ) ) self . dict [ headerseen ] = x . strip ( ) continue elif self . iscomment ( line ) : # It's a comment. Ignore it. continue elif self . islast ( line ) : # Note! No pushback here! The delimiter line gets eaten. break headerseen = self . isheader ( line ) if headerseen : # It's a legal header line, save it. lst . append ( line ) self . dict [ headerseen ] = line [ len ( headerseen ) + 1 : ] . strip ( ) continue elif headerseen is not None : # An empty header name. These aren't allowed in HTTP, but it's # probably a benign mistake. Don't add the header, just keep # going. continue else : # It's not a header line; throw it back and stop here. if not self . dict : self . status = 'No headers' else : self . status = 'Non-header line where header expected' # Try to undo the read. if unread : unread ( line ) elif tell : self . fp . seek ( startofline ) else : self . status = self . status + '; bad seek' break
3095	def http ( self , * args , * * kwargs ) : return self . credentials . authorize ( transport . get_http_object ( * args , * * kwargs ) )
6407	def hmean ( nums ) : if len ( nums ) < 1 : raise AttributeError ( 'hmean requires at least one value' ) elif len ( nums ) == 1 : return nums [ 0 ] else : for i in range ( 1 , len ( nums ) ) : if nums [ 0 ] != nums [ i ] : break else : return nums [ 0 ] if 0 in nums : if nums . count ( 0 ) > 1 : return float ( 'nan' ) return 0 return len ( nums ) / sum ( 1 / i for i in nums )
11998	def _encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . _hmac_generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . _aes_encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
4575	def hsv2rgb_360 ( hsv ) : h , s , v = hsv r , g , b = colorsys . hsv_to_rgb ( h / 360.0 , s , v ) return ( int ( r * 255.0 ) , int ( g * 255.0 ) , int ( b * 255.0 ) )
11459	def strip_fields ( self ) : for tag in self . record . keys ( ) : if tag in self . fields_list : record_delete_fields ( self . record , tag )
6917	def simple_flare_find ( times , mags , errs , smoothbinsize = 97 , flare_minsigma = 4.0 , flare_maxcadencediff = 1 , flare_mincadencepoints = 3 , magsarefluxes = False , savgol_polyorder = 2 , * * savgol_kwargs ) : # if no errs are given, assume 0.1% errors if errs is None : errs = 0.001 * mags # get rid of nans first finiteind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes = times [ finiteind ] fmags = mags [ finiteind ] ferrs = errs [ finiteind ] # now get the smoothed mag series using the filter # kwargs are provided to the savgol_filter function smoothed = savgol_filter ( fmags , smoothbinsize , savgol_polyorder , * * savgol_kwargs ) subtracted = fmags - smoothed # calculate some stats # the series_median is ~zero after subtraction series_mad = np . median ( np . abs ( subtracted ) ) series_stdev = 1.483 * series_mad # find extreme positive deviations if magsarefluxes : extind = np . where ( subtracted > ( flare_minsigma * series_stdev ) ) else : extind = np . where ( subtracted < ( - flare_minsigma * series_stdev ) ) # see if there are any extrema if extind and extind [ 0 ] : extrema_indices = extind [ 0 ] flaregroups = [ ] # find the deviations within the requested flaremaxcadencediff for ind , extrema_index in enumerate ( extrema_indices ) : # FIXME: finish this pass
5226	def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
13546	def get_users ( self , params = { } ) : param_list = [ ( k , params [ k ] ) for k in sorted ( params ) ] url = "/2/users/?%s" % urlencode ( param_list ) data = self . _get_resource ( url ) users = [ ] for entry in data [ "users" ] : users . append ( self . user_from_json ( entry ) ) return users
40	def add ( self , * args , * * kwargs ) : idx = self . _next_idx super ( ) . add ( * args , * * kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
2369	def keywords ( self ) : for table in self . tables : if isinstance ( table , KeywordTable ) : for keyword in table . keywords : yield keyword
13657	def _forObject ( self , obj ) : router = type ( self ) ( ) router . _routes = list ( self . _routes ) router . _self = obj return router
1498	def process_incoming_tuples ( self ) : # back-pressure if self . output_helper . is_out_queue_available ( ) : self . _read_tuples_and_execute ( ) self . output_helper . send_out_tuples ( ) else : # update outqueue full count self . bolt_metrics . update_out_queue_full_count ( )
4943	def get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = None , program_uuid = None ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) # pylint: disable=invalid-name try : if course_id : return get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) return get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : return None
4768	def is_type_of ( self , some_type ) : if type ( some_type ) is not type and not issubclass ( type ( some_type ) , type ) : raise TypeError ( 'given arg must be a type' ) if type ( self . val ) is not some_type : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some_type . __name__ ) ) return self
9013	def knitting_pattern_set ( self , values ) : self . _start ( ) pattern_collection = self . _new_pattern_collection ( ) self . _fill_pattern_collection ( pattern_collection , values ) self . _create_pattern_set ( pattern_collection , values ) return self . _pattern_set
9733	def get_6d_euler ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , euler = QRTPacket . _get_exact ( RT6DBodyEuler , data , component_position ) append_components ( ( position , euler ) ) return components
4631	def point ( self ) : string = unhexlify ( self . unCompressed ( ) ) return ecdsa . VerifyingKey . from_string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point
2966	def _sm_to_pain ( self , * args , * * kwargs ) : _logger . info ( "Starting chaos for blockade %s" % self . _blockade_name ) self . _do_blockade_event ( ) # start the timer to end the pain millisec = random . randint ( self . _run_min_time , self . _run_max_time ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
10939	def calc_accel_correction ( self , damped_JTJ , delta0 ) : #Get the derivative: _ = self . update_function ( self . param_vals ) rm0 = self . calc_residuals ( ) . copy ( ) _ = self . update_function ( self . param_vals + delta0 ) rm1 = self . calc_residuals ( ) . copy ( ) _ = self . update_function ( self . param_vals - delta0 ) rm2 = self . calc_residuals ( ) . copy ( ) der2 = ( rm2 + rm1 - 2 * rm0 ) corr , res , rank , s = np . linalg . lstsq ( damped_JTJ , np . dot ( self . J , der2 ) , rcond = self . min_eigval ) corr *= - 0.5 return corr
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) # This serves one of two purposes depending on run-mode: # - Minimize number of files checked by full-content comparison (hash) # - Minimize chances of file handle exhaustion and limit seeking (exact) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
10595	def Nu_L ( self , L , theta , Ts , * * statef ) : return self . Nu_x ( L , theta , Ts , * * statef ) / 0.75
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
8854	def on_save_as ( self ) : path = self . tabWidget . current_widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = QtWidgets . QFileDialog . getSaveFileName ( self , 'Save' , path ) if filename : self . tabWidget . save_current ( filename ) self . recent_files_manager . open_file ( filename ) self . menu_recents . update_actions ( ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True ) self . _update_status_bar ( self . tabWidget . current_widget ( ) )
8030	def groupByContent ( paths ) : handles , results = [ ] , [ ] # Silently ignore files we don't have permission to read. hList = [ ] for path in paths : try : hList . append ( ( path , open ( path , 'rb' ) , '' ) ) except IOError : pass # TODO: Verbose-mode output here. handles . append ( hList ) while handles : # Process more blocks. more , done = compareChunks ( handles . pop ( 0 ) ) # Add the results to the top-level lists. handles . extend ( more ) results . extend ( done ) # Keep the same API as the others. return dict ( ( x [ 0 ] , x ) for x in results )
4750	def start ( self ) : self . __thread = Threads ( target = self . run , args = ( True , True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
9977	def fix_lamdaline ( source ) : # Using undocumented generate_tokens due to a tokenize.tokenize bug # See https://bugs.python.org/issue23297 strio = io . StringIO ( source ) gen = tokenize . generate_tokens ( strio . readline ) tkns = [ ] try : for t in gen : tkns . append ( t ) except tokenize . TokenError : pass # Find the position of 'lambda' lambda_pos = [ ( t . type , t . string ) for t in tkns ] . index ( ( tokenize . NAME , "lambda" ) ) # Ignore tokes before 'lambda' tkns = tkns [ lambda_pos : ] # Find the position of th las OP lastop_pos = ( len ( tkns ) - 1 - [ t . type for t in tkns [ : : - 1 ] ] . index ( tokenize . OP ) ) lastop = tkns [ lastop_pos ] # Remove OP from the line fiedlineno = lastop . start [ 0 ] fixedline = lastop . line [ : lastop . start [ 1 ] ] + lastop . line [ lastop . end [ 1 ] : ] tkns = tkns [ : lastop_pos ] fixedlines = "" last_lineno = 0 for t in tkns : if last_lineno == t . start [ 0 ] : continue elif t . start [ 0 ] == fiedlineno : fixedlines += fixedline last_lineno = t . start [ 0 ] else : fixedlines += t . line last_lineno = t . start [ 0 ] return fixedlines
8092	def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if s . fill : s . _ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
13664	def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # check if UUID already exists uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : # add the new item to the JSON file products_data . append ( item ) # save the new JSON to the temp file json . dump ( products_data , temp_file ) return True return None
510	def _updateMinDutyCycles ( self ) : if self . _globalInhibition or self . _inhibitionRadius > self . _numInputs : self . _updateMinDutyCyclesGlobal ( ) else : self . _updateMinDutyCyclesLocal ( )
3015	def _from_parsed_json_keyfile ( cls , keyfile_dict , scopes , token_uri = None , revoke_uri = None ) : creds_type = keyfile_dict . get ( 'type' ) if creds_type != client . SERVICE_ACCOUNT : raise ValueError ( 'Unexpected credentials type' , creds_type , 'Expected' , client . SERVICE_ACCOUNT ) service_account_email = keyfile_dict [ 'client_email' ] private_key_pkcs8_pem = keyfile_dict [ 'private_key' ] private_key_id = keyfile_dict [ 'private_key_id' ] client_id = keyfile_dict [ 'client_id' ] if not token_uri : token_uri = keyfile_dict . get ( 'token_uri' , oauth2client . GOOGLE_TOKEN_URI ) if not revoke_uri : revoke_uri = keyfile_dict . get ( 'revoke_uri' , oauth2client . GOOGLE_REVOKE_URI ) signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) credentials = cls ( service_account_email , signer , scopes = scopes , private_key_id = private_key_id , client_id = client_id , token_uri = token_uri , revoke_uri = revoke_uri ) credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem return credentials
9298	def paginate_query ( self , query , count , offset = None , sort = None ) : assert isinstance ( query , peewee . Query ) assert isinstance ( count , int ) assert isinstance ( offset , ( str , int , type ( None ) ) ) assert isinstance ( sort , ( list , set , tuple , type ( None ) ) ) # ensure our model has a primary key fields = query . model . _meta . get_primary_keys ( ) if len ( fields ) == 0 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model without primary key' ) # ensure our model doesn't use a compound primary key if len ( fields ) > 1 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model with compound primary key' ) # apply offset if offset is not None : query = query . where ( fields [ 0 ] >= offset ) # do we need to apply sorting? order_bys = [ ] if sort : for field , direction in sort : # does this field have a valid sort direction? if not isinstance ( direction , str ) : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) direction = direction . lower ( ) . strip ( ) if direction not in [ 'asc' , 'desc' ] : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) # apply sorting order_by = peewee . SQL ( field ) order_by = getattr ( order_by , direction ) ( ) order_bys += [ order_by ] # add primary key ordering after user sorting order_bys += [ fields [ 0 ] . asc ( ) ] # apply ordering and limits query = query . order_by ( * order_bys ) query = query . limit ( count ) return query
7197	def plot ( self , spec = "rgb" , * * kwargs ) : if self . shape [ 0 ] == 1 or ( "bands" in kwargs and len ( kwargs [ "bands" ] ) == 1 ) : if "cmap" in kwargs : cmap = kwargs [ "cmap" ] del kwargs [ "cmap" ] else : cmap = "Greys_r" self . _plot ( tfm = self . _single_band , cmap = cmap , * * kwargs ) else : if spec == "rgb" and self . _has_token ( * * kwargs ) : self . _plot ( tfm = self . rgb , * * kwargs ) else : self . _plot ( tfm = getattr ( self , spec ) , * * kwargs )
7711	def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
10150	def generate ( self , title = None , version = None , base_path = None , info = None , swagger = None , * * kwargs ) : title = title or self . api_title version = version or self . api_version info = info or self . swagger . get ( 'info' , { } ) swagger = swagger or self . swagger base_path = base_path or self . base_path swagger = swagger . copy ( ) info . update ( title = title , version = version ) swagger . update ( swagger = '2.0' , info = info , basePath = base_path ) paths , tags = self . _build_paths ( ) # Update the provided tags with the extracted ones preserving order if tags : swagger . setdefault ( 'tags' , [ ] ) tag_names = { t [ 'name' ] for t in swagger [ 'tags' ] } for tag in tags : if tag [ 'name' ] not in tag_names : swagger [ 'tags' ] . append ( tag ) # Create/Update swagger sections with extracted values where not provided if paths : swagger . setdefault ( 'paths' , { } ) merge_dicts ( swagger [ 'paths' ] , paths ) definitions = self . definitions . definition_registry if definitions : swagger . setdefault ( 'definitions' , { } ) merge_dicts ( swagger [ 'definitions' ] , definitions ) parameters = self . parameters . parameter_registry if parameters : swagger . setdefault ( 'parameters' , { } ) merge_dicts ( swagger [ 'parameters' ] , parameters ) responses = self . responses . response_registry if responses : swagger . setdefault ( 'responses' , { } ) merge_dicts ( swagger [ 'responses' ] , responses ) return swagger
7461	def save_json ( data ) : ## data as dict #### skip _ipcluster because it's made new #### skip _headers because it's loaded new #### statsfiles save only keys #### samples save only keys datadict = OrderedDict ( [ ( "_version" , data . __dict__ [ "_version" ] ) , ( "_checkpoint" , data . __dict__ [ "_checkpoint" ] ) , ( "name" , data . __dict__ [ "name" ] ) , ( "dirs" , data . __dict__ [ "dirs" ] ) , ( "paramsdict" , data . __dict__ [ "paramsdict" ] ) , ( "samples" , data . __dict__ [ "samples" ] . keys ( ) ) , ( "populations" , data . __dict__ [ "populations" ] ) , ( "database" , data . __dict__ [ "database" ] ) , ( "clust_database" , data . __dict__ [ "clust_database" ] ) , ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "barcodes" , data . __dict__ [ "barcodes" ] ) , ( "stats_files" , data . __dict__ [ "stats_files" ] ) , ( "_hackersonly" , data . __dict__ [ "_hackersonly" ] ) , ] ) ## sample dict sampledict = OrderedDict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . _to_fulldict ( ) ## json format it using cumstom Encoder class fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
12902	def _set_range ( self , start , stop , value , value_len ) : assert stop >= start and value_len >= 0 range_len = stop - start if range_len < value_len : self . _insert_zeros ( stop , stop + value_len - range_len ) self . _copy_to_range ( start , value , value_len ) elif range_len > value_len : self . _del_range ( stop - ( range_len - value_len ) , stop ) self . _copy_to_range ( start , value , value_len ) else : self . _copy_to_range ( start , value , value_len )
12448	def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
5099	def _matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" # add the opcode bs += opcode . to_bytes ( 1 , byteorder = "little" ) # Add the index if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
8035	def summarize ( text , char_limit , sentence_filter = None , debug = False ) : debug_info = { } sents = list ( tools . sent_splitter_ja ( text ) ) words_list = [ # pulp variables should be utf-8 encoded w . encode ( 'utf-8' ) for s in sents for w in tools . word_segmenter_ja ( s ) ] tf = collections . Counter ( ) for words in words_list : for w in words : tf [ w ] += 1.0 if sentence_filter is not None : valid_indices = [ i for i , s in enumerate ( sents ) if sentence_filter ( s ) ] sents = [ sents [ i ] for i in valid_indices ] words_list = [ words_list [ i ] for i in valid_indices ] sent_ids = [ str ( i ) for i in range ( len ( sents ) ) ] # sentence id sent_id2len = dict ( ( id_ , len ( s ) ) for id_ , s in zip ( sent_ids , sents ) ) # c word_contain = dict ( ) # a for id_ , words in zip ( sent_ids , words_list ) : word_contain [ id_ ] = collections . defaultdict ( lambda : 0 ) for w in words : word_contain [ id_ ] [ w ] = 1 prob = pulp . LpProblem ( 'summarize' , pulp . LpMaximize ) # x sent_vars = pulp . LpVariable . dicts ( 'sents' , sent_ids , 0 , 1 , pulp . LpBinary ) # z word_vars = pulp . LpVariable . dicts ( 'words' , tf . keys ( ) , 0 , 1 , pulp . LpBinary ) # first, set objective function: sum(w*z) prob += pulp . lpSum ( [ tf [ w ] * word_vars [ w ] for w in tf ] ) # next, add constraints # limit summary length: sum(c*x) <= K prob += pulp . lpSum ( [ sent_id2len [ id_ ] * sent_vars [ id_ ] for id_ in sent_ids ] ) <= char_limit , 'lengthRequirement' # for each term, sum(a*x) <= z for w in tf : prob += pulp . lpSum ( [ word_contain [ id_ ] [ w ] * sent_vars [ id_ ] for id_ in sent_ids ] ) >= word_vars [ w ] , 'z:{}' . format ( w ) prob . solve ( ) # print("Status:", pulp.LpStatus[prob.status]) sent_indices = [ ] for v in prob . variables ( ) : # print v.name, "=", v.varValue if v . name . startswith ( 'sents' ) and v . varValue == 1 : sent_indices . append ( int ( v . name . split ( '_' ) [ - 1 ] ) ) return [ sents [ i ] for i in sent_indices ] , debug_info
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
5412	def build_action ( name = None , image_uri = None , commands = None , entrypoint = None , environment = None , pid_namespace = None , flags = None , port_mappings = None , mounts = None , labels = None ) : return { 'name' : name , 'imageUri' : image_uri , 'commands' : commands , 'entrypoint' : entrypoint , 'environment' : environment , 'pidNamespace' : pid_namespace , 'flags' : flags , 'portMappings' : port_mappings , 'mounts' : mounts , 'labels' : labels , }
536	def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance
5606	def extract_from_array ( in_raster = None , in_affine = None , out_tile = None ) : if isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_raster = in_raster . data # get range within array minrow , maxrow , mincol , maxcol = bounds_to_ranges ( out_bounds = out_tile . bounds , in_affine = in_affine , in_shape = in_raster . shape ) # if output window is within input window if ( minrow >= 0 and mincol >= 0 and maxrow <= in_raster . shape [ - 2 ] and maxcol <= in_raster . shape [ - 1 ] ) : return in_raster [ ... , minrow : maxrow , mincol : maxcol ] # raise error if output is not fully within input else : raise ValueError ( "extraction fails if output shape is not within input" )
8507	def _get_param_names ( self ) : template = Template ( self . yaml_string ) names = [ 'yaml_string' ] # always include the template for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names
10248	def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
6954	def make_combined_periodogram ( pflist , outfile , addmethods = False ) : import matplotlib . pyplot as plt for pf in pflist : if pf [ 'method' ] == 'pdm' : plt . plot ( pf [ 'periods' ] , np . max ( pf [ 'lspvals' ] ) / pf [ 'lspvals' ] - 1.0 , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) else : plt . plot ( pf [ 'periods' ] , pf [ 'lspvals' ] / np . max ( pf [ 'lspvals' ] ) , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) plt . xlabel ( 'period [days]' ) plt . ylabel ( 'normalized periodogram power' ) plt . xscale ( 'log' ) plt . legend ( ) plt . tight_layout ( ) plt . savefig ( outfile ) plt . close ( 'all' ) return outfile
12995	def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , * * kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
4175	def window_blackman ( N , alpha = 0.16 ) : a0 = ( 1. - alpha ) / 2. a1 = 0.5 a2 = alpha / 2. if ( N == 1 ) : win = array ( [ 1. ] ) else : k = arange ( 0 , N ) / float ( N - 1. ) win = a0 - a1 * cos ( 2 * pi * k ) + a2 * cos ( 4 * pi * k ) return win
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
11786	def sanitize ( self , example ) : return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
153	def min_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . left is not None : node = node . left return node . key , node . value
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
13448	def authed_get ( self , url , response_code = 200 , headers = { } , follow = False ) : if not self . authed : self . authorize ( ) response = self . client . get ( url , follow = follow , * * headers ) self . assertEqual ( response_code , response . status_code ) return response
8925	def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
9077	def make_downloader ( url : str , path : str ) -> Callable [ [ bool ] , str ] : # noqa: D202 def download_data ( force_download : bool = False ) -> str : """Download the data. :param force_download: If true, overwrites a previously cached file """ if os . path . exists ( path ) and not force_download : log . info ( 'using cached data at %s' , path ) else : log . info ( 'downloading %s to %s' , url , path ) urlretrieve ( url , path ) return path return download_data
6240	def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
6865	def get_time_flux_errs_from_Ames_lightcurve ( infile , lctype , cadence_min = 2 ) : warnings . warn ( "Use the astrotess.read_tess_fitslc and " "astrotess.consolidate_tess_fitslc functions instead of this function. " "This function will be removed in astrobase v0.4.2." , FutureWarning ) if lctype not in ( 'PDCSAP' , 'SAP' ) : raise ValueError ( 'unknown light curve type requested: %s' % lctype ) hdulist = pyfits . open ( infile ) main_hdr = hdulist [ 0 ] . header lc_hdr = hdulist [ 1 ] . header lc = hdulist [ 1 ] . data if ( ( 'Ames' not in main_hdr [ 'ORIGIN' ] ) or ( 'LIGHTCURVE' not in lc_hdr [ 'EXTNAME' ] ) ) : raise ValueError ( 'could not understand input LC format. ' 'Is it a TESS TOI LC file?' ) time = lc [ 'TIME' ] flux = lc [ '{:s}_FLUX' . format ( lctype ) ] err_flux = lc [ '{:s}_FLUX_ERR' . format ( lctype ) ] # REMOVE POINTS FLAGGED WITH: # attitude tweaks, safe mode, coarse/earth pointing, argabrithening events, # reaction wheel desaturation events, cosmic rays in optimal aperture # pixels, manual excludes, discontinuities, stray light from Earth or Moon # in camera FoV. # (Note: it's not clear to me what a lot of these mean. Also most of these # columns are probably not correctly propagated right now.) sel = ( lc [ 'QUALITY' ] == 0 ) sel &= np . isfinite ( time ) sel &= np . isfinite ( flux ) sel &= np . isfinite ( err_flux ) sel &= ~ np . isnan ( time ) sel &= ~ np . isnan ( flux ) sel &= ~ np . isnan ( err_flux ) sel &= ( time != 0 ) sel &= ( flux != 0 ) sel &= ( err_flux != 0 ) time = time [ sel ] flux = flux [ sel ] err_flux = err_flux [ sel ] # ensure desired cadence lc_cadence_diff = np . abs ( np . nanmedian ( np . diff ( time ) ) * 24 * 60 - cadence_min ) if lc_cadence_diff > 1.0e-2 : raise ValueError ( 'the light curve is not at the required cadence specified: %.2f' % cadence_min ) fluxmedian = np . nanmedian ( flux ) flux /= fluxmedian err_flux /= fluxmedian return time , flux , err_flux
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
